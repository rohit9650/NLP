<doc id="19447" url="https://en.wikipedia.org/wiki?curid=19447" title="Group (mathematics)">
Group (mathematics)

In mathematics, a group is an algebraic structure consisting of a set of elements together with an operation that combines any two elements to form a third element. The operation satisfies four conditions called the group axioms, namely closure, associativity, identity and invertibility. One of the most familiar examples of a group is the set of integers together with the addition operation, but the abstract formalization of the group axioms, detached as it is from the concrete nature of any particular group and its operation, applies much more widely. It allows entities with highly diverse mathematical origins in abstract algebra and beyond to be handled in a flexible way while retaining their essential structural aspects. The ubiquity of groups in numerous areas within and outside mathematics makes them a central organizing principle of contemporary mathematics.
Groups share a fundamental kinship with the notion of symmetry. For example, a symmetry group encodes symmetry features of a geometrical object: the group consists of the set of transformations that leave the object unchanged and the operation of combining two such transformations by performing one after the other. Lie groups are the symmetry groups used in the Standard Model of particle physics; Point groups are used to help understand symmetry phenomena in molecular chemistry; and Poincaré groups can express the physical symmetry underlying special relativity.
The concept of a group arose from the study of polynomial equations, starting with Évariste Galois in the 1830s. After contributions from other fields such as number theory and geometry, the group notion was generalized and firmly established around 1870. Modern group theory—an active mathematical discipline—studies groups in their own right. To explore groups, mathematicians have devised various notions to break groups into smaller, better-understandable pieces, such as subgroups, quotient groups and simple groups. In addition to their abstract properties, group theorists also study the different ways in which a group can be expressed concretely (its group representations), both from a theoretical and a computational point of view. A theory has been developed for finite groups, which culminated with the classification of finite simple groups announced in 1983. Since the mid-1980s, geometric group theory, which studies finitely generated groups as geometric objects, has become a particularly active area in group theory.
Definition and illustration.
First example: the integers.
One of the most familiar groups is the set of integers Z which consists of the numbers
The following properties of integer addition serve as a model for the abstract group axioms given in the definition below.
The integers, together with the operation +, form a mathematical object belonging to a broad class sharing similar structural aspects. To appropriately understand these structures as a collective, the following abstract definition is developed.
Definition.
A group is a set, "G", together with an operation • (called the "group law" of "G") that combines any two elements "a" and "b" to form another element, denoted or "ab". To qualify as a group, the set and operation, , must satisfy four requirements known as the "group axioms":
The result of an operation may depend on the order of the operands. In other words, the result of combining element "a" with element "b" need not yield the same result as combining element "b" with element "a"; the equation
may not always be true. This equation always holds in the group of integers under addition, because for any two integers (commutativity of addition). Groups for which the commutativity equation always holds are called "abelian groups" (in honor of Niels Abel). The symmetry group described in the following section is an example of a group that is not abelian.
The identity element of a group "G" is often written as 1 or 1"G", a notation inherited from the multiplicative identity. The identity element may also be written as 0, especially if the group operation is denoted by +, in which case the group is called an additive group. The identity element can also be written as "id".
The set "G" is called the "underlying set" of the group . Often the group's underlying set "G" is used as a short name for the group . Along the same lines, shorthand expressions such as "a subset of the group "G"" or "an element of group "G"" are used when what is actually meant is "a subset of the underlying set "G" of the group " or "an element of the underlying set "G" of the group ". Usually, it is clear from the context whether a symbol like "G" refers to a group or to an underlying set.
Second example: a symmetry group.
Two figures in the plane are congruent if one can be changed into the other using a combination of rotations, reflections, and translations. Any figure is congruent to itself. However, some figures are congruent to themselves in more than one way, and these extra congruences are called symmetries. A square has eight symmetries. These are:
These symmetries are represented by functions. Each of these functions sends a point in the square to the corresponding point under the symmetry. For example, r1 sends a point to its rotation 90° clockwise around the square's center, and fh sends a point to its reflection across the square's vertical middle line. Composing two of these symmetry functions gives another symmetry function. These symmetries determine a group called the dihedral group of degree 4 and denoted D4. The underlying set of the group is the above set of symmetry functions, and the group operation is function composition. Two symmetries are combined by composing them as functions, that is, applying the first one to the square, and the second one to the result of the first application. The result of performing first "a" and then "b" is written symbolically "from right to left" as
The right-to-left notation is the same notation that is used for composition of functions.
The group table on the right lists the results of all such compositions possible. For example, rotating by 270° clockwise (r3) and then reflecting horizontally (fh) is the same as performing a reflection along the diagonal (fd). Using the above symbols, highlighted in blue in the group table:
Given this set of symmetries and the described operation, the group axioms can be understood as follows:
In contrast to the group of integers above, where the order of the operation is irrelevant, it does matter in D4: but In other words, D4 is not abelian, which makes the group structure more difficult than the integers introduced first.
History.
The modern concept of an abstract group developed out of several fields of mathematics. The original motivation for group theory was the quest for solutions of polynomial equations of degree higher than 4. The 19th-century French mathematician Évariste Galois, extending prior work of Paolo Ruffini and Joseph-Louis Lagrange, gave a criterion for the solvability of a particular polynomial equation in terms of the symmetry group of its roots (solutions). The elements of such a Galois group correspond to certain permutations of the roots. At first, Galois' ideas were rejected by his contemporaries, and published only posthumously. More general permutation groups were investigated in particular by Augustin Louis Cauchy. Arthur Cayley's "On the theory of groups, as depending on the symbolic equation θn = 1" (1854) gives the first abstract definition of a finite group.
Geometry was a second field in which groups were used systematically, especially symmetry groups as part of Felix Klein's 1872 Erlangen program. After novel geometries such as hyperbolic and projective geometry had emerged, Klein used group theory to organize them in a more coherent way. Further advancing these ideas, Sophus Lie founded the study of Lie groups in 1884.
The third field contributing to group theory was number theory. Certain abelian group structures had been used implicitly in Carl Friedrich Gauss' number-theoretical work "Disquisitiones Arithmeticae" (1798), and more explicitly by Leopold Kronecker. In 1847, Ernst Kummer made early attempts to prove Fermat's Last Theorem by developing groups describing factorization into prime numbers.
The convergence of these various sources into a uniform theory of groups started with Camille Jordan's "Traité des substitutions et des équations algébriques" (1870). Walther von Dyck (1882) introduced the idea of specifying a group by means of generators and relations, and was also the first to give an axiomatic definition of an "abstract group", in the terminology of the time. As of the 20th century, groups gained wide recognition by the pioneering work of Ferdinand Georg Frobenius and William Burnside, who worked on representation theory of finite groups, Richard Brauer's modular representation theory and Issai Schur's papers. The theory of Lie groups, and more generally locally compact groups was studied by Hermann Weyl, Élie Cartan and many others. Its algebraic counterpart, the theory of algebraic groups, was first shaped by Claude Chevalley (from the late 1930s) and later by the work of Armand Borel and Jacques Tits.
The University of Chicago's 1960–61 Group Theory Year brought together group theorists such as Daniel Gorenstein, John G. Thompson and Walter Feit, laying the foundation of a collaboration that, with input from numerous other mathematicians, classified all finite simple groups in 1982. This project exceeded previous mathematical endeavours by its sheer size, in both length of proof and number of researchers. Research is ongoing to simplify the proof of this classification. These days, group theory is still a highly active mathematical branch, impacting many other fields.
Elementary consequences of the group axioms.
Basic facts about all groups that can be obtained directly from the group axioms are commonly subsumed under "elementary group theory". For example, repeated applications of the associativity axiom show that the unambiguity of
generalizes to more than three factors. Because this implies that parentheses can be inserted anywhere within such a series of terms, parentheses are usually omitted.
The axioms may be weakened to assert only the existence of a left identity and left inverses. Both can be shown to be actually two-sided, so the resulting definition is equivalent to the one given above.
Uniqueness of identity element and inverses.
Two important consequences of the group axioms are the uniqueness of the identity element and the uniqueness of inverse elements. There can be only one identity element in a group, and each element in a group has exactly one inverse element. Thus, it is customary to speak of "the" identity, and "the" inverse of an element.
To prove the uniqueness of an inverse element of "a", suppose that "a" has two inverses, denoted "b" and "c", in a group ("G", •). Then
The two extremal terms "b" and "c" are equal, since they are connected by a chain of equalities. In other words, there is only one inverse element of "a". Similarly, to prove that the identity element of a group is unique, assume "G" is a group with two identity elements "e" and "f". Then "e" = "e" • "f" = "f", hence "e" and "f" are equal.
Division.
In groups, the invertibility of the group action means that division is possible: given elements "a" and "b" of the group "G", there is exactly one solution "x" in "G" to the equation . In fact, right multiplication of the equation by "a"−1 gives the solution . Similarly there is exactly one solution "y" in "G" to the equation , namely . If the • operation is commutative, we get that "x" = "y". If not, "x" may be different from "y".
A consequence of this is that multiplying by a group element "g" is a bijection. Specifically, if "g" is an element of the group "G", there is a bijection from "G" to itself called "left translation" by "g" sending to . Similarly, "right translation" by "g" is a bijection from "G" to itself sending "h" to . If "G" is abelian, left and right translation by a group element are the same.
Basic concepts.
To understand groups beyond the level of mere symbolic manipulations as above, more structural concepts have to be employed. There is a conceptual principle underlying all of the following notions: to take advantage of the structure offered by groups (which sets, being "structureless", do not have), constructions related to groups have to be "compatible" with the group operation. This compatibility manifests itself in the following notions in various ways. For example, groups can be related to each other via functions called group homomorphisms. By the mentioned principle, they are required to respect the group structures in a precise sense. The structure of groups can also be understood by breaking them into pieces called subgroups and quotient groups. The principle of "preserving structures"—a recurring topic in mathematics throughout—is an instance of working in a category, in this case the category of groups.
Group homomorphisms.
"Group homomorphisms" are functions that preserve group structure. A function between two groups and is called a "homomorphism" if the equation
holds for all elements "g", "k" in "G". In other words, the result is the same when performing the group operation after or before applying the map "a". This requirement ensures that , and also for all "g" in "G". Thus a group homomorphism respects all the structure of "G" provided by the group axioms.
Two groups "G" and "H" are called "isomorphic" if there exist group homomorphisms and , such that applying the two functions one after another in each of the two possible orders gives the identity functions of "G" and "H". That is, and for any "g" in "G" and "h" in "H". From an abstract point of view, isomorphic groups carry the same information. For example, proving that for some element "g" of "G" is equivalent to proving that , because applying "a" to the first equality yields the second, and applying "b" to the second gives back the first.
Subgroups.
Informally, a "subgroup" is a group "H" contained within a bigger one, "G". Concretely, the identity element of "G" is contained in "H", and whenever "h"1 and "h"2 are in "H", then so are and "h"1−1, so the elements of "H", equipped with the group operation on "G" restricted to "H", indeed form a group.
In the example above, the identity and the rotations constitute a subgroup highlighted in red in the group table above: any two rotations composed are still a rotation, and a rotation can be undone by (i.e. is inverse to) the complementary rotations 270° for 90°, 180° for 180°, and 90° for 270° (note that rotation in the opposite direction is not defined). The subgroup test is a necessary and sufficient condition for a subset "H" of a group "G" to be a subgroup: it is sufficient to check that for all elements . Knowing the subgroups is important in understanding the group as a whole.
Given any subset "S" of a group "G", the subgroup generated by "S" consists of products of elements of "S" and their inverses. It is the smallest subgroup of "G" containing "S". In the introductory example above, the subgroup generated by r2 and fv consists of these two elements, the identity element id and . Again, this is a subgroup, because combining any two of these four elements or their inverses (which are, in this particular case, these same elements) yields an element of this subgroup.
Cosets.
In many situations it is desirable to consider two group elements the same if they differ by an element of a given subgroup. For example, in D4 above, once a reflection is performed, the square never gets back to the r2 configuration by just applying the rotation operations (and no further reflections), i.e. the rotation operations are irrelevant to the question whether a reflection has been performed. Cosets are used to formalize this insight: a subgroup "H" defines left and right cosets, which can be thought of as translations of "H" by arbitrary group elements "g". In symbolic terms, the "left" and "right" cosets of "H" containing "g" are
The cosets of any subgroup "H" form a partition of "G"; that is, the union of all left cosets is equal to "G" and two left cosets are either equal or have an empty intersection. The first case happens precisely when , i.e. if the two elements differ by an element of "H". Similar considerations apply to the right cosets of "H". The left and right cosets of "H" may or may not be equal. If they are, i.e. for all "g" in "G", , then "H" is said to be a "normal subgroup".
In D4, the introductory symmetry group, the left cosets "gR" of the subgroup "R" consisting of the rotations are either equal to "R", if "g" is an element of "R" itself, or otherwise equal to (highlighted in green). The subgroup "R" is also normal, because and similarly for any element other than fc. (In fact, in the case of D4, observe that all such cosets are equal, such that .)
Quotient groups.
In some situations the set of cosets of a subgroup can be endowed with a group law, giving a "quotient group" or "factor group". For this to be possible, the subgroup has to be normal. Given any normal subgroup "N", the quotient group is defined by
This set inherits a group operation (sometimes called coset multiplication, or coset addition) from the original group "G": for all "g" and "h" in "G". This definition is motivated by the idea (itself an instance of general structural considerations outlined above) that the map that associates to any element "g" its coset "gN" be a group homomorphism, or by general abstract considerations called universal properties. The coset serves as the identity in this group, and the inverse of "gN" in the quotient group is .
The elements of the quotient group are "R" itself, which represents the identity, and . The group operation on the quotient is shown at the right. For example, . Both the subgroup as well as the corresponding quotient are abelian, whereas D4 is not abelian. Building bigger groups by smaller ones, such as D4 from its subgroup "R" and the quotient is abstracted by a notion called semidirect product.
Quotient groups and subgroups together form a way of describing every group by its "presentation": any group is the quotient of the free group over the "generators" of the group, quotiented by the subgroup of "relations". The dihedral group D4, for example, can be generated by two elements "r" and "f" (for example, "r" = r1, the right rotation and "f" = fv the vertical (or any other) reflection), which means that every symmetry of the square is a finite composition of these two symmetries or their inverses. Together with the relations
the group is completely described. A presentation of a group can also be used to construct the Cayley graph, a device used to graphically capture discrete groups.
Sub- and quotient groups are related in the following way: a subset "H" of "G" can be seen as an injective map , i.e. any element of the target has at most one element that maps to it. The counterpart to injective maps are surjective maps (every element of the target is mapped onto), such as the canonical map . Interpreting subgroup and quotients in light of these homomorphisms emphasizes the structural concept inherent to these definitions alluded to in the introduction. In general, homomorphisms are neither injective nor surjective. Kernel and image of group homomorphisms and the first isomorphism theorem address this phenomenon.
Examples and applications.
Examples and applications of groups abound. A starting point is the group Z of integers with addition as group operation, introduced above. If instead of addition multiplication is considered, one obtains multiplicative groups. These groups are predecessors of important constructions in abstract algebra.
Groups are also applied in many other mathematical areas. Mathematical objects are often examined by associating groups to them and studying the properties of the corresponding groups. For example, Henri Poincaré founded what is now called algebraic topology by introducing the fundamental group. By means of this connection, topological properties such as proximity and continuity translate into properties of groups. For example, elements of the fundamental group are represented by loops. The second image at the right shows some loops in a plane minus a point. The blue loop is considered null-homotopic (and thus irrelevant), because it can be continuously shrunk to a point. The presence of the hole prevents the orange loop from being shrunk to a point. The fundamental group of the plane with a point deleted turns out to be infinite cyclic, generated by the orange loop (or any other loop winding once around the hole). This way, the fundamental group detects the hole.
In more recent applications, the influence has also been reversed to motivate geometric constructions by a group-theoretical background. In a similar vein, geometric group theory employs geometric concepts, for example in the study of hyperbolic groups. Further branches crucially applying groups include algebraic geometry and number theory.
In addition to the above theoretical applications, many practical applications of groups exist. Cryptography relies on the combination of the abstract group theory approach together with algorithmical knowledge obtained in computational group theory, in particular when implemented for finite groups. Applications of group theory are not restricted to mathematics; sciences such as physics, chemistry and computer science benefit from the concept.
Numbers.
Many number systems, such as the integers and the rationals enjoy a naturally given group structure. In some cases, such as with the rationals, both addition and multiplication operations give rise to group structures. Such number systems are predecessors to more general algebraic structures known as rings and fields. Further abstract algebraic concepts such as modules, vector spaces and algebras also form groups.
Integers.
The group of integers Z under addition, denoted (Z, +), has been described above. The integers, with the operation of multiplication instead of addition, (Z, ·) do "not" form a group. The closure, associativity and identity axioms are satisfied, but inverses do not exist: for example, is an integer, but the only solution to the equation in this case is , which is a rational number, but not an integer. Hence not every element of Z has a (multiplicative) inverse.
Rationals.
The desire for the existence of multiplicative inverses suggests considering fractions
Fractions of integers (with "b" nonzero) are known as rational numbers. The set of all such fractions is commonly denoted Q. There is still a minor obstacle for the rationals with multiplication, being a group: because the rational number 0 does not have a multiplicative inverse (i.e., there is no "x" such that ), is still not a group.
However, the set of all "nonzero" rational numbers does form an abelian group under multiplication, denoted . Associativity and identity element axioms follow from the properties of integers. The closure requirement still holds true after removing zero, because the product of two nonzero rationals is never zero. Finally, the inverse of "a"/"b" is "b"/"a", therefore the axiom of the inverse element is satisfied.
The rational numbers (including 0) also form a group under addition. Intertwining addition and multiplication operations yields more complicated structures called rings and—if division is possible, such as in Q—fields, which occupy a central position in abstract algebra. Group theoretic arguments therefore underlie parts of the theory of those entities.
Modular arithmetic.
In modular arithmetic, two integers are added and then the sum is divided by a positive integer called the "modulus." The result of modular addition is the remainder of that division. For any modulus, "n", the set of integers from 0 to forms a group under modular addition: the inverse of any element "a" is , and 0 is the identity element. This is familiar from the addition of hours on the face of a clock: if the hour hand is on 9 and is advanced 4 hours, it ends up on 1, as shown at the right. This is expressed by saying that 9 + 4 equals 1 "modulo 12" or, in symbols,
The group of integers modulo "n" is written Z"n" or Z/"n"Z.
For any prime number "p", there is also the multiplicative group of integers modulo "p". Its elements are the integers 1 to . The group operation is multiplication modulo "p". That is, the usual product is divided by "p" and the remainder of this division is the result of modular multiplication. For example, if , there are four group elements 1, 2, 3, 4. In this group, , because the usual product 16 is equivalent to 1, which divided by 5 yields a remainder of 1. for 5 divides , denoted
The primality of "p" ensures that the product of two integers neither of which is divisible by "p" is not divisible by "p" either, hence the indicated set of classes is closed under multiplication. The identity element is 1, as usual for a multiplicative group, and the associativity follows from the corresponding property of integers. Finally, the inverse element axiom requires that given an integer "a" not divisible by "p", there exists an integer "b" such that
The inverse "b" can be found by using Bézout's identity and the fact that the greatest common divisor equals 1. In the case above, the inverse of 4 is 4, and the inverse of 3 is 2, as . Hence all group axioms are fulfilled. Actually, this example is similar to above: it consists of exactly those elements in Z/"p"Z that have a multiplicative inverse. These groups are denoted F"p"×. They are crucial to public-key cryptography.
Cyclic groups.
A "cyclic group" is a group all of whose elements are powers of a particular element "a". In multiplicative notation, the elements of the group are:
where "a"2 means "a" • "a", and "a−3" stands for "a"−1 • "a"−1 • "a"−1 = ("a" • "a" • "a")−1 etc. Such an element "a" is called a generator or a primitive element of the group. In additive notation, the requirement for an element to be primitive is that each element of the group can be written as
In the groups Z/"n"Z introduced above, the element 1 is primitive, so these groups are cyclic. Indeed, each element is expressible as a sum all of whose terms are 1. Any cyclic group with "n" elements is isomorphic to this group. A second example for cyclic groups is the group of "n"-th complex roots of unity, given by complex numbers "z" satisfying . These numbers can be visualized as the vertices on a regular "n"-gon, as shown in blue at the right for . The group operation is multiplication of complex numbers. In the picture, multiplying with "z" corresponds to a counter-clockwise rotation by 60°. Using some field theory, the group F"p"× can be shown to be cyclic: for example, if , 3 is a generator since , , , and .
Some cyclic groups have an infinite number of elements. In these groups, for every non-zero element "a", all the powers of "a" are distinct; despite the name "cyclic group", the powers of the elements do not cycle. An infinite cyclic group is isomorphic to , the group of integers under addition introduced above. As these two prototypes are both abelian, so is any cyclic group.
The study of finitely generated abelian groups is quite mature, including the fundamental theorem of finitely generated abelian groups; and reflecting this state of affairs, many group-related notions, such as center and commutator, describe the extent to which a given group is not abelian.
Symmetry groups.
"Symmetry groups" are groups consisting of symmetries of given mathematical objects—be they of geometric nature, such as the introductory symmetry group of the square, or of algebraic nature, such as polynomial equations and their solutions. Conceptually, group theory can be thought of as the study of symmetry. Symmetries in mathematics greatly simplify the study of geometrical or analytical objects. A group is said to act on another mathematical object "X" if every group element performs some operation on "X" compatibly to the group law. In the rightmost example below, an element of order 7 of the (2,3,7) triangle group acts on the tiling by permuting the highlighted warped triangles (and the other ones, too). By a group action, the group pattern is connected to the structure of the object being acted on.
In chemical fields, such as crystallography, space groups and point groups describe molecular symmetries and crystal symmetries. These symmetries underlie the chemical and physical behavior of these systems, and group theory enables simplification of quantum mechanical analysis of these properties. For example, group theory is used to show that optical transitions between certain quantum levels cannot occur simply because of the symmetry of the states involved.
Not only are groups useful to assess the implications of symmetries in molecules, but surprisingly they also predict that molecules sometimes can change symmetry. The Jahn-Teller effect is a distortion of a molecule of high symmetry when it adopts a particular ground state of lower symmetry from a set of possible ground states that are related to each other by the symmetry operations of the molecule.
Likewise, group theory helps predict the changes in physical properties that occur when a material undergoes a phase transition, for example, from a cubic to a tetrahedral crystalline form. An example is ferroelectric materials, where the change from a paraelectric to a ferroelectric state occurs at the Curie temperature and is related to a change from the high-symmetry paraelectric state to the lower symmetry ferroelectic state, accompanied by a so-called soft phonon mode, a vibrational lattice mode that goes to zero frequency at the transition.
Such spontaneous symmetry breaking has found further application in elementary particle physics, where its occurrence is related to the appearance of Goldstone bosons.
Finite symmetry groups such as the Mathieu groups are used in coding theory, which is in turn applied in error correction of transmitted data, and in CD players. Another application is differential Galois theory, which characterizes functions having antiderivatives of a prescribed form, giving group-theoretic criteria for when solutions of certain differential equations are well-behaved. Geometric properties that remain stable under group actions are investigated in (geometric) invariant theory.
General linear group and representation theory.
Matrix groups consist of matrices together with matrix multiplication. The "general linear group" consists of all invertible "n"-by-"n" matrices with real entries. Its subgroups are referred to as "matrix groups" or "linear groups". The dihedral group example mentioned above can be viewed as a (very small) matrix group. Another important matrix group is the special orthogonal group SO("n"). It describes all possible rotations in "n" dimensions. Via Euler angles, rotation matrices are used in computer graphics.
"Representation theory" is both an application of the group concept and important for a deeper understanding of groups. It studies the group by its group actions on other spaces. A broad class of group representations are linear representations, i.e. the group is acting on a vector space, such as the three-dimensional Euclidean space R3. A representation of "G" on an "n"-dimensional real vector space is simply a group homomorphism
from the group to the general linear group. This way, the group operation, which may be abstractly given, translates to the multiplication of matrices making it accessible to explicit computations.
Given a group action, this gives further means to study the object being acted on. On the other hand, it also yields information about the group. Group representations are an organizing principle in the theory of finite groups, Lie groups, algebraic groups and topological groups, especially (locally) compact groups.
Galois groups.
"Galois groups" were developed to help solve polynomial equations by capturing their symmetry features. For example, the solutions of the quadratic equation are given by
Exchanging "+" and "−" in the expression, i.e. permuting the two solutions of the equation can be viewed as a (very simple) group operation. Similar formulae are known for cubic and quartic equations, but do "not" exist in general for degree 5 and higher. Abstract properties of Galois groups associated with polynomials (in particular their solvability) give a criterion for polynomials that have all their solutions expressible by radicals, i.e. solutions expressible using solely addition, multiplication, and roots similar to the formula above.
The problem can be dealt with by shifting to field theory and considering the splitting field of a polynomial. Modern Galois theory generalizes the above type of Galois groups to field extensions and establishes—via the fundamental theorem of Galois theory—a precise relationship between fields and groups, underlining once again the ubiquity of groups in mathematics.
Finite groups.
A group is called "finite" if it has a finite number of elements. The number of elements is called the order of the group. An important class is the "symmetric groups" S"N", the groups of permutations of "N" letters. For example, the symmetric group on 3 letters S3 is the group consisting of all possible orderings of the three letters "ABC", i.e. contains the elements "ABC", "ACB", ..., up to "CBA", in total 6 (or 3 factorial) elements. This class is fundamental insofar as any finite group can be expressed as a subgroup of a symmetric group S"N" for a suitable integer "N" (Cayley's theorem). Parallel to the group of symmetries of the square above, S3 can also be interpreted as the group of symmetries of an equilateral triangle.
The order of an element "a" in a group "G" is the least positive integer "n" such that "a n = e", where "a n" represents
i.e. application of the operation • to "n" copies of "a". (If • represents multiplication, then "a""n" corresponds to the "n"th power of "a".) In infinite groups, such an "n" may not exist, in which case the order of "a" is said to be infinity. The order of an element equals the order of the cyclic subgroup generated by this element.
More sophisticated counting techniques, for example counting cosets, yield more precise statements about finite groups: Lagrange's Theorem states that for a finite group "G" the order of any finite subgroup "H" divides the order of "G". The Sylow theorems give a partial converse.
The dihedral group (discussed above) is a finite group of order 8. The order of r1 is 4, as is the order of the subgroup "R" it generates (see above). The order of the reflection elements fv etc. is 2. Both orders divide 8, as predicted by Lagrange's theorem. The groups F"p"× above have order .
Classification of finite simple groups.
Mathematicians often strive for a complete classification (or list) of a mathematical notion. In the context of finite groups, this aim leads to difficult mathematics. According to Lagrange's theorem, finite groups of order "p", a prime number, are necessarily cyclic (abelian) groups Z"p". Groups of order "p"2 can also be shown to be abelian, a statement which does not generalize to order "p"3, as the non-abelian group D4 of order 8 = 23 above shows. Computer algebra systems can be used to list small groups, but there is no classification of all finite groups. An intermediate step is the classification of finite simple groups. A nontrivial group is called "simple" if its only normal subgroups are the trivial group and the group itself. The Jordan–Hölder theorem exhibits finite simple groups as the building blocks for all finite groups. Listing all finite simple groups was a major achievement in contemporary group theory. 1998 Fields Medal winner Richard Borcherds succeeded to prove the monstrous moonshine conjectures, a surprising and deep relation of the largest finite simple sporadic group—the "monster group"—with certain modular functions, a piece of classical complex analysis, and string theory, a theory supposed to unify the description of many physical phenomena.
Groups with additional structure.
Many groups are simultaneously groups and examples of other mathematical structures. In the language of category theory, they are group objects in a category, meaning that they are objects (that is, examples of another mathematical structure) which come with transformations (called morphisms) that mimic the group axioms. For example, every group (as defined above) is also a set, so a group is a group object in the category of sets.
Topological groups.
Some topological spaces may be endowed with a group law. In order for the group law and the topology to interweave well, the group operations must be continuous functions, that is, , and "g"−1 must not vary wildly if "g" and "h" vary only little. Such groups are called "topological groups," and they are the group objects in the category of topological spaces. The most basic examples are the reals R under addition, , and similarly with any other topological field such as the complex numbers or "p"-adic numbers. All of these groups are locally compact, so they have Haar measures and can be studied via harmonic analysis. The former offer an abstract formalism of invariant integrals. Invariance means, in the case of real numbers for example:
for any constant "c". Matrix groups over these fields fall under this regime, as do adele rings and adelic algebraic groups, which are basic to number theory. Galois groups of infinite field extensions such as the absolute Galois group can also be equipped with a topology, the so-called Krull topology, which in turn is central to generalize the above sketched connection of fields and groups to infinite field extensions. An advanced generalization of this idea, adapted to the needs of algebraic geometry, is the étale fundamental group.
Lie groups.
"Lie groups" (in honor of Sophus Lie) are groups which also have a manifold structure, i.e. they are spaces looking locally like some Euclidean space of the appropriate dimension. Again, the additional structure, here the manifold structure, has to be compatible, i.e. the maps corresponding to multiplication and the inverse have to be smooth.
A standard example is the general linear group introduced above: it is an open subset of the space of all "n"-by-"n" matrices, because it is given by the inequality
where "A" denotes an "n"-by-"n" matrix.
Lie groups are of fundamental importance in modern physics: Noether's theorem links continuous symmetries to conserved quantities. Rotation, as well as translations in space and time are basic symmetries of the laws of mechanics. They can, for instance, be used to construct simple models—imposing, say, axial symmetry on a situation will typically lead to significant simplification in the equations one needs to solve to provide a physical description. Another example are the Lorentz transformations, which relate measurements of time and velocity of two observers in motion relative to each other. They can be deduced in a purely group-theoretical way, by expressing the transformations as a rotational symmetry of Minkowski space. The latter serves—in the absence of significant gravitation—as a model of space time in special relativity. The full symmetry group of Minkowski space, i.e. including translations, is known as the Poincaré group. By the above, it plays a pivotal role in special relativity and, by implication, for quantum field theories. Symmetries that vary with location are central to the modern description of physical interactions with the help of gauge theory.
Generalizations.
In abstract algebra, more general structures are defined by relaxing some of the axioms defining a group. For example, if the requirement that every element has an inverse is eliminated, the resulting algebraic structure is called a monoid. The natural numbers N (including 0) under addition form a monoid, as do the nonzero integers under multiplication , see above. There is a general method to formally add inverses to elements to any (abelian) monoid, much the same way as is derived from , known as the Grothendieck group.
Groupoids are similar to groups except that the composition "a" • "b" need not be defined for all "a" and "b". They arise in the study of more complicated forms of symmetry, often in topological and analytical structures, such as the fundamental groupoid or stacks. Finally, it is possible to generalize any of these concepts by replacing the binary operation with an arbitrary "n"-ary one (i.e. an operation taking "n" arguments). With the proper generalization of the group axioms this gives rise to an "n"-ary group. The table gives a list of several structures generalizing groups.

</doc>
<doc id="12695" url="https://en.wikipedia.org/wiki?curid=12695" title="Group representation">
Group representation

In the mathematical field of representation theory, group representations describe abstract groups in terms of linear transformations of vector spaces; in particular, they can be used to represent group elements as matrices so that the group operation can be represented by matrix multiplication. Representations of groups are important because they allow many group-theoretic problems to be reduced to problems in linear algebra, which is well understood. They are also important in physics because, for example, they describe how the symmetry group of a physical system affects the solutions of equations describing that system. 
The term "representation of a group" is also used in a more general sense to mean any "description" of a group as a group of transformations of some mathematical object. More formally, a "representation" means a homomorphism from the group to the automorphism group of an object. If the object is a vector space we have a "linear representation". Some people use "realization" for the general notion and reserve the term "representation" for the special case of linear representations. The bulk of this article describes linear representation theory; see the last section for generalizations.
Branches of group representation theory.
The representation theory of groups divides into subtheories depending on the kind of group being represented. The various theories are quite different in detail, though some basic definitions and concepts are similar. The most important divisions are:
Representation theory also depends heavily on the type of vector space on which the group acts. One distinguishes between finite-dimensional representations and infinite-dimensional ones. In the infinite-dimensional case, additional structures are important (e.g. whether or not the space is a Hilbert space, Banach space, etc.).
One must also consider the type of field over which the vector space is defined. The most important case is the field of complex numbers. The other important cases are the field of real numbers, finite fields, and fields of p-adic numbers. In general, algebraically closed fields are easier to handle than non-algebraically closed ones. The characteristic of the field is also significant; many theorems for finite groups depend on the characteristic of the field not dividing the order of the group.
Definitions.
A representation of a group "G" on a vector space "V" over a field "K" is a group homomorphism from "G" to GL("V"), the general linear group on "V". That is, a representation is a map 
such that
Here "V" is called the representation space and the dimension of "V" is called the dimension of the representation. It is common practice to refer to "V" itself as the representation when the homomorphism is clear from the context.
In the case where "V" is of finite dimension "n" it is common to choose a basis for "V" and identify GL("V") with , the group of "n"-by-"n" invertible matrices on the field "K".
Examples.
Consider the complex number "u" = e2πi / 3 which has the property "u"3 = 1. The cyclic group "C"3 = {1, "u", "u"2} has a representation ρ on C2 given by:
This representation is faithful because ρ is a one-to-one map.
An isomorphic representation for "C"3 is
The group "C"3 may also be faithfully represented on R2 by
where 
Reducibility.
A subspace "W" of "V" that is invariant under the group action is called a "subrepresentation". If "V" has exactly two subrepresentations, namely the zero-dimensional subspace and "V" itself, then the representation is said to be "irreducible"; if it has a proper subrepresentation of nonzero dimension, the representation is said to be "reducible". The representation of dimension zero is considered to be neither reducible nor irreducible, just like the number 1 is considered to be neither composite nor prime.
Under the assumption that the characteristic of the field "K" does not divide the size of the group, representations of finite groups can be decomposed into a direct sum of irreducible subrepresentations (see Maschke's theorem). This holds in particular for any representation of a finite group over the complex numbers, since the characteristic of the complex numbers is zero, which never divides the size of a group.
In the example above, the first two representations given are both decomposable into two 1-dimensional subrepresentations (given by span{(1,0)} and span{(0,1)}), while the third representation is irreducible.
Generalizations.
Set-theoretical representations.
A "set-theoretic representation" (also known as a group action or "permutation representation") of a group "G" on a set "X" is given by a function ρ : "G" → "X""X", the set of functions from "X" to "X", such that for all "g"1, "g"2 in "G" and all "x" in "X":
This condition and the axioms for a group imply that ρ("g") is a bijection (or permutation) for all "g" in "G". Thus we may equivalently define a permutation representation to be a group homomorphism from G to the symmetric group S"X" of "X".
For more information on this topic see the article on group action.
Representations in other categories.
Every group "G" can be viewed as a category with a single object; morphisms in this category are just the elements of "G". Given an arbitrary category "C", a "representation" of "G" in "C" is a functor from "G" to "C". Such a functor selects an object "X" in "C" and a group homomorphism from "G" to Aut("X"), the automorphism group of "X".
In the case where "C" is Vect"K", the category of vector spaces over a field "K", this definition is equivalent to a linear representation. Likewise, a set-theoretic representation is just a representation of "G" in the category of sets.
When "C" is Ab, the category of abelian groups, the objects obtained are called "G"-modules.
For another example consider the category of topological spaces, Top. Representations in Top are homomorphisms from "G" to the homeomorphism group of a topological space "X".
Two types of representations closely related to linear representations are:

</doc>
<doc id="12778" url="https://en.wikipedia.org/wiki?curid=12778" title="Group velocity">
Group velocity

The group velocity of a wave is the velocity with which the overall shape of the waves' amplitudes—known as the "modulation" or "envelope" of the wave—propagates through space.
For example, if a stone is thrown into the middle of a very still pond, a circular pattern of waves with a quiescent center appears in the water. The expanding ring of waves is the wave group, within which one can discern individual wavelets of differing wavelengths traveling at different speeds. The longer waves travel faster than the group as a whole, but their amplitudes diminish as they approach the leading edge. The shorter waves travel more slowly, and their amplitudes diminish as they emerge from the trailing boundary of the group.
Definition and interpretation.
Definition.
The group velocity "vg" is defined by the equation:
where "ω" is the wave's angular frequency (usually expressed in radians per second), and "k" is the angular wavenumber (usually expressed in radians per meter).
The function "ω"("k"), which gives "ω" as a function of "k", is known as the dispersion relation.
Derivation.
One derivation of the formula for group velocity is as follows.
Consider a wave packet as a function of position "x" and time "t": "α"("x","t"). Let "A"("k") be its Fourier transform at time "t"=0:
By the superposition principle, the wavepacket at any time "t" is:
where ω is implicitly a function of "k". We assume that the wave packet α is almost monochromatic, so that "A"("k") is sharply peaked around a central wavenumber "k"0. Then, linearization gives:
where formula_5 and formula_6 (see next section for discussion of this step). Then, after some algebra,
There are two factors in this expression. The first factor, formula_8, describes a perfect monochromatic wave with wavevector formula_9, with peaks and troughs moving at the phase velocity formula_10 within the envelope of the wavepacket. The other factor, formula_11, gives the envelope of the wavepacket. This envelope function depends on position and time "only" through the combination formula_12. Therefore, the envelope of the wavepacket travels at velocity formula_13. This explains the group velocity formula.
Higher-order terms in dispersion.
Part of the previous derivation is the assumption:
If the wavepacket has a relatively large frequency spread, or if the dispersion formula_15 has sharp variations (such as due to a resonance), or if the packet travels over very long distances, this assumption is not valid, and higher-order terms in the Taylor expansion become important. As a result, the envelope of the wave packet not only moves, but also "distorts," in a manner that can be described by the material's group velocity dispersion. Loosely speaking, different frequency-components of the wavepacket travel at different speeds, with the faster components moving towards the front of the wavepacket and the slower moving towards the back. Eventually, the wave packet gets stretched out. This is an important effect in the propagation of signals through optical fibers and in the design of high-power, short-pulse lasers.
History.
The idea of a group velocity distinct from a wave's phase velocity was first proposed by W.R. Hamilton in 1839, and the first full treatment was by Rayleigh in his "Theory of Sound" in 1877.
Other expressions.
For light, the refractive index "n", vacuum wavelength λ0, and wavelength in the medium λ, are related by
with "v"p = "ω"/"k" the phase velocity.
The group velocity, therefore, can be calculated by any of the following formulas:
In three dimensions.
For waves traveling through three dimensions, such as light waves, sound waves, and matter waves, the formulas for phase and group velocity are generalized in a straightforward way:
where formula_20 means the gradient of the angular frequency formula_21 as a function of the wave vector formula_22, and formula_23 is the unit vector in direction k.
If the waves are propagating through an anisotropic (i.e., not rotationally symmetric) medium, for example a crystal, then the phase velocity vector and group velocity vector may point in different directions.
In lossy or gainful media.
The group velocity is often thought of as the velocity at which energy or information is conveyed along a wave. In most cases this is accurate, and the group velocity can be thought of as the signal velocity of the waveform. However, if the wave is travelling through an absorptive or gainful medium, this does not always hold. In these cases the group velocity may not be a well-defined quantity, or may not be a meaningful quantity.
In his text “Wave Propagation in Periodic Structures”, Brillouin argued that in a dissipative medium the group velocity ceases to have a clear physical meaning. An example concerning the transmission of electromagnetic waves through an atomic gas is given by Loudon. Another example is mechanical waves in the solar photosphere: The waves are damped (by radiative heat flow from the peaks to the troughs), and related to that, the energy velocity is often substantially lower than the waves' group velocity.
Despite this ambiguity, a common way to extend the concept of group velocity to complex media is to consider spatially damped plane wave solutions inside the medium, which are characterized by a "complex-valued" wavevector. Then, the imaginary part of the wavevector is arbitrarily discarded and the usual formula for group velocity is applied to the real part of wavevector, i.e.:
Or, equivalently, in terms of the real part of complex refractive index formula_25 one has
It can be shown that this generalization of group velocity continues to be related to the apparent speed of the peak of a wavepacket. The above definition is not universal, however: alternatively one may consider the time damping of standing waves (real formula_27, complex formula_21), or, allow group velocity to be a complex-valued quantity. Different considerations yield distinct velocities, yet all definitions agree for the case of a lossless, gainless medium.
The above generalization of group velocity for complex media can behave strangely, and the example of anomalous dispersion serves as a good illustration.
At the edges of a region of anomalous dispersion, formula_29 becomes infinite (surpassing even the speed of light in vacuum), and formula_29 may easily become negative (its sign opposes formula_31) inside the band of anomalous dispersion.
Superluminal group velocities.
Since the 1980s, various experiments have verified that it is possible for the group velocity (as defined above) of laser light pulses sent through lossy materials, or gainful materials, to significantly exceed the speed of light in vacuum "c". The peaks of wavepackets were also seen to move faster than "c". In all these cases, however, there is no possibility that signals could be carried faster than the speed of light in vacuum, since the high value of formula_29 does not help to speed up the true motion of the sharp wavefront that would occur at the start of any real signal.

</doc>
<doc id="1228679" url="https://en.wikipedia.org/wiki?curid=1228679" title="Gyromagnetic ratio">
Gyromagnetic ratio

In physics, the gyromagnetic ratio (also sometimes known as the magnetogyric ratio in other disciplines) of a particle or system is the ratio of its magnetic dipole moment to its angular momentum, and it is often denoted by the symbol γ, gamma. Its SI unit is the radian per second per tesla (rad⋅s−1⋅T−1) or, equivalently, the coulomb per kilogram (C⋅kg−1).
The term "gyromagnetic ratio" is sometimes used as a synonym for a "different" but closely related quantity, the g-factor. The g-factor, unlike the gyromagnetic ratio, is dimensionless. For more on the g-factor, see below, or see the article g-factor.
Gyromagnetic ratio and Larmor precession.
Any free system with a constant gyromagnetic ratio, such as a rigid system of charges, a nucleus, or an electron, when placed in an external magnetic field "B" (measured in teslas) that is not aligned with its magnetic moment, will precess at a frequency "f" (measured in hertz), that is proportional to the external field:
For this reason, values of "γ"/(2"π"), in units of hertz per tesla (Hz/T), are often quoted instead of "γ".
This relationship also explains an apparent contradiction between the two equivalent terms, gyromagnetic ratio versus magnetogyric ratio: whereas it is a ratio of a magnetic property (i.e. dipole moment) to a "gyric" (rotational, from , "turn") property (i.e. angular momentum), it is also, "at the same time", a ratio between the angular precession frequency (another "gyric" property) and the magnetic field.
Gyromagnetic ratio for a classical rotating body.
Consider a charged body rotating about an axis of symmetry. According to the laws of classical physics, it has both a magnetic dipole moment and an angular momentum due to its rotation. It can be shown that as long as its charge and mass are distributed identically (e.g., both distributed uniformly), its gyromagnetic ratio is
where "q" is its charge and "m" is its mass. The derivation of this relation is as follows:
It suffices to demonstrate this for an infinitesimally narrow circular ring within the body, as the general result follows from an integration. Suppose the ring has radius "r", area , mass "m", charge "q", and angular momentum . Then the magnitude of the magnetic dipole moment is
Gyromagnetic ratio for an isolated electron.
An isolated electron has an angular momentum and a magnetic moment resulting from its spin. While an electron's spin is sometimes visualized as a literal rotation about an axis, it cannot be attributed to mass distributed identically to the charge. The above classical relation does not hold, giving the wrong result by a dimensionless factor called the electron g-factor, denoted "g"e (or just "g" when there is no risk of confusion):
where "μ"B is the Bohr magneton. As mentioned above, in classical physics one would expect the "g"-factor to be . However in the framework of relativistic quantum mechanics,
where formula_6 is the fine-structure constant. Here the small corrections to the relativistic result come from the quantum field theory. Experimentally, the electron "g"-factor has been measured to twelve decimal places:
The electron gyromagnetic ratio is given by NIST as
The g-factor and "γ" are in excellent agreement with theory; see Precision tests of QED for details.
Gyromagnetic factor as a consequence of relativity.
Since a gyromagnetic factor equal to 2 follows from the Dirac's equation it is a frequent misconception to think that a g-factor 2 is a consequence of relativity; it is not. The factor 2 can be obtained from the linearization of both the Schrödinger equation and the relativistic Klein–Gordon equation (which leads to Dirac's). In both cases a 4-spinor is obtained and for both linearizations the g-factor is found to be equal to 2; Therefore, the factor 2 is a consequence of the wave equation dependency on the first (and not the second) derivatives with respect to space and time.
Physical spin-1/2 particles which can not be described by the linear gauged Dirac equation satisfy the gauged Klein-Gordon equation extended by the g term according to,
Here, and stand for the Lorentz group generators in the Dirac space, and the electromagnetic tensor respectively, while is the electromagnetic four-potential. An example for such a particle, according to, is the spin-1/2 companion to spin-3/2 in the representation space of the Lorentz group. This particle has been shown to be characterized by and consequently to behave as a truly quadratic fermion.
Gyromagnetic ratio for a nucleus.
Protons, neutrons, and many nuclei carry nuclear spin, which gives rise to a gyromagnetic ratio as above. The ratio is conventionally written in terms of the proton mass and charge, even for neutrons and for other nuclei, for the sake of simplicity and consistency. The formula is:
where formula_12 is the nuclear magneton, and formula_13 is the g-factor of the nucleon or nucleus in question.
The gyromagnetic ratio of a nucleus plays a role in nuclear magnetic resonance (NMR) and magnetic resonance imaging (MRI). These procedures rely on the fact that bulk magnetization due to nuclear spins precess in a magnetic field at a rate called the Larmor frequency, which is simply the product of the gyromagnetic ratio with the magnetic field strength. With this phenomenon, the sign of "γ" determines the sense (clockwise vs counterclockwise) of precession.
Most common nuclei such as 1H and 13C have positive gyromagnetic ratios. 
Approximate values for some common nuclei are given in the table below.

</doc>
<doc id="44125" url="https://en.wikipedia.org/wiki?curid=44125" title="Gyroscope">
Gyroscope

A gyroscope (from Greek γῦρος "gûros", "circle" and σκοπέω "skopéō", "to look") is a spinning wheel or disc in which the axis of rotation is free to assume any orientation. When rotating, the orientation of this axis is unaffected by tilting or rotation of the mounting, according to the conservation of angular momentum. Because of this, gyroscopes are useful for measuring or maintaining orientation.
Gyroscopes based on other operating principles also exist, such as the electronic, microchip-packaged MEMS gyroscopes found in consumer electronics devices, solid-state ring lasers, fibre optic gyroscopes, and the extremely sensitive quantum gyroscope.
Applications of gyroscopes include inertial navigation systems where magnetic compasses would not work (as in the Hubble telescope) or would not be precise enough (as in intercontinental ballistic missiles), or for the stabilization of flying vehicles like radio-controlled helicopters or unmanned aerial vehicles, and recreational boats and commercial ships. Due to their precision, gyroscopes are also used in gyrotheodolites to maintain direction in tunnel mining. Gyroscopes can be used to construct gyrocompasses, which complement or replace magnetic compasses (in ships, aircraft and spacecraft, vehicles in general), to assist in stability (Hubble Space Telescope, bicycles, motorcycles, and ships) or be used as part of an inertial guidance system.
Description and diagram.
Within mechanical systems or devices, a conventional "gyroscope" is a mechanism comprising a rotor journaled to spin about one axis, the journals of the rotor being mounted in an inner gimbal or ring; the inner gimbal is journaled for oscillation in an outer gimbal for a total of two gimbals.
The outer gimbal or ring, which is the gyroscope frame, is mounted so as to pivot about an axis in its own plane determined by the support. This outer gimbal possesses one degree of rotational freedom and its axis possesses none. The next inner gimbal is mounted in the gyroscope frame (outer gimbal) so as to pivot about an axis in its own plane that is always perpendicular to the pivotal axis of the gyroscope frame (outer gimbal). This inner gimbal has two degrees of rotational freedom.
The axle of the spinning wheel defines the spin axis. The rotor is journaled to spin about an axis, which is always perpendicular to the axis of the inner gimbal. So the rotor possesses three degrees of rotational freedom and its axis possesses two.
The wheel responds to a force applied about the input axis by a reaction force about the output axis.
The behaviour of a gyroscope can be most easily appreciated by consideration of the front wheel of a bicycle. If the wheel is leaned away from the vertical so that the top of the wheel moves to the left, the forward rim of the wheel also turns to the left. In other words, rotation on one axis of the turning wheel produces rotation of the third axis.
A gyroscope flywheel will roll or resist about the output axis depending upon whether the output gimbals are of a free- or fixed- configuration. Examples of some free-output-gimbal devices would be the attitude reference gyroscopes used to sense or measure the pitch, roll and yaw attitude angles in a spacecraft or aircraft.
The centre of gravity of the rotor can be in a fixed position. The rotor simultaneously spins about one axis and is capable of oscillating about the two other axes, and, thus, except for its inherent resistance due to rotor spin, it is free to turn in any direction about the fixed point. Some gyroscopes have mechanical equivalents substituted for one or more of the elements. For example, the spinning rotor may be suspended in a fluid, instead of being pivotally mounted in gimbals. A control moment gyroscope (CMG) is an example of a fixed-output-gimbal device that is used on spacecraft to hold or maintain a desired attitude angle or pointing direction using the gyroscopic resistance force.
In some special cases, the outer gimbal (or its equivalent) may be omitted so that the rotor has only two degrees of freedom. In other cases, the centre of gravity of the rotor may be offset from the axis of oscillation, and, thus, the centre of gravity of the rotor and the centre of suspension of the rotor may not coincide.
History.
Essentially, a gyroscope is a top combined with a pair of gimbals. Tops were invented in many different civilizations, including classical Greece, Rome, and China. Most of these were not utilized as instruments.
The first known apparatus similar to a gyroscope (the "Whirling Speculum" or "Serson's Speculum") was invented by John Serson in 1743. It was used as a level, to locate the horizon in foggy or misty conditions.
The first instrument used more like an actual gyroscope was made by German Johann Bohnenberger, who first wrote about it in 1817. At first he called it the "Machine". Bohnenberger's machine was based on a rotating massive sphere. In 1832, American Walter R. Johnson developed a similar device that was based on a rotating disc. The French mathematician Pierre-Simon Laplace, working at the École Polytechnique in Paris, recommended the machine for use as a teaching aid, and thus it came to the attention of Léon Foucault. In 1852, Foucault used it in an experiment involving the rotation of the Earth. It was Foucault who gave the device its modern name, in an experiment to see (Greek "skopeein", to see) the Earth's rotation (Greek "gyros", circle or rotation), which was visible in the 8 to 10 minutes before friction slowed the spinning rotor.
In the 1860s, the advent of electric motors made it possible for a gyroscope to spin indefinitely; this led to the first prototype heading indicators, and a rather more complicated device, the gyrocompass. The first functional gyrocompass was patented in 1904 by German inventor Hermann Anschütz-Kaempfe. The American Elmer Sperry followed with his own design later that year, and other nations soon realized the military importance of the invention—in an age in which naval prowess was the most significant measure of military power—and created their own gyroscope industries. The Sperry Gyroscope Company quickly expanded to provide aircraft and naval stabilizers as well, and other gyroscope developers followed suit.
In 1917, the Chandler Company of Indianapolis, created the "Chandler gyroscope", a toy gyroscope with a pull string and pedestal. Chandler continued to produce the toy until the company was purchased by TEDCO inc. in 1982. The chandler toy is still produced by TEDCO today.
In the first several decades of the 20th century, other inventors attempted (unsuccessfully) to use gyroscopes as the basis for early black box navigational systems by creating a stable platform from which accurate acceleration measurements could be performed (in order to bypass the need for star sightings to calculate position). Similar principles were later employed in the development of inertial navigation systems for ballistic missiles.
During World War II, the gyroscope became the prime component for aircraft and anti-aircraft gun sights. After the war, the race to miniaturize gyroscopes for guided missiles and weapons navigation systems resulted in the development and manufacturing of so-called midget gyroscopes that weighed less than and had a diameter of approximately . Some of these miniaturized gyroscopes could reach a speed of 24,000 revolutions per minute in less than 10 seconds.
Three-axis MEMS-based gyroscopes are also being used in portable electronic devices such as tablets, smartphones, and smartwatches. This adds to the 3-axis acceleration sensing ability available on previous generations of devices. Together these sensors provide 6 component motion sensing; acceleration for X,Y, and Z movement, and gyroscopes for measuring the extent and rate of rotation in space (roll, pitch and yaw). Some devices (e.g. the iPhone) additionally incorporate a magnetometer to provide absolute angular measurements relative to the Earth's magnetic field. Newer MEMS-based inertial measurement units incorporate up to all nine axes of sensing in a single integrated circuit package, providing inexpensive and widely available motion sensing.
Variations.
Gyrostat.
A gyrostat is a variant of the gyroscope. It consists of a massive flywheel concealed in a solid casing. Its behaviour on a table, or with various modes of suspension or support, serves to illustrate the curious reversal of the ordinary laws of static equilibrium due to the gyrostatic behaviour of the interior invisible flywheel when rotated rapidly. The first gyrostat was designed by Lord Kelvin to illustrate the more complicated state of motion of a spinning body when free to wander about on a horizontal plane, like a top spun on the pavement, or a hoop or bicycle on the road. Kelvin also made use of gyrostats to develop mechanical theories of the elasticity of matter and of the ether; these theories are of purely historical interest today.
In modern times, the gyrostat concept is used in the design of attitude control systems for orbiting spacecraft and satellites. For instance, the Mir space station had three pairs of internally mounted flywheels known as "gyrodynes" or "control moment gyros".
In physics, there are several systems whose dynamical equations resemble the equations of motion of a gyrostat. Examples include a solid body with a cavity filled with an inviscid, incompressible, homogeneous liquid, the static equilibrium configuration of a stressed elastic rod in elastica theory, the polarization dynamics of a light pulse propagating through a nonlinear medium, the Lorenz system in chaos theory, and the motion of an ion in a Penning trap mass spectrometer.
MEMS.
A MEMS gyroscope takes the idea of the Foucault pendulum and uses a vibrating element, known as a MEMS (microelectromechanical system). The MEMS-based gyro was initially made practical and producible by Systron Donner Inertial (SDI). Today, SDI is a large manufacturer of MEMS gyroscopes.
FOG.
A fiber optic gyroscope (FOG) is a gyroscope that uses the interference of light to detect mechanical rotation. The sensor is a coil of fiber optic cable which can be over 5 km of optical fiber. The development of low-loss single-mode optical fiber in the early 1970s for the telecommunications industry enabled the development of Sagnac effect fiber optic gyroscopes. The Sagnac effect is the same principle used in FOG's as in RLG's
HRG.
The hemispherical resonator gyroscope (HRG), also called wine-glass gyroscope or mushroom gyro, makes using a thin solid-state hemispherical shell, anchored by a thick stem. This shell is driven to a flexural resonance by electrostatic forces generated by electrodes which are deposited directly onto separate fused-quartz structures that surround the shell. Gyroscopic effect is obtained from the inertial property of the flexural standing waves.
VSG or CVG.
A vibrating structure gyroscope (VSG), also called a Coriolis vibratory gyroscope (CVG), uses a resonator made of different metallic alloys. It takes a position between the low-accuracy, low-cost MEMS gyroscope and the higher-accuracy and higher-cost FOG. Accuracy parameters are increased by using low-intrinsic damping materials, resonator vacuumization, and digital electronics to reduce temperature dependent drift and instability of control signals.
High quality wine-glass resonators are used for precise sensors like HRG or CRG.
DTG.
A dynamically tuned gyroscope (DTG) is a rotor suspended by a universal joint with flexure pivots. The flexure spring stiffness is independent of spin rate. However, the dynamic inertia (from the gyroscopic reaction effect) from the gimbal provides negative spring stiffness proportional to the square of the spin speed (Howe and Savet, 1964; Lawrence, 1998). Therefore, at a particular speed, called the tuning speed, the two moments cancel each other, freeing the rotor from torque, a necessary condition for an ideal gyroscope.
RLG.
A ring laser gyroscope relies on the Sagnac effect to measure rotation by measuring the shifting interference pattern of a split beam of light as it moves around the ring in opposite directions.
London moment.
A London moment gyroscope relies on the quantum-mechanical phenomenon, whereby a spinning superconductor generates a magnetic field whose axis lines up exactly with the spin axis of the gyroscopic rotor. A magnetometer determines the orientation of the generated field, which is interpolated to determine the axis of rotation. Gyroscopes of this type can be extremely accurate and stable. For example, those used in the Gravity Probe B experiment measured changes in gyroscope spin axis orientation to better than 0.5 milliarcseconds (1.4 degrees) over a one-year period. This is equivalent to an angular separation the width of a human hair viewed from away.
The GP-B gyro consists of a nearly-perfect spherical rotating mass made of fused quartz, which provides a dielectric support for a thin layer of niobium superconducting material. To eliminate friction found in conventional bearings, the rotor assembly is centered by the electric field from six electrodes. After the initial spin-up by a jet of helium which brings the rotor to 4,000 RPM, the polished gyroscope housing is evacuated to an ultra-high vacuum to further reduce drag on the rotor. Provided the suspension electronics remain powered, the extreme rotational symmetry, lack of friction, and low drag will allow the angular momentum of the rotor to keep it spinning for about 15,000 years.
A sensitive DC SQUID is able to discriminate changes as small as one quantum, or about 2 Wb, is used to monitor the gyroscope. A precession, or tilt, in the orientation of the rotor causes the London moment magnetic field to shift relative to the housing. The moving field passes through a superconducting pickup loop fixed to the housing, inducing a small electric current. The current produces a voltage across a shunt resistance, which is resolved to spherical coordinates by a microprocessor. The system is designed to minimize Lorentz torque on the rotor.
Modern uses.
In addition to being used in compasses, aircraft, computer pointing devices, etc., gyroscopes have been introduced into consumer electronics. Since the gyroscope allows the calculation of orientation and rotation, designers have incorporated them into modern technology. The integration of the gyroscope has allowed for more accurate recognition of movement within a 3D space than the previous lone accelerometer within a number of smartphones. Gyroscopes in consumer electronics are frequently combined with accelerometers (acceleration sensors) for more robust direction- and motion-sensing. Examples of such applications include smartphones such as the Samsung Galaxy Note 4, HTC Titan, Nexus 5, iPhone 5s, Nokia 808 PureView and Sony Xperia, game console peripherals such as the PlayStation 3 controller and the Wii Remote, and virtual reality sets such as the Oculus Rift.
Nintendo has integrated a gyroscope into the Wii console's Wii Remote controller by an additional piece of hardware called "Wii MotionPlus". It is also included in the 3DS and the Wii U GamePad, which detects movement when turning.
Cruise ships use gyroscopes to level motion-sensitive devices such as self-leveling pool tables.
Described as a "motorbike-car" hybrid, the Lit Motors C-1 two wheeler uses a set of futuristic electronic gyroscopes, or control moment gyroscopes, to ensure it remains upright and balanced, similar to the positioning technology used in the International Space Station and the Hubble Space Telescope. A similarity the Lit C-1 shares with the Segway is an IMU. A similar device has been used in the RYNO and Honda UX3 monocycles.
An electric powered flywheel gyroscope inserted in a bicycle wheel is being sold as a training wheel replacement.

</doc>
<doc id="13821" url="https://en.wikipedia.org/wiki?curid=13821" title="Hadron">
Hadron

In particle physics, a hadron (, "hadrós", "stout, thick") is a composite particle made of quarks held together by the strong force (in a similar way as molecules are held together by the electromagnetic force).
Hadrons are categorized into two families: baryons, made of three quarks, and mesons, made of one quark and one antiquark. Protons and neutrons are examples of baryons; pions are an example of a meson. Hadrons containing more than three valence quarks (exotic hadrons) have been discovered in recent years. A tetraquark state (an exotic meson), named the Z(4430)−, was discovered in 2007 by the Belle Collaboration and confirmed as a resonance in 2014 by the LHCb collaboration. Two pentaquark states (exotic baryons), named and , were discovered in 2015 by the LHCb collaboration. There are several more exotic hadron candidates, and other colour-singlet quark combinations may also exist.
Of the hadrons, protons are stable, and neutrons bound within atomic nuclei are stable. Other hadrons are unstable under ordinary conditions; free neutrons decay with a half-life of about 611 seconds. Experimentally, hadron physics is studied by colliding protons or nuclei of heavy elements such as lead, and detecting the debris in the produced particle showers.
Etymology.
The term "hadron" was introduced by Lev B. Okun in a plenary talk at the 1962 International Conference on High Energy Physics. In this talk he said:
Not withstanding the fact that this report deals with weak interactions, we shall frequently have to speak of strongly interacting particles. These particles pose not only numerous scientific problems, but also a terminological problem. The point is that "strongly interacting particles" is a very clumsy term which does not yield itself to the formation of an adjective. For this reason, to take but one instance, decays into strongly interacting particles are called non-leptonic. This definition is not exact because "non-leptonic" may also signify "photonic". In this report I shall call strongly interacting particles "hadrons", and the corresponding decays "hadronic" (the Greek "ἁδρός" signifies "large", "massive", in contrast to "λεπτός" which means "small", "light"). I hope that this terminology will prove to be 
Properties.
According to the quark model, the properties of hadrons are primarily determined by their so-called "valence quarks". For example, a proton is composed of two up quarks (each with electric charge +, for a total of + together) and one down quark (with electric charge −). Adding these together yields the proton charge of +1. Although quarks also carry color charge, hadrons must have zero total color charge because of a phenomenon called color confinement. That is, hadrons must be "colorless" or "white". These are the simplest of the two ways: three quarks of different colors, or a quark of one color and an antiquark carrying the corresponding anticolor. Hadrons with the first arrangement are called baryons, and those with the second arrangement are mesons.
Hadrons, however, are not composed of just three or two quarks, because of the strength of the strong force. More accurately, strong force gluons have enough energy ("E") to have resonances composed of massive ("m") quarks ("E > mc2") . Thus, virtual quarks and antiquarks, in a 1:1 ratio, form the majority of massive particles inside a hadron. The two or three quarks are the excess of quarks vs. antiquarks in hadrons, and vice versa in anti-hadrons. Because the virtual quarks are not stable wave packets (quanta), but irregular and transient phenomena, it is not meaningful to ask which quark is real and which virtual; only the excess is apparent from the outside. Massless virtual gluons compose the numerical majority of particles inside hadrons.
Like all subatomic particles, hadrons are assigned quantum numbers corresponding to the representations of the Poincaré group: "JPC"("m"), where "J" is the spin quantum number, "P" the intrinsic parity (or P-parity), and "C", the charge conjugation (or C-parity), and the particle's mass, "m". Note that the mass of a hadron has very little to do with the mass of its valence quarks; rather, due to mass–energy equivalence, most of the mass comes from the large amount of energy associated with the strong interaction. Hadrons may also carry flavor quantum numbers such as isospin (or G parity), and strangeness. All quarks carry an additive, conserved quantum number called a baryon number ("B"), which is + for quarks and − for antiquarks. This means that baryons (groups of three quarks) have "B" = 1 whereas mesons have "B" = 0.
Hadrons have excited states known as resonances. Each ground state hadron may have several excited states; several hundreds of resonances have been observed in particle physics experiments. Resonances decay extremely quickly (within about 10−24 seconds) via the strong nuclear force.
In other phases of matter the hadrons may disappear. For example, at very high temperature and high pressure, unless there are sufficiently many flavors of quarks, the theory of quantum chromodynamics (QCD) predicts that quarks and gluons will no longer be confined within hadrons, "because the strength of the strong interaction diminishes with energy". This property, which is known as asymptotic freedom, has been experimentally confirmed in the energy range between 1 GeV (gigaelectronvolt) and 1 TeV (teraelectronvolt).
All free hadrons except the proton (and antiproton) are unstable.
Baryons.
All known baryons are made of three valence quarks, so they are fermions, "i.e.", they have odd half-integer spin, because they have an odd number of quarks. As quarks possess baryon number "B" = , baryons have baryon number "B" = 1. The best-known baryons are the proton and the neutron.
One can hypothesise baryons with further quark-antiquark pairs in addition to their three quarks. Hypothetical baryons with one extra quark-antiquark pair (5 quarks in all) are called pentaquarks. As of August 2015, there are two known pentaquarks, and , both discovered in 2015 by the LHCb collaboration.
Each type of baryon has a corresponding antiparticle (antibaryon) in which quarks are replaced by their corresponding antiquarks. For example, just as a proton is made of two up-quarks and one down-quark, its corresponding antiparticle, the antiproton, is made of two up-antiquarks and one down-antiquark.
Mesons.
Mesons are hadrons composed of a quark-antiquark pair. They are bosons, meaning they have integral spin, "i.e.", 0, 1, or −1, as they have an even number of quarks. They have baryon number "B" = 0. Examples of mesons commonly produced in particle physics experiments include pions and kaons. Pions also play a role in holding atomic nuclei together via the residual strong force.
In principle, mesons with more than one quark-antiquark pair may exist; a hypothetical meson with two pairs is called a tetraquark. Several tetraquark candidates were found in the 2000s, but their status is under debate. Several other hypothetical "exotic" mesons lie outside the quark model of classification. These include glueballs and hybrid mesons (mesons bound by excited gluons).

</doc>
<doc id="13606" url="https://en.wikipedia.org/wiki?curid=13606" title="Half-life">
Half-life

Half-life (t1⁄2) is the amount of time required for the amount of something to fall to half its initial value. The term is very commonly used in nuclear physics to describe how quickly unstable atoms undergo radioactive decay, but it is also used more generally for discussing any type of exponential decay.
The original term, dating to Ernest Rutherford's discovery of the principle in 1907, was "half-life period", which was shortened to "half-life" in the early 1950s. Rutherford applied the principle of a radioactive element's half-life to studies of age determination of rocks by measuring the decay period of radium to lead-206.
Half-life is used to describe a quantity undergoing exponential decay, and is constant over the lifetime of the decaying quantity. It is a characteristic unit for the exponential decay equation. The term "half-life" may generically be used to refer to any period of time in which a quantity falls by half, even if the decay is not exponential. The table on the right shows the reduction of a quantity in the number of half-lives elapsed. For a general introduction and description of exponential decay, see exponential decay. For a general introduction and description of non-exponential decay, see rate law. The converse of half-life is doubling time.
Probabilistic nature of half-life.
A half-life usually describes the decay of discrete entities, such as radioactive atoms. In that case, it does not work to use the definition "half-life is the time required for exactly half of the entities to decay". For example, if there are 3 radioactive atoms with a half-life of one second, there will not be "1.5 atoms" left after one second.
Instead, the half-life is defined in terms of probability: "Half-life is the time required for exactly half of the entities to decay "on average"". In other words, the "probability" of a radioactive atom decaying within its half-life is 50%.
For example, the image on the right is a simulation of many identical atoms undergoing radioactive decay. Note that after one half-life there are not "exactly" one-half of the atoms remaining, only "approximately", because of the random variation in the process. Nevertheless, when there are many identical atoms decaying (right boxes), the law of large numbers suggests that it is a "very good approximation" to say that half of the atoms remain after one half-life.
There are various simple exercises that demonstrate probabilistic decay, for example involving flipping coins or running a statistical computer program.
Formulas for half-life in exponential decay.
An exponential decay process can be described by any of the following three equivalent formulas:
where
The three parameters formula_4, formula_2, and formula_3 are all directly related in the following way:
where ln(2) is the natural logarithm of 2 (approximately 0.693).
By plugging in and manipulating these relationships, we get all of the following equivalent descriptions of exponential decay, in terms of the half-life:
Regardless of how it's written, we can plug into the formula to get
Decay by two or more processes.
Some quantities decay by two exponential-decay processes simultaneously. In this case, the actual half-life T1⁄2 can be related to the half-lives t1 and t2 that the quantity would have if each of the decay processes acted in isolation:
For three or more processes, the analogous formula is:
For a proof of these formulas, see Exponential decay#Decay by two or more processes.
Examples.
There is a half-life describing any exponential-decay process. For example:
The half life of a species is the time it takes for the concentration of the substance to fall to half of its initial value.
Half-life in non-exponential decay.
The decay of many physical quantities is not exponential—for example, the evaporation of water from a puddle, or (often) the chemical reaction of a molecule. In such cases, the half-life is defined the same way as before: as the time elapsed before half of the original quantity has decayed. However, unlike in an exponential decay, the half-life depends on the initial quantity, and the prospective half-life will change over time as the quantity decays.
As an example, the radioactive decay of carbon-14 is exponential with a half-life of 5730 years. A quantity of carbon-14 will decay to half of its original amount (on average) after 5730 years, regardless of how big or small the original quantity was. After another 5730 years, one-quarter of the original will remain. On the other hand, the time it will take a puddle to half-evaporate depends on how deep the puddle is. Perhaps a puddle of a certain size will evaporate down to half its original volume in one day. But on the second day, there is no reason to expect that one-quarter of the puddle will remain; in fact, it will probably be much less than that. This is an example where the half-life reduces as time goes on. (In other non-exponential decays, it can increase instead.)
The decay of a mixture of two or more materials which each decay exponentially, but with different half-lives, is not exponential. Mathematically, the sum of two exponential functions is not a single exponential function. A common example of such a situation is the waste of nuclear power stations, which is a mix of substances with vastly different half-lives. Consider a sample containing a rapidly decaying element A, with a half-life of 1 second, and a slowly decaying element B, with a half-life of one year. After a few seconds, almost all atoms of the element A have decayed after repeated halving of the initial total number of atoms; but very few of the atoms of element B will have decayed yet as only a tiny fraction of a half-life has elapsed. Thus, the mixture taken as a whole does not decay by halves.
Half-life in biology and pharmacology.
A biological half-life or elimination half-life is the time it takes for a substance (drug, radioactive nuclide, or other) to lose one-half of its pharmacologic, physiologic, or radiological activity. In a medical context, the half-life may also describe the time that it takes for the concentration in blood plasma of a substance to reach one-half of its steady-state value (the "plasma half-life").
The relationship between the biological and plasma half-lives of a substance can be complex, due to factors including accumulation in tissues, active metabolites, and receptor interactions.
While a radioactive isotope decays almost perfectly according to so-called "first order kinetics" where the rate constant is a fixed number, the elimination of a substance from a living organism usually follows more complex chemical kinetics.
For example, the biological half-life of water in a human being is about 9 to 10 days, though this can be altered by behavior and various other conditions. The biological half-life of cesium in human beings is between one and four months.

</doc>
<doc id="4852151" url="https://en.wikipedia.org/wiki?curid=4852151" title="Hamilton's principle">
Hamilton's principle

In physics, Hamilton's principle is William Rowan Hamilton's formulation of the principle of stationary action (see that article for historical formulations). It states that the dynamics of a physical system is determined by a variational problem for a functional based on a single function, the Lagrangian, which contains all physical information concerning the system and the forces acting on it. The variational problem is equivalent to and allows for the derivation of the "differential" equations of motion of the physical system. Although formulated originally for classical mechanics, Hamilton's principle also applies to classical fields such as the electromagnetic and gravitational fields, and has even been extended to quantum mechanics, quantum field theory and criticality theories.
Mathematical formulation.
Hamilton's principle states that the true evolution q("t") of a system described by "N" generalized coordinates q = ("q"1, "q"2, ..., "q""N") between two specified states q1 = q("t"1) and q2 = q("t"2) at two specified times "t"1 and "t"2 is a stationary point (a point where the variation is zero), of the action functional
where formula_2 is the Lagrangian function for the system. In other words, any "first-order" perturbation of the true evolution results in (at most) "second-order" changes in formula_3. The action formula_3 is a functional, i.e., something that takes as its input a function and returns a single number, a scalar. In terms of functional analysis, Hamilton's principle states that the true evolution of a physical system is a solution of the functional equation
</math>
Euler–Lagrange equations derived from the action integral.
Requiring that the true trajectory q("t") be a stationary point of the action functional formula_3 is equivalent to a set of differential equations for q("t") (the Euler–Lagrange equations), which may be derived as follows.
Let q("t") represent the true evolution of the system between two specified states q1 = q("t"1) and q2 = q("t"2) at two specified times "t"1 and "t"2, and let ε("t") be a small perturbation that is zero at the endpoints of the trajectory
To first order in the perturbation ε("t"), the change in the action functional formula_7 would be
where we have expanded the Lagrangian "L" to first order in the perturbation ε("t").
Applying integration by parts to the last term results in
The boundary conditions formula_6 causes the first term to vanish
Hamilton's principle requires that this first-order change formula_12 is zero for all possible perturbations ε("t"), i.e., the true path is a stationary point of the action functional formula_3 (either a minimum, maximum or saddle point). This requirement can be satisfied if and only if
 - \frac{d}{dt}\frac{\partial L}{\partial \dot{\mathbf{q}}} = 0</math>
These equations are called the Euler–Lagrange equations for the variational problem.
Canonical momenta and constants of motion.
The conjugate momentum "pk" for a generalized coordinate "qk" is defined by the equation
An important special case of the Euler–Lagrange equation occurs when "L" does not contain a generalized coordinate "qk" explicitly,
that is, the conjugate momentum is a "constant of the motion".
In such cases, the coordinate "qk" is called a cyclic coordinate. For example, if we use polar coordinates "t, r, θ" to describe the planar motion of a particle, and if "L" does not depend on "θ", the conjugate momentum is the conserved angular momentum.
Example: Free particle in polar coordinates.
Trivial examples help to appreciate the use of the action principle via the Euler–Lagrange equations. A free particle (mass "m" and velocity "v") in Euclidean space moves in a straight line. Using the Euler–Lagrange equations, this can be shown in polar coordinates as follows. In the absence of a potential, the Lagrangian is simply equal to the kinetic energy 
in orthonormal ("x","y") coordinates, where the dot represents differentiation with respect to the curve parameter (usually the time, "t"). Therefore, upon application of the Euler–Lagrange equations,
And likewise for "y". Thus the Euler–Lagrange formulation can be used to derive Newton's laws.
In polar coordinates ("r", φ) the kinetic energy and hence the Lagrangian becomes
The radial "r" and "φ" components of the Euler–Lagrange equations become, respectively
The solution of these two equations is given by
for a set of constants "a, b, c, d" determined by initial conditions.
Thus, indeed, "the solution is a straight line" given in polar coordinates: "a" is the velocity, "c" is the distance of the closest approach to the origin, and "d" is the angle of motion.
Hamilton's principle applied to deformable bodies.
Hamilton's principle is an important variational principle in elastodynamics. As opposed to a system composed of rigid bodies, deformable bodies have an infinite number of degrees of freedom and occupy continuous regions of space; consequently, the state of the system is described by using continuous functions of space and time. The extended Hamilton Principle for such bodies is given by
where "T" is the kinetic energy, "U" is the elastic energy, "We" is the work done by
external loads on the body, and "t"1, "t"2 the initial and final times. If the system is conservative, the work done by external forces may be derived from a scalar potential "V". In this case,
This is called Hamilton's principle and it is invariant under coordinate transformations.
Comparison with Maupertuis' principle.
Hamilton's principle and Maupertuis' principle are occasionally confused and both have been called (incorrectly) the principle of least action. They differ in three important ways: 
Action principle for fields.
Classical field theory.
The action principle can be extended to obtain the equations of motion for fields, such as the electromagnetic field or gravity.
The Einstein equation utilizes the "Einstein–Hilbert action" as constrained by a variational principle.
The path of a body in a gravitational field (i.e. free fall in space time, a so-called geodesic) can be found using the action principle.
Quantum mechanics and quantum field theory.
In quantum mechanics, the system does not follow a single path whose action is stationary, but the behavior of the system depends on all imaginable paths and the value of their action. The action corresponding to the various paths is used to calculate the path integral, that gives the probability amplitudes of the various outcomes.
Although equivalent in classical mechanics with Newton's laws, the action principle is better suited for generalizations and plays an important role in modern physics. Indeed, this principle is one of the great generalizations in physical science. In particular, it is fully appreciated and best understood within quantum mechanics. Richard Feynman's path integral formulation of quantum mechanics is based on a stationary-action principle, using path integrals. Maxwell's equations can be derived as conditions of stationary action.

</doc>
<doc id="14381" url="https://en.wikipedia.org/wiki?curid=14381" title="Hamiltonian (quantum mechanics)">
Hamiltonian (quantum mechanics)

In quantum mechanics, the Hamiltonian is the operator corresponding to the total energy of the system in most of the cases. It is usually denoted by "H", also "Ȟ" or "Ĥ". Its spectrum is the set of possible outcomes when one measures the total energy of a system. Because of its close relation to the time-evolution of a system, it is of fundamental importance in most formulations of quantum theory.
The Hamiltonian is named after Sir William Rowan Hamilton (1805–1865), an Irish physicist, astronomer, and mathematician, best known for his reformulation of Newtonian mechanics, now called Hamiltonian mechanics.
Introduction.
The Hamiltonian is the sum of the kinetic energies of all the particles, plus the potential energy of the particles associated with the system. For different situations or number of particles, the Hamiltonian is different since it includes the sum of kinetic energies of the particles, and the potential energy function corresponding to the situation.
The Schrödinger Hamiltonian.
One particle.
By analogy with classical mechanics, the Hamiltonian is commonly expressed as the sum of operators corresponding to the kinetic and potential energies of a system in the form
where
is the potential energy operator and 
is the kinetic energy operator in which "m" is the mass of the particle, the dot denotes the dot product of vectors, and 
is the momentum operator wherein ∇ is the del operator. The dot product of ∇ with itself is the Laplacian ∇2. In three dimensions using Cartesian coordinates the Laplace operator is
However, complications can arise in the many-body problem. Since the potential energy depends on the spatial arrangement of the particles, the kinetic energy will also depend on the spatial configuration to conserve energy. The motion due to any one particle will vary due to the motion of all the other particles in the system. For this reason cross terms for kinetic energy may appear in the Hamiltonian; a mix of the gradients for two particles:
where "M" denotes the mass of the collection of particles resulting in this extra kinetic energy. Terms of this form are known as "mass polarization terms", and appear in the Hamiltonian of many electron atoms (see below).
For "N" interacting particles, i.e. particles which interact mutually and constitute a many-body situation, the potential energy function "V" is "not" simply a sum of the separate potentials (and certainly not a product, as this is dimensionally incorrect). The potential energy function can only be written as above: a function of all the spatial positions of each particle.
For non-interacting particles, i.e. particles which do not interact mutually and move independently, the potential of the system is the sum of the separate potential energy for each particle, that is
The general form of the Hamiltonian in this case is:
where the sum is taken over all particles and their corresponding potentials; the result is that the Hamiltonian of the system is the sum of the separate Hamiltonians for each particle. This is an idealized situation - in practice the particles are usually always influenced by some potential, and there are many-body interactions. One illustrative example of a two-body interaction where this form would not apply is for electrostatic potentials due to charged particles, because they interact with each other by Coulomb interaction (electrostatic force), as shown below.
Schrödinger equation.
The Hamiltonian generates the time evolution of quantum states. If formula_9 is the state of the system at time "t", then
This equation is the Schrödinger equation. It takes the same form as the Hamilton–Jacobi equation, which is one of the reasons "H" is also called the Hamiltonian. Given the state at some initial time ("t" = 0), we can solve it to obtain the state at any subsequent time. In particular, if "H" is independent of time, then
The exponential operator on the right hand side of the Schrödinger equation is usually defined by the corresponding power series in "H". One might notice that taking polynomials or power series of unbounded operators that are not defined everywhere may not make mathematical sense. Rigorously, to take functions of unbounded operators, a functional calculus is required. In the case of the exponential function, the continuous, or just the holomorphic functional calculus suffices. We note again, however, that for common calculations the physicists' formulation is quite sufficient.
By the *-homomorphism property of the functional calculus, the operator
is a unitary operator. It is the "time evolution operator", or "propagator", of a closed quantum system. If the Hamiltonian is time-independent, {U(t)} form a one parameter unitary group (more than a semigroup); this gives rise to the physical principle of detailed balance.
Dirac formalism.
However, in the more general formalism of Dirac, the Hamiltonian is typically implemented as an operator on a Hilbert space in the following way:
The eigenkets (eigenvectors) of "H", denoted formula_13, provide an orthonormal basis for the Hilbert space. The spectrum of allowed energy levels of the system is given by the set of eigenvalues, denoted {"E"a}, solving the equation:
Since "H" is a Hermitian operator, the energy is always a real number.
From a mathematically rigorous point of view, care must be taken with the above assumptions. Operators on infinite-dimensional Hilbert spaces need not have eigenvalues (the set of eigenvalues does not necessarily coincide with the spectrum of an operator). However, all routine quantum mechanical calculations can be done using the physical formulation.
Expressions for the Hamiltonian.
Following are expressions for the Hamiltonian in a number of situations. Typical ways to classify the expressions are the number of particles, number of dimensions, and the nature of the potential energy function - importantly space and time dependence. Masses are denoted by "m", and charges by "q".
Free particle.
The particle is not bound by any potential energy, so the potential is zero and this Hamiltonian is the simplest. For one dimension:
and in three dimensions:
Constant-potential well.
For a particle in a region of constant potential "V" = "V"0 (no dependence on space or time), in one dimension, the Hamiltonian is:
in three dimensions
This applies to the elementary "particle in a box" problem, and step potentials.
Simple harmonic oscillator.
For a simple harmonic oscillator in one dimension, the potential varies with position (but not time), according to:
where the angular frequency, effective spring constant "k", and mass "m" of the oscillator satisfy:
so the Hamiltonian is:
For three dimensions, this becomes
where the three-dimensional position vector r using cartesian coordinates is ("x", "y", "z"), its magnitude is
Writing the Hamiltonian out in full shows it is simply the sum of the one-dimensional Hamiltonians in each direction:
Rigid rotor.
For a rigid rotor – i.e. system of particles which can rotate freely about any axes, not bound in any potential (such as free molecules with negligible vibrational degrees of freedom, say due to double or triple chemical bonds), Hamiltonian is:
where "Ixx", "Iyy", and "Izz" are the moment of inertia components (technically the diagonal elements of the moment of inertia tensor), and formula_26, formula_27 and formula_28 are the total angular momentum operators (components), about the "x", "y", and "z" axes respectively.
Electrostatic or coulomb potential.
The Coulomb potential energy for two point charges "q"1 and "q"2 (i.e. charged particles, since particles have no spatial extent), in three dimensions, is (in SI units - rather than Gaussian units which are frequently used in electromagnetism):
However, this is only the potential for one point charge due to another. If there are many charged particles, each charge has a potential energy due to every other point charge (except itself). For "N" charges, the potential energy of charge "qj" due to all other charges is (see also Electrostatic potential energy stored in a configuration of discrete point charges):
where "φ"(ri) is the electrostatic potential of charge "qj" at ri. The total potential of the system is then the sum over "j":
so the Hamiltonian is:
Electric dipole in an electric field.
For an electric dipole moment d constituting charges of magnitude "q", in a uniform, electrostatic field (time-independent) E, positioned in one place, the potential is:
the dipole moment itself is the operator
Since the particle is stationary, there is no translational kinetic energy of the dipole, so the Hamiltonian of the dipole is just the potential energy:
Magnetic dipole in a magnetic field.
For a magnetic dipole moment μ in a uniform, magnetostatic field (time-independent) B, positioned in one place, the potential is:
Since the particle is stationary, there is no translational kinetic energy of the dipole, so the Hamiltonian of the dipole is just the potential energy:
For a Spin-½ particle, the corresponding spin magnetic moment is:
where "gs" is the spin gyromagnetic ratio (a.k.a. "spin g-factor"), "e" is the electron charge, S is the spin operator vector, whose components are the Pauli matrices, hence
Charged particle in an electromagnetic field.
For a charged particle "q" in an electromagnetic field, described by the scalar potential "φ" and vector potential A, there are two parts to the Hamiltonian to substitute for. The momentum operator must be replaced by the kinetic momentum operator, which includes a contribution from the A field:
where formula_41 is the canonical momentum operator given as the usual momentum operator:
so the corresponding kinetic energy operator is:
and the potential energy, which is due to the "φ" field:
Casting all of these into the Hamiltonian gives:
Energy eigenket degeneracy, symmetry, and conservation laws.
In many systems, two or more energy eigenstates have the same energy. A simple example of this is a free particle, whose energy eigenstates have wavefunctions that are propagating plane waves. The energy of each of these plane waves is inversely proportional to the square of its wavelength. A wave propagating in the "x" direction is a different state from one propagating in the "y" direction, but if they have the same wavelength, then their energies will be the same. When this happens, the states are said to be "degenerate".
It turns out that degeneracy occurs whenever a nontrivial unitary operator "U" commutes with the Hamiltonian. To see this, suppose that formula_46 is an energy eigenket. Then formula_47 is an energy eigenket with the same eigenvalue, since
Since "U" is nontrivial, at least one pair of formula_46 and formula_47 must represent distinct states. Therefore, "H" has at least one pair of degenerate energy eigenkets. In the case of the free particle, the unitary operator which produces the symmetry is the rotation operator, which rotates the wavefunctions by some angle while otherwise preserving their shape.
The existence of a symmetry operator implies the existence of a conserved observable. Let "G" be the Hermitian generator of "U":
It is straightforward to show that if "U" commutes with "H", then so does "G":
Therefore,
In obtaining this result, we have used the Schrödinger equation, as well as its dual,
Thus, the expected value of the observable "G" is conserved for any state of the system. In the case of the free particle, the conserved quantity is the angular momentum.
Hamilton's equations.
Hamilton's equations in classical Hamiltonian mechanics have a direct analogy in quantum mechanics. Suppose we have a set of basis states formula_55, which need not necessarily be eigenstates of the energy. For simplicity, we assume that they are discrete, and that they are orthonormal, i.e.,
Note that these basis states are assumed to be independent of time. We will assume that the Hamiltonian is also independent of time.
The instantaneous state of the system at time "t", formula_57, can be expanded in terms of these basis states:
where
The coefficients "an(t)" are complex variables. We can treat them as coordinates which specify the state of the system, like the position and momentum coordinates which specify a classical system. Like classical coordinates, they are generally not constant in time, and their time dependence gives rise to the time dependence of the system as a whole.
The expectation value of the Hamiltonian of this state, which is also the mean energy, is
where the last step was obtained by expanding formula_57 in terms of the basis states.
Each of the "an(t)"'s actually corresponds to "two" independent degrees of freedom, since the variable has a real part and an imaginary part. We now perform the following trick: instead of using the real and imaginary parts as the independent variables, we use "an(t)" and its complex conjugate "an*(t)". With this choice of independent variables, we can calculate the partial derivative
By applying Schrödinger's equation and using the orthonormality of the basis states, this further reduces to
Similarly, one can show that
If we define "conjugate momentum" variables "πn" by
then the above equations become
which is precisely the form of Hamilton's equations, with the formula_67s as the generalized coordinates, the formula_68s as the conjugate momenta, and formula_69 taking the place of the classical Hamiltonian.

</doc>
<doc id="198319" url="https://en.wikipedia.org/wiki?curid=198319" title="Hamiltonian mechanics">
Hamiltonian mechanics

Hamiltonian mechanics is a theory developed as a reformulation of classical mechanics and predicts the same outcomes as non-Hamiltonian classical mechanics. It uses a different mathematical formalism, providing a more abstract understanding of the theory. Historically, it was an important reformulation of classical mechanics, which later contributed to the formulation of quantum mechanics.
Hamiltonian mechanics was first formulated by William Rowan Hamilton in 1833, starting from Lagrangian mechanics, a previous reformulation of classical mechanics introduced by Joseph Louis Lagrange in 1788.
Overview.
In Hamiltonian mechanics, a classical physical system is described by a set of canonical coordinates formula_1, where each component of the coordinate formula_2 is indexed to the frame of reference of the system.
The time evolution of the system is uniquely defined by Hamilton's equations:
\end{align} </math>
where formula_3 is the Hamiltonian, which often corresponds to the total energy of the system. For a closed system, it is the sum of the kinetic and potential energy in the system.
In Newtonian mechanics, the time evolution is obtained by computing the total force being exerted on each particle of the system, and from Newton's second law, the time-evolutions of both position and velocity are computed. In contrast, in Hamiltonian mechanics, the time evolution is obtained by computing the Hamiltonian of the system in the generalized coordinates and inserting it in the Hamiltonian equations. This approach is equivalent to the one used in Lagrangian mechanics. In fact, as is shown below, the Hamiltonian is the Legendre transform of the Lagrangian, and thus both approaches give the same equations for the same generalized momentum. The main motivation to use Hamiltonian mechanics instead of Lagrangian mechanics comes from the symplectic structure of Hamiltonian systems.
While Hamiltonian mechanics can be used to describe simple systems such as a bouncing ball, a pendulum or an oscillating spring in which energy changes from kinetic to potential and back again over time, its strength is shown in more complex dynamic systems, such as planetary orbits in celestial mechanics. The more degrees of freedom the system has, the more complicated its time evolution is and, in most cases, it becomes chaotic.
Basic physical interpretation.
A simple interpretation of the Hamilton mechanics comes from its application on a one-dimensional system consisting of one particle of mass "m". The Hamiltonian represents the total energy of the system,
which is the sum of kinetic and potential energy, traditionally denoted "T" and "V", respectively. Here "q" is the coordinate and "p" is the momentum, "mv." Then
Note that "T" is a function of "p" alone, while "V" is a function of "q" alone (i.e., "T" and "V" are scleronomic ).
In this example, the time-derivative of the momentum "p" equals the "Newtonian force", and so the first Hamilton equation means that the force equals the negative gradient of potential energy. The time-derivative of "q" is the velocity, and so the second Hamilton equation means that the particle’s velocity equals the derivative of its kinetic energy with respect to its momentum.
Calculating a Hamiltonian from a Lagrangian.
Given a Lagrangian in terms of the generalized coordinates formula_5 and generalized velocities formula_6 and time,
Deriving Hamilton's equations.
Hamilton's equations can be derived by looking at how the total differential of the Lagrangian depends on time, generalized positions formula_13 and generalized velocities formula_14
Now the generalized momenta were defined as
If this is substituted into the total differential of the Lagrangian, one gets
We can rewrite this as
and rearrange again to get
The term on the left-hand side is just the Hamiltonian that we have defined before, so we find that
We can also calculate the total differential of the Hamiltonian formula_10 with respect to time directly, as we did with the Lagrangian formula_11 above, yielding:
It follows from the previous two independent equations that their right-hand sides are equal with each other.
Thus we obtain the equation
Since this calculation was done off-shell, we can associate corresponding terms from both sides of this equation to yield:
On-shell, Lagrange's equations tell us that
We can rearrange this to get
Thus Hamilton's equations hold on-shell:
As a reformulation of Lagrangian mechanics.
Starting with Lagrangian mechanics, the equations of motion are based on generalized coordinates
and matching generalized velocities
We write the Lagrangian as
with the subscripted variables understood to represent all "N" variables of that type. Hamiltonian mechanics aims to replace the generalized velocity variables with generalized momentum variables, also known as "conjugate momenta". By doing so, it is possible to handle certain systems, such as aspects of quantum mechanics, that would otherwise be even more complicated.
For each generalized velocity, there is one corresponding conjugate momentum, defined as:
In Cartesian coordinates, the generalized momenta are precisely the physical linear momenta. In circular polar coordinates, the generalized momentum corresponding to the angular velocity is the physical angular momentum. For an arbitrary choice of generalized coordinates, it may not be possible to obtain an intuitive interpretation of the conjugate momenta.
One thing which is not too obvious in this coordinate dependent formulation is that different generalized coordinates are really nothing more than different coordinate patches on the same symplectic manifold (see "Mathematical formalism", below).
The "Hamiltonian" is the Legendre transform of the Lagrangian:
If the transformation equations defining the generalized coordinates are independent of "t", and the Lagrangian is a sum of products of functions (in the generalized coordinates) which are homogeneous of order 0, 1 or 2, then it can be shown that "H" is equal to the total energy "E" = "T" + "V".
Each side in the definition of "formula_10" produces a differential:
Substituting the previous definition of the conjugate momenta into this equation and matching coefficients, we obtain the equations of motion of Hamiltonian mechanics, known as the canonical equations of Hamilton:
Hamilton's equations consist of 2n first-order differential equations, while Lagrange's equations consist of n second-order equations. However, Hamilton's equations usually don't reduce the difficulty of finding explicit solutions. They still offer some advantages, since important theoretical results can be derived because coordinates and momenta are independent variables with nearly symmetric roles.
Hamilton's equations have another advantage over Lagrange's equations: if a system has a symmetry, such that a coordinate does not occur in the Hamiltonian, the corresponding momentum is conserved, and that coordinate can be ignored in the other equations of the set. Effectively, this reduces the problem from n coordinates to (n-1) coordinates. In the Lagrangian framework, of course the result that the corresponding momentum is conserved still follows immediately, but all the generalized velocities still occur in the Lagrangian - we still have to solve a system of equations in n coordinates.
The Lagrangian and Hamiltonian approaches provide the groundwork for deeper results in the theory of classical mechanics, and for formulations of quantum mechanics.
Geometry of Hamiltonian systems.
A Hamiltonian system may be understood as a fiber bundle "E" over time "R", with the fibers "E""t", "t" ∈ "R", being the position space. The Lagrangian is thus a function on the jet bundle "J" over "E"; taking the fiberwise Legendre transform of the Lagrangian produces a function on the dual bundle over time whose fiber at "t" is the cotangent space "T"*"E""t", which comes equipped with a natural symplectic form, and this latter function is the Hamiltonian.
Generalization to quantum mechanics through Poisson bracket.
Hamilton's equations above work well for classical mechanics, but not for quantum mechanics, since the differential equations discussed assume that one can specify the exact position and momentum of the particle simultaneously at any point in time. However, the equations can be further generalized to then be extended to apply to quantum mechanics as well as to classical mechanics, through the deformation of the Poisson algebra over "p" and "q" to the algebra of Moyal brackets.
Specifically, the more general form of the Hamilton's equation reads
where "f" is some function of "p" and "q", and "H" is the Hamiltonian. To find out the rules for evaluating a Poisson bracket without resorting to differential equations, see Lie algebra; a Poisson bracket is the name for the Lie bracket in a Poisson algebra. These Poisson brackets can then be extended to Moyal brackets comporting to an inequivalent Lie algebra, as proven by H. Groenewold, and thereby describe quantum mechanical diffusion in phase space (See the phase space formulation and the Wigner-Weyl transform). This more algebraic approach not only permits ultimately extending probability distributions in phase space to Wigner quasi-probability distributions, but, at the mere Poisson bracket classical setting, also provides more power in helping analyze the relevant conserved quantities in a system.
Mathematical formalism.
Any smooth real-valued function "H" on a symplectic manifold can be used to define a Hamiltonian system. The function "H" is known as the Hamiltonian or the energy function. The symplectic manifold is then called the phase space. The Hamiltonian induces a special vector field on the symplectic manifold, known as the Hamiltonian vector field.
The Hamiltonian vector field (a special type of symplectic vector field) induces a Hamiltonian flow on the manifold. This is a one-parameter family of transformations of the manifold (the parameter of the curves is commonly called the time); in other words an isotopy of symplectomorphisms, starting with the identity. By Liouville's theorem, each symplectomorphism preserves the volume form on the phase space. The collection of symplectomorphisms induced by the Hamiltonian flow is commonly called the Hamiltonian mechanics of the Hamiltonian system.
The symplectic structure induces a Poisson bracket. The Poisson bracket gives the space of functions on the manifold the structure of a Lie algebra.
Given a function "f"
If we have a probability distribution, ρ, then (since the phase space velocity (formula_39) has zero divergence, and probability is conserved) its convective derivative can be shown to be zero and so
This is called Liouville's theorem. Every smooth function "G" over the symplectic manifold generates a one-parameter family of symplectomorphisms and if { "G", "H" } = 0, then "G" is conserved and the symplectomorphisms are symmetry transformations.
A Hamiltonian may have multiple conserved quantities "G""i". If the symplectic manifold has dimension 2"n" and there are "n" functionally independent conserved quantities "G""i" which are in involution (i.e., { "G""i", "G""j" } = 0), then the Hamiltonian is Liouville integrable. The Liouville-Arnold theorem says that locally, any Liouville integrable Hamiltonian can be transformed via a symplectomorphism in a new Hamiltonian with the conserved quantities "G""i" as coordinates; the new coordinates are called "action-angle coordinates". The transformed Hamiltonian depends only on the "G""i", and hence the equations of motion have the simple form
for some function "F" (Arnol'd et al., 1988). There is an entire field focusing on small deviations from integrable systems governed by the KAM theorem.
The integrability of Hamiltonian vector fields is an open question. In general, Hamiltonian systems are chaotic; concepts of measure, completeness, integrability and stability are poorly defined. At this time, the study of dynamical systems is primarily qualitative, and not a quantitative science.
Riemannian manifolds.
An important special case consists of those Hamiltonians that are quadratic forms, that is, Hamiltonians that can be written as
where formula_43 is a smoothly varying inner product on the fibers formula_44, the cotangent space to the point "q" in the configuration space, sometimes called a cometric. This Hamiltonian consists entirely of the kinetic term.
If one considers a Riemannian manifold or a pseudo-Riemannian manifold, the Riemannian metric induces a linear isomorphism between the tangent and cotangent bundles. (See Musical isomorphism). Using this isomorphism, one can define a cometric. (In coordinates, the matrix defining the cometric is the inverse of the matrix defining the metric.) The solutions to the Hamilton–Jacobi equations for this Hamiltonian are then the same as the geodesics on the manifold. In particular, the Hamiltonian flow in this case is the same thing as the geodesic flow. The existence of such solutions, and the completeness of the set of solutions, are discussed in detail in the article on geodesics. See also Geodesics as Hamiltonian flows.
Sub-Riemannian manifolds.
When the cometric is degenerate, then it is not invertible. In this case, one does not have a Riemannian manifold, as one does not have a metric. However, the Hamiltonian still exists. In the case where the cometric is degenerate at every point "q" of the configuration space manifold "Q", so that the rank of the cometric is less than the dimension of the manifold "Q", one has a sub-Riemannian manifold.
The Hamiltonian in this case is known as a sub-Riemannian Hamiltonian. Every such Hamiltonian uniquely determines the cometric, and vice versa. This implies that every sub-Riemannian manifold is uniquely determined by its sub-Riemannian Hamiltonian, and that the converse is true: every sub-Riemannian manifold has a unique sub-Riemannian Hamiltonian. The existence of sub-Riemannian geodesics is given by the Chow–Rashevskii theorem.
The continuous, real-valued Heisenberg group provides a simple example of a sub-Riemannian manifold. For the Heisenberg group, the Hamiltonian is given by
formula_46 is not involved in the Hamiltonian.
Poisson algebras.
Hamiltonian systems can be generalized in various ways. Instead of simply looking at the algebra of smooth functions over a symplectic manifold, Hamiltonian mechanics can be formulated on general commutative unital real Poisson algebras. A state is a continuous linear functional on the Poisson algebra (equipped with some suitable topology) such that for any element "A" of the algebra, "A"² maps to a nonnegative real number.
A further generalization is given by Nambu dynamics.
Charged particle in an electromagnetic field.
A good illustration of Hamiltonian mechanics is given by the Hamiltonian of a charged particle in an electromagnetic field. In Cartesian coordinates (i.e. formula_47), the Lagrangian of a non-relativistic classical particle in an electromagnetic field is (in SI Units):
where e is the electric charge of the particle (not necessarily the elementary charge), formula_49 is the electric scalar potential, and the formula_50 are the components of the magnetic vector potential (these may be modified through a gauge transformation). This is called minimal coupling.
The generalized momenta are given by:
Rearranging, the velocities are expressed in terms of the momenta:
If we substitute the definition of the momenta, and the definitions of the velocities in terms of the momenta, into the definition of the Hamiltonian given above, and then simplify and rearrange, we get:
This equation is used frequently in quantum mechanics.
Relativistic charged particle in an electromagnetic field.
The Lagrangian for a relativistic charged particle is given by:
Thus the particle's canonical (total) momentum is
that is, the sum of the kinetic momentum and the potential momentum.
Solving for the velocity, we get
So the Hamiltonian is
From this we get the force equation (equivalent to the Euler–Lagrange equation)
from which one can derive
An equivalent expression for the Hamiltonian as function of the relativistic (kinetic) momentum, formula_60 is
This has the advantage that formula_62 can be measured experimentally whereas formula_63 cannot. Notice that the Hamiltonian (total energy) can be viewed as the sum of the relativistic energy (kinetic+rest), formula_64 plus the potential energy, formula_65

</doc>
<doc id="897539" url="https://en.wikipedia.org/wiki?curid=897539" title="Hamilton–Jacobi equation">
Hamilton–Jacobi equation

In mathematics, the Hamilton–Jacobi equation (HJE) is a necessary condition describing extremal geometry in generalizations of problems from the calculus of variations, and is a special case of the Hamilton–Jacobi–Bellman equation. It is named for William Rowan Hamilton and Carl Gustav Jacob Jacobi. In physics, it is a formulation of classical mechanics, equivalent to other formulations such as Newton's laws of motion, Lagrangian mechanics and Hamiltonian mechanics. The Hamilton–Jacobi equation is particularly useful in identifying conserved quantities for mechanical systems, which may be possible even when the mechanical problem itself cannot be solved completely.
The HJE is also the only formulation of mechanics in which the motion of a particle can be represented as a wave. In this sense, the HJE fulfilled a long-held goal of theoretical physics (dating at least to Johann Bernoulli in the 18th century) of finding an analogy between the propagation of light and the motion of a particle. The wave equation followed by mechanical systems is similar to, but not identical with, Schrödinger's equation, as described below; for this reason, the HJE is considered the "closest approach" of classical mechanics to quantum mechanics.
Notation.
Boldface variables such as formula_1 represent a list of formula_2 generalized coordinates, 
A dot over a variable or list signifies the time derivative, e.g.,
The dot product notation between two lists of the same number of coordinates is a shorthand for the sum of the products of corresponding components, e.g.,
Mathematical formulation.
The Hamilton–Jacobi equation is a first-order, non-linear partial differential equation
where
is the classical Hamiltonian function,
is called Hamilton's principal function (also the action, see below), "qi" are the "N" generalized coordinates ("i" = 1,2..."N") which define the configuration of the system, and "t" is time.
As described below, this equation may be derived from Hamiltonian mechanics by treating "S" as the generating function for a canonical transformation of the classical Hamiltonian
The conjugate momenta correspond to the first derivatives of "S" with respect to the generalized coordinates
As a solution to the Hamilton–Jacobi equation, the principal function contains "N" + 1 undetermined constants, the first "N" of them denoted as "α"1, "α"2 ... "αN", and the last one coming from the integration of formula_10.
The relationship between p and q then describes the orbit in phase space in terms of these constants of motion. Furthermore, the quantities
are also constants of motion, and these equations can be inverted to find q as a function of all the α and β constants and time.
Comparison with other formulations of mechanics.
The HJE is a "single", first-order partial differential equation for the function "S" of the "N" generalized coordinates "q"1..."qN" and the time "t". The generalized momenta do not appear, except as derivatives of "S". Remarkably, the function "S" is equal to the classical action.
For comparison, in the equivalent Euler–Lagrange equations of motion of Lagrangian mechanics, the conjugate momenta also do not appear; however, those equations are a "system" of "N", generally second-order equations for the time evolution of the generalized coordinates. Similarly, Hamilton's equations of motion are another "system" of 2"N" first-order equations for the time evolution of the generalized coordinates and their conjugate momenta "p"1..."pN".
Since the HJE is an equivalent expression of an integral minimization problem such as Hamilton's principle, the HJE can be useful in other problems of the calculus of variations and, more generally, in other branches of mathematics and physics, such as dynamical systems, symplectic geometry and quantum chaos. For example, the Hamilton–Jacobi equations can be used to determine the geodesics on a Riemannian manifold, an important variational problem in Riemannian geometry.
Derivation.
Any canonical transformation involving a type-2 generating function "G"2(q, P, "t") leads to the relations
and Hamilton's equations in terms of the new variables P, Q and new Hamiltonian "K" have the same form:
To derive the HJE, we "choose" a generating function "G"2(q, P, "t") in such a way that, it will make the new Hamiltonian "K" = 0.
Hence, all its derivatives are also zero, and the transformed Hamilton's equations become trivial
so the new generalized coordinates and momenta are "constants" of motion. As they are constants, in this context the new generalized momenta P are usually denoted "α"1, "α"2 ... "αN", i.e. "Pm" = "αm", and the new generalized coordinates Q are typically denoted as "β"1, "β"2 ... "βN", so "Qm" = "βm".
Setting the generating function equal to Hamilton's principal function, plus an arbitrary constant "A":
the HJE automatically arises:
Once we have solved for "S"(q, α, "t"), these also give us the useful equations
or written in components for clarity
Ideally, these "N" equations can be inverted to find the original generalized coordinates q as a function of the constants α, β and "t", thus solving the original problem.
Action and Hamilton's functions.
Hamilton's principal function "S" and classical function "H" are both closely related to action. The total differential of "S" is:
so the time derivative of "S" is
Therefore
so "S" is actually the classical action plus an undetermined constant.
When "H" does not explicitly depend on time,
in this case "W" is the same as abbreviated action.
Separation of variables.
The HJE is most useful when it can be solved via additive separation of variables, which directly identifies constants of motion. For example, the time "t" can be separated if the Hamiltonian does not depend on time explicitly. In that case, the time derivative formula_10 in the HJE must be a constant, usually denoted (–"E"), giving the separated solution
where the time-independent function "W"(q) is sometimes called Hamilton's characteristic function. The reduced Hamilton–Jacobi equation can then be written
To illustrate separability for other variables, we assume that a certain generalized coordinate "qk" and its derivative formula_26 appear together as a single function
in the Hamiltonian
In that case, the function "S" can be partitioned into two functions, one that depends only on "qk" and another that depends only on the remaining generalized coordinates
Substitution of these formulae into the Hamilton–Jacobi equation shows that the function "ψ" must be a constant (denoted here as Γ"k"), yielding a first-order ordinary differential equation for "Sk(qk)"
In fortunate cases, the function "S" can be separated completely into "N" functions "Sm"("qm")
In such a case, the problem devolves to "N" ordinary differential equations.
The separability of "S" depends both on the Hamiltonian and on the choice of generalized coordinates. For orthogonal coordinates and Hamiltonians that have no time dependence and are quadratic in the generalized momenta, "S" will be completely separable if the potential energy is additively separable in each coordinate, where the potential energy term for each coordinate is multiplied by the coordinate-dependent factor in the corresponding momentum term of the Hamiltonian (the Staeckel conditions). For illustration, several examples in orthogonal coordinates are worked in the next sections.
Examples in various coordinate systems.
Spherical coordinates.
In spherical coordinates the Hamiltonian of a free particle moving in a conservative potential "U" can be written
The Hamilton–Jacobi equation is completely separable in these coordinates provided that there exist functions "Ur"("r"), "Uθ"("θ") and "Uϕ"("ϕ") such that "U" can be written in the analogous form
Substitution of the completely separated solution
into the HJE yields
This equation may be solved by successive integrations of ordinary differential equations, beginning with the equation for "ϕ"
where Γ"ϕ" is a constant of the motion that eliminates the "ϕ" dependence from the Hamilton–Jacobi equation
The next ordinary differential equation involves the "θ" generalized coordinate
where Γ"θ" is again a constant of the motion that eliminates the "θ" dependence and reduces the HJE to the final ordinary differential equation
whose integration completes the solution for "S".
Elliptic cylindrical coordinates.
The Hamiltonian in elliptic cylindrical coordinates can be written
where the foci of the ellipses are located at ±"a" on the "x"-axis. The Hamilton–Jacobi equation is completely separable in these coordinates provided that "U" has an analogous form
where "Uμ"("μ"), "Uη"("η") and "Uz"("z") are arbitrary functions. Substitution of the completely separated solution
Separating the first ordinary differential equation
yields the reduced Hamilton–Jacobi equation (after re-arrangement and multiplication of both sides by the denominator)
which itself may be separated into two independent ordinary differential equations
that, when solved, provide a complete solution for "S".
Parabolic cylindrical coordinates.
The Hamiltonian in parabolic cylindrical coordinates can be written
The Hamilton–Jacobi equation is completely separable in these coordinates provided that "U" has an analogous form
where "Uσ"("σ"), "Uτ"("τ") and "Uz"("z") are arbitrary functions. Substitution of the completely separated solution
into the HJE yields
Separating the first ordinary differential equation
yields the reduced Hamilton–Jacobi equation (after re-arrangement and multiplication of both sides by the denominator)
which itself may be separated into two independent ordinary differential equations
that, when solved, provide a complete solution for "S".
Eikonal approximation and relationship to the Schrödinger equation.
The isosurfaces of the function "S"(q; "t") can be determined at any time "t". The motion of an "S"-isosurface as a function of time is defined by the motions of the particles beginning at the points q on the isosurface. The motion of such an isosurface can be thought of as a "wave" moving through q space, although it does not obey the wave equation exactly. To show this, let "S" represent the phase of a wave
where "ħ" is a constant (Planck's constant) introduced to make the exponential argument unitless; changes in the amplitude of the wave can be represented by having "S" be a complex number. We may then rewrite the Hamilton–Jacobi equation as
which is a "nonlinear" variant of the Schrödinger equation.
Conversely, starting with the Schrödinger equation and our ansatz for "ψ", we arrive at
The classical limit ("ħ" → 0) of the Schrödinger equation above becomes identical to the following variant of the Hamilton–Jacobi equation,
HJE in a gravitational field.
Using the energy–momentum relation in the form;
for a particle of rest mass "m" travelling in curved space, where "gαβ" are the contravariant coordinates of the metric tensor (i.e., the inverse metric) solved from the Einstein field equations, and "c" is the speed of light, setting the four-momentum "Pα" equal to the four-gradient of the action "S";
gives the Hamilton–Jacobi equation in the geometry determined by the metric "g":
in other words, in a gravitational field.
HJE in electromagnetic fields.
For a particle of rest mass formula_63 and electric charge formula_64 moving in electromagnetic field with four-potential formula_65 in vacuum, the Hamilton–Jacobi equation in geometry determined by the metric tensor formula_66 has a form
formula_67
and can be solved for the Hamilton Principal Action function formula_68 to obtain further solution for the particle trajectory and momentum:
formula_69, formula_70, formula_71,
formula_72, formula_73,
formula_74, formula_75, formula_76,
formula_77,
where formula_78 and averaging procedure was performed to reveal the particle closed periodic motion. Therefore:
a) For a wave with the circular polarization:
formula_79, formula_80,
formula_81, formula_82
hence
formula_83, formula_84,
formula_85, formula_86,
where formula_87,
implying the particle moving along a circular trajectory with a permanent radius formula_88 and an invariable value of momentum formula_89 directed along a magnetic field vector.
b) For the flat, monochromatic, linearly polarized wave with a field formula_90 directed along the axis formula_91
formula_92, formula_93, hence
formula_94, formula_95, formula_96, formula_97,
formula_98, formula_99,
formula_100, formula_101, formula_102, formula_103
implying the particle figure-8 trajectory with a long its axis oriented along the electric field formula_90 vector.
c) For the electromagnetic wave with axial (solenoidal) magnetic field:
formula_105, formula_106,
hence
formula_94, formula_108, formula_96, formula_110,
formula_111, formula_112,
formula_100, formula_114, formula_115, formula_116,
where formula_117 is the magnetic field magnitude in a solenoid with the effective radius formula_118, inductivity formula_119, number of windings formula_120, and an electric current magnitude formula_121 through the solenoid windings. The particle motion occurs along the figure-8 trajectory in formula_122 plane set perpendicular to the solenoid axis with arbitrary azimuth angle formula_123 due to axial symmetry of the solenoidal magnetic field.

</doc>
<doc id="41232" url="https://en.wikipedia.org/wiki?curid=41232" title="Harmonic">
Harmonic

The term harmonic in its strictest sense is any member of the harmonic series. The term is employed in various disciplines, including music and acoustics, electronic power transmission, radio technology, etc. It is typically applied when considering the frequencies of repeating signals, such as sinusoidal waves, that happen to relate as whole-numbered multiples. In that case, a harmonic is a signal whose frequency is a whole-numbered multiple of the frequency of some other given signal. For example, in alternating current of 60 cycles per second, "fifth-harmonic distortion" would produce an unwanted additional signal at 300 cycles per second, exactly five times the original frequency.
A harmonic of a wave is a component frequency of the signal that is an integer multiple of the fundamental frequency, i.e. if the fundamental frequency is "f", the harmonics have frequencies 2"f", 3"f", 4"f", . . . etc. The harmonics have the property that they are all periodic at the fundamental frequency, therefore the sum of harmonics is also periodic at that frequency. As multiples of the fundamental frequency, successive harmonics can be found by repeatedly adding the fundamental frequency. For example, if the fundamental frequency (first harmonic) is 25 Hz, the frequencies of the next harmonics are: 50 Hz (2nd harmonic), 75 Hz (3rd harmonic), 100 Hz (4th harmonic) etc.
Characteristics.
Most acoustic instruments emit complex tones containing many individual partials (component simple tones or sinusoidal waves), but the untrained human ear typically does not perceive those partials as separate phenomena. Rather, a musical note is perceived as one sound, the quality or timbre of that sound being a result of the relative strengths of the individual partials. 
Many acoustic oscillators, such as the human voice or a bowed violin string, produce complex tones that are more or less periodic, and thus are composed of partials that are near matches to integer multiples of the fundamental frequency and therefore resemble the ideal harmonics and are called "harmonic partials" or simply "harmonics" for convenience (although it's not strictly accurate to call a partial a harmonic, the first being real and the second being ideal). Oscillators that produce harmonic partials behave somewhat like 1-dimensional resonators, and are often long and thin, such as a guitar string or a column of air open at both ends (as with the modern orchestral transverse flute). Wind instruments whose air column is open at only one end, such as trumpets and clarinets, also produce partials resembling harmonics. However they only produce partials matching the odd harmonics, at least in theory. The reality of acoustic instruments is such that none of them behaves as perfectly as the somewhat simplified theoretical models would predict.
Partials whose frequencies are not integer multiples of the fundamental are referred to as "inharmonic partials". 
Some acoustic instruments emit a mix of harmonic and inharmonic partials but still produce an effect on the ear of having a definite fundamental pitch, such as pianos, strings plucked pizzicato, vibraphones, marimbas, and certain pure-sounding bells or chimes. Antique singing bowls are known for producing multiple harmonic partials or multiphonics.
Other oscillators, such as cymbals, drum heads, and other percussion instruments, naturally produce an abundance of inharmonic partials and do not imply any particular pitch, and therefore cannot be used melodically or harmonically in the same way other instruments can.
Harmonics and overtones.
An overtone is any frequency higher than the fundamental. The tight relation between overtones and harmonics in music often leads to their being used synonymously in a strictly musical context, but they are counted differently leading to some possible confusion. This chart demonstrates how they are counted:
In many musical instruments, it is possible to play the upper harmonics without the fundamental note being present. In a simple case (e.g., recorder) this has the effect of making the note go up in pitch by an octave, but in more complex cases many other pitch variations are obtained. In some cases it also changes the timbre of the note. This is part of the normal method of obtaining higher notes in wind instruments, where it is called "overblowing". The extended technique of playing multiphonics also produces harmonics. On string instruments it is possible to produce very pure sounding notes, called harmonics or "flageolets" by string players, which have an eerie quality, as well as being high in pitch. Harmonics may be used to check at a unison the tuning of strings that are not tuned to the unison. For example, lightly fingering the node found halfway down the highest string of a cello produces the same pitch as lightly fingering the node 1/3 of the way down the second highest string. For the human voice see Overtone singing, which uses harmonics. 
While it is true that electronically produced periodic tones (e.g. square waves or other non-sinusoidal waves) have "harmonics" that are whole number multiples of the fundamental frequency, practical instruments do not all have this characteristic. For example higher "harmonics"' of piano notes are not true harmonics but are "overtones" and can be very sharp, i.e. a higher frequency than given by a pure harmonic series. This is especially true of instruments other than stringed or brass/woodwind ones, e.g., xylophone, drums, bells etc., where not all the overtones have a simple whole number ratio with the fundamental frequency.
The fundamental frequency is the reciprocal of the period of the periodic phenomenon. 
Harmonics on stringed instruments.
The following table displays the stop points on a stringed instrument, such as the guitar (guitar harmonics), at which gentle touching of a string will force it into a harmonic mode when vibrated. String harmonics (flageolet tones) are described as having a "flutelike, silvery quality that can be highly effective as a special color" when used and heard in orchestration. It is unusual to encounter natural harmonics higher than the fifth partial on any stringed instrument except the double bass, on account of its much longer strings.
Artificial harmonics.
Although harmonics are most often used on open strings, occasionally a score will call for an artificial harmonic, produced by playing an overtone on a stopped string. As a performance technique, it is accomplished by using two fingers on the fingerboard, the first to shorten the string to the desired fundamental, with the second touching the node corresponding to the appropriate harmonic.
Other information.
Harmonics may be either used or considered as the basis of just intonation systems. 
Composer Arnold Dreyblatt is able to bring out different harmonics on the single string of his modified double bass by slightly altering his unique bowing technique halfway between hitting and bowing the strings. 
Composer Lawrence Ball uses harmonics to generate music electronically.
External links.
[[Category:Acoustics]]

</doc>
<doc id="14147" url="https://en.wikipedia.org/wiki?curid=14147" title="Harmonic analysis">
Harmonic analysis

Harmonic analysis is a branch of mathematics concerned with the representation of functions or signals as the superposition of basic waves, and the study of and generalization of the notions of Fourier series and Fourier transforms (i.e. an extended form of Fourier analysis). In the past two centuries, it has become a vast subject with applications in areas as diverse as signal processing, quantum mechanics, tidal analysis and neuroscience.
The term "harmonics" originated as the ancient Greek word, "harmonikos," meaning "skilled in music." In physical eigenvalue problems it began to mean waves whose frequencies are integer multiples of one another, as are the frequencies of the harmonics of music notes, but the term has been generalized beyond its original meaning.
The classical Fourier transform on R"n" is still an area of ongoing research, particularly concerning Fourier transformation on more general objects such as tempered distributions. For instance, if we impose some requirements on a distribution "f", we can attempt to translate these requirements in terms of the Fourier transform of "f". The Paley–Wiener theorem is an example of this. The Paley–Wiener theorem immediately implies that if "f" is a nonzero distribution of compact support (these include functions of compact support), then its Fourier transform is never compactly supported. This is a very elementary form of an uncertainty principle in a harmonic analysis setting. See also: Convergence of Fourier series.
Fourier series can be conveniently studied in the context of Hilbert spaces, which provides a connection between harmonic analysis and functional analysis.
Applied harmonic analysis.
Many applications of harmonic analysis in science and engineering begin with the idea or hypothesis that a phenomenon or signal is composed of a sum of individual oscillatory components. Ocean tides and vibrating strings are common and simple examples. The theoretical approach is often to try to describe the system by a differential equation or system of equations to predict the essential features, including the amplitude, frequency, and phases of the oscillatory components. The specific equations depend on the field, but theories generally try to select equations that represent major principles that are applicable.
The experimental approach is usually to acquire data that accurately quantifies the phenomenon. For example, in a study of tides, the experimentalist would acquire samples of water depth as a function of time at closely enough spaced intervals to see each oscillation and over a long enough duration that multiple oscillatory periods are likely included. In a study on vibrating strings, it is common for the experimentalist to acquire a sound waveform sampled at a rate at least twice that of the highest frequency expected and for a duration many times the period of the lowest frequency expected.
For example, the top signal at the right is a sound waveform of a bass guitar playing an open string corresponding to an A note with fundamental frequency or 55 Hz. The waveform appears oscillatory, but it is more complex than a simple sine wave, indicating the presence of additional waves. The different wave components contributing to the sound can be revealed by applying a mathematical analysis technique known as the Fourier transform, which is shown in the lower figure. Note that there is a prominent peak at 55 Hz, but that there are other peaks at 110 Hz, 165 Hz, and at other frequencies corresponding to integer multiples of 55 Hz. In this case, 55 Hz is identified as the fundamental frequency of the string vibration, and the integer multiples are known as harmonics.
Abstract harmonic analysis.
One of the most modern branches of harmonic analysis, having its roots in the mid-twentieth century, is analysis on topological groups. The core motivating ideas are the various Fourier transforms, which can be generalized to a transform of functions defined on Hausdorff locally compact topological groups.
The theory for abelian locally compact groups is called Pontryagin duality.
Harmonic analysis studies the properties of that duality and Fourier transform, and attempts to extend those features to different settings, for instance to the case of non-abelian Lie groups.
For general non-abelian locally compact groups, harmonic analysis is closely related to the theory of unitary group representations. For compact groups, the Peter–Weyl theorem explains how one may get harmonics by choosing one irreducible representation out of each equivalence class of representations. This choice of harmonics enjoys some of the useful properties of the classical Fourier transform in terms of carrying convolutions to pointwise products, or otherwise showing a certain understanding of the underlying group structure. See also: Non-commutative harmonic analysis.
If the group is neither abelian nor compact, no general satisfactory theory is currently known. ("Satisfactory" means at least as strong as the Plancherel theorem.) However, many specific cases have been analyzed, for example SL"n". In this case, representations in infinite dimensions play a crucial role.

</doc>
<doc id="13899" url="https://en.wikipedia.org/wiki?curid=13899" title="Harmonic oscillator">
Harmonic oscillator

In classical mechanics, a harmonic oscillator is a system that, when displaced from its equilibrium position, experiences a restoring force, "F", proportional to the displacement, "x":
where "k" is a positive constant.
If "F" is the only force acting on the system, the system is called a simple harmonic oscillator, and it undergoes simple harmonic motion: sinusoidal oscillations about the equilibrium point, with a constant amplitude and a constant frequency (which does not depend on the amplitude).
If a frictional force (damping) proportional to the velocity is also present, the harmonic oscillator is described as a damped oscillator. Depending on the friction coefficient, the system can:
The boundary solution between an underdamped oscillator and an overdamped oscillator occurs at a particular value of the friction coefficient and is called "critically damped."
If an external time dependent force is present, the harmonic oscillator is described as a driven oscillator.
Mechanical examples include pendulums (with small angles of displacement), masses connected to springs, and acoustical systems. Other analogous systems include electrical harmonic oscillators such as RLC circuits. The harmonic oscillator model is very important in physics, because any mass subject to a force in stable equilibrium acts as a harmonic oscillator for small vibrations. Harmonic oscillators occur widely in nature and are exploited in many manmade devices, such as clocks and radio circuits. They are the source of virtually all sinusoidal vibrations and waves.
Simple harmonic oscillator.
A simple harmonic oscillator is an oscillator that is neither driven nor damped. It consists of a mass "m", which experiences a single force, "F", which pulls the mass in the direction of the point "x"=0 and depends only on the mass's position "x" and a constant "k". Balance of forces (Newton's second law) for the system is
Solving this differential equation, we find that the motion is described by the function
where
The motion is periodic, repeating itself in a sinusoidal fashion with constant amplitude, "A". In addition to its amplitude, the motion of a simple harmonic oscillator is characterized by its period "T", the time for a single oscillation or its frequency "f" = , the number of cycles per unit time. The position at a given time "t" also depends on the phase, "φ", which determines the starting point on the sine wave. The period and frequency are determined by the size of the mass "m" and the force constant "k", while the amplitude and phase are determined by the starting position and velocity.
The velocity and acceleration of a simple harmonic oscillator oscillate with the same frequency as the position but with shifted phases. The velocity is maximum for zero displacement, while the acceleration is in the opposite direction as the displacement.
The potential energy stored in a simple harmonic oscillator at position "x" is
Damped harmonic oscillator.
In real oscillators, friction, or damping, slows the motion of the system. Due to frictional force, the velocity decreases in proportion to the acting frictional force. While simple harmonic motion oscillates with only the restoring force acting on the system, damped harmonic motion experiences friction. In many vibrating systems the frictional force "F"f can be modeled as being proportional to the velocity "v" of the object: , where "c" is called the "viscous damping coefficient".
Balance of forces (Newton's second law) for damped harmonic oscillators is then
When no external forces are present (i.e. when formula_7), this can be rewritten into the form
where
The value of the damping ratio "ζ" critically determines the behavior of the system. A damped harmonic oscillator can be:
The Q factor of a damped oscillator is defined as 
"Q" is related to the damping ratio by the equation formula_14
Driven harmonic oscillators.
Driven harmonic oscillators are damped oscillators further affected by an externally applied force "F"("t").
Newton's second law takes the form
It is usually rewritten into the form
This equation can be solved exactly for any driving force, using the solutions "z"("t") which satisfy the unforced equation:
and which can be expressed as damped sinusoidal oscillations,
in the case where "ζ" ≤ 1. The amplitude "A" and phase "φ" determine the behavior needed to match the initial conditions.
Step input.
In the case "ζ" < 1 and a unit step input with "x"(0) = 0:
the solution is:
with phase "φ" given by
The time an oscillator needs to adapt to changed external conditions is of the order "τ" = 1/("ζω"0). In physics, the adaptation is called relaxation, and τ is called the relaxation time.
In electrical engineering, a multiple of τ is called the "settling time", i.e. the time necessary to ensure the signal is within a fixed departure from final value, typically within 10%. The term "overshoot" refers to the extent the maximum response exceeds final value, and "undershoot" refers to the extent the response falls below final value for times following the maximum response.
Sinusoidal driving force.
In the case of a sinusoidal driving force:
where formula_23 is the driving amplitude and formula_24 is the driving frequency for a sinusoidal driving mechanism. This type of system appears in AC driven RLC circuits (resistor-inductor-capacitor) and driven spring systems having internal mechanical resistance or external air resistance.
The general solution is a sum of a transient solution that depends on initial conditions, and a steady state that is independent of initial conditions and depends only on the driving amplitude formula_23, driving frequency, formula_24, undamped angular frequency formula_27, and the damping ratio formula_28.
The steady-state solution is proportional to the driving force with an induced phase change of formula_29:
where
is the absolute value of the impedance or linear response function and
is the phase of the oscillation relative to the driving force, if the arctan value is taken to be between -180 degrees and 0 (that is, it represents a phase lag, for both positive and negative values of the arctan's argument).
For a particular driving frequency called the resonance, or resonant frequency formula_33, the amplitude (for a given formula_23) is maximum. This resonance effect only occurs when formula_35, i.e. for significantly underdamped systems. For strongly underdamped systems the value of the amplitude can become quite large near the resonance frequency.
The transient solutions are the same as the unforced (formula_36) damped harmonic oscillator and represent the systems response to other events that occurred previously. The transient solutions typically die out rapidly enough that they can be ignored.
Parametric oscillators.
A parametric oscillator is a driven harmonic oscillator in which the drive energy is provided by varying the parameters of the oscillator, such as the damping or restoring force.
A familiar example of parametric oscillation is "pumping" on a playground swing.
A person on a moving swing can increase the amplitude of the swing's oscillations without any external drive force (pushes) being applied, by changing the moment of inertia of the swing by rocking back and forth ("pumping") or alternately standing and squatting, in rhythm with the swing's oscillations. The varying of the parameters drives the system. Examples of parameters that may be varied are its resonance frequency formula_37 and damping formula_38.
Parametric oscillators are used in many applications. The classical varactor parametric oscillator oscillates when the diode's capacitance is varied periodically. The circuit that varies the diode's capacitance is called the "pump" or "driver". In microwave electronics, waveguide/YAG based parametric oscillators operate in the same fashion. The designer varies a parameter periodically to induce oscillations.
Parametric oscillators have been developed as low-noise amplifiers, especially in the radio and microwave frequency range. Thermal noise is minimal, since a reactance (not a resistance) is varied. Another common use is frequency conversion, e.g., conversion from audio to radio frequencies. For example, the Optical parametric oscillator converts an input laser wave into two output waves of lower frequency (formula_39).
Parametric resonance occurs in a mechanical system when a system is parametrically excited and oscillates at one of its resonant frequencies. Parametric excitation differs from forcing, since the action appears as a time varying modification on a system parameter. This effect is different from regular resonance because it exhibits the instability phenomenon.
Universal oscillator equation.
The equation 
is known as the universal oscillator equation since all second order linear oscillatory systems can be reduced to this form. This is done through nondimensionalization.
If the forcing function is "f"("t") = cos("ωt") = cos("ωtcτ") = cos("ωτ"), where "ω" = "ωt""c", the equation becomes
The solution to this differential equation contains two parts, the "transient" and the "steady state".
Transient solution.
The solution based on solving the ordinary differential equation is for arbitrary constants "c"1 and "c"2
formula_42
The transient solution is independent of the forcing function.
Steady-state solution.
Apply the "complex variables method" by solving the auxiliary equation below and then finding the real part of its solution:
Supposing the solution is of the form
Its derivatives from zero to 2nd order are
Substituting these quantities into the differential equation gives
Dividing by the exponential term on the left results in
Equating the real and imaginary parts results in two independent equations
Amplitude part.
Squaring both equations and adding them together gives
Therefore,
Compare this result with the theory section on resonance, as well as the "magnitude part" of the RLC circuit. This amplitude function is particularly important in the analysis and understanding of the frequency response of second-order systems.
Phase part.
To solve for φ, divide both equations to get
This phase function is particularly important in the analysis and understanding of the frequency response of second-order systems.
Full solution.
Combining the amplitude and phase portions results in the steady-state solution
The solution of original universal oscillator equation is a superposition (sum) of the transient and steady-state solutions
For a more complete description of how to solve the above equation, see linear ODEs with constant coefficients.
Equivalent systems.
Harmonic oscillators occurring in a number of areas of engineering are equivalent in the sense that their mathematical models are identical (see universal oscillator equation above). Below is a table showing analogous quantities in four harmonic oscillator systems in mechanics and electronics. If analogous parameters on the same line in the table are given numerically equal values, the behavior of the oscillators—their output waveform, resonant frequency, damping factor, etc.—are the same.
Application to a conservative force.
The problem of the simple harmonic oscillator occurs frequently in physics, because a mass at equilibrium under the influence of any conservative force, in the limit of small motions, behaves as a simple harmonic oscillator.
A conservative force is one that has a potential energy function. The potential energy function of a harmonic oscillator is:
Given an arbitrary potential energy function formula_55, one can do a Taylor expansion in terms of formula_56 around an energy minimum (formula_57) to model the behavior of small perturbations from equilibrium.
Because formula_59 is a minimum, the first derivative evaluated at formula_60 must be zero, so the linear term drops out:
The constant term "V"("x"0) is arbitrary and thus may be dropped, and a coordinate transformation allows the form of the simple harmonic oscillator to be retrieved:
Thus, given an arbitrary potential energy function formula_55 with a non-vanishing second derivative, one can use the solution to the simple harmonic oscillator to provide an approximate solution for small perturbations around the equilibrium point.
Examples.
Simple pendulum.
Assuming no damping, the differential equation governing a simple pendulum is
If the maximum displacement of the pendulum is small, we can use the approximation formula_65 and instead consider the equation
The solution to this equation is given by:
where formula_68 is the largest angle attained by the pendulum. The period, the time for one complete oscillation, is given by the expression
which is a good approximation of the actual period when formula_68 is small.
Pendulum swinging over turntable.
Simple harmonic motion can in some cases be considered to be the one-dimensional projection of two-dimensional circular motion. Consider a long pendulum swinging over the turntable of a record player. On the edge of the turntable there is an object. If the object is viewed from the same level as the turntable, a projection of the motion of the object seems to be moving backwards and forwards on a straight line orthogonal to the view direction, sinusoidally like the pendulum.
Spring/mass system.
When a spring is stretched or compressed by a mass, the spring develops a restoring force. Hooke's law gives the relationship of the force exerted by the spring when the spring is compressed or stretched a certain length:
where "F" is the force, "k" is the spring constant, and "x" is the displacement of the mass with respect to the equilibrium position. The minus sign in the equation indicates that the force exerted by the spring always acts in a direction that is opposite to the displacement (i.e. the force always acts towards the zero position), and so prevents the mass from flying off to infinity.
By using either force balance or an energy method, it can be readily shown that the motion of this system is given by the following differential equation:
...the latter being Newton's second law of motion.
If the initial displacement is A, and there is no initial velocity, the solution of this equation is given by:
Given an ideal massless spring, formula_74 is the mass on the end of the spring. If the spring itself has mass, its effective mass must be included in formula_74.
Energy variation in the spring–damping system.
In terms of energy, all systems have two types of energy, potential energy and kinetic energy. When a spring is stretched or compressed, it stores elastic potential energy, which then is transferred into kinetic energy. The potential energy within a spring is determined by the equation formula_76
When the spring is stretched or compressed, kinetic energy of the mass gets converted into potential energy of the spring. By conservation of energy, assuming the datum is defined at the equilibrium position, when the spring reaches its maximum potential energy, the kinetic energy of the mass is zero. When the spring is released, it tries to return to equilibrium, and all its potential energy converts to kinetic energy of the mass.

</doc>
<doc id="3027077" url="https://en.wikipedia.org/wiki?curid=3027077" title="Helicity (particle physics)">
Helicity (particle physics)

In particle physics, helicity is the projection of the angular momentum onto the direction of momentum. The angular momentum is the sum of an orbital momentum and a spin , and by definition its relation to linear momentum is
so its component in the direction of is zero. Thus, helicity is also the projection of the spin onto the direction of momentum. This quantity is conserved. 
Because the eigenvalues of spin with respect to an axis have discrete values, the eigenvalues of helicity are also discrete. For a particle of spin , the eigenvalues of helicity are , , ..., −. The measured helicity of a spin particle will range from − to +.
For massless spin- particles, helicity is equivalent to the chirality operator multiplied by /2. By contrast, for massive particles, distinct chirality states (e.g., as occur in the weak interaction charges) have both positive and negative helicity components, in ratios proportional to the mass of the particle.
Little group.
In dimensions, the little group for a massless particle is the double cover of SE(2). This has unitary representations which are invariant under the SE(2) "translations" and transform as under a SE(2) rotation by . This is the helicity representation. There is also another unitary representation which transforms non-trivially under the SE(2) translations. This is the "continuous spin" representation.
In dimensions, the little group is the double cover of SE() (the case where is more complicated because of anyons, etc.). As before, there are unitary representations which don't transform under the SE() "translations" (the "standard" representations) and "continuous spin" representations.

</doc>
<doc id="195984" url="https://en.wikipedia.org/wiki?curid=195984" title="Hermite polynomials">
Hermite polynomials

In mathematics, the Hermite polynomials are a classical orthogonal polynomial sequence.
The polynomials arise in:
Hermite polynomials were defined by though in scarcely recognizable form, and studied in detail by Chebyshev (1859). Chebyshev's work was overlooked and they were named later after Charles Hermite who wrote on the polynomials in 1864 describing them as new. They were consequently not new although in later 1865 papers Hermite was the first to define the multidimensional polynomials.
Definition.
There are two different ways of standardizing the Hermite polynomials:
These two definitions are not exactly identical; each one is a rescaling of the other,
These are Hermite polynomial sequences of different variances; see the material on variances below.
The notation "He" and "H" is that used in the standard references and Abramowitz & Stegun. 
The polynomials "He""n" are sometimes denoted by "H""n", especially in probability theory, because
is the probability density function for the normal distribution with expected value 0 and standard deviation 1.
The first eleven probabilists' Hermite polynomials are:
and the first eleven physicists' Hermite polynomials are:
Properties.
"Hn" is a polynomial of degree "n". The probabilists' version "He" has leading coefficient 1, while the physicists' version "H" has leading coefficient 2"n".
Orthogonality.
"Hn"("x") and "Hen"("x") are "n"th-degree polynomials for "n" = 0, 1, 2, 3, ... These polynomials are orthogonal with respect to the "weight function" (measure)
or
i.e., we have
Furthermore,
or
The probabilist polynomials are thus orthogonal with respect to the standard normal probability density function.
Completeness.
The Hermite polynomials (probabilist or physicist) form an orthogonal basis of the Hilbert space of functions satisfying
in which the inner product is given by the integral including the Gaussian weight function defined in the preceding section,
An orthogonal basis for is a "complete" orthogonal system. For an orthogonal system, "completeness" is equivalent to the fact that the 0 function is the only function orthogonal to "all" functions in the system.
Since the linear span of Hermite polynomials is the space of all polynomials, one has to show (in physicist case) that if satisfies
for every  ≥ 0, then  = 0.
One possible way to do this is to appreciate that the entire function
vanishes identically. The fact then that for every real means that the Fourier transform of is 0, hence is 0 almost everywhere. Variants of the above completeness proof apply to other weights with exponential decay.
In the Hermite case, it is also possible to prove an explicit identity that implies completeness (see section on the Completeness relation below).
An equivalent formulation of the fact that Hermite polynomials are an orthogonal basis for consists in introducing Hermite "functions" (see below), and in saying that the Hermite functions are an orthonormal basis for .
Hermite's differential equation.
The probabilists' Hermite polynomials are solutions of the differential equation
where is a constant, with the boundary conditions that should be polynomially bounded at infinity. With these boundary conditions, the equation has solutions only if λ is a non-negative integer, and up to an overall scaling, the solution is uniquely given by .
Rewriting the differential equation as an eigenvalue problem
solutions are the eigenfunctions of the differential operator . This eigenvalue problem is called the Hermite equation, although the term is also used for the closely related equation
whose solutions are the physicists' Hermite polynomials.
With more general boundary conditions, the Hermite polynomials can be generalized to obtain more general analytic functions for a complex index. An explicit formula can be given in terms of a contour integral .
Recursion relation.
The sequence of Hermite polynomials also satisfies the recursion
Individual coefficients are related by the following recursion formula,
and [0,0]=1, [1,0]=0, [1,1]=1.<br>
Individual coefficients are related by the following recursion formula,
and "a"[0,0]=1, "a"[1,0]=0, "a"[1,1]=2.
The Hermite polynomials constitute an Appell sequence, i.e., they are a polynomial sequence satisfying the identity
Equivalently, by Taylor expanding,
these umbral identities are self-evident and included in the #differential operator representation detailed below, 
In consequence, for the -th derivatives the following relations hold:
It follows that the Hermite polynomials also satisfy the recurrence relation
These last relations, together with the initial polynomials "H"0("x") and "H"1("x"), can be used in practice to compute the polynomials quickly.
Turán's inequalities are
Moreover, the following multiplication theorem holds,
Explicit expression.
The physicists' Hermite polynomials can be written explicitly as
for even value of and
for odd values of .
These two equations may be combined into one using the floor function,
The probabilists' Hermite polynomials "He" have similar formulas, which may be obtained from these by replacing the power of 2 with the corresponding power of (√2)"x", and multiplying the entire sum by .
Inverse Explicit Expression.
The inverse of the above explicit expressions, that is, those for monomials in terms of probabilists’ Hermite polynomials, , are
The corresponding expressions for the physicists’ Hermite polynomials, , follow directly by properly scaling this,
Generating function.
The Hermite polynomials are given by the exponential generating function
This equality is valid for all "x", "t" complex, and can be obtained by writing the Taylor expansion at "x" of the entire function "z" → exp(−"z"2) (in physicist's case). One can also derive the (physicist's) generating function by using Cauchy's Integral Formula to write the Hermite polynomials as
Using this in the sum 
one can evaluate the remaining integral using the calculus of residues and arrive at the desired generating function.
Expected values.
If "X" is a random variable with a normal distribution with standard deviation 1 and expected value , then
The moments of the standard normal may be read off directly from the relation for even indices
where formula_69 is the double factorial. Note that the above expression is a special case of the representation of the probabilists' Hermite polynomials as moments
Asymptotic expansion.
Asymptotically, as "n" → ∞, the expansion
holds true. For certain cases concerning a wider range of evaluation, it is necessary to include a factor for changing amplitude
Which, using Stirling's approximation, can be further simplified, in the limit, to
This expansion is needed to resolve the wave-function of a quantum harmonic oscillator such that it agrees with the classical approximation in the limit of the correspondence principle.
A better approximation, which accounts for the variation in frequency is given by
A finer approximation, which takes into account the uneven spacing of the zeros near the edges, makes use of the substitution 
with which one has the uniform approximation
Similar approximations hold for the monotonic and transition regions. Specifically, if 
then
while for 
with complex and bounded, then
where Ai(·) is the Airy function of the first kind.
Special Values.
The Hermite polynomials evaluated at zero argument formula_81 are called Hermite numbers.
which satisfy the recursion relation formula_83.
In terms of the probabilist's polynomials this translates to
Relations to other functions.
Laguerre polynomials.
The Hermite polynomials can be expressed as a special case of the Laguerre polynomials.
Relation to confluent hypergeometric functions.
The Hermite polynomials can be expressed as a special case of the parabolic cylinder functions.
where formula_88 is Whittaker's confluent hypergeometric function. Similarly,
where formula_91 is Kummer's confluent hypergeometric function.
Differential operator representation.
The probabilists' Hermite polynomials satisfy the identity
where represents differentiation with respect to , and the exponential is interpreted by expanding it as a power series. There are no delicate questions of convergence of this series when it operates on polynomials, since all but finitely many terms vanish.
Since the power series coefficients of the exponential are well known, and higher order derivatives of the monomial can be written down explicitly, this differential operator representation gives rise to a concrete formula for the coefficients of that can be used to quickly compute these polynomials.
Since the formal expression for the Weierstrass transform is , we see that the Weierstrass transform of . Essentially the Weierstrass transform thus turns a series of Hermite polynomials into a corresponding Maclaurin series.
The existence of some formal power series with nonzero constant coefficient, such that , is another equivalent to the statement that these polynomials form an Appell sequence. Since they are an Appell sequence, they are "a fortiori" a Sheffer sequence.
Contour integral representation.
From the generating function representation above, we see that the Hermite polynomials have a representation in terms of a contour integral, as
with the contour encircling the origin.
Generalizations.
The (probabilists') Hermite polynomials defined above are orthogonal with respect to the standard normal probability distribution, whose density function is
which has expected value 0 and variance 1.
Scaling, one may analogously speak of generalized Hermite polynomials
of variance , where is any positive number. These are then orthogonal with respect to the normal probability distribution whose density function is
They are given by
Now, if 
then the polynomial sequence whose th term is
is called the umbral composition of the two polynomial sequences. It can be shown to satisfy the identities
and
The last identity is expressed by saying that this parameterized family of polynomial sequences is a cross-sequence. (See the above section on Appel sequences and on the #Differential operator representation, which leads to a ready derivation of it. This binomial type identity, for = = 1/2, has already been encountered in the above section on #Recursion relations.)
"Negative variance".
Since polynomial sequences form a group under the operation of umbral composition, one may denote by
the sequence that is inverse to the one similarly denoted but without the minus sign, and thus speak of Hermite polynomials of negative variance. For > 0, the coefficients of are just the absolute values of the corresponding coefficients of .
These arise as moments of normal probability distributions: The "n"-th moment of the normal distribution with expected value and variance 2 is
where is a random variable with the specified normal distribution. A special case of the cross-sequence identity then says that
Applications.
Hermite functions.
One can define the Hermite functions from the physicists' polynomials:
Since these functions contain the square root of the weight function, and have been scaled appropriately, they are orthonormal:
and form an orthonormal basis of "L"2(R). This fact is equivalent to the corresponding statement for Hermite polynomials (see above).
The Hermite functions are closely related to the Whittaker function (Whittaker and Watson, 1962) , 
and thereby to other parabolic cylinder functions.
The Hermite functions satisfy the differential equation,
This equation is equivalent to the Schrödinger equation for a harmonic oscillator in quantum mechanics, so these functions are the eigenfunctions.
Recursion relation.
Following recursion relations of Hermite polynomials, the Hermite functions obey
as well as
Extending the first relation to the arbitrary m-th derivatives for any positive integer "m" leads to
This formula can be used in connection with the recurrence relations for and to calculate any derivative of the Hermite functions efficiently.
Cramér's inequality.
The Hermite functions satisfy the following bound due to Harald Cramér
for real, where the constant is less than 1.086435.
Hermite functions as eigenfunctions of the Fourier transform.
The Hermite functions are a set of eigenfunctions of the continuous Fourier transform. To see this, take the physicist's version of the generating function and multiply by . This gives
Choosing the unitary representation of the Fourier transform, the Fourier transform of the left hand side is given by
The Fourier transform of the right hand side is given by
Equating like powers of "t" in the transformed versions of the left- and right-hand sides finally yields
The Hermite functions are thus an orthonormal basis of which "diagonalizes the Fourier transform operator".
In this case, we chose the unitary version of the Fourier transform, so the eigenvalues are . The ensuing resolution of the identity then serves to define powers, including fractional ones, of the Fourier transform, to wit a Fractional Fourier transform generalization, in effect a Mehler kernel.
Wigner distributions of Hermite functions.
The Wigner distribution function of the "n"-th order Hermite function is related to the "n"-th order Laguerre polynomial.
The Laguerre polynomials are 
leading to the oscillator Laguerre functions,
For all natural integers , it is straightforward to see that
where the Wigner distribution of a function is defined as
This is a fundamental result for the quantum harmonic oscillator discovered by Hip Groenewold in 1946 in his PhD thesis. It is the standard paradigm of quantum mechanics in phase space.
There are further relations between the two families of polynomials.
Combinatorial interpretation of coefficients.
In the Hermite polynomial "He""n"("x") of variance 1, the absolute value of the coefficient of "x""k" is the number of (unordered) partitions of an "n"-member set into "k" singletons and ("n" − "k")/2 (unordered) pairs. The sum of the absolute values of the coefficients gives the total number of partitions into singletons and pairs, the so-called telephone numbers
These numbers may also be expressed as a special value of the Hermite polynomials
Completeness relation.
The Christoffel–Darboux formula for Hermite polynomials reads
Moreover, the following completeness identity for the above Hermite functions holds in the sense of distributions
where is the Dirac delta function, the Hermite functions, and "δ"("x" − "y") represents the Lebesgue measure on the line "y" = "x" in ℝ2, normalized so that its projection on the horizontal axis is the usual Lebesgue measure.
This distributional identity follows (N.Wiener) by taking in Mehler's formula, valid when ,
which is often stated equivalently as a separable kernel,
The function is the bivariate Gaussian probability density on ℝ2 which is, when is close to 1, very concentrated around the line  = , and very spread out on that line. It follows that
when are continuous and compactly supported.
This yields that can be expressed in Hermite functions, as the sum of a series of vectors in "L"2(ℝ), namely
In order to prove the above equality for , the Fourier transform of Gaussian functions is used repeatedly,
The Hermite polynomial is then represented as
With this representation for and , it is evident that
and this yields the desired resolution of the identity result, using again the Fourier transform of Gaussian kernels under the substitution

</doc>
<doc id="189682" url="https://en.wikipedia.org/wiki?curid=189682" title="Hermitian matrix">
Hermitian matrix

In mathematics, a Hermitian matrix (or self-adjoint matrix) is a square matrix with complex entries that is equal to its own conjugate transpose—that is, the element in the -th row and -th column is equal to the complex conjugate of the element in the -th row and -th column, for all indices and :
Hermitian matrices can be understood as the complex extension of real symmetric matrices.
If the conjugate transpose of a matrix formula_3 is denoted by formula_4, then the Hermitian property can be written concisely as
Hermitian matrices are named after Charles Hermite, who demonstrated in 1855 that matrices of this form share a property with real symmetric matrices of always having real eigenvalues.
Examples.
See the following example:
The diagonal elements must be real, as they must be their own complex conjugate.
Well-known families of Pauli matrices, Gell-Mann matrices and their generalizations are Hermitian. In theoretical physics such Hermitian matrices are often multiplied by imaginary coefficients, which results in "skew-Hermitian" matrices (see below).
Here we offer another useful Hermitian matrix using an abstract example.
If a square matrix formula_7 equals the multiplication of a matrix and its conjugate transpose, that is, formula_8, then formula_7 is a Hermitian positive semi-definite matrix. Furthermore, if formula_10 is row full-rank, then formula_7 is positive definite.
Further properties.
Additional facts related to Hermitian matrices include:
Rayleigh quotient.
In mathematics, for a given complex Hermitian matrix "M" and nonzero vector "x", the Rayleigh quotient formula_33, is defined as:
For real matrices and vectors, the condition of being Hermitian reduces to that of being symmetric, and the conjugate transpose formula_35 to the usual transpose formula_36. Note that formula_37 for any non-zero real scalar formula_38. Recall that a Hermitian (or real symmetric) matrix has real eigenvalues. It can be shown that, for a given matrix, the Rayleigh quotient reaches its minimum value formula_39 (the smallest eigenvalue of M) when formula_40 is formula_41 (the corresponding eigenvector). Similarly, formula_42 and formula_43.
The Rayleigh quotient is used in the min-max theorem to get exact values of all eigenvalues. It is also used in eigenvalue algorithms to obtain an eigenvalue approximation from an eigenvector approximation. Specifically, this is the basis for Rayleigh quotient iteration.
The range of the Rayleigh quotient (for matrix that is not necessarily Hermitian) is called a numerical range, (or spectrum in functional analysis). When the matrix is Hermitian, the numerical range is equal to the spectral norm. Still in functional analysis, formula_44 is known as the spectral radius. In the context of C*-algebras or algebraic quantum mechanics, the function that to M associates the Rayleigh-Ritz quotient R(M,x) for a fixed x and M varying through the algebra would be referred to as "vector state" of the algebra.

</doc>
<doc id="211602" url="https://en.wikipedia.org/wiki?curid=211602" title="Hidden variable theory">
Hidden variable theory

Historically, in physics, hidden variable theories were espoused by some physicists who argued that the state of a physical system, as formulated by quantum mechanics, does not give a complete description for the system; i.e., that quantum mechanics is ultimately incomplete, and that a complete theory would provide descriptive categories to account for all observable behavior and thus avoid any indeterminism. The existence of indeterminacy for some measurements is a characteristic of prevalent interpretations of quantum mechanics; moreover, bounds for indeterminacy can be expressed in a quantitative form by the Heisenberg uncertainty principle.
Albert Einstein, the most famous proponent of hidden variables, objected to the fundamentally probabilistic nature of quantum mechanics, and famously declared "I am convinced God does not play dice". Einstein, Podolsky, and Rosen argued that "elements of reality" (hidden variables) must be added to quantum mechanics to explain entanglement without action at a distance. Later, Bell's theorem would suggest that local hidden variables of certain types are impossible, or that they evolve non-locally. A famous non-local theory is De Broglie–Bohm theory.
Motivation.
Under the standard Copenhagen interpretation, quantum mechanics is non-deterministic, meaning that it generally does not predict the outcome of any measurement with certainty. Instead, it indicates what the probabilities of the outcomes are, with the indeterminism of observable quantities constrained by the uncertainty principle. The question arises whether there might be some deeper reality hidden beneath quantum mechanics, to be described by a more fundamental theory that can always predict the outcome of each measurement with certainty: if the exact properties of every subatomic particle were known the entire system could be modeled exactly using deterministic physics similar to classical physics.
In other words, it is conceivable that the standard interpretation of quantum mechanics is an incomplete description of nature. The designation of variables as underlying "hidden" variables depends on the level of physical description (so, for example, "if a gas is described in terms of temperature, pressure, and volume, then the velocities of the individual atoms in the gas would be hidden variables".). Physicists supporting De Broglie–Bohm theory maintain that underlying the observed probabilistic nature of the universe is a deterministic objective foundation/property—the hidden variable. Others, however, believe that there is no deeper deterministic reality in quantum mechanics—experiments have shown a vast class of hidden variable theories to be incompatible with observations.
Lack of a kind of realism (the latter understood here as asserting independent existence and evolution of physical quantities, such as position or momentum, without the process of measurement) is crucial in the Copenhagen interpretation. Realistic interpretations (which were already incorporated, to an extent, into the physics of Feynman), on the other hand, assume that particles have certain trajectories. Under such view, these trajectories will almost always be continuous, which follows both from the finiteness of the perceived speed of light ("leaps" should rather be precluded) and, more importantly, from the principle of least action, as deduced in quantum physics by Dirac. But continuous movement, in accordance with the mathematical definition, implies deterministic movement for a range of time arguments; and thus realism is, under modern physics, one more reason for seeking (at least certain limited) determinism and thus a hidden variable theory (especially that such theory exists: see De Broglie–Bohm interpretation).
Although determinism was initially a major motivation for physicists looking for hidden variable theories, nondeterministic theories trying to explain what the supposed reality underlying the quantum mechanics formalism looks like are also considered hidden variable theories; for example Edward Nelson's stochastic mechanics.
"God does not play dice".
In June 1926, Max Born published a paper, "Zur Quantenmechanik der Stoßvorgänge" ("Quantum Mechanics of Collision Phenomena") in the scientific journal "Zeitschrift für Physik", in which he was the first to clearly enunciate the probabilistic interpretation of the quantum wavefunction, which had been introduced by Erwin Schrödinger earlier in the year. Born concluded the paper as follows:
Here the whole problem of determinism comes up. From the standpoint of our quantum mechanics there is no quantity which in any individual case causally fixes the consequence of the collision; but also experimentally we have so far no reason to believe that there are some inner properties of the atom which conditions a definite outcome for the collision. Ought we to hope later to discover such properties ... and determine them in individual cases? Or ought we to believe that the agreement of theory and experiment—as to the impossibility of prescribing conditions for a causal evolution—is a pre-established harmony founded on the nonexistence of such conditions? I myself am inclined to give up determinism in the world of atoms. But that is a philosophical question for which physical arguments alone are not decisive.
Born's interpretation of the wavefunction was criticized by Schrödinger, who had previously attempted to interpret it in real physical terms, but Albert Einstein's response became one of the earliest and most famous assertions that quantum mechanics is incomplete:
Quantum mechanics is very worthy of regard. But an inner voice tells me that this is not yet the right track. The theory yields much, but it hardly brings us closer to the Old One's secrets. I, in any case, am convinced that "He" does not play dice.
Early attempts at hidden variable theories.
Shortly after making his famous "God does not play dice" comment, Einstein attempted to formulate a deterministic counterproposal to quantum mechanics, presenting a paper at a meeting of the Academy of Sciences in Berlin, on 5 May 1927, titled "Bestimmt Schrödinger's Wellenmechanik die Bewegung eines Systems vollständig oder nur im Sinne der Statistik?" (“Does Schrödinger’s wave mechanics determine the motion of a system completely or only in the statistical sense?”). However, as the paper was being prepared for publication in the academy's journal, Einstein decided to withdraw it, possibly because he discovered that implied non-separability of entangled systems could not be eliminated, as he had hoped.
At the Fifth Solvay Congress, held in Belgium in October 1927 and attended by all the major theoretical physicists of the era, Louis de Broglie presented his own version of a deterministic hidden-variable theory, apparently unaware of Einstein's aborted attempt earlier in the year. In his theory, every particle had an associated, hidden "pilot wave" which served to guide its trajectory through space. The theory was subject to criticism at the Congress, particularly by Wolfgang Pauli, which de Broglie did not adequately answer. De Broglie abandoned the theory shortly thereafter.
Declaration of completeness of quantum mechanics.
Also at the Fifth Solvay Congress, Max Born and Werner Heisenberg made a presentation summarizing the recent tremendous theoretical development of the subject. At the conclusion of the presentation, they declared:
[W]hile we consider ... a quantum mechanical treatment of the electromagnetic field ... as not yet finished, we consider quantum mechanics to be a closed theory, whose fundamental physical and mathematical assumptions are no longer susceptible of any modification...
On the question of the 'validity of the law of causality' we have this opinion: as long as one takes into account only experiments that lie in the domain of our currently acquired physical and quantum mechanical experience, the assumption of indeterminism in principle, here taken as fundamental, agrees with experience.
Bohr–Einstein debates.
Although there is no record of Einstein responding to Born and Heisenberg during the technical sessions of the Fifth Solvay Congress, he did challenge the completeness of quantum mechanics during informal discussions over meals, presenting a thought experiment intended to demonstrate that quantum mechanics could not be entirely correct. He did likewise during the Sixth Solvay Congress held in 1930. Both times, Niels Bohr is generally considered to have successfully defended quantum mechanics by discovering errors in Einstein's arguments.
EPR paradox.
The debates between Bohr and Einstein essentially concluded in 1935, when Einstein finally expressed what is widely considered his best argument against the completeness of quantum mechanics. Einstein, Podolsky, and Rosen had proposed their definition of a "complete" description as one which uniquely determines the values of all its measurable properties. Einstein later summarized their argument as follows:
Consider a mechanical system consisting of two partial systems A and B which interact with each other only during a limited time. Let the ψ function [i.e., wavefunction ] before their interaction be given. Then the Schrödinger equation will furnish the ψ function after the interaction has taken place. Let us now determine the physical state of the partial system A as completely as possible by measurements. Then quantum mechanics allows us to determine the ψ function of the partial system B from the measurements made, and from the ψ function of the total system. This determination, however, gives a result which depends upon which of the physical quantities (observables) of A have been measured (for instance, coordinates or momenta). Since there can be only one physical state of B after the interaction which cannot reasonably be considered to depend on the particular measurement we perform on the system A separated from B it may be concluded that the ψ function is not unambiguously coordinated to the physical state. This coordination of several ψ functions to the same physical state of system B shows again that the ψ function cannot be interpreted as a (complete) description of a physical state of a single system.
Bohr answered Einstein's challenge by pointing out that, in accordance with the principle of complementarity, one of Einstein's assumptions, regarding the act of measurement, is not true in quantum mechanics: a quantum mechanical measurement not only "states", but also "prepares" the physics of a system:
[The argument of] Einstein, Podolsky and Rosen contains an ambiguity as regards the meaning of the expression "without in any way disturbing a system." ... [E]ven at this stage [i.e., the measurement of, for example, a particle that is part of an entangled pair], there is essentially the question of an influence on the very conditions which define the possible types of predictions regarding the future behavior of the system. Since these conditions constitute an inherent element of the description of any phenomenon to which the term "physical reality" can be properly attached, we see that the argumentation of the mentioned authors does not justify their conclusion that quantum-mechanical description is essentially incomplete."
Quantum mechanics is also not "local", essentially because the state of a system is described by the Hilbert vector formula_1, which includes the value at every site, formula_2. So in this instance Einstein was simply wrong, although he did sharply identify implications of the formalism of quantum mechanics that had been previously overlooked.
Bell's theorem.
In 1964, John Bell showed through his famous theorem that if local hidden variables exist, certain experiments could be performed involving quantum entanglement where the result would satisfy a Bell inequality. If, on the other hand, statistical correlations resulting from quantum entanglement could not be explained by local hidden variables, the Bell inequality would be violated. Another no-go theorem concerning hidden variable theories is the Kochen–Specker theorem.
Physicists such as Alain Aspect and Paul Kwiat have performed experiments that have found violations of these inequalities up to 242 standard deviations (excellent scientific certainty). This rules out local hidden variable theories, but does not rule out non-local ones. Theoretically, there could be experimental problems that affect the validity of the experimental findings.
Gerard 't Hooft has disputed the validity of Bell's theorem on the basis of the superdeterminism loophole and proposed some ideas to construct local deterministic models.
Bohm's hidden variable theory.
Assuming the validity of Bell's theorem, any deterministic hidden-variable theory which is consistent with quantum mechanics would have to be non-local, maintaining the existence of instantaneous or faster-than-light relations (correlations) between physically separated entities. The currently best-known hidden-variable theory, the "causal" interpretation of the physicist and philosopher David Bohm, originally published in 1952, is a "non-local hidden variable theory". Bohm unknowingly rediscovered (and extended) the idea that Louis de Broglie had proposed in 1927 (and abandoned) – hence this theory is commonly called "de Broglie-Bohm theory". Bohm posited "both" the quantum particle, e.g. an electron, and a hidden 'guiding wave' that governs its motion. Thus, in this theory electrons are quite clearly particles—when a double-slit experiment is performed, its trajectory goes through one slit rather than the other. Also, the slit passed through is not random but is governed by the (hidden) guiding wave, resulting in the wave pattern that is observed.
Such a view does not contradict the idea of local events that is used in both classical atomism and relativity theory as Bohm's theory (and quantum mechanics) are still locally causal (that is, information travel is still restricted to the speed of light) but allow nonlocal correlations. It points to a view of a more holistic, mutually interpenetrating and interacting world. Indeed, Bohm himself stressed the holistic aspect of quantum theory in his later years, when he became interested in the ideas of Jiddu Krishnamurti.
In Bohm's interpretation, the (nonlocal) quantum potential constitutes an implicate (hidden) order which organizes a particle, and which may itself be the result of yet a further implicate order: a "superimplicate order" which organizes a field. Nowadays Bohm's theory is considered to be one of many interpretations of quantum mechanics which give a realist interpretation, and not merely a positivistic one, to quantum-mechanical calculations. Some consider it the simplest theory to explain quantum phenomena. Nevertheless, it "is" a hidden variable theory, and necessarily so. The major reference for Bohm's theory today is his book with Basil Hiley, published posthumously.
A possible weakness of Bohm's theory is that some (including Einstein, Pauli, and Heisenberg) feel that it looks contrived. (Indeed, Bohm thought this of his original formulation of the theory.) It was deliberately designed to give predictions that are in all details identical to conventional quantum mechanics. Bohm's original aim was not to make a serious counterproposal but simply to demonstrate that hidden-variable theories are indeed possible. (It thus provided a supposed counterexample to the famous proof by John von Neumann that was generally believed to demonstrate that no deterministic theory reproducing the statistical predictions of quantum mechanics is possible.) Bohm said he considered his theory to be unacceptable as a physical theory due to the guiding wave's existence in an abstract multi-dimensional configuration space, rather than three-dimensional space. His hope was that the theory would lead to new insights and experiments that would lead ultimately to an acceptable one; his aim was not to set out a deterministic, mechanical viewpoint, but rather to show that it was possible to attribute properties to an underlying reality, in contrast to the conventional approach to quantum mechanics.
Recent developments.
In August 2011, Roger Colbeck and Renato Renner published a proof that any extension of quantum mechanical theory, whether using hidden variables or otherwise, cannot provide a more accurate prediction of outcomes, assuming that observers can freely choose the measurement settings. Colbeck and Renner write: "In the present work, we have ... excluded the possibility that any extension of quantum theory (not necessarily in the form of local hidden variables) can help predict the outcomes of any measurement on any quantum state. In this sense, we show the following: under the assumption that measurement settings can be chosen freely, quantum theory really is complete".
In January 2013, GianCarlo Ghirardi and Raffaele Romano described a model which, "under a different free choice assumption [...] violates [the statement by Colbeck and Renner] for almost all states of a bipartite two-level system, in a possibly experimentally testable way".

</doc>
<doc id="20556903" url="https://en.wikipedia.org/wiki?curid=20556903" title="Higgs boson">
Higgs boson

The Higgs boson is an elementary particle in the Standard Model of particle physics. It is the quantum excitation of the Higgs field—a fundamental field of crucial importance to particle physics theory, first suspected to exist in the 1960s, that unlike other known fields such as the electromagnetic field, takes a non-zero constant value almost everywhere. The question of the Higgs field's existence has been the last unverified part of the Standard Model of particle physics and, according to some, "the central problem in particle physics". The presence of this field, now believed to be confirmed, explains why some fundamental particles have mass when, based on the symmetries controlling their interactions, they should be massless. The existence of the Higgs field would also resolve several other long-standing puzzles, such as the reason for the weak force's extremely short range.
Although it is hypothesized that the Higgs field permeates the entire Universe, evidence for its existence has been very difficult to obtain. In principle, the Higgs field can be detected through its excitations, manifest as Higgs particles, but these are extremely difficult to produce and detect. The importance of this fundamental question led to a 40 year search, and the construction of one of the world's most expensive and complex experimental facilities to date, CERN's Large Hadron Collider, in an attempt to create Higgs bosons and other particles for observation and study. On 4 July 2012, the discovery of a new particle with a mass between 125 and was announced; physicists suspected that it was the Higgs boson. Since then, the particle has been shown to behave, interact, and decay in many of the ways predicted by the Standard Model. It was also tentatively confirmed to have even parity and zero spin, two fundamental attributes of a Higgs boson. This appears to be the first elementary scalar particle discovered in nature. More studies are needed to verify that the discovered particle has properties matching those predicted for the Higgs boson by the Standard Model, or whether, as predicted by some theories, multiple Higgs bosons exist.
The Higgs boson is named after Peter Higgs, one of six physicists who, in 1964, proposed the mechanism that suggested the existence of such a particle. On December 10, 2013, two of them, Peter Higgs and François Englert, were awarded the Nobel Prize in Physics for their work and prediction (Englert's co-researcher Robert Brout had died in 2011 and the Nobel Prize is not ordinarily given posthumously). Although Higgs's name has come to be associated with this theory, several researchers between about 1960 and 1972 each independently developed different parts of it. In mainstream media the Higgs boson has often been called the "God particle", from ; the nickname is strongly disliked by many physicists, including Higgs, who regard it as sensationalistic.
In the Standard Model, the Higgs particle is a boson with no spin, electric charge, or colour charge. It is also very unstable, decaying into other particles almost immediately. It is a quantum excitation of one of the four components of the Higgs field. The latter constitutes a scalar field, with two neutral and two electrically charged components that form a complex doublet of the weak isospin SU(2) symmetry. The Higgs field is tachyonic (this does not refer to faster-than-light speeds, it means that symmetry-breaking through condensation of a particle must occur under certain conditions), and has a "Mexican hat" shaped potential with nonzero strength everywhere (including otherwise empty space), which in its vacuum state breaks the weak isospin symmetry of the electroweak interaction. When this happens, three components of the Higgs field are "absorbed" by the SU(2) and U(1) gauge bosons (the "Higgs mechanism") to become the longitudinal components of the now-massive W and Z bosons of the weak force. The remaining electrically neutral component separately couples to other particles known as fermions (via Yukawa couplings), causing these to acquire mass as well. Some versions of the theory predict more than one kind of Higgs fields and bosons. Alternative "Higgsless" models would have been considered if the Higgs boson was not discovered.
A non-technical summary.
Overview.
Physicists explain the properties and forces between elementary particles in terms of the Standard Model—a widely accepted and "remarkably" accurate framework based on gauge invariance and symmetries, believed to explain almost everything in the known universe, other than gravity. But by around 1960 all attempts to create a gauge invariant theory for two of the four fundamental forces had consistently failed at one crucial point: although gauge invariance seemed extremely important, it seemed to make any theory of electromagnetism and the weak force go haywire, by demanding that either many particles with mass were massless or that non-existent forces and massless particles had to exist. Scientists had no idea how to get past this point.
In 1962 physicist Philip Anderson wrote a paper that built upon work by Yoichiro Nambu concerning "broken symmetries" in superconductivity and particle physics. He suggested that "broken symmetries" might also be the missing piece needed to solve the problems of gauge invariance. In 1964 a theory was created almost simultaneously by 3 different groups of researchers, that showed Anderson's suggestion was possible - the gauge theory and "mass problems" could indeed be resolved if an unusual kind of field, now generally called the "Higgs field", existed throughout the universe; if the Higgs field did exist, it would apparently cause existing particles to acquire mass instead of new massless particles being formed. Although these ideas did not gain much initial support or attention, by 1972 they had been developed into a comprehensive theory and proved capable of giving "sensible" results that accurately described particles known at the time, and which accurately predicted of several other particles discovered during the following years. During the 1970s these theories rapidly became the "standard model". There was not yet any direct evidence that the Higgs field actually existed, but even without proof of the field, the accuracy of its predictions led scientists to believe the theory might be true. By the 1980s the question whether or not the Higgs field existed had come to be regarded as one of the most important unanswered questions in particle physics. 
If the Higgs field could be shown to exist, it would be a monumental discovery for science and human knowledge, and would open doorways to new knowledge in many disciplines. If not, then other more complicated theories would need to be considered. The simplest means to test the existence of the Higgs field would be a search for a new elementary particle that the field would have to give off, a particle known as the "Higgs boson" or the "Higgs particle". This particle would be extremely difficult to find. After significant technological advancements, by the 1990s two large experimental installations were being designed and constructed that allowed to search for the Higgs boson.
While several symmetries in nature are spontaneously broken through a form of the Higgs mechanism, in the context of the Standard Model the term "Higgs mechanism" almost always means symmetry breaking of the electroweak field. It is considered confirmed, but revealing the exact cause has been difficult. Various analogies have also been invented to describe the Higgs field and boson, including analogies with well-known symmetry breaking effects such as the rainbow and prism, electric fields, ripples, and resistance of macro objects moving through media, like people moving through crowds or some objects moving through syrup or molasses. However, analogies based on simple resistance to motion are inaccurate as the Higgs field does not work by resisting motion.
Significance.
Scientific impact.
Evidence of the Higgs field and its properties has been extremely significant scientifically, for many reasons. The Higgs boson's importance is largely that it is able to be examined using existing knowledge and experimental technology, as a way to confirm and study the entire Higgs field theory. Conversely, proof that the Higgs field and boson do "not" exist would also have been significant. In discussion form, the relevance includes:
Practical and technological impact of discovery.
As yet, there are no known immediate technological benefits of finding the Higgs particle. However, a common pattern for fundamental discoveries is for practical applications to follow later, once the discovery has been explored further, at which point they become the basis for new technologies of importance to society.
The challenges in particle physics have furthered major technological progress of widespread importance. For example, the World Wide Web began as a project to improve CERN's communication system. CERN's requirement to process massive amounts of data produced by the Large Hadron Collider also led to contributions to the fields of distributed and cloud computing.
History.
Particle physicists study matter made from fundamental particles whose interactions are mediated by exchange particles - gauge bosons - acting as force carriers. At the beginning of the 1960s a number of these particles had been discovered or proposed, along with theories suggesting how they relate to each other, some of which had already been reformulated as field theories in which the objects of study are not particles and forces, but quantum fields and their symmetries. However, attempts to unify known fundamental forces such as the electromagnetic force and the weak nuclear force were known to be incomplete. One known omission was that gauge invariant approaches, including non-abelian models such as Yang–Mills theory (1954), which held great promise for unified theories, also seemed to predict known massive particles as massless. Goldstone's theorem, relating to continuous symmetries within some theories, also appeared to rule out many obvious solutions, since it appeared to show that zero-mass particles would have to also exist that were "simply not seen". According to Guralnik, physicists had "no understanding" how these problems could be overcome.
Particle physicist and mathematician Peter Woit summarised the state of research at the time:
The Higgs mechanism is a process by which vector bosons can get rest mass "without" explicitly breaking gauge invariance, as a byproduct of spontaneous symmetry breaking. The mathematical theory behind spontaneous symmetry breaking was initially conceived and published within particle physics by Yoichiro Nambu in 1960, the concept that such a mechanism could offer a possible solution for the "mass problem" was originally suggested in 1962 by Philip Anderson (who had previously written papers on broken symmetry and its outcomes in superconductivity and concluded in his 1963 paper on Yang-Mills theory that "considering the superconducting analog... [t]hese two types of bosons seem capable of canceling each other out... leaving finite mass bosons"), and Abraham Klein and Benjamin Lee showed in March 1964 that Goldstone's theorem could be avoided this way in at least some non-relativistic cases and speculated it might be possible in truly relativistic cases.
These approaches were quickly developed into a full relativistic model, independently and almost simultaneously, by three groups of physicists: by François Englert and Robert Brout in August 1964; by Peter Higgs in October 1964; and by Gerald Guralnik, Carl Hagen, and Tom Kibble (GHK) in November 1964. Higgs also wrote a short but important response published in September 1964 to an objection by Gilbert, which showed that if calculating within the radiation gauge, Goldstone's theorem and Gilbert's objection would become inapplicable. (Higgs later described Gilbert's objection as prompting his own paper.) Properties of the model were further considered by Guralnik in 1965, by Higgs in 1966, by Kibble in 1967, and further by GHK in 1967. The original three 1964 papers showed that when a gauge theory is combined with an additional field that spontaneously breaks the symmetry, the gauge bosons can consistently acquire a finite mass. In 1967, Steven Weinberg and Abdus Salam independently showed how a Higgs mechanism could be used to break the electroweak symmetry of Sheldon Glashow's unified model for the weak and electromagnetic interactions (itself an extension of work by Schwinger), forming what became the Standard Model of particle physics. Weinberg was the first to observe that this would also provide mass terms for the fermions. 
However, the seminal papers on spontaneous breaking of gauge symmetries were at first largely ignored, because it was widely believed that the (non-Abelian gauge) theories in question were a dead-end, and in particular that they could not be renormalised. In 1971–72, Martinus Veltman and Gerard 't Hooft proved renormalisation of Yang–Mills was possible in two papers covering massless, and then massive, fields. Their contribution, and others' work on the renormalization group - including "substantial" theoretical work by Russian physicists Ludvig Faddeev, Andrei Slavnov, Efim Fradkin and Igor Tyutin - was eventually "enormously profound and influential", but even with all key elements of the eventual theory published there was still almost no wider interest. For example, Coleman found in a study that "essentially no-one paid any attention" to Weinberg's paper prior to 1971 and discussed by David Politzer in his 2004 Nobel speech. – now the most cited in particle physics – and even in 1970 according to Politzer, Glashow's teaching of the weak interaction contained no mention of Weinberg's, Salam's, or Glashow's own work. In practice, Politzer states, almost everyone learned of the theory due to physicist Benjamin Lee, who combined the work of Veltman and 't Hooft with insights by others, and popularised the completed theory. In this way, from 1971, interest and acceptance "exploded"  and the ideas were quickly absorbed in the mainstream.
The resulting electroweak theory and Standard Model have accurately predicted (among other things) weak neutral currents, three bosons, the top and charm quarks, and with great precision, the mass and other properties of some of these. Many of those involved eventually won Nobel Prizes or other renowned awards. A 1974 paper and comprehensive review in "Reviews of Modern Physics" commented that "while no one doubted the [mathematical] correctness of these arguments, no one quite believed that nature was diabolically clever enough to take advantage of them", adding that the theory had so far produced accurate answers that accorded with experiment, but it was unknown whether the theory was fundamentally correct. By 1986 and again in the 1990s it became possible to write that understanding and proving the Higgs sector of the Standard Model was "the central problem today in particle physics".
Summary and impact of the PRL papers.
The three papers written in 1964 were each recognised as milestone papers during "Physical Review Letters" 50th anniversary celebration. Their six authors were also awarded the 2010 J. J. Sakurai Prize for Theoretical Particle Physics for this work. (A controversy also arose the same year, because in the event of a Nobel Prize only up to three scientists could be recognised, with six being credited for the papers. ) Two of the three PRL papers (by Higgs and by GHK) contained equations for the hypothetical field that eventually would become known as the Higgs field and its hypothetical quantum, the Higgs boson. Higgs' subsequent 1966 paper showed the decay mechanism of the boson; only a massive boson can decay and the decays can prove the mechanism.
In the paper by Higgs the boson is massive, and in a closing sentence Higgs writes that "an essential feature" of the theory "is the prediction of incomplete multiplets of scalar and vector bosons". (Frank Close comments that 1960s gauge theorists were focused on the problem of massless "vector" bosons, and the implied existence of a massive "scalar" boson was not seen as important; only Higgs directly addressed it.) In the paper by GHK the boson is massless and decoupled from the massive states. In reviews dated 2009 and 2011, Guralnik states that in the GHK model the boson is massless only in a lowest-order approximation, but it is not subject to any constraint and acquires mass at higher orders, and adds that the GHK paper was the only one to show that there are no massless Goldstone bosons in the model and to give a complete analysis of the general Higgs mechanism. All three reached similar conclusions, despite their very different approaches: Higgs' paper essentially used classical techniques, Englert and Brout's involved calculating vacuum polarization in perturbation theory around an assumed symmetry-breaking vacuum state, and GHK used operator formalism and conservation laws to explore in depth the ways in which Goldstone's theorem may be worked around.
Theoretical properties.
Theoretical need for the Higgs.
Gauge invariance is an important property of modern particle theories such as the Standard Model, partly due to its success in other areas of fundamental physics such as electromagnetism and the strong interaction (quantum chromodynamics). However, there were great difficulties in developing gauge theories for the weak nuclear force or a possible unified electroweak interaction. Fermions with a mass term would violate gauge symmetry and therefore cannot be gauge invariant. (This can be seen by examining the Dirac Lagrangian for a fermion in terms of left and right handed components; we find none of the spin-half particles could ever flip helicity as required for mass, so they must be massless.) W and Z bosons are observed to have mass, but a boson mass term contains terms, which clearly depend on the choice of gauge and therefore these masses too cannot be gauge invariant. Therefore, it seems that "none" of the standard model fermions "or" bosons could "begin" with mass as an inbuilt property except by abandoning gauge invariance. If gauge invariance were to be retained, then these particles had to be acquiring their mass by some other mechanism or interaction. Additionally, whatever was giving these particles their mass, had to not "break" gauge invariance as the basis for other parts of the theories where it worked well, "and" had to not require or predict unexpected massless particles and long-range forces (seemingly an inevitable consequence of Goldstone's theorem) which did not actually seem to exist in nature.
A solution to all of these overlapping problems came from the discovery of a previously unnoticed borderline case hidden in the mathematics of Goldstone's theorem, that under certain conditions it "might" theoretically be possible for a symmetry to be broken "without" disrupting gauge invariance and "without" any new massless particles or forces, and having "sensible" (renormalisable) results mathematically: this became known as the Higgs mechanism.
The Standard Model hypothesizes a field which is responsible for this effect, called the Higgs field (symbol: formula_1), which has the unusual property of a non-zero amplitude in its ground state; i.e., a non-zero vacuum expectation value. It can have this effect because of its unusual "Mexican hat" shaped potential whose lowest "point" is not at its "centre". Below a certain extremely high energy level the existence of this non-zero vacuum expectation spontaneously breaks electroweak gauge symmetry which in turn gives rise to the Higgs mechanism and triggers the acquisition of mass by those particles interacting with the field. This effect occurs because scalar field components of the Higgs field are "absorbed" by the massive bosons as degrees of freedom, and couple to the fermions via Yukawa coupling, thereby producing the expected mass terms. In effect when symmetry breaks under these conditions, the Goldstone bosons that arise "interact" with the Higgs field (and with other particles capable of interacting with the Higgs field) instead of becoming new massless particles, the intractable problems of both underlying theories "neutralise" each other, and the residual outcome is that elementary particles acquire a consistent mass based on how strongly they interact with the Higgs field. It is the simplest known process capable of giving mass to the gauge bosons while remaining compatible with gauge theories. Its quantum would be a scalar boson, known as the Higgs boson.
Properties of the Higgs field.
In the Standard Model, the Higgs field is a scalar tachyonic field – 'scalar' meaning it does not transform under Lorentz transformations, and 'tachyonic' meaning the field (but not the particle) has imaginary mass and in certain configurations must undergo symmetry breaking. It consists of four components, two neutral ones and two charged component fields. Both of the charged components and one of the neutral fields are Goldstone bosons, which act as the longitudinal third-polarization components of the massive W+, W−, and Z bosons. The quantum of the remaining neutral component corresponds to (and is theoretically realised as) the massive Higgs boson, this component can interact with fermions via Yukawa coupling to give them mass, as well.
Mathematically, the Higgs field has imaginary mass and is therefore a tachyonic field. While tachyons (particles that move faster than light) are a purely hypothetical concept, fields with imaginary mass have come to play an important role in modern physics. Under no circumstances do any excitations ever propagate faster than light in such theories — the presence or absence of a tachyonic mass has no effect whatsoever on the maximum velocity of signals (there is no violation of causality). Instead of faster-than-light particles, the imaginary mass creates an instability:- any configuration in which one or more field excitations are tachyonic must spontaneously decay, and the resulting configuration contains no physical tachyons. This process is known as tachyon condensation, and is now believed to be the explanation for how the Higgs mechanism itself arises in nature, and therefore the reason behind electroweak symmetry breaking.
Although the notion of imaginary mass might seem troubling, it is only the field, and not the mass itself, that is quantized. Therefore, the field operators at spacelike separated points still commute (or anticommute), and information and particles still do not propagate faster than light. Tachyon condensation drives a physical system that has reached a local limit and might naively be expected to produce physical tachyons, to an alternate stable state where no physical tachyons exist. Once a tachyonic field such as the Higgs field reaches the minimum of the potential, its quanta are not tachyons any more but rather are ordinary particles such as the Higgs boson.
Properties of the Higgs boson.
Since the Higgs field is scalar, the Higgs boson has no spin. The Higgs boson is also its own antiparticle and is CP-even, and has zero electric and colour charge.
The Minimal Standard Model does not predict the mass of the Higgs boson. If that mass is between 115 and , then the Standard Model can be valid at energy scales all the way up to the Planck scale (1019 GeV). Many theorists expect new physics beyond the Standard Model to emerge at the TeV-scale, based on unsatisfactory properties of the Standard Model. The highest possible mass scale allowed for the Higgs boson (or some other electroweak symmetry breaking mechanism) is 1.4 TeV; beyond this point, the Standard Model becomes inconsistent without such a mechanism, because unitarity is violated in certain scattering processes.
It is also possible, although experimentally difficult, to estimate the mass of the Higgs boson indirectly. In the Standard Model, the Higgs boson has a number of indirect effects; most notably, Higgs loops result in tiny corrections to masses of W and Z bosons. Precision measurements of electroweak parameters, such as the Fermi constant and masses of W/Z bosons, can be used to calculate constraints on the mass of the Higgs. As of July 2011, the precision electroweak measurements tell us that the mass of the Higgs boson is likely to be less than about at 95% confidence level (this upper limit would increase to if the lower bound of from the LEP-2 direct search is allowed for). These indirect constraints rely on the assumption that the Standard Model is correct. It may still be possible to discover a Higgs boson above these masses if it is accompanied by other particles beyond those predicted by the Standard Model.
Production.
If Higgs particle theories are valid, then a Higgs particle can be produced much like other particles that are studied, in a particle collider. This involves accelerating a large number of particles to extremely high energies and extremely close to the speed of light, then allowing them to smash together. Protons and lead ions (the bare nuclei of lead atoms) are used at the LHC. In the extreme energies of these collisions, the desired esoteric particles will occasionally be produced and this can be detected and studied; any absence or difference from theoretical expectations can also be used to improve the theory. The relevant particle theory (in this case the Standard Model) will determine the necessary kinds of collisions and detectors. The Standard Model predicts that Higgs bosons could be formed in a number of ways, although the probability of producing a Higgs boson in any collision is always expected to be very small—for example, only 1 Higgs boson per 10 billion collisions in the Large Hadron Collider. The most common expected processes for Higgs boson production are:
Decay.
Quantum mechanics predicts that if it is possible for a particle to decay into a set of lighter particles, then it will eventually do so. This is also true for the Higgs boson. The likelihood with which this happens depends on a variety of factors including: the difference in mass, the strength of the interactions, etc. Most of these factors are fixed by the Standard Model, except for the mass of the Higgs boson itself. For a Higgs boson with a mass of the SM predicts a mean life time of about .
Since it interacts with all the massive elementary particles of the SM, the Higgs boson has many different processes through which it can decay. Each of these possible processes has its own probability, expressed as the "branching ratio"; the fraction of the total number decays that follows that process. The SM predicts these branching ratios as a function of the Higgs mass (see plot).
One way that the Higgs can decay is by splitting into a fermion–antifermion pair. As general rule, the Higgs is more likely to decay into heavy fermions than light fermions, because the mass of a fermion is proportional to the strength of its interaction with the Higgs. By this logic the most common decay should be into a top–antitop quark pair. However, such a decay is only possible if the Higgs is heavier than ~, twice the mass of the top quark. For a Higgs mass of the SM predicts that the most common decay is into a bottom–antibottom quark pair, which happens 56.1% of the time. The second most common fermion decay at that mass is a tau–antitau pair, which happens only about 6% of the time.
Another possibility is for the Higgs to split into a pair of massive gauge bosons. The most likely possibility is for the Higgs to decay into a pair of W bosons (the light blue line in the plot), which happens about 23.1% of the time for a Higgs boson with a mass of . The W bosons can subsequently decay either into a quark and an antiquark or into a charged lepton and a neutrino. However, the decays of W bosons into quarks are difficult to distinguish from the background, and the decays into leptons cannot be fully reconstructed (because neutrinos are impossible to detect in particle collision experiments). A cleaner signal is given by decay into a pair of Z-bosons (which happens about 2.9% of the time for a Higgs with a mass of ), if each of the bosons subsequently decays into a pair of easy-to-detect charged leptons (electrons or muons).
Decay into massless gauge bosons (i.e., gluons or photons) is also possible, but requires intermediate loop of virtual heavy quarks (top or bottom) or massive gauge bosons. The most common such process is the decay into a pair of gluons through a loop of virtual heavy quarks. This process, which is the reverse of the gluon fusion process mentioned above, happens approximately 8.5% of the time for a Higgs boson with a mass of . Much rarer is the decay into a pair of photons mediated by a loop of W bosons or heavy quarks, which happens only twice for every thousand decays. However, this process is very relevant for experimental searches for the Higgs boson, because the energy and momentum of the photons can be measured very precisely, giving an accurate reconstruction of the mass of the decaying particle.
Alternative models.
The Minimal Standard Model as described above is the simplest known model for the Higgs mechanism with just one Higgs field. However, an extended Higgs sector with additional Higgs particle doublets or triplets is also possible, and many extensions of the Standard Model have this feature. The non-minimal Higgs sector favoured by theory are the two-Higgs-doublet models (2HDM), which predict the existence of a quintet of scalar particles: two CP-even neutral Higgs bosons h0 and H0, a CP-odd neutral Higgs boson A0, and two charged Higgs particles H±. Supersymmetry ("SUSY") also predicts relations between the Higgs-boson masses and the masses of the gauge bosons, and could accommodate a neutral Higgs boson.
The key method to distinguish between these different models involves study of the particles' interactions ("coupling") and exact decay processes ("branching ratios"), which can be measured and tested experimentally in particle collisions. In the Type-I 2HDM model one Higgs doublet couples to up and down quarks, while the second doublet does not couple to quarks. This model has two interesting limits, in which the lightest Higgs couples to just fermions ("gauge-phobic") or just gauge bosons ("fermiophobic"), but not both. In the Type-II 2HDM model, one Higgs doublet only couples to up-type quarks, the other only couples to down-type quarks. The heavily researched Minimal Supersymmetric Standard Model (MSSM) includes a Type-II 2HDM Higgs sector, so it could be disproven by evidence of a Type-I 2HDM Higgs.
In other models the Higgs scalar is a composite particle. For example, in technicolor the role of the Higgs field is played by strongly bound pairs of fermions called techniquarks. Other models, feature pairs of top quarks (see top quark condensate). In yet other models, there is no Higgs field at all and the electroweak symmetry is broken using extra dimensions.
Further theoretical issues and hierarchy problem.
The Standard Model leaves the mass of the Higgs boson as a parameter to be measured, rather than a value to be calculated. This is seen as theoretically unsatisfactory, particularly as quantum corrections (related to interactions with virtual particles) should apparently cause the Higgs particle to have a mass immensely higher than that observed, but at the same time the Standard Model requires a mass of the order of 100 to 1000 GeV to ensure unitarity (in this case, to unitarise longitudinal vector boson scattering). Reconciling these points appears to require explaining why there is an almost-perfect cancellation resulting in the visible mass of ~ 125 GeV, and it is not clear how to do this. Because the weak force is about 1032 times stronger than gravity, and (linked to this) the Higgs boson's mass is so much less than the Planck mass or the grand unification energy, it appears that either there is some underlying connection or reason for these observations which is unknown and not described by the Standard Model, or some unexplained and extremely precise fine-tuning of parameters – however at present neither of these explanations is proven. This is known as a hierarchy problem. More broadly, the hierarchy problem amounts to the worry that a future theory of fundamental particles and interactions should not have excessive fine-tunings or unduly delicate cancellations, and should allow masses of particles such as the Higgs boson to be calculable. The problem is in some ways unique to spin-0 particles (such as the Higgs boson), which can give rise to issues related to quantum corrections that do not affect particles with spin. A number of solutions have been proposed, including supersymmetry, conformal solutions and solutions via extra dimensions such as braneworld models.
There are also issues of quantum triviality, which suggests that it may not be possible to create a consistent quantum field theory involving elementary scalar particles.
Experimental search.
To produce Higgs bosons, two beams of particles are accelerated to very high energies and allowed to collide within a particle detector. Occasionally, although rarely, a Higgs boson will be created fleetingly as part of the collision byproducts. Because the Higgs boson decays very quickly, particle detectors cannot detect it directly. Instead the detectors register all the decay products (the "decay signature") and from the data the decay process is reconstructed. If the observed decay products match a possible decay process (known as a "decay channel") of a Higgs boson, this indicates that a Higgs boson may have been created. In practice, many processes may produce similar decay signatures. Fortunately, the Standard Model precisely predicts the likelihood of each of these, and each known process, occurring. So, if the detector detects more decay signatures consistently matching a Higgs boson than would otherwise be expected if Higgs bosons did not exist, then this would be strong evidence that the Higgs boson exists.
Because Higgs boson production in a particle collision is likely to be very rare (1 in 10 billion at the LHC), and many other possible collision events can have similar decay signatures, the data of hundreds of trillions of collisions needs to be analysed and must "show the same picture" before a conclusion about the existence of the Higgs boson can be reached. To conclude that a new particle has been found, particle physicists require that the statistical analysis of two independent particle detectors each indicate that there is lesser than a one-in-a-million chance that the observed decay signatures are due to just background random Standard Model events—i.e., that the observed number of events is more than 5 standard deviations (sigma) different from that expected if there was no new particle. More collision data allows better confirmation of the physical properties of any new particle observed, and allows physicists to decide whether it is indeed a Higgs boson as described by the Standard Model or some other hypothetical new particle.
To find the Higgs boson, a powerful particle accelerator was needed, because Higgs bosons might not be seen in lower-energy experiments. The collider needed to have a high luminosity in order to ensure enough collisions were seen for conclusions to be drawn. Finally, advanced computing facilities were needed to process the vast amount of data (25 petabytes per year as at 2012) produced by the collisions. For the announcement of 4 July 2012, a new collider known as the Large Hadron Collider was constructed at CERN with a planned eventual collision energy of 14 TeV—over seven times any previous collider—and over 300 trillion (3×1014) LHC proton–proton collisions were analysed by the LHC Computing Grid, the world's largest computing grid (as of 2012), comprising over 170 computing facilities in a worldwide network across 36 countries.
Search prior to 4 July 2012.
The first extensive search for the Higgs boson was conducted at the Large Electron–Positron Collider (LEP) at CERN in the 1990s. At the end of its service in 2000, LEP had found no conclusive evidence for the Higgs. This implied that if the Higgs boson were to exist it would have to be heavier than .
The search continued at Fermilab in the United States, where the Tevatron—the collider that discovered the top quark in 1995—had been upgraded for this purpose. There was no guarantee that the Tevatron would be able to find the Higgs, but it was the only supercollider that was operational since the Large Hadron Collider (LHC) was still under construction and the planned Superconducting Super Collider had been cancelled in 1993 and never completed. The Tevatron was only able to exclude further ranges for the Higgs mass, and was shut down on 30 September 2011 because it no longer could keep up with the LHC. The final analysis of the data excluded the possibility of a Higgs boson with a mass between and . In addition, there was a small (but not significant) excess of events possibly indicating a Higgs boson with a mass between and .
The Large Hadron Collider at CERN in Switzerland, was designed specifically to be able to either confirm or exclude the existence of the Higgs boson. Built in a 27 km tunnel under the ground near Geneva originally inhabited by LEP, it was designed to collide two beams of protons, initially at energies of per beam (7 TeV total), or almost 3.6 times that of the Tevatron, and upgradeable to (14 TeV total) in future. Theory suggested if the Higgs boson existed, collisions at these energy levels should be able to reveal it. As one of the most complicated scientific instruments ever built, its operational readiness was delayed for 14 months by a magnet quench event nine days after its inaugural tests, caused by a faulty electrical connection that damaged over 50 superconducting magnets and contaminated the vacuum system.
Data collection at the LHC finally commenced in March 2010. By December 2011 the two main particle detectors at the LHC, ATLAS and CMS, had narrowed down the mass range where the Higgs could exist to around 116-130 GeV (ATLAS) and 115-127 GeV (CMS). There had also already been a number of promising event excesses that had "evaporated" and proven to be nothing but random fluctuations. However, from around May 2011, both experiments had seen among their results, the slow emergence of a small yet consistent excess of gamma and 4-lepton decay signatures and several other particle decays, all hinting at a new particle at a mass around . By around November 2011, the anomalous data at 125 GeV was becoming "too large to ignore" (although still far from conclusive), and the team leaders at both ATLAS and CMS each privately suspected they might have found the Higgs. On November 28, 2011, at an internal meeting of the two team leaders and the director general of CERN, the latest analyses were discussed outside their teams for the first time, suggesting both ATLAS and CMS might be converging on a possible shared result at 125 GeV, and initial preparations commenced in case of a successful finding. While this information was not known publicly at the time, the narrowing of the possible Higgs range to around 115–130 GeV and the repeated observation of small but consistent event excesses across multiple channels at both ATLAS and CMS in the 124-126 GeV region (described as "tantalising hints" of around 2-3 sigma) were public knowledge with "a lot of interest". It was therefore widely anticipated around the end of 2011, that the LHC would provide sufficient data to either exclude or confirm the finding of a Higgs boson by the end of 2012, when their 2012 collision data (with slightly higher 8 TeV collision energy) had been examined.
Discovery of candidate boson at CERN.
On 22 June 2012 CERN announced an upcoming seminar covering tentative findings for 2012, and shortly afterwards (from around 1 July 2012 according to an analysis of the spreading rumour in social media) rumours began to spread in the media that this would include a major announcement, but it was unclear whether this would be a stronger signal or a formal discovery. Speculation escalated to a "fevered" pitch when reports emerged that Peter Higgs, who proposed the particle, was to be attending the seminar, and that "five leading physicists" had been invited – generally believed to signify the five living 1964 authors – with Higgs, Englert, Guralnik, Hagen attending and Kibble confirming his invitation (Brout having died in 2011).
On 4 July 2012 both of the CERN experiments announced they had independently made the same discovery: CMS of a previously unknown boson with mass 125.3 ± 0.6 GeV/"c"2 and ATLAS of a boson with mass 126.0 ± 0.6 GeV/"c"2. Using the combined analysis of two interaction types (known as 'channels'), both experiments independently reached a local significance of 5 sigma — implying that the probability of getting at least as strong a result by chance alone is less than 1 in 3 million. When additional channels were taken into account, the CMS significance was reduced to 4.9 sigma.
The two teams had been working 'blinded' from each other from around late 2011 or early 2012, meaning they did not discuss their results with each other, providing additional certainty that any common finding was genuine validation of a particle. This level of evidence, confirmed independently by two separate teams and experiments, meets the formal level of proof required to announce a confirmed discovery.
On 31 July 2012, the ATLAS collaboration presented additional data analysis on the "observation of a new particle", including data from a third channel, which improved the significance to 5.9 sigma (1 in 588 million chance of obtaining at least as strong evidence by random background effects alone) and mass ,
 and CMS improved the significance to 5-sigma and mass .
The new particle tested as a possible Higgs boson.
Following the 2012 discovery, it was still unconfirmed whether or not the 125 GeV/"c"2 particle was a Higgs boson. On one hand, observations remained consistent with the observed particle being the Standard Model Higgs boson, and the particle decayed into at least some of the predicted channels. Moreover, the production rates and branching ratios for the observed channels broadly matched the predictions by the Standard Model within the experimental uncertainties. However, the experimental uncertainties currently still left room for alternative explanations, meaning an announcement of the discovery of a Higgs boson would have been premature. To allow more opportunity for data collection, the LHC's proposed 2012 shutdown and 2013–14 upgrade were postponed by 7 weeks into 2013.
In November 2012, in a conference in Kyoto researchers said evidence gathered since July was falling into line with the basic Standard Model more than its alternatives, with a range of results for several interactions matching that theory's predictions. Physicist Matt Strassler highlighted "considerable" evidence that the new particle is not a pseudoscalar negative parity particle (consistent with this required finding for a Higgs boson), "evaporation" or lack of increased significance for previous hints of non-Standard Model findings, expected Standard Model interactions with W and Z bosons, absence of "significant new implications" for or against supersymmetry, and in general no significant deviations to date from the results expected of a Standard Model Higgs boson. However some kinds of extensions to the Standard Model would also show very similar results; so commentators noted that based on other particles that are still being understood long after their discovery, it may take years to be sure, and decades to fully understand the particle that has been found.
These findings meant that as of January 2013, scientists were very sure they had found an unknown particle of mass ~ 125 GeV/"c"2, and had not been misled by experimental error or a chance result. They were also sure, from initial observations, that the new particle was some kind of boson. The behaviours and properties of the particle, so far as examined since July 2012, also seemed quite close to the behaviours expected of a Higgs boson. Even so, it could still have been a Higgs boson or some other unknown boson, since future tests could show behaviours that do not match a Higgs boson, so as of December 2012 CERN still only stated that the new particle was "consistent with" the Higgs boson, and scientists did not yet positively say it was the Higgs boson. Despite this, in late 2012, widespread media reports announced (incorrectly) that a Higgs boson had been confirmed during the year.
In January 2013, CERN director-general Rolf-Dieter Heuer stated that based on data analysis to date, an answer could be possible 'towards' mid-2013, and the deputy chair of physics at Brookhaven National Laboratory stated in February 2013 that a "definitive" answer might require "another few years" after the collider's 2015 restart. In early March 2013, CERN Research Director Sergio Bertolucci stated that confirming spin-0 was the major remaining requirement to determine whether the particle is at least some kind of Higgs boson.
Preliminary confirmation of existence and current status.
On 14 March 2013 CERN confirmed that:
This also makes the particle the first elementary scalar particle to be discovered in nature.
Examples of tests used to validate whether the 125 GeV particle is a Higgs boson:
Public discussion.
Naming.
Names used by physicists.
The name most strongly associated with the particle and field is the Higgs boson and Higgs field. For some time the particle was known by a combination of its PRL author names (including at times Anderson), for example the Brout–Englert–Higgs particle, the Anderson-Higgs particle, or the Englert–Brout–Higgs–Guralnik–Hagen–Kibble mechanism, and these are still used at times. Fueled in part by the issue of recognition and a potential shared Nobel Prize, the most appropriate name is still occasionally a topic of debate as at 2012. (Higgs himself prefers to call the particle either by an acronym of all those involved, or "the scalar boson", or "the so-called Higgs particle".)
A considerable amount has been written on how Higgs' name came to be exclusively used. Two main explanations are offered.
Nickname.
The Higgs boson is often referred to as the "God particle" in popular media outside the scientific community. The nickname comes from the title of the 1993 book on the Higgs boson and particle physics - by Nobel Physics prizewinner and Fermilab director Leon Lederman. Lederman wrote it in the context of failing US government support for the Superconducting Super Collider, a part-constructed titanic competitor to the Large Hadron Collider with planned collision energies of that was championed by Lederman since its 1983 inception and shut down in 1993. The book sought in part to promote awareness of the significance and need for such a project in the face of its possible loss of funding. Lederman, a leading researcher in the field, wanted to title his book “The Goddamn Particle: If the Universe is the Answer, What is the Question?” But his editor decided that the title was too controversial and convinced Lederman to change the title to “The God Particle: If the Universe is the Answer, What is the Question?”
While media use of this term may have contributed to wider awareness and interest, many scientists feel the name is inappropriate since it is sensational hyperbole and misleads readers; the particle also has nothing to do with God, leaves open numerous questions in fundamental physics, and does not explain the ultimate origin of the universe. Higgs, an atheist, was reported to be displeased and stated in a 2008 interview that he found it "embarrassing" because it was "the kind of misuse... which I think might offend some people". Science writer Ian Sample stated in his 2010 book on the search that the nickname is "universally hate[d]" by physicists and perhaps the "worst derided" in the history of physics, but that (according to Lederman) the publisher rejected all titles mentioning "Higgs" as unimaginative and too unknown.
Lederman begins with a review of the long human search for knowledge, and explains that his tongue-in-cheek title draws an analogy between the impact of the Higgs field on the fundamental symmetries at the Big Bang, and the apparent chaos of structures, particles, forces and interactions that resulted and shaped our present universe, with the biblical story of Babel in which the primordial single language of early Genesis was fragmented into many disparate languages and cultures.
Lederman asks whether the Higgs boson was added just to perplex and confound those seeking knowledge of the universe, and whether physicists will be confounded by it as recounted in that story, or ultimately surmount the challenge and understand "how beautiful is the universe [God has] made".
Other proposals.
A renaming competition by British newspaper "The Guardian" in 2009 resulted in their science correspondent choosing the name "the champagne bottle boson" as the best submission: "The bottom of a champagne bottle is in the shape of the Higgs potential and is often used as an illustration in physics lectures. So it's not an embarrassingly grandiose name, it is memorable, and [it] has some physics connection too."
The name "Higgson" was suggested as well, in an opinion piece in the Institute of Physics' online publication "physicsworld.com".
Media explanations and analogies.
There has been considerable public discussion of analogies and explanations for the Higgs particle and how the field creates mass, including coverage of explanatory attempts in their own right and a competition in 1993 for the best popular explanation by then-UK Minister for Science Sir William Waldegrave and articles in newspapers worldwide.
An educational collaboration involving an LHC physicist and a High School Teachers at CERN educator suggests that dispersion of light – responsible for the rainbow and dispersive prism – is a useful analogy for the Higgs field's symmetry breaking and mass-causing effect.
Matt Strassler uses electric fields as an analogy:
A similar explanation was offered by "The Guardian":
The Higgs field's effect on particles was famously described by physicist David Miller as akin to a room full of political party workers spread evenly throughout a room: the crowd gravitates to and slows down famous people but does not slow down others. He also drew attention to well-known effects in solid state physics where an electron's effective mass can be much greater than usual in the presence of a crystal lattice.
Analogies based on drag effects, including analogies of "syrup" or "molasses" are also well known, but can be somewhat misleading since they may be understood (incorrectly) as saying that the Higgs field simply resists some particles' motion but not others' – a simple resistive effect could also conflict with Newton's third law.
Recognition and awards.
There has been considerable discussion of how to allocate the credit if the Higgs boson is proven, made more pointed as a Nobel prize had been expected, and the very wide basis of people entitled to consideration. These include a range of theoreticians who made the Higgs mechanism theory possible, the theoreticians of the 1964 PRL papers (including Higgs himself), the theoreticians who derived from these, a working electroweak theory and the Standard Model itself, and also the experimentalists at CERN and other institutions who made possible the proof of the Higgs field and boson in reality. The Nobel prize has a limit of 3 persons to share an award, and some possible winners are already prize holders for other work, or are deceased (the prize is only awarded to persons in their lifetime). Existing prizes for works relating to the Higgs field, boson, or mechanism include:
Additionally Physical Review Letters' 50-year review (2008) recognized the 1964 PRL symmetry breaking papers and Weinberg's 1967 paper "A model of Leptons" (the most cited paper in particle physics, as of 2012) "milestone Letters".
Following reported observation of the Higgs-like particle in July 2012, several Indian media outlets reported on the supposed neglect of credit to Indian physicist Satyendra Nath Bose after whose work in the 1920s the class of particles "bosons" is named (although physicists have described Bose's connection to the discovery as tenuous).
Technical aspects and mathematical formulation.
In the Standard Model, the Higgs field is a four-component scalar field that forms a complex doublet of the weak isospin SU(2) symmetry:
\left(
\phi^1 + i\phi^2 \\ \phi^0+i\phi^3
\right)\;,
while the field has charge +1/2 under the weak hypercharge U(1) symmetry (in the convention where the electric charge, "Q", the weak isospin, "I3", and the weak hypercharge, "Y", are related by "Q = I3 + Y").
The Higgs part of the Lagrangian is
where formula_2 and formula_3 are the gauge bosons of the SU(2) and U(1) symmetries, formula_4 and formula_5 their respective coupling constants, formula_6 (where formula_7 are the Pauli matrices) a complete set generators of the SU(2) symmetry, and formula_8 and formula_9, so that the ground state breaks the SU(2) symmetry (see figure). The ground state of the Higgs field (the bottom of the potential) is degenerate with different ground states related to each other by a SU(2) gauge transformation. It is always possible to pick a gauge such that in the ground state formula_10. The expectation value of formula_11 in the ground state (the vacuum expectation value or vev) is then formula_12, where formula_13. The measured value of this parameter is ~. It has units of mass, and is the only free parameter of the Standard Model that is not a dimensionless number. Quadratic terms in formula_14 and formula_15 arise, which give masses to the "W" and "Z" bosons:
with their ratio determining the Weinberg angle, formula_16, and leave a massless U(1) photon, formula_17.
The quarks and the leptons interact with the Higgs field through Yukawa interaction terms:
+\lambda_u^{ij}\frac{\phi^1-i\phi^2}{\sqrt{2}}\overline d_L^i u_R^j\\
&-\lambda_d^{ij}\frac{\phi^0+i\phi^3}{\sqrt{2}}\overline d_L^i d_R^j
-\lambda_d^{ij}\frac{\phi^1+i\phi^2}{\sqrt{2}}\overline u_L^i d_R^j\\
&-\lambda_e^{ij}\frac{\phi^0+i\phi^3}{\sqrt{2}}\overline e_L^i e_R^j
-\lambda_e^{ij}\frac{\phi^1+i\phi^2}{\sqrt{2}}\overline \nu_L^i e_R^j
where formula_18 are left-handed and right-handed quarks and leptons of the "i"th generation, formula_19are matrices of Yukawa couplings where h.c. denotes the hermitian conjugate terms. In the symmetry breaking ground state, only the terms containing formula_11 remain, giving rise to mass terms for the fermions. Rotating the quark and lepton fields to the basis where the matrices of Yukawa couplings are diagonal, one gets
where the masses of the fermions are formula_21, and formula_22 denote the eigenvalues of the Yukawa matrices.

</doc>
<doc id="34143255" url="https://en.wikipedia.org/wiki?curid=34143255" title="Higgs bundle">
Higgs bundle

In mathematics, a Higgs bundle is a pair (E,φ) consisting of a holomorphic vector bundle "E" and a Higgs field φ, a holomorphic 1-form taking values in End("E") such that φ ∧ φ = 0. 
Such pairs were introduced by , who named the field φ after Peter Higgs because of an analogy with Higgs bosons. The term 'Higgs bundle', and the condition φ ∧ φ = 0 (which is vacuous in Hitchin's original set-up on Riemann surfaces) was introduced later by Simpson.

</doc>
<doc id="644550" url="https://en.wikipedia.org/wiki?curid=644550" title="Higgs mechanism">
Higgs mechanism

In the Standard Model of particle physics, the Higgs mechanism is essential to explain the generation mechanism of the property "mass" for gauge bosons. Without the Higgs mechanism, or some other effect like it, all bosons (a type of fundamental particle) would be massless, but measurements show that the W+, W−, and Z bosons actually have relatively large masses of around 80 GeV/c2. The Higgs field resolves this conundrum. The simplest description of the mechanism adds a quantum field (the Higgs field) that permeates all space, to the Standard Model. Below some extremely high temperature, the field causes spontaneous symmetry breaking during interactions. The breaking of symmetry triggers the Higgs mechanism, causing the bosons it interacts with to have mass. In the Standard Model, the phrase "Higgs mechanism" refers specifically to the generation of masses for the W±, and Z weak gauge bosons through electroweak symmetry breaking. The Large Hadron Collider at CERN announced results consistent with the Higgs particle on March 14, 2013, making it extremely likely that the field, or one like it, exists, and explaining how the Higgs mechanism takes place in nature.
The mechanism was proposed in 1962 by Philip Warren Anderson, following work in the late 1950s on symmetry breaking in superconductivity and a 1960 paper by Yoichiro Nambu that discussed its application within particle physics. A theory able to finally explain mass generation without "breaking" gauge theory was published almost simultaneously by three independent groups in 1964: by Robert Brout and François Englert; by Peter Higgs; and by Gerald Guralnik, C. R. Hagen, and Tom Kibble. The Higgs mechanism is therefore also called the Brout–Englert–Higgs mechanism or Englert–Brout–Higgs–Guralnik–Hagen–Kibble mechanism, Anderson–Higgs mechanism, Anderson–Higgs-Kibble mechanism, Higgs–Kibble mechanism by Abdus Salam and ABEGHHK'tH mechanism [for Anderson, Brout, Englert, Guralnik, Hagen, Higgs, Kibble and 't Hooft] by Peter Higgs. 
On October 8, 2013, following the discovery at CERN's Large Hadron Collider of a new particle that appeared to be the long-sought Higgs boson predicted by the theory, it was announced that Peter Higgs and François Englert had been awarded the 2013 Nobel Prize in Physics (Englert's co-author Robert Brout had died in 2011 and the Nobel Prize is not usually awarded posthumously).
Standard model.
The Higgs mechanism was incorporated into modern particle physics by Steven Weinberg and Abdus Salam, and is an essential part of the standard model.
In the standard model, at temperatures high enough that electroweak symmetry is unbroken, all elementary particles are massless. At a critical temperature the Higgs field becomes tachyonic, the symmetry is spontaneously broken by condensation, and the W and Z bosons acquire masses. ("EWSB", electroweak symmetry breaking, is an abbreviation used for this.)
Fermions, such as the leptons and quarks in the Standard Model, can also acquire mass as a result of their interaction with the Higgs field, but not in the same way as the gauge bosons.
Structure of the Higgs field.
In the standard model, the Higgs field is an SU(2) doublet, a complex scalar with four real components (or equivalently with two complex components).
Its (weak hypercharge) U(1) charge is 1. That means that it transforms as a spinor under SU(2). Under U(1) rotations, it is multiplied by a phase, which thus mixes the real and imaginary parts of the complex spinor into each other—so this is "not the same" as two complex spinors mixing under U(1) (which would have eight real components between them), but instead is the spinor representation of the group U(2).
The Higgs field, through the interactions specified (summarized, represented, or even simulated) by its potential, induces spontaneous breaking of three out of the four generators ("directions") of the gauge group SU(2) × U(1): three out of its four components would ordinarily amount to Goldstone bosons, if they were not coupled to gauge fields.
However, after symmetry breaking, these three of the four degrees of freedom in the Higgs field mix with the three W and Z bosons (, and ), and are only observable as spin components of these weak bosons, which are now massive; while the one remaining degree of freedom becomes the Higgs boson—a new scalar particle.
The photon as the part that remains massless.
The gauge group of the electroweak part of the standard model is SU(2) × U(1). The group SU(2) is the group of all 2-by-2 unitary matrices with unit determinant; all the orthonormal changes of coordinates in a complex two dimensional vector space.
Rotating the coordinates so that the second basis vector points in the direction of the Higgs boson makes the vacuum expectation value of "H" the spinor (0, "v"). The generators for rotations about the "x", "y", and "z" axes are by half the Pauli matrices σ"x", σ"y", and σ"z", so that a rotation of angle θ about the "z"-axis takes the vacuum to
While the "T"x and "T"y generators mix up the top and bottom components of the spinor, the "T"z rotations only multiply each by opposite phases. This phase can be undone by a U(1) rotation of angle θ. Consequently, under both an SU(2) "T"z-rotation and a U(1) rotation by an amount θ, "the vacuum is invariant".
This combination of generators
defines the unbroken part of the gauge group, where "Q" is the electric charge, "T"z is the generator of rotations around the z-axis in the SU(2) and "Y" is the hypercharge generator of the U(1). This combination of generators (a "z" rotation in the SU(2) and a simultaneous U(1) rotation by half the angle) preserves the vacuum, and defines the unbroken gauge group in the standard model, namely "the electric charge" group. The part of the gauge field in this direction stays massless, and amounts to the physical photon.
Consequences for fermions.
In spite of the introduction of spontaneous symmetry breaking, the mass terms oppose the chiral gauge invariance. For these fields the mass terms should always be replaced by a gauge-invariant "Higgs" mechanism. One possibility is some kind of "Yukawa coupling" (see below) between the fermion field ψ and the Higgs field Φ, with unknown couplings "G"ψ, which after symmetry breaking (more precisely: after expansion of the Lagrange density around a suitable ground state) again results in the original mass terms, which are now, however (i.e. by introduction of the Higgs field) written in a gauge-invariant way. The Lagrange density for the "Yukawa" interaction of a fermion field ψ and the Higgs field Φ is
where again the gauge field "A" only enters "D"μ (i.e., it is only indirectly visible). The quantities γμ are the Dirac matrices, and "G"ψ is the already-mentioned "Yukawa" coupling parameter. Already now the mass-generation follows the same principle as above, namely from the existence of a finite expectation value formula_4, as described above. Again, this is crucial for the existence of the property "mass".
History of research.
Background.
Spontaneous symmetry breaking offered a framework to introduce bosons into relativistic quantum field theories. However, according to Goldstone's theorem, these bosons should be massless. The only observed particles which could be approximately interpreted as Goldstone bosons were the pions, which Yoichiro Nambu related to chiral symmetry breaking.
A similar problem arises with Yang–Mills theory (also known as non-abelian gauge theory), which predicts massless spin-1 gauge bosons. Massless weakly interacting gauge bosons lead to long-range forces, which are only observed for electromagnetism and the corresponding massless photon. Gauge theories of the weak force needed a way to describe massive gauge bosons in order to be consistent.
Discovery.
The mechanism was proposed in 1962 by Philip Warren Anderson, who discussed its consequences for particle physics but did not work out an explicit relativistic model. The relativistic model was developed in 1964 by three independent groups – Robert Brout and François Englert; Peter Higgs; and Gerald Guralnik, Carl Richard Hagen, and Tom Kibble. Slightly later, in 1965, but independently from the other publications the mechanism was also proposed by Alexander Migdal and Alexander Polyakov, at that time Soviet undergraduate students. However, the paper was delayed by the Editorial Office of JETP, and was published only in 1966.
The mechanism is closely analogous to phenomena previously discovered by Yoichiro Nambu involving the "vacuum structure" of quantum fields in superconductivity. A similar but distinct effect (involving an affine realization of what is now recognized as the Higgs field), known as the Stueckelberg mechanism, had previously been studied by Ernst Stueckelberg.
These physicists discovered that when a gauge theory is combined with an additional field that spontaneously breaks the symmetry group, the gauge bosons can consistently acquire a nonzero mass. In spite of the large values involved (see below) this permits a gauge theory description of the weak force, which was independently developed by Steven Weinberg and Abdus Salam in 1967. Higgs's original article presenting the model was rejected by Physics Letters. When revising the article before resubmitting it to Physical Review Letters, he added a sentence at the end, mentioning that it implies the existence of one or more new, massive scalar bosons, which do not form complete representations of the symmetry group; these are the Higgs bosons.
The three papers by Brout and Englert; Higgs; and Guralnik, Hagen, and Kibble were each recognized as "milestone letters" by "Physical Review Letters" in 2008. While each of these seminal papers took similar approaches, the contributions and differences among the 1964 PRL symmetry breaking papers are noteworthy. All six physicists were jointly awarded the 2010 J. J. Sakurai Prize for Theoretical Particle Physics for this work.
Benjamin W. Lee is often credited with first naming the "Higgs-like" mechanism, although there is debate around when this first occurred. One of the first times the "Higgs" name appeared in print was in 1972 when Gerardus 't Hooft and Martinus J. G. Veltman referred to it as the "Higgs–Kibble mechanism" in their Nobel winning paper.
Examples.
The Higgs mechanism occurs whenever a charged field has a vacuum expectation value. In the nonrelativistic context, this is the Landau model of a charged Bose–Einstein condensate, also known as a superconductor. In the relativistic condensate, the condensate is a scalar field, and is relativistically invariant.
Landau model.
The Higgs mechanism is a type of superconductivity which occurs in the vacuum. It occurs when all of space is filled with a sea of particles which are charged, or, in field language, when a charged field has a nonzero vacuum expectation value. Interaction with the quantum fluid filling the space prevents certain forces from propagating over long distances (as it does in a superconducting medium; e.g., in the Ginzburg–Landau theory).
A superconductor expels all magnetic fields from its interior, a phenomenon known as the Meissner effect. This was mysterious for a long time, because it implies that electromagnetic forces somehow become short-range inside the superconductor. Contrast this with the behavior of an ordinary metal. In a metal, the conductivity shields electric fields by rearranging charges on the surface until the total field cancels in the interior. But magnetic fields can penetrate to any distance, and if a magnetic monopole (an isolated magnetic pole) is surrounded by a metal the field can escape without collimating into a string. In a superconductor, however, electric charges move with no dissipation, and this allows for permanent surface currents, not just surface charges. When magnetic fields are introduced at the boundary of a superconductor, they produce surface currents which exactly neutralize them. The Meissner effect is due to currents in a thin surface layer, whose thickness, the London penetration depth, can be calculated from a simple model
(the Ginzburg–Landau theory).
This simple model treats superconductivity as a charged Bose–Einstein condensate. Suppose that a superconductor contains bosons with charge "q". The wavefunction of the bosons can be described by introducing a quantum field, ψ, which obeys the Schrödinger equation as a field equation (in units where the reduced Planck constant, "ħ", is set to 1):
The operator ψ("x") annihilates a boson at the point "x", while its adjoint ψ† creates a new boson at the same point. The wavefunction of the Bose–Einstein condensate is then the expectation value ψ of ψ("x"), which is a classical function that obeys the same equation. The interpretation of the expectation value is that it is the phase that one should give to a newly created boson so that it will coherently superpose with all the other bosons already in the condensate.
When there is a charged condensate, the electromagnetic interactions are screened. To see this, consider the effect of a gauge transformation on the field. A gauge transformation rotates the phase of the condensate by an amount which changes from point to point, and shifts the vector potential by a gradient:
When there is no condensate, this transformation only changes the definition of the phase of ψ at every point. But when there is a condensate, the phase of the condensate defines a preferred choice of phase.
The condensate wave function can be written as
where ρ is real amplitude, which determines the local density of the condensate. If the condensate were neutral, the flow would be along the gradients of θ, the direction in which the phase of the Schrödinger field changes. If the phase θ changes slowly, the flow is slow and has very little energy. But now θ can be made equal to zero just by making a gauge transformation to rotate the phase of the field.
The energy of slow changes of phase can be calculated from the Schrödinger kinetic energy,
and taking the density of the condensate ρ to be constant,
Fixing the choice of gauge so that the condensate has the same phase everywhere, the electromagnetic field energy has an extra term,
When this term is present, electromagnetic interactions become short-ranged. Every field mode, no matter how long the wavelength, oscillates with a nonzero frequency. The lowest frequency can be read off from the energy of a long wavelength A mode,
where now the nonabelian field A is contained in "D" and in the tensor components formula_12 and formula_13 (the relation between A and those components is well-known from the Yang–Mills theory).
It is exactly analogous to the Abelian Higgs model. Now the field φ is in a representation of the gauge group, and the gauge covariant derivative is defined by the rate of change of the field minus the rate of change from parallel transport using the gauge field A as a connection.
Again, the expectation value of Φ defines a preferred gauge where the vacuum is constant, and fixing this gauge, fluctuations in the gauge field A come with a nonzero energy cost.
Depending on the representation of the scalar field, not every gauge field acquires a mass. A simple example is in the renormalizable version of an early electroweak model due to Julian Schwinger. In this model, the gauge group is SO(3) (or SU(2) − there are no spinor representations in the model), and the gauge invariance is broken down to U(1) or SO(2) at long distances. To make a consistent renormalizable version using the Higgs mechanism, introduce a scalar field φa which transforms as a vector (a triplet) of SO(3). If this field has a vacuum expectation value, it points in some direction in field space. Without loss of generality, one can choose the "z"-axis in field space to be the direction that φ is pointing, and then the vacuum expectation value of φ is (0, 0, "A"), where "A" is a constant with dimensions of mass (formula_15).
Rotations around the "z"-axis form a U(1) subgroup of SO(3) which preserves the vacuum expectation value of φ, and this is the unbroken gauge group. Rotations around the "x" and "y"-axis do not preserve the vacuum, and the components of the SO(3) gauge field which generate these rotations become massive vector mesons. There are two massive W mesons in the Schwinger model, with a mass set by the mass scale "A", and one massless U(1) gauge boson, similar to the photon.
The Schwinger model predicts magnetic monopoles at the electroweak unification scale, and does not predict the Z meson. It doesn't break electroweak symmetry properly as in nature. But historically, a model similar to this (but not using the Higgs mechanism) was the first in which the weak force and the electromagnetic force were unified.
Affine Higgs mechanism.
Ernst Stueckelberg discovered a version of the Higgs mechanism by analyzing the theory of quantum electrodynamics with a massive photon. Effectively, Stueckelberg's model is a limit of the regular Mexican hat Abelian Higgs model, where the vacuum expectation value "H" goes to infinity and the charge of the Higgs field goes to zero in such a way that their product stays fixed. The mass of the Higgs boson is proportional to "H", so the Higgs boson becomes infinitely massive and decouples, so is not present in the discussion. The vector meson mass, however, equals to the product "eH", and stays finite.
The interpretation is that when a U(1) gauge field does not require quantized charges, it is possible to keep only the angular part of the Higgs oscillations, and discard the radial part. The angular part of the Higgs field θ has the following gauge transformation law:
The gauge covariant derivative for the angle (which is actually gauge invariant) is:
In order to keep θ fluctuations finite and nonzero in this limit, θ should be rescaled by H, so that its kinetic term in the action stays normalized. The action for the theta field is read off from the Mexican hat action by substituting formula_19.
since "eH" is the gauge boson mass. By making a gauge transformation to set θ = 0, the gauge freedom in the action is eliminated, and the action becomes that of a massive vector field:
To have arbitrarily small charges requires that the U(1) is not the circle of unit complex numbers under multiplication, but the real numbers R under addition, which is only different in the global topology. Such a U(1) group is "non-compact". The field θ transforms as an affine representation of the gauge group. Among the allowed gauge groups, only non-compact U(1) admits affine representations, and the U(1) of electromagnetism is experimentally known to be compact, since charge quantization holds to extremely high accuracy.
The Higgs condensate in this model has infinitesimal charge, so interactions with the Higgs boson do not violate charge conservation. The theory of quantum electrodynamics with a massive photon is still a renormalizable theory, one in which electric charge is still conserved, but magnetic monopoles are not allowed. For nonabelian gauge theory, there is no affine limit, and the Higgs oscillations cannot be too much more massive than the vectors.

</doc>
<doc id="715691" url="https://en.wikipedia.org/wiki?curid=715691" title="Higgsino">
Higgsino

In particle physics, a Higgsino, symbol , is the theoretical superpartner of the Higgs boson, as predicted by supersymmetry. The Higgsino is a Dirac fermion and that is a weak isodoublet with hypercharge half under the Standard Model gauge symmetries. After electroweak symmetry breaking the Higgsino becomes a pair of neutral Majorana fermions called neutralinos and a charged Dirac fermion called a chargino (plus and minus). These states finally mix with the neutralinos (photino and zino) and chargino (charged wino plus and minus) to form the predicted particles which are four neutralinos and two charginos (plus and minus each). Such a linear combination of the Higgsino, bino and wino makes up the lightest supersymmetric particle (LSP), which is a particle physics candidate for the dark matter of the universe. In order to be such a candidate, it must be neutral (i.e. a neutralino rather than chargino).
In natural scenarios of SUSY, top squarks, bottom squarks, gluinos, and higgsino-enriched neutralinos and charginos are expected to be relatively light, enhancing their production cross sections. Higgsino searches have been performed by both the ATLAS and CMS experiments at the Large Hadron Collider at CERN, where physicists have searched for the direct electroweak pair production of Higgsinos. As of March 2014, no experimental evidence for Higgsinos has been reported.
Higgsino Mass.
If dark matter is composed only of Higgsinos then the Higgsino mass is 1.1 TeV. On other hand if dark matter has multi-components then the Higgsino mass depends on the relevant multiverse distribution functions making the mass of the Higgsino lighter.

</doc>
<doc id="20598932" url="https://en.wikipedia.org/wiki?curid=20598932" title="Hilbert space">
Hilbert space

The mathematical concept of a Hilbert space, named after David Hilbert, generalizes the notion of Euclidean space. It extends the methods of vector algebra and calculus from the two-dimensional Euclidean plane and three-dimensional space to spaces with any finite or infinite number of dimensions. A Hilbert space is an abstract vector space possessing the structure of an inner product that allows length and angle to be measured. Furthermore, Hilbert spaces are complete: there are enough limits in the space to allow the techniques of calculus to be used.
Hilbert spaces arise naturally and frequently in mathematics and physics, typically as infinite-dimensional function spaces. The earliest Hilbert spaces were studied from this point of view in the first decade of the 20th century by David Hilbert, Erhard Schmidt, and Frigyes Riesz. They are indispensable tools in the theories of partial differential equations, quantum mechanics, Fourier analysis (which includes applications to signal processing and heat transfer)—and ergodic theory, which forms the mathematical underpinning of thermodynamics. John von Neumann coined the term "Hilbert space" for the abstract concept that underlies many of these diverse applications. The success of Hilbert space methods ushered in a very fruitful era for functional analysis. Apart from the classical Euclidean spaces, examples of Hilbert spaces include spaces of square-integrable functions, spaces of sequences, Sobolev spaces consisting of generalized functions, and Hardy spaces of holomorphic functions.
Geometric intuition plays an important role in many aspects of Hilbert space theory. Exact analogs of the Pythagorean theorem and parallelogram law hold in a Hilbert space. At a deeper level, perpendicular projection onto a subspace (the analog of "dropping the altitude" of a triangle) plays a significant role in optimization problems and other aspects of the theory. An element of a Hilbert space can be uniquely specified by its coordinates with respect to a set of coordinate axes (an orthonormal basis), in analogy with Cartesian coordinates in the plane. When that set of axes is countably infinite, this means that the Hilbert space can also usefully be thought of in terms of infinite sequences that are square-summable. Linear operators on a Hilbert space are likewise fairly concrete objects: in good cases, they are simply transformations that stretch the space by different factors in mutually perpendicular directions in a sense that is made precise by the study of their spectrum.
Definition and illustration.
Motivating example: Euclidean space.
One of the most familiar examples of a Hilbert space is the Euclidean space consisting of three-dimensional vectors, denoted by , and equipped with the dot product. The dot product takes two vectors and , and produces a real number . If and are represented in Cartesian coordinates, then the dot product is defined by
The dot product satisfies the properties:
An operation on pairs of vectors that, like the dot product, satisfies these three properties is known as a (real) inner product. A vector space equipped with such an inner product is known as a (real) inner product space. Every finite-dimensional inner product space is also a Hilbert space. The basic feature of the dot product that connects it with Euclidean geometry is that it is related to both the length (or norm) of a vector, denoted , and to the angle between two vectors and by means of the formula
Multivariable calculus in Euclidean space relies on the ability to compute limits, and to have useful criteria for concluding that limits exist. A mathematical series
consisting of vectors in is absolutely convergent provided that the sum of the lengths converges as an ordinary series of real numbers:
Just as with a series of scalars, a series of vectors that converges absolutely also converges to some limit vector in the Euclidean space, in the sense that
This property expresses the "completeness" of Euclidean space: that a series that converges absolutely also converges in the ordinary sense.
Hilbert spaces are often taken over the complex numbers. The complex plane denoted by ℂ is equipped with a notion of magnitude, the complex modulus which is defined as the square root of the product of with its complex conjugate:
If is a decomposition of into its real and imaginary parts, then the modulus is the usual Euclidean two-dimensional length:
The inner product of a pair of complex numbers and is the product of with the complex conjugate of :
This is complex-valued. The real part of gives the usual two-dimensional Euclidean dot product.
A second example is the space whose elements are pairs of complex numbers . Then the inner product of with another such vector is given by
The real part of is then the four-dimensional Euclidean dot product. This inner product is "Hermitian" symmetric, which means that the result of interchanging and is the complex conjugate:
Definition.
A Hilbert space "H" is a real or complex inner product space that is also a complete metric space with respect to the distance function induced by the inner product. To say that "H" is a complex inner product space means that "H" is a complex vector space on which there is an inner product formula_11 associating a complex number to each pair of elements "x","y" of "H" that satisfies the following properties:
It follows from properties 1 and 2 that a complex inner product is antilinear in its second argument, meaning that
A real inner product space is defined in the same way, except that "H" is a real vector space and the inner product takes real values. Such an inner product will be bilinear: that is, linear in each argument.
The norm is the real-valued function
and the distance "d" between two points "x","y" in "H" is defined in terms of the norm by
That this function is a distance function means (1) that it is symmetric in "x" and "y", (2) that the distance between "x" and itself is zero, and otherwise the distance between "x" and "y" must be positive, and (3) that the triangle inequality holds, meaning that the length of one leg of a triangle "xyz" cannot exceed the sum of the lengths of the other two legs:
This last property is ultimately a consequence of the more fundamental Cauchy–Schwarz inequality, which asserts
with equality if and only if "x" and "y" are linearly dependent.
Relative to a distance function defined in this way, any inner product space is a metric space, and sometimes is known as a pre-Hilbert space. Any pre-Hilbert space that is additionally also a complete space is a Hilbert space. Completeness is expressed using a form of the Cauchy criterion for sequences in "H": a pre-Hilbert space "H" is complete if every Cauchy sequence converges with respect to this norm to an element in the space. Completeness can be characterized by the following equivalent condition: if a series of vectors formula_20 converges absolutely in the sense that
then the series converges in "H", in the sense that the partial sums converge to an element of "H".
As a complete normed space, Hilbert spaces are by definition also Banach spaces. As such they are topological vector spaces, in which topological notions like the openness and closedness of subsets are well-defined. Of special importance is the notion of a closed linear subspace of a Hilbert space that, with the inner product induced by restriction, is also complete (being a closed set in a complete metric space) and therefore a Hilbert space in its own right.
Second example: sequence spaces.
The sequence space "ℓ"2 consists of all infinite sequences z = ("z""1","z"2...) of complex numbers such that the series
converges. The inner product on "ℓ"2 is defined by
with the latter series converging as a consequence of the Cauchy–Schwarz inequality.
Completeness of the space holds provided that whenever a series of elements from "ℓ"2 converges absolutely (in norm), then it converges to an element of "ℓ"2. The proof is basic in mathematical analysis, and permits mathematical series of elements of the space to be manipulated with the same ease as series of complex numbers (or vectors in a finite-dimensional Euclidean space).
History.
Prior to the development of Hilbert spaces, other generalizations of Euclidean spaces were known to mathematicians and physicists. In particular, the idea of an abstract linear space had gained some traction towards the end of the 19th century: this is a space whose elements can be added together and multiplied by scalars (such as real or complex numbers) without necessarily identifying these elements with "geometric" vectors, such as position and momentum vectors in physical systems. Other objects studied by mathematicians at the turn of the 20th century, in particular spaces of sequences (including series) and spaces of functions, can naturally be thought of as linear spaces. Functions, for instance, can be added together or multiplied by constant scalars, and these operations obey the algebraic laws satisfied by addition and scalar multiplication of spatial vectors.
In the first decade of the 20th century, parallel developments led to the introduction of Hilbert spaces. The first of these was the observation, which arose during David Hilbert and Erhard Schmidt's study of integral equations, that two square-integrable real-valued functions and on an interval have an "inner product"
which has many of the familiar properties of the Euclidean dot product. In particular, the idea of an orthogonal family of functions has meaning. Schmidt exploited the similarity of this inner product with the usual dot product to prove an analog of the spectral decomposition for an operator of the form
where is a continuous function symmetric in and . The resulting eigenfunction expansion expresses the function as a series of the form
where the functions are orthogonal in the sense that for all . The individual terms in this series are sometimes referred to as elementary product solutions. However, there are eigenfunction expansions that fail to converge in a suitable sense to a square-integrable function: the missing ingredient, which ensures convergence, is completeness.
The second development was the Lebesgue integral, an alternative to the Riemann integral introduced by Henri Lebesgue in 1904. The Lebesgue integral made it possible to integrate a much broader class of functions. In 1907, Frigyes Riesz and Ernst Sigismund Fischer independently proved that the space of square Lebesgue-integrable functions is a complete metric space. As a consequence of the interplay between geometry and completeness, the 19th century results of Joseph Fourier, Friedrich Bessel and Marc-Antoine Parseval on trigonometric series easily carried over to these more general spaces, resulting in a geometrical and analytical apparatus now usually known as the Riesz–Fischer theorem.
Further basic results were proved in the early 20th century. For example, the Riesz representation theorem was independently established by Maurice Fréchet and Frigyes Riesz in 1907. John von Neumann coined the term "abstract Hilbert space" in his work on unbounded Hermitian operators. Although other mathematicians such as Hermann Weyl and Norbert Wiener had already studied particular Hilbert spaces in great detail, often from a physically motivated point of view, von Neumann gave the first complete and axiomatic treatment of them. Von Neumann later used them in his seminal work on the foundations of quantum mechanics, and in his continued work with Eugene Wigner. The name "Hilbert space" was soon adopted by others, for example by Hermann Weyl in his book on quantum mechanics and the theory of groups.
The significance of the concept of a Hilbert space was underlined with the realization that it offers one of the best mathematical formulations of quantum mechanics. In short, the states of a quantum mechanical system are vectors in a certain Hilbert space, the observables are hermitian operators on that space, the symmetries of the system are unitary operators, and measurements are orthogonal projections. The relation between quantum mechanical symmetries and unitary operators provided an impetus for the development of the unitary representation theory of groups, initiated in the 1928 work of Hermann Weyl. On the other hand, in the early 1930s it became clear that classical mechanics can be described in terms of Hilbert space (Koopman–von Neumann classical mechanics) and that certain properties of classical dynamical systems can be analyzed using Hilbert space techniques in the framework of ergodic theory.
The algebra of observables in quantum mechanics is naturally an algebra of operators defined on a Hilbert space, according to Werner Heisenberg's matrix mechanics formulation of quantum theory. Von Neumann began investigating operator algebras in the 1930s, as rings of operators on a Hilbert space. The kind of algebras studied by von Neumann and his contemporaries are now known as von Neumann algebras. In the 1940s, Israel Gelfand, Mark Naimark and Irving Segal gave a definition of a kind of operator algebras called C*-algebras that on the one hand made no reference to an underlying Hilbert space, and on the other extrapolated many of the useful features of the operator algebras that had previously been studied. The spectral theorem for self-adjoint operators in particular that underlies much of the existing Hilbert space theory was generalized to C*-algebras. These techniques are now basic in abstract harmonic analysis and representation theory.
Examples.
Lebesgue spaces.
Lebesgue spaces are function spaces associated to measure spaces ("X", "M", "μ"), where "X" is a set, "M" is a σ-algebra of subsets of "X", and "μ" is a countably additive measure on "M". Let "L"2("X", μ) be the space of those complex-valued measurable functions on "X" for which the Lebesgue integral of the square of the absolute value of the function is finite, i.e., for a function "f" in "L"2("X",μ),
and where functions are identified if and only if they differ only on a set of measure zero.
The inner product of functions "f" and "g" in "L"2("X", μ) is then defined as
For "f" and "g" in "L"2, this integral exists because of the Cauchy–Schwarz inequality, and defines an inner product on the space. Equipped with this inner product, "L"2 is in fact complete. The Lebesgue integral is essential to ensure completeness: on domains of real numbers, for instance, not enough functions are Riemann integrable.
The Lebesgue spaces appear in many natural settings. The spaces "L"2(R) and "L"2([0,1]) of square-integrable functions with respect to the Lebesgue measure on the real line and unit interval, respectively, are natural domains on which to define the Fourier transform and Fourier series. In other situations, the measure may be something other than the ordinary Lebesgue measure on the real line. For instance, if "w" is any positive measurable function, the space of all measurable functions "f" on the interval [0, 1] satisfying
is called the weighted "L"2 space "L"([0,1]), and "w" is called the weight function. The inner product is defined by
The weighted space "L"([0,1]) is identical with the Hilbert space "L"2([0,1],μ) where the measure μ of a Lebesgue-measurable set "A" is defined by
Weighted "L"2 spaces like this are frequently used to study orthogonal polynomials, because different families of orthogonal polynomials are orthogonal with respect to different weighting functions.
Sobolev spaces.
Sobolev spaces, denoted by "H""s" or , are Hilbert spaces. These are a special kind of function space in which differentiation may be performed, but that (unlike other Banach spaces such as the Hölder spaces) support the structure of an inner product. Because differentiation is permitted, Sobolev spaces are a convenient setting for the theory of partial differential equations. They also form the basis of the theory of direct methods in the calculus of variations.
For "s" a non-negative integer and , the Sobolev space "H""s"(Ω) contains L2 functions whose weak derivatives of order up to "s" are also L2. The inner product in "H"s(Ω) is
where the dot indicates the dot product in the Euclidean space of partial derivatives of each order. Sobolev spaces can also be defined when "s" is not an integer.
Sobolev spaces are also studied from the point of view of spectral theory, relying more specifically on the Hilbert space structure. If Ω is a suitable domain, then one can define the Sobolev space "H""s"(Ω) as the space of Bessel potentials; roughly,
Here Δ is the Laplacian and (1 − Δ)−"s"/2 is understood in terms of the spectral mapping theorem. Apart from providing a workable definition of Sobolev spaces for non-integer "s", this definition also has particularly desirable properties under the Fourier transform that make it ideal for the study of pseudodifferential operators. Using these methods on a compact Riemannian manifold, one can obtain for instance the Hodge decomposition, which is the basis of Hodge theory.
Spaces of holomorphic functions.
The Hardy spaces are function spaces, arising in complex analysis and harmonic analysis, whose elements are certain holomorphic functions in a complex domain. Let "U" denote the unit disc in the complex plane. Then the Hardy space "H"2("U") is defined as the space of holomorphic functions "f" on "U" such that the means
remain bounded for . The norm on this Hardy space is defined by
Hardy spaces in the disc are related to Fourier series. A function "f" is in "H"2("U") if and only if
where
Thus "H"2("U") consists of those functions that are L2 on the circle, and whose negative frequency Fourier coefficients vanish.
The Bergman spaces are another family of Hilbert spaces of holomorphic functions. Let "D" be a bounded open set in the complex plane (or a higher-dimensional complex space) and let "L"2,"h"("D") be the space of holomorphic functions "f" in "D" that are also in "L"2("D") in the sense that
where the integral is taken with respect to the Lebesgue measure in "D". Clearly "L"2, "h"("D") is a subspace of "L"2("D"); in fact, it is a closed subspace, and so a Hilbert space in its own right. This is a consequence of the estimate, valid on compact subsets "K" of "D", that
which in turn follows from Cauchy's integral formula. Thus convergence of a sequence of holomorphic functions in "L"2("D") implies also compact convergence, and so the limit function is also holomorphic. Another consequence of this inequality is that the linear functional that evaluates a function "f" at a point of "D" is actually continuous on "L"2,"h"("D"). The Riesz representation theorem implies that the evaluation functional can be represented as an element of "L"2,"h"("D"). Thus, for every "z" ∈ "D", there is a function η"z" ∈ "L"2,"h"("D") such that
for all "f" ∈ "L"2,"h"("D"). The integrand
is known as the Bergman kernel of "D". This integral kernel satisfies a reproducing property
A Bergman space is an example of a reproducing kernel Hilbert space, which is a Hilbert space of functions along with a kernel "K"(ζ,"z") that verifies a reproducing property analogous to this one. The Hardy space "H"2("D") also admits a reproducing kernel, known as the Szegő kernel. Reproducing kernels are common in other areas of mathematics as well. For instance, in harmonic analysis the Poisson kernel is a reproducing kernel for the Hilbert space of square-integrable harmonic functions in the unit ball. That the latter is a Hilbert space at all is a consequence of the mean value theorem for harmonic functions.
Applications.
Many of the applications of Hilbert spaces exploit the fact that Hilbert spaces support generalizations of simple geometric concepts like projection and change of basis from their usual finite dimensional setting. In particular, the spectral theory of continuous self-adjoint linear operators on a Hilbert space generalizes the usual spectral decomposition of a matrix, and this often plays a major role in applications of the theory to other areas of mathematics and physics.
Sturm–Liouville theory.
In the theory of ordinary differential equations, spectral methods on a suitable Hilbert space are used to study the behavior of eigenvalues and eigenfunctions of differential equations. For example, the Sturm–Liouville problem arises in the study of the harmonics of waves in a violin string or a drum, and is a central problem in ordinary differential equations. The problem is a differential equation of the form
for an unknown function "y" on an interval ["a","b"], satisfying general homogeneous Robin boundary conditions
The functions "p", "q", and "w" are given in advance, and the problem is to find the function "y" and constants λ for which the equation has a solution. The problem only has solutions for certain values of λ, called eigenvalues of the system, and this is a consequence of the spectral theorem for compact operators applied to the integral operator defined by the Green's function for the system. Furthermore, another consequence of this general result is that the eigenvalues λ of the system can be arranged in an increasing sequence tending to infinity.
Partial differential equations.
Hilbert spaces form a basic tool in the study of partial differential equations. For many classes of partial differential equations, such as linear elliptic equations, it is possible to consider a generalized solution (known as a weak solution) by enlarging the class of functions. Many weak formulations involve the class of Sobolev functions, which is a Hilbert space. A suitable weak formulation reduces to a geometrical problem the analytic problem of finding a solution or, often what is more important, showing that a solution exists and is unique for given boundary data. For linear elliptic equations, one geometrical result that ensures unique solvability for a large class of problems is the Lax–Milgram theorem. This strategy forms the rudiment of the Galerkin method (a finite element method) for numerical solution of partial differential equations.
A typical example is the Poisson equation with Dirichlet boundary conditions in a bounded domain Ω in R2. The weak formulation consists of finding a function "u" such that, for all continuously differentiable functions "v" in Ω vanishing on the boundary:
This can be recast in terms of the Hilbert space "H"(Ω) consisting of functions "u" such that "u", along with its weak partial derivatives, are square integrable on Ω, and vanish on the boundary. The question then reduces to finding "u" in this space such that for all "v" in this space
where "a" is a continuous bilinear form, and "b" is a continuous linear functional, given respectively by
Since the Poisson equation is elliptic, it follows from Poincaré's inequality that the bilinear form "a" is coercive. The Lax–Milgram theorem then ensures the existence and uniqueness of solutions of this equation.
Hilbert spaces allow for many elliptic partial differential equations to be formulated in a similar way, and the Lax–Milgram theorem is then a basic tool in their analysis. With suitable modifications, similar techniques can be applied to parabolic partial differential equations and certain hyperbolic partial differential equations.
Ergodic theory.
The field of ergodic theory is the study of the long-term behavior of chaotic dynamical systems. The protypical case of a field that ergodic theory applies to is thermodynamics, in which—though the microscopic state of a system is extremely complicated (it is impossible to understand the ensemble of individual collisions between particles of matter)—the average behavior over sufficiently long time intervals is tractable. The laws of thermodynamics are assertions about such average behavior. In particular, one formulation of the zeroth law of thermodynamics asserts that over sufficiently long timescales, the only functionally independent measurement that one can make of a thermodynamic system in equilibrium is its total energy, in the form of temperature.
An ergodic dynamical system is one for which, apart from the energy—measured by the Hamiltonian—there are no other functionally independent conserved quantities on the phase space. More explicitly, suppose that the energy "E" is fixed, and let Ω"E" be the subset of the phase space consisting of all states of energy "E" (an energy surface), and let "T""t" denote the evolution operator on the phase space. The dynamical system is ergodic if there are no continuous non-constant functions on Ω"E" such that
for all "w" on Ω"E" and all time "t". Liouville's theorem implies that there exists a measure μ on the energy surface that is invariant under the time translation. As a result, time translation is a unitary transformation of the Hilbert space "L"2(Ω"E",μ) consisting of square-integrable functions on the energy surface Ω"E" with respect to the inner product
The von Neumann mean ergodic theorem states the following:
For an ergodic system, the fixed set of the time evolution consists only of the constant functions, so the ergodic theorem implies the following: for any function "f" ∈ "L"2(Ω"E",μ),
That is, the long time average of an observable "f" is equal to its expectation value over an energy surface.
Fourier analysis.
One of the basic goals of Fourier analysis is to decompose a function into a (possibly infinite) linear combination of given basis functions: the associated Fourier series. The classical Fourier series associated to a function "f" defined on the interval [0, 1] is a series of the form
where
The example of adding up the first few terms in a Fourier series for a sawtooth function is shown in the figure. The basis functions are sine waves with wavelengths λ/"n" ("n"=integer) shorter than the wavelength λ of the sawtooth itself (except for "n"=1, the "fundamental" wave). All basis functions have nodes at the nodes of the sawtooth, but all but the fundamental have additional nodes. The oscillation of the summed terms about the sawtooth is called the Gibbs phenomenon.
A significant problem in classical Fourier series asks in what sense the Fourier series converges, if at all, to the function "f". Hilbert space methods provide one possible answer to this question. The functions "en"(θ) = e2πi"n"θ form an orthogonal basis of the Hilbert space "L"2([0,1]). Consequently, any square-integrable function can be expressed as a series
and, moreover, this series converges in the Hilbert space sense (that is, in the "L"2 mean).
The problem can also be studied from the abstract point of view: every Hilbert space has an orthonormal basis, and every element of the Hilbert space can be written in a unique way as a sum of multiples of these basis elements. The coefficients appearing on these basis elements are sometimes known abstractly as the Fourier coefficients of the element of the space. The abstraction is especially useful when it is more natural to use different basis functions for a space such as "L"2([0,1]). In many circumstances, it is desirable not to decompose a function into trigonometric functions, but rather into orthogonal polynomials or wavelets for instance, and in higher dimensions into spherical harmonics.
For instance, if "e""n" are any orthonormal basis functions of "L"2[0,1], then a given function in "L"2[0,1] can be approximated as a finite linear combination
The coefficients {"a""j"} are selected to make the magnitude of the difference ||||2 as small as possible. Geometrically, the best approximation is the orthogonal projection of "ƒ" onto the subspace consisting of all linear combinations of the {"e""j"}, and can be calculated by
That this formula minimizes the difference ||||2 is a consequence of Bessel's inequality and Parseval's formula.
In various applications to physical problems, a function can be decomposed into physically meaningful eigenfunctions of a differential operator (typically the Laplace operator): this forms the foundation for the spectral study of functions, in reference to the spectrum of the differential operator. A concrete physical application involves the problem of hearing the shape of a drum: given the fundamental modes of vibration that a drumhead is capable of producing, can one infer the shape of the drum itself? The mathematical formulation of this question involves the Dirichlet eigenvalues of the Laplace equation in the plane, that represent the fundamental modes of vibration in direct analogy with the integers that represent the fundamental modes of vibration of the violin string.
Spectral theory also underlies certain aspects of the Fourier transform of a function. Whereas Fourier analysis decomposes a function defined on a compact set into the discrete spectrum of the Laplacian (which corresponds to the vibrations of a violin string or drum), the Fourier transform of a function is the decomposition of a function defined on all of Euclidean space into its components in the continuous spectrum of the Laplacian. The Fourier transformation is also geometrical, in a sense made precise by the Plancherel theorem, that asserts that it is an isometry of one Hilbert space (the "time domain") with another (the "frequency domain"). This isometry property of the Fourier transformation is a recurring theme in abstract harmonic analysis, as evidenced for instance by the Plancherel theorem for spherical functions occurring in noncommutative harmonic analysis.
Quantum mechanics.
In the mathematically rigorous formulation of quantum mechanics, developed by John von Neumann, the possible states (more precisely, the pure states) of a quantum mechanical system are represented by unit vectors (called "state vectors") residing in a complex separable Hilbert space, known as the state space, well defined up to a complex number of norm 1 (the phase factor). In other words, the possible states are points in the projectivization of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system; for example, the position and momentum states for a single non-relativistic spin zero particle is the space of all square-integrable functions, while the states for the spin of a single proton are unit elements of the two-dimensional complex Hilbert space of spinors. Each observable is represented by a self-adjoint linear operator acting on the state space. Each eigenstate of an observable corresponds to an eigenvector of the operator, and the associated eigenvalue corresponds to the value of the observable in that eigenstate.
The inner product between two state vectors is a complex number known as a probability amplitude. During an ideal measurement of a quantum mechanical system, the probability that a system collapses from a given initial state to a particular eigenstate is given by the square of the absolute value of the probability amplitudes between the initial and final states. The possible results of a measurement are the eigenvalues of the operator—which explains the choice of self-adjoint operators, for all the eigenvalues must be real. The probability distribution of an observable in a given state can be found by computing the spectral decomposition of the corresponding operator.
For a general system, states are typically not pure, but instead are represented as statistical mixtures of pure states, or mixed states, given by density matrices: self-adjoint operators of trace one on a Hilbert space. Moreover, for general quantum mechanical systems, the effects of a single measurement can influence other parts of a system in a manner that is described instead by a positive operator valued measure. Thus the structure both of the states and observables in the general theory is considerably more complicated than the idealization for pure states.
Properties.
Pythagorean identity.
Two vectors "u" and "v" in a Hilbert space "H" are orthogonal when formula_57 = 0. The notation for this is . More generally, when "S" is a subset in "H", the notation means that "u" is orthogonal to every element from "S".<br>
When "u" and "v" are orthogonal, one has
By induction on "n", this is extended to any family "u"1...,"un" of "n" orthogonal vectors,
Whereas the Pythagorean identity as stated is valid in any inner product space, completeness is required for the extension of the Pythagorean identity to series. A series Σ "uk" of "orthogonal" vectors converges in "H"  if and only if the series of squares of norms converges, and
Furthermore, the sum of a series of orthogonal vectors is independent of the order in which it is taken.
Parallelogram identity and polarization.
By definition, every Hilbert space is also a Banach space. Furthermore, in every Hilbert space the following parallelogram identity holds:
Conversely, every Banach space in which the parallelogram identity holds is a Hilbert space, and the inner product is uniquely determined by the norm by the polarization identity. For real Hilbert spaces, the polarization identity is
For complex Hilbert spaces, it is
The parallelogram law implies that any Hilbert space is a uniformly convex Banach space.
Best approximation.
This subsection employs the Hilbert projection theorem. If "C" is a non-empty closed convex subset of a Hilbert space "H" and "x" a point in "H", there exists a unique point "y" ∈ "C" that minimizes the distance between "x" and points in "C",
This is equivalent to saying that there is a point with minimal norm in the translated convex set "D" = . The proof consists in showing that every minimizing sequence ("dn") ⊂ "D" is Cauchy (using the parallelogram identity) hence converges (using completeness) to a point in "D" that has minimal norm. More generally, this holds in any uniformly convex Banach space.
When this result is applied to a closed subspace "F" of "H", it can be shown that the point "y" ∈ "F" closest to "x" is characterized by
This point "y" is the "orthogonal projection" of "x" onto "F", and the mapping "PF" : is linear (see Orthogonal complements and projections). This result is especially significant in applied mathematics, especially numerical analysis, where it forms the basis of least squares methods.
In particular, when "F" is not equal to "H", one can find a non-zero vector "v" orthogonal to "F" (select "x" not in "F" and "v" = . A very useful criterion is obtained by applying this observation to the closed subspace "F" generated by a subset "S" of "H".
Duality.
The dual space "H*" is the space of all continuous linear functions from the space "H" into the base field. It carries a natural norm, defined by
This norm satisfies the parallelogram law, and so the dual space is also an inner product space. The dual space is also complete, and so it is a Hilbert space in its own right.
The Riesz representation theorem affords a convenient description of the dual. To every element "u" of "H", there is a unique element φ"u" of "H*", defined by
The mapping formula_68 is an antilinear mapping from "H" to "H*". The Riesz representation theorem states that this mapping is an antilinear isomorphism. Thus to every element "φ" of the dual "H*" there exists one and only one "u"φ in "H" such that
for all "x" ∈ "H". The inner product on the dual space "H*" satisfies
The reversal of order on the right-hand side restores linearity in φ from the antilinearity of "u"φ. In the real case, the antilinear isomorphism from "H" to its dual is actually an isomorphism, and so real Hilbert spaces are naturally isomorphic to their own duals.
The representing vector "u"φ is obtained in the following way. When "φ" ≠ 0, the kernel "F" = Ker(φ) is a closed vector subspace of "H", not equal to "H", hence there exists a non-zero vector "v" orthogonal to "F". The vector "u" is a suitable scalar multiple "λv" of "v". The requirement that φ("v") = ⟨"v", "u"⟩ yields
This correspondence "φ" ↔ "u" is exploited by the bra–ket notation popular in physics. It is common in physics to assume that the inner product, denoted by ⟨"x"|"y"⟩, is linear on the right,
The result ⟨"x"|"y"⟩ can be seen as the action of the linear functional ⟨"x"| (the "bra") on the vector  |"y"⟩ (the "ket").
The Riesz representation theorem relies fundamentally not just on the presence of an inner product, but also on the completeness of the space. In fact, the theorem implies that the topological dual of any inner product space can be identified with its completion. An immediate consequence of the Riesz representation theorem is also that a Hilbert space "H" is reflexive, meaning that the natural map from "H" into its double dual space is an isomorphism.
Weakly convergent sequences.
In a Hilbert space "H", a sequence {"x""n"} is weakly convergent to a vector "x" ∈ "H" when
for every .
For example, any orthonormal sequence {"f""n"} converges weakly to 0, as a consequence of Bessel's inequality. Every weakly convergent sequence {"x""n"} is bounded, by the uniform boundedness principle.
Conversely, every bounded sequence in a Hilbert space admits weakly convergent subsequences (Alaoglu's theorem). This fact may be used to prove minimization results for continuous convex functionals, in the same way that the Bolzano–Weierstrass theorem is used for continuous functions on R"d". Among several variants, one simple statement is as follows:
This fact (and its various generalizations) are fundamental for direct methods in the calculus of variations. Minimization results for convex functionals are also a direct consequence of the slightly more abstract fact that closed bounded convex subsets in a Hilbert space "H" are weakly compact, since "H" is reflexive. The existence of weakly convergent subsequences is a special case of the Eberlein–Šmulian theorem.
Banach space properties.
Any general property of Banach spaces continues to hold for Hilbert spaces. The open mapping theorem states that a continuous surjective linear transformation from one Banach space to another is an open mapping meaning that it sends open sets to open sets. A corollary is the bounded inverse theorem, that a continuous and bijective linear function from one Banach space to another is an isomorphism (that is, a continuous linear map whose inverse is also continuous). This theorem is considerably simpler to prove in the case of Hilbert spaces than in general Banach spaces. The open mapping theorem is equivalent to the closed graph theorem, which asserts that a function from one Banach space to another is continuous if and only if its graph is a closed set. In the case of Hilbert spaces, this is basic in the study of unbounded operators (see closed operator).
The (geometrical) Hahn–Banach theorem asserts that a closed convex set can be separated from any point outside it by means of a hyperplane of the Hilbert space. This is an immediate consequence of the best approximation property: if "y" is the element of a closed convex set "F" closest to "x", then the separating hyperplane is the plane perpendicular to the segment "xy" passing through its midpoint.
Operators on Hilbert spaces.
Bounded operators.
The continuous linear operators "A" : "H"1 → "H"2 from a Hilbert space "H"1 to a second Hilbert space "H"2 are "bounded" in the sense that they map bounded sets to bounded sets. Conversely, if an operator is bounded, then it is continuous. The space of such bounded linear operators has a norm, the operator norm given by
The sum and the composite of two bounded linear operators is again bounded and linear. For "y" in "H"2, the map that sends "x" ∈ "H"1 to ⟨"Ax", "y"⟩ is linear and continuous, and according to the Riesz representation theorem can therefore be represented in the form
for some vector "A*" "y" in "H"1. This defines another bounded linear operator "A*": "H"2 → "H"1, the adjoint of "A". One can see that .
The set B("H") of all bounded linear operators on "H", together with the addition and composition operations, the norm and the adjoint operation, is a C*-algebra, which is a type of operator algebra.
An element "A"  of B("H") is called "self-adjoint" or "Hermitian" if "A*"= "A". If "A"  is Hermitian and 0 for every "x", then "A"  is called "non-negative", written "A" ≥ 0; if equality holds only when "x" = 0, then "A"  is called "positive". The set of self adjoint operators admits a partial order, in which "A" ≥ "B" if "A" − "B" ≥ 0. If "A"  has the form "B*" "B"  for some "B", then "A" is non-negative; if "B" is invertible, then "A"  is positive. A converse is also true in the sense that, for a non-negative operator "A", there exists a unique non-negative square root "B" such that
In a sense made precise by the spectral theorem, self-adjoint operators can usefully be thought of as operators that are "real". An element "A" of B("H") is called "normal" if "A*" "A" = "A" "A*". Normal operators decompose into the sum of a self-adjoint operators and an imaginary multiple of a self adjoint operator
that commute with each other. Normal operators can also usefully be thought of in terms of their real and imaginary parts.
An element "U"  of B("H") is called unitary if "U"  is invertible and its inverse is given by "U*". This can also be expressed by requiring that "U"  be onto for all "x" and "y" in "H". The unitary operators form a group under composition, which is the isometry group of "H".
An element of B("H") is compact if it sends bounded sets to relatively compact sets. Equivalently, a bounded operator "T" is compact if, for any bounded sequence {"xk"}, the sequence {"Txk"} has a convergent subsequence. Many integral operators are compact, and in fact define a special class of operators known as Hilbert–Schmidt operators that are especially important in the study of integral equations. Fredholm operators differ from a compact operator by a multiple of the identity, and are equivalently characterized as operators with a finite dimensional kernel and cokernel. The index of a Fredholm operator "T" is defined by
The index is homotopy invariant, and plays a deep role in differential geometry via the Atiyah–Singer index theorem.
Unbounded operators.
Unbounded operators are also tractable in Hilbert spaces, and have important applications to quantum mechanics. An unbounded operator "T" on a Hilbert space "H" is defined as a linear operator whose domain "D"("T") is a linear subspace of "H". Often the domain "D"("T") is a dense subspace of "H", in which case "T" is known as a densely defined operator.
The adjoint of a densely defined unbounded operator is defined in essentially the same manner as for bounded operators. Self-adjoint unbounded operators play the role of the "observables" in the mathematical formulation of quantum mechanics. Examples of self-adjoint unbounded operators on the Hilbert space "L"2(R) are:
These correspond to the momentum and position observables, respectively. Note that neither "A" nor "B" is defined on all of "H", since in the case of "A" the derivative need not exist, and in the case of "B" the product function need not be square integrable. In both cases, the set of possible arguments form dense subspaces of "L"2(R).
Constructions.
Direct sums.
Two Hilbert spaces "H"1 and "H"2 can be combined into another Hilbert space, called the (orthogonal) direct sum, and denoted
consisting of the set of all ordered pairs ("x"1, "x"2) where , , and inner product defined by
More generally, if "H""i" is a family of Hilbert spaces indexed by , then the direct sum of the "H""i", denoted
consists of the set of all indexed families
in the Cartesian product of the "H""i" such that
The inner product is defined by
Each of the "H""i" is included as a closed subspace in the direct sum of all of the "H""i". Moreover, the "H""i" are pairwise orthogonal. Conversely, if there is a system of closed subspaces, "V""i", , in a Hilbert space "H", that are pairwise orthogonal and whose union is dense in "H", then "H" is canonically isomorphic to the direct sum of "V""i". In this case, "H" is called the internal direct sum of the "V""i". A direct sum (internal or external) is also equipped with a family of orthogonal projections "E""i" onto the "i"th direct summand "H"i. These projections are bounded, self-adjoint, idempotent operators that satisfy the orthogonality condition
The spectral theorem for compact self-adjoint operators on a Hilbert space "H" states that "H" splits into an orthogonal direct sum of the eigenspaces of an operator, and also gives an explicit decomposition of the operator as a sum of projections onto the eigenspaces. The direct sum of Hilbert spaces also appears in quantum mechanics as the Fock space of a system containing a variable number of particles, where each Hilbert space in the direct sum corresponds to an additional degree of freedom for the quantum mechanical system. In representation theory, the Peter–Weyl theorem guarantees that any unitary representation of a compact group on a Hilbert space splits as the direct sum of finite-dimensional representations.
Tensor products.
If and , then one defines an inner product on the (ordinary) tensor product as follows. On simple tensors, let
This formula then extends by sesquilinearity to an inner product on . The Hilbertian tensor product of and , sometimes denoted by formula_89, is the Hilbert space obtained by completing for the metric associated to this inner product.
An example is provided by the Hilbert space . The Hilbertian tensor product of two copies of is isometrically and linearly isomorphic to the space of square-integrable functions on the square . This isomorphism sends a simple tensor to the function
on the square.
This example is typical in the following sense. Associated to every simple tensor product is the rank one operator from to that maps a given formula_91 as
This mapping defined on simple tensors extends to a linear identification between and the space of finite rank operators from to . This extends to a linear isometry of the Hilbertian tensor product formula_89 with the Hilbert space of Hilbert–Schmidt operators from to .
Orthonormal bases.
The notion of an orthonormal basis from linear algebra generalizes over to the case of Hilbert spaces. In a Hilbert space "H", an orthonormal basis is a family {"e""k"} of elements of "H" satisfying the conditions:
A system of vectors satisfying the first two conditions basis is called an orthonormal system or an orthonormal set (or an orthonormal sequence if "B" is countable). Such a system is always linearly independent. Completeness of an orthonormal system of vectors of a Hilbert space can be equivalently restated as:
This is related to the fact that the only vector orthogonal to a dense linear subspace is the zero vector, for if "S" is any orthonormal set and "v" is orthogonal to "S", then "v" is orthogonal to the closure of the linear span of "S", which is the whole space.
Examples of orthonormal bases include:
In the infinite-dimensional case, an orthonormal basis will not be a basis in the sense of linear algebra; to distinguish the two, the latter basis is also called a Hamel basis. That the span of the basis vectors is dense implies that every vector in the space can be written as the sum of an infinite series, and the orthogonality implies that this decomposition is unique.
Sequence spaces.
The space "ℓ" 2 of square-summable sequences of complex numbers is the set of infinite sequences
of complex numbers such that
This space has an orthonormal basis:
More generally, if "B" is any set, then one can form a Hilbert space of sequences with index set "B", defined by
The summation over "B" is here defined by
the supremum being taken over all finite subsets of "B". It follows that, for this sum to be finite, every element of "ℓ" 2("B") has only countably many nonzero terms. This space becomes a Hilbert space with the inner product
for all "x" and "y" in "ℓ" 2("B"). Here the sum also has only countably many nonzero terms, and is unconditionally convergent by the Cauchy–Schwarz inequality.
An orthonormal basis of "ℓ" 2("B") is indexed by the set "B", given by
Bessel's inequality and Parseval's formula.
Let be a finite orthonormal system in "H". For an arbitrary vector "x" in "H", let
Then  = for every "k" = . It follows that is orthogonal to each "f""k", hence is orthogonal to "y". Using the Pythagorean identity twice, it follows that
Let , be an arbitrary orthonormal system in "H". Applying the preceding inequality to every finite subset "J" of "I" gives the "Bessel inequality"
(according to the definition of the sum of an arbitrary family of non-negative real numbers).
Geometrically, Bessel's inequality implies that the orthogonal projection of "x" onto the linear subspace spanned by the "f""i" has norm that does not exceed that of "x". In two dimensions, this is the assertion that the length of the leg of a right triangle may not exceed the length of the hypotenuse.
Bessel's inequality is a stepping stone to the more powerful Parseval identity, which governs the case when Bessel's inequality is actually an equality. If {"e""k"}"k" ∈ "B" is an orthonormal basis of "H", then every element "x" of "H" may be written as
Even if "B" is uncountable, Bessel's inequality guarantees that the expression is well-defined and consists only of countably many nonzero terms. This sum is called the "Fourier expansion" of "x", and the individual coefficients ⟨"x","e""k"⟩ are the "Fourier coefficients" of "x". Parseval's formula is then
Conversely, if {"e""k"} is an orthonormal set such that Parseval's identity holds for every "x", then {"e""k"} is an orthonormal basis.
Hilbert dimension.
As a consequence of Zorn's lemma, "every" Hilbert space admits an orthonormal basis; furthermore, any two orthonormal bases of the same space have the same cardinality, called the Hilbert dimension of the space. For instance, since "ℓ"2("B") has an orthonormal basis indexed by "B", its Hilbert dimension is the cardinality of "B" (which may be a finite integer, or a countable or uncountable cardinal number).
As a consequence of Parseval's identity, if {"e""k"}"k" ∈ "B" is an orthonormal basis of "H", then the map  ℓ2("B") defined by is an isometric isomorphism of Hilbert spaces: it is a bijective linear mapping such that
for all "x" and "y" in "H". The cardinal number of "B" is the Hilbert dimension of "H". Thus every Hilbert space is isometrically isomorphic to a sequence space  ℓ2("B") for some set "B".
Separable spaces.
A Hilbert space is separable if and only if it admits a countable orthonormal basis. All infinite-dimensional separable Hilbert spaces are therefore isometrically isomorphic to "ℓ"2.
In the past, Hilbert spaces were often required to be separable as part of the definition. Most spaces used in physics are separable, and since these are all isomorphic to each other, one often refers to any infinite-dimensional separable Hilbert space as ""the" Hilbert space" or just "Hilbert space". Even in quantum field theory, most of the Hilbert spaces are in fact separable, as stipulated by the Wightman axioms. However, it is sometimes argued that non-separable Hilbert spaces are also important in quantum field theory, roughly because the systems in the theory possess an infinite number of degrees of freedom and any infinite Hilbert tensor product (of spaces of dimension greater than one) is non-separable. For instance, a bosonic field can be naturally thought of as an element of a tensor product whose factors represent harmonic oscillators at each point of space. From this perspective, the natural state space of a boson might seem to be a non-separable space. However, it is only a small separable subspace of the full tensor product that can contain physically meaningful fields (on which the observables can be defined). Another non-separable Hilbert space models the state of an infinite collection of particles in an unbounded region of space. An orthonormal basis of the space is indexed by the density of the particles, a continuous parameter, and since the set of possible densities is uncountable, the basis is not countable.
Orthogonal complements and projections.
If "S" is a subset of a Hilbert space "H", the set of vectors orthogonal to "S" is defined by
"S"⊥ is a closed subspace of "H" (can be proved easily using the linearity and continuity of the inner product) and so forms itself a Hilbert space. If "V" is a closed subspace of "H", then "V"⊥ is called the "orthogonal complement" of "V". In fact, every "x" in "H" can then be written uniquely as "x" = "v" + "w", with "v" in "V" and "w" in "V"⊥. Therefore, "H" is the internal Hilbert direct sum of "V" and "V"⊥.
The linear operator P"V" : "H" → "H" that maps "x" to "v" is called the "orthogonal projection" onto "V". There is a natural one-to-one correspondence between the set of all closed subspaces of "H" and the set of all bounded self-adjoint operators "P" such that "P"2 = "P". Specifically,
This provides the geometrical interpretation of "PV"("x"): it is the best approximation to "x" by elements of "V".
Projections "PU" and "PV" are called mutually orthogonal if "P""U""P""V" = 0. This is equivalent to "U" and "V" being orthogonal as subspaces of "H". The sum of the two projections "P""U" and "P""V" is a projection only if "U" and "V" are orthogonal to each other, and in that case "P""U" + "P""V" = "P""U"+"V". The composite "P""U""P""V" is generally not a projection; in fact, the composite is a projection if and only if the two projections commute, and in that case "P""U""P""V" = "P""U"∩"V".
By restricting the codomain to the Hilbert space "V", the orthogonal projection "P""V" gives rise to a projection mapping π: "H" → "V"; it is the adjoint of the inclusion mapping
meaning that
for all "x" ∈ "V" and "y" ∈ "H".
The operator norm of the orthogonal projection "P""V" onto a non-zero closed subspace "V" is equal to one:
Every closed subspace "V" of a Hilbert space is therefore the image of an operator "P" of norm one such that "P"2 = "P". The property of possessing appropriate projection operators characterizes Hilbert spaces:
While this result characterizes the metric structure of a Hilbert space, the structure of a Hilbert space as a topological vector space can itself be characterized in terms of the presence of complementary subspaces:
The orthogonal complement satisfies some more elementary results. It is a monotone function in the sense that if "U" ⊂ "V", then formula_113 with equality holding if and only if "V" is contained in the closure of "U". This result is a special case of the Hahn–Banach theorem. The closure of a subspace can be completely characterized in terms of the orthogonal complement: If "V" is a subspace of "H", then the closure of "V" is equal to formula_114. The orthogonal complement is thus a Galois connection on the partial order of subspaces of a Hilbert space. In general, the orthogonal complement of a sum of subspaces is the intersection of the orthogonal complements: formula_115. If the "V""i" are in addition closed, then formula_116.
Spectral theory.
There is a well-developed spectral theory for self-adjoint operators in a Hilbert space, that is roughly analogous to the study of symmetric matrices over the reals or self-adjoint matrices over the complex numbers. In the same sense, one can obtain a "diagonalization" of a self-adjoint operator as a suitable sum (actually an integral) of orthogonal projection operators.
The spectrum of an operator "T", denoted σ("T") is the set of complex numbers λ such that "T" − λ lacks a continuous inverse. If "T" is bounded, then the spectrum is always a compact set in the complex plane, and lies inside the disc formula_117 If "T" is self-adjoint, then the spectrum is real. In fact, it is contained in the interval ["m","M"] where
Moreover, "m" and "M" are both actually contained within the spectrum.
The eigenspaces of an operator "T" are given by
Unlike with finite matrices, not every element of the spectrum of "T" must be an eigenvalue: the linear operator "T" − λ may only lack an inverse because it is not surjective. Elements of the spectrum of an operator in the general sense are known as "spectral values". Since spectral values need not be eigenvalues, the spectral decomposition is often more subtle than in finite dimensions.
However, the spectral theorem of a self-adjoint operator "T" takes a particularly simple form if, in addition, "T" is assumed to be a compact operator. The spectral theorem for compact self-adjoint operators states:
This theorem plays a fundamental role in the theory of integral equations, as many integral operators are compact, in particular those that arise from Hilbert–Schmidt operators.
The general spectral theorem for self-adjoint operators involves a kind of operator-valued Riemann–Stieltjes integral, rather than an infinite summation. The "spectral family" associated to "T" associates to each real number λ an operator "E"λ, which is the projection onto the nullspace of the operator ("T" − λ)+, where the positive part of a self-adjoint operator is defined by
The operators "E"λ are monotone increasing relative to the partial order defined on self-adjoint operators; the eigenvalues correspond precisely to the jump discontinuities. One has the spectral theorem, which asserts
The integral is understood as a Riemann–Stieltjes integral, convergent with respect to the norm on B("H"). In particular, one has the ordinary scalar-valued integral representation
A somewhat similar spectral decomposition holds for normal operators, although because the spectrum may now contain non-real complex numbers, the operator-valued Stieltjes measure "dE"λ must instead be replaced by a resolution of the identity.
A major application of spectral methods is the spectral mapping theorem, which allows one to apply to a self-adjoint operator "T" any continuous complex function "f" defined on the spectrum of "T" by forming the integral
The resulting continuous functional calculus has applications in particular to pseudodifferential operators.
The spectral theory of "unbounded" self-adjoint operators is only marginally more difficult than for bounded operators. The spectrum of an unbounded operator is defined in precisely the same way as for bounded operators: λ is a spectral value if the resolvent operator
fails to be a well-defined continuous operator. The self-adjointness of "T" still guarantees that the spectrum is real. Thus the essential idea of working with unbounded operators is to look instead at the resolvent "R"λ where λ is non-real. This is a "bounded" normal operator, which admits a spectral representation that can then be transferred to a spectral representation of "T" itself. A similar strategy is used, for instance, to study the spectrum of the Laplace operator: rather than address the operator directly, one instead looks as an associated resolvent such as a Riesz potential or Bessel potential.
A precise version of the spectral theorem in this case is:
There is also a version of the spectral theorem that applies to unbounded normal operators.

</doc>
<doc id="229553" url="https://en.wikipedia.org/wiki?curid=229553" title="Hooke's law">
Hooke's law

Hooke's law is a principle of physics that states that the force needed to extend or compress a spring by some distance is proportional to that distance. That is: , where is a constant factor characteristic of the spring, its stiffness. The law is named after 17th century British physicist Robert Hooke. He first stated the law in 1660 as a Latin anagram. He published the solution of his anagram in 1678 as: "ut tensio, sic vis" ("as the extension, so the force" or "the extension is proportional to the force").
Hooke's equation in fact holds (to some extent) in many other situations where an elastic body is deformed, such as wind blowing on a tall building, a musician plucking a string of a guitar, or the filling of a party balloon. An elastic body or material for which this equation can be assumed is said to be linear-elastic or Hookean. 
Hooke's law is only a first order linear approximation to the real response of springs and other elastic bodies to applied forces. It must eventually fail once the forces exceed some limit, since no material can be compressed beyond a certain minimum size, or stretched beyond a maximum size, without some permanent deformation or change of state. In fact, many materials will noticeably deviate from Hooke's law well before those elastic limits are reached.
On the other hand, Hooke's law is an accurate approximation for most solid bodies, as long as the forces and deformations are small enough. For this reason, Hooke's law is extensively used in all branches of science and engineering, and is the foundation of many disciplines such as seismology, molecular mechanics and acoustics. It is also the fundamental principle behind the spring scale, the manometer, and the balance wheel of the mechanical clock.
The modern theory of elasticity generalizes Hooke's law to say that the strain (deformation) of an elastic object or material is proportional to the stress applied to it. However, since general stresses and strains may have multiple independent components, the "proportionality factor" may no longer be just a single real number, but rather a linear map (a tensor) that can be represented by a matrix of real numbers.
In this general form, Hooke's law makes it possible to deduce the relation between strain and stress for complex objects in terms of intrinsic properties of the materials it is made of. For example, one can deduce that a homogeneous rod with uniform cross section will behave like a simple spring when stretched, with a stiffness directly proportional to its cross-section area and inversely proportional to its length.
Formal definition.
For linear springs.
Consider a simple helical spring that has one end attached to some fixed object, while the free end is being pulled by a force whose magnitude is . Suppose that the spring has reached a state of equilibrium, where its length is not changing anymore. Let be the amount by which the free end of the spring was displaced from its "relaxed" position (when it is not being stretched). Hooke's law states that
or, equivalently,
where is a positive real number, characteristic of the spring. Moreover, the same formula holds when the spring is compressed, with and both negative in that case. According to this formula, the graph of the applied force as a function of the displacement will be a straight line passing through the origin, whose slope is .
Hooke's law for a spring is often stated under the convention that is the restoring (reaction) force exerted by the spring on whatever is pulling its free end. In that case the equation becomes
since the direction of the restoring force is opposite to that of the displacement.
General "scalar" springs.
Hooke's spring law usually applies to any elastic object, of arbitrary complexity, as long as both the deformation and the stress can be expressed by a single number that can be both positive and negative.
For example, when a block of rubber attached to two parallel plates is deformed by shearing, rather than stretching or compression, the shearing force and the sideways displacement of the plates obey Hooke's law (for small enough deformations).
Hooke's law also applies when a straight steel bar or concrete beam, supported at both ends, is bent by a weight placed at some intermediate point. The displacement in this case is the deviation of the beam, measured in the transversal direction, relative to its unloaded shape.
The law also applies when a stretched steel wire is twisted by pulling on a lever attached to one end. In this case the stress can be taken as the force applied to the lever, and as the distance traveled by it along its circular path. Or, equivalently, one can let be the torque applied by the lever to the end of the wire, and be the angle by which that end turns. In either case is proportional to (although the constant is different in each case.)
Vector formulation.
In the case of a helical spring that is stretched or compressed along its axis, the applied (or restoring) force and the resulting elongation or compression have the same direction (which is the direction of said axis). Therefore, if and are defined as vectors, Hooke's equation still holds, and says that the force vector is the elongation vector multiplied by a fixed scalar.
General tensor form.
Some elastic bodies will deform in one direction when subjected to a force with a different direction. One example is a horizontal wood beam with non-square rectangular cross section that is bent by a transverse load that is neither vertical nor horizontal. In such cases, the "magnitude" of the displacement formula_4 will be proportional to the magnitude of the force formula_5, as long as the direction of the latter remains the same (and its value is not too large); so the scalar version of Hooke's law formula_6 will hold. However, the force and displacement "vectors" will not be scalar multiples of each other, since they have different directions. Moreover, the ratio formula_7 between their magnitudes will depend on the direction of the vector formula_5.
Yet, in such cases there is often a fixed linear relation between the force and deformation vectors, as long as they are small enough. Namely, there is a function formula_9 from vectors to vectors, such that formula_10, and formula_11 for any real numbers formula_12 and any displacement vectors formula_13. Such a function is called a (second-order) tensor.
With respect to an arbitrary Cartesian coordinate system, the force and displacement vectors can be represented by 3×1 matrices of real numbers. Then the tensor formula_9 connecting them can be represented by a 3×3 matrix formula_9 of real coefficients, that, when multiplied by the displacement vector, gives the force vector:
That is,
for formula_18 equal to 1,2, and 3. Therefore, Hooke's law formula_19 can be said to hold also when formula_4 and formula_5 are vectors with variable directions, except that the stiffness of the object is a tensor formula_9, rather than a single real number formula_7.
Hooke's law for continuous media.
The stresses and strains of the material inside a continuous elastic material (such as a block of rubber, the wall of a boiler, or a steel bar) are connected by a linear relationship that is mathematically similar to Hooke's spring law, and is often referred to by that name.
However, the strain state in a solid medium around some point cannot be described by a single vector. The same parcel of material, no matter how small, can be compressed, stretched, and sheared at the same time, along different directions. Likewise, the stresses in that parcel can be at once pushing, pulling, and shearing.
In order to capture this complexity, the relevant state of the medium around a point must be represented by two second-order tensors, the strain tensor formula_24 (in lieu of the displacement formula_4) and the stress tensor formula_26 (replacing the restoring force formula_5).The analogous of Hooke's spring law for continuous media is then
where formula_29 is a fourth-order tensor (that is, a linear map between second-order tensors) usually called the stiffness tensor or elasticity tensor. One may also write it as 
where the tensor formula_31, called the compliance tensor, represents the inverse of said linear map.
In a Cartesian coordinate system, the stress and strain tensors can be represented by 3×3 matrices
Being a linear mapping between the nine numbers formula_33 and the nine numbers formula_34, the stiffness tensor formula_29 is represented by a matrix of 3×3×3×3 = 81 real numbers formula_36. Hooke's law then says that
where formula_18 and formula_39 are 1, 2, or 3.
All three tensors generally vary from point to point inside the medium, and may vary with time as well. The strain tensor formula_24 merely specifies the displacement of the medium particles in the neighborhood of the point, while the stress tensor formula_26 specifies the forces that neighboring parcels of the medium are exerting on each other. Therefore, they are independent of the composition and physical state of the material. The stiffness tensor formula_29, on the other hand, is a property of the material, and often depends on physical state variables such as temperature, pressure, and microstructure.
Due to the inherent symmetries of formula_26, formula_24, and formula_29, only 21 elastic coefficients of the latter are independent. For isotropic media (which have the same physical properties in any direction), formula_29 can be reduced to only two independent numbers, the bulk modulus formula_47 and the shear modulus formula_48, that quantify the material's resistance to changes in volume and to shearing deformations, respectively.
Analogous laws.
Since Hooke's law is a simple proportionality between two quantities, its formulas and consequences are mathematically similar to those of many other physical laws, such as those describing the motion of fluids, or the polarization of a dielectric by an electric field.
In particular, the tensor equation formula_49 relating elastic stresses to strains is entirely similar to the equation formula_50 relating the viscous stress tensor formula_51 and the strain rate tensor formula_52 in flows of viscous fluids; although the former pertains to static stresses (related to "amount" of deformation) while the latter pertains to dynamical stresses (related to the "rate" of deformation).
Units of measurement.
In SI units, displacements are measured in metres (m), and forces in newtons (N or kg·m/s2). Therefore the spring constant formula_7, and each element of the tensor formula_9, is measured in newtons per metre (N/m), or kilograms per second squared (kg/s2).
For continuous media, each element of the stress tensor formula_26 is a force divided by an area; it is therefore measured in units of pressure, namely pascals (Pa, or N/m2, or kg/(m·s2). The elements of the strain tensor formula_24 are dimensionless (displacements divided by distances). Therefore the entries of formula_57 are also expressed in units of pressure.
General application to elastic materials.
Objects that quickly regain their original shape after being deformed by a force, with the molecules or atoms of their material returning to the initial state of stable equilibrium, often obey Hooke's law.
Hooke's law only holds for some materials under certain loading conditions. Steel exhibits linear-elastic behavior in most engineering applications; Hooke's law is valid for it throughout its elastic range (i.e., for stresses below the yield strength). For some other materials, such as aluminium, Hooke's law is only valid for a portion of the elastic range. For these materials a proportional limit stress is defined, below which the errors associated with the linear approximation are negligible.
Rubber is generally regarded as a "non-hookean" material because its elasticity is stress dependent and sensitive to temperature and loading rate.
Generalizations of Hooke's law for the case of large deformations is provided by models of neo-Hookean solids and Mooney-Rivlin solids.
Derived formulae.
Tensional stiffness of a uniform bar.
A rod of any elastic material may be viewed as a linear spring. The rod has length "L" and cross-sectional area "A". Its extension (strain) is linearly proportional to its tensile stress "σ" by a constant factor formula_58, the inverse of its modulus of elasticity "E", such that
formula_59.
In turn,
formula_60    ("i.e.", [change in length] as a fraction or percentage of total length),
and because
formula_61 ,
such that
formula_62 ,
this relationship may also be expressed as
Spring energy.
The potential energy formula_64 stored in a spring is given by
which comes from adding up the energy it takes to incrementally compress the spring. That is, the integral of force over displacement. Since the external force has the same general direction as the displacement, the potential energy of a spring is always non-negative.
This potential "formula_66" can be visualized as a parabola on the "U"-"x" plane such that formula_67. As the spring is stretched in the positive x-direction, the potential energy increases parabolically (the same thing happens as the spring is compressed). Since the change in potential energy changes at a constant rate: formula_68. Note that the change in the change in U is constant even when the displacement and acceleration are zero.
Relaxed force constants (generalized compliance constants).
Relaxed force constants (the inverse of Grunenberg`s generalized compliance constants) are uniquely defined for molecular systems, in contradistinction to the usual “rigid” force constants, and thus their use allows meaningful correlations to be made between force fields calculated for reactants, transition state, and product of a chemical reaction. Just as the potential energy can be written as a quadratic form in the internal coordinates, so it can also be written in terms of generalized forces. The resulting coefficients are termed compliance constants. The suitability of relaxed force constants (inverse compliance constants) as covalent bond strength descriptors was demonstrated as early as ~1980, at the University of Cambridge. Recently, the suitability as non-covalent bond strength descriptors was demonstrated, too.
Harmonic oscillator.
A mass "m" attached to the end of a spring is a classic example of a harmonic oscillator. By pulling slightly on the mass and then releasing it, the system will be set in sinusoid oscillating motion about the equilibrium position. To the extent that the spring obeys Hooke's law, and that one can neglect friction and the mass of the spring, the amplitude of the oscillation will remain constant; and its frequency formula_69 will be independent of its amplitude, determined only by the mass and the stiffness of the spring:
This phenomenon made possible the construction of accurate mechanical clocks and watches that could be carried on ships and people's pockets.
Rotation in gravity-free space.
If the mass were attached to a spring with force constant and rotating in free space, the spring tension () would supply the required centripetal force ():
Since and , then:
Given that , this leads to the same frequency equation as above:
Linear elasticity theory for continuous media.
Isotropic materials.
Isotropic materials are characterized by properties which are independent of direction in space. Physical equations involving isotropic materials must therefore be independent of the coordinate system chosen to represent them. The strain tensor is a symmetric tensor. Since the trace of any tensor is independent of any coordinate system, the most complete coordinate-free decomposition of a symmetric tensor is to represent it as the sum of a constant tensor and a traceless symmetric tensor. Thus in index notation:
where formula_76 is the Kronecker delta. In direct tensor notation:
where formula_78 is the second-order identity tensor.
The first term on the right is the constant tensor, also known as the volumetric strain tensor, and the second term is the traceless symmetric tensor, also known as the deviatoric strain tensor or shear tensor.
The most general form of Hooke's law for isotropic materials may now be written as a linear combination of these two tensors:
where "K" is the bulk modulus and "G" is the shear modulus.
Using the relationships between the elastic moduli, these equations may also be expressed in various other ways. A common form of Hooke's law for isotropic materials, expressed in direct tensor notation, is
where formula_81 and formula_82 are the Lamé constants, formula_78 is the second-rank identity tensor, and formula_84 is the symmetric part of the fourth-rank identity tensor. In index notation:
The inverse relationship is
Therefore the compliance tensor in the relation formula_87 is
In terms of Young's modulus and Poisson's ratio, Hooke's law for isotropic materials can then be expressed as
This is the form in which the strain is expressed in terms of the stress tensor in engineering. The expression in expanded form is
where "E" is Young's modulus and formula_91 is Poisson's ratio. (See 3-D elasticity).
In matrix form, Hooke's law for isotropic materials can be written as
where formula_93 is the engineering shear strain.
The inverse relation may be written as
which can be simplified thanks to the Lamé constants :
Plane stress.
Under plane stress conditions formula_96. In that case Hooke's law takes the form
The inverse relation is usually written in the reduced form
Anisotropic materials.
The symmetry of the Cauchy stress tensor (formula_99) and the generalized Hooke's laws (formula_100) implies that formula_101. Similarly, the symmetry of the infinitesimal strain tensor implies that formula_102. These symmetries are called the minor symmetries of the stiffness tensor (formula_103). This reduces the number of elastic constants from 81 to 36.
If in addition, since the displacement gradient and the Cauchy stress are work conjugate, the stress–strain relation can be derived from a strain energy density functional (formula_104), then
The arbitrariness of the order of differentiation implies that formula_106. These are called the major symmetries of the stiffness tensor. This reduces the number of elastic constants to 21 from 36. The major and minor symmetries indicate that the stiffness tensor has only 21 independent components.
Matrix representation (stiffness tensor).
It is often useful to express the anisotropic form of Hooke's law in matrix notation, also called Voigt notation. To do this we take advantage of the symmetry of the stress and strain tensors and express them as six-dimensional vectors in an orthonormal coordinate system (formula_107) as
Then the stiffness tensor (formula_103) can be expressed as
and Hooke's law is written as
Similarly the compliance tensor (formula_112) can be written as
Change of coordinate system.
If a linear elastic material is rotated from a reference configuration to another, then the material is symmetric with respect to the rotation if the components of the stiffness tensor in the rotated configuration are related to the components in the reference configuration by the relation
where formula_115 are the components of an orthogonal rotation matrix formula_116. The same relation also holds for inversions.
In matrix notation, if the transformed basis (rotated or inverted) is related to the reference basis by
then
In addition, if the material is symmetric with respect to the transformation formula_116 then
Orthotropic materials.
Orthotropic materials have three orthogonal planes of symmetry. If the basis vectors (formula_107) are normals to the planes of symmetry then the coordinate transformation relations imply that
The inverse of this relation is commonly written as
where
Under "plane stress" conditions, formula_132, Hooke's law for an orthotropic material takes the form
The inverse relation is
The transposed form of the above stiffness matrix is also often used.
Transversely isotropic materials.
A transversely isotropic material is symmetric with respect to a rotation about an axis of symmetry. For such a material, if formula_135 is the axis of symmetry, Hooke's law can be expressed as
More frequently, the formula_137 axis is taken to be the axis of symmetry and the inverse Hooke's law is written as
Thermodynamic basis.
Linear deformations of elastic materials can be approximated as adiabatic. Under these conditions and for quasistatic processes the first law of thermodynamics for a deformed body can be expressed as
where formula_140 is the increase in internal energy and formula_141 is the work done by external forces. The work can be split into two terms
where formula_143 is the work done by surface forces while formula_144 is the work done by body forces. If formula_145 is a variation of the displacement field formula_146 in the body, then the two external work terms can be expressed as
where formula_148 is the surface traction vector, formula_149 is the body force vector, formula_150 represents the body and formula_151 represents its surface. Using the relation between the Cauchy stress and the surface traction, formula_152 (where formula_153 is the unit outward normal to formula_151), we have
Converting the surface integral into a volume integral via the divergence theorem gives
Using the symmetry of the Cauchy stress and the identity
we have the following
From the definition of strain and from the equations of equilibrium we have
Hence we can write
and therefore the variation in the internal energy density is given by
An elastic material is defined as one in which the total internal energy is equal to the potential energy of the internal forces (also called the elastic strain energy). Therefore the internal energy density is a function of the strains, formula_162 and the variation of the internal energy can be expressed as
Since the variation of strain is arbitrary, the stress–strain relation of an elastic material is given by
For a linear elastic material, the quantity formula_165 is a linear function of formula_166, and can therefore be expressed as
where formula_103 is a fourth-rank tensor of material constants, also called the stiffness tensor. We can see why formula_103 must be a fourth-rank tensor by noting that, for a linear elastic material,
In index notation
The right-hand side constant requires four indices and is a fourth-rank quantity. We can also see that this quantity must be a tensor because it is a linear transformation that takes the strain tensor to the stress tensor. We can also show that the constant obeys the tensor transformation rules for fourth-rank tensors.

</doc>
<doc id="13255" url="https://en.wikipedia.org/wiki?curid=13255" title="Hydrogen">
Hydrogen

Hydrogen is a chemical element with chemical symbol H and atomic number 1. With an atomic weight of , hydrogen is the lightest element on the periodic table. Its monatomic form (H) is the most abundant chemical substance in the Universe, constituting roughly 75% of all baryonic mass. Non-remnant stars are mainly composed of hydrogen in its plasma state. The most common isotope of hydrogen, termed "protium" (name rarely used, symbol 1H), has one proton and no neutrons.
The universal emergence of atomic hydrogen first occurred during the recombination epoch. At standard temperature and pressure, hydrogen is a colorless, odorless, tasteless, non-toxic, nonmetallic, highly combustible diatomic gas with the molecular formula H2. Since hydrogen readily forms covalent compounds with most non-metallic elements, most of the hydrogen on Earth exists in molecular forms such as in the form of water or organic compounds. Hydrogen plays a particularly important role in acid–base reactions as many acid-base reactions involve the exchange of protons between soluble molecules. In ionic compounds, hydrogen can take the form of a negative charge (i.e., anion) when it is known as a hydride, or as a positively charged (i.e., cation) species denoted by the symbol H+. The hydrogen cation is written as though composed of a bare proton, but in reality, hydrogen cations in ionic compounds are always more complex species than that would suggest. As the only neutral atom for which the Schrödinger equation can be solved analytically, study of the energetics and bonding of the hydrogen atom has played a key role in the development of quantum mechanics.
Hydrogen gas was first artificially produced in the early 16th century, via the mixing of metals with acids. In 1766–81, Henry Cavendish was the first to recognize that hydrogen gas was a discrete substance, and that it produces water when burned, a property which later gave it its name: in Greek, hydrogen means "water-former".
Industrial production is mainly from the steam reforming of natural gas, and less often from more energy-intensive hydrogen production methods like the electrolysis of water. Most hydrogen is employed near its production site, with the two largest uses being fossil fuel processing (e.g., hydrocracking) and ammonia production, mostly for the fertilizer market. Hydrogen is a concern in metallurgy as it can embrittle many metals, complicating the design of pipelines and storage tanks.
Properties.
Combustion.
Hydrogen gas (dihydrogen or molecular hydrogen) is highly flammable and will burn in air at a very wide range of concentrations between 4% and 75% by volume. The enthalpy of combustion for hydrogen is −286 kJ/mol:
Hydrogen gas forms explosive mixtures with air if it is 4–74% concentrated and with chlorine if it is 5–95% concentrated. The mixtures may be ignited by spark, heat or sunlight. The hydrogen autoignition temperature, the temperature of spontaneous ignition in air, is . Pure hydrogen-oxygen flames emit ultraviolet light and with high oxygen mix are nearly invisible to the naked eye, as
illustrated by the faint plume of the Space Shuttle Main Engine compared to the highly visible plume of a Space Shuttle Solid Rocket Booster. The detection of a burning hydrogen leak may require a flame detector; such leaks can be very dangerous. Hydrogen flames in other conditions are blue, resembling blue natural gas flames. The destruction of the Hindenburg airship was an infamous example of hydrogen combustion; the cause is debated, but the visible orange flames were the result of a rich mixture of hydrogen to oxygen combined with carbon compounds from the airship skin.
H2 reacts with every oxidizing element. Hydrogen can react spontaneously and violently at room temperature with chlorine and fluorine to form the corresponding hydrogen halides, hydrogen chloride and hydrogen fluoride, which are also potentially dangerous acids.
Electron energy levels.
The ground state energy level of the electron in a hydrogen atom is −13.6 eV, which is equivalent to an ultraviolet photon of roughly 91 nm wavelength.
The energy levels of hydrogen can be calculated fairly accurately using the Bohr model of the atom, which conceptualizes the electron as "orbiting" the proton in analogy to the Earth's orbit of the Sun. However, the electromagnetic force attracts electrons and protons to one another, while planets and celestial objects are attracted to each other by gravity. Because of the discretization of angular momentum postulated in early quantum mechanics by Bohr, the electron in the Bohr model can only occupy certain allowed distances from the proton, and therefore only certain allowed energies.
A more accurate description of the hydrogen atom comes from a purely quantum mechanical treatment that uses the Schrödinger equation, Dirac equation or even the Feynman path integral formulation to calculate the probability density of the electron around the proton. The most complicated treatments allow for the small effects of special relativity and vacuum polarization. In the quantum mechanical treatment, the electron in a ground state hydrogen atom has no angular momentum at all—an illustration of how the "planetary orbit" conception of electron motion differs from reality.
Elemental molecular forms.
There exist two different spin isomers of hydrogen diatomic molecules that differ by the relative spin of their nuclei. In the orthohydrogen form, the spins of the two protons are parallel and form a triplet state with a molecular spin quantum number of 1 (+); in the parahydrogen form the spins are antiparallel and form a singlet with a molecular spin quantum number of 0 (–). At standard temperature and pressure, hydrogen gas contains about 25% of the para form and 75% of the ortho form, also known as the "normal form". The equilibrium ratio of orthohydrogen to parahydrogen depends on temperature, but because the ortho form is an excited state and has a higher energy than the para form, it is unstable and cannot be purified. At very low temperatures, the equilibrium state is composed almost exclusively of the para form. The liquid and gas phase thermal properties of pure parahydrogen differ significantly from those of the normal form because of differences in rotational heat capacities, as discussed more fully in "spin isomers of hydrogen". The ortho/para distinction also occurs in other hydrogen-containing molecules or functional groups, such as water and methylene, but is of little significance for their thermal properties.
The uncatalyzed interconversion between para and ortho H2 increases with increasing temperature; thus rapidly condensed H2 contains large quantities of the high-energy ortho form that converts to the para form very slowly. The ortho/para ratio in condensed H2 is an important consideration in the preparation and storage of liquid hydrogen: the conversion from ortho to para is exothermic and produces enough heat to evaporate some of the hydrogen liquid, leading to loss of liquefied material. Catalysts for the ortho-para interconversion, such as ferric oxide, activated carbon, platinized asbestos, rare earth metals, uranium compounds,
chromic oxide, or some nickel compounds, are used during hydrogen cooling.
Compounds.
Covalent and organic compounds.
While H2 is not very reactive under standard conditions, it does form compounds with most elements. Hydrogen can form compounds with elements that are more electronegative, such as halogens (e.g., F, Cl, Br, I), or oxygen; in these compounds hydrogen takes on a partial positive charge. When bonded to fluorine, oxygen, or nitrogen, hydrogen can participate in a form of medium-strength noncovalent bonding with other similar molecules between their hydrogens called hydrogen bonding, which is critical to the stability of many biological molecules. Hydrogen also forms compounds with less electronegative elements, such as the metals and metalloids, in which it takes on a partial negative charge. These compounds are often known as hydrides.
Hydrogen forms a vast array of compounds with carbon called the hydrocarbons, and an even vaster array with heteroatoms that, because of their general association with living things, are called organic compounds. The study of their properties is known as organic chemistry and their study in the context of living organisms is known as biochemistry. By some definitions, "organic" compounds are only required to contain carbon. However, most of them also contain hydrogen, and because it is the carbon-hydrogen bond which gives this class of compounds most of its particular chemical characteristics, carbon-hydrogen bonds are required in some definitions of the word "organic" in chemistry. Millions of hydrocarbons are known, and they are usually formed by complicated synthetic pathways, which seldom involve elementary hydrogen.
Hydrides.
Compounds of hydrogen are often called hydrides, a term that is used fairly loosely. The term "hydride" suggests that the H atom has acquired a negative or anionic character, denoted H−, and is used when hydrogen forms a compound with a more electropositive element. The existence of the hydride anion, suggested by Gilbert N. Lewis in 1916 for group I and II salt-like hydrides, was demonstrated by Moers in 1920 by the electrolysis of molten lithium hydride (LiH), producing a stoichiometry quantity of hydrogen at the anode. For hydrides other than group I and II metals, the term is quite misleading, considering the low electronegativity of hydrogen. An exception in group II hydrides is , which is polymeric. In lithium aluminium hydride, the anion carries hydridic centers firmly attached to the Al(III).
Although hydrides can be formed with almost all main-group elements, the number and combination of possible compounds varies widely; for example, there are over 100 binary borane hydrides known, but only one binary aluminium hydride. Binary indium hydride has not yet been identified, although larger complexes exist.
In inorganic chemistry, hydrides can also serve as bridging ligands that link two metal centers in a coordination complex. This function is particularly common in group 13 elements, especially in boranes (boron hydrides) and aluminium complexes, as well as in clustered carboranes.
Protons and acids.
Oxidation of hydrogen removes its electron and gives H+, which contains no electrons and a nucleus which is usually composed of one proton. That is why is often called a proton. This species is central to discussion of acids. Under the Bronsted-Lowry theory, acids are proton donors, while bases are proton acceptors.
A bare proton, , cannot exist in solution or in ionic crystals, because of its unstoppable attraction to other atoms or molecules with electrons. Except at the high temperatures associated with plasmas, such protons cannot be removed from the electron clouds of atoms and molecules, and will remain attached to them. However, the term 'proton' is sometimes used loosely and metaphorically to refer to positively charged or cationic hydrogen attached to other species in this fashion, and as such is denoted "" without any implication that any single protons exist freely as a species.
To avoid the implication of the naked "solvated proton" in solution, acidic aqueous solutions are sometimes considered to contain a less unlikely fictitious species, termed the "hydronium ion" (). However, even in this case, such solvated hydrogen cations are more realistically conceived as being organized into clusters that form species closer to H. Other oxonium ions are found when water is in acidic solution with other solvents.
Although exotic on Earth, one of the most common ions in the universe is the ion, known as protonated molecular hydrogen or the trihydrogen cation.
Isotopes.
Hydrogen has three naturally occurring isotopes, denoted , and . Other, highly unstable nuclei ( to ) have been synthesized in the laboratory but not observed in nature.
Hydrogen is the only element that has different names for its isotopes in common use today. During the early study of radioactivity, various heavy radioactive isotopes were given their own names, but such names are no longer used, except for deuterium and tritium. The symbols D and T (instead of and ) are sometimes used for deuterium and tritium, but the corresponding symbol for protium, P, is already in use for phosphorus and thus is not available for protium. In its nomenclatural guidelines, the International Union of Pure and Applied Chemistry allows any of D, T, , and to be used, although and are preferred.
History.
Discovery and use.
In 1671, Robert Boyle discovered and described the reaction between iron filings and dilute acids, which results in the production of hydrogen gas. In 1766, Henry Cavendish was the first to recognize hydrogen gas as a discrete substance, by naming the gas from a metal-acid reaction "flammable air". He speculated that "flammable air" was in fact identical to the hypothetical substance called "phlogiston" and further finding in 1781 that the gas produces water when burned. He is usually given credit for its discovery as an element. In 1783, Antoine Lavoisier gave the element the name hydrogen (from the Greek ὑδρο- "hydro" meaning "water" and -γενής "genes" meaning "creator") when he and Laplace reproduced Cavendish's finding that water is produced when hydrogen is burned.
Lavoisier produced hydrogen for his experiments on mass conservation by reacting a flux of steam with metallic iron through an incandescent iron tube heated in a fire. Anaerobic oxidation of iron by the protons of water at high temperature can be schematically represented by the set of following reactions:
Many metals such as zirconium undergo a similar reaction with water leading to the production of hydrogen.
Hydrogen was liquefied for the first time by James Dewar in 1898 by using regenerative cooling and his invention, the vacuum flask. He produced solid hydrogen the next year. Deuterium was discovered in December 1931 by Harold Urey, and tritium was prepared in 1934 by Ernest Rutherford, Mark Oliphant, and Paul Harteck. Heavy water, which consists of deuterium in the place of regular hydrogen, was discovered by Urey's group in 1932. François Isaac de Rivaz built the first de Rivaz engine, an internal combustion engine powered by a mixture of hydrogen and oxygen in 1806. Edward Daniel Clarke invented the hydrogen gas blowpipe in 1819. The Döbereiner's lamp and limelight were invented in 1823.
The first hydrogen-filled balloon was invented by Jacques Charles in 1783. Hydrogen provided the lift for the first reliable form of air-travel following the 1852 invention of the first hydrogen-lifted airship by Henri Giffard. German count Ferdinand von Zeppelin promoted the idea of rigid airships lifted by hydrogen that later were called Zeppelins; the first of which had its maiden flight in 1900. Regularly scheduled flights started in 1910 and by the outbreak of World War I in August 1914, they had carried 35,000 passengers without a serious incident. Hydrogen-lifted airships were used as observation platforms and bombers during the war.
The first non-stop transatlantic crossing was made by the British airship "R34" in 1919. Regular passenger service resumed in the 1920s and the discovery of helium reserves in the United States promised increased safety, but the U.S. government refused to sell the gas for this purpose. Therefore, H2 was used in the "Hindenburg" airship, which was destroyed in a midair fire over New Jersey on 6 May 1937. The incident was broadcast live on radio and filmed. Ignition of leaking hydrogen is widely assumed to be the cause, but later investigations pointed to the ignition of the aluminized fabric coating by static electricity. But the damage to hydrogen's reputation as a lifting gas was already done.
In the same year the first hydrogen-cooled turbogenerator went into service with gaseous hydrogen as a coolant in the rotor and the stator in 1937 at Dayton, Ohio, by the Dayton Power & Light Co.; because of the thermal conductivity of hydrogen gas, this is the most common type in its field today.
The nickel hydrogen battery was used for the first time in 1977 aboard the U.S. Navy's Navigation technology satellite-2 (NTS-2). For example, the ISS, Mars Odyssey and the Mars Global Surveyor are equipped with nickel-hydrogen batteries.
In the dark part of its orbit, the Hubble Space Telescope is also powered by nickel-hydrogen batteries, which were finally replaced in May 2009, more than 19 years after launch, and 13 years over their design life.
Role in quantum theory.
Because of its simple atomic structure, consisting only of a proton and an electron, the hydrogen atom, together with the spectrum of light produced from it or absorbed by it, has been central to the development of the theory of atomic structure. Furthermore, the corresponding simplicity of the hydrogen molecule and the corresponding cation allowed fuller understanding of the nature of the chemical bond, which followed shortly after the quantum mechanical treatment of the hydrogen atom had been developed in the mid-1920s.
One of the first quantum effects to be explicitly noticed (but not understood at the time) was a Maxwell observation involving hydrogen, half a century before full quantum mechanical theory arrived. Maxwell observed that the specific heat capacity of H2 unaccountably departs from that of a diatomic gas below room temperature and begins to increasingly resemble that of a monatomic gas at cryogenic temperatures. According to quantum theory, this behavior arises from the spacing of the (quantized) rotational energy levels, which are particularly wide-spaced in H2 because of its low mass. These widely spaced levels inhibit equal partition of heat energy into rotational motion in hydrogen at low temperatures. Diatomic gases composed of heavier atoms do not have such widely spaced levels and do not exhibit the same effect.
Antihydrogen () is the antimatter counterpart to hydrogen. It consists of an antiproton with a positron. Antihydrogen is the only type of antimatter atom to have been produced as of 2015.
Natural occurrence.
Hydrogen, as atomic H, is the most abundant chemical element in the universe, making up 75% of normal matter by mass and over 90% by number of atoms (most of the mass of the universe, however, is not in the form of chemical-element type matter, but rather is postulated to occur as yet-undetected forms of mass such as dark matter and dark energy). This element is found in great abundance in stars and gas giant planets. Molecular clouds of H2 are associated with star formation. Hydrogen plays a vital role in powering stars through the proton-proton reaction and the CNO cycle nuclear fusion.
Throughout the universe, hydrogen is mostly found in the atomic and plasma states whose properties are quite different from molecular hydrogen. As a plasma, hydrogen's electron and proton are not bound together, resulting in very high electrical conductivity and high emissivity (producing the light from the Sun and other stars). The charged particles are highly influenced by magnetic and electric fields. For example, in the solar wind they interact with the Earth's magnetosphere giving rise to Birkeland currents and the aurora. Hydrogen is found in the neutral atomic state in the interstellar medium. The large amount of neutral hydrogen found in the damped Lyman-alpha systems is thought to dominate the cosmological baryonic density of the Universe up to redshift "z"=4.
Under ordinary conditions on Earth, elemental hydrogen exists as the diatomic gas, H2. However, hydrogen gas is very rare in the Earth's atmosphere (1 ppm by volume) because of its light weight, which enables it to escape from Earth's gravity more easily than heavier gases. However, hydrogen is the third most abundant element on the Earth's surface, mostly in the form of chemical compounds such as hydrocarbons and water. Hydrogen gas is produced by some bacteria and algae and is a natural component of flatus, as is methane, itself a hydrogen source of increasing importance.
A molecular form called protonated molecular hydrogen () is found in the interstellar medium, where it is generated by ionization of molecular hydrogen from cosmic rays. This charged ion has also been observed in the upper atmosphere of the planet Jupiter. The ion is relatively stable in the environment of outer space due to the low temperature and density. is one of the most abundant ions in the Universe, and it plays a notable role in the chemistry of the interstellar medium.
Neutral triatomic hydrogen H3 can only exist in an excited form and is unstable. By contrast, the positive hydrogen molecular ion () is a rare molecule in the universe.
Production.
 is produced in chemistry and biology laboratories, often as a by-product of other reactions; in industry for the hydrogenation of unsaturated substrates; and in nature as a means of expelling reducing equivalents in biochemical reactions.
Metal-acid.
In the laboratory, is usually prepared by the reaction of dilute non-oxidizing acids on some reactive metals such as zinc with Kipp's apparatus.
Aluminium can also produce upon treatment with bases:
The electrolysis of water is a simple method of producing hydrogen. A low voltage current is run through the water, and gaseous oxygen forms at the anode while gaseous hydrogen forms at the cathode. Typically the cathode is made from platinum or another inert metal when producing hydrogen for storage. If, however, the gas is to be burnt on site, oxygen is desirable to assist the combustion, and so both electrodes would be made from inert metals. (Iron, for instance, would oxidize, and thus decrease the amount of oxygen given off.) The theoretical maximum efficiency (electricity used vs. energetic value of hydrogen produced) is in the range 80–94%.
An alloy of aluminium and gallium in pellet form added to water can be used to generate hydrogen. The process also produces alumina, but the expensive gallium, which prevents the formation of an oxide skin on the pellets, can be re-used. This has important potential implications for a hydrogen economy, as hydrogen can be produced on-site and does not need to be transported.
Steam reforming.
Hydrogen can be prepared in several different ways, but economically the most important processes involve removal of hydrogen from hydrocarbons. Commercial bulk hydrogen is usually produced by the steam reforming of natural gas. At high temperatures (1000–1400 K, 700–1100 °C or 1300–2000 °F), steam (water vapor) reacts with methane to yield carbon monoxide and .
This reaction is favored at low pressures but is nonetheless conducted at high pressures (2.0  MPa, 20 atm or 600 inHg). This is because high-pressure is the most marketable product and Pressure Swing Adsorption (PSA) purification systems work better at higher pressures. The product mixture is known as "synthesis gas" because it is often used directly for the production of methanol and related compounds. Hydrocarbons other than methane can be used to produce synthesis gas with varying product ratios. One of the many complications to this highly optimized technology is the formation of coke or carbon:
Consequently, steam reforming typically employs an excess of . Additional hydrogen can be recovered from the steam by use of carbon monoxide through the water gas shift reaction, especially with an iron oxide catalyst. This reaction is also a common industrial source of carbon dioxide:
Other important methods for production include partial oxidation of hydrocarbons:
and the coal reaction, which can serve as a prelude to the shift reaction above:
Hydrogen is sometimes produced and consumed in the same industrial process, without being separated. In the Haber process for the production of ammonia, hydrogen is generated from natural gas. Electrolysis of brine to yield chlorine also produces hydrogen as a co-product.
Thermochemical.
There are more than 200 thermochemical cycles which can be used for water splitting, around a dozen of these cycles such as the iron oxide cycle, cerium(IV) oxide–cerium(III) oxide cycle, zinc zinc-oxide cycle, sulfur-iodine cycle, copper-chlorine cycle and hybrid sulfur cycle are under research and in testing phase to produce hydrogen and oxygen from water and heat without using electricity. A number of laboratories (including in France, Germany, Greece, Japan, and the USA) are developing thermochemical methods to produce hydrogen from solar energy and water.
Anaerobic corrosion.
Under anaerobic conditions, iron and steel alloys are slowly oxidized by the protons of water concomitantly reduced in molecular hydrogen (). The anaerobic corrosion of iron leads first to the formation of ferrous hydroxide (green rust) and can be described by the following reaction:
In its turn, under anaerobic conditions, the ferrous hydroxide ( ) can be oxidized by the protons of water to form magnetite and molecular hydrogen.
This process is described by the Schikorr reaction:
The well crystallized magnetite () is thermodynamically more stable than the ferrous hydroxide ( ).
This process occurs during the anaerobic corrosion of iron and steel in oxygen-free groundwater and in reducing soils below the water table.
Geological occurrence: the serpentinization reaction.
In the absence of atmospheric oxygen (), in deep geological conditions prevailing far away from Earth atmosphere, hydrogen () is produced during the process of serpentinization by the anaerobic oxidation by the water protons (H+) of the ferrous (Fe2+) silicate present in the crystal lattice of the fayalite (, the olivine iron-endmember). The corresponding reaction leading to the formation of magnetite (), quartz (Si) and hydrogen () is the following:
This reaction closely resembles the Schikorr reaction observed in the anaerobic oxidation of the ferrous hydroxide in contact with water.
Formation in transformers.
From all the fault gases formed in power transformers, hydrogen is the most common and is generated under most fault conditions; thus, formation of hydrogen is an early indication of serious problems in the transformer's life cycle.
Applications.
Consumption in processes.
Large quantities of are needed in the petroleum and chemical industries. The largest application of is for the processing ("upgrading") of fossil fuels, and in the production of ammonia. The key consumers of in the petrochemical plant include hydrodealkylation, hydrodesulfurization, and hydrocracking. has several other important uses. is used as a hydrogenating agent, particularly in increasing the level of saturation of unsaturated fats and oils (found in items such as margarine), and in the production of methanol. It is similarly the source of hydrogen in the manufacture of hydrochloric acid. is also used as a reducing agent of metallic ores.
Hydrogen is highly soluble in many rare earth and transition metals and is soluble in both nanocrystalline and amorphous metals. Hydrogen solubility in metals is influenced by local distortions or impurities in the crystal lattice. These properties may be useful when hydrogen is purified by passage through hot palladium disks, but the gas's high solubility is a metallurgical problem, contributing to the embrittlement of many metals, complicating the design of pipelines and storage tanks.
Apart from its use as a reactant, has wide applications in physics and engineering. It is used as a shielding gas in welding methods such as atomic hydrogen welding. H2 is used as the rotor coolant in electrical generators at power stations, because it has the highest thermal conductivity of any gas. Liquid H2 is used in cryogenic research, including superconductivity studies. Because is lighter than air, having a little more than of the density of air, it was once widely used as a lifting gas in balloons and airships.
In more recent applications, hydrogen is used pure or mixed with nitrogen (sometimes called forming gas) as a tracer gas for minute leak detection. Applications can be found in the automotive, chemical, power generation, aerospace, and telecommunications industries. Hydrogen is an authorized food additive (E 949) that allows food package leak testing among other anti-oxidizing properties.
Hydrogen's rarer isotopes also each have specific applications. Deuterium (hydrogen-2) is used in nuclear fission applications as a moderator to slow neutrons, and in nuclear fusion reactions. Deuterium compounds have applications in chemistry and biology in studies of reaction isotope effects. Tritium (hydrogen-3), produced in nuclear reactors, is used in the production of hydrogen bombs, as an isotopic label in the biosciences, and as a radiation source in luminous paints.
The triple point temperature of equilibrium hydrogen is a defining fixed point on the ITS-90 temperature scale at 13.8033 kelvins.
Coolant.
Hydrogen is commonly used in power stations as a coolant in generators due to a number of favorable properties that are a direct result of its light diatomic molecules. These include low density, low viscosity, and the highest specific heat and thermal conductivity of all gases.
Energy carrier.
Hydrogen is not an energy resource, except in the hypothetical context of commercial nuclear fusion power plants using deuterium or tritium, a technology presently far from development. The Sun's energy comes from nuclear fusion of hydrogen, but this process is difficult to achieve controllably on Earth. Elemental hydrogen from solar, biological, or electrical sources require more energy to make it than is obtained by burning it, so in these cases hydrogen functions as an energy carrier, like a battery. Hydrogen may be obtained from fossil sources (such as methane), but these sources are unsustainable.
The energy density per unit "volume" of both liquid hydrogen and compressed hydrogen gas at any practicable pressure is significantly less than that of traditional fuel sources, although the energy density per unit fuel "mass" is higher. Nevertheless, elemental hydrogen has been widely discussed in the context of energy, as a possible future "carrier" of energy on an economy-wide scale. For example, sequestration followed by carbon capture and storage could be conducted at the point of production from fossil fuels. Hydrogen used in transportation would burn relatively cleanly, with some NOx emissions, but without carbon emissions. However, the infrastructure costs associated with full conversion to a hydrogen economy would be substantial. Fuel cells can convert hydrogen and oxygen directly to electricity more efficiently than internal combustion engines.
Semiconductor industry.
Hydrogen is employed to saturate broken ("dangling") bonds of amorphous silicon and amorphous carbon that helps stabilizing material properties. It is also a potential electron donor in various oxide materials, including ZnO, SnO2, CdO, MgO, ZrO2, HfO2, La2O3, Y2O3, TiO2, SrTiO3, LaAlO3, SiO2, Al2O3, ZrSiO4, HfSiO4, and SrZrO3.
Biological reactions.
H2 is a product of some types of anaerobic metabolism and is produced by several microorganisms, usually via reactions catalyzed by iron- or nickel-containing enzymes called hydrogenases. These enzymes catalyze the reversible redox reaction between H2 and its component two protons and two electrons. Creation of hydrogen gas occurs in the transfer of reducing equivalents produced during pyruvate fermentation to water. The natural cycle of hydrogen production and consumption by organisms is called the hydrogen cycle.
Water splitting, in which water is decomposed into its component protons, electrons, and oxygen, occurs in the light reactions in all photosynthetic organisms. Some such organisms, including the alga "Chlamydomonas reinhardtii" and cyanobacteria, have evolved a second step in the dark reactions in which protons and electrons are reduced to form H2 gas by specialized hydrogenases in the chloroplast. Efforts have been undertaken to genetically modify cyanobacterial hydrogenases to efficiently synthesize H2 gas even in the presence of oxygen. Efforts have also been undertaken with genetically modified alga in a bioreactor.
Safety and precautions.
Hydrogen poses a number of hazards to human safety, from potential detonations and fires when mixed with air to being an asphyxiant in its pure, oxygen-free form. In addition, liquid hydrogen is a cryogen and presents dangers (such as frostbite) associated with very cold liquids. Hydrogen dissolves in many metals, and, in addition to leaking out, may have adverse effects on them, such as hydrogen embrittlement, leading to cracks and explosions. Hydrogen gas leaking into external air may spontaneously ignite. Moreover, hydrogen fire, while being extremely hot, is almost invisible, and thus can lead to accidental burns.
Even interpreting the hydrogen data (including safety data) is confounded by a number of phenomena. Many physical and chemical properties of hydrogen depend on the parahydrogen/orthohydrogen ratio (it often takes days or weeks at a given temperature to reach the equilibrium ratio, for which the data is usually given). Hydrogen detonation parameters, such as critical detonation pressure and temperature, strongly depend on the container geometry.

</doc>
<doc id="14052" url="https://en.wikipedia.org/wiki?curid=14052" title="Hyperbola">
Hyperbola

In mathematics, a hyperbola (plural "hyperbolas" or "hyperbolae") is a type of smooth curve lying in a plane, defined by its geometric properties or by equations for which it is the solution set. A hyperbola has two pieces, called connected components or branches, that are mirror images of each other and resemble two infinite bows. The hyperbola is one of the four kinds of conic section, formed by the intersection of a plane and a double cone. (The other conic sections are the parabola, the ellipse, and the circle; the circle is a special case of the ellipse). If the plane intersects both halves of the double cone but does not pass through the apex of the cones, then the conic is a hyperbola.
Hyperbolas arise in many ways: as the curve representing the function formula_1 in the Cartesian plane, as the path followed by the shadow of the tip of a sundial, as the shape of an open orbit (as distinct from a closed elliptical orbit), such as the orbit of a spacecraft during a gravity assisted swing-by of a planet or more generally any spacecraft exceeding the escape velocity of the nearest planet, as the path of a single-apparition comet (one travelling too fast ever to return to the solar system), as the scattering trajectory of a subatomic particle (acted on by repulsive instead of attractive forces but the principle is the same), and so on.
Each branch of the hyperbola has two arms which become straighter (lower curvature) further out from the center of the hyperbola. Diagonally opposite arms, one from each branch, tend in the limit to a common line, called the asymptote of those two arms. So there are two asymptotes, whose intersection is at the center of symmetry of the hyperbola, which can be thought of as the mirror point about which each branch reflects to form the other branch. In the case of the curve formula_1 the asymptotes are the two coordinate axes.
Hyperbolas share many of the ellipses' analytical properties such as eccentricity, focus, and directrix. Typically the correspondence can be made with nothing more than a change of sign in some term. Many other mathematical objects have their origin in the hyperbola, such as hyperbolic paraboloids (saddle surfaces), hyperboloids ("wastebaskets"), hyperbolic geometry (Lobachevsky's celebrated non-Euclidean geometry), hyperbolic functions (sinh, cosh, tanh, etc.), and gyrovector spaces (a geometry proposed for use in both relativity and quantum mechanics which is not Euclidean).
Etymology and history.
The word "hyperbola" derives from the Greek , meaning "over-thrown" or "excessive", from which the English term hyperbole also derives. Hyperbolae were discovered by Menaechmus in his investigations of the problem of doubling the cube, but were then called sections of obtuse cones. The term hyperbola is believed to have been coined by Apollonius of Perga (c. 262–c. 190 BC) in his definitive work on the conic sections, the "Conics". For comparison, the other two general conic sections, the ellipse and the parabola, derive from the corresponding Greek words for "deficient" and "comparable"; these terms may refer to the eccentricity of these curves, which is greater than one (hyperbola), less than one (ellipse) and exactly one (parabola).
Nomenclature and features.
Similar to a parabola, a hyperbola is an open curve, meaning that it continues indefinitely to infinity, rather than closing on itself as an ellipse does. A hyperbola consists of two disconnected curves called its arms or branches.
The points on the two branches that are closest to each other are called the vertices; they are the points where the curve has its smallest radius of curvature. The line segment connecting the vertices is called the "transverse axis" or "major axis", corresponding to the major diameter of an ellipse. The midpoint of the transverse axis is known as the hyperbola's "center". The distance "a" from the center to each vertex is called the semi-major axis. Outside of the transverse axis but on the same line are the two "focal points (foci)" of the hyperbola. The line through these five points is one of the two principal axes of the hyperbola, the other being the perpendicular bisector of the transverse axis. The hyperbola has mirror symmetry about its principal axes, and is also symmetric under a 180° turn about its center.
At large distances from the center, the hyperbola approaches two lines, its asymptotes, which intersect at the hyperbola's center. A hyperbola approaches its asymptotes arbitrarily closely as the distance from its center increases, but it never intersects them; however, a degenerate hyperbola consists only of its asymptotes. Consistent with the symmetry of the hyperbola, if the transverse axis is aligned with the "x"-axis of a Cartesian coordinate system, the slopes of the asymptotes are equal in magnitude but opposite in sign, ±, where "b"="a" × tan(θ) and where θ is the angle between the transverse axis and either asymptote. The distance "b" (not shown) is the length of the perpendicular segment from either vertex to the asymptotes.
A "conjugate axis" of length 2"b", corresponding to the "minor axis" of an ellipse, is sometimes drawn on the non-transverse principal axis; its endpoints ±"b" lie on the minor axis at the height of the asymptotes over/under the hyperbola's vertices. Because of the minus sign in some of the formulas below, it is also called the "imaginary axis" of the hyperbola.
If , the angle 2θ between the asymptotes equals 90° and the hyperbola is said to be "rectangular" or "equilateral". In this special case, the rectangle joining the four points on the asymptotes directly above and below the vertices is a square, since the lengths of its sides "2a" "2b".
If the transverse axis of any hyperbola is aligned with the "x"-axis of a Cartesian coordinate system and is centered on the origin, the equation of the hyperbola can be written as
A hyperbola aligned in this way is called an "East-West opening hyperbola". Likewise, a hyperbola with its transverse axis aligned with the "y"-axis is called a "North–South opening hyperbola" and has equation
Every hyperbola is congruent to the origin-centered East-West opening hyperbola sharing its same scale and eccentricity "e" (its shape, or degree of "spread"), and is also congruent to the origin-centered North–South opening hyperbola with identical scale and eccentricity "e" — that is, it can be rotated so that it opens in the desired direction and can be translated (rigidly moved in the plane) so that it is centered at the origin. For convenience, hyperbolas are usually analyzed in terms of their centered East-West opening form.
If formula_5 is the distance from the center to either focus, then formula_6.
The shape of a hyperbola is defined entirely by its eccentricity "e", which is a dimensionless number always greater than one. The distance "c" from the center to the foci equals "ae". The eccentricity can also be defined as the ratio of the distances to either focus and to a corresponding line known as the directrix; hence, the distance from the center to the directrices equals "a"/"e". In terms of the parameters "a", "b", "c" and the angle θ, the eccentricity equals
For example, the eccentricity of a rectangular hyperbola , equals the square root of two: "e" =  formula_8.
Every hyperbola has a conjugate hyperbola, in which the transverse and conjugate axes are exchanged without changing the asymptotes. The equation of the conjugate hyperbola of formula_9 is formula_10. If the graph of the conjugate hyperbola is rotated 90° to restore the east-west opening orientation (so that "x" becomes "y" and vice versa), the equation of the resulting rotated conjugate hyperbola is the same as the equation of the original hyperbola except with "a" and "b" exchanged. For example, the angle θ of the conjugate hyperbola equals 90° minus the angle of the original hyperbola. Thus, the angles in the original and conjugate hyperbolas are complementary angles, which implies that they have different eccentricities unless θ = 45° (a rectangular hyperbola). Hence, the conjugate hyperbola does "not" in general correspond to a 90° rotation of the original hyperbola; the two hyperbolas are generally different in shape.
A few other lengths are used to describe hyperbolas. Consider a line perpendicular to the transverse axis (i.e., parallel to the conjugate axis) that passes through one of the hyperbola's foci. The line segment connecting the two intersection points of this line with the hyperbola is known as the "latus rectum" and has a length formula_11. The "semi-latus rectum" "l" is half of this length, i.e., formula_12. The "focal parameter" "p" is the distance from a focus to its corresponding directrix, and equals formula_13.
Mathematical definitions.
A hyperbola can be defined mathematically in several equivalent ways.
Conic section.
A hyperbola may be defined as the curve of intersection between a right circular conical surface and a plane that cuts through both halves of the cone. The other major types of conic sections are the ellipse and the parabola; in these cases, the plane cuts through only one half of the double cone. If the plane passes through the central apex of the double cone a degenerate hyperbola results — two straight lines that cross at the apex point.
Difference of distances to foci.
A hyperbola may be defined equivalently as the locus of points where the absolute value of the "difference" of the distances to the two foci is a constant equal to 2"a", the distance between its two vertices. This definition accounts for many of the hyperbola's applications, such as multilateration; this is the problem of determining position from the "difference" in arrival times of synchronized signals, as in GPS.
This definition may be expressed also in terms of tangent circles. The center of any circles externally tangent to two given circles lies on a hyperbola, whose foci are the centers of the given circles and where the vertex distance 2"a" equals the difference in radii of the two circles. As a special case, one given circle may be a point located at one focus; since a point may be considered as a circle of zero radius, the other given circle—which is centered on the other focus—must have radius 2"a". This provides a simple technique for constructing a hyperbola, as shown below. It follows from this definition that a tangent line to the hyperbola at a point P bisects the angle formed with the two foci, i.e., the angle F1P F2. Consequently, the feet of perpendiculars drawn from each focus to such a tangent line lies on a circle of radius "a" that is centered on the hyperbola's own center.
A proof that this characterization of the hyperbola is equivalent to the conic-section characterization can be done without coordinate geometry by means of Dandelin spheres.
Directrix and focus.
A hyperbola can be defined as the locus of points for which the ratio of the distances to one focus and to a line (called the directrix) is a constant "e" that is larger than 1. This constant is the eccentricity of the hyperbola. The eccentricity equals the secant of half the angle between the asymptotes of the hyperbola, so the eccentricity of the hyperbola "xy" = 1 equals the square root of 2.
By symmetry a hyperbola has two directrices, which are parallel to the conjugate axis and are between it and the tangent to the hyperbola at a vertex. One directrix and its focus is enough to produce both arms of the hyperbola.
Reciprocation of a circle.
The reciprocation of a circle "B" in a circle "C" always yields a conic section such as a hyperbola. The process of "reciprocation in a circle "C"" consists of replacing every line and point in a geometrical figure with their corresponding pole and polar, respectively. The "pole" of a line is the inversion of its closest point to the circle "C", whereas the polar of a point is the converse, namely, a line whose closest point to "C" is the inversion of the point.
The eccentricity of the conic section obtained by reciprocation is the ratio of the distances between the two circles' centers to the radius "r" of reciprocation circle "C". If B and C represent the points at the centers of the corresponding circles, then
Since the eccentricity of a hyperbola is always greater than one, the center B must lie outside of the reciprocating circle "C".
This definition implies that the hyperbola is both the locus of the poles of the tangent lines to the circle "B", as well as the envelope of the polar lines of the points on "B". Conversely, the circle "B" is the envelope of polars of points on the hyperbola, and the locus of poles of tangent lines to the hyperbola. Two tangent lines to "B" have no (finite) poles because they pass through the center C of the reciprocation circle "C"; the polars of the corresponding tangent points on "B" are the asymptotes of the hyperbola. The two branches of the hyperbola correspond to the two parts of the circle "B" that are separated by these tangent points.
Quadratic equation.
A hyperbola can also be defined as a second-degree equation in the Cartesian coordinates ("x", "y") of the plane
provided that the constants "A""xx", "A""xy", "A""yy", "B""x", "B""y", and "C" satisfy the determinant condition
A special case of a hyperbola—the "degenerate hyperbola" consisting of two intersecting lines—occurs when another determinant is zero
This determinant Δ is sometimes called the discriminant of the conic section.
Given the above general parametrization of the hyperbola in Cartesian coordinates, the eccentricity can be found using the formula in Conic section#Eccentricity in terms of parameters of the quadratic form.
The center ("x""c", "y""c") of the hyperbola may be determined from the formulae
In terms of new coordinates, and , the defining equation of the hyperbola can be written
The principal axes of the hyperbola make an angle Φ with the positive "x"-axis that is given by
Rotating the coordinate axes so that the "x"-axis is aligned with the transverse axis brings the equation into its canonical form
The major and minor semiaxes "a" and "b" are defined by the equations
where λ1 and λ2 are the roots of the quadratic equation
For comparison, the corresponding equation for a degenerate hyperbola is
The tangent line to a given point ("x"0, "y"0) on the hyperbola is defined by the equation
where "E", "F" and "G" are defined by
The normal line to the hyperbola at the same point is given by the equation
The normal line is perpendicular to the tangent line, and both pass through the same point ("x"0, "y"0).
From the equation
the basic property that with formula_33 and formula_34 being the distances from a point formula_35 to the left focus formula_36 and the right focus formula_37 one has for a point on the right branch that
and for a point on the left branch that
can be proved as follows:
If x,y is a point on the hyperbola the distance to the left focal point is
To the right focal point the distance is
If x,y is a point on the right branch of the hyperbola then formula_42 and
Subtracting these equations one gets
If x,y is a point on the left branch of the hyperbola then formula_46 and
Subtracting these equations one gets
True anomaly.
In the section above it is shown that using the coordinate system in which the equation of the hyperbola takes its canonical form
the distance formula_51 from a point formula_52 on the left branch of the hyperbola to the left focal point formula_53 is
Introducing polar coordinates formula_55 with origin at the left focal point, the coordinates relative to the canonical coordinate system are
and the equation above takes the form
from which it follows that
This is the representation of the near branch of a hyperbola in polar coordinates with respect to a focal point.
The polar angle formula_60 of a point on a hyperbola relative to the near focal point as described above is called the true anomaly of the point.
Geometrical constructions.
Similar to the ellipse, a hyperbola can be constructed using a taut thread. A straightedge of length "S" is attached to one focus F1 at one of its corners A so that it is free to rotate about that focus. A thread of length "L" = "S" - 2"a" is attached between the other focus F2 and the other corner B of the straightedge. A sharp pencil is held up against the straightedge, sandwiching the thread tautly against the straightedge. Let the position of the pencil be denoted as P. The total length "L" of the thread equals the sum of the distances "L"2 from F2 to P and "L"B from P to B. Similarly, the total length "S" of the straightedge equals the distance "L"1 from F1 to P and "L"B. Therefore, the difference in the distances to the foci, equals the constant 2"a"
A second construction uses intersecting circles, but is likewise based on the constant difference of distances to the foci. Consider a hyperbola with two foci F1 and F2, and two vertices P and Q; these four points all lie on the transverse axis. Choose a new point T also on the transverse axis and to the right of the rightmost vertex P; the difference in distances to the two vertices, = 2"a", since 2"a" is the distance between the vertices. Hence, the two circles centered on the foci F1 and F2 of radius QT and PT, respectively, will intersect at two points of the hyperbola.
A third construction relies on the definition of the hyperbola as the reciprocation of a circle. Consider the circle centered on the center of the hyperbola and of radius "a"; this circle is tangent to the hyperbola at its vertices. A line "g" drawn from one focus may intersect this circle in two points M and N; perpendiculars to "g" drawn through these two points are tangent to the hyperbola. Drawing a set of such tangent lines reveals the envelope of the hyperbola.
A fourth construction is using the parallelogram method. It is similar to such method for parabola and ellipse construction: certain equally spaced points lying on parallel lines are connected with each other by two straight lines and their intersection point lies on the hyperbola.
Reflections and tangent lines.
The ancient Greek geometers recognized a reflection property of hyperbolas. If a ray of light emerges from one focus and is reflected from either branch of the hyperbola, the light-ray appears to have come from the other focus. Equivalently, by reversing the direction of the light, rays directed at one of the foci are reflected towards the other focus. This property is analogous to the property of ellipses that a ray emerging from one focus is reflected from the ellipse directly "towards" the other focus (rather than "away" as in the hyperbola). Expressed mathematically, lines drawn from each focus to the same point on the hyperbola intersect it at equal angles; the tangent line to a hyperbola at a point P bisects the angle formed with the two foci, F1PF2.
Tangent lines to a hyperbola have another remarkable geometrical property. If a tangent line at a point T intersects the asymptotes at two points K and L, then T bisects the line segment KL, and the product of distances to the hyperbola's center, OK×OL is a constant.
Hyperbolic functions and equations.
Just as the sine and cosine functions give a parametric equation for the ellipse, so the hyperbolic sine and hyperbolic cosine give a parametric equation for the hyperbola.
As
one has for any hyperbolic angle formula_63 that the point
satisfies the equation
which is the equation of a hyperbola relative its canonical coordinate system.
When "μ" varies over the interval formula_67 one gets with this formula all points formula_68 on the right branch of the hyperbola.
The left branch for which formula_69 is in the same way obtained as
In the figure the points formula_72 given by
for
on the left branch of a hyperbola with eccentricity 1.2 are marked as dots.
Relation to other conic sections.
There are three types of conic sections: hyperbolas, ellipses and parabolas. Since the parabola may be seen as a limiting case poised exactly between an ellipse and a hyperbola, there are only two non-limiting types, ellipses and hyperbolas. These two types are related in that formulae for one type can often be applied to the other.
The canonical equation for a hyperbola is
Any hyperbola can be rotated so that it is east-west opening and positioned with its center at the origin, so that the equation describing it is this canonical equation.
The canonical equation for the hyperbola may be seen as a version of the corresponding ellipse equation
in which the semi-minor axis length "b" is imaginary. That is, if in the ellipse equation "b" is replaced by "ib" where "b" is real, one obtains the hyperbola equation.
Similarly, the parametric equations for a hyperbola and an ellipse are expressed in terms of hyperbolic and trigonometric functions, respectively, which are again related by an imaginary circular angle, for example,
Hence, many formulae for the ellipse can be extended to hyperbolas by adding the imaginary unit "i" in front of the semi-minor axis "b" and the angle. For example, the arc length of a segment of an ellipse can be determined using an incomplete elliptic integral of the second kind. The corresponding arclength of a hyperbola is given by the same function with imaginary parameters "b" and μ, namely, "ib E(iμ, c)".
Conic section analysis of the hyperbolic appearance of circles.
Besides providing a uniform description of circles, ellipses, parabolas, and hyperbolas, conic sections can also be understood as a natural model of the geometry of perspective in the case where the scene being viewed consists of a circle, or more generally an ellipse. The viewer is typically a camera or the human eye. In the simplest case the viewer's lens is just a pinhole; the role of more complex lenses is merely to gather far more light while retaining as far as possible the simple pinhole geometry in which all rays of light from the scene pass through a single point. Once through the lens, the rays then spread out again, in air in the case of a camera, in the vitreous humor in the case of the eye, eventually distributing themselves over the film, imaging device, or retina, all of which come under the heading of image plane. The lens plane is a plane parallel to the image plane at the lens; all rays pass through a single point on the lens plane, namely the lens itself.
When the circle directly faces the viewer, the viewer's lens is on-axis, meaning on the line normal to the circle through its center (think of the axle of a wheel). The rays of light from the circle through the lens to the image plane then form a cone with circular cross section whose apex is the lens. The image plane concretely realizes the abstract cutting plane in the conic section model.
When in addition the viewer directly faces the circle, the circle is rendered faithfully on the image plane without perspective distortion, namely as a scaled-down circle. When the viewer turns attention or gaze away from the center of the circle the image plane then cuts the cone in an ellipse, parabola, or hyperbola depending on how far the viewer turns, corresponding exactly to what happens when the surface cutting the cone to form a conic section is rotated.
A parabola arises when the lens plane is tangent to (touches) the circle. A viewer with perfect 180-degree wide-angle vision will see the whole parabola; in practice this is impossible and only a finite portion of the parabola is captured on the film or retina.
When the viewer turns further so that the lens plane cuts the circle in two points, the shape on the image plane becomes that of a hyperbola. The viewer still sees only a finite curve, namely a portion of one branch of the hyperbola, and is unable to see the second branch at all, which corresponds to the portion of the circle behind the viewer, more precisely, on the same side of the lens plane as the viewer. In practice the finite extent of the image plane makes it impossible to see any portion of the circle near where it is cut by the lens plane. Further back however one could imagine rays from the portion of the circle well behind the viewer passing through the lens, were the viewer transparent. In this case the rays would pass through the image plane before the lens, yet another impracticality ensuring that no portion of the second branch could possibly be visible.
The tangents to the circle where it is cut by the lens plane constitute the asymptotes of the hyperbola. Were these tangents to be drawn in ink in the plane of the circle, the eye would perceive them as asymptotes to the visible branch. Whether they converge in front of or behind the viewer depends on whether the lens plane is in front of or behind the center of the circle respectively.
If the circle is drawn on the ground and the viewer gradually transfers gaze from straight down at the circle up towards the horizon, the lens plane eventually cuts the circle producing first a parabola then a hyperbola on the image plane as shown in Figure 10. As the gaze continues to rise the asymptotes of the hyperbola, if realized concretely, appear coming in from left and right, swinging towards each other and converging at the horizon when the gaze is horizontal. Further elevation of the gaze into the sky then brings the point of convergence of the asymptotes towards the viewer.
By the same principle with which the back of the circle appears on the image plane were all the physical obstacles to its projection to be overcome, the portion of the two tangents behind the viewer appear on the image plane as an extension of the visible portion of the tangents in front of the viewer. Like the second branch this extension materializes in the sky rather than on the ground, with the horizon marking the boundary between the physically visible (scene in front) and invisible (scene behind), and the visible and invisible parts of the tangents combining in a single X shape. As the gaze is raised and lowered about the horizon, the X shape moves oppositely, lowering as the gaze is raised and vice versa but always with the visible portion being on the ground and stopping at the horizon, with the center of the X being on the horizon when the gaze is horizontal.
All of the above was for the case when the circle faces the viewer, with only the viewer's gaze varying. When the circle starts to face away from the viewer the viewer's lens is no longer on-axis. In this case the cross section of the cone is no longer a circle but an ellipse (never a parabola or hyperbola). However the principle of conic sections does not depend on the cross section of the cone being circular, and applies without modification to the case of eccentric cones.
It is not difficult to see that even in the off-axis case a circle can appear circular, namely when the image plane (and hence lens plane) is parallel to the plane of the circle. That is, to see a circle as a circle when viewing it obliquely, look not at the circle itself but at the plane in which it lies. From this it can be seen that when viewing a plane filled with many circles, all of them will appear circular simultaneously when the plane is looked at directly.
A common misperception about the hyperbola is that it is a mathematical curve rarely if ever encountered in daily life. The reality is that one sees a hyperbola whenever catching sight of portion of a circle cut by one's lens plane (and a parabola when the lens plane is tangent to, i.e. just touches, the circle). The inability to see very much of the arms of the visible branch, combined with the complete absence of the second branch, makes it virtually impossible for the human visual system to recognize the connection with hyperbolas such as "y" = 1/"x" where both branches are on display simultaneously.
Derived curves.
Several other curves can be derived from the hyperbola by inversion, the so-called inverse curves of the hyperbola. If the center of inversion is chosen as the hyperbola's own center, the inverse curve is the lemniscate of Bernoulli; the lemniscate is also the envelope of circles centered on a rectangular hyperbola and passing through the origin. If the center of inversion is chosen at a focus or a vertex of the hyperbola, the resulting inverse curves are a limaçon or a strophoid, respectively.
Coordinate systems.
Cartesian coordinates.
An east-west opening hyperbola centered at ("h","k") has the equation
The major axis runs through the center of the hyperbola and intersects both arms of the hyperbola at the vertices (bend points) of the arms. The foci lie on the extension of the major axis of the hyperbola.
The minor axis runs through the center of the hyperbola and is perpendicular to the major axis.
In both formulas "a" is the semi-major axis (half the distance between the two arms of the hyperbola measured along the major axis), and "b" is the semi-minor axis (half the distance between the asymptotes along a line tangent to the hyperbola at a vertex).
If one forms a rectangle with vertices on the asymptotes and two sides that are tangent to the hyperbola, the sides tangent to the hyperbola are "2b" in length while the sides that run parallel to the line between the foci (the major axis) are "2a" in length. Note that "b" may be larger than "a" despite the names "minor" and "major".
If one calculates the distance from any point on the hyperbola to each focus, the absolute value of the difference of those two distances is always "2a".
The eccentricity is given by
If "c" equals the distance from the center to either focus, then
where
The distance "c" is known as the linear eccentricity of the hyperbola. The distance between the foci is 2"c" or 2"ae".
The foci for an east-west opening hyperbola are given by
and for a north-south opening hyperbola are given by
The directrices for an east-west opening hyperbola are given by
and for a north-south opening hyperbola are given by
Polar coordinates.
The polar coordinates used most commonly for the hyperbola are defined relative to the Cartesian coordinate system that has its origin in a focus and its x-axis pointing towards the origin of the "canonical coordinate system" as illustrated in the figure of the section True anomaly.
Relative to this coordinate system one has that
and the range of the true anomaly formula_88 is
With polar coordinate relative to the "canonical coordinate system"
one has that
For the right branch of the hyperbola the range of formula_93 is
Parametric equations.
"East-west opening hyperbola:"
"North-south opening hyperbola:"
In all formulae ("h","k") are the center coordinates of the hyperbola, "a" is the length of the semi-major axis, and "b" is the length of the semi-minor axis.
Elliptic coordinates.
A family of confocal hyperbolas is the basis of the system of elliptic coordinates in two dimensions. These hyperbolas are described by the equation
where the foci are located at a distance "c" from the origin on the "x"-axis, and where θ is the angle of the asymptotes with the "x"-axis. Every hyperbola in this family is orthogonal to every ellipse that shares the same foci. This orthogonality may be shown by a conformal map of the Cartesian coordinate system "w" = "z" + 1/"z", where "z"= "x" + "iy" are the original Cartesian coordinates, and "w"="u" + "iv" are those after the transformation.
Other orthogonal two-dimensional coordinate systems involving hyperbolas may be obtained by other conformal mappings. For example, the mapping "w" = "z"2 transforms the Cartesian coordinate system into two families of orthogonal hyperbolas.
Rectangular hyperbola.
A rectangular hyperbola, equilateral hyperbola, or right hyperbola is a hyperbola for which the asymptotes are perpendicular.
Rectangular hyperbolas with the coordinate axes parallel to their asymptotes have the equation
Rectangular hyperbolas have eccentricity formula_99 with semi-major axis and semi-minor axis given by formula_100.
The simplest example of rectangular hyperbolas occurs when the center ("h", "k") is at the origin:
describing quantities "x" and "y" that are inversely proportional. By rotating the coordinate axes counterclockwise by 45 degrees, with the new coordinate axes labelled formula_102 the equation of the hyperbola is given by canonical form
If the scale factor "m"=1/2, then this canonical rectangular hyperbola is the unit hyperbola.
A circumconic passing through the orthocenter of a triangle is a rectangular hyperbola.
Applications.
Sundials.
Hyperbolas may be seen in many sundials. On any given day, the sun revolves in a circle on the celestial sphere, and its rays striking the point on a sundial traces out a cone of light. The intersection of this cone with the horizontal plane of the ground forms a conic section. At most populated latitudes and at most times of the year, this conic section is a hyperbola. In practical terms, the shadow of the tip of a pole traces out a hyperbola on the ground over the course of a day (this path is called the "declination line"). The shape of this hyperbola varies with the geographical latitude and with the time of the year, since those factors affect the cone of the sun's rays relative to the horizon. The collection of such hyperbolas for a whole year at a given location was called a "pelekinon" by the Greeks, since it resembles a double-bladed axe.
Multilateration.
A hyperbola is the basis for solving multilateration problems, the task of locating a point from the differences in its distances to given points — or, equivalently, the difference in arrival times of synchronized signals between the point and the given points. Such problems are important in navigation, particularly on water; a ship can locate its position from the difference in arrival times of signals from a LORAN or GPS transmitters. Conversely, a homing beacon or any transmitter can be located by comparing the arrival times of its signals at two separate receiving stations; such techniques may be used to track objects and people. In particular, the set of possible positions of a point that has a distance difference of 2"a" from two given points is a hyperbola of vertex separation 2"a" whose foci are the two given points.
Path followed by a particle.
The path followed by any particle in the classical Kepler problem is a conic section. In particular, if the total energy "E" of the particle is greater than zero (i.e., if the particle is unbound), the path of such a particle is a hyperbola. This property is useful in studying atomic and sub-atomic forces by scattering high-energy particles; for example, the Rutherford experiment demonstrated the existence of an atomic nucleus by examining the scattering of alpha particles from gold atoms. If the short-range nuclear interactions are ignored, the atomic nucleus and the alpha particle interact only by a repulsive Coulomb force, which satisfies the inverse square law requirement for a Kepler problem.
Korteweg-de Vries equation.
The hyperbolic trig function formula_112 appears as one solution to the Korteweg-de Vries equation which describes the motion of a soliton wave in a canal.
Angle trisection.
As shown first by Apollonius of Perga, a hyperbola can be used to trisect any angle, a well studied problem of geometry. Given an angle, first draw a circle centered at its vertex O, which intersects the sides of the angle at points A and B. Next draw the line segment with endpoints A and B and its perpendicular bisector formula_113. Construct a hyperbola of eccentricity "e"=2 with formula_113 as directrix and B as a focus. Let P be the intersection (upper) of the hyperbola with the circle. Angle POB trisects angle AOB. To prove this, reflect the line segment OP about the line formula_113 obtaining the point P' as the image of P. Segment AP' has the same length as segment BP due to the reflection, while segment PP' has the same length as segment BP due to the eccentricity of the hyperbola. As OA, OP', OP and OB are all radii of the same circle (and so, have the same length), the triangles OAP', OPP' and OPB are all congruent. Therefore, the angle has been trisected, since 3×POB = AOB.
Efficient portfolio frontier.
In portfolio theory, the locus of mean-variance efficient portfolios (called the efficient frontier) is the upper half of the east-opening branch of a hyperbola drawn with the portfolio return's standard deviation plotted horizontally and its expected value plotted vertically; according to this theory, all rational investors would choose a portfolio characterized by some point on this locus.
Extensions.
The three-dimensional analog of a hyperbola is a hyperboloid. Hyperboloids come in two varieties, those of one sheet and those of two sheets. A simple way of producing a hyperboloid is to rotate a hyperbola about the axis of its foci or about its symmetry axis perpendicular to the first axis; these rotations produce hyperboloids of two and one sheet, respectively.

</doc>
<doc id="1237700" url="https://en.wikipedia.org/wiki?curid=1237700" title="Hyperbolic 3-manifold">
Hyperbolic 3-manifold

A hyperbolic 3-manifold is a 3-manifold equipped with a complete Riemannian metric of constant sectional curvature -1. In other words, it is the quotient of three-dimensional hyperbolic space by a subgroup of hyperbolic isometries acting freely and properly discontinuously. See also Kleinian model.
Its thick-thin decomposition has a thin part consisting of tubular neighborhoods of closed geodesics and/or ends which are the product of a Euclidean surface and the closed half-ray. The manifold is of finite volume if and only if its thick part is compact. In this case, the ends are of the form torus cross the closed half-ray and are called cusps.
Constructions.
The first cusped hyperbolic 3-manifold to be discovered was the Gieseking manifold, in 1912. It is constructed by glueing faces of an ideal hyperbolic tetrahedron together.
The complements of knots and links in the 3-sphere are frequently cusped hyperbolic manifolds. Examples include the complements of the figure-eight knot and the Borromean rings and the Whitehead link. More generally, geometrization implies that a knot which is neither a satellite knot nor a torus knot is a hyperbolic knot.
Thurston's theorem on hyperbolic Dehn surgery states that, provided a finite collection of filling slopes are avoided, the remaining Dehn fillings on hyperbolic links are hyperbolic 3-manifolds.
The Seifert–Weber space is a compact hyperbolic 3-manifold, obtained by gluing opposite faces of a dodecahedron together.
The hyperbolic volume can be defined on any closed orientable hyperbolic 3-manifold. The Weeks manifold has the smallest volume of any closed orientable hyperbolic 3-manifold.
Thurston gave a necessary and sufficient criterion for a surface bundle over the circle to be hyperbolic: the monodromy of the bundle should be pseudo-Anosov. This is part of his celebrated hyperbolization theorem for Haken manifolds.
According to Thurston's geometrization conjecture, proved by Perelman, any closed, irreducible, atoroidal 3-manifold with infinite fundamental group is hyperbolic. There is an analogous statement for 3-manifolds with boundary.

</doc>
<doc id="56567" url="https://en.wikipedia.org/wiki?curid=56567" title="Hyperbolic function">
Hyperbolic function

In mathematics, hyperbolic functions are analogs of the ordinary trigonometric, or circular functions. The basic hyperbolic functions are the hyperbolic sine "sinh" ( or ), and the hyperbolic cosine "cosh" (), from which are derived the hyperbolic tangent "tanh" ( or ), hyperbolic cosecant "csch" or "cosech" ( or ), hyperbolic secant "sech" ( or ), and hyperbolic cotangent "coth" ( or ), corresponding to the derived trigonometric functions. The inverse hyperbolic functions are the area hyperbolic sine "arsinh" (also called "asinh" or sometimes "arcsinh") and so on.
Just as the points (cos "t", sin "t") form a circle with a unit radius, the points (cosh "t", sinh "t") form the right half of the equilateral hyperbola. The hyperbolic functions take a real argument called a hyperbolic angle. The size of a hyperbolic angle is the area of its hyperbolic sector. The hyperbolic functions may be defined in terms of the legs of a right triangle covering this sector.
Hyperbolic functions occur in the solutions of some important linear differential equations, for example the equation defining a catenary, of some cubic equations, and of Laplace's equation in Cartesian coordinates. The latter is important in many areas of physics, including electromagnetic theory, heat transfer, fluid dynamics, and special relativity.
In complex analysis, the hyperbolic functions arise as the imaginary parts of sine and cosine. When considered defined by a complex variable, the hyperbolic functions are rational functions of exponentials, and are hence meromorphic.
Hyperbolic functions were introduced in the 1760s independently by Vincenzo Riccati and Johann Heinrich Lambert. Riccati used "Sc." and "Cc." ("[co]sinus circulare") to refer to circular functions and "Sh." and "Ch." ("[co]sinus hyperbolico") to refer to hyperbolic functions. Lambert adopted the names but altered the abbreviations to what they are today. The abbreviations "sh" and "ch" are still used in some other languages, like French and Russian.
Standard analytic expressions.
The hyperbolic functions are:
Hyperbolic functions can be introduced via imaginary circular angles:
where "i" is the imaginary unit with the property that .
The complex forms in the definitions above derive from Euler's formula.
Special meanings.
Hyperbolic cosine.
It can be shown that the area under the curve of cosh ("x") over a finite interval is always equal to the arc length corresponding to that interval:
Hyperbolic tangent.
The hyperbolic tangent is the solution to the differential equation formula_18 with f(0)=0 and the nonlinear boundary value problem:
Useful relations.
Odd and even functions:
Hence:
It can be seen that cosh "x" and sech "x" are even functions; the others are odd functions.
Hyperbolic sine and cosine satisfy: 
the last which is similar to the Pythagorean trigonometric identity.
One also has
for the other functions.
Sums of arguments.
particularly
Also:
Second derivatives.
The second derivatives of sinh and cosh are the same function:
All functions with this property are linear combinations of sinh and cosh, notably the exponential functions formula_43 and formula_44, and the zero function formula_45.
Standard integrals.
formula_46
The following integrals can be proved using hyperbolic substitution:
formula_47
where "C" is the constant of integration.
Taylor series expressions.
It is possible to express the above functions as Taylor series:
The function sinh "x" has a Taylor series expression with only odd exponents for "x". Thus it is an odd function, that is, −sinh "x" = sinh(−"x"), and sinh 0 = 0.
The function cosh "x" has a Taylor series expression with only even exponents for "x". Thus it is an even function, that is, symmetric with respect to the "y"-axis. The sum of the sinh and cosh series is the infinite series expression of the exponential function.
where:
Comparison with circular functions.
The hyperbolic functions represent an expansion of trigonometry beyond the circular functions. Both types depend on an argument, either circular angle or hyperbolic angle.
Since the area of a circular sector with radius "r" and angle "u" is formula_53 it will be equal to "u" when "r" = square root of 2. In the diagram such a circle is tangent to the hyperbola "xy" = 1 at (1,1). The yellow sector depicts an area and angle magnitude. Similarly, the yellow and red sectors together depict an area and hyperbolic angle magnitude.
The legs of the two right triangles with hypotenuse on the ray defining the angles are of length √2 times the circular and hyperbolic functions.
Mellon Haskell of University of California, Berkeley described the basis of hyperbolic functions in areas of hyperbolic sectors in an 1895 article in Bulletin of the American Mathematical Society (see External links). He refers to the hyperbolic angle as an invariant measure with respect to the squeeze mapping just as circular angle is invariant under rotation.
Identities.
The hyperbolic functions satisfy many identities, all of them similar in form to the trigonometric identities. In fact, Osborn's rule states that one can convert any trigonometric identity into a hyperbolic identity by expanding it completely in terms of integral powers of sines and cosines, changing sine to sinh and cosine to cosh, and switching the sign of every term which contains a product of 2, 6, 10, 14, ... sinhs. This yields for example the addition theorems
the "double argument formulas"
and the "half-argument formulas"
The derivative of sinh "x" is cosh "x" and the derivative of cosh "x" is sinh "x"; this is similar to trigonometric functions, albeit the sign is different (i.e., the derivative of cos "x" is −sin "x").
The Gudermannian function gives a direct relationship between the circular functions and the hyperbolic ones that does not involve complex numbers.
The graph of the function "a" cosh("x"/"a") is the catenary, the curve formed by a uniform flexible chain hanging freely between two fixed points under uniform gravity.
Relationship to the exponential function.
From the definitions of the hyperbolic sine and cosine, we can derive the following identities:
and
These expressions are analogous to the expressions for sine and cosine, based on Euler's formula, as sums of complex exponentials.
Hyperbolic functions for complex numbers.
Since the exponential function can be defined for any complex argument, we can extend the definitions of the hyperbolic functions also to complex arguments. The functions sinh "z" and cosh "z" are then holomorphic.
Relationships to ordinary trigonometric functions are given by Euler's formula for complex numbers:
so:
Thus, hyperbolic functions are periodic with respect to the imaginary component, with period formula_64 (formula_65 for hyperbolic tangent and cotangent).

</doc>
<doc id="241291" url="https://en.wikipedia.org/wiki?curid=241291" title="Hyperbolic geometry">
Hyperbolic geometry

In mathematics, hyperbolic geometry (also called Bolyai–Lobachevskian geometry or Lobachevskian geometry) is a non-Euclidean geometry. In hyperbolic geometry the parallel postulate of Euclidean geometry is replaced with:
Hyperbolic plane geometry is also the geometry of saddle surface or pseudospherical surfaces, surfaces with a constant negative Gaussian curvature.
A modern use of hyperbolic geometry is in the theory of special relativity, particularly Minkowski spacetime and gyrovector space.
When geometers first realised they worked with something else than the standard Euclidean geometry they described their geometry under many different names; Felix Klein finally gave the subject the name hyperbolic geometry. It was for putting it in the now rarely used sequence elliptic geometry (spherical geometry), parabolic geometry (Euclidean geometry), and hyperbolic geometry.
In Russia it is commonly called Lobachevskian geometry after one of its discoverers, the Russian geometer Nikolai Lobachevsky.
This page is mainly about the 2 dimensional or plane hyperbolic geometry and the differences and similarities between Euclidean and hyperbolic geometry.
Hyperbolic geometry can be extended to three and more dimensions; see hyperbolic space for more on the three and higher dimensional cases.
Properties.
Relation to Euclidean geometry.
Hyperbolic geometry is more closely related to Euclidean geometry than it seems: the only axiomatic difference is the parallel postulate. 
When the parallel postulate is removed from Euclidean geometry the resulting geometry is absolute geometry.
There are two kinds of absolute geometry, Euclidean and hyperbolic.
All theorems of absolute geometry, including the first 28 propositions of book one of Euclid's Elements are valid in Euclidean and hyperbolic geometry.
Propositions 27 and 28 of Book One of Euclid's Elements prove the existence of parallel/non-intersecting lines.
This difference also has many consequences: concepts that are equivalent in Euclidean geometry are not equivalent in hyperbolic geometry; new concepts need to be introduced.
Further, because of the angle of parallelism hyperbolic geometry has an absolute scale, a relation between distance and angle measurements.
Lines.
Hyperbolic geometry also has several properties that differ from those of Euclidean geometry. There are at least two distinct lines through a point "P" not on a line "R" that do not intersect "R", where all lie in one plane. This means that there are through "P" an infinite number of coplanar lines that do not intersect "R".
These non-intersecting lines are divided into two classes: Two of them ("x" and "y" in the diagram) are limiting parallels: they asymptotically approach "R". The remainder are called "ultraparallel" and will eventually diverge from "R" on both sides of "P".
The limiting parallels make an angle "θ" with "PB"; this angle depends only on the Gaussian curvature of the plane and the distance "PB" and is called the angle of parallelism.
For ultraparallel lines, the ultraparallel theorem states that there is a unique line in the hyperbolic plane that is perpendicular to each pair of ultraparallel lines.
In hyperbolic geometry, there is no line that remains equidistant from another. Instead, the points that have all the same orthogonal distance from a given line are on a curve called a hypercycle.
Circles and disks.
In hyperbolic geometry, the circumference of a circle of radius "r" is greater than formula_1.
Let formula_2, where formula_3 is the (negative) Gaussian curvature of the plane.
Then the circumference of a circle of radius "r" is equal to:
And the area of the enclosed disk is:
Hypercycles and horocycles.
In hyperbolic geometry, there is no line that remains equidistant from another. Instead, the points that have all the same orthogonal distance from a given line are on a curve called a hypercycle.
Another special curve is the horocycle, a curve whose normal radii are limiting parallel to each other (all converge asymptotically in one direction to the same ideal point). Through every pair of points there are 2 horocycles. The centres of the horocycles are the ideal points of the perpendicular bisector of the line-segment between them.
3 distinct points lie on either a line, a hypercycle, a horocycle, or a circle.
The length of the line-segment is the shortest length between two points. The arc-length of an hypercycle connecting two points is longer that that of the line segment and shorter than that of a horocycle, connecting the same two points. The arclength of both horocycles connecting two points are equal. The arc-length of a circle between two points is larger the arc-length of a horocycles connecting two points.
Triangles.
Unlike Euclidean triangles, where the angles always add up to π radians (180°, a straight angle), in hyperbolic geometry the sum of the angles of a hyperbolic triangle is always strictly less than π radians. The difference is referred to as the defect.
The area of a hyperbolic triangle is given by its defect in radians multiplied by "R"2. As a consequence, all hyperbolic triangles have an area that is less than "R"2π. The area of a hyperbolic ideal triangle in which all three angles are 0° is equal to this maximum.
As in Euclidean geometry, each hyperbolic triangle has an inscribed circle. In hyperbolic geometry, if its vertices lie on an horocycle or hypercycle, a triangle has no circumscribed circle.
As in spherical and elliptical geometry, in hyperbolic geometry the only similar triangles are congruent triangles.
Standardized Gaussian curvature.
Though hyperbolic geometry applies for any surface with a constant negative Gaussian curvature, it is usual to assume a scale in which the curvature "K" is −1.
This results in some formulas becoming simpler. Some examples are:
Cartesian-like coordinate systems.
In hyperbolic geometry rectangles do not exist. (the sum of the angles of a rectangle is always less than 4 right angles) also there are no equidistant lines. (see hypercycles above). This all has influences on the coordinate systems.
There are however different coordinate systems for hyperbolic plane geometry. All are based around choosing a point (the origin) on a chosen directed line (the "x"-axis) and after that many choices exist.
The Lobachevski coordinates "x" and "y" are found by dropping a perpendicular onto the "x"-axis. "x" will be the label of the foot of the perpendicular. "y" will be the distance along the perpendicular of the given point from its foot (positive on one side and negative on the other).
Another coordinate system measures the distance from the point to the horocycle trough the origin centered around formula_6 and the length along this horocycle.
Other coordinate systems use the Klein model or the Poincare disk model described below, and take the Euclidean coordinates as hyperbolic.
Distance.
Construct a Cartesian-like coordinate system as follows. Choose a line (the "x"-axis) in the hyperbolic plane (with a standardized curvature of -1) and label the points on it by their distance from an origin ("x"=0) point on the "x"-axis (positive on one side and negative on the other). For any point in the plane, one can define coordinates "x" and "y" by dropping a perpendicular onto the "x"-axis. "x" will be the label of the foot of the perpendicular. "y" will be the distance along the perpendicular of the given point from its foot (positive on one side and negative on the other). Then the distance between two such points will be
This formula can be derived from the formulas about hyperbolic triangles.
The corresponding metric tensor is: formula_8.
In this coordinate system, straight lines are either perpendicular to the "x"-axis (with equation "x" = a constant) or described by equations of the form
where "A" and "B" are real parameters which characterize the straight line.
History.
Since the publication of Euclid's Elements circa 300 BCE, many geometers made attempts to prove the parallel postulate. Some tried to prove it by assuming its negation and trying to derive a contradiction. Foremost among these were Proclus, Ibn al-Haytham (Alhacen), Omar Khayyám, Nasīr al-Dīn al-Tūsī, Witelo, Gersonides, Alfonso, and later Giovanni Gerolamo Saccheri, John Wallis, Johann Heinrich Lambert, and Legendre.
Their attempts were doomed to failure (as we now know, the parallel postulate is not provable from the other postulates), but their efforts led to the discovery of hyperbolic geometry.
The theorems of Alhacen, Khayyam and al-Tūsī on quadrilaterals, including the Ibn al-Haytham–Lambert quadrilateral and Khayyam–Saccheri quadrilateral, were the first theorems on hyperbolic geometry. Their works on hyperbolic geometry had a considerable influence on its development among later European geometers, including Witelo, Gersonides, Alfonso, John Wallis and Saccheri.
In the 18th century, Johann Heinrich Lambert introduced the hyperbolic functions and computed the area of a hyperbolic triangle.
19th century developments.
In the 19th century, hyperbolic geometry was explored extensively by János Bolyai, Nikolai Ivanovich Lobachevsky and Carl Friedrich Gauss. Other than their predecessors, who just wanted to eliminate the parallel postulate from the axioms of Euclidean geometry, Gauss, Bolyai and Lobachevsky realized they had discovered a new geometry.
Gauss wrote in an 1824 letter to Franz Taurinus that he had constructed it, but Gauss did not publish his work. 
Gauss called it "non-Euclidean geometry" causing several modern authors still consider "non-Euclidean geometry" and "hyperbolic geometry" to be synonyms.
Lobachevsky published in 1830, while Bolyai discovered it independently and published in 1832.
In 1868, Eugenio Beltrami provided models (see below) of hyperbolic geometry, and used this to prove that hyperbolic geometry was consistent if and only if Euclidean geometry was.
The term "hyperbolic geometry" was introduced by Felix Klein in 1871. Klein followed an initiative of Arthur Cayley to use the transformations of projective geometry to produce isometries. The idea used a conic section or quadric to define a region, and used cross ratio to define a metric . The projective transformations that leave the conic section or quadric stable are the isometries. "Klein showed that if the Cayley absolute is a real curve then the part of the projective plane in its interior is isometric to the hyperbolic plane..."
For more history, see article on non-Euclidean geometry, and the references Coxeter and Milnor.
Philosophical consequences.
The discovery of hyperbolic geometry had important philosophical consequences. Before its discovery many philosophers (for example Hobbes and Spinoza) viewed philosophical rigour in terms of the "geometrical method", referring to the method of reasoning used in "Euclid's Elements".
Kant in the "Critique of Pure Reason" came to the conclusion that space (in Euclidean geometry) and time are not discovered by humans as objective features of the world, but are part of an unavoidable systematic framework for organizing our experiences.
It is said that Gauss did not publish anything about hyperbolic geometry out of fear of the "uproar of the Boeotians", which would ruin his status as "princeps mathematicorum" (Latin, "the Prince of Mathematicians"). 
The "uproar of the Boeotians" came and went, and gave an impetus to great improvements in mathematical rigour, analytical philosophy and logic. Hyperbolic geometry was finally proved consistent and is therefore another valid geometry.
Geometry of the universe (Spatial dimensions only).
Because Euclidean, hyperbolic and elliptic geometry are all consistent, the question arises: which is the real geometry of space, and if it is hyperbolic or elliptic, what is its curvature?
Lobachevsky had already tried to measure the curvature of the universe by measuring the parallax of Sirius and treating Sirius as the ideal point of an angle of parallelism. He realised that his measurements were not precise enough to give a definite answer, but he did reach the conclusion that if the geometry of the universe is hyperbolic, then the absolute length is at least one million times the diameter of the earth's orbit (, 10 parsec).
Some argue that his measurements were methodologically flawed.
Henri Poincaré, with his sphere-world thought experiment, came to the conclusion that everyday experience does not necessarily rule out other geometries.
The geometrization conjecture gives a complete list of eight possibilities for the fundamental geometry of our space. The problem in determining which one applies is that, to reach a definitive answer, we need to be able to look at extremely large shapes – much larger than anything on Earth or perhaps even in our galaxy.
Geometry of the universe (Special relativity).
Special relativity places space and time on equal footing, so that one considers the geometry of a unified spacetime instead of considering space and time separately. Minkowski geometry replaces Galilean geometry (which is the three-dimensional Euclidean space with time of Galilean relativity).
In relativity, rather than considering Euclidean, hyperbolic and elliptic geometries, the appropriate geometries to consider are Minkowski space, de Sitter space and anti-de Sitter space.
The space of relativistic velocities has a three-dimensional hyperbolic geometry, where the distance function is determined from the relative velocities of "nearby" points (velocities).
Hyperbolic plane geometry as the geometry of saddle surfaces.
Hyperbolic plane geometry is the geometry of saddle surfaces with constant negative Gaussian curvature (for example the pseudosphere).
By Hilbert's theorem, it is not possible to isometrically immerse a complete hyperbolic plane (a complete regular surface of constant negative Gaussian curvature) in a three-dimensional Euclidean space.
There exist various pseudospherical surfaces that have a finite area of constant negative Gaussian curvature.
Physical realizations of the hyperbolic plane.
The hyperbolic plane is a plane where every point is a saddle point. By Hilbert's theorem, it is not possible to isometrically immerse a complete hyperbolic plane (a complete regular surface of constant negative Gaussian curvature) in a three-dimensional Euclidean space.
There exist various pseudospheres in Euclidean space that have a finite area of constant negative Gaussian curvature.
Other useful models of hyperbolic geometry exist in Euclidean space, in which the metric is not preserved. A particularly well-known paper model based on the pseudosphere is due to William Thurston.
The art of crochet has been used (see ) to demonstrate hyperbolic planes with the first being made by Daina Taimina.
In 2000, Keith Henderson demonstrated a quick-to-make paper model dubbed the "hyperbolic soccerball" (more precisely, a truncated order-7 triangular tiling).
Instructions on how to make a hyperbolic quilt, designed by Helaman Ferguson, has been made available by Jeff Weeks.
Models of the hyperbolic plane.
There are different pseudospherical surfaces that have for a large area a constant negative Gaussian curvature, the pseudosphere being the best well known of them.
But it is easier to do hyperbolic geometry on other models.
There are four models commonly used for hyperbolic geometry: the Klein model, the Poincaré disk model, the Poincaré half-plane model, and the Lorentz or hyperboloid model. These models define a hyperbolic plane which satisfies the axioms of a hyperbolic geometry.
Despite their names, the first three mentioned above were introduced as models of hyperbolic space by Beltrami, not by Poincaré or Klein. All these models are extendable to more dimensions.
The Beltrami–Klein model.
The Beltrami–Klein model, also known as the projective disk model, Klain disk model and Klein model, is named after Eugenio Beltrami and Felix Klein.
For the two dimensions this model uses the interior of the unit circle for the complete hyperbolic plane, and the chords of this circle are the hyperbolic lines.
For higher dimensions this model uses the interior of the unit ball, and the chords of this "n"-ball are the hyperbolic lines.
The Poincaré disk model.
The Poincaré disk model, also known as the conformal disk model, also employs the interior of the unit circle, but lines are represented by arcs of circles that are orthogonal to the boundary circle, plus diameters of the boundary circle.
The Poincaré half-plane model.
The Poincaré half-plane model takes one-half of the Euclidean plane, bounded by a line "B" of the plane, to be a model of the hyperbolic plane. The line "B" is not included in the model.
The Euclidean plane may be taken to be a plane with the Cartesian coordinate system and the x-axis is taken as line "B" and the half plane is the upper half ("y" > 0 ) of this plane.
The hyperboloid model.
The hyperboloid model or Lorentz model employs a 2-dimensional hyperboloid of revolution (of two sheets, but using one) embedded in 3-dimensional Minkowski space. This model is generally credited to Poincaré, but Reynolds says that Wilhelm Killing used this model in 1885
Other models of the hyperbolic plane.
The hemisphere model.
The hemisphere model is not often used as model by itself, but it functions as a useful tool for visualising transformations between the other models.
The hemisphere model uses the upper half of the unit sphere:
formula_10
The hemisphere model is part of a Riemann sphere, and different projections give different models of the hyperbolic plane:
See further: Connection between the models (below)
The Gans Model.
The Gans model or flattened hyperboloid model
In 1966 David Gans proposed this model in the journal American Mathematical Monthly It is an orthographic projection of the hyperboloid model onto the xy-plane.
This model is not as widely used as other models but nevertheless is quite useful in the understanding of hyperbolic geometry.
Connection between the models.
All models essentially describe the same structure. The difference between them is that they represent different coordinate charts laid down on the same metric space, namely the hyperbolic space.
The characteristic feature of the hyperbolic space itself is that it has a constant negative Gaussian curvature, which is indifferent to the coordinate chart used. The geodesics are similarly invariant: that is, geodesics map to geodesics under coordinate transformation.
Hyperbolic geometry generally is introduced in terms of the geodesics and their intersections on the hyperbolic space.
Once we choose a coordinate chart (one of the "models"), we can always embed it in a Euclidean space of same dimension, but the embedding is clearly not isometric (since the curvature of Euclidean space is 0). The hyperbolic space can be represented by infinitely many different charts; but the embeddings in Euclidean space due to these four specific charts show some interesting characteristics.
Since the four models describe the same metric space, each can be transformed into the other.
See, for example:
Hyperbolic geometry in art.
M. C. Escher's famous prints "Circle Limit III" and "Circle Limit IV"
illustrate the conformal disc model (Poincaré disk model) quite well. The white lines in "III" are not quite geodesics (they are hypercycles), but are quite close to them. It is also possible to see quite plainly the negative curvature of the hyperbolic plane, through its effect on the sum of angles in triangles and squares.
For example, in "Circle Limit III" every vertex belongs to three triangles and three squares. In the Euclidean plane, their angles would sum to 450°; i.e., a circle and a quarter. From this we see that the sum of angles of a triangle in the hyperbolic plane must be smaller than 180°. Another visible property is exponential growth. In "Circle Limit III", for example, one can see that the number of fishes within a distance of "n" from the center rises exponentially. The fishes have equal hyperbolic area, so the area of a ball of radius "n" must rise exponentially in "n".
The art of crochet has been used to demonstrate hyperbolic planes with the first being made by Daina Taimina, whose book "Crocheting Adventures with Hyperbolic Planes" won the 2009 Bookseller/Diagram Prize for Oddest Title of the Year.
Higher dimensions.
Hyperbolic geometry in not limited to 2 dimensions; a hyperbolic geometry exists for every higher number of dimensions.
Homogeneous structure.
Hyperbolic space of dimension "n" is a special case of a Riemannian symmetric space of noncompact type, as it is isomorphic to the quotient 
The orthogonal group acts by norm-preserving transformations on Minkowski space R1,"n", and it acts transitively on the two-sheet hyperboloid of norm 1 vectors. Timelike lines (i.e., those with positive-norm tangents) through the origin pass through antipodal points in the hyperboloid, so the space of such lines yields a model of hyperbolic "n"-space. The stabilizer of any particular line is isomorphic to the product of the orthogonal groups O("n") and O(1), where O("n") acts on the tangent space of a point in the hyperboloid, and O(1) reflects the line through the origin. Many of the elementary concepts in hyperbolic geometry can be described in linear algebraic terms: geodesic paths are described by intersections with planes through the origin, dihedral angles between hyperplanes can be described by inner products of normal vectors, and hyperbolic reflection groups can be given explicit matrix realizations.
In small dimensions, there are exceptional isomorphisms of Lie groups that yield additional ways to consider symmetries of hyperbolic spaces. For example, in dimension 2, the isomorphisms allow one to interpret the upper half plane model as the quotient and the Poincaré disc model as the quotient . In both cases, the symmetry groups act by fractional linear transformations, since both groups are the orientation-preserving stabilizers in of the respective subspaces of the Riemann sphere. The Cayley transformation not only takes one model of the hyperbolic plane to the other, but realizes the isomorphism of symmetry groups as conjugation in a larger group. In dimension 3, the fractional linear action of on the Riemann sphere is identified with the action on the conformal boundary of hyperbolic 3-space induced by the isomorphism . This allows one to study isometries of hyperbolic 3-space by considering spectral properties of representative complex matrices. For example, parabolic transformations are conjugate to rigid translations in the upper half-space model, and they are exactly those transformations that can be represented by unipotent upper triangular matrices.

</doc>
<doc id="1575825" url="https://en.wikipedia.org/wiki?curid=1575825" title="Hyperbolic partial differential equation">
Hyperbolic partial differential equation

In mathematics, a hyperbolic partial differential equation of order "n" is a partial differential equation (PDE) that, roughly speaking, has a well-posed initial value problem for the first "n"−1 derivatives. More precisely, the Cauchy problem can be locally solved for arbitrary initial data along any non-characteristic hypersurface. Many of the equations of mechanics are hyperbolic, and so the study of hyperbolic equations is of substantial contemporary interest. The model hyperbolic equation is the wave equation. In one spatial dimension, this is
The equation has the property that, if "u" and its first time derivative are arbitrarily specified initial data on the line "t" = 0 (with sufficient smoothness properties), then there exists a solution for all time "t".
The solutions of hyperbolic equations are "wave-like." If a disturbance is made in the initial data of a hyperbolic differential equation, then not every point of space feels the disturbance at once. Relative to a fixed time coordinate, disturbances have a finite propagation speed. They travel along the characteristics of the equation. This feature qualitatively distinguishes hyperbolic equations from elliptic partial differential equations and parabolic partial differential equations. A perturbation of the initial (or boundary) data of an elliptic or parabolic equation is felt at once by essentially all points in the domain.
Although the definition of hyperbolicity is fundamentally a qualitative one, there are precise criteria that depend on the particular kind of differential equation under consideration. There is a well-developed theory for linear differential operators, due to Lars Gårding, in the context of microlocal analysis. Nonlinear differential equations are hyperbolic if their linearizations are hyperbolic in the sense of Gårding. There is a somewhat different theory for first order systems of equations coming from systems of conservation laws.
Definition.
A partial differential equation is hyperbolic at a point "P" provided that the Cauchy problem is uniquely solvable in a neighborhood of "P" for any initial data given on a non-characteristic hypersurface passing through "P". Here the prescribed initial data consist of all (transverse) derivatives of the function on the surface up to one less than the order of the differential equation.
Examples.
By a linear change of variables, any equation of the form
with
can be transformed to the wave equation, apart from lower order terms which are inessential for the qualitative understanding of the equation. This definition is analogous to the definition of a planar hyperbola.
The one-dimensional wave equation:
is an example of a hyperbolic equation. The two-dimensional and three-dimensional wave equations also fall into the category of hyperbolic PDE. This type of second-order hyperbolic partial differential equation may be transformed to a hyperbolic system of first-order differential equations.
Hyperbolic system of partial differential equations.
The following is a system of formula_5 first order partial differential equations for formula_5 unknown functions formula_7, formula_8, where formula_9:
where formula_11 are once continuously differentiable functions, nonlinear in general.
Next, for each formula_12 a matrix formula_13 is defined
The system formula_15 is hyperbolic if for all formula_16 the matrix formula_17
has only real eigenvalues and is diagonalizable.
If the matrix formula_18 has "n" "distinct" real eigenvalues, it follows that it's diagonalizable. In this case the system formula_15 is called strictly hyperbolic.
If the matrix formula_18 is symmetric, it follows that it's diagonalizable and the eigenvalues are real. In this case the system formula_15 is called symmetric hyperbolic.
Hyperbolic system and conservation laws.
There is a connection between a hyperbolic system and a conservation law. Consider a hyperbolic system of one partial differential equation for one unknown function formula_22. Then the system formula_15 has the form
Here, formula_25 can be interpreted as a quantity that moves around according to the flux given by formula_26. To see that the quantity formula_25 is conserved, integrate formula_28 over a domain formula_29
If formula_25 and formula_32 are sufficiently smooth functions, we can use the divergence theorem and change the order of the integration and formula_33 to get a conservation law for the quantity formula_25 in the general form
which means that the time rate of change of formula_25 in the domain formula_29 is equal to the net flux of formula_25 through its boundary formula_39. Since this is an equality, it can be concluded that formula_25 is conserved within formula_29.

</doc>
<doc id="318742" url="https://en.wikipedia.org/wiki?curid=318742" title="Hyperbolic space">
Hyperbolic space

In mathematics, hyperbolic space is a homogeneous space that has a constant negative curvature, where in this case the curvature is the sectional curvature.
It is hyperbolic geometry in more than 2 dimensions, and is distinguished from Euclidean spaces with zero curvature that define the Euclidean geometry, and elliptic geometry that have a constant positive curvature.
When embedded to a Euclidean space (of a higher dimension), every point of a hyperbolic space is a saddle point. Another distinctive property is the amount of space covered by the "n"-ball in hyperbolic "n"-space: it increases exponentially with respect to the radius of the ball for large radii, rather than polynomially.
Formal definition.
Hyperbolic "n"-space, denoted H"n", is the maximally symmetric, simply connected, "n"-dimensional Riemannian manifold with a constant negative sectional curvature. 
Hyperbolic space is a space exhibiting hyperbolic geometry. 
It is the negative-curvature analogue of the "n"-sphere. Although hyperbolic space H"n" is diffeomorphic to R"n", its negative-curvature metric gives it very different geometric properties.
Hyperbolic 2-space, H2, is also called the hyperbolic plane.
Models of hyperbolic space.
Hyperbolic space, developed independently by Nikolai Lobachevsky and János Bolyai, is a geometrical space analogous to Euclidean space, but such that Euclid's parallel postulate is no longer assumed to hold. Instead, the parallel postulate is replaced by the following alternative (in two dimensions):
It is then a theorem that there are infinitely many such lines through "P". This axiom still does not uniquely characterize the hyperbolic plane up to isometry; there is an extra constant, the curvature , which must be specified. However, it does uniquely characterize it up to homothety, meaning up to bijections which only change the notion of distance by an overall constant. By choosing an appropriate length scale, one can thus assume, without loss of generality, that .
Models of hyperbolic spaces that can be embedded in a flat (e.g. Euclidean) spaces may be constructed. In particular, the existence of model spaces implies that the parallel postulate is logically independent of the other axioms of Euclidean geometry.
There are several important models of hyperbolic space: the Klein model, the hyperboloid model, the Poincaré ball model and the Poincaré half space model. These all model the same geometry in the sense that any two of them can be related by a transformation that preserves all the geometrical properties of the space, including isometry (though not with respect to the metric of a Euclidean embedding).
The hyperboloid model.
The hyperboloid model realizes hyperbolic space as a hyperboloid in R"n"+1 = {("x"0...,"x""n")|"x""i"∈R, "i"=0,1...,"n"}. The hyperboloid is the locus H"n" of points whose coordinates satisfy
In this model a "line" (or geodesic) is the curve formed by the intersection of H"n" with a plane through the origin in R"n"+1.
The hyperboloid model is closely related to the geometry of Minkowski space. The quadratic form
which defines the hyperboloid, polarizes to give the bilinear form
The space R"n"+1, equipped with the bilinear form "B", is an ("n"+1)-dimensional Minkowski space R"n",1.
One can associate a "distance" on the hyperboloid model by defining the distance between two points "x" and "y" on "H" to be
This function satisfies the axioms of a metric space. It is preserved by the action of the Lorentz group on R"n",1. Hence the Lorentz group acts as a transformation group preserving isometry on H"n".
The Klein model.
An alternative model of hyperbolic geometry is on a certain domain in projective space. The Minkowski quadratic form "Q" defines a subset given as the locus of points for which in the homogeneous coordinates "x". The domain "U""n" is the Klein model of hyperbolic space.
The lines of this model are the open line segments of the ambient projective space which lie in "U""n". The distance between two points "x" and "y" in "U""n" is defined by
This is well-defined on projective space, since the ratio under the inverse hyperbolic cosine is homogeneous of degree 0.
This model is related to the hyperboloid model as follows. Each point corresponds to a line "L""x" through the origin in R"n"+1, by the definition of projective space. This line intersects the hyperboloid H"n" in a unique point. Conversely, through any point on H"n", there passes a unique line through the origin (which is a point in the projective space). This correspondence defines a bijection between "U""n" and H"n". It is an isometry, since evaluating along reproduces the definition of the distance given for the hyperboloid model.
The Poincaré ball model.
A closely related pair of models of hyperbolic geometry are the Poincaré ball and Poincaré half-space models.
The ball model comes from a stereographic projection of the hyperboloid in R"n"+1 onto the hyperplane {"x"0 = 0}. In detail, let "S" be the point in R"n",1 with coordinates (−1,0,0...,0): the "South pole" for the stereographic projection. For each point "P" on the hyperboloid H"n", let "P"∗ be the unique point of intersection of the line "SP" with the plane {"x"0 = 0}.
This establishes a bijective mapping of Hn into the unit ball
in the plane {"x"0 = 0}.
The geodesics in this model are semicircles that are perpendicular to the boundary sphere of "B""n". Isometries of the ball are generated by spherical inversion in hyperspheres perpendicular to the boundary.
The Poincaré half space model.
The half-space model results from applying inversion in a circle with centre a boundary point of the Poincaré ball model "B""n" above and a radius of twice the radius.
This sends circles to circles and lines, and is moreover a conformal transformation. Consequently, the geodesics of the half-space model are lines and circles perpendicular to the boundary hyperplane.
Hyperbolic manifolds.
Every complete, connected, simply connected manifold of constant negative curvature −1 is isometric to the real hyperbolic space H"n". As a result, the universal cover of any closed manifold "M" of constant negative curvature −1, which is to say, a hyperbolic manifold, is H"n". Thus, every such "M" can be written as H"n"/Γ where Γ is a torsion-free discrete group of isometries on H"n". That is, Γ is a lattice in SO+("n",1).
Riemann surfaces.
Two-dimensional hyperbolic surfaces can also be understood according to the language of Riemann surfaces. According to the uniformization theorem, every Riemann surface is either elliptic, parabolic or hyperbolic. Most hyperbolic surfaces have a non-trivial fundamental group π1=Γ; the groups that arise this way are known as Fuchsian groups. The quotient space H²/Γ of the upper half-plane modulo the fundamental group is known as the Fuchsian model of the hyperbolic surface. The Poincaré half plane is also hyperbolic, but is simply connected and noncompact. It is the universal cover of the other hyperbolic surfaces.
The analogous construction for three-dimensional hyperbolic surfaces is the Kleinian model.

</doc>
<doc id="1129026" url="https://en.wikipedia.org/wiki?curid=1129026" title="Hyperon">
Hyperon

In particle physics, a hyperon is any baryon containing one or more strange quarks, but no charm, bottom, or top quark.
Properties and behavior of hyperons.
Being baryons, all hyperons are fermions. That is, they have half-integer spin and obey Fermi–Dirac statistics. They all interact via the strong nuclear force, making them types of hadron. They are composed of three light quarks, at least one of which is a strange quark, which makes them strange baryons. Hyperons decay weakly with non-conserved parity.
List of hyperons.
Notes:
It takes multiple flavor-changing weak decays for it to decay into a proton or neutron. Murray Gell-Mann's and Yuval Ne'eman's SU(3) model (sometimes called the Eightfold Way) predicted this hyperon's existence, mass and that it will only undergo weak decay processes. Experimental evidence for its existence was discovered in 1964 at Brookhaven National Laboratory. Further examples of its formation and observation using particle accelerators confirmed the SU(3) model.
Hyperon research.
The first research into hyperons happened in the 1950s, and spurred physicists on to the creation of an organized classification of particles. Today, research in this area is carried out on data taken at many facilities around the world, including CERN, Fermilab, SLAC, JLAB, Brookhaven National Laboratory, KEK, and others. Physics topics include searches for CP violation, measurements of spin, studies of excited states (commonly referred to as "spectroscopy"), and hunts for exotic states such as pentaquarks and dibaryons.

</doc>
<doc id="15069" url="https://en.wikipedia.org/wiki?curid=15069" title="Identity function">
Identity function

In mathematics, an identity function, also called an identity relation or identity map or identity transformation, is a function that always returns the same value that was used as its argument. In equations, the function is given by .
Definition.
Formally, if "M" is a set, the identity function "f" on "M" is defined to be that function with domain and codomain "M" which satisfies
In other words, the function assigns to each element "x" of "M" the element "x" of "M".
The identity function "f" on "M" is often denoted by id"M".
In set theory, where a function is defined as a particular kind of binary relation, the identity function is given by the identity relation, or "diagonal" of "M".
Algebraic property.
If is any function, then we have (where "∘" denotes function composition). In particular, id"M" is the identity element of the monoid of all functions from "M" to "M".
Since the identity element of a monoid is unique, one can alternately define the identity function on "M" to be this identity element. Such a definition generalizes to the concept of an identity morphism in category theory, where the endomorphisms of "M" need not be functions.

</doc>
<doc id="59718" url="https://en.wikipedia.org/wiki?curid=59718" title="Identity matrix">
Identity matrix

In linear algebra, the identity matrix or unit matrix of size "n" is the "n" × "n" square matrix with ones on the main diagonal and zeros elsewhere. It is denoted by "I""n", or simply by "I" if the size is immaterial or can be trivially determined by the context. (In some fields, such as quantum mechanics, the identity matrix is denoted by a boldface one, 1; otherwise it is identical to "I".) Less frequently, some mathematics books use "U" or "E" to represent the identity matrix, meaning "unit matrix" and the German word "Einheitsmatrix", respectively.
When "A" is "m"×"n", it is a property of matrix multiplication that
In particular, the identity matrix serves as the unit of the ring of all "n"×"n" matrices, and as the identity element of the general linear group GL("n") consisting of all invertible "n"×"n" matrices. (The identity matrix itself is invertible, being its own inverse.)
Where "n"×"n" matrices are used to represent linear transformations from an "n"-dimensional vector space to itself, "In" represents the identity function, regardless of the basis.
The "i"th column of an identity matrix is the unit vector "ei". It follows that the determinant of the identity matrix is 1 and the trace is "n".
Using the notation that is sometimes used to concisely describe diagonal matrices, we can write:
It can also be written using the Kronecker delta notation:
The identity matrix also has the property that, when it is the product of two square matrices, the matrices can be said to be the inverse of one another.
The identity matrix of a given size is the only idempotent matrix of that size having full rank. That is, it is the only matrix such that (a) when multiplied by itself the result is itself, and (b) all of its rows, and all of its columns, are linearly independent.
The principal square root of an identity matrix is itself, and this is its only positive definite square root. However, every identity matrix with at least two rows and columns has an infinitude of symmetric square roots.

</doc>
<doc id="52358" url="https://en.wikipedia.org/wiki?curid=52358" title="Imaginary unit">
Imaginary unit

The term imaginary unit or unit imaginary number refers to a solution to the equation . By convention, the solution is usually denoted . Since there is no real number with this property, it extends the real numbers, and under the assumption that the familiar properties of addition and multiplication (namely closure, associativity, commutativity and distributivity) continue to hold for this extension, the complex numbers are generated by including it.
Imaginary numbers are an important mathematical concept, which extends the real number system to the complex number system , which in turn provides at least one root for every nonconstant polynomial . (See Algebraic closure and Fundamental theorem of algebra.) The term "imaginary" is used because there is no real number having a negative square.
There are two complex square roots of , namely and , just as there are two complex square roots of every real number other than zero, which has one double square root.
In contexts where is ambiguous or problematic, or the Greek is sometimes used (see ). In the disciplines of electrical engineering and control systems engineering, the imaginary unit is often denoted by instead of , because is commonly used to denote electric current.
For the history of the imaginary unit, see .
Definition.
The imaginary number is defined solely by the property that its square is −1:
With defined this way, it follows directly from algebra that and are both square roots of −1.
Although the construction is called "imaginary", and although the concept of an imaginary number may be intuitively more difficult to grasp than that of a real number, the construction is perfectly valid from a mathematical standpoint. Real number operations can be extended to imaginary and complex numbers by treating as an unknown quantity while manipulating an expression, and then using the definition to replace any occurrence of with −1. Higher integral powers of can also be replaced with , 1, , or −1:
Similarly, as with any non-zero real number:
As a complex number, is represented in rectangular form as , having a unit imaginary component and no real component (i.e., the real component is zero). In polar form, is represented as , having an absolute value (or magnitude) of 1 and an argument (or angle) of π/2. In the complex plane (also known as the Cartesian plane), is the point located one unit from the origin along the imaginary axis (which is at a right angle to the real axis).
and.
Being a quadratic polynomial with no multiple root, the defining equation has "two" distinct solutions, which are equally valid and which happen to be additive and multiplicative inverses of each other. More precisely, once a solution of the equation has been fixed, the value , which is distinct from , is also a solution. Since the equation is the only definition of , it appears that the definition is ambiguous (more precisely, not well-defined). However, no ambiguity results as long as one or other of the solutions is chosen and labelled as "", with the other one then being labelled as . This is because, although and are not "quantitatively" equivalent (they "are" negatives of each other), there is no "algebraic" difference between and . Both imaginary numbers have equal claim to being the number whose square is −1. If all mathematical textbooks and published literature referring to imaginary or complex numbers were rewritten with replacing every occurrence of (and therefore every occurrence of replaced by ), all facts and theorems would continue to be equivalently valid. The distinction between the two roots of with one of them labelled with a minus sign is purely a notational relic; neither root can be said to be more primary or fundamental than the other, and neither of them is "positive" or "negative".
The issue can be a subtle one. The most precise explanation is to say that although the complex field, defined as , (see complex number) is unique up to isomorphism, it is "not" unique up to a "unique" isomorphism — there are exactly 2 field automorphisms of which keep each real number fixed: the identity and the automorphism sending to . See also Complex conjugate and Galois group.
A similar issue arises if the complex numbers are interpreted as 2 × 2 real matrices (see matrix representation of complex numbers), because then both
are solutions to the matrix equation
In this case, the ambiguity results from the geometric choice of which "direction" around the unit circle is "positive" rotation. A more precise explanation is to say that the automorphism group of the special orthogonal group SO (2, ) has exactly 2 elements — the identity and the automorphism which exchanges "CW" (clockwise) and "CCW" (counter-clockwise) rotations. See orthogonal group.
All these ambiguities can be solved by adopting a more rigorous definition of complex number, and explicitly "choosing" one of the solutions to the equation to be the imaginary unit. For example, the ordered pair (0, 1), in the usual construction of the complex numbers with two-dimensional vectors.
Proper use.
The imaginary unit is sometimes written in advanced mathematics contexts (as well as in less advanced popular texts). However, great care needs to be taken when manipulating formulas involving radicals. The notation is reserved either for the principal square root function, which is "only" defined for real , or for the principal branch of the complex square root function. Attempting to apply the calculation rules of the principal (real) square root function to manipulate the principal branch of the complex square root function will produce false results:
Attempting to correct the calculation by specifying both the positive and negative roots only produces ambiguous results:
Similarly:
The calculation rules
and
are only valid for real, non-negative values of and .
These problems are avoided by writing and manipulating , rather than expressions like . For a more thorough discussion, see Square root and Branch point.
Properties.
Square roots.
The square root of can be expressed as either of two complex numbers
Indeed, squaring the right-hand side gives
This result can also be derived with Euler's formula
by substituting , giving
Taking the square root of both sides gives
which, through application of Euler's formula to , gives
Similarly, the square root of can be expressed as either of two complex numbers using Euler's formula:
by substituting , giving
Taking the square root of both sides gives
which, through application of Euler's formula to , gives
Multiplying the square root of by also gives:
Multiplication and division.
Multiplying a complex number by gives:
Dividing by is equivalent to multiplying by the reciprocal of :
Using this identity to generalize division by to all complex numbers gives:
Powers.
The powers of repeat in a cycle expressible with the following pattern, where is any integer:
This leads to the conclusion that
where "mod" represents the modulo operation. Equivalently:
raised to the power of.
Making use of Euler's formula, is
where formula_35, the set of integers.
The principal value (for ) is or approximately 0.207879576...
Factorial.
The factorial of the imaginary unit is most often given in terms of the gamma function evaluated at :
Also,
Other operations.
Many mathematical operations that can be carried out with real numbers can also be carried out with , such as exponentiation, roots, logarithms, and trigonometric functions. However, it should be noted that all of the following functions are complex multi-valued functions, and it should be clearly stated which branch of the Riemann surface the function is defined on in practice. Listed below are results for the most commonly chosen branch.
A number raised to the power is:
The root of a number is:
The imaginary-base logarithm of a number is:
As with any complex logarithm, the log base {{mvar|i}} is not uniquely defined.
The cosine of {{mvar|i}} is a real number:
And the sine of {{mvar|i}} is purely imaginary:
Matrices.
When 2 × 2 real matrices "m" are used for a source, and the number one (1) is identified with the identity matrix, and minus one (−1) with the negative of the identity matrix, then there are many solutions to "m"2 = −1. In fact, there are many solutions to "m"2 = +1 and "m"2 = 0 also. Any such m can be taken as a basis vector, along with 1, to form a planar algebra.

</doc>
<doc id="211922" url="https://en.wikipedia.org/wiki?curid=211922" title="Impulse (physics)">
Impulse (physics)

In classical mechanics, impulse (symbolized by J or Imp) is the integral of a force, F, over the time interval, t, for which it acts. Since force is a vector quantity, impulse is also a vector in the same direction. Impulse applied to an object produces an equivalent vector change in its linear momentum, also in the same direction. The SI unit of impulse is the newton second (N·s), and the dimensionally equivalent unit of momentum is the kilogram meter per second (kg·m/s). The corresponding English engineering units are the pound-second (lbf·s) and the slug-foot per second (slug·ft/s).
A resultant force causes acceleration and a change in the velocity of the body for as long as it acts. A resultant force applied over a longer time therefore produces a bigger change in linear momentum than the same force applied briefly: the change in momentum is equal to the product of the average force and duration. Conversely, a small force applied for a long time produces the same change in momentum—the same impulse—as a larger force applied briefly.
The impulse is the integral of the resultant force ("F") with respect to time:
Mathematical derivation in the case of an object of constant mass.
Impulse J produced from time "t"1 to "t"2 is defined to be
where F is the resultant force applied from "t"1 to "t"2.
From Newton's second law, force is related to momentum p by
Therefore,
where Δp is the change in linear momentum from time "t"1 to "t"2. This is often called the impulse-momentum theorem.
As a result, an impulse may also be regarded as the change in momentum of an object to which a resultant force is applied. The impulse may be expressed in a simpler form when the mass is constant:
where
The term "impulse" is also used to refer to a fast-acting force or impact. This type of impulse is often "idealized" so that the change in momentum produced by the force happens with no change in time. This sort of change is a step change, and is not physically possible. However, this is a useful model for computing the effects of ideal collisions (such as in game physics engines).
Impulse has the same units and dimensions as momentum. In the International System of Units, these are . In English engineering units, they are .
Variable mass.
The application of Newton's second law for variable mass allows impulse and momentum to be used as analysis tools for jet- or rocket-propelled vehicles. In the case of rockets, the impulse imparted can be normalized by unit of propellant expended, to create a performance parameter, specific impulse. This fact can be used to derive the Tsiolkovsky rocket equation, which relates the vehicle's propulsive change in velocity to the engine's specific impulse (or nozzle exhaust velocity) and the vehicle's propellant-mass ratio.

</doc>
<doc id="1460235" url="https://en.wikipedia.org/wiki?curid=1460235" title="Indeterminate (variable)">
Indeterminate (variable)

In mathematics, and particularly in formal algebra, an indeterminate is a symbol that is treated as a variable, but does not stand for anything else but itself and is used as a placeholder in objects such as polynomials and formal power series. In particular it does not designate a constant or a parameter of the problem, it is not an unknown that could be solved for, and it is not a variable designating a function argument or being summed or integrated over; it is not any type of bound variable.
Polynomials.
A polynomial in an indeterminate "X" is an expression of the form formula_1, where the "a""i" are called the coefficients of the polynomial. Two such polynomials are equal only if the corresponding coefficients are equal. In contrast, two polynomial functions in a variable "x" may be equal or not depending on the value of "x". 
For example, the functions
are equal when "x"=3 and not equal otherwise. But the two polynomials 
are unequal since 2 does not equal 5 and 3 does not equal 2. In fact 
does not hold "unless" "a" = 2 and "b" = 3. This is because "X" is not, and does not designate, a number.
The distinction is subtle since a polynomial in "X" can be changed to a function in "x" by substitution. But the distinction is important because information may be lost when this substitution is made. Working in modulo 2:
so the polynomial function "x"−"x"2 is identically equal to 0 for "x" having any value in the modulo 2 system. But the polynomial "X"-"X"2 is not the zero polynomial since the coefficients, 0, 1 and −1, are not all zero.
Formal power series.
A formal power series in an indeterminate X is an expression of the form "a"0+"a"1"X"+"a"2"X"2+…. This is similar to the definition of a polynomial except that an infinite number of the coefficients may be nonzero. Unlike the power series encountered in calculus, questions of convergence are irrelevant. So power series that would diverge for values of "x", such as 1+"x"+2"x"2+6"x"3+…+"n"!"x""n"+…, are allowed.
As generators.
Indeterminates are useful in abstract algebra for generating mathematical structures. For example, given a field "K", the set of polynomials with coefficients in "K" is the polynomial ring with polynomial addition and multiplication as operations. If two indeterminates "X" and "Y" are used, the polynomial ring "K"["X,Y"] also uses these operations, and convention holds that "XY" = "YX".
Indeterminates may also be used to generate a free algebra over a commutative ring "A". For instance, with two indeterminates "X" and "Y" the free algebra "A"⟨"X,Y"⟩ includes sums of strings in "X" and "Y", with coefficients in "A", and with the stipulation that "XY" and "YX" are distinct.

</doc>
<doc id="14909" url="https://en.wikipedia.org/wiki?curid=14909" title="Inertia">
Inertia

Inertia is the resistance of any physical object to any change in its state of motion including changes to its speed and direction or the state of rest. It is the tendency of objects to keep moving in a straight line at constant velocity. The principle of inertia is one of the fundamental principles of classical physics that are used to describe the motion of objects and how they are affected by applied forces. Inertia comes from the Latin word, "iners", meaning idle, sluggish. Inertia is one of the primary manifestations of mass, which is a quantitative property of physical systems. Isaac Newton defined inertia as his first law in his "Philosophiæ Naturalis Principia Mathematica", which states:
In common usage, the term "inertia" may refer to an object's "amount of resistance to change in velocity" (which is quantified by its mass), or sometimes to its momentum, depending on the context. The term "inertia" is more properly understood as shorthand for "the principle of inertia" as described by Newton in his First Law of Motion: that an object not subject to any net external force moves at a constant velocity. Thus, an object will continue moving at its current velocity until some force causes its speed or direction to change.
On the surface of the Earth, inertia is often masked by the effects of friction and air resistance, both of which tend to decrease the speed of moving objects (commonly to the point of rest), and gravity. This misled the philosopher Aristotle, to believe that objects would move only as long as force was applied to them:
History and development of the concept.
Early understanding of motion.
Prior to the Renaissance, the most generally accepted theory of motion in Western philosophy was based on Aristotle (around 335 BC to 322 BC) who said that, in the absence of an external motive power, all objects (on Earth) would come to rest and that moving objects only continue to move so long as there is a power inducing them to do so. Aristotle explained the continued motion of projectiles, which are separated from their projector, by the action of the surrounding medium which continues to move the projectile in some way. Aristotle concluded that such violent motion in a void was impossible.
Despite its general acceptance, Aristotle's concept of motion was disputed on several occasions by notable philosophers over nearly two millennia. For example, Lucretius (following, presumably, Epicurus) stated that the 'default state' of matter was motion, not stasis. In the 6th century John Philoponus criticized the inconsistency between Aristotle's discussion of projectiles, where the medium keeps projectiles going, and his discussion of the void, where the medium would hinder a body's motion. Philoponus proposed that motion was not maintained by the action of a surrounding medium but by some property imparted to the object when it was set in motion. Although this was not the modern concept of inertia, for there was still the need for a power to keep a body in motion, it proved a fundamental step in that direction. This view was strongly opposed by Averroes and by many scholastic philosophers who supported Aristotle. However this view did not go unchallenged in the Islamic world, where Philoponus did have several supporters who further developed his ideas.
Theory of impetus.
In the 14th century, Jean Buridan rejected the notion that a motion-generating property, which he named "impetus", dissipated spontaneously. Buridan's position was that a moving object would be arrested by the resistance of the air and the weight of the body which would oppose its impetus. Buridan also maintained that impetus increased with speed; thus, his initial idea of impetus was similar in many ways to the modern concept of momentum. Despite the obvious similarities to more modern ideas of inertia, Buridan saw his theory as only a modification to Aristotle's basic philosophy, maintaining many other peripatetic views, including the belief that there was still a fundamental difference between an object in motion and an object at rest. Buridan also believed that impetus could be not only linear, but also circular in nature, causing objects (such as celestial bodies) to move in a circle.
Buridan's thought was followed up by his pupil Albert of Saxony (1316–1390) and the Oxford Calculators, who performed various experiments that further undermined the classical, Aristotelian view. Their work in turn was elaborated by Nicole Oresme who pioneered the practice of demonstrating laws of motion in the form of graphs.
Shortly before Galileo's theory of inertia, Giambattista Benedetti modified the growing theory of impetus to involve linear motion alone:
Benedetti cites the motion of a rock in a sling as an example of the inherent linear motion of objects, forced into circular motion.
Classical inertia.
The law of inertia states that it is the tendency of an object to resist a change in motion. According to Newton, an object will stay at rest or stay in motion (i.e. 'maintain its velocity' in modern terms) unless acted on by a net external force, whether it results from gravity, friction, contact, or some other source. The Aristotelian division of motion into mundane and celestial became increasingly problematic in the face of the conclusions of Nicolaus Copernicus in the 16th century, who argued that the earth (and everything on it) was in fact never "at rest", but was actually in constant motion around the sun. Galileo, in his further development of the Copernican model, recognized these problems with the then-accepted nature of motion and, at least partially as a result, included a restatement of Aristotle's description of motion in a void as a basic physical principle:
A body moving on a level surface will continue in the same direction at a constant speed unless disturbed. 
Galileo writes that 'all external impediments removed, a heavy body on a spherical surface concentric with the earth will maintain itself in that state in which it has been; if placed in movement towards the west (for example), it will maintain itself in that movement'. This notion which is termed 'circular inertia' or 'horizontal circular inertia' by historians of science, is a precursor to, but distinct from, Newton's notion of rectilinear inertia. For Galileo, a motion is 'horizontal' if it does not carry the moving body towards or away from the centre of the earth, and for him 'a ship, for instance, having once received some impetus through the tranquil sea, would move continually around our globe without ever stopping'.
It is also worth noting that Galileo later went on to conclude that based on this initial premise of inertia, it is impossible to tell the difference between a moving object and a stationary one without some outside reference to compare it against. This observation ultimately came to be the basis for Einstein to develop the theory of Special Relativity.
Concepts of inertia in Galileo's writings would later come to be refined, modified and codified by Isaac Newton as the first of his Laws of Motion (first published in Newton's work, "Philosophiae Naturalis Principia Mathematica", in 1687):
Unless acted upon by a net unbalanced force, an object will maintain a constant velocity.
Note that "velocity" in this context is defined as a vector, thus Newton's "constant velocity" implies both constant speed and constant direction (and also includes the case of zero speed, or no motion). Since initial publication, Newton's Laws of Motion (and by extension this first law) have come to form the basis for the branch of physics known as classical mechanics.
The actual term "inertia" was first introduced by Johannes Kepler in his "Epitome Astronomiae Copernicanae" (published in three parts from 1618–1621); however, the meaning of Kepler's term (which he derived from the Latin word for "idleness" or "laziness") was not quite the same as its modern interpretation. Kepler defined inertia only in terms of a resistance to movement, once again based on the presumption that rest was a natural state which did not need explanation. It was not until the later work of Galileo and Newton unified rest and motion in one principle that the term "inertia" could be applied to these concepts as it is today.
Nevertheless, despite defining the concept so elegantly in his laws of motion, even Newton did not actually use the term "inertia" to refer to his First Law. In fact, Newton originally viewed the phenomenon he described in his First Law of Motion as being caused by "innate forces" inherent in matter, which resisted any acceleration. Given this perspective, and borrowing from Kepler, Newton actually attributed the term "inertia" to mean "the innate force possessed by an object which resists changes in motion"; thus Newton defined "inertia" to mean the cause of the phenomenon, rather than the phenomenon itself. However, Newton's original ideas of "innate resistive force" were ultimately problematic for a variety of reasons, and thus most physicists no longer think in these terms. As no alternate mechanism has been readily accepted, and it is now generally accepted that there may not be one which we can know, the term "inertia" has come to mean simply the phenomenon itself, rather than any inherent mechanism. Thus, ultimately, "inertia" in modern classical physics has come to be a name for the same phenomenon described by Newton's First Law of Motion, and the two concepts are now considered to be equivalent.
Relativity.
Albert Einstein's theory of special relativity, as proposed in his 1905 paper, "On the Electrodynamics of Moving Bodies," was built on the understanding of inertia and inertial reference frames developed by Galileo and Newton. While this revolutionary theory did significantly change the meaning of many Newtonian concepts such as mass, energy, and distance, Einstein's concept of inertia remained unchanged from Newton's original meaning (in fact the entire theory was based on Newton's definition of inertia). However, this resulted in a limitation inherent in special relativity that the principle of relativity could only apply to reference frames that were "inertial" in nature (meaning when no acceleration was present). In an attempt to address this limitation, Einstein proceeded to develop his general theory of relativity ("The Foundation of the General Theory of Relativity," 1916), which ultimately provided a unified theory for both "inertial" and "noninertial" (accelerated) reference frames. However, in order to accomplish this, in general relativity Einstein found it necessary to redefine several fundamental concepts (such as gravity) in terms of a new concept of "curvature" of space-time, instead of the more traditional system of forces understood by Newton.
As a result of this redefinition, Einstein also redefined the concept of "inertia" in terms of geodesic deviation instead, with some subtle but significant additional implications. The result of this is that according to general relativity, when dealing with very large scales, the traditional Newtonian idea of "inertia" does not actually apply, and cannot necessarily be relied upon. Luckily, for sufficiently small regions of spacetime, the special theory can be used, in which inertia still means the same (and works the same) as in the classical model.
Another profound conclusion of the theory of special relativity, perhaps the most well-known, was that energy and mass are not separate things, but are, in fact, interchangeable. This new relationship, however, also carried with it new implications for the concept of inertia. The logical conclusion of special relativity was that if mass exhibits the principle of inertia, then inertia must also apply to energy. This theory, and subsequent experiments confirming some of its conclusions, have also served to radically expand the definition of inertia in some contexts to apply to a much wider context including energy as well as matter.
Interpretations.
Mass and inertia.
Physics and mathematics appear to be less inclined to use the popular concept of inertia as "a tendency to maintain momentum" and instead favor the mathematically useful definition of inertia as the measure of a body's resistance to changes in velocity or simply a body's inertial mass.
This was clear in the beginning of the 20th century, when the theory of relativity was not yet created. Mass, "m", denoted something like an amount of substance or quantity of matter. And at the same time mass was the quantitative measure of inertia of a body.
The mass of a body determines the momentum formula_1 of the body at given velocity formula_2; it is a proportionality factor in the formula:
The factor "m" is referred to as inertial mass.
But mass, as related to the 'inertia' of a body, can also be defined by the formula:
Here, "F" is force, "m" is inertial mass, and "a" is acceleration.
By this formula, the greater its mass, the less a body accelerates under given force. Masses formula_5 defined by formula (1) and (2) are equal because formula (2) is a consequence of formula (1) if mass does not depend on time and velocity. Thus, "mass is the quantitative or numerical measure of a body’s inertia, that is of its resistance to being accelerated".
This meaning of a "body's inertia" therefore is altered from the popular meaning as "a tendency to maintain momentum" to a description of the measure of how difficult it is to change the velocity of a body. But it is consistent with the fact that motion in one reference frame can disappear in another, so it is the change in velocity that is important.
Inertial mass.
There is no measurable difference between gravitational mass and inertial mass. The gravitational mass is defined by the quantity of gravitational field material a mass possesses, including its energy. The "inertial mass" (relativistic mass) is a function of the acceleration a mass has undergone and its resultant speed. A mass that has been accelerated to speeds close to the speed of light has its "relativistic mass" increased, and that is why the magnetic field strength in particle accelerators must be increased to force the mass's path to curve. In practice, "inertial mass" is normally taken to be "invariant mass" and so is identical to gravitational mass without the energy component.
Gravitational mass is measured by comparing the force of gravity of an unknown mass to the force of gravity of a known mass. This is typically done with some sort of balance. Equal masses will match on a balance because the gravitational field applies to them equally, producing identical weight. This assumption breaks down near supermassive objects such as black holes and neutron stars due to tidal effects. It also breaks down in weightless environments, because no matter what objects are compared, it will yield a balanced reading.
Inertial mass is found by applying a known net force to an unknown mass, measuring the resulting acceleration, and applying Newton's Second Law, m = F/a. This gives an accurate value for mass, limited only by the accuracy of the measurements. When astronauts need to be measured in the weightlessness of free fall, they actually find their inertial mass in a special chair called a body mass measurement device (BMMD).
At high speeds, and especially near the speed of light, inertial mass can be determined by measuring the magnetic field strength and the curvature of the path of an electrically-charged mass such as an electron.
No physical difference has been found between gravitational and inertial mass in a given inertial frame. In experimental measurements, the two always agree within the margin of error for the experiment. Einstein used the fact that gravitational and inertial mass were equal to begin his general theory of relativity in which he postulated that gravitational mass was the same as inertial mass, and that the acceleration of gravity is a result of a 'valley' or slope in the space-time continuum that masses 'fell down.' Dennis Sciama later showed that the reaction force produced by the combined gravity of all matter in the universe upon an accelerating object is mathematically equal to the object's inertia , but this would only be a workable physical explanation if by some mechanism the gravitational effects operated instantaneously.
At high speeds, relativistic mass always exceeds gravitational mass. If the mass is made to travel close to the speed of light, its "inertial mass" (relativistic) as observed from a stationary frame would be very great while its gravitational mass would remain at its rest value, but the gravitational effect of the extra energy would exactly balance the measured increase in inertial mass.
Inertial frames.
In a location such as a steadily moving railway carriage, a dropped ball (as seen by an observer in the carriage) would behave as it would if it were dropped in a stationary carriage. The ball would simply descend vertically. It is possible to ignore the motion of the carriage by defining it as an inertial frame. In a moving but non-accelerating frame, the ball behaves normally because the train and its contents continue to move at a constant velocity. Before being dropped, the ball was traveling with the train at the same speed, and the ball's inertia ensured that it continued to move in the same speed and direction as the train, even while dropping. Note that, here, it is inertia which ensured that, not its mass.
In an inertial frame all the observers in uniform (non-accelerating) motion will observe the same laws of physics. However observers in another inertial frame can make a simple, and intuitively obvious, transformation (the Galilean transformation), to convert their observations. Thus, an observer from outside the moving train could deduce that the dropped ball within the carriage fell vertically downwards.
However, in reference frames which are experiencing acceleration (non-inertial reference frames), objects appear to be affected by fictitious forces. For example, if the railway carriage were accelerating, the ball would not fall vertically within the carriage but would appear to an observer to be deflected because the carriage and the ball would not be traveling at the same speed while the ball was falling. Other examples of fictitious forces occur in rotating frames such as the earth. For example, a missile at the North Pole could be aimed directly at a location and fired southwards. An observer would see it apparently deflected away from its target by a force (the Coriolis force) but in reality the southerly target has moved because earth has rotated while the missile is in flight. Because the earth is rotating, a useful inertial frame of reference is defined by the stars, which only move imperceptibly during most observations.The law of inertia is also known as Isaac Newton's first law of motion.
In summary, the principle of inertia is intimately linked with the principles of conservation of energy and conservation of momentum.
Rotational inertia.
Another form of inertia is "rotational inertia" (→ moment of inertia), the property that a rotating rigid body maintains its state of uniform rotational motion. Its angular momentum is unchanged, unless an external torque is applied; this is also called conservation of angular momentum. Rotational inertia depends on the object remaining structurally intact as a rigid body, and also has practical consequences; For example, a gyroscope uses the property that it resists any change in the axis of rotation.
Source of inertia; speculative theories.
Various efforts by notable physicists such as Ernst Mach (see Mach's principle), Albert Einstein, Dennis William Sciama, and Bernard Haisch have been put towards the study and theorizing of inertia.

</doc>
<doc id="14838" url="https://en.wikipedia.org/wiki?curid=14838" title="Inertial frame of reference">
Inertial frame of reference

In physics, an inertial frame of reference (also inertial reference frame or inertial frame, Galilean reference frame or inertial space) is a frame of reference that describes time and space homogeneously, isotropically, and in a time-independent manner.
All inertial frames are in a state of constant, rectilinear motion with respect to one another; an accelerometer moving with any of them would detect zero acceleration. Measurements in one inertial frame can be converted to measurements in another by a simple transformation (the Galilean transformation in Newtonian physics and the Lorentz transformation in special relativity). In general relativity, in any region small enough for the curvature of spacetime to be negligible, one can find a set of inertial frames that approximately describe that region.
Physical laws take the same form in all inertial frames. By contrast, in a non-inertial reference frame the laws of physics vary depending on the acceleration of that frame with respect to an inertial frame, and the usual physical forces must be supplemented by fictitious forces. For example, a ball dropped towards the ground does not go exactly straight down because the Earth is rotating. Someone rotating with the Earth must account for the Coriolis effect—in this case thought of as a force—to predict the horizontal motion. Another example of such a fictitious force associated with rotating reference frames is the centrifugal effect, or centrifugal force.
Introduction.
The motion of a body can only be described relative to something else - other bodies, observers, or a set of space-time coordinates. These are called frames of reference. If the coordinates are chosen badly, the laws of motion may be more complex than necessary. For example, suppose a free body (one having no external forces on it) is at rest at some instant. In many coordinate systems, it would begin to move at the next instant, even though there are no forces on it. However, a frame of reference can always be chosen in which it remains stationary. Similarly, if space is not described uniformly or time independently, a coordinate system could describe the simple flight of a free body in space as a complicated zig-zag in its coordinate system. Indeed, an intuitive summary of inertial frames can be given as: In an inertial reference frame, the laws of mechanics take their simplest form.
In an inertial frame, Newton's first law (the "law of inertia") is satisfied: Any free motion has a constant magnitude and direction. Newton's second law for a particle takes the form:
with F the net force (a vector), "m" the mass of a particle and a the acceleration of the particle (also a vector) which would be measured by an observer at rest in the frame. The force F is the vector sum of all "real" forces on the particle, such as electromagnetic, gravitational, nuclear and so forth. In contrast, Newton's second law in a rotating frame of reference, rotating at angular rate "Ω" about an axis, takes the form:
which looks the same as in an inertial frame, but now the force F′ is the resultant of not only F, but also additional terms (the paragraph following this equation presents the main points without detailed mathematics):
where the angular rotation of the frame is expressed by the vector Ω pointing in the direction of the axis of rotation, and with magnitude equal to the angular rate of rotation "Ω", symbol × denotes the vector cross product, vector x"B" locates the body and vector v"B" is the velocity of the body according to a rotating observer (different from the velocity seen by the inertial observer).
The extra terms in the force F′ are the "fictitious" forces for this frame. (The first extra term is the Coriolis force, the second the centrifugal force, and the third the Euler force.) These terms all have these properties: they vanish when "Ω" = 0; that is, they are zero for an inertial frame (which, of course, does not rotate); they take on a different magnitude and direction in every rotating frame, depending upon its particular value of Ω; they are ubiquitous in the rotating frame (affect every particle, regardless of circumstance); and they have no apparent source in identifiable physical sources, in particular, matter. Also, fictitious forces do not drop off with distance (unlike, for example, nuclear forces or electrical forces). For example, the centrifugal force that appears to emanate from the axis of rotation in a rotating frame increases with distance from the axis.
All observers agree on the real forces, F; only non-inertial observers need fictitious forces. The laws of physics in the inertial frame are simpler because unnecessary forces are not present.
In Newton's time the fixed stars were invoked as a reference frame, supposedly at rest relative to absolute space. In reference frames that were either at rest with respect to the fixed stars or in uniform translation relative to these stars, Newton's laws of motion were supposed to hold. In contrast, in frames accelerating with respect to the fixed stars, an important case being frames rotating relative to the fixed stars, the laws of motion did not hold in their simplest form, but had to be supplemented by the addition of fictitious forces, for example, the Coriolis force and the centrifugal force. Two interesting experiments were devised by Newton to demonstrate how these forces could be discovered, thereby revealing to an observer that they were not in an inertial frame: the example of the tension in the cord linking two spheres rotating about their center of gravity, and the example of the curvature of the surface of water in a rotating bucket. In both cases, application of Newton's second law would not work for the rotating observer without invoking centrifugal and Coriolis forces to account for their observations (tension in the case of the spheres; parabolic water surface in the case of the rotating bucket).
As we now know, the fixed stars are not fixed. Those that reside in the Milky Way turn with the galaxy, exhibiting proper motions. Those that are outside our galaxy (such as nebulae once mistaken to be stars) participate in their own motion as well, partly due to expansion of the universe, and partly due to peculiar velocities. (The Andromeda galaxy is on collision course with the Milky Way at a speed of 117 km/s.) The concept of inertial frames of reference is no longer tied to either the fixed stars or to absolute space. Rather, the identification of an inertial frame is based upon the simplicity of the laws of physics in the frame. In particular, the absence of fictitious forces is their identifying property.
In practice, although not a requirement, using a frame of reference based upon the fixed stars as though it were an inertial frame of reference introduces very little discrepancy. For example, the centrifugal acceleration of the Earth because of its rotation about the Sun is about thirty million times greater than that of the Sun about the galactic center.
To illustrate further, consider the question: "Does our Universe rotate?" To answer, we might attempt to explain the shape of the Milky Way galaxy using the laws of physics. (Other observations might be more definitive (that is, provide larger discrepancies or less measurement uncertainty), like the anisotropy of the microwave background radiation or Big Bang nucleosynthesis.) Just how flat the disc of the Milky Way is depends on its rate of rotation in an inertial frame of reference. If we attribute its apparent rate of rotation entirely to rotation in an inertial frame, a different "flatness" is predicted than if we suppose part of this rotation actually is due to rotation of the Universe and should not be included in the rotation of the galaxy itself. Based upon the laws of physics, a model is set up in which one parameter is the rate of rotation of the Universe. If the laws of physics agree more accurately with observations in a model with rotation than without it, we are inclined to select the best-fit value for rotation, subject to all other pertinent experimental observations. If no value of the rotation parameter is successful and theory is not within observational error, a modification of physical law is considered. (For example, dark matter is invoked to explain the galactic rotation curve.) So far, observations show any rotation of the Universe is very slow (no faster than once every 60·1012 years (10−13 rad/yr)), and debate persists over whether there is "any" rotation. However, if rotation were found, interpretation of observations in a frame tied to the Universe would have to be corrected for the fictitious forces inherent in such rotation. Evidently, such an approach adopts the view that "an inertial frame of reference is one where our laws of physics apply" (or need the least modification).
When quantum effects are important, there are additional conceptual complications that arise in quantum reference frames.
Background.
A brief comparison of inertial frames in special relativity and in Newtonian mechanics, and the role of absolute space is next.
A set of frames where the laws of physics are simple.
According to the first postulate of special relativity, all physical laws take their simplest form in an inertial frame, and there exist multiple inertial frames interrelated by uniform translation: 
The principle of simplicity can be used within Newtonian physics as well as in special relativity; see Nagel and also Blagojević.
In practical terms, the equivalence of inertial reference frames means that scientists within a box moving uniformly cannot determine their absolute velocity by any experiment (otherwise the differences would set up an absolute standard reference frame). According to this definition, supplemented with the constancy of the speed of light, inertial frames of reference transform among themselves according to the Poincaré group of symmetry transformations, of which the Lorentz transformations are a subgroup. In Newtonian mechanics, which can be viewed as a limiting case of special relativity in which the speed of light is infinite, inertial frames of reference are related by the Galilean group of symmetries.
Absolute space.
Newton posited an absolute space considered well approximated by a frame of reference stationary relative to the fixed stars. An inertial frame was then one in uniform translation relative to absolute space. However, some scientists (called "relativists" by Mach), even at the time of Newton, felt that absolute space was a defect of the formulation, and should be replaced.
Indeed, the expression "inertial frame of reference" () was coined by Ludwig Lange in 1885, to replace Newton's definitions of "absolute space and time" by a more operational definition. As translated by Iro, Lange proposed the following definition:
A discussion of Lange's proposal can be found in Mach.
The inadequacy of the notion of "absolute space" in Newtonian mechanics is spelled out by Blagojević: 
The utility of operational definitions was carried much further in the special theory of relativity. Some historical background including Lange's definition is provided by DiSalle, who says in summary:
Newton's inertial frame of reference.
Within the realm of Newtonian mechanics, an inertial frame of reference, or inertial reference frame, is one in which Newton's first law of motion is valid. However, the principle of special relativity generalizes the notion of inertial frame to include all physical laws, not simply Newton's first law.
Newton viewed the first law as valid in any reference frame that is in uniform motion relative to the fixed stars; that is, neither rotating nor accelerating relative to the stars. Today the notion of "absolute space" is abandoned, and an inertial frame in the field of classical mechanics is defined as:
Hence, with respect to an inertial frame, an object or body accelerates only when a physical force is applied, and (following Newton's first law of motion), in the absence of a net force, a body at rest will remain at rest and a body in motion will continue to move uniformly—that is, in a straight line and at constant speed. Newtonian inertial frames transform among each other according to the Galilean group of symmetries.
If this rule is interpreted as saying that straight-line motion is an indication of zero net force, the rule does not identify inertial reference frames because straight-line motion can be observed in a variety of frames. If the rule is interpreted as defining an inertial frame, then we have to be able to determine when zero net force is applied. The problem was summarized by Einstein:
There are several approaches to this issue. One approach is to argue that all real forces drop off with distance from their sources in a known manner, so we have only to be sure that a body is far enough away from all sources to ensure that no force is present. A possible issue with this approach is the historically long-lived view that the distant universe might affect matters (Mach's principle). Another approach is to identify all real sources for real forces and account for them. A possible issue with this approach is that we might miss something, or account inappropriately for their influence, perhaps, again, due to Mach's principle and an incomplete understanding of the universe. A third approach is to look at the way the forces transform when we shift reference frames. Fictitious forces, those that arise due to the acceleration of a frame, disappear in inertial frames, and have complicated rules of transformation in general cases. On the basis of universality of physical law and the request for frames where the laws are most simply expressed, inertial frames are distinguished by the absence of such fictitious forces.
Newton enunciated a principle of relativity himself in one of his corollaries to the laws of motion: 
This principle differs from the special principle in two ways: first, it is restricted to mechanics, and second, it makes no mention of simplicity. It shares with the special principle the invariance of the form of the description among mutually translating reference frames. The role of fictitious forces in classifying reference frames is pursued further below.
Separating non-inertial from inertial reference frames.
Theory.
Inertial and non-inertial reference frames can be distinguished by the absence or presence of fictitious forces, as explained shortly. 
The presence of fictitious forces indicates the physical laws are not the simplest laws available so, in terms of the special principle of relativity, a frame where fictitious forces are present is not an inertial frame:
Bodies in non-inertial reference frames are subject to so-called "fictitious" forces (pseudo-forces); that is, forces that result from the acceleration of the reference frame itself and not from any physical force acting on the body. Examples of fictitious forces are the centrifugal force and the Coriolis force in rotating reference frames.
How then, are "fictitious" forces to be separated from "real" forces? It is hard to apply the Newtonian definition of an inertial frame without this separation. For example, consider a stationary object in an inertial frame. Being at rest, no net force is applied. But in a frame rotating about a fixed axis, the object appears to move in a circle, and is subject to centripetal force (which is made up of the Coriolis force and the centrifugal force). How can we decide that the rotating frame is a non-inertial frame? There are two approaches to this resolution: one approach is to look for the origin of the fictitious forces (the Coriolis force and the centrifugal force). We will find there are no sources for these forces, no associated force carriers, no originating bodies. A second approach is to look at a variety of frames of reference. For any inertial frame, the Coriolis force and the centrifugal force disappear, so application of the principle of special relativity would identify these frames where the forces disappear as sharing the same and the simplest physical laws, and hence rule that the rotating frame is not an inertial frame.
Newton examined this problem himself using rotating spheres, as shown in Figure 2 and Figure 3. He pointed out that if the spheres are not rotating, the tension in the tying string is measured as zero in every frame of reference. If the spheres only appear to rotate (that is, we are watching stationary spheres from a rotating frame), the zero tension in the string is accounted for by observing that the centripetal force is supplied by the centrifugal and Coriolis forces in combination, so no tension is needed. If the spheres really are rotating, the tension observed is exactly the centripetal force required by the circular motion. Thus, measurement of the tension in the string identifies the inertial frame: it is the one where the tension in the string provides exactly the centripetal force demanded by the motion as it is observed in that frame, and not a different value. That is, the inertial frame is the one where the fictitious forces vanish.
So much for fictitious forces due to rotation. However, for linear acceleration, Newton expressed the idea of undetectability of straight-line accelerations held in common:
This principle generalizes the notion of an inertial frame. For example, an observer confined in a free-falling lift will assert that he himself is a valid inertial frame, even if he is accelerating under gravity, so long as he has no knowledge about anything outside the lift. So, strictly speaking, inertial frame is a relative concept. With this in mind, we can define inertial frames collectively as a set of frames which are stationary or moving at constant velocity with respect to each other, so that a single inertial frame is defined as an element of this set.
For these ideas to apply, everything observed in the frame has to be subject to a base-line, common acceleration shared by the frame itself. That situation would apply, for example, to the elevator example, where all objects are subject to the same gravitational acceleration, and the elevator itself accelerates at the same rate.
In 1899 the astronomer Karl Schwarzschild pointed out an observation about double stars. The motion of two stars orbiting each other is planar, the two orbits of the stars of the system lie in a plane. In the case of sufficiently near double star systems, it can be seen from Earth whether the perihelion of the orbits of the two stars remains pointing in the same direction with respect to the solar system. Schwarzschild pointed out that that was invariably seen: the direction of the angular momentum of all observed double star systems remains fixed with respect to the direction of the angular momentum of the Solar system. The logical inference is that just like gyroscopes, the angular momentum of all celestial bodies is angular momentum with respect to a universal inertial space.
Applications.
Inertial navigation systems used a cluster of gyroscopes and accelerometers to determine accelerations relative to inertial space. After a gyroscope is spun up in a particular orientation in inertial space, the law of conservation of angular momentum requires that it retain that orientation as long as no external forces are applied to it. Three orthogonal gyroscopes establish an inertial reference frame, and the accelerators measure acceleration relative to that frame. The accelerations, along with a clock, can then be used to calculate the change in position. Thus, inertial navigation is a form of dead reckoning that requires no external input, and therefore cannot be jammed by any external or internal signal source.
A gyrocompass, employed for navigation of seagoing vessels, finds the geometric north. It does so, not by sensing the Earth's magnetic field, but by using inertial space as its reference. The outer casing of the gyrocompass device is held in such a way that it remains aligned with the local plumb line. When the gyroscope wheel inside the gyrocompass device is spun up, the way the gyroscope wheel is suspended causes the gyroscope wheel to gradually align its spinning axis with the Earth's axis. Alignment with the Earth's axis is the only direction for which the gyroscope's spinning axis can be stationary with respect to the Earth and not be required to change direction with respect to inertial space. After being spun up, a gyrocompass can reach the direction of alignment with the Earth's axis in as little as a quarter of an hour.
Newtonian mechanics.
Classical mechanics, which includes relativity, assumes the equivalence of all inertial reference frames. Newtonian mechanics makes the additional assumptions of absolute space and absolute time. Given these two assumptions, the coordinates of the same event (a point in space and time) described in two inertial reference frames are related by a Galilean transformation.
where r0 and "t"0 represent shifts in the origin of space and time, and v is the relative velocity of the two inertial reference frames. Under Galilean transformations, the time "t"2 − "t"1 between two events is the same for all inertial reference frames and the distance between two simultaneous events (or, equivalently, the length of any object, |r2 − r1|) is also the same.
Special relativity.
Einstein's theory of special relativity, like Newtonian mechanics, assumes the equivalence of all inertial reference frames, but makes an additional assumption, foreign to Newtonian mechanics, namely, that in free space light always is propagated with the speed of light "c"0, a defined value independent of its direction of propagation and its frequency, and also independent of the state of motion of the emitting body. This second assumption has been verified experimentally and leads to counter-intuitive deductions including:
These deductions are logical consequences of the stated assumptions, and are general properties of space-time, typically without regard to a consideration of properties pertaining to the structure of individual objects like atoms or stars, nor to the mechanisms of clocks.
These effects are expressed mathematically by the Lorentz transformation
where shifts in origin have been ignored, the relative velocity is assumed to be in the formula_10-direction and the Lorentz factor γ is defined by:
The Lorentz transformation is equivalent to the Galilean transformation in the limit "c"0 → ∞ (a hypothetical case) or "v" → 0 (low speeds).
Under Lorentz transformations, the time and distance between events may differ among inertial reference frames; however, the Lorentz scalar distance "s" between two events is the same in all inertial reference frames
From this perspective, the speed of light is only accidentally a property of light, and is rather a property of spacetime, a conversion factor between conventional time units (such as seconds) and length units (such as meters).
Incidentally, because of the limitations on speeds faster than the speed of light, notice that in a rotating frame of reference (which is a non-inertial frame, of course) stationarity is not possible at arbitrary distances because at large radius the object would move faster than the speed of light.
General relativity.
General relativity is based upon the principle of equivalence:
This idea was introduced in Einstein's 1907 article "Principle of Relativity and Gravitation" and later developed in 1911. Support for this principle is found in the Eötvös experiment, which determines whether the ratio of inertial to gravitational mass is the same for all bodies, regardless of size or composition. To date no difference has been found to a few parts in 1011. For some discussion of the subtleties of the Eötvös experiment, such as the local mass distribution around the experimental site (including a quip about the mass of Eötvös himself), see Franklin.
Einstein’s general theory modifies the distinction between nominally "inertial" and "noninertial" effects by replacing special relativity's "flat" Minkowski Space with a metric that produces non-zero curvature. In general relativity, the principle of inertia is replaced with the principle of geodesic motion, whereby objects move in a way dictated by the curvature of spacetime. As a consequence of this curvature, it is not a given in general relativity that inertial objects moving at a particular rate with respect to each other will continue to do so. This phenomenon of geodesic deviation means that inertial frames of reference do not exist globally as they do in Newtonian mechanics and special relativity.
However, the general theory reduces to the special theory over sufficiently small regions of spacetime, where curvature effects become less important and the earlier inertial frame arguments can come back into play. Consequently, modern special relativity is now sometimes described as only a "local theory".

</doc>
<doc id="7265745" url="https://en.wikipedia.org/wiki?curid=7265745" title="Inertialess drive">
Inertialess drive

The inertialess drive is a fictional means of accelerating to close to the speed of light or faster-than-light travel, originally used in "Triplanetary" and the "Lensman" series by E.E. "Doc" Smith, and later by Robert A. Heinlein, 
Larry Niven, and Alastair Reynolds.
The concept of inertia.
Inertia is the measure of the resistance of a material body to a change in state of motion (acceleration) under the effect of an applied force. This resistance is proportional to the mass of the body, and is usually expressed (in simplest form) as F = ma.
For a body to be rendered inertialess, in principle, its mass should be reduced to zero. In classical special and general relativity, massless bodies are constrained to always move at exactly the speed of light (the speed of photons in a vacuum), and the term relativity in this context in fact implies that light is always measured to move at the same speed by an observer, no matter how rapidly the observer is moving relative to any body defined as fixed. This aspect of inertialessness is explored by the more exacting authors who have adopted the term after Smith's groundbreaking use.
In classical special relativity, in order to move faster than the speed of light, a particle must have not zero mass—and inertia—but mathematically imaginary mass. This is, of course, as viewed by an external observer, and one can postulate that Smith's inertialess field is thus similar in form to a space warp that acts as a tachyon when viewed by an observer in the external universe. Whether this formulation can be expressed self-sufficiently in the context of Smith's fiction is, of course, irrelevant to the enjoyment of it.
Historical claims.
In the late 1990s one Michael Pedler claimed to be developing an inertialess drive. His Inertialess Drive Corporation Limited was established in New Zealand in 1995, and obtained more than $6.8 million from 1,200 investors, mostly New Zealanders. However the company was placed into liquidation in 2001, and struck off the Companies Office register in 2004, without having produced a workable drive. Pedler also established Inertialess Drive Incorporation USA.
Appearances in fiction.
"Triplanetary" Universe.
The possibility of inertialess travel was first suggested in "Theoretical and Physical Chemistry,"
published in 1912
by the Tellurian chemist Samuel Lawrence Bigelow,
an alumnus of Harvard.
The first faster-than-light drive, which achieved only partial neutralization of inertia, was developed on the planet Nevia. Soon thereafter, two Tellurian scientists, Lyman Cleveland and Frederick Rodebush developed the one-hundred-percent inertialess Rodebush-Cleveland drive, which traveled (and decelerated) much faster. (In contrast to accounts in later versions, there were no critical flaws with this drive, and no contribution by any scientist named Bergenholm or by Arisians.)
"Lensman" Universe.
Galactic Civilization developed for a long period using only the semi-inert drive, which was presumably similar to Nevian partially inertialess drive discussed above. The exact duration is not known, but even as late as the Third Galactic Survey it still “took years to cross the galaxy.” Because the key piece of inertialess technology is known as the Bergenholm,
it seems likely that an engineer by that name was responsible for a key advance, but little is known for certain. Dr. Bergenholm is referred to as the “late Dr. Bergenholm himself” in the original, which suggests that he was a more recent figure than in the ret-con version, below.
"Triplanetary"/"Lensman" Ret-con Universe.
In the revised book versions of "Triplanetary" and the core "Lensman" novels, the partially inertialess drive was given by Arisia to Nevia, and the fully inertialess drive was initially developed by scientists Rodebush and Cleveland, but the early drive was considered to be a "man-killer."
Progress was made when the Triplanetary scientist Nels Bergenholm, activated by the Arisian Drounli, came up with a "hunch" which solved the phasing problems of the original Rodebush-Cleveland drive. This made the drive safe and commercially practical, and in recognition of his achievement, the drive was thereafter called a Bergenholm.
Other authors’ universes.
Inertialessness, though not for faster-than-light travel, is discussed in Robert A. Heinlein’s "Methuselah's Children", Isaac Asimov's short story The Billiard Ball, Larry Niven’s Known Space universe, Orson Scott Card's "Speaker for the Dead", Arthur C. Clarke's , and 
Alastair Reynolds’ "Redemption Ark."
See the Lensman Technology article for details. Authors, Ken Farmer and Buck Stienke have the Tyranians (Grays) utilizing the inertialess drive in their new SyFy series, "Aurora", beginning with "Legend of Aurora".
Prior to the release of the Necron fifth-edition codex, inertialess drive was the means of FTL travel used by the Necron warships of the Warhammer 40,000 universe. Among the many changes which this codex brought to the Necrons' backstory was the retconning out of existence of any form of starship-based FTL travel for the Necron fleet.

</doc>
<doc id="379845" url="https://en.wikipedia.org/wiki?curid=379845" title="Inflection point">
Inflection point

In differential calculus, an inflection point, point of inflection, flex, or inflection (inflexion) is a point on a curve at which the curve changes from being concave (concave downward) to convex (concave upward), or vice versa.
A point where the curvature vanishes but does not change sign is sometimes called a point of undulation or undulation point.
In algebraic geometry an inflection point is defined slightly more generally, as a point where the tangent meets the curve to order at least 3, and an undulation point or hyperflex is defined as a point where the tangent meets the curve to order at least 4.
Equivalent forms.
A differentiable function has an inflection point at ("x", "f"("x")) if and only if its first derivative, "f′", has an isolated extremum at "x". (This is not the same as saying that "f" has an extremum). That is, in some neighborhood, "x" is the one and only point at which "f′" has a (local) minimum or maximum. If all extrema of "f′" are isolated, then an inflection point is a point on the graph of "f" at which the tangent crosses the curve. 
For an algebraic curve, a non singular point is an inflection point if and only if the multiplicity of the intersection of the tangent line and the curve (at the point of tangency) is odd and greater than 2.
For a curve given by parametric equations, a point is an inflection point if its signed curvature changes from plus to minus or from minus to plus, i.e., changes sign.
For a twice differentiable function, an inflection point is a point on the graph at which the second derivative changes sign.
A necessary but not sufficient condition.
If "x" is an inflection point for "f" then the second derivative, "f″"("x"), is equal to zero if it exists, but this condition does not provide a sufficient definition of a point of inflection. One also needs the lowest-order (above the second) non-zero derivative to be of odd order (third, fifth, etc.). If the lowest-order non-zero derivative is of even order, the point is not a point of inflection, but an "undulation point". However, in algebraic geometry, both inflection points and undulation points are usually called "inflection points". An example of such an undulation point is "x" = 0 for the function "f" given by "f"("x") = "x"4.
This definition assumes that "f" has some higher-order non-zero derivative at "x", which is not necessarily the case, But if it has one, it follows from the definition that the sign of "f′"("x") is the same on either side of "x" in a neighborhood of "x". If this is positive, the point is a "rising point of inflection"; if it is negative, the point is a "falling point of inflection".
Categorization of points of inflection.
Points of inflection can also be categorized according to whether "f′"("x") is zero or not zero.
An example of a saddle point is the point (0,0) on the graph "y" = "x"3. The tangent is the "x"-axis, which cuts the graph at this point.
A non-stationary point of inflection can be visualised if the graph "y" = "x"3 is rotated slightly about the origin. The tangent at the origin still cuts the graph in two, but its gradient is non-zero.
Functions with discontinuities.
Some functions change concavity without having points of inflection. Instead, they can change concavity around vertical asymptotes or discontinuities. Take, for example, the function . It is concave when |"x"| > 1 and convex when |"x"| < 1. However, it has no points of inflection because 1 and -1 are not in the domain of the function.

</doc>
<doc id="498383" url="https://en.wikipedia.org/wiki?curid=498383" title="Initial value problem">
Initial value problem

In mathematics, in the field of differential equations, an initial value problem (also called the Cauchy problem by some authors) is an ordinary differential equation together with a specified value, called the initial condition, of the unknown function at a given point in the domain of the solution. In physics or other sciences, modeling a system frequently amounts to solving an initial value problem; in this context, the differential equation is an evolution equation specifying how, given initial conditions, the system will evolve with time.
Definition.
An initial value problem is a differential equation
together with a point in the domain of formula_5
called the initial condition.
A solution to an initial value problem is a function formula_7 that is a solution to the differential equation and satisfies
In higher dimensions, the differential equation is replaced with a family of equations formula_9, and formula_10 is viewed as the vector formula_11. More generally, the unknown function formula_7 can take values on infinite dimensional spaces, such as Banach spaces or spaces of distributions.
Initial value problems are extended to higher orders by treating the derivatives in the same way as an independent function, e.g. formula_13.
Existence and uniqueness of solutions.
For a large class of initial value problems, the existence and uniqueness of a solution can be illustrated through the use of a calculator.
The Picard–Lindelöf theorem guarantees a unique solution on some interval containing "t"0 if ƒ is continuous on a region containing "t"0 and "y"0 and satisfies the Lipschitz condition on the variable "y". 
The proof of this theorem proceeds by reformulating the problem as an equivalent integral equation. The integral can be considered an operator which maps one function into another, such that the solution is a fixed point of the operator. The Banach fixed point theorem is then invoked to show that there exists a unique fixed point, which is the solution of the initial value problem.
An older proof of the Picard–Lindelöf theorem constructs a sequence of functions which converge to the solution of the integral equation, and thus, the solution of the initial value problem. Such a construction is sometimes called "Picard's method" or "the method of successive approximations". This version is essentially a special case of the Banach fixed point theorem.
Hiroshi Okamura obtained a necessary and sufficient condition for the solution of an initial value problem to be unique. This condition has to do with the existence of a Lyapunov function for the system.
In some situations, the function ƒ is not of class "C"1, or even Lipschitz, so the usual result guaranteeing the local existence of a unique solution does not apply. The Peano existence theorem however proves that even for ƒ merely continuous, solutions are guaranteed to exist locally in time; the problem is that there is no guarantee of uniqueness. The result may be found in Coddington & Levinson (1955, Theorem 1.3) or Robinson (2001, Theorem 2.6). An even more general result is the Carathéodory existence theorem, which proves existence for some discontinuous functions ƒ.
Examples.
A simple example is to solve formula_14 and formula_15. We are trying to find a formula for formula_10 that satisfies these two equations.
Start by noting that formula_17, so
Now rearrange the equation so that formula_7 is on the left and formula_20 on the right
Now integrate both sides (this introduces an unknown constant formula_22).
Eliminate the formula_24
Let formula_26 be a new unknown constant, formula_27, so
Now we need to find a value for formula_26. Use formula_15 as given at the start and substitute 0 for formula_20 and 19 for formula_7
this gives the final solution of formula_35.
The solution of
can be found to be
Indeed,

</doc>
<doc id="14856" url="https://en.wikipedia.org/wiki?curid=14856" title="Inner product space">
Inner product space

In linear algebra, an inner product space is a vector space with an additional structure called an inner product. This additional structure associates each pair of vectors in the space with a scalar quantity known as the inner product of the vectors. Inner products allow the rigorous introduction of intuitive geometrical notions such as the length of a vector or the angle between two vectors. They also provide the means of defining orthogonality between vectors (zero inner product). Inner product spaces generalize Euclidean spaces (in which the inner product is the dot product, also known as the scalar product) to vector spaces of any (possibly infinite) dimension, and are studied in functional analysis.
An inner product naturally induces an associated norm, thus an inner product space is also a normed vector space. A complete space with an inner product is called a Hilbert space. An incomplete space with an inner product is called a pre-Hilbert space, since its completion with respect to the norm induced by the inner product is a Hilbert space. Inner product spaces over the field of complex numbers are sometimes referred to as unitary spaces.
Definition.
In this article, the field of scalars denoted is either the field of real numbers or the field of complex numbers .
Formally, an inner product space is a vector space over the field together with an "inner product", i.e., with a map
that satisfies the following three axioms for all vectors formula_2 and all scalars formula_3:
Alternative definitions, notations and remarks.
Some authors, especially in physics and matrix algebra, prefer to define the inner product and the sesquilinear form with linearity in the second argument rather than the first. Then the first argument becomes conjugate linear, rather than the second. In those disciplines we would write the product formula_9 as formula_10 (the bra–ket notation of quantum mechanics), respectively formula_11 (dot product as a case of the convention of forming the matrix product as the dot products of rows of with columns of ). Here the kets and columns are identified with the vectors of and the bras and rows with the dual vectors or linear functionals of the dual space , with conjugacy associated with duality. This reverse order is now occasionally followed in the more abstract literature, taking formula_9 to be conjugate linear in rather than . A few instead find a middle ground by recognizing both formula_13 and formula_14 as distinct notations differing only in which argument is conjugate linear.
There are various technical reasons why it is necessary to restrict the basefield to and in the definition. Briefly, the basefield has to contain an ordered subfield in order for non-negativity to make sense, and therefore has to have characteristic equal to 0 (since any ordered field has to have such characteristic). This immediately excludes finite fields. The basefield has to have additional structure, such as a distinguished automorphism. More generally any quadratically closed subfield of or will suffice for this purpose, e.g., the algebraic numbers, but when it is a proper subfield (i.e., neither nor ) even finite-dimensional inner product spaces will fail to be metrically complete. In contrast all finite-dimensional inner product spaces over or , such as those used in quantum computation, are automatically metrically complete and hence Hilbert spaces.
In some cases we need to consider non-negative "semi-definite" sesquilinear forms. This means that formula_15 is only required to be non-negative. We show how to treat these below.
Elementary properties.
When , conjugate symmetry reduces to symmetry. That is, formula_16 for ; while for , formula_17 is equal to the complex conjugate.
Notice that conjugate symmetry implies that formula_18 is real for all , since we have:
Moreover, sesquilinearity (see below) implies that
Conjugate symmetry and linearity in the first variable gives
so an inner product is a sesquilinear form. Conjugate symmetry is also called Hermitian symmetry, and a conjugate symmetric sesquilinear form is called a "Hermitian form". While the above axioms are more mathematically economical, a compact verbal definition of an inner product is a "positive-definite Hermitian form".
In the case of , conjugate-symmetry reduces to symmetry, and sesquilinear reduces to bilinear. So, an inner product on a real vector space is a "positive-definite symmetric bilinear form".
From the linearity property it is derived that implies formula_23 while from the positive-definiteness axiom we obtain the converse, formula_24 implies . Combining these two, we have the property that formula_24 if and only if .
Combining the linearity of the inner product in its first argument and the conjugate symmetry gives the following important generalization of the familiar square expansion:
Assuming the underlying field to be , the inner product becomes symmetric, and we obtain
The property of an inner product space that
is also known as "additivity".
Norms on inner product spaces.
A linear space with a norm such as:
is a normed space but not an inner product space, because this norm does not satisfy the parallelogram equality required of a norm to have an inner product associated with it.
However, inner product spaces have a naturally defined norm based upon the inner product of the space itself that does satisfy the parallelogram equality:
This is well defined by the nonnegativity axiom of the definition of inner product space. The norm is thought of as the length of the vector . Directly from the axioms, we can prove the following:
Orthonormal sequences.
Let be a finite dimensional inner product space of dimension . Recall that every basis of consists of exactly linearly independent vectors. Using the Gram–Schmidt process we may start with an arbitrary basis and transform it into an orthonormal basis. That is, into a basis in which all the elements are orthogonal and have unit norm. In symbols, a basis formula_53 is orthonormal if formula_54 if formula_55 and formula_56 for each "i".
This definition of orthonormal basis generalizes to the case of infinite-dimensional inner product spaces in the following way. Let be any inner product space. Then a collection
is a "basis" for if the subspace of generated by finite linear combinations of elements of is dense in (in the norm induced by the inner product). We say that is an "orthonormal basis" for if it is a basis and
if formula_59 and formula_60 for all formula_61.
Using an infinite-dimensional analog of the Gram-Schmidt process one may show:
Theorem. Any separable inner product space has an orthonormal basis.
Using the Hausdorff maximal principle and the fact that in a complete inner product space orthogonal projection onto linear subspaces is well-defined, one may also show that
Theorem. Any complete inner product space has an orthonormal basis.
The two previous theorems raise the question of whether all inner product spaces have an orthonormal basis. The answer, it turns out is negative. This is a non-trivial result, and is proved below. The following proof is taken from Halmos's A Hilbert Space Problem Book (see the references).
Parseval's identity leads immediately to the following theorem:
Theorem. Let be a separable inner product space and {"e""k"}"k" an orthonormal basis of . Then the map
is an isometric linear map with a dense image.
This theorem can be regarded as an abstract form of Fourier series, in which an arbitrary orthonormal basis plays the role of the sequence of trigonometric polynomials. Note that the underlying index set can be taken to be any countable set (and in fact any set whatsoever, provided is defined appropriately, as is explained in the article Hilbert space). In particular, we obtain the following result in the theory of Fourier series:
Theorem. Let be the inner product space formula_63. Then the sequence (indexed on set of all integers) of continuous functions
is an orthonormal basis of the space formula_63 with the inner product. The mapping
is an isometric linear map with dense image.
Orthogonality of the sequence {"ek"}"k" follows immediately from the fact that if "k" ≠ "j", then
Normality of the sequence is by design, that is, the coefficients are so chosen so that the norm comes out to 1. Finally the fact that the sequence has a dense algebraic span, in the "inner product norm", follows from the fact that the sequence has a dense algebraic span, this time in the space of continuous periodic functions on formula_68 with the uniform norm. This is the content of the Weierstrass theorem on the uniform density of trigonometric polynomials.
Operators on inner product spaces.
Several types of linear maps from an inner product space to an inner product space are of relevance:
From the point of view of inner product space theory, there is no need to distinguish between two spaces which are isometrically isomorphic. The spectral theorem provides a canonical form for symmetric, unitary and more generally normal operators on finite dimensional inner product spaces. A generalization of the spectral theorem holds for continuous normal operators in Hilbert spaces.
Generalizations.
Any of the axioms of an inner product may be weakened, yielding generalized notions. The generalizations that are closest to inner products occur where bilinearity and conjugate symmetry are retained, but positive-definiteness is weakened.
Degenerate inner products.
If is a vector space and formula_13 a semi-definite sesquilinear form, then the function:
makes sense and satisfies all the properties of norm except that ‖"x"‖ = 0 does not imply (such a functional is then called a semi-norm). We can produce an inner product space by considering the quotient "W" = "V"/{ "x" : ‖"x"‖ = 0}. The sesquilinear form formula_73 factors through .
This construction is used in numerous contexts. The Gelfand–Naimark–Segal construction is a particularly important example of the use of this technique. Another example is the representation of semi-definite kernels on arbitrary sets.
Nondegenerate conjugate symmetric forms.
Alternatively, one may require that the pairing be a nondegenerate form, meaning that for all non-zero there exists some such that formula_74 though need not equal ; in other words, the induced map to the dual space is injective. This generalization is important in differential geometry: a manifold whose tangent spaces have an inner product is a Riemannian manifold, while if this is related to nondegenerate conjugate symmetric form the manifold is a pseudo-Riemannian manifold. By Sylvester's law of inertia, just as every inner product is similar to the dot product with positive weights on a set of vectors, every nondegenerate conjugate symmetric form is similar to the dot product with "nonzero" weights on a set of vectors, and the number of positive and negative weights are called respectively the positive index and negative index. Product of vectors in Minkowski space is an example of indefinite inner product, although, technically speaking, it is not an inner product according to the standard definition above. Minkowski space has four dimensions and indices 3 and 1 (assignment of "+" and "−" to them differs depending on conventions).
Purely algebraic statements (ones that do not use positivity) usually only rely on the nondegeneracy (the injective homomorphism ) and thus hold more generally.
Related products.
The term "inner product" is opposed to outer product, which is a slightly more general opposite. Simply, in coordinates, the inner product is the product of a 1×"n" "co"vector with an "n"×1 vector, yielding a 1×1 matrix (a scalar), while the outer product is the product of an "m"×1 vector with a 1×"n" covector, yielding an "m"×"n" matrix. Note that the outer product is defined for different dimensions, while the inner product requires the same dimension. If the dimensions are the same, then the inner product is the "trace" of the outer product (trace only being properly defined for square matrices).
On an inner product space, or more generally a vector space with a nondegenerate form (so an isomorphism ) vectors can be sent to covectors (in coordinates, via transpose), so one can take the inner product and outer product of two vectors, not simply of a vector and a covector.
In a quip: "inner is horizontal times vertical and shrinks down, outer is vertical times horizontal and expands out".
More abstractly, the outer product is the bilinear map formula_75 sending a vector and a covector to a rank 1 linear transformation (simple tensor of type (1,1)), while the inner product is the bilinear evaluation map formula_76 given by evaluating a covector on a vector; the order of the domain vector spaces here reflects the covector/vector distinction.
The inner product and outer product should not be confused with the interior product and exterior product, which are instead operations on vector fields and differential forms, or more generally on the exterior algebra.
As a further complication, in geometric algebra the inner product and the "exterior" (Grassmann) product are combined in the geometric product (the Clifford product in a Clifford algebra) – the inner product sends two vectors (1-vectors) to a scalar (a 0-vector), while the exterior product sends two vectors to a bivector (2-vector) – and in this context the exterior product is usually called the ""outer" (alternatively, wedge) product". The inner product is more correctly called a "scalar" product in this context, as the nondegenerate quadratic form in question need not be positive definite (need not be an inner product).

</doc>
<doc id="15532" url="https://en.wikipedia.org/wiki?curid=15532" title="Integral">
Integral

In mathematics, an integral assigns numbers to functions in a way that can describe displacement, area, volume, and other concepts that arise by combining infinitesimal data. Integration is one of the two main operations in calculus, with its inverse, differentiation, being the other. Given a function of a real variable and an interval of the real line, the definite integral
is defined informally as the signed area of the region in the -plane that is bounded by the graph of , the -axis and the vertical lines and . The area above the -axis adds to the total and that below the -axis subtracts from the total.
Roughly speaking, the operation of integration is the reverse of differentiation. For this reason, the term "integral" may also refer to the related notion of the antiderivative, a function whose derivative is the given function . In this case, it is called an "indefinite integral" and is written:
The integrals discussed in this article are those termed "definite integrals". It is the fundamental theorem of calculus that connects differentiation with the definite integral: if is a continuous real-valued function defined on a closed interval , then, once an antiderivative of is known, the definite integral of over that interval is given by
The principles of integration were formulated independently by Isaac Newton and Gottfried Leibniz in the late 17th century, who thought of the integral as an infinite sum of rectangles of infinitesimal width. A rigorous mathematical definition of the integral was given by Bernhard Riemann. It is based on a limiting procedure which approximates the area of a curvilinear region by breaking the region into thin vertical slabs. Beginning in the nineteenth century, more sophisticated notions of integrals began to appear, where the type of the function as well as the domain over which the integration is performed has been generalised. A line integral is defined for functions of two or three variables, and the interval of integration is replaced by a certain curve connecting two points on the plane or in the space. In a surface integral, the curve is replaced by a piece of a surface in the three-dimensional space.
History.
Pre-calculus integration.
The first documented systematic technique capable of determining integrals is the method of exhaustion of the ancient Greek astronomer Eudoxus ("ca." 370 BC), which sought to find areas and volumes by breaking them up into an infinite number of divisions for which the area or volume was known. This method was further developed and employed by Archimedes in the 3rd century BC and used to calculate areas for parabolas and an approximation to the area of a circle.
A similar method was independently developed in China around the 3rd century AD by Liu Hui, who used it to find the area of the circle. This method was later used in the 5th century by Chinese father-and-son mathematicians Zu Chongzhi and Zu Geng to find the volume of a sphere (; ).
The next significant advances in integral calculus did not begin to appear until the 16th century. At this time the work of Cavalieri with his method of Indivisibles, and work by Fermat, began to lay the foundations of modern calculus, with Cavalieri computing the integrals of up to degree in Cavalieri's quadrature formula. Further steps were made in the early 17th century by Barrow and Torricelli, who provided the first hints of a connection between integration and differentiation. Barrow provided the first proof of the fundamental theorem of calculus. Wallis generalized Cavalieri's method, computing integrals of to a general power, including negative powers and fractional powers.
Newton and Leibniz.
The major advance in integration came in the 17th century with the independent discovery of the fundamental theorem of calculus by Newton and Leibniz. The theorem demonstrates a connection between integration and differentiation. This connection, combined with the comparative ease of differentiation, can be exploited to calculate integrals. In particular, the fundamental theorem of calculus allows one to solve a much broader class of problems. Equal in importance is the comprehensive mathematical framework that both Newton and Leibniz developed. Given the name infinitesimal calculus, it allowed for precise analysis of functions within continuous domains. This framework eventually became modern calculus, whose notation for integrals is drawn directly from the work of Leibniz.
Formalization.
While Newton and Leibniz provided a systematic approach to integration, their work lacked a degree of rigour. Bishop Berkeley memorably attacked the vanishing increments used by Newton, calling them "ghosts of departed quantities". Calculus acquired a firmer footing with the development of limits. Integration was first rigorously formalized, using limits, by Riemann. Although all bounded piecewise continuous functions are Riemann-integrable on a bounded interval, subsequently more general functions were considered—particularly in the context of Fourier analysis—to which Riemann's definition does not apply, and Lebesgue formulated a different definition of integral, founded in measure theory (a subfield of real analysis). Other definitions of integral, extending Riemann's and Lebesgue's approaches, were proposed. These approaches based on the real number system are the ones most common today, but alternative approaches exist, such as a definition of integral as the standard part of an infinite Riemann sum, based on the hyperreal number system.
Historical notation.
Isaac Newton used a small vertical bar above a variable to indicate integration, or placed the variable inside a box. The vertical bar was easily confused with or , which Newton used to indicate differentiation, and the box notation was difficult for printers to reproduce, so these notations were not widely adopted.
The modern notation for the indefinite integral was introduced by Gottfried Leibniz in 1675 (; ). He adapted the integral symbol, ∫, from the letter "ſ" (long s), standing for "summa" (written as "ſumma"; Latin for "sum" or "total"). The modern notation for the definite integral, with limits above and below the integral sign, was first used by Joseph Fourier in "Mémoires" of the French Academy around 1819–20, reprinted in his book of 1822 (; ).
Terminology and notation.
Standard.
The integral with respect to of a real-valued function of a real variable on the interval is written as
The integral sign represents integration. The symbol , called the differential of the variable , indicates that the variable of integration is . The function to be integrated is called the integrand. The symbol is separated from the integrand by a space (as shown). If a function has an integral, it is said to be integrable. The points and are called the limits of the integral. An integral where the limits are specified is called a definite integral. The integral is said to be over the interval .
When the limits are omitted, as in
the integral is called an indefinite integral (also known as antiderivative). The fundamental theorem of calculus relates indefinite integrals and definite integrals. There are many extensions of this notation to generalizations of the integral.
Variants.
In modern Arabic mathematical notation, a reflected integral symbol is used instead of the symbol . Some authors use an upright "d" to indicate the variable of integration (i.e., instead of ). The symbol is not always placed after , as for instance in
Interpretations of the integral.
Integrals appear in many practical situations. If a swimming pool is rectangular with a flat bottom, then from its length, width, and depth we can easily determine the volume of water it can contain (to fill it), the area of its surface (to cover it), and the length of its edge (to rope it). But if it is oval with a rounded bottom, all of these quantities call for integrals. Practical approximations may suffice for such trivial examples, but precision engineering (of any discipline) requires exact and rigorous values for these elements.
To start off, consider the curve between and with (see figure). We ask:
and call this (yet unknown) area the (definite) integral of . The notation for this integral will be
As a first approximation, look at the unit square given by the sides to and and . Its area is exactly 1. As it is, the true value of the integral must be somewhat less than 1. Decreasing the width of the approximation rectangles and increasing the number of rectangles shall give a better result; so cross the interval in five steps, using the approximation points 0, 1/5, 2/5, and so on to 1. Fit a box for each step using the right end height of each curve piece, thus , , and so on to . Summing the areas of these rectangles, we get a better approximation for the sought integral, namely
We are taking a sum of finitely many function values of , multiplied with the differences of two subsequent approximation points. We can easily see that the approximation is still too large. Using more steps produces a closer approximation, but will never be exact: replacing the 5 subintervals by twelve in the same way, but with the left end height of each piece, we will get an approximate value for the area of 0.6203, which is too small. The key idea is the transition from adding "finitely many" differences of approximation points multiplied by their respective function values to using infinitely many fine, or "infinitesimal" steps.
The notation
conceives the integral as a weighted sum, denoted by the elongated , of function values, , multiplied by infinitesimal step widths, the so-called "differentials", denoted by . The multiplication sign is usually omitted.
Historically, after the failure of early efforts to rigorously interpret infinitesimals, Riemann formally defined integrals as a limit of weighted sums, so that the suggested the limit of a difference (namely, the interval width). Shortcomings of Riemann's dependence on intervals and continuity motivated newer definitions, especially the Lebesgue integral, which is founded on an ability to extend the idea of "measure" in much more flexible ways. Thus the notation
refers to a weighted sum in which the function values are partitioned, with measuring the weight to be assigned to each value. Here denotes the region of integration.
Formal definitions.
There are many ways of formally defining an integral, not all of which are equivalent. The differences exist mostly to deal with differing special cases which may not be integrable under other definitions, but also occasionally for pedagogical reasons. The most commonly used definitions of integral are Riemann integrals and Lebesgue integrals.
Riemann integral.
The Riemann integral is defined in terms of Riemann sums of functions with respect to "tagged partitions" of an interval. Let be a closed interval of the real line; then a "tagged partition" of is a finite sequence
This partitions the interval into sub-intervals indexed by , each of which is "tagged" with a distinguished point . A "Riemann sum" of a function with respect to such a tagged partition is defined as
thus each term of the sum is the area of a rectangle with height equal to the function value at the distinguished point of the given sub-interval, and width the same as the sub-interval width. Let be the width of sub-interval ; then the "mesh" of such a tagged partition is the width of the largest sub-interval formed by the partition, . The "Riemann integral" of a function over the interval is equal to if:
When the chosen tags give the maximum (respectively, minimum) value of each interval, the Riemann sum becomes an upper (respectively, lower) Darboux sum, suggesting the close connection between the Riemann integral and the Darboux integral.
Lebesgue integral.
It is often of interest, both in theory and applications, to be able to pass to the limit under the integral. For instance, a sequence of functions can frequently be constructed that approximate, in a suitable sense, the solution to a problem. Then the integral of the solution function should be the limit of the integrals of the approximations. However, many functions that can be obtained as limits are not Riemann-integrable, and so such limit theorems do not hold with the Riemann integral. Therefore, it is of great importance to have a definition of the integral that allows a wider class of functions to be integrated .
Such an integral is the Lebesgue integral, that exploits the following fact to enlarge the class of integrable functions: if the values of a function are rearranged over the domain, the integral of a function should remain the same. Thus Henri Lebesgue introduced the integral bearing his name, explaining this integral thus in a letter to Paul Montel:
As puts it, "To compute the Riemann integral of , one partitions the domain into subintervals", while in the Lebesgue integral, "one is in effect partitioning the range of ". The definition of the Lebesgue integral thus begins with a measure, μ. In the simplest case, the Lebesgue measure of an interval is its width, , so that the Lebesgue integral agrees with the (proper) Riemann integral when both exist. In more complicated cases, the sets being measured can be highly fragmented, with no continuity and no resemblance to intervals.
Using the "partitioning the range of " philosophy, the integral of a non-negative function should be the sum over of the areas between a thin horizontal strip between and . This area is just . Let }. The Lebesgue integral of is then defined by 
where the integral on the right is an ordinary improper Riemann integral ( is a strictly decreasing positive function, and therefore has a well-defined improper Riemann integral). For a suitable class of functions (the measurable functions) this defines the Lebesgue integral.
A general measurable function is Lebesgue-integrable if the area between the graph of and the -axis is finite:
In that case, the integral is, as in the Riemannian case, the difference between the area above the -axis and the area below the -axis:
where
Other integrals.
Although the Riemann and Lebesgue integrals are the most widely used definitions of the integral, a number of others exist, including:
Properties.
Linearity.
The collection of Riemann-integrable functions on a closed interval forms a vector space under the operations of pointwise addition and multiplication by a scalar, and the operation of integration
is a linear functional on this vector space. Thus, firstly, the collection of integrable functions is closed under taking linear combinations; and, secondly, the integral of a linear combination is the linear combination of the integrals,
Similarly, the set of real-valued Lebesgue-integrable functions on a given measure space with measure is closed under taking linear combinations and hence form a vector space, and the Lebesgue integral
is a linear functional on this vector space, so that
More generally, consider the vector space of all measurable functions on a measure space , taking values in a locally compact complete topological vector space over a locally compact topological field . Then one may define an abstract integration map assigning to each function an element of or the symbol ,
that is compatible with linear combinations. In this situation the linearity holds for the subspace of functions whose integral is an element of (i.e. "finite"). The most important special cases arise when is , , or a finite extension of the field of p-adic numbers, and is a finite-dimensional vector space over , and when and is a complex Hilbert space.
Linearity, together with some natural continuity properties and normalisation for a certain class of "simple" functions, may be used to give an alternative definition of the integral. This is the approach of Daniell for the case of real-valued functions on a set , generalized by Nicolas Bourbaki to functions with values in a locally compact topological vector space. See for an axiomatic characterisation of the integral.
Inequalities.
A number of general inequalities hold for Riemann-integrable functions defined on a closed and bounded interval and can be generalized to other notions of integral (Lebesgue and Daniell).
Conventions.
In this section is a real-valued Riemann-integrable function. The integral
over an interval is defined if . This means that the upper and lower sums of the function are evaluated on a partition whose values are increasing. Geometrically, this signifies that integration takes place "left to right", evaluating within intervals where an interval with a higher index lies to the right of one with a lower index. The values and , the end-points of the interval, are called the limits of integration of . Integrals can also be defined if :
This, with , implies:
The first convention is necessary in consideration of taking integrals over subintervals of ; the second says that an integral taken over a degenerate interval, or a point, should be zero. One reason for the first convention is that the integrability of on an interval implies that is integrable on any subinterval , but in particular integrals have the property that:
With the first convention, the resulting relation
is then well-defined for any cyclic permutation of , , and .
Fundamental theorem of calculus.
The "fundamental theorem of calculus" is the statement that differentiation and integration are inverse operations: if a continuous function is first integrated and then differentiated, the original function is retrieved. An important consequence, sometimes called the "second fundamental theorem of calculus", allows one to compute integrals by using an antiderivative of the function to be integrated.
Statements of theorems.
Fundamental theorem of calculus.
Let be a continuous real-valued function defined on a closed interval . Let be the function defined, for all in , by
Then, is continuous on , differentiable on the open interval , and
for all in .
Second fundamental theorem of calculus.
Let be a real-valued function defined on a closed interval that admits an antiderivative on . That is, and are functions such that for all in ,
If is integrable on then
Calculating integrals.
The second fundamental theorem allows many integrals to be calculated explicitly. For example, to calculate the integral
of the square root function between 0 and 1, it is sufficient to find an antiderivative, that is, a function whose derivative equals :
One such function is . Then the value of the integral in question is 
This is a case of a general rule, that for , with , the related function, the so-called antiderivative is Tables of this and similar antiderivatives can be used to calculate integrals explicitly, in much the same way that tables of derivatives can be used.
Extensions.
Improper integrals.
A "proper" Riemann integral assumes the integrand is defined and finite on a closed and bounded interval, bracketed by the limits of integration. An improper integral occurs when one or more of these conditions is not satisfied. In some cases such integrals may be defined by considering the limit of a sequence of proper Riemann integrals on progressively larger intervals.
If the interval is unbounded, for instance at its upper end, then the improper integral is the limit as that endpoint goes to infinity.
If the integrand is only defined or finite on a half-open interval, for instance , then again a limit may provide a finite result.
That is, the improper integral is the limit of proper integrals as one endpoint of the interval of integration approaches either a specified real number, or , or . In more complicated cases, limits are required at both endpoints, or at interior points.
Multiple integration.
Just as the definite integral of a positive function of one variable represents the area of the region between the graph of the function and the "x"-axis, the "double integral" of a positive function of two variables represents the volume of the region between the surface defined by the function and the plane which contains its domain. For example, a function in two dimensions depends on two real variables, "x" and "y", and the integral of a function "f" over the rectangle "R" given as the Cartesian product of two intervals formula_46 can be written
where the differential indicates that integration is taken with respect to area. This double integral can be defined using Riemann sums, and represents the (signed) volume under the graph of over the domain "R". Under suitable conditions (e.g., if "f" is continuous), then Fubini's theorem guarantees that this integral can be expressed as an equivalent iterated integral
This reduces the problem of computing a double integral to computing one-dimensional integrals. Because of this, another notation for the integral over "R" uses a double integral sign:
Integration over more general domains is possible. The integral of a function "f", with respect to volume, over a subset "D" of ℝn is denoted by notation such as
or similar. See volume integral.
Line integrals.
The concept of an integral can be extended to more general domains of integration, such as curved lines and surfaces. Such integrals are known as line integrals and surface integrals respectively. These have important applications in physics, as when dealing with vector fields.
A "line integral" (sometimes called a "path integral") is an integral where the function to be integrated is evaluated along a curve. Various different line integrals are in use. In the case of a closed curve it is also called a "contour integral".
The function to be integrated may be a scalar field or a vector field. The value of the line integral is the sum of values of the field at all points on the curve, weighted by some scalar function on the curve (commonly arc length or, for a vector field, the scalar product of the vector field with a differential vector in the curve). This weighting distinguishes the line integral from simpler integrals defined on intervals. Many simple formulas in physics have natural continuous analogs in terms of line integrals; for example, the fact that work is equal to force, , multiplied by displacement, , may be expressed (in terms of vector quantities) as:
For an object moving along a path in a vector field such as an electric field or gravitational field, the total work done by the field on the object is obtained by summing up the differential work done in moving from to . This gives the line integral
Surface integrals.
A "surface integral" is a definite integral taken over a surface (which may be a curved set in space); it can be thought of as the double integral analog of the line integral. The function to be integrated may be a scalar field or a vector field. The value of the surface integral is the sum of the field at all points on the surface. This can be achieved by splitting the surface into surface elements, which provide the partitioning for Riemann sums.
For an example of applications of surface integrals, consider a vector field on a surface ; that is, for each point in , is a vector. Imagine that we have a fluid flowing through , such that determines the velocity of the fluid at . The flux is defined as the quantity of fluid flowing through in unit amount of time. To find the flux, we need to take the dot product of with the unit surface normal to at each point, which will give us a scalar field, which we integrate over the surface:
The fluid flux in this example may be from a physical fluid such as water or air, or from electrical or magnetic flux. Thus surface integrals have applications in physics, particularly with the classical theory of electromagnetism.
Contour integrals.
In complex analysis, the integrand is a complex-valued function of a complex variable instead of a real function of a real variable . When a complex function is integrated along a curve formula_54 in the complex plane, the integral is denoted as follows
This is known as a contour integral.
Integrals of differential forms.
A differential form is a mathematical concept in the fields of multivariable calculus, differential topology, and tensors. Differential forms are organized by degree. For example, a one-form is a weighted sum of the differentials of the coordinates, such as:
where "E", "F", "G" are functions in three dimensions. A differential one-form can be integrated over an oriented path, and the resulting integral is just another way of writing a line integral. Here the basic differentials "dx", "dy", "dz" measure infinitesimal oriented lengths parallel to the three coordinate axes.
A differential two-form is a sum of the form
Here the basic two-forms formula_58 measure oriented areas parallel to the coordinate two-planes. The symbol formula_59 denotes the wedge product, which is similar to the cross product in the sense that the wedge product of two forms representing oriented lengths represents an oriented area. A two-form can be integrated over an oriented surface, and the resulting integral is equivalent to the surface integral giving the flux of formula_60.
Unlike the cross product, and the three-dimensional vector calculus, the wedge product and the calculus of differential forms makes sense in arbitrary dimension and on more general manifolds (curves, surfaces, and their higher-dimensional analogs). The exterior derivative plays the role of the gradient and curl of vector calculus, and Stokes' theorem simultaneously generalizes the three theorems of vector calculus: the divergence theorem, Green's theorem, and the Kelvin-Stokes theorem.
Summations.
The discrete equivalent of integration is summation. Summations and integrals can be put on the same foundations using the theory of Lebesgue integrals or time scale calculus.
Computation.
Analytical.
The most basic technique for computing definite integrals of one real variable is based on the fundamental theorem of calculus. Let be the function of to be integrated over a given interval . Then, find an antiderivative of ; that is, a function such that on the interval. Provided the integrand and integral have no singularities on the path of integration, by the fundamental theorem of calculus,
The integral is not actually the antiderivative, but the fundamental theorem provides a way to use antiderivatives to evaluate definite integrals.
The most difficult step is usually to find the antiderivative of . It is rarely possible to glance at a function and write down its antiderivative. More often, it is necessary to use one of the many techniques that have been developed to evaluate integrals. Most of these techniques rewrite one integral as a different one which is hopefully more tractable. Techniques include:
Alternative methods exist to compute more complex integrals. Many nonelementary integrals can be expanded in a Taylor series and integrated term by term. Occasionally, the resulting infinite series can be summed analytically. The method of convolution using Meijer G-functions can also be used, assuming that the integrand can be written as a product of Meijer G-functions. There are also many less common ways of calculating definite integrals; for instance, Parseval's identity can be used to transform an integral over a rectangular region into an infinite sum. Occasionally, an integral can be evaluated by a trick; for an example of this, see Gaussian integral.
Computations of volumes of solids of revolution can usually be done with disk integration or shell integration.
Specific results which have been worked out by various techniques are collected in the list of integrals.
Symbolic.
Many problems in mathematics, physics, and engineering involve integration where an explicit formula for the integral is desired. Extensive tables of integrals have been compiled and published over the years for this purpose. With the spread of computers, many professionals, educators, and students have turned to computer algebra systems that are specifically designed to perform difficult or tedious tasks, including integration. Symbolic integration has been one of the motivations for the development of the first such systems, like Macsyma.
A major mathematical difficulty in symbolic integration is that in many cases, a closed formula for the antiderivative of a rather simple-looking function does not exist. For instance, it is known that the antiderivatives of the functions and cannot be expressed in the closed form involving only rational and exponential functions, logarithm, trigonometric and inverse trigonometric functions, and the operations of multiplication and composition; in other words, none of the three given functions is integrable in elementary functions, which are the functions which may be built from rational functions, roots of a polynomial, logarithm, and exponential functions. The Risch algorithm provides a general criterion to determine whether the antiderivative of an elementary function is elementary, and, if it is, to compute it. Unfortunately, it turns out that functions with closed expressions of antiderivatives are the exception rather than the rule. Consequently, computerized algebra systems have no hope of being able to find an antiderivative for a randomly constructed elementary function. On the positive side, if the 'building blocks' for antiderivatives are fixed in advance, it may be still be possible to decide whether the antiderivative of a given function can be expressed using these blocks and operations of multiplication and composition, and to find the symbolic answer whenever it exists. The Risch algorithm, implemented in Mathematica and other computer algebra systems, does just that for functions and antiderivatives built from rational functions, radicals, logarithm, and exponential functions.
Some special integrands occur often enough to warrant special study. In particular, it may be useful to have, in the set of antiderivatives, the special functions of physics (like the Legendre functions, the hypergeometric function, the Gamma function, the Incomplete Gamma function and so on — see Symbolic integration for more details). Extending the Risch's algorithm to include such functions is possible but challenging and has been an active research subject.
More recently a new approach has emerged, using "D"-finite function, which are the solutions of linear differential equations with polynomial coefficients. Most of the elementary and special functions are "D"-finite and the integral of a "D"-finite function is also a "D"-finite function. This provide an algorithm to express the antiderivative of a "D"-finite function as the solution of a differential equation.
This theory allows also to compute a definite integrals of a "D"-function as the sum of a series given by the first coefficients and an algorithm to compute any coefficient.
Numerical.
Some integrals found in real applications can be computed by closed-form antiderivatives. Others are not so accommodating. Some antiderivatives do not have closed forms, some closed forms require special functions which themselves are a challenge to compute, and others are so complex that finding the exact answer is too slow. This motivates the study and application of numerical approximations of integrals. This subject, called "numerical integration" or "numerical quadrature", arose early in the study of integration for the purpose of making hand calculations. The development of general-purpose computers made numerical integration more practical and drove a desire for improvements. The goals of numerical integration are accuracy, reliability, efficiency, and generality, and sophisticated modern methods can vastly outperform a naive method by all four measures (; ; ).
Consider, for example, the integral
which has the exact answer . (In ordinary practice the answer is not known in advance, so an important task — not explored here — is to decide when an approximation is good enough.) A “calculus book” approach divides the integration range into, say, 16 equal pieces, and computes function values.
Using the left end of each piece, the rectangle method sums 16 function values and multiplies by the step width, , here 0.25, to get an approximate value of 3.94325 for the integral. The accuracy is not impressive, but calculus formally uses pieces of infinitesimal width, so initially this may seem little cause for concern. Indeed, repeatedly doubling the number of steps eventually produces an approximation of 3.76001. However, 218 pieces are required, a great computational expense for such little accuracy; and a reach for greater accuracy can force steps so small that arithmetic precision becomes an obstacle.
A better approach replaces the rectangles used in a Riemann sum with trapezoids. The trapezoid rule is almost as easy to calculate; it sums all 17 function values, but weights the first and last by one half, and again multiplies by the step width. This immediately improves the approximation to 3.76925, which is noticeably more accurate. Furthermore, only 210 pieces are needed to achieve 3.76000, substantially less computation than the rectangle method for comparable accuracy. The idea behind the trapezoid rule, that more accurate approximations to the function yield better approximations to the integral, can be carried further. Simpson's rule approximates the integrand by a piecewise quadratic function. Riemann sums, the trapezoid rule, and Simpson's rule are examples of a family of quadrature rules called Newton–Cotes formulas. The degree Newton–Cotes quadrature rule approximates the polynomial on each subinterval by a degree polynomial. This polynomial is chosen to interpolate the values of the function on the interval. Higher degree Newton-Cotes approximations can be more accurate, but they require more function evaluations (already Simpson's rule requires twice the function evaluations of the trapezoid rule), and they can suffer from numerical inaccuracy due to Runge's phenomenon. One solution to this problem is Clenshaw–Curtis quadrature. In Clenshaw–Curtis quadrature, the integrand is approximated by expanding it in terms of Chebyshev polynomials. This produces an approximation whose values never deviate far from those of the original function.
Romberg's method builds on the trapezoid method to great effect. First, the step lengths are halved incrementally, giving trapezoid approximations denoted by , and so on, where is half of . For each new step size, only half the new function values need to be computed; the others carry over from the previous size (as shown in the table above). But the really powerful idea is to interpolate a polynomial through the approximations, and extrapolate to . With this method a numerically "exact" answer here requires only four pieces (five function values). The Lagrange polynomial interpolating {(4.00,6.128), (2.00,4.352), (1.00,3.908)} is 3.76 + 0.148, producing the extrapolated value 3.76 at .
Gaussian quadrature often requires noticeably less work for superior accuracy. In this example, it can compute the function values at just two positions, , then double each value and sum to get the numerically exact answer. The explanation for this dramatic success lies in the choice of points. Unlike Newton–Cotes rules, which interpolate the integrand at evenly spaced points, Gaussian quadrature evaluates the function at the roots of a set of orthogonal polynomials. An -point Gaussian method is exact for polynomials of degree up to . The function in this example is a degree 3 polynomial, plus a term that cancels because the chosen endpoints are symmetric around zero. (Cancellation also benefits the Romberg method.)
In practice, each method must use extra evaluations to ensure an error bound on an unknown function; this tends to offset some of the advantage of the pure Gaussian method, and motivates the popular Gauss–Kronrod quadrature formulae. More broadly, adaptive quadrature partitions a range into pieces based on function properties, so that data points are concentrated where they are needed most.
The computation of higher-dimensional integrals (for example, volume calculations) makes important use of such alternatives as Monte Carlo integration.
A calculus text is no substitute for numerical analysis, but the reverse is also true. Even the best adaptive numerical code sometimes requires a user to help with the more demanding integrals. For example, improper integrals may require a change of variable or methods that can avoid infinite function values, and known properties like symmetry and periodicity may provide critical leverage. For example, the integral formula_63 is difficult to evaluate numerically because it is infinite at . However, the substitution transforms the integral into formula_64, which has no singularities at all.
Mechanical.
The area of an arbitrary two-dimensional shape can be determined using a measuring instrument called planimeter. The volume of irregular objects can be measured with precision by the fluid displaced as the object is submerged.
Geometrical.
Area can be found via geometrical compass-and-straightedge constructions of an equivalent square, e.g., squaring the circle.

</doc>
<doc id="474234" url="https://en.wikipedia.org/wiki?curid=474234" title="Integral equation">
Integral equation

In mathematics, an integral equation is an equation in which an unknown function appears under an integral sign. There is a close connection between differential and integral equations, and some problems may be formulated either way. See, for example, Maxwell's equations.
Overview.
The most basic type of integral equation is called a "Fredholm equation of the first type":
The notation follows Arfken. Here is an unknown function, is a known function,
and "K" is another known function of two variables, often called the kernel function. Note that the limits of integration are constant; this is what characterizes a Fredholm equation.
If the unknown function occurs both inside and outside of the integral, it is known as a "Fredholm equation of the second type":
The parameter is an unknown factor, which plays the same role as the eigenvalue in linear algebra.
If one limit of integration is variable, it is called a Volterra equation. The following are called "Volterra equations of the first and second types", respectively:
In all of the above, if the known function is identically zero, it is called a "homogeneous integral equation". If is nonzero, it is called an "inhomogeneous integral equation".
Numerical Solution.
It is worth noting that Integral Equations often do not have an analytical solution, and must be solved numerically. An example of this is evaluating the Electric-Field Integral Equation (EFIE) or Magnetic-Field Integral Equation (MFIE) over an arbitrarily shaped object in an electromagnetic scattering problem.
One method to solve numerically requires discretizing variables and replacing integral by a quadrature rule
Then we have a system with equations and variables. By solving it we get the value of the variables
Classification.
Integral equations are classified according to three different dichotomies, creating eight different kinds:
Integral equations are important in many applications. Problems in which integral equations are encountered include radiative energy transfer and the oscillation of a string, membrane, or axle. Oscillation problems may also be solved as differential equations.
Both Fredholm and Volterra equations are linear integral equations, due to the linear behaviour of under the integral. A nonlinear Volterra integral equation has the general form:
where is a known function.
Wiener-Hopf integral equations.
Originally, such equations were studied in connection with problems in radiative transfer, and more recently, they have been related to the solution of boundary integral equations for planar problems in which the boundary is only piecewise smooth.
Power series solution for integral equations.
In many cases if the Kernel of the integral equation is of the form and the Mellin transform of exists we can find the solution of the integral equation
in a form of a power series
with
are the Z-transform of the function and is the Mellin transform of the Kernel.
Integral equations as a generalization of eigenvalue equations.
Certain homogeneous linear integral equations can be viewed as the continuum limit of eigenvalue equations. Using index notation, an eigenvalue equation can be written as
where is a matrix, is one of its eigenvectors, and is the associated eigenvalue.
Taking the continuum limit, by replacing the discrete indices and with continuous variables and , gives
where the sum over has been replaced by an integral over and the matrix and the vector have been replaced by the "kernel" and the eigenfunction . (The limits on the integral are fixed, analogously to the limits on the sum over .) This gives a linear homogeneous Fredholm equation of the second type.
In general, can be a distribution, rather than a function in the strict sense. If the distribution has support only at the point , then the integral equation reduces to a differential eigenfunction equation.

</doc>
<doc id="147252" url="https://en.wikipedia.org/wiki?curid=147252" title="Integration by parts">
Integration by parts

In calculus, and more generally in mathematical analysis, integration by parts is a theorem that relates the integral of a product of functions to the integral of their derivative and antiderivative. It is frequently used to transform the antiderivative of a product of functions into an antiderivative for which a solution can be more easily found. The rule can be derived in one line simply by integrating the product rule of differentiation.
If and , while and , then integration by parts states that:
or more compactly:
More general formulations of integration by parts exist for the Riemann–Stieltjes integral and Lebesgue–Stieltjes integral. The discrete analogue for sequences is called summation by parts.
Theorem.
Product of two functions.
The theorem can be derived as follows. Suppose "u"("x") and "v"("x") are two continuously differentiable functions. The product rule states (in Leibniz’ notation):
Integrate both sides with respect to "x",
then apply the definition of indefinite integral,
gives the formula for integration by parts.
Since "du" and "dv" are differentials of a function of one variable "x",
The original integral ∫"uv"′ "dx" contains "v"′ (derivative of "v"); in order to apply the theorem, "v" (antiderivative of "v"′) must be found, and then the resulting integral ∫"vu"′ "dx" must be evaluated.
Product of many functions.
Integrating the product rule for three multiplied functions, "u"("x"), "v"("x"), "w"("x"), gives a similar result:
In general for "n" factors
which leads to
where the product is of all functions except for the one differentiated in the same term.
Visualization.
Define a parametric curve by ("x", "y") = ("f"("t"), "g"("t")). Assuming that the curve is locally one-to-one, we can define
The area of the blue region is
Similarly, the area of the red region is
The total area "A"1 + "A"2 is equal to the area of the bigger rectangle, "x"2"y"2, minus the area of the smaller one, "x"1"y"1:
Assuming the curve is smooth within a neighborhood, this generalizes to indefinite integrals:
Rearranging:
Thus integration by parts may be thought of as deriving the area of the blue region from the total area and that of the red region.
This visualisation also explains why integration by parts may help find the integral of an inverse function "f"−1("x") when the integral of the function "f"("xv") is known. Indeed, the functions "x"("y") and "y"("x") are inverses, and the integral ∫"x dy" may be calculated as above from knowing the integral ∫"y dx".
Application to find antiderivatives.
Strategy.
Integration by parts is a heuristic rather than a purely mechanical process for solving integrals; given a single function to integrate, the typical strategy is to carefully separate it into a product of two functions "u"("x")"v"("x") such that the integral produced by the integration by parts formula is easier to evaluate than the original one. The following form is useful in illustrating the best strategy to take:
Note that on the right-hand side, "u" is differentiated and "v" is integrated; consequently it is useful to choose "u" as a function that simplifies when differentiated, and/or to choose "v" as a function that simplifies when integrated. As a simple example, consider:
Since the derivative of ln "x" is 1/"x", we make (ln "x") part "u"; since the antiderivative of 1/"x"2 is −1/"x", we make (1/"x"2)"dx" part "dv". The formula now yields:
The antiderivative of −1/"x"2 can be found with the power rule and is 1/"x".
Alternatively, we may choose "u" and "v" such that the product "u"' (∫"v dx") simplifies due to cancellation. For example, suppose we wish to integrate:
If we choose "u"("x") = ln |sin "x"| and "v"("x") = sec2x, then "u" differentiates to 1/ tan "x" using the chain rule and "v" integrates to tan "x"; so the formula gives:
The integrand simplifies to 1, so the antiderivative is "x". Finding a simplifying combination frequently involves experimentation.
In some applications, it may not be necessary to ensure that the integral produced by integration by parts has a simple form; for example, in numerical analysis, it may suffice that it has small magnitude and so contributes only a small error term. Some other special techniques are demonstrated in the examples below.
In order to calculate
let:
then:
where "C" is an arbitrary constant of integration.
For higher powers of "x" in the form
repeatedly using integration by parts can evaluate integrals such as these; each application of the theorem lowers the power of "x" by one.
An example commonly used to examine the workings of integration by parts is
Here, integration by parts is performed twice. First let
then:
Now, to evaluate the remaining integral, we use integration by parts again, with:
Then:
Putting these together,
The same integral shows up on both sides of this equation. The integral can simply be added to both sides to get
which rearranges to:
where again "C" (and "C"' = "C"/2) is an arbitrary constant of integration.
A similar method is used to find the integral of secant cubed.
Two other well-known examples are when integration by parts is applied to a function expressed as a product of 1 and itself. This works if the derivative of the function is known, and the integral of this derivative times "x" is also known.
The first example is ∫ ln("x") d"x". We write this as:
Let:
then:
where "C" is the constant of integration.
The second example is the inverse tangent function arctan("x"):
Rewrite this as
Now let:
then
using a combination of the inverse chain rule method and the natural logarithm integral condition.
LIATE rule.
A rule of thumb proposed by Herbert Kasube of Bradley University advises that whichever function comes first in the following list should be "u":
The function which is to be "dv" is whichever comes last in the list: functions lower on the list have easier antiderivatives than the functions above them. The rule is sometimes written as "DETAIL" where "D" stands for "dv".
To demonstrate the LIATE rule, consider the integral
Following the LIATE rule, "u" = "x" and "dv" = cos "x" dx, hence "du" = "dx" and "v" = sin "x", which makes the integral become
which equals
In general, one tries to choose "u" and "dv" such that "du" is simpler than "u" and "dv" is easy to integrate. If instead cos "x" was chosen as "u", and "x.dx" as "dv", we would have the integral
which, after recursive application of the integration by parts formula, would clearly result in an infinite recursion and lead nowhere.
Although a useful rule of thumb, there are exceptions to the LIATE rule. A common alternative is to consider the rules in the "ILATE" order instead. Also, in some cases, polynomial terms need to be split in non-trivial ways. For example, to integrate
one would set
so that
Then
Finally, this results in
Applications in pure mathematics.
Integration by parts is often used as a tool to prove theorems in mathematical analysis. This section gives a few examples.
Use in special functions.
The gamma function is an example of a special function, defined as an improper integral. Integration by parts illustrates it to be an extension of the factorial:
yielding the famous identity
For integer "z", applying this formula repeatedly gives the factorial (denoted by the !):
Use in harmonic analysis.
Integration by parts is often used in harmonic analysis, particularly Fourier analysis, to show that quickly oscillating integrals with sufficiently smooth integrands decay quickly. The most common example of this is its use in showing that the decay of function's Fourier transform depends on the smoothness of that function, as described below.
If "f" is a "k"-times continuously differentiable function and all derivatives up to the "k"th one decay to zero at infinity, then its Fourier transform satisfies
where is the "k"th derivative of "f". (The exact constant on the right depends on the convention of the Fourier transform used.) This is proved by noting that
so using integration by parts on the Fourier transform of the derivative we get
Applying this inductively gives the result for general "k". A similar method can be used to find the Laplace transform of a derivative of a function.
The above result tells us about the decay of the Fourier transform, since it follows that if "f" and are integrable then
In other words, if "f" satisfies these conditions then its Fourier transform decays at infinity at least as quickly as . In particular, if then the Fourier transform is integrable.
The proof uses the fact, which is immediate from the definition of the Fourier transform, that
Using the same idea on the equality stated at the start of this subsection gives
Summing these two inequalities and then dividing by gives the stated inequality.
Use in operator theory.
One use of integration by parts in operator theory is that it shows that the (where ∆ is the Laplace operator) is a positive operator on (see "L""p" space). If "f" is smooth and compactly supported then, using integration by parts, we have
Recursive integration by parts.
Integration by parts can often be applied recursively to provide the following formula
This form is especially useful when "v"("n") becomes zero for some "n" (and, in particular, when "v" is a polynomial function with degree smaller than "n"). Hence, the integral evaluation can stop once the "v"("n" − 1) term has been reached.
Tabular integration by parts.
While the aforementioned recursive definition is correct, it is often tedious to remember and implement. A much easier visual representation of this process is often taught to students and is dubbed either "the tabular method", "the "Stand and Deliver" method", "rapid repeated integration" or "the tic-tac-toe method". This method works best when one of the two functions in the product is a polynomial, that is, after differentiating it several times one obtains zero. It may also be extended to work for functions that will repeat themselves.
For example, consider the integral
Let "u" = "x"3. Begin with this function and list in a column all the subsequent derivatives until zero is reached. Secondly, begin with the function "v" (in this case cos("x")) and list each integral of "v" until the size of the column is the same as that of "u". The result should appear as follows.
Now simply pair the 1st entry of column A with the 2nd entry of column B, the 2nd entry of column A with the 3rd entry of column B, etc... with alternating signs (beginning with the positive sign). Do so until further pairing leads to sums of zeros. The result is the following (notice the alternating signs in each term):
Which, with simplification, leads to the result
With proper understanding of the tabular method, it can be extended. Consider
In this case in the last step it is necessary to integrate the product of the two bottom cells obtaining:
which leads to
and yields the result:
Higher dimensions.
The formula for integration by parts can be extended to functions of several variables. Instead of an interval one needs to integrate over an "n"-dimensional set. Also, one replaces the derivative with a partial derivative.
More specifically, suppose Ω is an open bounded subset of ℝ"n" with a piecewise smooth boundary Γ. If "u" and "v" are two continuously differentiable functions on the closure of Ω, then the formula for integration by parts is
where formula_78 is the outward unit surface normal to Γ, formula_79 is its "i"-th component, and "i" ranges from 1 to "n".
Replacing "v" in the above formula with "v""i" and summing over "i" gives the vector formula
where v is a vector-valued function with components "v"1, ..., "v""n".
Setting "u" equal to the constant function 1 in the above formula gives the divergence theorem
For formula_82 where formula_83, one gets
which is the first Green's identity.
The regularity requirements of the theorem can be relaxed. For instance, the boundary Γ need only be Lipschitz continuous. In the first formula above, only "u", "v" ∈ "H"1(Ω) is necessary (where "H"1 is a Sobolev space); the other formulas have similarly relaxed requirements.

</doc>
<doc id="516756" url="https://en.wikipedia.org/wiki?curid=516756" title="Internal conversion">
Internal conversion

Internal conversion is a radioactive decay process wherein an excited nucleus interacts electromagnetically with one of the orbital electrons of the atom. This causes the electron to be emitted (ejected) from the atom. Thus, in an internal conversion process, a high-energy electron is emitted from the radioactive atom, but not from the nucleus. For this reason, the high-speed electrons resulting from internal conversion are not beta particles, since the latter come from beta decay, where they are newly created in the nuclear decay process. 
Internal conversion is possible whenever gamma decay is possible, except in the case where the atom is fully ionised. During internal conversion, the atomic number does not change, and thus (as is the case with gamma decay) no transmutation of one element to another takes place. 
Since an electron is lost from the atom, a hole appears in an electron shell which is subsequently filled by other electrons. This process produces characteristic X-ray(s), Auger electron(s), or both. The atom thus emits high-energy electrons and X-ray photons, but none of these originate in the nucleus, which supplies only the needed energy.
Since primary electrons from internal conversion carry a fixed (large) fraction of the characteristic decay energy, they have a discrete energy spectrum, rather than the spread (continuous) spectrum characteristic of beta particles. Whereas the energy spectrum of beta particles plots as a broad hump, the energy spectrum of internally converted electrons plots as a single sharp peak (see example below).
Mechanism.
In the quantum mechanical mathematical model for the internal conversion process, the wavefunction of an inner shell electron (usually an "s" electron) penetrates the volume of the atomic nucleus. This means that there is a finite probability of finding the electron within the nucleus. When this happens, the electron may couple to an excited energy state of the nucleus and take the energy of the nuclear transition directly, without an intermediate gamma ray being first produced. The kinetic energy of the emitted electron is equal to the transition energy in the nucleus, minus the binding energy of the electron to the atom.
Most internal conversion (IC) electrons come from the K shell (the 1s state), as these two electrons have the highest probability of being within the nucleus. However, the s states in the L, M, and N shells (i.e., the 2s, 3s, and 4s states) are also able to couple to the nuclear fields and cause IC electron ejections from those shells (called L or M or N internal conversion). Ratios of K-shell to other L, M, or N shell internal conversion probabilities for various nuclides have been prepared.
The atomic binding energy of the s electron must at least be supplied to that electron in order to eject it from the atom to result in IC; that is to say, internal conversion cannot happen if the decay energy of the nucleus is insufficient to overcome the binding energy. There are a few radionuclides in which the decay energy is not sufficient to convert (eject) a 1s (K shell) electron, and these nuclides, to decay by internal conversion, must decay by ejecting electrons from the L or M or N shells (i.e., by ejecting 2s, 3s, or 4s electrons) as these binding energies are lower.
Although s electrons are more likely for IC processes due to their superior nuclear penetration compared to electrons with orbital angular momentum, spectral studies show that p electrons (from shells L and higher) are occasionally ejected in the IC process.
After the IC electron has been emitted, the atom is left with a vacancy in one of its electron shells, usually an inner one. This hole will be filled with an electron from one of the higher shells, and consequently one or more characteristic X-rays or Auger electrons will be emitted as the remaining electrons in the atom cascade down to fill the vacancy.
An example: the decay of 203Hg.
The decay scheme on the left shows that 203Hg produces a continuous beta spectrum with maximum energy 214 keV, that leads to an excited state of the daughter nucleus 203Tl. This state decays very fast (within 2.8×10-10 s) to the ground state of 203Tl, emitting a gamma quantum of 279 keV.
The figure on the right shows the electron spectrum of 203Hg, measured by means of a magnetic spectrometer. You can see the continuous beta spectrum and also the K-, L-, and M-lines due to internal conversion. Since the binding energy of the K electrons in 203Tl amounts to 85 keV, the K line has an energy of 279 - 85 = 194 keV. Because of lesser binding energies, the L- and M-lines have higher energies. Because of the finite energy resolution of the spectrometer, the "lines" have a Gaussian shape of finite width.
When the process is expected.
Internal conversion (often abbreviated IC) is favoured whenever the energy available for a gamma transition is small, and it is also the primary mode of de-excitation for 0+→0+ (i.e. E0) transitions. The 0+→0+ transitions occur where an excited nucleus has zero-spin and positive parity, and decays to a ground state which also has zero-spin and positive parity (such as all nuclides with even numbers of protons and neutrons). In such cases, de-excitation cannot happen with emission of a single gamma ray, since this would violate conservation of angular momentum, so other mechanisms like IC predominate. This also shows that internal conversion (contrary to its name) is not a two-step process where a gamma ray would be first emitted and then converted.
The competition between internal conversion and gamma decay is quantified in the form of the internal conversion coefficient which is defined as formula_1 where formula_2 is the rate of conversion electrons and formula_3 is the rate of gamma-ray emission observed from a decaying nucleus. For example, in the decay of the excited state at 35 keV of 125Te (which is produced by the decay of 125I), 7% of the decays emit energy as a gamma ray, while 93% release energy as conversion electrons. Therefore, this excited state of has an internal conversion coefficient of formula_4.
For increasing atomic number (Z) and decreasing gamma-ray energy, internal conversion coefficients are observed to increase. As an example, calculated IC coefficients for electric dipole (E1) transitions, for Z = 40, 60, and 80, are shown in the figure.
The energy of the emitted gamma ray is a precise measure of the difference in energy between the excited states of the decaying nucleus. In the case of conversion electrons, the binding energy must also be taken into account: The energy of a conversion electron is given as formula_5, where formula_6 and formula_7 are the energies of the nucleus in its initial and final states, respectively, while formula_8 is the binding energy of the electron.
Similar processes.
Nuclei with zero-spin and high excitation energies (more than about 1.022 MeV) are also unable to rid themselves of energy by (single) gamma emission, but they do have sufficient decay energy to decay by pair production. In this type of decay, an electron and positron are both emitted from the atom at the same time, and conservation of angular momentum is solved by having these two product particles spin in opposite directions.
The internal conversion process should not be confused with the similar photoelectric effect. When a gamma ray emitted by the nucleus of an atom hits a different atom, it may be absorbed producing a photoelectron of well-defined energy (This used to be called "external conversion"). In internal conversion, however, the process happens within one atom, and without a real intermediate gamma ray.
As an atom may produce an internal conversion electron in place of a gamma ray, an atom may produce an Auger electron in place of an x ray if an electron is missing from one of the electron shells. Like IC electrons, Auger electrons have a discrete energy, resulting in a sharp energy peak in the spectrum.
The electron capture process also involves an inner shell electron, which in this case is retained in the nucleus (changing the atomic number) and leaving the atom (not the nucleus) in an excited state. The atom missing an inner electron can relax by a cascade of X-ray emissions as higher energy electrons in the atom fall to fill the vacancy left in the electron cloud by the captured electron. Such atoms also typically exhibit Auger electron emission. Electron capture, like beta decay, also typically results in excited atomic nuclei, which may then relax to a state of lowest nuclear energy by any of the methods permitted by spin constraints, including gamma decay and internal conversion decay.

</doc>
<doc id="26764" url="https://en.wikipedia.org/wiki?curid=26764" title="International System of Units">
International System of Units

The International System of Units (, SI) is the modern form of the metric system, and is the most widely used system of measurement. It comprises a coherent system of units of measurement built on seven base units. It defines twenty-two named units, and includes many more unnamed coherent derived units. The system also establishes a set of twenty prefixes to the unit names and unit symbols that may be used when specifying multiples and fractions of the units.
The system was published in 1960 as the result of an initiative that started in 1948. It is based on the metre-kilogram-second system of units (MKS) rather than any variant of the centimetre–gram–second system (CGS). SI is intended to be an evolving system, so prefixes and units are created and unit definitions are modified through international agreement as the technology of measurement progresses and the precision of measurements improves. The 25th General Conference on Weights and Measures (CGPM) in 2014, for example, discussed a proposal to change the definition of the kilogram.
The motivation for the development of the SI was the diversity of units that had sprung up within the CGS systems and the lack of coordination between the various disciplines that used them. The CGPM, which was established by the Metre Convention of 1875, brought together many international organisations to not only agree on the definitions and standards of the new system but also agree rules on writing and presenting measurements in a standardised manner around the world.
The International System of Units has been adopted by most developed countries, however, the adoption has not been universal in all English-speaking countries. While metrication in the United States is consistent in science, medicine, government, and various fields of technology and engineering, common measurements are mostly performed in United States customary units, although these have officially been defined in terms of SI units. The United Kingdom has officially adopted a policy of partial metrication, with no intention of replacing imperial units immediately. Canada has adopted the SI for most governmental, medical and scientific purposes and for such varied uses as grocery weights, weather reports, traffic signs and gasoline sales, but imperial units are still legally permitted and remain in common use throughout many sectors of Canadian society, particularly in the building trade and the railway sector.
History.
The metric system was first implemented during the French Revolution (1790s) with just the metre and kilogram as standards of length and mass respectively. In the 1830s Carl Friedrich Gauss laid the foundations for a coherent system based on length, mass, and time. In the 1860s a group working under the auspices of the British Association for the Advancement of Science formulated the requirement for a coherent system of units with base units and derived units. The inclusion of electrical units into the system was hampered by the customary use of more than one set of units, until 1900 when Giovanni Giorgi identified the need to define one single electrical quantity as a fourth base quantity alongside the original three base quantities.
Meanwhile, in 1875, the Treaty of the Metre passed responsibility for verification of the kilogram and metre against agreed prototypes from French to international control. In 1921, the Treaty was extended to include all physical quantities including electrical units originally defined in 1893.
In 1948, an overhaul of the metric system was set in motion which resulted in the development of the "Practical system of units" which, on its publication in 1960, was given the name "The International System of Units". In 1954, the 10th General Conference on Weights and Measures (CGPM) identified electric current as the fourth base quantity in the practical system of units and added two more base quantities—temperature and luminous intensity—making six base quantities in all. The units associated with these quantities were the metre, kilogram, second, ampere, kelvin and candela. In 1971, a seventh base quantity, amount of substance represented by the mole, was added to the definition of SI.
Early development.
The metric system was developed from 1791 onwards by a committee of the French Academy of Sciences, commissioned by the National Assembly and Louis XVI to create a unified and rational system of measures. The group, which included Antoine Lavoisier (the "father of modern chemistry") and the mathematicians Pierre-Simon Laplace and Adrien-Marie Legendre, used the same principles for relating length, volume, and mass that had been proposed by the English clergyman John Wilkins in 1668 and the concept of using the Earth's meridian as the basis of the definition of length, originally proposed in 1670 by the French abbot Mouton.
On 30 March 1791, the Assembly adopted the committee's proposed principles for the new decimal system of measure and authorised a survey between Dunkirk and Barcelona to establish the length of the meridian. On 11 July 1792, the committee proposed the names "metre", "are", "litre" and "grave" for the units of length, area, capacity, and mass, respectively. The committee also proposed that multiples and submultiples of these units were to be denoted by decimal-based prefixes such as "centi" for a hundredth and "kilo" for a thousand.
The law of 7 April 1795 () defined the terms "gramme" and "kilogramme", which replaced the former terms "gravet" (correctly "milligrave") and "grave", and on 22 June 1799 (after Pierre Méchain and Jean-Baptiste Delambre had completed the meridian survey) the definitive standard "mètre des Archives" and "kilogramme des Archives" were deposited in the "Archives nationales". On 10 December 1799 (a month after Napoleon's coup d'état), the law by which the metric system was to be definitively adopted in France () 
was passed.
During the first half of the nineteenth century there was little consistency in the choice of preferred multiples of the base units – typically the myriametre ( metres) was in widespread use in both France and parts of Germany, while the kilogram ( grams) rather than the myriagram was used for mass.
In 1832, the German mathematician Carl Friedrich Gauss, assisted by Wilhelm Weber, implicitly defined the second as a base unit when he quoted the earth's magnetic field in terms of millimetres, grams, and seconds. Prior to this, the strength of the earth’s magnetic field had only been described in relative terms. The technique used by Gauss was to equate the torque induced on a suspended magnet of known mass by the earth’s magnetic field with the torque induced on an equivalent system under gravity. The resultant calculations enabled him to assign dimensions based on mass, length and time to the magnetic field.
In the 1860s, James Clerk Maxwell, William Thomson (later Lord Kelvin) and others working under the auspices of the British Association for the Advancement of Science, built on Gauss' work and formalised the concept of a coherent system of units with base units and derived units. The principle of coherence was successfully used to define a number of units of measure based on the centimetre–gram–second (CGS) system of units (CGS), including the erg for energy, the dyne for force, the barye for pressure, the poise for dynamic viscosity and the stokes for kinematic viscosity.
Metre Convention.
A French-inspired initiative for international cooperation in metrology led to the signing in 1875 of the Metre Convention. Initially the convention only covered standards for the metre and the kilogram. A set of 30 prototypes of the metre and 40 prototypes of the kilogram, in each case made of a 90% platinum-10% iridium alloy, were manufactured by the British firm Johnson, Matthey & Co and accepted by the CGPM in 1889. One of each was selected at random to become the International prototype metre and International prototype kilogram that replaced the "mètre des Archives" and "kilogramme des Archives" respectively. Each member state was entitled to one of each of the remaining prototypes to serve as the national prototype for that country.
The treaty established three international organisations to oversee the keeping of international standards of measurement:
In 1921, the Metre Convention was extended to include all physical units, including the ampere and others defined by the Fourth International Conference of Electricians in Chicago in 1893, thereby enabling the CGPM to address inconsistencies in the way that the metric system had been used.
The official language of the Metre Convention is French and the definitive version of all official documents published by or on behalf of the CGPM is the French-language version.
Towards the SI.
At the close of the 19th century three different systems of units of measure existed for electrical measurements: a CGS-based system for electrostatic units, also known as the Gaussian or ESU system, a CGS-based system for electromechanical units (EMU) and an MKS-based system ("international system") for electrical distribution systems. 
Attempts to resolve the electrical units in terms of length, mass, and time using dimensional analysis was beset with difficulties—the dimensions depended on whether one used the ESU or EMU systems. This anomaly was resolved in 1900 when Giovanni Giorgi published a paper in which he advocated using a fourth base unit alongside the existing three base units. The fourth unit could be chosen to be electric current , voltage, or electrical resistance.
In the late 19th and early 20th centuries, a number of non-coherent units of measure based on the gram/kilogram, the centimetre/metre, and the second, such as the "Pferdestärke" (metric horsepower) for power, the darcy for permeability and the use of "millimetres of mercury" for the measurement of both barometric and blood pressure were developed or propagated, some of which incorporated standard gravity in their definitions.
At the end of the Second World War, a number of different systems of measurement were in use throughout the world. Some of these systems were metric system variations, whereas others were based on customary systems of measure. In 1948, after representations by the International Union of Pure and Applied Physics (IUPAP) and by the French Government, the 9th General Conference on Weights and Measures (CGPM) asked the International Committee for Weights and Measures (CIPM) to conduct an international study of the measurement needs of the scientific, technical, and educational communities and "to make recommendations for a single practical system of units of measurement, suitable for adoption by all countries adhering to the Metre Convention".
On the basis of the findings of this study, the 10th CGPM in 1954 decided that an international system should be derived from six base units to provide for the measurement of temperature and optical radiation in addition to mechanical and electromagnetic quantities. Six base units were recommended: the metre, kilogram, second, ampere, degree Kelvin (later renamed kelvin), and candela. In 1960, the 11th CGPM named the system the "International System of Units", abbreviated SI from the French name, . The BIPM has also described SI as "the modern metric system". The seventh base unit, the mole, was added in 1971 by the 14th CGPM.
International System of Quantities.
The International System of Quantities (ISQ) is a system based on seven base quantities: length, mass, time, electric current, thermodynamic temperature, amount of substance, and luminous intensity. Other quantities such as area, pressure, and electrical resistance are derived from these base quantities by clear non-contradictory equations. The ISQ defines the quantities that are measured with the SI units. The ISQ is defined in the international standard ISO/IEC 80000, and was finalised in 2009 with the publication of ISO 80000-1.
SI Brochure and conversion factors.
The CGPM publishes a brochure which defines and presents SI. Its official version is in French, in line with the Metre Convention. It leaves some scope for local interpretation, particularly regarding names and terms in different languages, so for example the United States' National Institute of Standards and Technology (NIST) has produced a version of the CGPM document (NIST SP 330) which clarifies local interpretation for English-language publications that use American English and another document (NIST SP 811) that gives general guidance for the use of SI in the United States and conversion factors between SI and customary units.
The writing and maintenance of the CGPM brochure is carried out by one of the committees of the International Committee for Weights and Measures (CIPM), the Consultative Committee for Units (CCU). The CIPM nominates the chairman of this committee, but the committee includes representatives of various other international bodies rather than CIPM or CGPM nominees. This committee thus provides a forum for the bodies concerned to provide input to the CIPM in respect of ongoing enhancements to SI.
The definitions of the terms "quantity", "unit", "dimension" etc. that are used in the "SI Brochure" are those given in the International vocabulary of metrology, a publication produced by the Joint Committee for Guides in Metrology (JCGM), a working group consisting of eight international standards organisations under the chairmanship of the director of the BIPM. The quantities and equations that define the SI units are now referred to as the "International System of Quantities" (ISQ), and are set out in the International Standard "ISO/IEC 80000 Quantities and Units".
Units and prefixes.
The International System of Units consists of a set of base units, a set of derived units with special names, and a set of decimal-based multipliers that are used as prefixes. The term "SI Units" covers all three categories, but the term "coherent SI units" includes only base units and coherent derived units.
Base units.
The SI base units are the building blocks of the system and all other units are derived from them. When Maxwell first introduced the concept of a coherent system, he identified three quantities that could be used as base units: mass, length and time. Giorgi later identified the need for an electrical base unit. Theoretically any one of electric current, potential difference, electrical resistance, electrical charge or a number of other quantities could have provided the base unit, with the remaining units then being defined by the laws of physics. In the event, the unit of electric current was chosen for SI. Another three base units (for temperature, substance and luminous intensity) were added later.
Derived units.
The derived units in the SI are formed by powers, products or quotients of the base units and are unlimited in number. Derived units are associated with derived quantities, for example velocity is a quantity that is derived from the base quantities of time and length, so in SI the derived unit is metres per second (symbol m/s). The dimensions of derived units can be expressed in terms of the dimensions of the base units.
Coherent units are derived units that contain no numerical factor other than 1—quantities such as standard gravity and density of water are absent from their definitions. In the example above, "one" newton is the force required to accelerate a mass of "one" kilogram by "one" metre per second squared. Since the SI units of mass and acceleration are kg and m·s−2 respectively and , the units of force (and hence of newtons) is formed by multiplication to give kg·m·s−2. Since the newton is part of a coherent set of units, the constant of proportionality is 1.
For the sake of convenience, some derived units have special names and symbols. Such units may themselves be used in combination with the names and symbols for base units and for other derived units to express the units of other derived quantities. For example, the SI unit of force is the newton (N), the SI unit of pressure is the pascal (Pa)—and the pascal can be defined as "newtons per square metre" (N/m2).
Prefixes.
Prefixes are added to unit names to produce multiple and sub-multiples of the original unit. All multiples are integer powers of ten, and above a hundred or below a hundredth all are integer powers of a thousand. For example, "kilo-" denotes a multiple of a thousand and "milli-" denotes a multiple of a thousandth, so there are one thousand millimetres to the metre and one thousand metres to the kilometre. The prefixes are never combined, so for example a millionth of a metre is a "micrometre", not a millimillimetre. Multiples of the kilogram are named as if the gram were the base unit, so a millionth of a kilogram is a "milligram", not a microkilogram.
Non-SI units accepted for use with SI.
Although, in theory, SI can be used for any physical measurement, the CIPM has recognised that some non-SI units still appear in the scientific, technical, and commercial literature, and will continue to be used for many years to come. In addition, certain other units are so deeply embedded in the history and culture of the human race that they will continue to be used for the foreseeable future. The CIPM has catalogued several such units and published them in the SI Brochure so that their use may be consistent around the world. These units have been grouped as follows: 
Writing unit symbols and the values of quantities.
Before 1948, the writing of metric quantities was haphazard. In 1879, the CIPM published recommendations for writing the symbols for length, area, volume and mass, but it was outside its domain to publish recommendations for other quantities. Beginning in about 1900, physicists who had been using the symbol "μ" for "micrometre" (or "micron"), "λ" for "microlitre", and "γ" for "microgram" started to use the symbols "μm", "μL" and "μg", but it was only in 1935, a decade after the revision of the Metre Convention that the CIPM formally adopted this proposal and recommended that the symbol "μ" be used universally as a prefix for .
In 1948, the ninth CGPM approved the first formal recommendation for the writing of symbols in the metric system when the basis of the rules as they are now known was laid down. These rules were subsequently extended by International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) and now cover unit symbols and names, prefix symbols and names, how quantity symbols should be written and used and how the values of quantities should be expressed. Both ISO and the IEC have published rules for the presentation of SI units that are generally compatible with those published in the SI Brochure. ISO and IEC were in the process of merging their standards for quantities and units into a single set of compatible documents identified as the ISO/IEC 80000 Standard. The rules covering printing of quantities and units are part of ISO 80000-1:2009.
Unit names.
Names of units follow the grammatical rules associated with common nouns: in English and in French they start with a lowercase letter (e.g., newton, hertz, pascal), even when the symbol for the unit begins with a capital letter. This also applies to "degrees Celsius", since "degree" is the unit. In German, however, the names of units, as with all German nouns, start with capital letters. The spelling of unit names is a matter for the guardians of the language concerned – the official British and American spellings for certain SI units differ – British English, as well as Australian, Canadian and New Zealand English, uses the spelling "deca-", "metre", and "litre" whereas American English uses the spelling "deka-", "meter", and "liter", respectively.
Likewise, the plural forms of units follow the grammar of the language concerned: in English, the normal rules of English grammar are used, e.g. "henries" is the plural of "henry". However, the units lux, hertz, and siemens have irregular plurals in that they remain the same in both their singular and plural form.
In English, when unit names are combined to denote multiplication of the units concerned, they are separated with a hyphen or a space (e.g. newton-metre or newton metre). The plural is formed by converting the last unit name to the plural form (e.g. ten newton-metres).
Unit names as adjectives.
In English, a space is recommended between the number and the unit symbol when used as an adjective, e.g. "a 25 kg sphere".
The normal rules of English apply to unit names, where a hyphen is incorporated into the adjectival sense, e.g. "a 25-kilogram sphere".
Chinese and Japanese.
Chinese uses traditional logograms for writing the unit names, while in Japanese unit names are written in the phonetic katakana script; in both cases symbols are written using the internationally recognised Latin and Greek characters.
A set of characters representing various metric units was created in Japan in the late 19th century. Characters exist for three base units: the metre (), litre () and gram (). These were combined with a set of six prefix characters – "kilo-" (), "hecto-" (), "deca-" (), "deci-" (), "centi-" () and "milli-" () – to form an additional 18 single-character units. The seven length units (kilometre to millimetre), for example, are and . These characters, however, are not in common use today; instead, units are written out in katakana, the Japanese syllabary used for foreign borrowings, such as "" ("kiromētoru") for "kilometre". A few Sino-Japanese words for these units remain in use in Japanese, most significantly "" ("heibei") for "square metre", but otherwise borrowed pronunciations are used.
These characters are examples of the rare phenomenon of single-character loan words – a foreign word represented by a single Japanese character – and form the plurality of such words. Similar characters were also coined for other units, such as British units, though these also have fallen out of use; see Single character gairaigo: Metric units and Single character gairaigo: Other units for a full list.
The basic units are metre ( "mǐ"), litre ( "shēng"), gram ( "kè"), and second ( "miǎo"), while others include watt ( "wǎ"). Prefixes include "deci-" ( "fēn"), "centi-" ( "lí"), "milli-" ( "háo"), "micro-" ( "wēi"), "kilo-" ( "qiān"), and "mega-" ( "zhào"). These are combined to form disyllabic characters, such as "límǐ" "centimetre" or "qiānwǎ" "kilowatt". In the 19th century various compound characters were also used, similar to Japanese, either imported or formed on the same principles, such as for "qiānwǎ" (kilowatt) or for . These are generally not used today – for example centimetres is usually written "límǐ" – but are occasionally found in older or technical writing.
Unit symbols and the values of quantities.
Although the writing of unit names is language-specific, the writing of unit symbols and the values of quantities is consistent across all languages and therefore the SI Brochure has specific rules in respect of writing them. The guideline produced by the National Institute of Standards and Technology (NIST) clarifies language-specific areas in respect of American English that were left open by the SI Brochure, but is otherwise identical to the SI Brochure.
General rules.
General rules for writing SI units and quantities apply to text that is either handwritten or produced using an automated process:
Printing SI symbols.
Further rules are specified in respect of production of text using printing presses, word processors, typewriters and the like.
Realisation of units.
Metrologists carefully distinguish between the definition of a unit and its realisation. The definition of each base unit of the SI is drawn up so that it is unique and provides a sound theoretical basis on which the most accurate and reproducible measurements can be made. The realisation of the definition of a unit is the procedure by which the definition may be used to establish the value and associated uncertainty of a quantity of the same kind as the unit. A description of the "mise en pratique" of the base units is given in an electronic appendix to the SI Brochure.
The published "mise en pratique" is not the only way in which a base unit can be determined: the SI Brochure states that "any method consistent with the laws of physics could be used to realise any SI unit." In the current (2012) exercise to overhaul the definitions of the base units, various consultative committees of the CIPM have required that more than one "mise en pratique" shall be developed for determining the value of each unit. In particular:
Post-1960 changes.
The preamble to the Metre Convention read "Desiring the international uniformity and precision in standards of weight and measure, have resolved to conclude a convention ...". Changing technology has led to an evolution of the definitions and standards that has followed two principal strands – changes to SI itself and clarification of how to use units of measure that are not part of SI, but are still nevertheless used on a worldwide basis.
Changes to the SI.
Since 1960 the CGPM has made a number of changes to SI. These include:
In addition, advantage was taken of developments in technology to redefine many of the base units enabling the use of higher precision techniques.
Retention of non-SI units.
Although, in theory, SI can be used for any physical measurement, it is recognised that some non-SI units still appear in the scientific, technical and commercial literature, and will continue to be used for many years to come. In addition, certain other units are so deeply embedded in the history and culture of the human race that they will continue to be used for the foreseeable future. The CIPM has catalogued such units and included them in the SI Brochure so that they can be used consistently.
The first such group comprises the units of time and of angles and certain legacy non-SI metric units. Most of mankind has used the day and its subdivisions as a basis of time with the result that the second, minute, hour and day, unlike the foot or the pound, were the same regardless of where it was being measured. The second has been catalogued as an SI unit, its multiples as units of measure that may be used alongside the SI. The measurement of angles has likewise had a long history of consistent use – the radian, being of a revolution, has mathematical niceties, but it is cumbersome for navigation, hence the retention of the degree, minute and second of arc. The tonne, litre and hectare were adopted by the CGPM in 1879 and have been retained as units that may be used alongside SI units, having been given unique symbols.
Physicists often use units of measure that are based on natural phenomena such as the speed of light, the mass of a proton (approximately one dalton), the charge of an electron and the like. These too have been catalogued in the SI Brochure with consistent symbols, but with the caveat that their physical values need to be measured.
In the interests of standardising health-related units of measure used in the nuclear industry, the 12th CGPM (1964) accepted the continued use of the curie (symbol Ci) as a non-SI unit of activity for radionuclides; the becquerel, sievert and gray were adopted in later years. Similarly, the millimetre of mercury (symbol mmHg) was retained for measuring blood pressure.
Global adoption.
SI has become the world's most widely used system of measurement, used in both everyday commerce and science. The change to SI had little effect on everyday life in countries that used the metric system – the metre, kilogram, litre and second remained unchanged as did the way in which they were used – most of the changes only affected measurements in the workplace. The CGPM has a role of recommending changes, but no formal role in the enforcement of such changes—another inter-governmental organisation, the International Organization of Legal Metrology (OIML) provides a forum for harmonisation of national standards and legislation in respect of metrology.
Both the degree and rate of adoption of SI varied from country to country—countries that had not adopted the metric system by 1960 and subsequently adopted SI did so directly as part of their metrication programs while others migrated from the CGS system of units to SI. In 1960, the world's largest economy was that of the United States, followed by the United Kingdom, West Germany, France, Japan, China and India. The United States and the United Kingdom were non-metric, France and Germany had been using the metric system for about a century, and China had been using the metric system for 35 years, while India and Japan had adopted the metric system within the preceding five years. Other non-metric countries were those where the United Kingdom or the United States had considerable influence. These differences are brought out in the examples below:
United Kingdom and the former British Empire.
Even though the use of metric units was legalised for trade in the UK in 1864, the UK had signed the Metre Convention in 1884 and the UK Parliament had defined the yard and the pound in terms of the metre and the kilogram in 1897, the UK continued to use the imperial system of measure 
and to export the imperial system of units to the Empire. In 1932, the system of Imperial Preference was set up at the Ottawa Conference. Although Ireland left the Commonwealth in 1948 and South Africa in 1961, both continued their close economic ties with the Commonwealth.
When the SI standard was published in 1960, the only major Commonwealth country to have adopted the metric system was India. In 1863, the first reading of a bill that would have made the metric system compulsory passed its first reading in the House of Commons by 110 votes to 75. The bill, however, failed to make the statute book because of lack of parliamentary time. In 1965, after this and similar false starts the then Federation of British Industry informed the British Government that its members favoured the adoption of the metric system. The rationale behind the request was that 80% of British exports were to countries that used the metric system or that were considering changing to the metric system. The Board of Trade, on behalf of the Government, agreed to support a ten-year metrication programme. The government agreed to a voluntary policy requiring minimal legislation and costs to be borne where they fell. SI would be used from the outset. The rest of the Commonwealth, South Africa and Ireland followed within a few years; in some countries such as South Africa and Australia metrication was mandatory rather than voluntary.
By 1980 all apart from the United Kingdom, Canada and Ireland had effectively completed their programs. In the United Kingdom the breakdown of voluntary metrication in the mid-1970s coincided with the United Kingdom's obligations as part of the EEC to adopt the metric system, resulting in legislation to force metrication in certain areas and the Eurosceptic movement adopting an anti-metrication stance and the United Kingdom seeking a number of derogations from the relevant EEC directives. Once the metrication of most consumer goods was completed in 2000, aspects of British life, especially in government, commerce and industry used SI. Although SI or units approved for use alongside SI are used in most areas where units of measure are regulated imperial units are widely encountered in unregulated areas such as the press and everyday speech. Canada has adopted it for most purposes, but imperial units are still legally permitted and remain in common use throughout a few sectors of Canadian society, particularly in the buildings, trades and railways sectors. The situation in Ireland, apart from road signs which were metricated in the early 2000s, is similar to that in the United Kingdom.
United States.
Even though Congress set up a framework for the use of the metric system in the nineteenth century, the United States continues to use US customary units, based on English measure passed by parliament under the reign of Queen Anne in 1706, for most purposes apart from science and medicine. In Puerto Rico, metric units are widely used due to the vast majority of the population having Spanish heritage.
On 10 February 1964, the National Bureau of Standards (now the National Institute of Standards and Technology) issued a statement that it was to use SI except where this would have an obvious detrimental effect. In 1968 Congress authorised the U.S. Metric Study the emphasis of which was to examine the feasibility of adopting SI. The first volume was delivered in 1970. The study recommended that the United States adopt the International System of units, and in 1975 Congress passed the Metric Conversion Act of 1975 which established a national policy of coordinating and planning for the increased use of the metric measurement system in the United States. Metrication was voluntary and to be coordinated by the United States Metric Board (USMB).
Efforts during the Ford and Carter administrations to force metrication were seized on by many newspaper editorialists as being dictatorial. Public response included resistance, apathy, and sometimes ridicule. The underlying reasons for this response include a relative uniformity of weights and measures (though, notably, US liquid measure differed by about 20% from British Imperial measure, which was adopted throughout the British Empire in 1824) inherited from the United Kingdom in 1776, a homogeneous economy and the influence of business groups and populists in Congress caused the country to look at the short-term costs associated with the change-over, particularly those that would be borne by the consumer rather than long-term benefits of efficiency and international trade. The Metrication Board was disbanded under President Ronald Reagan's direction in 1982.
The 1988 Omnibus Foreign Trade and Competitiveness Act removed international trade barriers and amended the Metric Conversion Act of 1975, designating the metric system as "the Preferred system of weights and measures for United States trade and commerce". The legislation stated that the federal government has a responsibility to assist industry, especially small business, as it voluntarily converts to the metric system of measurement. Exceptions were made for the highway and construction industries; the Department of Transportation planned to require metric units by 2000, but this plan was cancelled by the 1998 highway bill TEA21. However, the US military uses the metric system widely, partly because of the need to work with armed services from other nations.
Although overall responsibility for labelling requirements of consumer goods lies with Congress and is therefore covered by federal law, details of labelling requirements for certain commodities are controlled by state law or by other authorities such as the Food and Drug Administration, Environmental Protection Agency and Alcohol and Tobacco Tax and Trade Bureau.
The federal Fair Packaging and Labeling Act (FPLA), originally passed in 1964, was amended in 1992 to require consumer goods directly under its jurisdiction to be labelled in both customary and metric units. Some industries are engaged in efforts to amend this law to allow manufacturers to use only metric labelling. The National Conference on Weights and Measures has developed the Uniform Packaging and Labeling Regulations (UPLR) which provides a standard approach to those sections of packaging law that are under state control. Acceptance of the UPLR varies from state to state – fourteen states accept it by merely citing it in their legislation. 
During the first decade of the 21st century, the EU directive 80/181/EEC had required that dual unit labelling of goods sold within the EU cease by the end of 2009. This was backed up by requests from other nations including Japan and New Zealand to permit metric-only labelling as an aid to trade with those countries. Opinion in the United States was split – a bill to permit metric-only labelling at the federal level was to have been introduced in 2005 but significant opposition from the Food Marketing Institute, representing US grocers, has delayed the introduction of the bill. During a routine decennial review of the directive in 2008, the EU postponed the sunset clause for dual units indefinitely.
Meanwhile, in 1999 the UPLR was amended to permit metric-only labelling and automatically became law in those states that accept UPLR "as is". By 1 January 2009, 48 out of 50 states permit metric-only labelling, either through UPLR or through their own legislation. the use of metric (and therefore SI) units in the United States does not follow any pattern. Dual-unit labelling on consumer goods is mandatory. Some consumer goods such as soft drinks are sold in metric quantities, others such as milk are sold in customary units. The engineering industry is equally split. The automotive industry is largely metric, but aircraft such as the Boeing 787 Dreamliner were designed using customary units.
European Union.
In 1960, all the largest industrialised nations that had an established history of using the metric system were members of the European Economic Community (EEC).
In 1972, in order to harmonise units of measure as part of a programme to facilitate trade between member states, the EEC issued directive 71/354/EEC. This directive catalogued units of measure that could be used for "economic, public health, public safety and administrative purposes" and also provided instructions for a transition from the existing units of measure that were in use. The directive replicated the CGPM SI recommendations and in addition pre-empted some of the additions whose use had been recommended by the CIPM in 1969, but had not been ratified by the CGPM. The directive also catalogued units of measure whose status would be reviewed by the end of 1977 (mainly coherent CGS units of measure) and also catalogued units of measure that were to be phased out by the end of 1977, including the use of obsolete names for the sale of timber such as the stere, the use of units of force and pressure that made use of the acceleration due to gravity, the use of non-coherent units of power such as the Pferdestärke (PS), the use of the calorie as a measure of energy and the stilb as a measure of luminance. The directive was silent in respect of units that were specific to one or two countries including the "pond", "pfund", "livre" (Dutch, German and French synonyms for 500 g), thereby effectively prohibiting their use as well.
When the directive was revisited during 1977, some of the older units that were being reviewed (such as millimetre of mercury for blood pressure) were retained but others were phased out, thereby broadly aligning the allowable units with SI. The directive was however overhauled to accommodate British and Irish interests in retaining the imperial system in certain circumstances. It was reissued as directive 80/181/EEC. During subsequent revisions, the directive has reflected changes in the definition of SI. The directive also formalised the use of "supplementary units", which in 1979 were permitted for a period of ten years. The cut-off date for the use of supplementary units was extended a number of times and in 2009 was extended indefinitely.
India.
India was one of the last countries to start a metrication programme before the advent of SI. When it became independent in 1947, both imperial and native units of measure were in use. Its metrication programme started in 1956 with the passing of the Standards of Weights and Measures Act. Part of the act fixed the value of the seer (a legacy unit of mass) to 0.9331 kg exactly; elsewhere the Act declared that from 1960 all non-metric units of measure were to be illegal.
Four years after the Indian Government announced its metrication programme, SI was published. The result was that the initial metrication programme was a conversion to the CGS system of units and the subsequent adoption of SI has been haphazard. Fifty years later, many of the country's schoolbooks still use CGS or imperial units. Originally the Indian Government had planned to replace all units of measure with metric units by 1960. In 1976 a new Weights and Measures Act replaced the 1956 Act which, amongst other things, required that all weighing devices be approved before being released onto the market place. However, in 2012, it was reported that traditional units were still encountered in small manufacturing establishments and in the marketplace alongside CGS, SI and imperial measures, particularly in the poorer areas.
The use of the Indian numbering system of crores () and lakhs (), which do not map onto the SI system of prefixes, is widespread and is often found alongside or in place of the western numbering system.
Redefinition of units.
After the metre was redefined in 1960, the kilogram remained the only SI base unit that relied on a specific physical artifact, the international prototype of the kilogram (IPK), for its definition and thus the only unit that was still subject to periodic comparisons of national standard kilograms with the IPK. After the 1996–1998 recalibration, a clear divergence between the various prototype kilograms was observed.
At its 23rd meeting, held in 2007, the CGPM recommended that the CIPM should continue to investigate methods to provide exact fixed values for physical constants of nature that could then be used in the definitions of units of measure in place of the IPK, thus enabling the transition from explicit unit definitions to explicit constant definitions.
At a meeting of the CCU held in Reading, United Kingdom, in September 2010, a resolution and draft changes to the SI Brochure that were to be presented to the next meeting of the CIPM in October 2010 were agreed to in principle. The proposals that the CCU put forward were:
The CIPM meeting of October 2010 reviewed progress towards establishing fixed values for the constants but found that "the conditions set by the General Conference at its 23rd meeting have not yet been fully met. For this reason the CIPM does not propose a revision of the SI at the present time".
At the 24th CGPM meeting, held in October 2011, the CIPM sponsored a resolution in which the requisite definition changes were agreed to in principle and in which the conditions required to be met before the redefinitions could be implemented were restated.
By November 2014 the conditions set out at the 23rd meeting of the CGPM for the unit redefinitions had still not been met, and the 25th meeting of the CGPM, held in November 2014, adopted a similar resolution encouraging further work towards establishing fixed values for the fundamental constants.

</doc>
<doc id="54738" url="https://en.wikipedia.org/wiki?curid=54738" title="Interpretations of quantum mechanics">
Interpretations of quantum mechanics

An interpretation of quantum mechanics is a set of statements which attempt to explain quantum mechanics beyond existing theory. Although quantum mechanics has held up to rigorous and thorough experimental testing, many of these experiments are open to different interpretations. There exist a number of contending schools of thought, differing over whether quantum mechanics can be understood to be deterministic, which elements of quantum mechanics can be considered "real", and other matters.
This question is of special interest to philosophers of physics, as physicists continue to show a strong interest in the subject. They usually consider an interpretation of quantum mechanics as an interpretation of the mathematical formalism of quantum mechanics, specifying the physical meaning of the mathematical entities of the theory.
History of interpretations.
The definition of quantum theorists' terms, such as "wavefunctions" and "matrix mechanics", progressed through many stages. For instance, Erwin Schrödinger originally viewed the electron's wavefunction as its charge density smeared across the field, whereas Max Born reinterpreted it as the electron's probability density distributed across the field. There was detailed and vigorous debate about this and many other related questions at the fifth Solvay Conference in 1927. Debate has continued right to present times.
An early interpretation has acquired the label Copenhagen interpretation, and is often used. Amongst more recent interpretational concepts are quantum decoherence and many worlds.
During most of the 20th century, collapse theories were clearly the mainstream view, and the question of "interpretation" of quantum mechanics mostly revolved around how to interpret "collapse". 
Proponents of either "pilot-wave" (de Broglie-Bohm-like) or "many-worlds" (Everettian) interpretations tend to emphasize how their respective camps were intellectually marginalized throughout 1950s to 1980s. In this sense, all non-collapse theories are (historically) "minority" interpretations. 
However, since the 1990s, there has been a resurgence of interest in non-collapse theories. The "Stanford Encyclopedia" as of 2015 groups interpretations of quantum mechanics
into 
"Bohmian mechanics" (pilot-wave theories), 
"collapse theories", 
"many-worlds interpretations",
"modal interpretation" 
and "relational interpretations" 
as classes of into which most suggestions may be grouped. 
As a rough guide development of the mainstream view during the 1990s to 2000s, consider the "snapshot" of opinions collected in a poll by Schlosshauer et al. at the 2011 "Quantum Physics and the Nature of Reality" conference of July 2011.
The authors reference a similarly informal poll carried out by Max Tegmark at the "Fundamental Problems
in Quantum Theory" conference in August 1997.
The main conclusion of the authors is that "the Copenhagen interpretation still reigns supreme", receiving the most votes in their poll (42%), besides the rise to mainstream notability of the 
many-worlds interpretations:
Nature of interpretation.
An interpretation of quantum mechanics is a conceptual or argumentative way of relating between:
Two qualities vary among interpretations:
In philosophy of science, the distinction of knowledge versus reality is termed "epistemic" versus "ontic". A general law is a "regularity" of outcomes (epistemic), whereas a causal mechanism may "regulate" the outcomes (ontic). A phenomenon can receive interpretation either ontic or epistemic. For instance, indeterminism may be attributed to limitations of human observation and perception (epistemic), or may be explained as a real existing "maybe" encoded in the universe (ontic). Confusing the epistemic with the ontic, like if one were to presume that a general law actually "governs" outcomes—and that the statement of a regularity has the role of a causal mechanism—is a category mistake.
In a broad sense, scientific theory can be viewed as offering scientific realism—approximately true description or explanation of the natural world—or might be perceived with antirealism. A realist stance seeks the epistemic and the ontic, whereas an antirealist stance seeks epistemic but not the ontic. In the 20th century's first half, antirealism was mainly logical positivism, which sought to exclude unobservable aspects of reality from scientific theory.
Since the 1950s, antirealism is more modest, usually instrumentalism, permitting talk of unobservable aspects, but ultimately discarding the very question of realism and posing scientific theory as a tool to help humans make predictions, not to attain metaphysical understanding of the world. The instrumentalist view is carried by the famous quote of David Mermin, "Shut up and calculate".
Other approaches to resolve conceptual problems introduce new mathematical formalism, and so propose additional theories with their interpretations. An example is Bohmian mechanics, which is empirically equivalent with the standard formalisms, but requires extra equations to describe the precise trajectory through state space taken by the actual world. This extra ontological cost provides the explanatory benefit of explaining how the probabilities observed in measurements can arise somewhat naturally from a deterministic process.
Challenges for interpretations.
Interpretations of quantum mechanics attempt to provide a conceptual framework for understanding the many aspects of quantum mechanics which are not easily handled by the conceptual framework used for classical physics:
The mathematical structure of quantum mechanics is based on rather abstract mathematics, like Hilbert spaces. In classical field theory, a physical property at a given location in the field is readily derived. In Heisenberg's formalism, on the other hand, to derive physical information about a location in the field, one must apply a quantum operation to a quantum state, an elaborate mathematical process.
Schrödinger's formalism describes a waveform governing the probability of outcomes across a field. Yet how do we find in a specific location a particle whose wavefunction, a mere probability distribution of existence, spans a vast region of space?
The act of measurement can interact with the system state in peculiar ways, as found in double-slit experiments. The Copenhagen interpretation holds that the myriad probabilities across a quantum field are unreal, yet that the act of observation/measurement collapses the wavefunction and sets a single possibility to become real. Yet quantum decoherence grants that all the possibilities can be real, and that the act of observation/measurement sets up new subsystems.
A key interpretational question is posed by Dirac's famous sentences about quantum interference: "Each photon then interferes only with itself. Interference between two different photons never occurs." Dirac stops short of repeating this statement for objects other than photons, such as electrons, contenting himself with saying "... for particles even as light as electrons the associated wave frequency is so high that it is not easy to demonstrate interference." Dirac was of course well familiar with the concept of electron diffraction by crystals, which is usually regarded as an interference phenomenon. The comments of Dirac surrounding these sentences indicate that he considers them to be interpretive. No experiment can directly test them, an actual particular photon being detectable only once.
Quantum entanglement, as illustrated in the EPR paradox, seemingly violates principles of local causality.
Complementarity holds that no set of classical physical concepts can simultaneously refer to all properties of a quantum system. For instance, wave description "A" and particulate description "B" can each describe a quantum system "S", but not simultaneously. Still, complementarity does not usually imply that classical logic is at fault (although Hilary Putnam took such view in "Is logic empirical?"); rather, the composition of physical properties of "S" does not obey the rules of classical propositional logic when using propositional connectives (see "Quantum logic"). As now well known, the "origin of complementarity lies in the non-commutativity of operators" that describe quantum objects (Omnès 1999).
Since the intricacy of a quantum system is exponential, it is difficult to derive classical approximations.
Instrumentalist description.
Any modern scientific theory requires at the very least a description that relates the mathematical formalism to experimental practice and prediction. 
In the case of quantum mechanics, the most common description is an assertion of statistical regularity between state preparation processes and measurement processes. That is, if a measurement of a real-value quantity is performed many times, each time starting with the same initial conditions, the outcome is a well-defined probability distribution agreeing with the real numbers; moreover, quantum mechanics provides a computational instrument to determine statistical properties of this distribution, such as its expectation value.
Calculations for measurements performed on a system S postulate a Hilbert space "H" over the complex numbers. When the system S is prepared in a pure state, it is associated with a vector in "H". Measurable quantities are associated with Hermitian operators acting on "H": these are referred to as observables.
Repeated measurement of an observable "A" where S is prepared in state ψ yields a distribution of values. The expectation value of this distribution is given by the expression
This mathematical machinery gives a simple, direct way to compute a statistical property of the outcome of an experiment, once it is understood how to associate the initial state with a Hilbert space vector, and the measured quantity with an observable (that is, a specific Hermitian operator).
As an example of such a computation, the probability of finding the system in a given state formula_2 is given by computing the expectation value of a (rank-1) projection operator.
The probability is then the non-negative real number given by
In the context of quantum mechanics, the "instrumentalist interpretation" refers to the position that there can be no meaningful "interpretation" beyond the mere instrumentalist description, a position often equated with eschewing all interpretation. It is summarized by the sentence "Shut up and calculate!".
Depending on the understanding of the term instrumentalism, the "instrumentalist" or "shut up and calculate"-interpretation may either express pragmatism (the formalism of quantum mechanics has no application outside of the description of predicting experimental outcomes) or radical empiricism or phenomenalism (there is no valid concept of reality outside of perceptual phenomena).
Concerns of Einstein.
An interpretation of the mathematical formalism of quantum mechanics can be characterized by its treatment of some physical or micro-cosmological problems that Einstein saw in Copenhagenism, such as:
To explain these problems, we need to be more explicit about the kind of picture an interpretation provides. To that end we will regard an interpretation as a correspondence between the elements of the mathematical formalism M and the elements of an interpreting structure I, where:
One way of assessing an interpretation is whether the elements of I are regarded as physically real. Hence the bare instrumentalist view of quantum mechanics outlined in the previous section is not an interpretation at all, for it makes no claims about elements of physical reality.
The current usage of realism and completeness originated in the 1935 paper in which Einstein and others proposed the EPR paradox. In that paper the authors proposed the concepts of "element of reality" and of "completeness of a physical theory". They characterised element of reality as a quantity whose value can be predicted with certainty before measuring or otherwise disturbing it, and defined a complete physical theory as one in which every element of physical reality is accounted for by the theory. The paper proposed that an interpretation is complete if every element of the interpreting structure is present in the mathematics. Realism is also a property of each of the elements of the maths; an element is real if it corresponds to something physical in the interpreting structure. For example, in some interpretations of quantum mechanics (such as the many-worlds interpretation) the ket vector associated to the system state is said to correspond to an element of physical reality, while in other interpretations it is not. Einstein was not the active author of the EPR paper, and it did not quite focus on his principal concern, which was about causality.
Determinism is a property characterizing state changes due to the passage of time, namely that the state at a future instant is a uniquely defined mathematical function of the state in the present (see time evolution). It may not always be clear whether a particular interpretation is deterministic or not, as there may not be a clear choice of a time parameter. Moreover, a given theory may have two interpretations, one of which is deterministic and the other not.
Local realism is an attempt to formulate in relevant mathematical terms the subtle physical, micro-cosmological, or metaphysical concept of causality. It has two aspects:
A precise formulation of local realism in terms of a local hidden variable theory was proposed by John Bell. Bell's theorem, combined with experimental testing, restricts the kinds of properties a quantum theory can have, the primary implication being that quantum mechanics cannot satisfy both the principle of locality and counterfactual definiteness.
Summary of interpretations.
Collapse theories.
The Copenhagen interpretation.
The Copenhagen interpretation is the interpretation of quantum mechanics formulated by Niels Bohr and Werner Heisenberg while collaborating in Copenhagen around 1927. For most of the twentieth century it was regarded as the "standard" interpretation. Bohr and Heisenberg extended the probabilistic interpretation of the wavefunction proposed originally by Max Born. The Copenhagen interpretation rejects questions like "where was the particle before I measured its position?" as meaningless. The measurement process picks out exactly one of the many possibilities allowed for by the state's wave function in a manner consistent with the well-defined probabilities that are assigned to each possible state. According to the interpretation, the interaction of an observer or apparatus that is external to the quantum system is the cause of wave function collapse, thus according to Paul Davies, "reality is in the observations, not in the electron".
Consciousness causes collapse.
In his treatise "The Mathematical Foundations of Quantum Mechanics", John von Neumann deeply analyzed the so-called measurement problem. He concluded that the entire physical universe could be made subject to the Schrödinger equation (the universal wave function). He also described how measurement could cause a collapse of the wave function. This point of view was prominently expanded on by Eugene Wigner, who argued that human experimenter consciousness (or maybe even dog consciousness) was critical for the collapse, but he later abandoned this interpretation.
Variations of the von Neumann interpretation include:
Other physicists have elaborated their own variations of the von Neumann interpretation; including:
Objective collapse theories.
Objective collapse theories differ from the Copenhagen interpretation in regarding both the wavefunction and the process of collapse as ontologically objective. In objective theories, collapse occurs randomly ("spontaneous localization"), or when some physical threshold is reached, with observers having no special role. Thus, they are realistic, indeterministic, no-hidden-variables theories. The mechanism of collapse is not specified by standard quantum mechanics, which needs to be extended if this approach is correct, meaning that Objective Collapse is more of a theory than an interpretation. Examples include the Ghirardi-Rimini-Weber theory and the Penrose interpretation.
Many worlds theories.
The many-worlds interpretation is an interpretation of quantum mechanics in which a universal wavefunction obeys the same deterministic, reversible laws at all times; in particular there is no (indeterministic and irreversible) wavefunction collapse associated with measurement. The phenomena associated with measurement are claimed to be explained by decoherence, which occurs when states interact with the environment producing entanglement, repeatedly splitting the universe into mutually unobservable alternate histories—distinct universes within a greater multiverse. In this interpretation the wavefunction has objective reality.
Many minds.
The many-minds interpretation of quantum mechanics extends the many-worlds interpretation by proposing that the distinction between worlds should be made at the level of the mind of an individual observer.
Hidden variables.
Pilot-wave theories.
The de Broglie–Bohm or "pilot wave" theories form a class of interpretations of quantum mechanics based on a theory of Louis de Broglie later extended by David Bohm. 
Particles, which always have positions, are guided by the wavefunction. The wavefunction evolves according to the Schrödinger wave equation, and the wavefunction never collapses. The theory takes place in a single space-time, is non-local, and is deterministic. The simultaneous determination of a particle's position and velocity is subject to the usual uncertainty principle constraint. The theory is considered to be a hidden variable theory, and by embracing non-locality it satisfies Bell's inequality. The measurement problem is resolved, since the particles have definite positions at all times. The appearance of collapse is explained as phenomenological.
Time-symmetric theories.
Several theories have been proposed which modify the equations of quantum mechanics to be symmetric with respect to time reversal. (E.g. see Wheeler–Feynman time-symmetric theory). This creates retrocausality: events in the future can affect ones in the past, exactly as events in the past can affect ones in the future. In these theories, a single measurement cannot fully determine the state of a system (making them a type of hidden variables theory), but given two measurements performed at different times, it is possible to calculate the exact state of the system at all intermediate times. The collapse of the wavefunction is therefore not a physical change to the system, just a change in our knowledge of it due to the second measurement. Similarly, they explain entanglement as not being a true physical state but just an illusion created by ignoring retrocausality. The point where two particles appear to "become entangled" is simply a point where each particle is being influenced by events that occur to the other particle in the future.
Not all advocates of time-symmetric causality favour modifying the unitary dynamics of standard quantum mechanics. Thus a leading exponent of the two-state vector formalism, Lev Vaidman, highlights how well the two-state vector formalism dovetails with Hugh Everett's many-worlds interpretation.
Transactional Interpretation.
The transactional interpretation of quantum mechanics (TIQM) by John G. Cramer is an interpretation of quantum mechanics inspired by the Wheeler–Feynman absorber theory. It describes a quantum interaction in terms of a standing wave formed by the sum of a retarded (forward-in-time) and an advanced (backward-in-time) wave. The author argues that it avoids the philosophical problems with the Copenhagen interpretation and the role of the observer, and resolves various quantum paradoxes.
Stochastic mechanics.
An entirely classical derivation and interpretation of Schrödinger's wave equation by analogy with Brownian motion was suggested by Princeton University professor Edward Nelson in 1966. Similar considerations had previously been published, for example by R. Fürth (1933), I. Fényes (1952), and Walter Weizel (1953), and are referenced in Nelson's paper. More recent work on the stochastic interpretation has been done by M. Pavon. An alternative stochastic interpretation was developed by Roumen Tsekov.
Scale relativity.
An approach closely related to stochastic mechanics is scale relativity developed by Laurent Nottale. The main difference with stochastic mechanics is that the stochastic fluctuations which transform classical mechanics into quantum mechanics are the consequence of the scale relativistic nature of the law of motion. This is a generalisation of Einstein's theory of relativity to include scale transformations. It is scale covariance that makes spacetime fractal and hence particle paths become non-differentiable fractal paths just like quantum paths.
Scale relativity is also more general than Nelson stochastic mechanics since it not only derives the Schrödinger equation from quantum mechanics but also the equations of Quantum field theory. Depending of the form of the scale covariance law one gets different theories ranging from standard non-relativistic quantum mechanics, over non-linear non-relativistic Schrödinger equations, to relativistic quantum mechanics (Klein-Gordon & Dirac equation) and various quantum field theories.
Popper's experiment.
Karl Popper took part in the "EPR controversy", by exchanging letters with Einstein, Bell etc. about the issue, and by proposing his own Bell test experiment.
Popper first proposed an experiment that would test indeterminacy in Quantum Mechanics in two works of 1934. However, Einstein wrote a letter to Popper about the experiment in which he raised some crucial objections, causing Popper to admit that his initial idea was "based on a mistake". In the 1950s he returned to the subject and formulated this later experiment, which was finally published in 1982.
Since Popper holds both counterfactual definiteness and locality to be true, it is under dispute whether his view is an interpretation (which is what he claimed) or a modification of Quantum Mechanics (which is what many Physicists claim), and, in case of the latter, if it has been empirically refuted or not by Bell test experiments.
Other than that, Popper's theory is a variant of the de Broglie–Bohm theory that interprets the probabilities as a stochastic element in the particle movement instead of uncertainties in their initial position. In this sense it is a position between de Broglie–Bohm and stochastic mechanics, accepting the reality of the wave function like the former (in Popper's view, it is a propensity field), and the stochastic element like the latter (see also note 138 in Popper's autobiography "Unended Quest", where he expresses some sympathy for Nelson's stochastic interpretation).
Information-based interpretations.
Quantum informational approaches have attracted growing support during the 2000s. 
Hagar and Hemmo (2008) even refer to it (critically) as "a new
orthodoxy in the foundations of quantum mechanics."
J. A. Wheeler (1990) with his "It from Bit" ("It": physical entity, "Bit": unit of information) has been described as "the cheerleader of this sort of view".
These approaches have been described as a revival of immaterialism
Relational quantum mechanics.
The essential idea behind relational quantum mechanics, following the precedent of special relativity, is that different observers may give different accounts of the same series of events: for example, to one observer at a given point in time, a system may be in a single, "collapsed" eigenstate, while to another observer at the same time, it may be in a superposition of two or more states. 
Consequently, if quantum mechanics is to be a complete theory, relational quantum mechanics argues that the notion of "state" describes not the observed system itself, but the relationship, or correlation, between the system and its observer(s). The state vector of conventional quantum mechanics becomes a description of the correlation of some "degrees of freedom" in the observer, with respect to the observed system. However, it is held by relational quantum mechanics that this applies to all physical objects, whether or not they are conscious or macroscopic. Any "measurement event" is seen simply as an ordinary physical interaction, an establishment of the sort of correlation discussed above. Thus the physical content of the theory has to do not with objects themselves, but the relations between them.
An independent relational approach to quantum mechanics was developed in analogy with David Bohm's elucidation of special relativity, in which a detection event is regarded as establishing a relationship between the quantized field and the detector. The inherent ambiguity associated with applying Heisenberg's uncertainty principle is subsequently avoided.
Quantum Bayesianism.
Interpretations where quantum mechanics is said to describe an observer's knowledge of the world, rather than the world itself. 
Quantum Bayesianism ("QBism") aims at giving a "subjective Bayesian account of quantum probability", to derive quantum mechanics from informational considerations. 
This approach has some similarity with Bohr's thinking. Collapse (also known as reduction) is often interpreted as an observer acquiring information from a measurement, rather than as an objective event. These approaches have been appraised as similar to instrumentalism.
Other.
Ensemble interpretation.
The ensemble interpretation, or statistical interpretation can be viewed as a minimalist approach; it is a quantum mechanical interpretation that claims to make the fewest assumptions associated with the standard mathematical formalization. At its heart, it takes to the fullest extent the statistical Born rule. 
It does not attempt to justify, or otherwise derive, or explain quantum mechanics from any deterministic process, or make any other statement about the real nature of quantum phenomena; it is simply a statement as to the manner of wave function interpretation.
The wave function in this interpretation is not a property of any individual system, it is by its nature a statistical description of a hypothetical "ensemble" of similar systems. The probabilistic nature of quantum mechanical predictions thus follow directly from the construction or scope of the theory rather than from any intrinsic property of nature. 
Probably the most notable supporter of such an interpretation was Albert Einstein:
A prominent advocate of the ensemble interpretation is Leslie E. Ballentine, Professor at Simon Fraser University, and writer of the graduate-level textbook "Quantum Mechanics, A Modern Development".
Modal interpretations.
Modal interpretations of quantum mechanics were first conceived of in 1972 by B. van Fraassen, in his paper "A formal approach to the philosophy of science." However, this term now is used to describe a class of models that grew out of this approach. 
The "Stanford Encyclopedia of Philosophy" has an article on "Modal Interpretations of Quantum Mechanics"
which presents "modal interpretations" as a class contrasting with objective collapse theories, pilot-wave theories and many-worlds interpretations.
Van Fraassen's proposal distinguished a "dynamical state" from a "value state". The dynamical state corresponds to the ordinary quantum state, which however never collapses.
The value state is the feature which replaces the idea of "collapse".
An observable of a system is taken to have a sharp value even if the dynamical state is not an eigenstate of that same observable.
Van Fraassen's proposal is "modal" because it leads to a modal logic of quantum propositions. 
Since the 1980s, a number of authors have developed other "realist" proposals which can in retrospect be classed with van Fraassen's "modal" proposal.
Consistent histories.
The consistent histories interpretation is based on a consistency criterion that allows the history of a system to be described so that the probabilities for each history obey the additive rules of classical probability.
According to this interpretation, the purpose of a quantum-mechanical theory is to predict the relative probabilities of various alternative histories (for example, of a particle). It is claimed to be consistent with the Schrödinger equation. It attempts to provide a natural interpretation of quantum cosmology.
According to Robert E. Griffiths "It is in fact not necessary to interpret quantum mechanics in terms of measurements."
Nevertheless, Griffiths also says "A quantum theory of measurements is a necessary part of any consistent way of understanding quantum theory for a fairly obvious reason." Griffiths' explanation of this is that quantum measurement theory is derived from the principles of quantum mechanics, which, however, do not themselves explicitly postulate a primary ontological category of measurement in its own right, and which can be interpreted without explicit talk of measurement. Griffiths writes "Thus quantum measurements can, at least in principle, be analyzed using quantum theory." This contradicts the postulate of the orthodox interpretation, that the wave function changes in two ways, (1) according to the Schrödinger equation, which does not involve measurement, and (2) in the so-called 'collapse' or 'reduction' that occurs upon particle detection in the process of measurement.
Tabular comparison.
The most common interpretations are summarized in the table below. The values shown in the cells of the table are not without controversy, for the precise meanings of some of the concepts involved are unclear and, in fact, are themselves at the center of the controversy surrounding the given interpretation.
No empirical evidence exists that distinguishes among these interpretations. To that extent, the physical theory stands, and is consistent within itself and with observation and experiment; difficulties arise only when one attempts to "interpret" the theory. Nevertheless, designing experiments which would test the various interpretations is the subject of active research.
Most of these interpretations have variants. For example, it is difficult to get a precise definition of the Copenhagen interpretation as it was developed and argued about by many people.
Further reading.
Almost all authors below are professional physicists.

</doc>
<doc id="291912" url="https://en.wikipedia.org/wiki?curid=291912" title="Introduction to gauge theory">
Introduction to gauge theory

A gauge theory is a type of theory in physics. Modern theories describe physical forces in terms of fields, e.g., the electromagnetic field, the gravitational field, and fields that describe forces between the elementary particles. A general feature of these field theories is that the fundamental fields cannot be directly measured; however, some associated quantities can be measured, such as charges, energies, and velocities. In field theories, different configurations of the unobservable fields can result in identical observable quantities. A transformation from one such field configuration to another is called a gauge transformation; the lack of change in the measurable quantities, despite the field being transformed, is a property called gauge invariance. Since any kind of invariance under a field transformation is considered a symmetry, gauge invariance is sometimes called gauge symmetry. Generally, any theory that has the property of gauge invariance is considered a gauge theory.
For example, in electromagnetism the electric and magnetic fields, E and B, are observable, while the potentials "V" ("voltage") and A (the vector potential) are not. Under a gauge transformation in which a constant is added to "V", no observable change occurs in E or B.
With the advent of quantum mechanics in the 1920s, and with successive advances in quantum field theory, the importance of gauge transformations has steadily grown. Gauge theories constrain the laws of physics, because all the changes induced by a gauge transformation have to cancel each other out when written in terms of observable quantities. Over the course of the 20th century, physicists gradually realized that all forces (fundamental interactions) arise from the constraints imposed by "local" gauge symmetries, in which case the transformations vary from point to point in space and time. Perturbative quantum field theory (usually employed for scattering theory) describes forces in terms of force-mediating particles called gauge bosons. The nature of these particles is determined by the nature of the gauge transformations. The culmination of these efforts is the Standard Model, a quantum field theory that accurately predicts all of the fundamental interactions except gravity.
History and importance.
The earliest field theory having a gauge symmetry was Maxwell's formulation, in 1864–65, of electrodynamics ("A Dynamical Theory of the Electromagnetic Field"). The importance of this symmetry remained unnoticed in the earliest formulations. Similarly unnoticed, Hilbert had derived Einstein's equations of general relativity by postulating a symmetry under any change of coordinates. Later Hermann Weyl, in an attempt to unify general relativity and electromagnetism, conjectured (incorrectly, as it turned out) that invariance under the change of scale or "gauge" (a term inspired by the various track gauges of railroads) might also be a local symmetry of general relativity. Although Weyl's choice of the gauge was incorrect, the name "gauge" stuck to the approach. After the development of quantum mechanics, Weyl, Fock and London modified their gauge choice by replacing the scale factor with a change of wave phase, and applying it successfully to electromagnetism. Gauge symmetry was generalized mathematically in 1954 by Chen Ning Yang and Robert Mills in an attempt to describe the strong nuclear forces. This idea, dubbed Yang–Mills theory, later found application in the quantum field theory of the weak force, and its unification with electromagnetism in the electroweak theory.
The importance of gauge theories for physics stems from their tremendous success in providing a unified framework to describe the quantum-mechanical behavior of electromagnetism, the weak force and the strong force. This gauge theory, known as the Standard Model, accurately describes experimental predictions regarding three of the four fundamental forces of nature.
In classical physics.
Electromagnetism.
Historically, the first example of gauge symmetry to be discovered was classical electromagnetism. A static electric field can be described in terms of an electric potential (voltage) that is defined at every point in space, and in practical work it is conventional to take the Earth as a physical reference that defines the zero level of the potential, or ground. But only "differences" in potential are physically measurable, which is the reason that a voltmeter must have two probes, and can only report the voltage difference between them. Thus one could choose to define all voltage differences relative to some other standard, rather than the Earth, resulting in the addition of a constant offset. If the potential formula_1 is a solution to Maxwell's equations then, after this gauge transformation, the new potential formula_2 is also a solution to Maxwell's equations and no experiment can distinguish between these two solutions. In other words, the laws of physics governing electricity and magnetism (that is, Maxwell equations) are invariant under gauge transformation. In other words, Maxwell's equations have a gauge symmetry.
Generalizing from static electricity to electromagnetism, we have a second potential, the magnetic vector potential A, which can also undergo gauge transformations. These transformations may be local. That is, rather than adding a constant onto "V", one can add a function that takes on different values at different points in space and time. If A is also changed in certain corresponding ways, then the same E and B fields result. The detailed mathematical relationship between the fields E and B and the potentials "V" and A is given in the article Gauge fixing, along with the precise statement of the nature of the gauge transformation. The relevant point here is that the fields remain the same under the gauge transformation, and therefore Maxwell's equations are still satisfied.
Gauge symmetry is closely related to charge conservation. Suppose that there existed some process by which one could briefly violate conservation of charge by creating a charge "q" at a certain point in space, 1, moving it to some other point 2, and then destroying it. We might imagine that this process was consistent with conservation of energy. We could posit a rule stating that creating the charge required an input of energy "E"1="qV"1 and destroying it released "E"2="qV"2, which would seem natural since "qV" measures the extra energy stored in the electric field because of the existence of a charge at a certain point. Outside of the interval during which the particle exists, conservation of energy would be satisfied, because the net energy released by creation and destruction of the particle, "qV"2-"qV"1, would be equal to the work done in moving the particle from 1 to 2, "qV"2-"qV"1. But although this scenario salvages conservation of energy, it violates gauge symmetry. Gauge symmetry requires that the laws of physics be invariant under the transformation formula_2, which implies that no experiment should be able to measure the absolute potential, without reference to some external standard such as an electrical ground. But the proposed rules "E"1="qV"1 and "E"2="qV"2 for the energies of creation and destruction "would" allow an experimenter to determine the absolute potential, simply by comparing the energy input required to create the charge "q" at a particular point in space in the case where the potential is formula_1 and formula_5 respectively. The conclusion is that if gauge symmetry holds, and energy is conserved, then charge must be conserved.
General relativity.
As discussed above, the gauge transformations for classical (i.e., non-quantum mechanical) general relativity are arbitrary coordinate transformations. Technically, the transformations must be invertible, and both the transformation and its inverse must be smooth, in the sense of being differentiable an arbitrary number of times.
An example of a symmetry in a physical theory: translation invariance.
Some global symmetries under changes of coordinate predate both general relativity and the concept of a gauge. For example, Galileo and Newton introduced the notion of translation invariance, an advancement from the Aristotelian concept that different places in space, such as the earth versus the heavens, obeyed different physical rules.
Suppose, for example, that one observer examines the properties of a hydrogen atom on Earth, the other—on the Moon (or any other place in the universe), the observer will find that their hydrogen atoms exhibit completely identical properties. Again, if one observer had examined a hydrogen atom today and the other—100 years ago (or any other time in the past or in the future), the two experiments would again produce completely identical results. The invariance of the properties of a hydrogen atom with respect to the time and place where these properties were investigated is called translation invariance.
Recalling our two observers from different ages: the time in their experiments is shifted by 100 years. If the time when the older observer did the experiment was "t", the time of the modern experiment is "t"+100 years. Both observers discover the same laws of physics. Because light from hydrogen atoms in distant galaxies may reach the earth after having traveled across space for billions of years, in effect one can do such observations covering periods of time almost all the way back to the Big Bang, and they show that the laws of physics have always been the same.
In other words, if in the theory we change the time "t" to "t"+100 years (or indeed any other time shift) the theoretical predictions do not change.
Another example of a symmetry: the invariance of Einstein's field equation under arbitrary coordinate transformations.
In Einstein's general relativity, coordinates like "x", "y", "z", and "t" are not only "relative" in the global sense of translations like formula_6, rotations, etc., but become completely arbitrary, so that for example one can define an entirely new timelike coordinate according to some arbitrary rule such as formula_7, where formula_8 has units of time, and yet Einstein's equations will have the same form.
Invariance of the form of an equation under an arbitrary coordinate transformation is customarily referred to as general covariance and equations with this property are referred to as written in the covariant form. General covariance is a special case of gauge invariance.
Maxwell's equations can also be expressed in a generally covariant form, which is as invariant under general coordinate transformation as Einstein's field equation.
In quantum mechanics.
Quantum electrodynamics.
Until the advent of quantum mechanics, the only well known example of gauge symmetry was in electromagnetism, and the general significance of the concept was not fully understood. For example, it was not clear whether it was the fields E and B or the potentials "V" and A that were the fundamental quantities; if the former, then the gauge transformations could be considered as nothing more than a mathematical trick.
Aharonov–Bohm experiment.
In quantum mechanics a particle, such as an electron, is also described as a wave. For example, if the double-slit experiment is performed with electrons, then a wave-like interference pattern is observed. The electron has the highest probability of being detected at locations where the parts of the wave passing through the two slits are in phase with one another, resulting in constructive interference. The frequency of the electron "wave" is related to the kinetic energy of an individual electron "particle" via the quantum-mechanical relation "E" = "hf". If there are no electric or magnetic fields present in this experiment, then the electron's energy is constant, and, for example, there will be a high probability of detecting the electron along the central axis of the experiment, where by symmetry the two parts of the wave are in phase.
But now suppose that the electrons in the experiment are subject to electric or magnetic fields. For example, if an electric field was imposed on one side of the axis but not on the other, the results of the experiment would be affected. The part of the electron wave passing through that side oscillates at a different rate, since its energy has had −"eV" added to it, where −"e" is the charge of the electron and "V" the electrical potential. The results of the experiment will be different, because phase relationships between the two parts of the electron wave have changed, and therefore the locations of constructive and destructive interference will be shifted to one side or the other. It is the electric potential that occurs here, not the electric field, and this is a manifestation of the fact that it is the potentials and not the fields that are of fundamental significance in quantum mechanics.
Explanation with potentials.
It is even possible to have cases in which an experiment's results differ when the potentials are changed, even if no charged particle is ever exposed to a different field. One such example is the Aharonov–Bohm effect, shown in the figure. In this example, turning on the solenoid only causes a magnetic field B to exist within the solenoid. But the solenoid has been positioned so that the electron cannot possibly pass through its interior. If one believed that the fields were the fundamental quantities, then one would expect that the results of the experiment would be unchanged. In reality, the results are different, because turning on the solenoid changed the vector potential A in the region that the electrons do pass through. Now that it has been established that it is the potentials "V" and A that are fundamental, and not the fields E and B, we can see that the gauge transformations, which change "V" and A, have real physical significance, rather than being merely mathematical artifacts.
Gauge invariance: the results of the experiments are independent of the choice of the gauge for the potentials.
Note that in these experiments, the only quantity that affects the result is the "difference" in phase between the two parts of the electron wave. Suppose we imagine the two parts of the electron wave as tiny clocks, each with a single hand that sweeps around in a circle, keeping track of its own phase. Although this cartoon ignores some technical details, it retains the physical phenomena that are important here. If both clocks are sped up by the same amount, the phase relationship between them is unchanged, and the results of experiments are the same. Not only that, but it is not even necessary to change the speed of each clock by a "fixed" amount. We could change the angle of the hand on each clock by a "varying" amount θ, where θ could depend on both the position in space and on time. This would have no effect on the result of the experiment, since the final observation of the location of the electron occurs at a single place and time, so that the phase shift in each electron's "clock" would be the same, and the two effects would cancel out. This is another example of a gauge transformation: it is local, and it does not change the results of experiments.
Summary.
In summary, gauge symmetry attains its full importance in the context of quantum mechanics. In the application of quantum mechanics to electromagnetism, i.e., quantum electrodynamics, gauge symmetry applies to both electromagnetic waves and electron waves. These two gauge symmetries are in fact intimately related. If a gauge transformation θ is applied to the electron waves, for example, then one must also apply a corresponding transformation to the potentials that describe the electromagnetic waves. Gauge symmetry is required in order to make quantum electrodynamics a renormalizable theory, i.e., one in which the calculated predictions of all physically measurable quantities are finite.
Types of gauge symmetries.
The description of the electrons in the subsection above as little clocks is in effect a statement of the mathematical rules according to which the phases of electrons are to be added and subtracted: they are to be treated as ordinary numbers, except that in the case where the result of the calculation falls outside the range of 0≤θ<360°, we force it to "wrap around" into the allowed range, which covers a circle. Another way of putting this is that a phase angle of, say, 5° is considered to be completely equivalent to an angle of 365°. Experiments have verified this testable statement about the interference patterns formed by electron waves. Except for the "wrap-around" property, the algebraic properties of this mathematical structure are exactly the same as those of the ordinary real numbers.
In mathematical terminology, electron phases form an Abelian group under addition, called the circle group or "U"(1). "Abelian" means that addition commutes, so that θ + φ = φ + θ. Group means that addition associates and has an identity element, namely "0". Also, for every phase there exists an inverse such that the sum of a phase and its inverse is 0. Other examples of abelian groups are the integers under addition, 0, and negation, and the nonzero fractions under product, 1, and reciprocal.
As a way of visualizing the choice of a gauge, consider whether it is possible to tell if a cylinder has been twisted. If the cylinder has no bumps, marks, or scratches on it, we cannot tell. We could, however, draw an arbitrary curve along the cylinder, defined by some function θ("x"), where "x" measures distance along the axis of the cylinder. Once this arbitrary choice (the choice of gauge) has been made, it becomes possible to detect it if someone later twists the cylinder.
In 1954, Chen Ning Yang and Robert Mills proposed to generalize these ideas to noncommutative groups. A noncommutative gauge group can describe a field that, unlike the electromagnetic field, interacts with itself. For example, general relativity states that gravitational fields have energy, and special relativity concludes that energy is equivalent to mass. Hence a gravitational field induces a further gravitational field. The nuclear forces also have this self-interacting property.
Gauge bosons.
Surprisingly, gauge symmetry can give a deeper explanation for the existence of interactions, such as the electric and nuclear interactions. This arises from a type of gauge symmetry relating to the fact that all particles of a given type are experimentally indistinguishable from one other. Imagine that Alice and Betty are identical twins, labeled at birth by bracelets reading A and B. Because the girls are identical, nobody would be able to tell if they had been switched at birth; the labels A and B are arbitrary, and can be interchanged. Such a permanent interchanging of their identities is like a global gauge symmetry. There is also a corresponding local gauge symmetry, which describes the fact that from one moment to the next, Alice and Betty could swap roles while nobody was looking, and nobody would be able to tell. If we observe that Mom's favorite vase is broken, we can only infer that the blame belongs to one twin or the other, but we cannot tell whether the blame is 100% Alice's and 0% Betty's, or vice versa. If Alice and Betty are in fact quantum-mechanical particles rather than people, then they also have wave properties, including the property of superposition, which allows waves to be added, subtracted, and mixed arbitrarily. It follows that we are not even restricted to a complete swaps of identity. For example, if we observe that a certain amount of energy exists in a certain location in space, there is no experiment that can tell us whether that energy is 100% A's and 0% B's, 0% A's and 100% B's, or 20% A's and 80% B's, or some other mixture. The fact that the symmetry is local means that we cannot even count on these proportions to remain fixed as the particles propagate through space. The details of how this is represented mathematically depend on technical issues relating to the spins of the particles, but for our present purposes we consider a spinless particle, for which it turns out that the mixing can be specified by some arbitrary choice of gauge θ("x"), where an angle θ = 0° represents 100% A and 0% B, θ = 90° means 0% A and 100% B, and intermediate angles represent mixtures.
According to the principles of quantum mechanics, particles do not actually have trajectories through space. Motion can only be described in terms of waves, and the momentum "p" of an individual particle is related to its wavelength λ by "p" = "h"/"λ". In terms of empirical measurements, the wavelength can only be determined by observing a change in the wave between one point in space and another nearby point (mathematically, by differentiation). A wave with a shorter wavelength oscillates more rapidly, and therefore changes more rapidly between nearby points. Now suppose that we arbitrarily fix a gauge at one point in space, by saying that the energy at that location is 20% A's and 80% B's. We then measure the two waves at some other, nearby point, in order to determine their wavelengths. But there are two entirely different reasons that the waves could have changed. They could have changed because they were oscillating with a certain wavelength, or they could have changed because the gauge function changed from a 20-80 mixture to, say, 21-79. If we ignore the second possibility, the resulting theory doesn't work; strange discrepancies in momentum will show up, violating the principle of conservation of momentum. Something in the theory must be changed.
Again there are technical issues relating to spin, but in several important cases, including electrically charged particles and particles interacting via nuclear forces, the solution to the problem is to impute physical reality to the gauge function θ("x"). We say that if the function θ oscillates, it represents a new type of quantum-mechanical wave, and this new wave has its own momentum "p" = "h"/"λ", which turns out to patch up the discrepancies that otherwise would have broken conservation of momentum. In the context of electromagnetism, the particles A and B would be charged particles such as electrons, and the quantum mechanical wave represented by θ would be the electromagnetic field. (Here we ignore the technical issues raised by the fact that electrons actually have spin 1/2, not spin zero. This oversimplification is the reason that the gauge field θ comes out to be a scalar, whereas the electromagnetic field is actually represented by a vector consisting of "V" and A.) The result is that we have an explanation for the presence of electromagnetic interactions: if we try to construct a gauge-symmetric theory of identical, non-interacting particles, the result is not self-consistent, and can only be repaired by adding electric and magnetic fields that cause the particles to interact.
Although the function θ("x") describes a wave, the laws of quantum mechanics require that it also have particle properties. In the case of electromagnetism, the particle corresponding to electromagnetic waves is the photon. In general, such particles are called gauge bosons, where the term "boson" refers to a particle with integer spin. In the simplest versions of the theory, gauge bosons are massless, but it is also possible to construct versions in which they have mass, as is the case for the gauge bosons that transmit the nuclear decay forces.
Further reading.
These books are intended for general readers and employ the barest minimum of mathematics.

</doc>
<doc id="1411100" url="https://en.wikipedia.org/wiki?curid=1411100" title="Introduction to general relativity">
Introduction to general relativity

General relativity is a theory of gravitation that was developed by Albert Einstein between 1907 and 1915. According to general relativity, the observed gravitational effect between masses results from their warping of spacetime.
By the beginning of the 20th century, Newton's law of universal gravitation had been accepted for more than two hundred years as a valid description of the gravitational force between masses. In Newton's model, gravity is the result of an attractive force between massive objects. Although even Newton was troubled by the unknown nature of that force, the basic framework was extremely successful at describing motion.
Experiments and observations show that Einstein's description of gravitation accounts for several effects that are unexplained by Newton's law, such as minute anomalies in the orbits of Mercury and other planets. General relativity also predicts novel effects of gravity, such as gravitational waves, gravitational lensing and an effect of gravity on time known as gravitational time dilation. Many of these predictions have been confirmed by experiment, while others are the subject of ongoing research. For example, although there is indirect evidence for gravitational waves, direct evidence of their existence is still being sought by several teams of scientists in experiments such as the LIGO and GEO600 projects.
General relativity has developed into an essential tool in modern astrophysics. It provides the foundation for the current understanding of black holes, regions of space where the gravitational effect is so strong that even light cannot escape. Their strong gravity is thought to be responsible for the intense radiation emitted by certain types of astronomical objects (such as active galactic nuclei or microquasars). General relativity is also part of the framework of the standard Big Bang model of cosmology.
Although general relativity is not the only relativistic theory of gravity, it is the simplest such theory that is consistent with the experimental data. Nevertheless, a number of open questions remain, the most fundamental of which is how general relativity can be reconciled with the laws of quantum physics to produce a complete and self-consistent theory of quantum gravity.
From special to general relativity.
In September 1905, Albert Einstein published his theory of special relativity, which reconciles Newton's laws of motion with electrodynamics (the interaction between objects with electric charge). Special relativity introduced a new framework for all of physics by proposing new concepts of space and time. Some then-accepted physical theories were inconsistent with that framework; a key example was Newton's theory of gravity, which describes the mutual attraction experienced by bodies due to their mass.
Several physicists, including Einstein, searched for a theory that would reconcile Newton's law of gravity and special relativity. Only Einstein's theory proved to be consistent with experiments and observations. To understand the theory's basic ideas, it is instructive to follow Einstein's thinking between 1907 and 1915, from his simple thought experiment involving an observer in free fall to his fully geometric theory of gravity.
Equivalence principle.
A person in a free-falling elevator experiences weightlessness and objects either float motionless or drift at constant speed. Since everything in the elevator is falling together, no gravitational effect can be observed. In this way, the experiences of an observer in free fall are indistinguishable from those of an observer in deep space, far from any significant source of gravity. Such observers are the privileged ("inertial") observers Einstein described in his theory of special relativity: observers for whom light travels along straight lines at constant speed.
Einstein hypothesized that the similar experiences of weightless observers and inertial observers in special relativity represented a fundamental property of gravity, and he made this the cornerstone of his theory of general relativity, formalized in his equivalence principle. Roughly speaking, the principle states that a person in a free-falling elevator cannot tell that they are in free fall. Every experiment in such a free-falling environment has the same results as it would for an observer at rest or moving uniformly in deep space, far from all sources of gravity.
Gravity and acceleration.
Most effects of gravity vanish in free fall, but effects that seem the same as those of gravity can be "produced" by an accelerated frame of reference. An observer in a closed room cannot tell which of the following is true:
Conversely, any effect observed in an accelerated reference frame should also be observed in a gravitational field of corresponding strength. This principle allowed Einstein to predict several novel effects of gravity in 1907, as explained in the next section.
An observer in an accelerated reference frame must introduce what physicists call fictitious forces to account for the acceleration experienced by himself and objects around him. One example, the force pressing the driver of an accelerating car into his or her seat, has already been mentioned; another is the force you can feel pulling your arms up and out if you attempt to spin around like a top. Einstein's master insight was that the constant, familiar pull of the Earth's gravitational field is fundamentally the same as these fictitious forces. The apparent magnitude of the fictitious forces always appears to be proportional to the mass of any object on which they act - for instance, the driver's seat exerts just enough force to accelerate the driver at the same rate as the car. By analogy, Einstein proposed that an object in a gravitational field should feel a gravitational force proportional to its mass, as embodied in Newton's law of gravitation.
Physical consequences.
In 1907, Einstein was still eight years away from completing the general theory of relativity. Nonetheless, he was able to make a number of novel, testable predictions that were based on his starting point for developing his new theory: the equivalence principle.
The first new effect is the gravitational frequency shift of light. Consider two observers aboard an accelerating rocket-ship. Aboard such a ship, there is a natural concept of "up" and "down": the direction in which the ship accelerates is "up", and unattached objects accelerate in the opposite direction, falling "downward". Assume that one of the observers is "higher up" than the other. When the lower observer sends a light signal to the higher observer, the acceleration causes the light to be red-shifted, as may be calculated from special relativity; the second observer will measure a lower frequency for the light than the first. Conversely, light sent from the higher observer to the lower is blue-shifted, that is, shifted towards higher frequencies. Einstein argued that such frequency shifts must also be observed in a gravitational field. This is illustrated in the figure at left, which shows a light wave that is gradually red-shifted as it works its way upwards against the gravitational acceleration. This effect has been confirmed experimentally, as described below.
This gravitational frequency shift corresponds to a gravitational time dilation: Since the "higher" observer measures the same light wave to have a lower frequency than the "lower" observer, time must be passing faster for the higher observer. Thus, time runs more slowly for observers who are lower in a gravitational field.
It is important to stress that, for each observer, there are no observable changes of the flow of time for events or processes that are at rest in his or her reference frame. Five-minute-eggs as timed by each observer's clock have the same consistency; as one year passes on each clock, each observer ages by that amount; each clock, in short, is in perfect agreement with all processes happening in its immediate vicinity. It is only when the clocks are compared between separate observers that one can notice that time runs more slowly for the lower observer than for the higher. This effect is minute, but it too has been confirmed experimentally in multiple experiments, as described below.
In a similar way, Einstein predicted the gravitational deflection of light: in a gravitational field, light is deflected downward. Quantitatively, his results were off by a factor of two; the correct derivation requires a more complete formulation of the theory of general relativity, not just the equivalence principle.
Tidal effects.
The equivalence between gravitational and inertial effects does not constitute a complete theory of gravity. When it comes to explaining gravity near our own location on the Earth's surface, noting that our reference frame is not in free fall, so that fictitious forces are to be expected, provides a suitable explanation. But a freely falling reference frame on one side of the Earth cannot explain why the people on the opposite side of the Earth experience a gravitational pull in the opposite direction.
A more basic manifestation of the same effect involves two bodies that are falling side by side towards the Earth. In a reference frame that is in free fall alongside these bodies, they appear to hover weightlessly – but not exactly so. These bodies are not falling in precisely the same direction, but towards a single point in space: namely, the Earth's center of gravity. Consequently, there is a component of each body's motion towards the other (see the figure). In a small environment such as a freely falling lift, this relative acceleration is minuscule, while for skydivers on opposite sides of the Earth, the effect is large. Such differences in force are also responsible for the tides in the Earth's oceans, so the term "tidal effect" is used for this phenomenon.
The equivalence between inertia and gravity cannot explain tidal effects – it cannot explain variations in the gravitational field. For that, a theory is needed which describes the way that matter (such as the large mass of the Earth) affects the inertial environment around it.
From acceleration to geometry.
In exploring the equivalence of gravity and acceleration as well as the role of tidal forces, Einstein discovered several analogies with the geometry of surfaces. An example is the transition from an inertial reference frame (in which free particles coast along straight paths at constant speeds) to a rotating reference frame (in which extra terms corresponding to fictitious forces have to be introduced in order to explain particle motion): this is analogous to the transition from a Cartesian coordinate system (in which the coordinate lines are straight lines) to a curved coordinate system (where coordinate lines need not be straight).
A deeper analogy relates tidal forces with a property of surfaces called "curvature". For gravitational fields, the absence or presence of tidal forces determines whether or not the influence of gravity can be eliminated by choosing a freely falling reference frame. Similarly, the absence or presence of curvature determines whether or not a surface is equivalent to a plane. In the summer of 1912, inspired by these analogies, Einstein searched for a geometric formulation of gravity.
The elementary objects of geometry – points, lines, triangles – are traditionally defined in three-dimensional space or on two-dimensional surfaces. In 1907, Hermann Minkowski, Einstein's former mathematics professor at the Swiss Federal Polytechnic, introduced a geometric formulation of Einstein's special theory of relativity where the geometry included not only space but also time. The basic entity of this new geometry is four-dimensional spacetime. The orbits of moving bodies are curves in spacetime; the orbits of bodies moving at constant speed without changing direction correspond to straight lines.
For surfaces, the generalization from the geometry of a plane – a flat surface – to that of a general curved surface had been described in the early 19th century by Carl Friedrich Gauss. This description had in turn been generalized to higher-dimensional spaces in a mathematical formalism introduced by Bernhard Riemann in the 1850s. With the help of Riemannian geometry, Einstein formulated a geometric description of gravity in which Minkowski's spacetime is replaced by distorted, curved spacetime, just as curved surfaces are a generalization of ordinary plane surfaces. Embedding Diagrams are used to illustrate curved spacetime in educational contexts.
After he had realized the validity of this geometric analogy, it took Einstein a further three years to find the missing cornerstone of his theory: the equations describing how matter influences spacetime's curvature. Having formulated what are now known as Einstein's equations (or, more precisely, his field equations of gravity), he presented his new theory of gravity at several sessions of the Prussian Academy of Sciences in late 1915, culminating in his final presentation on November 25, 1915.
Geometry and gravitation.
Paraphrasing John Wheeler, Einstein's geometric theory of gravity can be summarized thus: spacetime tells matter how to move; matter tells spacetime how to curve. What this means is addressed in the following three sections, which explore the motion of so-called test particles, examine which properties of matter serve as a source for gravity, and, finally, introduce Einstein's equations, which relate these matter properties to the curvature of spacetime.
Probing the gravitational field.
In order to map a body's gravitational influence, it is useful to think about what physicists call probe or test particles: particles that are influenced by gravity, but are so small and light that we can neglect their own gravitational effect. In the absence of gravity and other external forces, a test particle moves along a straight line at a constant speed. In the language of spacetime, this is equivalent to saying that such test particles move along straight world lines in spacetime. In the presence of gravity, spacetime is non-Euclidean, or curved, and in curved spacetime straight world lines may not exist. Instead, test particles move along lines called geodesics, which are "as straight as possible", that is, they follow the shortest path between starting and ending points, taking the curvature into consideration.
A simple analogy is the following: In geodesy, the science of measuring Earth's size and shape, a geodesic (from Greek "geo", Earth, and "daiein", to divide) is the shortest route between two points on the Earth's surface. Approximately, such a route is a segment of a great circle, such as a line of longitude or the equator. These paths are certainly not straight, simply because they must follow the curvature of the Earth's surface. But they are as straight as is possible subject to this constraint.
The properties of geodesics differ from those of straight lines. For example, on a plane, parallel lines never meet, but this is not so for geodesics on the surface of the Earth: for example, lines of longitude are parallel at the equator, but intersect at the poles. Analogously, the world lines of test particles in free fall are spacetime geodesics, the straightest possible lines in spacetime. But still there are crucial differences between them and the truly straight lines that can be traced out in the gravity-free spacetime of special relativity. In special relativity, parallel geodesics remain parallel. In a gravitational field with tidal effects, this will not, in general, be the case. If, for example, two bodies are initially at rest relative to each other, but are then dropped in the Earth's gravitational field, they will move towards each other as they fall towards the Earth's center.
Compared with planets and other astronomical bodies, the objects of everyday life (people, cars, houses, even mountains) have little mass. Where such objects are concerned, the laws governing the behavior of test particles are sufficient to describe what happens. Notably, in order to deflect a test particle from its geodesic path, an external force must be applied. A chair someone is sitting on applies an external upwards force preventing the person from falling freely towards the center of the Earth and thus follow a geodesic, which they would otherwise do without matter in between them and the center of the Earth. In this way, general relativity explains the daily experience of gravity on the surface of the Earth "not" as the downwards pull of a gravitational force, but as the upwards push of external forces. These forces deflect all bodies resting on the Earth's surface from the geodesics they would otherwise follow. For matter objects whose own gravitational influence cannot be neglected, the laws of motion are somewhat more complicated than for test particles, although it remains true that spacetime tells matter how to move.
Sources of gravity.
In Newton's description of gravity, the gravitational force is caused by matter. More precisely, it is caused by a specific property of material objects: their mass. In Einstein's theory and related theories of gravitation, curvature at every point in spacetime is also caused by whatever matter is present. Here, too, mass is a key property in determining the gravitational influence of matter. But in a relativistic theory of gravity, mass cannot be the only source of gravity. Relativity links mass with energy, and energy with momentum.
The equivalence between mass and energy, as expressed by the formula "E" = "mc", is the most famous consequence of special relativity. In relativity, mass and energy are two different ways of describing one physical quantity. If a physical system has energy, it also has the corresponding mass, and vice versa. In particular, all properties of a body that are associated with energy, such as its temperature or the binding energy of systems such as nuclei or molecules, contribute to that body's mass, and hence act as sources of gravity.
In special relativity, energy is closely connected to momentum. Just as space and time are, in that theory, different aspects of a more comprehensive entity called spacetime, energy and momentum are merely different aspects of a unified, four-dimensional quantity that physicists call four-momentum. In consequence, if energy is a source of gravity, momentum must be a source as well. The same is true for quantities that are directly related to energy and momentum, namely internal pressure and tension. Taken together, in general relativity it is mass, energy, momentum, pressure and tension that serve as sources of gravity: they are how matter tells spacetime how to curve. In the theory's mathematical formulation, all these quantities are but aspects of a more general physical quantity called the energy–momentum tensor.
Einstein's equations.
Einstein's equations are the centerpiece of general relativity. They provide a precise formulation of the relationship between spacetime geometry and the properties of matter, using the language of mathematics. More concretely, they are formulated using the concepts of Riemannian geometry, in which the geometric properties of a space (or a spacetime) are described by a quantity called a metric. The metric encodes the information needed to compute the fundamental geometric notions of distance and angle in a curved space (or spacetime).
A spherical surface like that of the Earth provides a simple example. The location of any point on the surface can be described by two coordinates: the geographic latitude and longitude. Unlike the Cartesian coordinates of the plane, coordinate differences are not the same as distances on the surface, as shown in the diagram on the right: for someone at the equator, moving 30 degrees of longitude westward (magenta line) corresponds to a distance of roughly . On the other hand, someone at a latitude of 55 degrees, moving 30 degrees of longitude westward (blue line) covers a distance of merely . Coordinates therefore do not provide enough information to describe the geometry of a spherical surface, or indeed the geometry of any more complicated space or spacetime. That information is precisely what is encoded in the metric, which is a function defined at each point of the surface (or space, or spacetime) and relates coordinate differences to differences in distance. All other quantities that are of interest in geometry, such as the length of any given curve, or the angle at which two curves meet, can be computed from this metric function.
The metric function and its rate of change from point to point can be used to define a geometrical quantity called the Riemann curvature tensor, which describes exactly how the space or spacetime is curved at each point. In general relativity, the metric and the Riemann curvature tensor are quantities defined at each point in spacetime. As has already been mentioned, the matter content of the spacetime defines another quantity, the energy–momentum tensor T, and the principle that "spacetime tells matter how to move, and matter tells spacetime how to curve" means that these quantities must be related to each other. Einstein formulated this relation by using the Riemann curvature tensor and the metric to define another geometrical quantity G, now called the Einstein tensor, which describes some aspects of the way spacetime is curved. "Einstein's equation" then states that
i.e., up to a constant multiple, the quantity G (which measures curvature) is equated with the quantity T (which measures matter content). Here, "G" is the gravitational constant of Newtonian gravity, and "c" is the speed of light from special relativity.
This equation is often referred to in the plural as "Einstein's equations", since the quantities G and T are each determined by several functions of the coordinates of spacetime, and the equations equate each of these component functions. A solution of these equations describes a particular geometry of spacetime; for example, the Schwarzschild solution describes the geometry around a spherical, non-rotating mass such as a star or a black hole, whereas the Kerr solution describes a rotating black hole. Still other solutions can describe a gravitational wave or, in the case of the Friedmann–Lemaître–Robertson–Walker solution, an expanding universe. The simplest solution is the uncurved Minkowski spacetime, the spacetime described by special relativity.
Experiments.
No scientific theory is apodictically true; each is a model that must be checked by experiment. Newton's law of gravity was accepted because it accounted for the motion of planets and moons in the solar system with considerable accuracy. As the precision of experimental measurements gradually improved, some discrepancies with Newton's predictions were observed, and these were accounted for in the general theory of relativity. Similarly, the predictions of general relativity must also be checked with experiment, and Einstein himself devised three tests now known as the classical tests of the theory:
Of these tests, only the perihelion advance of Mercury was known prior to Einstein's final publication of general relativity in 1916. The subsequent experimental confirmation of his other predictions, especially the first measurements of the deflection of light by the sun in 1919, catapulted Einstein to international stardom. These three experiments justified adopting general relativity over Newton's theory and, incidentally, over a number of alternatives to general relativity that had been proposed.
Further tests of general relativity include precision measurements of the Shapiro effect or gravitational time delay for light, most recently in 2002 by the Cassini space probe. One set of tests focuses on effects predicted by general relativity for the behavior of gyroscopes travelling through space. One of these effects, geodetic precession, has been tested with the Lunar Laser Ranging Experiment (high-precision measurements of the orbit of the Moon). Another, which is related to rotating masses, is called frame-dragging. The geodetic and frame-dragging effects were both tested by the Gravity Probe B satellite experiment launched in 2004, with results confirming relativity to within 0.5% and 15%, respectively, as of December 2008.
By cosmic standards, gravity throughout the solar system is weak. Since the differences between the predictions of Einstein's and Newton's theories are most pronounced when gravity is strong, physicists have long been interested in testing various relativistic effects in a setting with comparatively strong gravitational fields. This has become possible thanks to precision observations of binary pulsars. In such a star system, two highly compact neutron stars orbit each other. At least one of them is a pulsar – an astronomical object that emits a tight beam of radiowaves. These beams strike the Earth at very regular intervals, similarly to the way that the rotating beam of a lighthouse means that an observer sees the lighthouse blink, and can be observed as a highly regular series of pulses. General relativity predicts specific deviations from the regularity of these radio pulses. For instance, at times when the radio waves pass close to the other neutron star, they should be deflected by the star's gravitational field. The observed pulse patterns are impressively close to those predicted by general relativity.
One particular set of observations is related to eminently useful practical applications, namely to satellite navigation systems such as the Global Positioning System that are used both for precise positioning and timekeeping. Such systems rely on two sets of atomic clocks: clocks aboard satellites orbiting the Earth, and reference clocks stationed on the Earth's surface. General relativity predicts that these two sets of clocks should tick at slightly different rates, due to their different motions (an effect already predicted by special relativity) and their different positions within the Earth's gravitational field. In order to ensure the system's accuracy, the satellite clocks are either slowed down by a relativistic factor, or that same factor is made part of the evaluation algorithm. In turn, tests of the system's accuracy (especially the very thorough measurements that are part of the definition of universal coordinated time) are testament to the validity of the relativistic predictions.
A number of other tests have probed the validity of various versions of the equivalence principle; strictly speaking, all measurements of gravitational time dilation are tests of the weak version of that principle, not of general relativity itself. So far, general relativity has passed all observational tests.
Astrophysical applications.
Models based on general relativity play an important role in astrophysics; the success of these models is further testament to the theory's validity.
Gravitational lensing.
Since light is deflected in a gravitational field, it is possible for the light of a distant object to reach an observer along two or more paths. For instance, light of a very distant object such as a quasar can pass along one side of a massive galaxy and be deflected slightly so as to reach an observer on Earth, while light
passing along the opposite side of that same galaxy is deflected as well, reaching the same observer from a slightly different direction. As a result, that particular observer will see one astronomical object in two different places in the night sky. This kind of focussing is well-known when it comes to optical lenses, and hence the corresponding gravitational effect is called gravitational lensing.
Observational astronomy uses lensing effects as an important tool to infer properties of the lensing object. Even in cases where that object is not directly visible, the shape of a lensed image provides information about the mass distribution responsible for the light deflection. In particular, gravitational lensing provides one way to measure the distribution of dark matter, which does not give off light and can be observed only by its gravitational effects. One particularly interesting application are large-scale observations, where the lensing masses are spread out over a significant fraction of the observable universe, and can be used to obtain information about the large-scale properties and evolution of our cosmos.
Gravitational waves.
Gravitational waves, a direct consequence of Einstein's theory, are distortions of geometry that propagate at the speed of light, and can be thought of as ripples in spacetime. They should not be confused with the gravity waves of fluid dynamics, which are a different concept.
Indirectly, the effect of gravitational waves has been detected in observations of specific binary stars. Such pairs of stars orbit each other and, as they do so, gradually lose energy by emitting gravitational waves. For ordinary stars like the Sun, this energy loss would be too small to be detectable, but this energy loss was observed in 1974 in a binary pulsar called PSR1913+16. In such a system, one of the orbiting stars is a pulsar. This has two consequences: a pulsar is an extremely dense object known as a neutron star, for which gravitational wave emission is much stronger than for ordinary stars. Also, a pulsar emits a narrow beam of electromagnetic radiation from its magnetic poles. As the pulsar rotates, its beam sweeps over the Earth, where it is seen as a regular series of radio pulses, just as a ship at sea observes regular flashes of light from the rotating light in a lighthouse. This regular pattern of radio pulses functions as a highly accurate "clock". It can be used to time the double star's orbital period, and it reacts sensitively to distortions of spacetime in its immediate neighborhood.
The discoverers of PSR1913+16, Russell Hulse and Joseph Taylor, were awarded the Nobel Prize in Physics in 1993. Since then, several other binary pulsars have been found. The most useful are those in which both stars are pulsars, since they provide the most accurate tests of general relativity.
Currently, one major goal of research in relativity is the direct detection of gravitational waves. To this end, a number of land-based gravitational wave detectors are in operation, and a mission to launch a space-based detector, LISA, is currently under development, with a precursor mission (LISA Pathfinder) due for launch in 2015. If gravitational waves are detected, they could be used to obtain information about compact objects such as neutron stars and black holes, and also to probe the state of the early universe fractions of a second after the Big Bang.
Black holes.
When mass is concentrated into a sufficiently compact region of space, general relativity predicts the formation of a black hole – a region of space with a gravitational effect so strong that not even light can escape. Certain types of black holes are thought to be the final state in the evolution of massive stars. On the other hand, supermassive black holes with the mass of millions or billions of Suns are assumed to reside in the cores of most galaxies, and they play a key role in current models of how galaxies have formed over the past billions of years.
Matter falling onto a compact object is one of the most efficient mechanisms for releasing energy in the form of radiation, and matter falling onto black holes is thought to be responsible for some of the brightest astronomical phenomena imaginable. Notable examples of great interest to astronomers are quasars and other types of active galactic nuclei. Under the right conditions, falling matter accumulating around a black hole can lead to the formation of jets, in which focused beams of matter are flung away into space at speeds near that of light.
There are several properties that make black holes most promising sources of gravitational waves. One reason is that black holes are the most compact objects that can orbit each other as part of a binary system; as a result, the gravitational waves emitted by such a system are especially strong. Another reason follows from what are called black-hole uniqueness theorems: over time, black holes retain only a minimal set of distinguishing features (these theorems have become known as "no-hair" theorems, since different hairstyles are a crucial part of what gives different people their different appearances). For instance, in the long term, the collapse of a hypothetical matter cube will not result in a cube-shaped black hole. Instead, the resulting black hole will be indistinguishable from a black hole formed by the collapse of a spherical mass, but with one important difference: in its transition to a spherical shape, the black hole formed by the collapse of a cube will emit gravitational waves.
Cosmology.
One of the most important aspects of general relativity is that it can be applied to the universe as a whole. A key point is that, on large scales, our universe appears to be constructed along very simple lines: all current observations suggest that, on average, the structure of the cosmos should be approximately the same, regardless of an observer's location or direction of observation: the universe is approximately homogeneous and isotropic. Such comparatively simple universes can be described by simple solutions of Einstein's equations. The current cosmological models of the universe are obtained by combining these simple solutions to general relativity with theories describing the properties of the universe's matter content, namely thermodynamics, nuclear- and particle physics. According to these models, our present universe emerged from an extremely dense high-temperature state – the Big Bang – roughly 14 billion years ago and has been expanding ever since.
Einstein's equations can be generalized by adding a term called the cosmological constant. When this term is present, empty space itself acts as a source of attractive (or, less commonly, repulsive) gravity. Einstein originally introduced this term in his pioneering 1917 paper on cosmology, with a very specific motivation: contemporary cosmological thought held the universe to be static, and the additional term was required for constructing static model universes within the framework of general relativity. When it became apparent that the universe is not static, but expanding, Einstein was quick to discard this additional term. Since the end of the 1990s, however, astronomical evidence indicating an accelerating expansion consistent with a cosmological constant – or, equivalently, with a particular and ubiquitous kind of dark energy – has steadily been accumulating.
Modern research.
General relativity is very successful in providing a framework for accurate models which describe an impressive array of physical phenomena. On the other hand, there are many interesting open questions, and in particular, the theory as a whole is almost certainly incomplete.
In contrast to all other modern theories of fundamental interactions, general relativity is a classical theory: it does not include the effects of quantum physics. The quest for a quantum version of general relativity addresses one of the most fundamental open questions in physics. While there are promising candidates for such a theory of quantum gravity, notably string theory and loop quantum gravity, there is at present no consistent and complete theory. It has long been hoped that a theory of quantum gravity would also eliminate another problematic feature of general relativity: the presence of spacetime singularities. These singularities are boundaries ("sharp edges") of spacetime at which geometry becomes ill-defined, with the consequence that general relativity itself loses its predictive power. Furthermore, there are so-called singularity theorems which predict that such singularities "must" exist within the universe if the laws of general relativity were to hold without any quantum modifications. The best-known examples are the singularities associated with the model universes that describe black holes and the beginning of the universe.
Other attempts to modify general relativity have been made in the context of cosmology. In the modern cosmological models, most energy in the universe is in forms that have never been detected directly, namely dark energy and dark matter. There have been several controversial proposals to obviate the need for these enigmatic forms of matter and energy, by modifying the laws governing gravity and the dynamics of cosmic expansion, for example modified Newtonian dynamics.
Beyond the challenges of quantum effects and cosmology, research on general relativity is rich with possibilities for further exploration: mathematical relativists explore the nature of singularities and the fundamental properties of Einstein's equations, ever more comprehensive computer simulations of specific spacetimes (such as those describing merging black holes) are run, and the race for the first direct detection of gravitational waves continues apace.
More than ninety years after the theory was first published, research is more active than ever.
External links.
"Additional resources, including more advanced material, can be found in General relativity resources."

</doc>
<doc id="2796131" url="https://en.wikipedia.org/wiki?curid=2796131" title="Introduction to quantum mechanics">
Introduction to quantum mechanics

Quantum mechanics is the science of the very small: the body of scientific principles that explains the behaviour of matter and its interactions with energy on the scale of atoms and subatomic particles.
Classical physics explains matter and energy on a scale familiar to human experience, including the behaviour of astronomical bodies. It remains the key to measurement for much of modern science and technology. However, towards the end of the 19th century, scientists discovered phenomena in both the large (macro) and the small (micro) worlds that classical physics could not explain. As Thomas Kuhn explains in his analysis of the philosophy of science, "The Structure of Scientific Revolutions", coming to terms with these limitations led to two major revolutions in physics which created a shift in the original scientific paradigm: the theory of relativity and the development of quantum mechanics. This article describes how physicists discovered the limitations of classical physics and developed the main concepts of the quantum theory that replaced it in the early decades of the 20th century. These concepts are described in roughly the order in which they were first discovered. For a more complete history of the subject, see "History of quantum mechanics".
In this sense, the word "quantum" means the minimum amount of any physical entity involved in an interaction. Certain characteristics of matter can take only discrete values.
Light behaves in some respects like particles and in other respects like waves. Matter—particles such as electrons and atoms—exhibits wavelike behaviour too. Some light sources, including neon lights, give off only certain discrete frequencies of light. Quantum mechanics shows that light, along with all other forms of electromagnetic radiation, comes in discrete units, called photons, and predicts its energies, colours, and spectral intensities.
Some aspects of quantum mechanics can seem counterintuitive or even paradoxical, because they describe behaviour quite different from that seen at larger length scales. In the words of Richard Feynman, quantum mechanics deals with "nature as She is – absurd". For example, the uncertainty principle of quantum mechanics means that the more closely one pins down one measurement (such as the position of a particle), the less precise another measurement pertaining to the same particle (such as its momentum) must become.
The first quantum theory: Max Planck and black-body radiation.
Thermal radiation is electromagnetic radiation emitted from the surface of an object due to the object's internal energy. If an object is heated sufficiently, it starts to emit light at the red end of the spectrum, as it becomes red hot.
Heating it further causes the colour to change from red to yellow, white, and blue, as light at shorter wavelengths (higher frequencies) begins to be emitted. A perfect emitter is also a perfect absorber: when it is cold, such an object looks perfectly black, because it absorbs all the light that falls on it and emits none. Consequently, an ideal thermal emitter is known as a black body, and the radiation it emits is called black-body radiation. 
In the late 19th century, thermal radiation had been fairly well characterized experimentally. However, classical physics was unable to explain the relationship between temperatures and predominant frequencies of radiation. Physicists searched for a single theory that explained all the experimental results.
[[File:RWP-comparison.svg|thumb|Predictions of the amount of thermal radiation of different frequencies emitted by a body. Correct values predicted by Planck's law (green) contrasted against the classical values of [[Rayleigh–Jeans law|Rayleigh-Jeans law]] (red) and [[Wien approximation]] (blue).]]
The first model that was able to explain the full spectrum of thermal radiation was put forward by [[Max Planck]] in 1900. He proposed a mathematical model in which the thermal radiation was in equilibrium with a set of [[harmonic oscillator]]s. To reproduce the experimental results, he had to assume that each oscillator produced an integer number of units of energy at its single characteristic frequency, rather than being able to emit any arbitrary amount of energy. In other words, the energy of each oscillator was "quantized". The [[quantum]] of energy for each oscillator, according to Planck, was proportional to the frequency of the oscillator; the constant of proportionality is now known as the [[Planck constant]]. The Planck constant, usually written as , has the value of . So, the energy of an oscillator of frequency is given by
To change the colour of such a radiating body, it is necessary to change its temperature. [[Planck's law]] explains why: increasing the temperature of a body allows it to emit more energy overall, and means that a larger proportion of the energy is towards the violet end of the spectrum.
[[Planck's law]] was the first quantum theory in physics, and Planck won the Nobel Prize in 1918 "in recognition of the services he rendered to the advancement of Physics by his discovery of energy quanta". At the time, however, Planck's view was that quantization was purely a mathematical construct, rather than (as is now believed) a fundamental change in our understanding of the world.
Photons: the quantisation of light.
[[File:Einstein patentoffice.jpg|thumb|upright|[[Albert Einstein]] in around 1905.]]
In 1905, [[Albert Einstein]] took an extra step. He suggested that quantisation was not just a mathematical construct, but that the energy in a beam of light actually occurs in individual packets, which are now called [[photon]]s. [[Photon energy|The energy of a single photon]] is given by its frequency multiplied by Planck's constant:
For centuries, scientists had debated between two possible theories of light: was it a [[wave]] or did it instead comprise a [[Corpuscular theory of light|stream of tiny particles]]? By the 19th century, the debate was generally considered to have been settled in favour of the wave theory, as it was able to explain observed effects such as [[refraction]], [[diffraction]], [[interference]] and [[polarization (waves)|polarization]]. [[James Clerk Maxwell]] had shown that electricity, magnetism and light are all manifestations of the same phenomenon: the [[electromagnetic field]]. [[Maxwell's equations]], which are the complete set of laws of [[classical electromagnetism]], describe light as waves: a combination of oscillating electric and magnetic fields. Because of the preponderance of evidence in favour of the wave theory, Einstein's ideas were met initially with great skepticism. Eventually, however, the photon model became favoured. One of the most significant pieces of evidence in its favour was its ability to explain several puzzling properties of the [[photoelectric effect]], described in the following section. Nonetheless, the wave analogy remained indispensable for helping to understand other characteristics of light: [[diffraction]], [[refraction]] and [[interference]].
The photoelectric effect.
[[File:Photoelectric effect.svg|thumb|right|Light (red arrows, left) is shone upon a metal. If the light is of sufficient frequency (i.e. sufficient energy), electrons are ejected (blue arrows, right).]]
In 1887, [[Heinrich Hertz]] observed that when light with sufficient frequency hits a metallic surface, it emits electrons. In 1902, [[Philipp Lenard]] discovered that the maximum possible energy of an ejected electron is related to the [[frequency]] of the light, not to its [[intensity (physics)|intensity]]: if the frequency is too low, no electrons are ejected regardless of the intensity. Strong beams of light toward the red end of the spectrum might produce no electrical potential at all, while weak beams of light toward the violet end of the spectrum would produce higher and higher voltages. The lowest frequency of light that can cause electrons to be emitted, called the threshold frequency, is different for different metals. This observation is at odds with classical electromagnetism, which predicts that the electron's energy should be proportional to the intensity of the radiation. So when physicists first discovered devices exhibiting the photoelectric effect, they initially expected that a higher intensity of light would produce a higher voltage from the photoelectric device.
Einstein explained the effect by postulating that a beam of light is a stream of particles ("[[photon]]s") and that, if the beam is of frequency , then each photon has an energy equal to . An electron is likely to be struck only by a single photon, which imparts at most an energy to the electron. Therefore, the intensity of the beam has no effect and only its frequency determines the maximum energy that can be imparted to the electron.
To explain the threshold effect, Einstein argued that it takes a certain amount of energy, called the "[[work function]]" and denoted by , to remove an electron from the metal. This amount of energy is different for each metal. If the energy of the photon is less than the work function, then it does not carry sufficient energy to remove the electron from the metal. The threshold frequency, , is the frequency of a photon whose energy is equal to the work function:
If is greater than , the energy is enough to remove an electron. The ejected electron has a [[kinetic energy]], , which is, at most, equal to the photon's energy minus the energy needed to dislodge the electron from the metal:
Einstein's description of light as being composed of particles extended Planck's notion of quantised energy, which is that a single photon of a given frequency, , delivers an invariant amount of energy, . In other words, individual photons can deliver more or less energy, but only depending on their frequencies. In nature, single photons are rarely encountered. The Sun and emission sources available in the 19th century emit vast numbers of photons every second, and so the importance of the energy carried by each individual photon was not obvious. Einstein's idea that the energy contained in individual units of light depends on their frequency made it possible to explain experimental results that had hitherto seemed quite counterintuitive. However, although the photon is a particle, it was still being described as having the wave-like property of frequency. Once again, the particle account of light was being compromised.
Consequences of the light being quantised.
The relationship between the frequency of electromagnetic radiation and the energy of each individual photon is why [[ultraviolet]] light can cause [[sunburn]], but visible or [[infrared]] light cannot. A photon of ultraviolet light will deliver a high amount of [[energy]] – enough to contribute to cellular damage such as occurs in a sunburn. A photon of infrared light will deliver a lower amount of energy – only enough to warm one's skin. So, an infrared lamp can warm a large surface, perhaps large enough to keep people comfortable in a cold room, but it cannot give anyone a sunburn.
All photons of the same frequency have identical energy, and all photons of different frequencies have proportionally different energies. However, although the energy imparted by photons is invariant at any given frequency, the initial energy state of the electrons in a photoelectric device prior to absorption of light is not necessarily uniform. Anomalous results may occur in the case of individual electrons. For instance, an electron that was already excited above the equilibrium level of the photoelectric device might be ejected when it absorbed uncharacteristically low frequency illumination. Statistically, however, the characteristic behaviour of a photoelectric device will reflect the behaviour of the vast majority of its electrons, which will be at their equilibrium level. This point is helpful in comprehending the distinction between the study of individual particles in quantum dynamics and the study of massed particles in classical physics.
The quantisation of matter: the Bohr model of the atom.
By the dawn of the 20th century, evidence required a model of the atom with a diffuse cloud of negatively charged [[electron]]s surrounding a small, dense, positively charged [[Atomic nucleus|nucleus]]. These properties suggested a model in which the electrons circle around the nucleus like planets orbiting a sun. However, it was also known that the atom in this model would be unstable: according to classical theory orbiting electrons are undergoing centripetal acceleration, and should therefore give off electromagnetic radiation, the loss of energy also causing them to spiral toward the nucleus, colliding with it in a fraction of a second.
A second, related, puzzle was the [[emission spectrum]] of atoms. When a gas is heated, it gives off light only at discrete frequencies. For example, the visible light given off by [[hydrogen]] consists of four different colours, as shown in the picture below. The intensity of the light at different frequencies is also different. By contrast, white light consists of a continuous emission across the whole range of visible frequencies. By the end of the nineteenth century, a simple rule known as [[Balmer series|Balmer's formula]] had been found which showed "how" the frequencies of the different lines were related to each other, though without explaining "why" this was, or making any prediction about the intensities. The formula also predicted some additional spectral lines in ultraviolet and infrared light which had not been observed at the time. These lines were later observed experimentally, raising confidence in the value of the formula.
[[File:Emission spectrum-H.svg|757px|thumb|none|[[Emission spectrum]] of [[hydrogen]]. When excited, hydrogen gas gives off light in four distinct colours (spectral lines) in the visible spectrum, as well as a number of lines in the infrared and ultraviolet.]]
In 1885 the Swiss mathematician [[Johann Balmer]] discovered that each wavelength (lambda) in the visible spectrum of hydrogen is related to some integer by the equation
where is a constant which Balmer determined to be equal to 364.56 nm.
In 1888 [[Johannes Rydberg]] generalized and greatly increased the explanatory utility of Balmer's formula. He predicted that is related to two integers and according to what is now known as the [[Rydberg formula]]:
where "R" is the [[Rydberg constant]], equal to 0.0110 nm−1, and "n" must be greater than "m".
Rydberg's formula accounts for the four visible wavelengths of hydrogen by setting and . It also predicts additional wavelengths in the emission spectrum: for and for , the emission spectrum should contain certain ultraviolet wavelengths, and for and , it should also contain certain infrared wavelengths. Experimental observation of these wavelengths came two decades later: in 1908 [[Louis Paschen]] found some of the predicted infrared wavelengths, and in 1914 [[Theodore Lyman]] found some of the predicted ultraviolet wavelengths.
Note that both Balmer and Rydberg's formulas involve integers: in modern terms, they imply that some property of the atom is quantised. Understanding exactly what this property was, and why it was quantised, was a major part in the development of quantum mechanics, as will be shown in the rest of this article. 
[[File:Bohr atom model English.svg|thumb|right|The [[Bohr model]] of the atom, showing an electron transitioning from one orbit to another by emitting a photon.]]
In 1913 [[Niels Bohr]] proposed [[Bohr model|a new model of the atom]] that included quantized electron orbits: electrons still orbit the nucleus much as planets orbit around the sun, but they are only permitted to inhabit certain orbits, not to orbit at any distance. When an atom emitted (or absorbed) energy, the electron did not move in a continuous trajectory from one orbit around the nucleus to another, as might be expected classically. Instead, the electron would jump instantaneously from one orbit to another, giving off the emitted light in the form of a photon. The possible energies of photons given off by each element were determined by the differences in energy between the orbits, and so the emission spectrum for each element would contain a number of lines.
[[File:Niels Bohr Date Unverified LOC.jpg|thumb|180px|left|upright|Niels Bohr as a young man|alt=Head and shoulders of young man in a suit and tie]]
Starting from only one simple assumption about the rule that the orbits must obey, the Bohr model was able to relate the observed spectral lines in the emission spectrum of hydrogen to previously known constants. In Bohr's model the electron simply wasn't allowed to emit energy continuously and crash into the nucleus: once it was in the closest permitted orbit, it was stable forever. Bohr's model didn't explain "why" the orbits should be quantised in that way, nor was it able to make accurate predictions for atoms with more than one electron, or to explain why some spectral lines are brighter than others.
Although some of the fundamental assumptions of the Bohr model were soon found to be wrong, the key result that the discrete lines in emission spectra are due to some property of the electrons in atoms being quantised is correct. The way that the electrons actually behave is strikingly different from Bohr's atom, and from what we see in the world of our everyday experience; this modern quantum mechanical model of the atom is discussed [[#Application to the hydrogen atom|below]].
Bohr theorised that the [[angular momentum]], , of an electron is quantised:
where is an integer and is the Planck constant. Starting from this assumption, [[Coulomb's law]] and the equations of [[uniform circular motion|circular motion]] show that an electron with units of angular momentum will orbit a proton at a distance given by
where is the [[Coulomb constant]], is the mass of an electron, and is the [[elementary charge|charge on an electron]].
For simplicity this is written as
where , called the [[Bohr radius]], is equal to 0.0529 nm.
The Bohr radius is the radius of the smallest allowed orbit.
The energy of the electron can also be calculated, and is given by
Thus Bohr's assumption that angular momentum is quantised means that an electron can only inhabit certain orbits around the nucleus, and that it can have only certain energies. A consequence of these constraints is that the electron will not crash into the nucleus: it cannot continuously emit energy, and it cannot come closer to the nucleus than "a"0 (the Bohr radius).
An electron loses energy by jumping instantaneously from its original orbit to a lower orbit; the extra energy is emitted in the form of a photon. Conversely, an electron that absorbs a photon gains energy, hence it jumps to an orbit that is farther from the nucleus.
Each photon from glowing atomic hydrogen is due to an electron moving from a higher orbit, with radius , to a lower orbit, . The energy of this photon is the difference in the energies and of the electron:
Since Planck's equation shows that the photon's energy is related to its wavelength by , the wavelengths of light that can be emitted are given by
This equation has the same form as the Rydberg formula, and predicts that the constant should be given by
Therefore, the Bohr model of the atom can predict the emission spectrum of hydrogen in terms of fundamental constants. However, it was not able to make accurate predictions for multi-electron atoms, or to explain why some spectral lines are brighter than others.
Wave-particle duality.
[[File:Broglie Big.jpg|thumb|180px|[[Louis de Broglie]] in 1929. De Broglie won the [[Nobel Prize in Physics]] for his prediction that matter acts as a wave, made in his 1924 PhD thesis.]]
Just as light has both wave-like and particle-like properties, [[de Broglie hypothesis|matter also has wave-like properties]].
Matter behaving as a wave was first demonstrated experimentally for electrons: a beam of electrons can exhibit [[diffraction]], just like a beam of light or a water wave. Similar wave-like phenomena were later shown for atoms and even small molecules.
The wavelength, "λ", associated with any object is related to its momentum, "p", through the [[Planck constant]], "h":
The relationship, called the de Broglie hypothesis, holds for all types of matter: all matter exhibits properties of both particles and waves.
The concept of wave–particle duality says that neither the classical concept of "particle" nor of "wave" can fully describe the behaviour of quantum-scale objects, either photons or matter. Wave–particle duality is an example of the [[complementarity (physics)|principle of complementarity]] in quantum physics. An elegant example of wave–particle duality, the double slit experiment, is discussed in the section below.
The double-slit experiment.
[[Image:Single slit and double slit2.jpg|right|350px|thumb|The diffraction pattern produced when light is shone through one slit (top) and the interference pattern produced by two slits (bottom). The much more complex pattern from two slits, with its small-scale interference fringes, demonstrates the wave-like propagation of light.]]
In the double-slit experiment, as originally performed by [[Thomas Young (scientist)|Thomas Young]] and [[Augustin Fresnel]] in 1827, a beam of light is directed through two narrow, closely spaced slits, producing an [[interference (wave propagation)|interference pattern]] of light and dark bands on a screen. If one of the slits is covered up, one might naively expect that the intensity of the fringes due to interference would be halved everywhere. In fact, a much simpler pattern is seen, a simple [[diffraction|diffraction pattern]]. Closing one slit results in a much simpler pattern diametrically opposite the open slit. Exactly the same behaviour can be demonstrated in water waves, and so the double-slit experiment was seen as a demonstration of the wave nature of light.
[[File:Wave-particle duality.ogv|thumb|left|The double slit experiment for a classical particle, a wave, and a quantum particle demonstrating wave-particle duality]]
The double-slit experiment has also been performed using electrons, atoms, and even molecules, and the same type of interference pattern is seen. Thus it has been demonstrated that all matter possesses both particle and wave characteristics.
Even if the source intensity is turned down, so that only one particle (e.g. photon or electron) is passing through the apparatus at a time, the same interference pattern develops over time. The quantum particle acts as a wave when passing through the double slits, but as a particle when it is detected. This is a typical feature of quantum complementarity: a quantum particle will act as a wave in an experiment to measure its wave-like properties, and like a particle in an experiment to measure its particle-like properties. The point on the detector screen where any individual particle shows up will be the result of a random process. However, the distribution pattern of many individual particles will mimic the diffraction pattern produced by waves.
Application to the Bohr model.
De Broglie expanded the [[Bohr model of the atom]] by showing that an electron in orbit around a nucleus could be thought of as having wave-like properties. In particular, an [[electron]] will be observed only in situations that permit a [[standing wave]] around a [[atomic nucleus|nucleus]]. An example of a standing wave is a violin string, which is fixed at both ends and can be made to vibrate. The waves created by a stringed instrument appear to oscillate in place, moving from crest to trough in an up-and-down motion. The wavelength of a standing wave is related to the length of the vibrating object and the boundary conditions. For example, because the violin string is fixed at both ends, it can carry standing waves of wavelengths 2"l"/"n", where "l" is the length and "n" is a positive integer. De Broglie suggested that the allowed electron orbits were those for which the circumference of the orbit would be an integer number of wavelengths. The electron's wavelength therefore determines that only Bohr orbits of certain distances from the nucleus are possible. In turn, at any distance from the nucleus smaller than a certain value it would be impossible to establish an orbit. The minimum possible distance from the nucleus is called the Bohr radius.
De Broglie's treatment of quantum events served as a starting point for Schrödinger when he set out to construct a wave equation to describe quantum theoretical events.
Spin.
[[File:Quantum spin and the Stern-Gerlach experiment.ogv|thumb|upright=1.5|Quantum spin versus classical magnet in the Stern-Gerlach experiment.]]
In 1922, [[Otto Stern]] and [[Walther Gerlach]] shot silver atoms through an (inhomogeneous) magnetic field. In classical mechanics, a magnet thrown through a magnetic field may be, depending on its orientation (if it is pointing with its northern pole upwards or down, or somewhere in between), deflected a small or large distance upwards or downwards. The atoms that Stern and Gerlach shot through the magnetic field acted in a similar way. However, while the magnets could be deflected variable distances, the atoms would always be deflected a constant distance either up or down. This implied that the property of the atom which corresponds to the magnet's orientation must be quantised, taking one of two values (either up or down), as opposed to being chosen freely from any angle.
[[Ralph Kronig]] originated the idea that particles such as atoms or electrons behave as if they rotate, or "spin", about an axis. Spin would account for the missing [[magnetic moment]], and allow two electrons in the same orbital to occupy distinct [[quantum state]]s if they "spun" in opposite directions, thus satisfying the [[Pauli exclusion principle|exclusion principle]]. The quantum number represented the sense (positive or negative) of spin.
The choice of orientation of the magnetic field used in the Stern-Gerlach experiment is arbitrary. In the animation shown here, the field is vertical and so the atoms are deflected either up or down. If the magnet is rotated a quarter turn, the atoms will be deflected either left or right. Using a vertical field shows that the spin along the vertical axis is quantised, and using a horizontal field shows that the spin along the horizontal axis is quantised.
If, instead of hitting a detector screen, one of the beams of atoms coming out of the Stern-Gerlach apparatus is passed into another (inhomogeneous) magnetic field oriented in the same direction, all of the atoms will be deflected the same way in this second field. However, if the second field is oriented at 90° to the first, then half of the atoms will be deflected one way and half the other, so that the atom's spin about the horizontal and vertical axes are independent of each other. However, if one of these beams (e.g. the atoms that were deflected up then left) is passed into a third magnetic field, oriented the same way as the first, half of the atoms will go one way and half the other. The action of measuring the atoms' spin with respect to a horizontal field has changed their spin with respect to a vertical field.
The Stern-Gerlach experiment demonstrates a number of important features of quantum mechanics:
Development of modern quantum mechanics.
In 1925, [[Werner Heisenberg]] attempted to solve one of the problems that the Bohr model left unanswered, explaining the intensities of the different lines in the hydrogen emission spectrum. Through a series of mathematical analogies, he wrote out the quantum mechanical analogue for the classical computation of intensities. Shortly afterwards, Heisenberg's colleague [[Max Born]] realised that Heisenberg's method of calculating the probabilities for transitions between the different energy levels could best be expressed by using the mathematical concept of [[Matrix (mathematics)|matrices]].
In the same year, building on de Broglie's hypothesis, [[Erwin Schrödinger]] developed the equation that describes the behaviour of a quantum mechanical wave. The mathematical model, called the [[Schrödinger equation]] after its creator, is central to quantum mechanics, defines the permitted stationary states of a quantum system, and describes how the quantum state of a physical system changes in time. The wave itself is described by a mathematical function known as a "[[wave function]]". Schrödinger said that the wave function provides the "means for predicting probability of measurement results".
Schrödinger was able to calculate the energy levels of hydrogen by treating a hydrogen atom's [[electron]] as a classical wave, moving in a well of electrical potential created by the proton. This calculation accurately reproduced the energy levels of the Bohr model.
In May 1926, Schrödinger proved that Heisenberg's [[matrix mechanics]] and his own [[Schrödinger equation|wave mechanics]] made the same predictions about the properties and behaviour of the electron; mathematically, the two theories were identical. Yet the two men disagreed on the interpretation of their mutual theory. For instance, Heisenberg saw no problem in the theoretical prediction of instantaneous transitions of electrons between orbits in an atom, but Schrödinger hoped that a theory based on continuous wave-like properties could avoid what he called (as paraphrased by [[Wilhelm Wien]]) "this nonsense about quantum jumps."
Copenhagen interpretation.
[[File:Niels Bohr Institute 1.jpg|thumb|right|The [[Niels Bohr Institute]] in Copenhagen, which was a focal point for researchers in quantum mechanics and related subjects in the 1920s and 1930s. Most of the world's best known theoretical physicists spent time there.|alt=A block shaped beige building with a sloped, red tiled roof]]
Bohr, Heisenberg and others tried to explain what these experimental results and mathematical models really mean. Their description, known as the Copenhagen interpretation of quantum mechanics, aimed to describe the nature of reality that was being probed by the measurements and described by the mathematical formulations of quantum mechanics.
The main principles of the Copenhagen interpretation are:
Various consequences of these principles are discussed in more detail in the following subsections.
Uncertainty principle.
[[File:Heisenberg 10.jpg|upright|thumb|right|[[Werner Heisenberg]] at the age of 26. Heisenberg won the [[Nobel Prize in Physics]] in 1932 for the work that he did at around this time.]]
Suppose it is desired to measure the position and speed of an object – for example a car going through a radar speed trap. It can be assumed that the car has a definite position and speed at a particular moment in time. How accurately these values can be measured depends on the quality of the measuring equipment – if the precision of the measuring equipment is improved, it will provide a result that is closer to the true value. In particular, it would be assumed that the precision of measuring the speed of the car does not affect its position, and vice versa.
In 1927, Heisenberg proved that these assumptions are not correct. Quantum mechanics shows that certain pairs of physical properties, like position and speed, cannot both be known to arbitrary precision: the more precisely one property is known, the less precisely the other can be known. This statement is known as the [[uncertainty principle]]. The uncertainty principle isn't a statement about the accuracy of our measuring equipment, but about the nature of the system itself – our assumption that the car had a definite position and speed was incorrect. On a scale of cars and people, these uncertainties are too small to notice, but when dealing with atoms and electrons they become critical.
Heisenberg gave, as an illustration, the measurement of the position and momentum of an electron using a photon of light. In measuring the electron's position, the higher the frequency of the photon, the more accurate is the measurement of the position of the impact, but the greater is the disturbance of the electron, which absorbs a random amount of energy, rendering the measurement obtained of its [[momentum]] increasingly uncertain (momentum is velocity multiplied by mass), for one is necessarily measuring its post-impact disturbed momentum from the collision products and not its original momentum. With a photon of lower frequency, the disturbance (and hence uncertainty) in the momentum is less, but so is the accuracy of the measurement of the position of the impact.
The uncertainty principle shows mathematically that the product of the uncertainty in the position and [[momentum]] of a particle (momentum is velocity multiplied by mass) could never be less than a certain value, and that this value is related to [[Planck's constant]].
Wave function collapse.
Wave function collapse is a forced expression for whatever just happened when it becomes appropriate to replace the description of an uncertain state of a system by a description of the system in a definite state. Explanations for the nature of the process of becoming certain are controversial. At any time before a photon "shows up" on a detection screen it can only be described by a set of probabilities for where it might show up. When it does show up, for instance in the [[Charge-coupled device|CCD]] of an electronic camera, the time and the space where it interacted with the device are known within very tight limits. However, the photon has disappeared, and the wave function has disappeared with it. In its place some physical change in the detection screen has appeared, e.g., an exposed spot in a sheet of photographic film, or a change in electric potential in some cell of a CCD.
Eigenstates and eigenvalues.
Because of the [[uncertainty principle]], statements about both the position and momentum of particles can only assign a [[probability]] that the position or momentum will have some numerical value. Therefore, it is necessary to formulate clearly the difference between the state of something that is indeterminate, such as an electron in a probability cloud, and the state of something having a definite value. When an object can definitely be "pinned-down" in some respect, it is said to possess an [[eigenstate]].
In the Stern-Gerlach experiment discussed [[#Spin|above]], the spin of the atom about the vertical axis has two eigenstates: up and down. Before measuring it, we can only say that any individual atom has equal probability of being found to have spin up or spin down. The measurement process causes the wavefunction to collapse into one of the two states.
The eigenstates of spin about the vertical axis are not simultaneously eigenstates of spin about the horizontal axis, so this atom has equal probability of being found to have either value of spin about the horizontal axis. As described in the section [[#Spin|above]], measuring the spin about the horizontal axis can allow an atom which was spin up to become spin down: measuring its spin about the horizontal axis collapses its wave function into one of the eigenstates of this measurement, which means it is no longer in an eigenstate of spin about the vertical axis, so can take either value.
The Pauli exclusion principle.
[[File:Wolfgang Pauli young.jpg|right|thumb|150px|[[Wolfgang Pauli]]]]
In 1924, [[Wolfgang Pauli]] proposed a new quantum degree of freedom (or [[quantum number]]), with two possible values, to resolve inconsistencies between observed molecular spectra and the predictions of quantum mechanics. In particular, the [[hydrogen spectrum|spectrum of atomic hydrogen]] had a [[Doublet (physics)|doublet]], or pair of lines differing by a small amount, where only one line was expected. Pauli formulated his "exclusion principle", stating that "There cannot exist an atom in such a quantum state that two electrons within [it] have the same set of quantum numbers."
A year later, [[George Eugene Uhlenbeck|Uhlenbeck]] and [[Samuel Goudsmit|Goudsmit]] identified Pauli's new degree of freedom with the property called [[spin (physics)|spin]] whose effects were observed in the Stern–Gerlach experiment.
Application to the hydrogen atom.
Bohr's model of the atom was essentially a planetary one, with the electrons orbiting around the nuclear "sun." However, the uncertainty principle states that an electron cannot simultaneously have an exact location and velocity in the way that a planet does. Instead of classical orbits, electrons are said to inhabit "[[atomic orbital]]s". An orbital is the "cloud" of possible locations in which an electron might be found, a distribution of probabilities rather than a precise location. Each orbital is three dimensional, rather than the two dimensional orbit, and is often depicted as a three-dimensional region within which there is a 95 percent probability of finding the electron.
Schrödinger was able to calculate the energy levels of hydrogen by treating a hydrogen atom's [[electron]] as a wave, represented by the "[[wave function]]" , in an [[electric potential]] [[potential well|well]], , created by the proton. The solutions to Schrödinger's equation are distributions of probabilities for electron positions and locations. Orbitals have a range of different shapes in three dimensions. The energies of the different orbitals can be calculated, and they accurately match the energy levels of the Bohr model.
Within Schrödinger's picture, each electron has four properties:
The collective name for these properties is the [[quantum state]] of the electron. The quantum state can be described by giving a number to each of these properties; these are known as the electron's [[quantum numbers]]. The quantum state of the electron is described by its wave function. The Pauli exclusion principle demands that no two electrons within an atom may have the same values of all four numbers. [[File:neon orbitals.JPG|right|thumb|400px|The shapes of the first five atomic orbitals: 1"s", 2"s", 2"p"x, 2"p"y, and 2"p"z. The colours show the phase of the wave function.]]
The first property describing the orbital is the [[principal quantum number]], , which is the same as in Bohr's model. denotes the energy level of each orbital. The possible values for are integers:
The next quantum number, the [[azimuthal quantum number]], denoted , describes the shape of the orbital. The shape is a consequence of the [[angular momentum]] of the orbital. The angular momentum represents the resistance of a spinning object to speeding up or slowing down under the influence of external force. The azimuthal quantum number represents the orbital angular momentum of an electron around its nucleus. The possible values for are integers from 0 to :
The shape of each orbital has its own letter as well. The first shape is denoted by the letter (a [[mnemonic]] being ""s"phere"). The next shape is denoted by the letter and has the form of a dumbbell. The other orbitals have more complicated shapes (see [[atomic orbital]]), and are denoted by the letters , , and .
The third quantum number, the [[magnetic quantum number]], describes the [[magnetic moment]] of the electron, and is denoted by (or simply "m"). The possible values for are integers from to :
The magnetic quantum number measures the component of the angular momentum in a particular direction. The choice of direction is arbitrary, conventionally the z-direction is chosen.
The fourth quantum number, the [[spin quantum number]] (pertaining to the "orientation" of the electron's spin) is denoted , with values + or −.
The chemist [[Linus Pauling]] wrote, by way of example:
It is the underlying structure and symmetry of atomic orbitals, and the way that electrons fill them, that leads to the organisation of the [[periodic table]]. The way the atomic orbitals on different atoms combine to form [[molecular orbital]]s determines the structure and strength of chemical bonds between atoms.
Dirac wave equation.
[[File:Dirac 3.jpg|upright|thumb|right|[[Paul Dirac]] (1902–1984)]]
In 1928, [[Paul Dirac]] extended the [[Pauli equation]], which described spinning electrons, to account for [[special relativity]]. The result was a theory that dealt properly with events, such as the speed at which an electron orbits the nucleus, occurring at a substantial fraction of the [[speed of light]]. By using the simplest [[electromagnetic interaction]], Dirac was able to predict the value of the magnetic moment associated with the electron's spin, and found the experimentally observed value, which was too large to be that of a spinning charged sphere governed by [[classical physics]]. He was able to solve for the [[hydrogen spectrum|spectral lines of the hydrogen atom]], and to reproduce from physical first principles [[Arnold Sommerfeld|Sommerfeld]]'s successful formula for the [[fine structure]] of the hydrogen spectrum.
Dirac's equations sometimes yielded a negative value for energy, for which he proposed a novel solution: he posited the existence of an [[antielectron]] and of a dynamical vacuum. This led to the many-particle [[quantum field theory]].
Quantum entanglement.
[[File:Superposition.svg|left|thumb|500px|Superposition of two quantum characteristics, and two resolution possibilities.]]
The Pauli exclusion principle says that two electrons in one system cannot be in the same state. Nature leaves open the possibility, however, that two electrons can have both states "superimposed" over each of them. Recall that the wave functions that emerge simultaneously from the double slits arrive at the detection screen in a state of superposition. Nothing is certain until the superimposed waveforms "collapse", At that instant an electron shows up somewhere in accordance with the probability that is the square of the absolute value of the sum of the complex-valued amplitudes of the two superimposed waveforms. The situation there is already very abstract. A concrete way of thinking about entangled photons, photons in which two contrary states are superimposed on each of them in the same event, is as follows:
Imagine that the superposition of a state that can be mentally labeled as blue and another state that can be mentally labeled as red will then appear (in imagination, of course) as a purple state. Two photons are produced as the result of the same atomic event. Perhaps they are produced by the excitation of a crystal that characteristically absorbs a photon of a certain frequency and emits two photons of half the original frequency. So the two photons come out "purple." If the experimenter now performs some experiment that will determine whether one of the photons is either blue or red, then that experiment changes the photon involved from one having a superposition of "blue" and "red" characteristics to a photon that has only one of those characteristics. The problem that Einstein had with such an imagined situation was that if one of these photons had been kept bouncing between mirrors in a laboratory on earth, and the other one had traveled halfway to the nearest star, when its twin was made to reveal itself as either blue or red, that meant that the distant photon now had to lose its "purple" status too. So whenever it might be investigated after its twin had been measured, it would necessarily show up in the opposite state to whatever its twin had revealed.
In trying to show that quantum mechanics was not a complete theory, Einstein started with the theory's prediction that two or more particles that have interacted in the past can appear strongly correlated when their various properties are later measured. He sought to explain this seeming interaction in a classical way, through their common past, and preferably not by some "spooky action at a distance." The argument is worked out in a famous paper, Einstein, Podolsky, and Rosen (1935; abbreviated EPR), setting out what is now called the [[EPR paradox]]. Assuming what is now usually called [[local realism]], EPR attempted to show from quantum theory that a particle has both position and momentum simultaneously, while according to the [[Copenhagen interpretation]], only one of those two properties actually exists and only at the moment that it is being measured. EPR concluded that quantum theory is incomplete in that it refuses to consider physical properties which objectively exist in nature. (Einstein, Podolsky, & Rosen 1935 is currently Einstein's most cited publication in physics journals.) In the same year, [[Erwin Schrödinger]] used the word "entanglement" and declared: "I would not call that "one" but rather "the" characteristic trait of quantum mechanics." The question of whether entanglement is a real condition is still in dispute. The [[Bell inequalities]] are the most powerful challenge to Einstein's claims.
Quantum field theory.
The idea of quantum field theory began in the late 1920s with British physicist [[Paul Dirac]], when he attempted to [[quantization (physics)|quantise]] the [[electromagnetic field]] – a procedure for constructing a quantum theory starting from a classical theory.
A "field" in physics is "a region or space in which a given effect (such as [[magnetism]]) exists." Other effects that manifest themselves as fields are [[gravitation]] and [[static electricity]]. In 2008, physicist [[Richard Hammond (physicist)|Richard Hammond]] wrote that
Sometimes we distinguish between quantum mechanics (QM) and quantum field theory (QFT). QM refers to a system in which the number of particles is fixed, and the fields (such as the electromechanical field) are continuous classical entities. QFT ... goes a step further and allows for the creation and annihilation of particles . . . .
He added, however, that "quantum mechanics" is often used to refer to "the entire notion of quantum view."
In 1931, Dirac proposed the existence of particles that later became known as [[anti-matter]]. Dirac shared the [[Nobel Prize in Physics]] for 1933 with [[Erwin Schrödinger|Schrödinger]], "for the discovery of new productive forms of [[atomic theory]]."
On its face, quantum field theory allows infinite numbers of particles, and leaves it up to the theory itself to predict how many and with which probabilities or numbers they should exist. When developed further, the theory often contradicts observation, so that its creation and annihilation operators can be empirically tied down. Furthermore, empirical conservation laws like that of mass-energy suggest certain constraints on the mathematical form of the theory, which are mathematically speaking finicky. The latter fact both serves to make quantum field theories difficult to handle, but has also lead to further restrictions on admissible forms of the theory; the complications are mentioned below under the rubrik of renormalization.
Quantum electrodynamics.
Quantum electrodynamics (QED) is the name of the quantum theory of the [[electromagnetic force]]. Understanding QED begins with understanding [[electromagnetism]]. Electromagnetism can be called "electrodynamics" because it is a dynamic interaction between electrical and [[magnetic force]]s. Electromagnetism begins with the [[electric charge]].
Electric charges are the sources of, and create, [[electric field]]s. An electric field is a field which exerts a force on any particles that carry electric charges, at any point in space. This includes the electron, proton, and even [[quark]]s, among others. As a force is exerted, electric charges move, a current flows and a magnetic field is produced. The changing magnetic field, in turn causes [[electric current]] (often moving electrons). The physical description of interacting [[charged particle]]s, electrical currents, electrical fields, and magnetic fields is called [[electromagnetism]].
In 1928 [[Paul Dirac]] produced a relativistic quantum theory of electromagnetism. This was the progenitor to modern quantum electrodynamics, in that it had essential ingredients of the modern theory. However, the problem of unsolvable infinities developed in this [[relativistic quantum theory]]. Years later, [[renormalization]] largely solved this problem. Initially viewed as a suspect, provisional procedure by some of its originators, renormalization eventually was embraced as an important and self-consistent tool in QED and other fields of physics. Also, in the late 1940s [[Feynman diagram|Feynman's diagrams]] depicted all possible interactions pertaining to a given event. The diagrams showed that the electromagnetic force is the interactions of photons between interacting particles.
An example of a prediction of quantum electrodynamics which has been verified experimentally is the [[Lamb shift]]. This refers to an effect whereby the quantum nature of the electromagnetic field causes the energy levels in an atom or ion to deviate slightly from what they would otherwise be. As a result, spectral lines may shift or split.
Similarly, within a freely propagating electromagnetic wave, the current can also be just an abstract [[displacement current]], instead of involving charge carriers. In QED, its full description makes essential use of short lived [[virtual particles]]. There, QED again validates an earlier, rather mysterious concept.
Standard Model.
In the 1960s [[physicist]]s realized that QED broke down at extremely high energies. From this inconsistency the [[Standard Model]] of particle physics was discovered, which remedied the higher energy breakdown in theory. It is another, extended quantum field theory which unifies the electromagnetic and [[weak interaction]]s into one theory. This is called the [[electroweak theory]].
Additionally the Standard Model contains a high energy unification of the electroweak theory with the [[strong force]], described by [[quantum chromodynamics]]. It also postulates a connection with [[gravity]] as yet another [[gauge theory]], but the connection is as of 2015 still poorly understood. The theory's prediction of the [[Higgs particle]] to explain inertial mass has stood recent empirical tests at the [[Large hadron collider]], and thus the Standard model is now considered the basic and more or less complete description of [[particle physics]] as we know it.
Interpretations.
The physical measurements, equations, and predictions pertinent to quantum mechanics are all consistent and hold a very high level of confirmation. However, the question of what these abstract models say about the underlying nature of the real world has received competing answers.
Applications.
Applications of quantum mechanics include the [[laser]], the [[transistor]], the [[electron microscope]], and [[Magnetic Resonance Imaging|magnetic resonance imaging]]. A special class of quantum mechanical applications is related to [[macroscopic quantum phenomena]] such as superfluid helium and superconductors. The study of semiconductors led to the invention of the [[diode]] and the [[transistor]], which are indispensable for modern [[electronics]].
In even the simple [[light switch]], [[quantum tunneling|quantum tunnelling]] is absolutely vital, as otherwise the electrons in the [[electric current]] could not penetrate the potential barrier made up of a layer of oxide. [[Flash memory]] chips found in [[USB flash drive|USB drives]] also use quantum tunnelling, to erase their memory cells.
Further reading.
The following titles, all by working physicists, attempt to communicate quantum theory to lay people, using a minimum of technical apparatus.
External links.
[[Category:Quantum mechanics| ]]
[[Category:Articles containing video clips]]

</doc>
<doc id="1126638" url="https://en.wikipedia.org/wiki?curid=1126638" title="Invariant (mathematics)">
Invariant (mathematics)

In mathematics, an invariant is a property, held by a class of mathematical objects, which remains unchanged when transformations of a certain type are applied to the objects. The particular class of objects and type of transformations are usually indicated by the context in which the term is used. For example, the area of a triangle is an invariant with respect to isometries of the Euclidean plane. The phrases "invariant under" and "invariant to" a transformation are both used. More generally, an invariant with respect to an equivalence relation is a property that is constant on each equivalence class. 
Invariants are used in diverse areas of mathematics such as geometry, topology and algebra. Some important classes of transformations are defined by an invariant they leave unchanged, for example conformal maps are defined as transformations of the plane that preserve angles. The discovery of invariants is an important step in the process of classifying mathematical objects.
Simple examples.
The most fundamental example of invariance is expressed in our ability to count. For a finite collection of objects of any kind, there appears to be a number to which we invariably arrive, regardless of how we count the objects in the set. The quantity—a cardinal number—is associated with the set, and is invariant under the process of counting.
An identity is an equation that remains true for all values of its variables. There are also inequalities that remain true when the values of their variables change.
Another simple example of invariance is that the distance between two points on a number line is not changed by adding the same quantity to both numbers. On the other hand, multiplication does not have this property, so distance is not invariant under multiplication.
Angles and ratios of distances are invariant under scalings, rotations, translations and reflections. These transformations produce similar shapes, which is the basis of trigonometry. All circles are similar. Therefore, they can be transformed into each other and the ratio of the circumference to the diameter is invariant and equal to pi.
More advanced examples.
Some more complicated examples:
Invariant set.
A subset "S" of the domain "U" of a mapping "T": "U" → "U" is an invariant set under the mapping when formula_2 Note that the elements of "S" are not fixed, but rather the set "S" is fixed in the power set of "U". (Some authors use the terminology "setwise invariant" vs. "pointwise invariant" to distinguish between these cases.)
For example, a circle is an invariant subset of the plane under a rotation about the circle’s center. Further, a conical surface is invariant as a set under a homothety of space.
An invariant set of an operation "T" is also said to be stable under "T". For example, the normal subgroups that are so important in group theory are those subgroups that are stable under the inner automorphisms of the ambient group.
Other examples occur in linear algebra. Suppose a linear transformation "T" has an eigenvector v. Then the line through 0 and v is an invariant set under "T". The eigenvectors span an invariant subspace which is stable under "T".
When "T" is a screw displacement, the screw axis is an invariant line, though if the pitch is non-zero, "T" has no fixed points.
Formal statement.
The notion of invariance is formalized in three different ways in mathematics: via group actions, presentations, and deformation.
Unchanged under group action.
Firstly, if one has a group "G" acting on a mathematical object (or set of objects) "X," then one may ask which points "x" are unchanged, "invariant" under the group action, or under an element "g" of the group.
Very frequently one will have a group acting on a set "X" and ask which objects in an "associated" set "F"("X") are invariant. For example, rotation in the plane about a point leaves the point about which it rotates invariant, while translation in the plane does not leave any points invariant, but does leave all lines parallel to the direction of translation invariant as lines. Formally, define the set of lines in the plane "P" as "L"("P"); then a rigid motion of the plane takes lines to lines – the group of rigid motions acts on the set of lines – and one may ask which lines are unchanged by an action.
More importantly, one may define a "function" on a set, such as "radius of a circle in the plane" and then ask if this function is invariant under a group action, such as rigid motions.
Dual to the notion of invariants are "coinvariants," also known as "orbits," which formalizes the notion of congruence: objects which can be taken to each other by a group action. For example, under the group of rigid motions of the plane, the perimeter of a triangle is an invariant, while the set of triangles congruent to a given triangle is a coinvariant.
These are connected as follows: invariants are constant on coinvariants (for example, congruent triangles have the same perimeter), while two objects which agree in the value of one invariant may or may not be congruent (two triangles with the same perimeter need not be congruent). In classification problems, one seeks to find a complete set of invariants, such that if two objects have the same values for this set of invariants, they are congruent. For example, triangles such that all three sides are equal are congruent, via SSS congruence, and thus the length of all three sides forms a complete set of invariants for triangles.
Independent of presentation.
Secondly, a function may be defined in terms of some presentation or decomposition of a mathematical object; for instance, the Euler characteristic of a cell complex is defined as the alternating sum of the number of cells in each dimension. One may forget the cell complex structure and look only at the underlying topological space (the manifold) – as different cell complexes give the same underlying manifold, one may ask if the function is "independent" of choice of "presentation," in which case it is an "intrinsically" defined invariant. This is the case for the Euler characteristic, and a general method for defining and computing invariants is to define them for a given presentation and then show that they are independent of the choice of presentation. Note that there is no notion of a group action in this sense.
The most common examples are:
Unchanged under perturbation.
Thirdly, if one is studying an object which varies in a family, as is common in algebraic geometry and differential geometry, one may ask if the property is unchanged under perturbation – if an object is constant on families or invariant under change of metric, for instance.

</doc>
<doc id="162321" url="https://en.wikipedia.org/wiki?curid=162321" title="Invariant mass">
Invariant mass

The invariant mass, rest mass, intrinsic mass, proper mass, or in the case of bound systems simply mass, is a characteristic of the total energy and momentum of an object or a system of objects that is the same in all frames of reference related by Lorentz transformations. If a center of momentum frame exists for the system, then the invariant mass of a system is simply the total energy divided by the speed of light squared. In other reference frames, the energy of the system increases, but system momentum is subtracted from this, so that the invariant mass remains unchanged.
Systems whose four-momentum is a null vector (for example a single photon or many photons moving in exactly the same direction) have zero invariant mass, and are referred to as "massless". A physical object or particle moving faster than the speed of light would have space-like four-momenta (such as the hypothesized tachyon), and these do not appear to exist. Any time-like four-momentum possesses a reference frame where the momentum (3-dimensional) is zero, which is a center of momentum frame. In this case, invariant mass is positive and is referred to as the rest mass.
If objects within a system are in relative motion, then the invariant mass of the whole system will differ from the sum of the objects' rest masses. This is also equal to the total energy of the system divided by "c"2. See mass–energy equivalence for a discussion of definitions of mass. Since the mass of systems must be measured with a weight or mass scale in a center of momentum frame in which the entire system has zero momentum, such a scale always measures the system's invariant mass. For example, a scale would measure the kinetic energy of the molecules in a bottle of gas to be part of invariant mass of the bottle, and thus also its rest mass. The same is true for massless particles in such system, which add invariant mass and also rest mass to systems, according to their energy.
For an isolated "massive" system, the center of mass of the system moves in a straight line with a steady sub-luminal velocity (with a velocity depending on the reference frame used to view it). Thus, an observer can always be placed to move along with it. In this frame, which is the center of momentum frame, the total momentum is zero, and the system as a whole may be thought of as being "at rest" if it is a bound system (like a bottle of gas). In this frame, which always exists, the invariant mass of the system is equal to the total system energy (in the zero-momentum frame) divided by . This total energy in the center of momentum frame, is the minimum energy which the system may be observed to have, when seen by various observers from various inertial frames.
Note that for reasons above, such a rest frame does not exist for single photons, or rays of light moving in one direction. When two or more photons move in different directions, however, a center of mass frame (or "rest frame" if the system is bound) exists. Thus, the mass of a system of several photons moving in different directions is positive, which means that an invariant mass exists for this system even though it does not exist for each photon.
Sum of rest masses.
Because the invariant mass includes the mass of any kinetic and potential energies which remain in the center of momentum frame, the invariant mass of a system is usually greater than sum of rest masses of its separate constituents. For example, rest mass and invariant mass are zero for individual photons even though they may add mass to the invariant mass of systems. For this reason, invariant mass is in general not an additive quantity (although there are a few rare situations where it may be, as is the case when massive particles in a system without potential or kinetic energy can be added to a total mass).
Consider the simple case of two-body system, where object A is moving towards another object B which is initially at rest (in any particular frame of reference). The magnitude of invariant mass of this two-body system (see definition below) is different from the sum of rest mass (i.e. their respective mass when stationary). Even if we consider the same system from center-of-momentum frame, where net momentum is zero, the magnitude of the system's invariant mass is not equal to the sum of the rest masses of the particles within it.
The kinetic energy of such particles and the potential energy of the force fields increase the total energy above the sum of the particle rest masses, and both terms contribute to the invariant mass of the system. The sum of the particle kinetic energies as calculated by an observer is smallest in the center of momentum frame (again, called the "rest frame" if the system is bound).
They will often also interact through one or more of the fundamental forces, giving them a potential energy of interaction, possibly negative.
For an isolated "massive" system, the center of mass moves in a straight line with a steady sub-luminal velocity. Thus, an observer can always be placed to move along with it. In this frame, which is the center of momentum frame, the total momentum is zero, and the system as a whole may be thought of as being "at rest" if it is a bound system (like a bottle of gas). In this frame, which always exists, the invariant mass of the system is equal to the total system energy (in the zero-momentum frame) divided by .
As defined in particle physics.
In particle physics, the invariant mass is equal to the mass in the rest frame of the particle, and can be calculated by the particle's energy  and its momentum  as measured in "any" frame, by the energy–momentum relation:
or in natural units where ,
This invariant mass is the same in all frames of reference (see also special relativity). This equation says that the invariant mass is the pseudo-Euclidean length of the four-vector , calculated using the relativistic version of the Pythagorean theorem which has a different sign for the space and time dimensions. This length is preserved under any Lorentz boost or rotation in four dimensions, just like the ordinary length of a vector is preserved under rotations. In quantum theory the invariant mass is a parameter in the relativistic Dirac equation for an elementary particle. The Dirac quantum operator corresponds to the particle four-momentum vector.
Since the invariant mass is determined from quantities which are conserved during a decay, the invariant mass calculated using the energy and momentum of the decay products of a single particle is equal to the mass of the particle that decayed.
The mass of a system of particles can be calculated from the general formula:
where
The term invariant mass is also used in inelastic scattering experiments. Given an inelastic reaction with total incoming energy larger than the total detected energy (i.e. not all outgoing particles are detected in the experiment), the invariant mass (also known as the "missing mass") of the reaction is defined as follows (in natural units):
If there is one dominant particle which was not detected during an experiment, a plot of the invariant mass will show a sharp peak at the mass of the missing particle.
In those cases when the momentum along one direction cannot be measured (i.e. in the case of a neutrino, whose presence is only inferred from the missing energy) the Transverse mass is used.
Example: two-particle collision.
In a two-particle collision (or a two-particle decay) the square of the invariant mass (in natural units) is
Massless particles.
The invariant mass of a system made of two massless particles whose momenta form an angle formula_8
has a convenient expression:
Collider experiments.
In particle collider experiments, one often defines the angular position of a particle in terms of an azimuthal angle formula_9 and pseudorapidity formula_10. Additionally the transverse momentum, formula_11, is usually measured. In this case if the particles are massless, or highly relativistic ( formula_12,) then the invariant mass becomes:
Rest energy.
The rest energy formula_13 of a particle is defined as:
where formula_15 is the speed of light in vacuum. In general, only differences in energy have physical significance.
The concept of rest energy follows from the special theory of relativity that leads to Einstein's famous conclusion about equivalence of energy and mass. See background for mass–energy equivalence.
On the other hand, the concept of the equivalent Dirac invariant rest mass may be defined in terms of the self energy corresponding to the product of a geometric matter current and a generalized potential as part of a single definition of mass in a geometric unified theory.

</doc>
<doc id="45199" url="https://en.wikipedia.org/wiki?curid=45199" title="Inverse element">
Inverse element

In abstract algebra, the idea of an inverse element generalises concepts of a negation (sign reversal) in relation to addition, and a reciprocal in relation to multiplication. The intuition is of an element that can 'undo' the effect of combination with another given element. While the precise definition of an inverse element varies depending on the algebraic structure involved, these definitions coincide in a group.
The word 'inverse' is derived from that means 'turned upside down', 'overturned'.
Formal definitions.
In a unital magma.
Let formula_1 be a set with a binary operation formula_2 (i.e., a magma note that a magma also has closure under the binary operation). If formula_3 is an identity element of formula_4 (i.e., "S" is a unital magma) and formula_5, then formula_6 is called a left inverse of formula_7 and formula_7 is called a right inverse of formula_6. If an element formula_10 is both a left inverse and a right inverse of formula_11, then formula_10 is called a two-sided inverse, or simply an inverse, of formula_11. An element with a two-sided inverse in formula_1 is called invertible in formula_1. An element with an inverse element only on one side is left invertible, resp. right invertible. A unital magma in which all elements are invertible is called a loop. A loop whose binary operation satisfies the associative law is a group.
Just like formula_4 can have several left identities or several right identities, it is possible for an element to have several left inverses or several right inverses (but note that their definition above uses a "two-sided" identity formula_3). It can even have several left inverses "and" several right inverses.
If the operation formula_2 is associative then if an element has both a left inverse and a right inverse, they are equal. In other words, in a monoid (an associative unital magma) every element has at most one inverse (as defined in this section). In a monoid, the set of (left and right) invertible elements is a group, called the group of units of formula_1, and denoted by formula_20 or "H"1.
A left-invertible element is left-cancellative, and analogously for right and two-sided.
In a semigroup.
The definition in the previous section generalizes the notion of inverse in group relative to the notion of identity. It's also possible, albeit less obvious, to generalize the notion of an inverse by dropping the identity element but keeping associativity, i.e. in a semigroup.
In a semigroup formula_1 an element "x" is called (von Neumann) regular if there exists some element "z" in S such that "xzx" = "x"; "z" is sometimes called a pseudoinverse. An element "y" is called (simply) an inverse of "x" if "xyx" = "x" and "y" = "yxy". Every regular element has at least one inverse: if "x" = "xzx" then it is easy to verify that "y" = "zxz" is an inverse of "x" as defined in this section. Another easy to prove fact: if "y" is an inverse of "x" then "e" = "xy" and "f" = "yx" are idempotents, that is "ee" = "e" and "ff" = "f". Thus, every pair of (mutually) inverse elements gives rise to two idempotents, and "ex" = "xf" = "x", "ye" = "fy" = "y", and "e" acts as a left identity on "x", while "f" acts a right identity, and the left/right roles are reversed for "y". This simple observation can be generalized using Green's relations: every idempotent "e" in an arbitrary semigroup is a left identity for "Re" and right identity for "Le". An intuitive description of this fact is that every pair of mutually inverse elements produces a local left identity, and respectively, a local right identity.
In a monoid, the notion of inverse as defined in the previous section is strictly narrower than the definition given in this section. Only elements in the Green class "H"1 have an inverse from the unital magma perspective, whereas for any idempotent "e", the elements of "H"e have an inverse as defined in this section. Under this more general definition, inverses need not be unique (or exist) in an arbitrary semigroup or monoid. If all elements are regular, then the semigroup (or monoid) is called regular, and every element has at least one inverse. If every element has exactly one inverse as defined in this section, then the semigroup is called an inverse semigroup. Finally, an inverse semigroup with only one idempotent is a group. An inverse semigroup may have an absorbing element 0 because 000 = 0, whereas a group may not.
Outside semigroup theory, a unique inverse as defined in this section is sometimes called a quasi-inverse. This is generally justified because in most applications (e.g. all examples in this article) associativity holds, which makes this notion a generalization of the left/right inverse relative to an identity.
"U"-semigroups.
A natural generalization of the inverse semigroup is to define an (arbitrary) unary operation ° such that ("a"°)° = "a" for all "a" in "S"; this endows "S" with a type 2,1 algebra. A semigroup endowed with such an operation is called a "U"-semigroup. Although it may seem that "a"° will be the inverse of "a", this is not necessarily the case. In order to obtain interesting notion(s), the unary operation must somehow interact with the semigroup operation. Two classes of "U"-semigroups have been studied:
Clearly a group is both an "I"-semigroup and a *-semigroup. A class of semigroups important in semigroup theory are completely regular semigroups; these are "I"-semigroups in which one additionally has "aa"° = "a"°"a"; in other words every element has commuting pseudoinverse "a"°. There are few concrete examples of such semigroups however; most are completely simple semigroups. In contrast, a subclass of *-semigroups, the *-regular semigroups (in the sense of Drazin), yield one of best known examples of a (unique) pseudoinverse, the Moore–Penrose inverse. In this case however the involution "a"* is not the pseudoinverse. Rather, the pseudoinverse of "x" is the unique element "y" such that "xyx" = "x", "yxy" = "y", ("xy")* = "xy", ("yx")* = "yx". Since *-regular semigroups generalize inverse semigroups, the unique element defined this way in a *-regular semigroup is called the generalized inverse or Penrose–Moore inverse.
Examples.
All examples in this section involve associative operators, thus we shall use the terms left/right inverse for the unital magma-based definition, and quasi-inverse for its more general version.
Real numbers.
Every real number formula_10 has an additive inverse (i.e. an inverse with respect to addition) given by formula_23. Every nonzero real number formula_10 has a multiplicative inverse (i.e. an inverse with respect to multiplication) given by formula_25 (or formula_26). By contrast, zero has no multiplicative inverse, but it has a unique quasi-inverse, 0 itself.
Functions and partial functions.
A function formula_27 is the left (resp. right) inverse of a function formula_28 (for function composition), if and only if formula_29 (resp. formula_30) is the identity function on the domain (resp. codomain) of formula_28. The inverse of a function formula_28 is often written formula_33, but this notation is sometimes ambiguous. Only bijections have two-sided inverses, but "any" function has a quasi-inverse, i.e. the full transformation monoid is regular. The monoid of partial functions is also regular, whereas the monoid of injective partial transformations is the prototypical inverse semigroup.
Galois connections.
The lower and upper adjoints in a (monotone) Galois connection, "L" and "G" are quasi-inverses of each other, i.e. "LGL" = "L" and "GLG" = "G" and one uniquely determines the other. They are not left or right inverses of each other however.
Matrices.
A square matrix formula_34 with entries in a field formula_35 is invertible (in the set of all square matrices of the same size, under matrix multiplication) if and only if its determinant is different from zero. If the determinant of formula_34 is zero, it is impossible for it to have a one-sided inverse; therefore a left inverse or right inverse implies the existence of the other one. See invertible matrix for more.
More generally, a square matrix over a commutative ring formula_37 is invertible if and only if its determinant is invertible in formula_37.
Non-square matrices of full rank have several one-sided inverses:
The left inverse can be used to determine the least norm solution of formula_43, which is also the least squares formula for regression and is given by formula_44
No rank deficient matrix has any (even one-sided) inverse. However, the Moore–Penrose pseudoinverse exists for all matrices, and coincides with the left or right (or true) inverse when it exists.
As an example of matrix inverses, consider:
So, as "m" < "n", we have a right inverse, formula_46 By components it is computed as
The left inverse doesn't exist, because
which is a singular matrix, and cannot be inverted.

</doc>
<doc id="217122" url="https://en.wikipedia.org/wiki?curid=217122" title="Invertible matrix">
Invertible matrix

In linear algebra, an "n"-by-"n" square matrix A is called invertible (also nonsingular or nondegenerate) if there exists an "n"-by-"n" square matrix B such that
where I"n" denotes the "n"-by-"n" identity matrix and the multiplication used is ordinary matrix multiplication. If this is the case, then the matrix B is uniquely determined by A and is called the inverse of A, denoted by A−1.
A square matrix that is not invertible is called singular or degenerate. A square matrix is singular if and only if its determinant is 0. Singular matrices are rare in the sense that a square matrix randomly selected from a continuous uniform distribution on its entries will almost never be singular.
Non-square matrices ("m"-by-"n" matrices for which "m ≠ n") do not have an inverse. However, in some cases such a matrix may have a left inverse or right inverse. If A is "m"-by-"n" and the rank of A is equal to "n", then A has a left inverse: an "n"-by-"m" matrix B such that BA = I. If A has rank "m", then it has a right inverse: an "n"-by-"m" matrix B such that AB = I.
Matrix inversion is the process of finding the matrix B that satisfies the prior equation for a given invertible matrix A.
While the most common case is that of matrices over the real or complex numbers, all these definitions can be given for matrices over any commutative ring. However, in this case the condition for a square matrix to be invertible is that its determinant is invertible in the ring, which in general is a much stricter requirement than being nonzero. The conditions for existence of left-inverse resp. right-inverse are more complicated since a notion of rank does not exist over rings.
Properties.
The invertible matrix theorem.
Let A be a square "n" by "n" matrix over a field "K" (for example the field R of real numbers). The following statements are equivalent, i.e., for any given matrix they are either all true or all false:
Other properties.
Furthermore, the following properties hold for an invertible matrix A:
A matrix that is its own inverse, i.e. A = A−1 and A2 = I, is called an involution.
In relation to its adjugate.
The adjugate of a matrix formula_2 can be used to find the inverse of formula_2 as follows:
If formula_2 is an formula_5 invertible matrix, then
In relation to the identity matrix.
It follows from the theory of matrices that if
for "finite square" matrices A and B, then also
Density.
Over the field of real numbers, the set of singular "n"-by-"n" matrices, considered as a subset of R"n"×"n", is a null set, i.e., has Lebesgue measure zero. This is true because singular matrices are the roots of the polynomial function in the entries of the matrix given by the determinant. Thus in the language of measure theory, almost all "n"-by-"n" matrices are invertible.
Furthermore the "n"-by-"n" invertible matrices are a dense open set in the topological space of all "n"-by-"n" matrices. Equivalently, the set of singular matrices is closed and nowhere dense in the space of "n"-by-"n" matrices.
In practice however, one may encounter non-invertible matrices. And in numerical calculations, matrices which are invertible, but close to a non-invertible matrix, can still be problematic; such matrices are said to be ill-conditioned.
Methods of matrix inversion.
Gaussian elimination.
Gauss–Jordan elimination is an algorithm that can be used to determine whether a given matrix is invertible and to find the inverse. An alternative is the LU decomposition which generates upper and lower triangular matrices which are easier to invert.
Newton's method.
A generalization of Newton's method as used for a multiplicative inverse algorithm may be convenient, if it is convenient to find a suitable starting seed:
Victor Pan and John Reif have done work that includes ways of generating a starting seed.
 Byte magazine summarised one of their approaches.-
Newton's method is particularly useful when dealing with families of related matrices that behave enough like the sequence manufactured for the homotopy above: sometimes a good starting point for refining an approximation for the new inverse can be the already obtained inverse of a previous matrix that nearly matches the current matrix, e.g. the pair of sequences of inverse matrices used in obtaining matrix square roots by Denman-Beavers iteration; this may need more than one pass of the iteration at each new matrix, if they are not close enough together for just one to be enough. Newton's method is also useful for "touch up" corrections to the Gauss–Jordan algorithm which has been contaminated by small errors due to imperfect computer arithmetic.
Cayley–Hamilton method.
Cayley–Hamilton theorem allows to represent the inverse of A in terms of det(A), traces and powers of A 
where "n" is dimension of A, and tr(A) is the trace of matrix A given by the sum of the main diagonal. The sum is taken over "s" and the sets of all "kl" ≥ 0 satisfying the linear Diophantine equation 
Eigen decomposition.
If matrix A can be eigendecomposed and if none of its eigenvalues are zero, then A is invertible and its inverse is given by
where Q is the square ("N"×"N") matrix whose "i"th column is the eigenvector formula_13 of A and Λ is the diagonal matrix whose diagonal elements are the corresponding eigenvalues, "i.e.", formula_14.
Furthermore, because Λ is a diagonal matrix, its inverse is easy to calculate:
Cholesky decomposition.
If matrix A is positive definite, then its inverse can be obtained as
where L is the lower triangular Cholesky decomposition of A, and L* denotes the conjugate transpose of L.
Analytic solution.
Writing the transpose of the matrix of cofactors, known as an adjugate matrix, can also be an efficient way to calculate the inverse of "small" matrices, but this recursive method is inefficient for large matrices. To determine the inverse, we calculate a matrix of cofactors:
so that
where |A| is the determinant of A, C is the matrix of cofactors, and CT represents the matrix transpose.
Inversion of 2×2 matrices.
The "cofactor equation" listed above yields the following result for 2×2 matrices. Inversion of these matrices can be done easily as follows:
This is possible because 1/(ad-bc) is the reciprocal of the determinant of the matrix in question, and the same strategy could be used for other matrix sizes.
The Cayley–Hamilton method gives
Inversion of 3×3 matrices.
A computationally efficient 3x3 matrix inversion is given by
(where the scalar "A" is not to be confused with the matrix A).
If the determinant is non-zero, the matrix is invertible, with the elements of the intermediary matrix on the right side above given by
The determinant of A can be computed by applying the rule of Sarrus as follows:
The Cayley–Hamilton decomposition gives
The general 3×3 inverse can be expressed concisely in terms of the cross product and triple product. If a matrix formula_25 (consisting of three column vectors, formula_26, formula_27, and formula_28) is invertible, its inverse is given by 
Note that formula_30 is equal to the triple product of formula_26, formula_27, and formula_28—the volume of the parallelepiped formed by the rows or columns: 
The correctness of the formula can be checked by using cross- and triple-product properties and by noting that for groups, left and right inverses always coincide. Intuitively, because of the cross products, each row of formula_35 is orthogonal to the non-corresponding two columns of formula_36 (causing the off-diagonal terms of formula_37 be zero). Dividing by 
causes the diagonal elements of formula_37 to be unity. For example, the first diagonal is:
Inversion of 4×4 matrices.
With increasing dimension, expressions for the inverse of A get complicated. For "n" = 4 the Cayley-Hamilton method leads to an expression that is still tractable:
Blockwise inversion.
Matrices can also be "inverted blockwise" by using the following analytic inversion formula:
where A, B, C and D are matrix sub-blocks of arbitrary size. (A must be square, so that it can be inverted. Furthermore, A and D−CA−1B must be nonsingular.) This strategy is particularly advantageous if A is diagonal and D−CA−1B (the Schur complement of A) is a small matrix, since they are the only matrices requiring inversion. 
This technique was reinvented several times and is due to Hans Boltz (1923), who used it for the inversion of geodetic matrices, and Tadeusz Banachiewicz (1937), who generalized it and proved its correctness.
The nullity theorem says that the nullity of A equals the nullity of the sub-block in the lower right of the inverse matrix, and that the nullity of B equals the nullity of the sub-block in the upper right of the inverse matrix.
The inversion procedure that led to Equation (1) performed matrix block operations that operated on C and D first. Instead, if A and B are operated on first, and provided D and A−BD−1C are nonsingular
, the result is
Equating Equations (1) and (2) leads to
where Equation (3) is the Woodbury matrix identity, which is equivalent to the binomial inverse theorem.
Since a blockwise inversion of an matrix requires inversion of two half-sized matrices and 6 multiplications between two half-sized matrices, it can be shown that a divide and conquer algorithm that uses blockwise inversion to invert a matrix runs with the same time complexity as the matrix multiplication algorithm that is used internally. There exist matrix multiplication algorithms with a complexity of operations, while the best proven lower bound is .
By Neumann series.
If a matrix A has the property that
then A is nonsingular and its inverse may be expressed by a Neumann series:
Truncating the sum results in an "approximate" inverse which may be useful as a preconditioner. Note that a truncated series can be accelerated exponentially by noting that the Neumann series is a geometric sum. As such, it satisfies 
Therefore, only formula_48 matrix multiplications are needed to compute formula_49 terms of the sum.
More generally, if A is "near" the invertible matrix X in the sense that
then A is nonsingular and its inverse is
If it is also the case that A-X has rank 1 then this simplifies to
"P"-adic approximation.
If "A" is an matrix with integer or rational coefficients and we seek a solution in arbitrary-precision rationals, then a p-adic approximation method converges to an exact solution in formula_53, assuming standard formula_54 matrix multiplication is used. The method relies on solving "n" linear systems via Dixon's method of "p"-adic approximation (each in formula_55) and is available as such in software specialized in arbitrary-precision matrix operations, e.g. in IML.
Derivative of the matrix inverse.
Suppose that the invertible matrix A depends on a parameter "t". Then the derivative of the inverse of A with respect to "t" is given by
To derive the above expression for the derivative of the inverse of A, one can differentiate the definition of the matrix inverse formula_57 and then solve for the inverse of A:
Subtracting formula_59 from both sides of the above and multiplying on the right by formula_35 gives the correct expression for the derivative of the inverse:
Similarly, if formula_62 is a small number then
Moore–Penrose pseudoinverse.
Some of the properties of inverse matrices are shared by Moore–Penrose pseudoinverses, which can be defined for any "m"-by-"n" matrix.
Applications.
For most practical applications, it is "not" necessary to invert a matrix to solve a system of linear equations; however, for a unique solution, it "is" necessary that the matrix involved be invertible.
Decomposition techniques like LU decomposition are much faster than inversion, and various fast algorithms for special classes of linear systems have also been developed.
Regression/least squares.
Although an explicit inverse is not necessary to estimate the vector of unknowns, it is unavoidable to estimate their precision, found in the diagonal of the posterior covariance matrix of the vector of unknowns.
Matrix inverses in real-time simulations.
Matrix inversion plays a significant role in computer graphics, particularly in 3D graphics rendering and 3D simulations. Examples include screen-to-world ray casting, world-to-subspace-to-world object transformations, and physical simulations.
Matrix inverses in MIMO wireless communication.
Matrix inversion also plays a significant role in the MIMO (Multiple-Input, Multiple-Output) technology in wireless communications. The MIMO system consists of N transmit and M receive antennas. Unique signals, occupying the same frequency band, are sent via N transmit antennas and are received via M receive antennas. The signal arriving at each receive antenna will be a linear combination of the N transmitted signals forming a NxM transmission matrix H. It is crucial for the matrix H to be invertible for the receiver to be able to figure out the transmitted information.

</doc>
<doc id="672218" url="https://en.wikipedia.org/wiki?curid=672218" title="Isospin">
Isospin

In nuclear physics and particle physics, isospin (isotopic spin, isobaric spin) is a quantum number related to the strong interaction. Particles that are affected equally by the strong force but have different charges (e.g. protons and neutrons) can be treated as being different states of the same particle with isospin values related to the number of charge states.
Although it does not have the units of angular momentum and is not a type of spin, the formalism that describes it is mathematically similar to that of angular momentum in quantum mechanics, which means it can be coupled in the same manner. For example, a proton-neutron pair can be coupled in a state of total isospin 1 or 0. It is a dimensionless quantity and the name derives from the fact that the mathematical structures used to describe it are very similar to those used to describe the intrinsic angular momentum (spin).
This term was derived from isotopic spin, a confusing term to which nuclear physicists prefer isobaric spin, which is more precise in meaning. Isospin symmetry is a subset of the flavour symmetry seen more broadly in the interactions of baryons and mesons. Isospin symmetry remains an important concept in particle physics, and a close examination of this symmetry historically led directly to the discovery and understanding of quarks and of the development of Yang–Mills theory.
Motivation for isospin.
Isospin was introduced by Werner Heisenberg in 1932 to explain symmetries of the then newly discovered neutron:
Thus, isospin was introduced as a concept well before the development in the 1960s of the quark model which provides our modern understanding.
The specific designation "isospin" however, was introduced by Eugene Wigner in 1937.
Protons and neutrons were then grouped together as nucleons because they both have nearly the same mass and interact in nearly the same way, if the electromagnetic interaction is neglected. It was convenient to treat them as being different states of the same particle. 
When constructing a physical theory of nuclear forces, one could simply assume that it does not depend on isospin, although the total isospin should be conserved.
Similarly to a spin particle, which has two states, protons and neutrons were said to be of isospin . The proton and neutron were then associated with different isospin projections "I"3 = + and − respectively. 
These considerations would also prove useful in the analysis of meson-nucleon interactions after the discovery of the pions in 1947. The three pions (, , ) could be assigned to an isospin triplet with "I" = 1 and "I"3 = +1, 0 or −1. By assuming that isospin was conserved by nuclear interactions, the new mesons were more easily accommodated by nuclear theory.
As further particles were discovered, they were assigned into isospin multiplets according to the number of different charge states seen: 2 doublets, "I" = − and "I" =  of K mesons (, ),(, ), a triplet "I" = 1 of Sigma baryons (, , ) a singlet "I" = 0 Lambda baryon (), a quartet "I" =  Delta baryons (, , , ), and so on. This multiplet structure was combined with strangeness in Murray Gell-Mann's eightfold way, ultimately leading to the quark model and quantum chromodynamics.
Modern understanding of isospin.
Observation of the light baryons (those made of up, down and strange quarks) lead us to believe that some of these particles are so similar in terms of their strong interactions that they can be treated as different states of the same particle. In the modern understanding of quantum chromodynamics, this is because up and down quarks are very similar in mass, and have the same strong interactions. Particles made of the same numbers of up and down quarks have similar masses and are grouped together. 
For example, the particles known as the Delta baryons—baryons of spin made of a mix of three up and down quarks—are grouped together because they all have nearly the same mass (approximately ), and interact in nearly the same way.
However, because the up and down quarks have different charges ( "e" and − "e" respectively), the four Deltas also have different charges ( (uuu), (uud), (udd), (ddd)). These Deltas could be treated as the same particle and the difference in charge being due to the particle being in different states. Isospin was devised as a parallel to spin to associate an isospin projection (denoted "I3") to each charged state. Since there were four Deltas, four projections were needed. 
Because isospin was modeled on spin, the isospin projections were made to vary in increments of 1 and to have four increments of 1, you needed an isospin value of (giving the projections "I"3 = , , −, −). Thus, all the Deltas were said to have isospin "I" =  and each individual charge had different "I"3 (e.g. the was associated with "I"3 = +). In the isospin picture, the four Deltas and the two nucleons were thought to be the different states of two particles. In the quark model, the Deltas can be thought of as the excited states of the nucleons.
After the quark model was elaborated, it was noted that the isospin projection was related to the up and down quark content of particles. The relation is
where "n"u and "n"d are the numbers of up and down quarks respectively, and "n" and "n" are the numbers of up and down antiquarks respectively.
By this, the value of "I"3 of the nucleons proton (symbol p) and neutron (symbol n) is determined by their quark composition, "uud" for the proton and "udd" for the neutron.
Isospin symmetry.
In quantum mechanics, when a Hamiltonian has a symmetry, that symmetry manifests itself through a set of states that have the same energy; that is, the states are degenerate. In particle physics, the near mass-degeneracy of the neutron and proton points to an approximate symmetry of the Hamiltonian describing the strong interactions. The neutron does have a slightly higher mass due to isospin breaking; this is due to the difference in the masses of the up and down quarks and the effects of the electromagnetic interaction. However, the appearance of an approximate symmetry is still useful, since the small breakings can be described by a perturbation theory, which gives rise to slight differences between the near-degenerate states.
SU(2).
Heisenberg's contribution was to note that the mathematical formulation of this symmetry was in certain respects similar to the mathematical formulation of spin, whence the name "isospin" derives. To be precise, the isospin symmetry is given by the invariance of the Hamiltonian of the strong interactions under the action of the Lie group SU(2). The neutron and the proton are assigned to the doublet (the spin-, 2, or fundamental representation) of SU(2). The pions are assigned to the triplet (the spin-1, 3, or adjoint representation) of SU(2). Though, there is a difference from the theory of spin: the group action does not preserve flavor.
Like the case for regular spin, the isospin operator I is vector-valued: it has three components I"x", I"y", I"z" which are coordinates in the same 3-dimensional vector space where the 3 representation acts. Note that it has nothing to do with the physical space, except similar mathematical formalism. Isospin is described by two quantum numbers: "I", the total isospin, and "I"3, an eigenvalue of the I"z" projection for which flavor states are eigenstates, not an "arbitrary projection" as in the case of spin. In other words, each "I"3 state specifies certain flavor state of a multiplet. The third coordinate ("z"), to which the "3" subscript refers, is chosen due to notational conventions which relate bases in 2 and 3 representation spaces. Namely, for the spin- case, components of I are equal to Pauli matrices divided by 2 and I"z" = "τ"3, where
While the forms of these matrices are the isomorphic to those of spin, "these" Pauli matrices only acts within the Hilbert space of isospin, not that of spin, and therefore is common to denote them with τ rather than σ to avoid confusion.
The power of isospin symmetry and related methods such as the Eightfold Way come from the observation that families of particles with similar masses tend to correspond to the invariant subspaces associated with the irreducible representations of the Lie algebra 𝖘𝖚(2). In this context, an invariant subspace is spanned by basis vectors which correspond to particles in a family. Under the action of the Lie algebra 𝖘𝖚(2), which generates rotations in isospin space, elements corresponding to definite particle states or superpositions of states can be rotated into each other, but can never leave the space (since the subspace is in fact invariant). This is reflective of the symmetry present. The fact that unitary matrices will commute with the Hamiltonian means that the physical quantities calculated do not change even under unitary transformation. In the case of isospin, this machinery is used to reflect the fact that the strong force behaves the same under the exchange of the up and down quark (and by extension the exchange of the proton and the neutron).
Relationship to flavor.
The discovery and subsequent analysis of additional particles, both mesons and baryons, made it clear that the concept of isospin symmetry could be broadened to an even larger symmetry group, now called flavor symmetry. Once the kaons and their property of strangeness became better understood, it started to become clear that these, too, seemed to be a part of an enlarged symmetry that contained isospin as a subgroup. The larger symmetry was named the Eightfold Way by Murray Gell-Mann, and was promptly recognized to correspond to the adjoint representation of SU(3). To better understand the origin of this symmetry, Gell-Mann proposed the existence of up, down and strange quarks which would belong to the fundamental representation of the SU(3) flavor symmetry.
Although isospin symmetry is very slightly broken, SU(3) symmetry is more badly broken, due to the much higher mass of the strange quark compared to the up and down. The discovery of charm, bottomness and topness could lead to further expansions up to SU(6) flavour symmetry, but the very large masses of these quarks makes such symmetries almost useless. In modern applications, such as lattice QCD, isospin symmetry is often treated as exact while the heavier quarks must be treated separately.
Quark content and isospin.
Up and down quarks each have isospin "I" = , and isospin 3-components ("I"3) of and − respectively. All other quarks have "I" = 0. In general
Hadron nomenclature.
Hadron nomenclature is based on isospin.
Isospin symmetry of quarks.
In the framework of the Standard Model, the isospin symmetry of the proton and neutron are reinterpreted as the isospin symmetry of the up and down quarks. Technically, the nucleon doublet states are seen to be linear combinations of products of 3-particle isospin doublet states and spin doublet states. That is, the (spin-up) proton wave function, in terms of quark-flavour eigenstates, is described by
and the (spin-up) neutron by
Here, formula_6 is the up quark flavour eigenstate, and formula_7 is the down quark flavour eigenstate, while formula_8 and formula_9 are the eigenstates of formula_10. Although these superpositions are the technically correct way of denoting a proton and neutron in terms of quark flavour and spin eigenstates, for brevity, they are often simply referred to as ""uud" and "udd"". Note also that the derivation above assumes exact isospin symmetry and is modified by SU(2)-breaking terms.
Similarly, the isospin symmetry of the pions are given by:
Weak isospin.
Isospin is similar to, but should not be confused with weak isospin. Briefly, weak isospin is the gauge symmetry of the weak interaction which connects quark and lepton doublets of left-handed particles in all generations; for example, up and down quarks, top and bottom quarks, electrons and electron neutrinos. By contrast (strong) isospin connects only up and down quarks, acts on both chiralities (left and right) and is a global (not a gauge) symmetry.
Gauged isospin symmetry.
Attempts have been made to promote isospin from a global to a local symmetry. In 1954, Chen Ning Yang and Robert Mills suggested that the notion of protons and neutrons, which are continuously rotated into each other by isospin, should be allowed to vary from point to point. To describe this, the proton and neutron direction in isospin space must be defined at every point, giving local basis for isospin. A gauge connection would then describe how to transform isospin along a path between two points.
This Yang–Mills theory describes interacting vector bosons, like the photon of electromagnetism. Unlike the photon, the SU(2) gauge theory would contain self-interacting gauge bosons. The condition of gauge invariance suggests that they have zero mass, just as in electromagnetism.
Ignoring the massless problem, as Yang and Mills did, the theory makes a firm prediction: the vector particle should couple to all particles of a given isospin "universally". The coupling to the nucleon would be the same as the coupling to the kaons. The coupling to the pions would be the same as the self-coupling of the vector bosons to themselves.
When Yang and Mills proposed the theory, there was no candidate vector boson. J. J. Sakurai in 1960 predicted that there should be a massive vector boson which is coupled to isospin, and predicted that it would show universal couplings. The rho mesons were discovered a short time later, and were quickly identified as Sakurai's vector bosons. The couplings of the rho to the nucleons and to each other were verified to be universal, as best as experiment could measure. The fact that the diagonal isospin current contains part of the electromagnetic current led to the prediction of rho-photon mixing and the concept of vector meson dominance, ideas which led to successful theoretical pictures of GeV-scale photon-nucleus scattering.
Although the discovery of the quarks led to reinterpretation of the rho meson as a vector bound state of a quark and an antiquark, it is sometimes still useful to think of it as the gauge boson of a hidden local symmetry

</doc>
<doc id="195351" url="https://en.wikipedia.org/wiki?curid=195351" title="Jacobian matrix and determinant">
Jacobian matrix and determinant

In vector calculus, the Jacobian matrix (, ) is the matrix of all first-order partial derivatives of a vector-valued function. When the matrix is a square matrix, both the matrix and its determinant are referred to as the Jacobian in literature.
Suppose is a function which takes as input the vector and produces as output the vector . Then the Jacobian matrix of is an matrix, usually defined and arranged as follows:
or, component-wise:
This matrix, whose entries are functions of , is also denoted by , , and . (Note that some literature defines the Jacobian as the transpose of the matrix given above.)
The Jacobian matrix is important because if the function is differentiable at a point (this is a slightly stronger condition than merely requiring that all partial derivatives exist there), then the Jacobian matrix defines a linear map , which is the best linear approximation of the function near the point . This linear map is thus the generalization of the usual notion of derivative, and is called the "derivative" or the "differential" of at .
If = , the Jacobian matrix is a square matrix, and its determinant, a function of , is the Jacobian determinant of . It carries important information about the local behavior of . In particular, the function has locally in the neighborhood of a point an inverse function that is differentiable if and only if the Jacobian determinant is nonzero at (see Jacobian conjecture). The Jacobian determinant occurs also when changing the variables in multi-variable integrals (see substitution rule for multiple variables).
If = 1, is a scalar field and the Jacobian matrix is reduced to a row vector of partial derivatives of —i.e. the gradient of .
These concepts are named after the mathematician Carl Gustav Jacob Jacobi (1804–1851).
Jacobian matrix.
The Jacobian generalizes the gradient of a scalar-valued function of multiple variables, which itself generalizes the derivative of a scalar-valued function of a single variable. In other words, the Jacobian for a scalar-valued multivariable function is the gradient and that of a scalar-valued function of single variable is simply its derivative. The Jacobian can also be thought of as describing the amount of "stretching", "rotating" or "transforming" that a transformation imposes locally. For example, if is used to transform an image, the Jacobian , describes how the image in the neighborhood of is transformed.
If a function is differentiable at a point, its derivative is given in coordinates by the Jacobian, but a function doesn't need to be differentiable for the Jacobian to be defined, since only the partial derivatives are required to exist.
If is a point in and is differentiable at , then its derivative is given by . In this case, the linear map described by is the best linear approximation of near the point , in the sense that
for close to and where is the little o-notation (for ) and is the distance between and .
Compare this to a Taylor series for a scalar function of a scalar argument, truncated to first order:
In a sense, both the gradient and Jacobian are "first derivatives"—the former the first derivative of a "scalar function" of several variables, the latter the first derivative of a "vector function" of several variables.
The Jacobian of the gradient of a scalar function of several variables has a special name: the Hessian matrix, which in a sense is the "second derivative" of the function in question.
Jacobian determinant.
If =, then is a function from to itself and the Jacobian matrix is a square matrix. We can then form its determinant, known as the Jacobian determinant. The Jacobian determinant is occasionally referred to as "the Jacobian".
The Jacobian determinant at a given point gives important information about the behavior of near that point. For instance, the continuously differentiable function is invertible near a point if the Jacobian determinant at is non-zero. This is the inverse function theorem. Furthermore, if the Jacobian determinant at is positive, then preserves orientation near ; if it is negative, reverses orientation. The absolute value of the Jacobian determinant at gives us the factor by which the function expands or shrinks volumes near ; this is why it occurs in the general substitution rule.
The Jacobian determinant is used when making a change of variables when evaluating a multiple integral of a function over a region within its domain. To accommodate for the change of coordinates the magnitude of the Jacobian determinant arises as a multiplicative factor within the integral. This is because the -dimensional element is in general a parallelepiped in the new coordinate system, and the -volume of a parallelepiped is the determinant of its edge vectors.
The Jacobian can also be used to solve systems of differential equations at an equilibrium point or approximate solutions near an equilibrium point.
Inverse.
According to the inverse function theorem, the matrix inverse of the Jacobian matrix of an invertible function is the Jacobian matrix of the "inverse" function. That is, if the Jacobian of the function is continuous and nonsingular at the point in , then is invertible when restricted to some neighborhood of and
Conversely, if the Jacobian determinant is not zero at a point, then the function is "locally invertible" near this point, that is there is neighbourhood of this point, in which the function is invertible.
The (unproved) Jacobian conjecture is related to global invertibility in the case of a polynomial functions, that is a function defined by "n" polynomials in "n" variables. It asserts that, if the Jacobian determinant is a non-zero constant (or, equivalently, that it does not have any complex zero), then the function is invertible and its inverse is a polynomial function.
Critical points.
If is a differentiable function, a "critical point" of is a point where the rank of the Jacobian matrix is not maximal. This means that the rank at the critical point is lower than the rank at some neighbour point. In other words, let be the maximal dimension of the open balls contained in the image of ; then a point is critical if all minors of rank of are zero.
In the case where 1 = = = , a point is critical if the Jacobian determinant is zero.
Examples.
Example 1.
Consider the function given by
Then we have
and
and the Jacobian matrix of is
and the Jacobian determinant is
Example 2: polar-Cartesian transformation.
The transformation from polar coordinates to Cartesian coordinates ("x", "y"), is given by the function with components:
The Jacobian determinant is equal to . This can be used to transform integrals between the two coordinate systems:
Example 3: spherical-Cartesian transformation.
The transformation from spherical coordinates to Cartesian coordinates ("x", "y", "z"), is given by the function with components:
The Jacobian matrix for this coordinate change is
The determinant is . As an example, since this determinant implies that the differential volume element . Nevertheless this determinant varies with coordinates.
Example 4.
The Jacobian matrix of the function with components
is
This example shows that the Jacobian need not be a square matrix.
Example 5.
The Jacobian determinant of the function with components
is
From this we see that reverses orientation near those points where and have the same sign; the function is locally invertible everywhere except near points where or . Intuitively, if one starts with a tiny object around the point and apply to that object, one will get a resulting object with approximately times the volume of the original one.
Other uses.
The Jacobian serves as a linearized design matrix in statistical regression and curve fitting; see non-linear least squares.
Dynamical systems.
Consider a dynamical system of the form , where is the (component-wise) time derivative of , and is differentiable. If , then is a stationary point (also called a critical point; this is not to be confused with fixed points). The behavior of the system near a stationary point is related to the eigenvalues of , the Jacobian of at the stationary point. Specifically, if the eigenvalues all have real parts that are negative, then the system is stable near the stationary point, if any eigenvalue has a real part that is positive, then the point is unstable. If the largest real part of the eigenvalues is zero, the Jacobian matrix does not allow for an evaluation of the stability.
Newton's method.
A system of coupled nonlinear equations can be solved iteratively by Newton's method. This method uses the Jacobian matrix of the system of equations.

</doc>
<doc id="928060" url="https://en.wikipedia.org/wiki?curid=928060" title="Jet bundle">
Jet bundle

In differential geometry, the jet bundle is a certain construction that makes a new smooth fiber bundle out of a given smooth fiber bundle. It makes it possible to write differential equations on sections of a fiber bundle in an invariant form. Jets may also be seen as the coordinate free versions of Taylor expansions.
Historically, jet bundles are attributed to Ehresmann, and were an advance on the method (prolongation) of Élie Cartan, of dealing "geometrically" with higher derivatives, by imposing differential form conditions on newly introduced formal variables. Jet bundles are sometimes called sprays, although sprays usually refer more specifically to the associated vector field induced on the corresponding bundle ("e.g.", the geodesic spray on Finsler manifolds.)
More recently, jet bundles have appeared as a concise way to describe phenomena associated with the derivatives of maps, particularly those associated with the calculus of variations. Consequently, the jet bundle is now recognized as the correct domain for a geometrical covariant field theory and much work is done in general relativistic formulations of fields using this approach.
Jets.
Suppose "M" is an "m"-dimensional manifold and that ("E", π, "M") is a fiber bundle. For "p" ∈ "M", let Γ(π) denote the set of all local sections whose domain contains "p". Let "I" = "(I(1), I(2), ..., I(m))" be a multi-index (an ordered "m"-tuple of integers), then
Define the local sections σ, η ∈ Γ(π) to have the same "r"-jet at "p" if
The relation that two maps have the same "r"-jet is an equivalence relation. An "r"-jet is an equivalence class under this relation, and the "r"-jet with representative σ is denoted formula_4. The integer "r" is also called the order of the jet, "p" is its source and σ("p") is its target.
Jet manifolds.
The "r"-th jet manifold of π is the set
and is denoted "Jr(π)". We may define projections π"r" and π"r",0 called the source and target projections respectively, by
If 1 ≤ "k" ≤ "r", then the "k"-jet projection is the function π"r,k" defined by
From this definition, it is clear that π"r" = π π"r",0 and that if 0 ≤ "m" ≤ "k", then π"r,m" = π"k,m" π"r,k". It is conventional to regard π"r,r" = id"Jr(π), the identity map on "Jr(π)" and to identify "J0(π)" with "E".
The functions π"r,k", π"r",0 and π"r" are smooth surjective submersions.
A coordinate system on "E" will generate a coordinate system on "Jr(π)". Let "(U, u)" be an adapted coordinate chart on "E", where "u" = "(xi, uα). The induced coordinate chart "(Ur, ur)" on "Jr(π)" is defined by
where
and the formula_13 functions
are specified by
and are known as the derivative coordinates.
Given an atlas of adapted charts "(U, u)" on "E", the corresponding collection of charts "(Ur, ur)" is a finite-dimensional "C"∞ atlas on "Jr(π)".
Jet bundles.
Since the atlas on each "Jr(π)" defines a manifold, the triples "(Jr(π), πr,k, Jk(π))", "(Jr(π), πr,0, E)" and "(Jr(π), πr, M)" all define fibered manifolds. In particular, if "(E, π, M)" is a fiber bundle, the triple "(Jr(π), πr, M)" defines the "r"-th jet bundle of π.
If "W" ⊂ "M" is an open submanifold, then
If "p" ∈ "M", then the fiber formula_17 is denoted formula_18.
Let σ be a local section of π with domain "W" ⊂ "M". The "r"-th jet prolongation of σ is the map "jrσ": "W" → "Jr(π)" defined by
Note that π"r" "jrσ" = id"W", so "jrσ" really is a section. In local coordinates, "jrσ" is given by
We identify "j0σ" with σ.
Example.
If π is the trivial bundle ("M" × R, pr1, "M"), then there is a canonical diffeomorphism between the first jet bundle "J1(π)" and "T*M" × R. To construct this diffeomorphism, for each σ in Γ"M"(π) write formula_21.
Then, whenever "p" ∈ "M"
Consequently, the mapping
is well-defined and is clearly injective. Writing it out in coordinates shows that it is a diffeomorphism, because if "(xi, u)" are coordinates on "M" × R, where "u" = idR is the identity coordinate, then the derivative coordinates "ui" on "J1(π)" correspond to the coordinates ∂"i" on "T*M".
Likewise, if π is the trivial bundle (R × "M", pr1, R), then there exists a canonical diffeomorphism between "J1(π)" and R × "TM".
Contact structure.
The space "Jr"(π) carries a natural distribution, that is, a sub-bundle of the tangent bundle "TJr"(π)), called the "Cartan distribution". The Cartan distribution is spanned by all tangent planes to graphs of holonomic sections; that is, sections of the form "jrφ" for φ a section of π.
The annihilator of the Cartan distribution is a space of differential one-forms called contact forms, on "Jr"(π). The space of differential one-forms on "Jr"(π) is denoted by formula_24 and the space of contact forms is denoted by formula_25. A one form is a contact form provided its pullback along every prolongation is zero. In other words, formula_26 is a contact form if and only if
for all local sections σ of π over "M".
The Cartan distribution is the main geometrical structure on jet spaces and plays an important role in the geometric theory of partial differential equations. The Cartan distributions are completely non-integrable. In particular, they are not involutive. The dimension of the Cartan distribution grows with the order of the jet space. However, on the space of infinite jets "J∞" the Cartan distribution becomes involutive and finite-dimensional: its dimension coincides with the dimension of the base manifold "M".
Example.
Let us consider the case "(E, π, M)", where "E" ≃ R2 and "M" ≃ R. Then, "(J1(π), π, M)" defines the first jet bundle, and may be coordinated by "(x, u, u1)", where
for all "p" ∈ "M" and σ in Γ"p"(π). A general 1-form on "J1(π)" takes the form
A section σ in Γ"p"(π) has first prolongation
Hence, "(j1σ)*θ" can be calculated as
This will vanish for all sections σ if and only if "c" = 0 and "a" = −"bσ′(x)". Hence, θ = "b(x, u, u1)θ0" must necessarily be a multiple of the basic contact form θ0 = "du" − "u1dx". Proceeding to the second jet space "J2(π)" with additional coordinate "u2", such that
a general 1-form has the construction
This is a contact form if and only if
which implies that "e" = 0 and "a" = −"bσ′(x)" − "cσ′′(x)". Therefore, θ is a contact form if and only if
where θ1 = "du1 − u2dx" is the next basic contact form (Note that here we are identifying the form θ0 with its pull-back formula_33 to "J2(π)").
In general, providing "x, u" ∈ R, a contact form on "Jr+1(π)" can be written as a linear combination of the basic contact forms
where formula_35.
Similar arguments lead to a complete characterization of all contact forms.
In local coordinates, every contact one-form on "Jr+1(π)" can be written as a linear combination
Note that the first prolongation of "V" can be recovered by omitting the second derivative terms in "V2", or by projecting back to "J1(π)".
Infinite Jet Spaces.
The inverse limit of the sequence of projections formula_37 gives rise to the infinite jet space "J∞(π)". A point formula_38 is the equivalence class of sections of π that have the same "k"-jet in "p" as σ for all values of "k". The natural projection π∞ maps formula_38 into "p".
Just by thinking in terms of coordinates, "J∞(π)" appears to be an infinite-dimensional geometric object. In fact, the simplest way of introducing a differentiable structure on "J∞(π)", not relying on differentiable charts, is given by the differential calculus over commutative algebras. Dual to the sequence of projections formula_37 of manifolds is the sequence of injections formula_41 of commutative algebras. Let's denote formula_42 simply by formula_43. Take now the direct limit formula_44 of the formula_43's. It will be a commutative algebra, which can be assumed to be the smooth functions algebra over the geometric object "J∞(π)". Observe that formula_44, being born as a direct limit, carries an additional structure: it is a filtered commutative algebra.
Roughly speaking, a concrete element formula_47 will always belong to some formula_43, so it is a smooth function on the finite-dimensional manifold "Jk"(π) in the usual sense.
Infinitely prolonged PDEs.
Given a "k"-th order system of PDEs "E" ⊆ "Jk(π)", the collection "I(E)" of vanishing on "E" smooth functions on "J∞(π)" is an ideal in the algebra formula_43, and hence in the direct limit formula_44 too.
Enhance "I(E)" by adding all the possible compositions of total derivatives applied to all its elements. This way we get a new ideal "I" of formula_44 which is now closed under the operation of taking total derivative. The submanifold "E"(∞) of "J"∞(π) cut out by "I" is called the infinite prolongation of "E".
Geometrically, "E"(∞) is the manifold of formal solutions of "E". A point formula_38 of "E"(∞) can be easily seen to be represented by a section σ whose "k"-jet's graph is tangent to "E" at the point formula_53 with arbitrarily high order of tangency.
Analytically, if "E" is given by φ = 0, a formal solution can be understood as the set of Taylor coefficients of a section σ in a point "p" that make vanish the Taylor series of formula_54 at the point "p".
Most importantly, the closure properties of "I" imply that "E"(∞) is tangent to the infinite-order contact structure formula_55 on "J∞(π)", so that by restricting formula_55 to "E"(∞) one gets the diffiety formula_57, and can study the associated C-spectral sequence.
Remark.
This article has defined jets of local sections of a bundle, but it is possible to define jets of functions "f: M" → "N", where "M" and "N" are manifolds; the jet of "f" then just corresponds to the jet of the section
("grf" is known as the graph of the function "f") of the trivial bundle ("M" × "N", π1, "M"). However, this restriction does not simplify the theory, as the global triviality of π does not imply the global triviality of π1.

</doc>
<doc id="17070" url="https://en.wikipedia.org/wiki?curid=17070" title="Kaluza–Klein theory">
Kaluza–Klein theory

In physics, Kaluza–Klein theory (KK theory) is a unified field theory of gravitation and electromagnetism built around the idea of a fifth dimension beyond the usual four of space and time. It is considered to be an important precursor to string theory.
The five-dimensional theory was developed in three steps. The original hypothesis came from Theodor Kaluza, who sent his results to Einstein in 1919, and published them in 1921. Kaluza's theory was a purely classical extension of general relativity to five dimensions. The 5-dimensional metric has 15 components. Ten components are identified with the 4-dimensional spacetime metric, 4 components with the electromagnetic vector potential, and one component with an unidentified scalar field sometimes called the "radion" or the "dilaton". Correspondingly, the 5-dimensional Einstein equations yield the 4-dimensional Einstein field equations, the Maxwell equations for the electromagnetic field, and an equation for the scalar field. Kaluza also introduced the hypothesis known as the "cylinder condition", that no component of the 5-dimensional metric depends on the fifth dimension. Without this assumption, the field equations of 5-dimensional relativity are enormously more complex. Standard 4-dimensional physics seems to manifest the cylinder condition. Kaluza also set the scalar field equal to a constant, in which case standard general relativity and electrodynamics are recovered identically.
In 1926, Oskar Klein gave Kaluza's classical 5-dimensional theory a quantum interpretation, to accord with the then-recent discoveries of Heisenberg and Schrödinger. Klein introduced the hypothesis that the fifth dimension was curled up and microscopic, to explain the cylinder condition. Klein also calculated a scale for the fifth dimension based on the quantum of charge.
It wasn't until the 1940s that the classical theory was completed, and the full field equations including the scalar field were obtained by three independent research groups:
Thiry, working in France on his dissertation under Lichnerowicz; Jordan, Ludwig, and Müller in Germany, with critical input from Pauli and Fierz; and Scherrer working alone in Switzerland. Jordan's work led to the scalar-tensor theory of Brans & Dicke; Brans and Dicke were apparently unaware of Thiry or Scherrer. The full Kaluza equations under the cylinder condition are quite complex, and most English-language reviews as well as the English translations of Thiry contain some errors. The complete Kaluza equations were recently evaluated using tensor algebra software.
The Kaluza Hypothesis.
In his 1921 paper, Kaluza established all the elements of the classical 5-dimensional theory: the metric, the field equations, the equations of motion, the stress-energy tensor, and the cylinder condition. The theory has no free parameters; it merely extends general relativity to five dimensions. One starts by hypothesizing a form of the 5-dimensional metric formula_1, where roman indices span 5 dimensions. Let us also introduce the 4-dimensional spacetime metric formula_2, where Greek indices span the usual 4 dimensions of space and time; a 4-vector formula_3 which will be identified with the electromagnetic vector potential; and a scalar field formula_4. Then decompose the 5D metric so that the 4D metric is framed by the electromagnetic vector potential, with the scalar field at the fifth diagonal. This can be visualized as:
More precisely, we can write
where the index formula_7 indicates the fifth coordinate by convention even though the first four coordinates are indexed with 0, 1, 2, and 3. The associated inverse metric is
So far, this decomposition is quite general and all terms are dimensionless. Kaluza then applies the machinery of standard general relativity to this metric. The field equations are obtained from 5-dimensional Einstein equations, and the equations of motion are obtained from the 5-dimensional geodesic hypothesis. The resulting field equations provide both the equations of general relativity and of electrodynamics; the equations of motion provide the 4-dimensional geodesic equation and the Lorentz force law. And one finds that electric charge is identified with motion in the fifth dimension.
The hypothesis for the metric implies an invariant 5-dimensional length element formula_9:
Field Equations from the Kaluza Hypothesis.
The field equations of the 5-dimensional theory were never adequately provided by Kaluza or Klein, mainly regarding the scalar field. The full Kaluza field equations are generally attributed to Thiry, who most famously obtained vacuum field equations, although Kaluza originally provided a stress-energy tensor for his theory and Thiry included a stress-energy tensor in his thesis. But as described by Gonner, several independent groups worked on the field equations in the 1940s and earlier. Thiry is perhaps best known only because an English translation was provided by Applequist, Chodos, & Freund in their review book. Applequist et al. also provided an English translation of Kaluza's paper. There are no English translations of the Jordan papers.
To obtain the 5D field equations, the 5D connections formula_11 are calculated from the 5D metric formula_12, and the 5D Ricci tensor formula_13 is calculated from the 5D connections.
The classic results of Thiry and other authors presume the cylinder condition: 
Without this assumption, the field equations become much more complex, providing many more degrees of freedom that can be identified with various new fields. Paul Wesson and colleagues have pursued relaxation of the cylinder condition to gain extra terms that can be identified with the matter fields, for which Kaluza otherwise inserted a stress-energy tensor by hand.
It has been an objection to the original Kaluza hypothesis to invoke the fifth dimension only to negate its dynamics. But Thiry argued that the interpretation of the Lorentz force law in terms of a 5-dimensional geodesic mitigates strongly for a fifth dimension irrespective of the cylinder condition. Most authors have therefore employed the cylinder condition in deriving the field equations. Furthermore, vacuum equations are typically assumed for which
where
and
The vacuum field equations obtained in this way by Thiry and Jordan's group are as follows.
The field equation for formula_4 is obtained from
where formula_20, where formula_21, and where formula_22 is a standard, 4D covariant derivative. It shows that the electromagnetic field is a source for the scalar field. Note that the scalar field cannot be set to a constant without constraining the electromagnetic field. The earlier treatments by Kaluza and Klein did not have an adequate description of the scalar field, and did not realize the implied constraint on the electromagnetic field by assuming the scalar field to be constant.
The field equation for formula_23 is obtained from
It has the form of the vacuum Maxwell equations if the scalar field is constant.
The field equation for the 4D Ricci tensor formula_25 is obtained from
where formula_27 is the standard 4D Ricci scalar.
This equation shows the remarkable result, called the "Kaluza miracle", that the precise form for the electromagnetic stress-energy tensor emerges from the 5D vacuum equations as a source in the 4D equations: field from the vacuum. This relation allows the definitive identification of formula_28 with the electromagnetic vector potential. Therefore the field needs to be rescaled with a conversion constant formula_29 such that formula_30.
The relation above shows that we must have
where formula_32 is the gravitational constant and formula_33 is the permeability of free space. In the Kaluza theory, the gravitational constant can be understood as an electromagnetic coupling constant in the metric. There is also a stress-energy tensor for the scalar field. The scalar field behaves like a variable gravitational constant, in terms of modulating the coupling of electromagnetic stress energy to spacetime curvature. The sign of formula_34 in the metric is fixed by correspondence with 4D theory so that electromagnetic energy densities are positive. This turns out to imply that the 5th coordinate is spacelike in its signature in the metric.
In the presence of matter, the 5D vacuum condition can not be assumed. Indeed, Kaluza did not assume it. The full field equations require evaluation of the 5D Einstein tensor
as seen in the recovery of the electromagnetic stress-energy tensor above. The 5D curvature tensors are complex, and most English-language reviews contain errors in either formula_36 or formula_37, as does the English translation of. See for a complete set of 5D curvature tensors under the cylinder condition, evaluated using tensor algebra software.
Equations of Motion from the Kaluza Hypothesis.
The equations of motion are obtained from the 5-dimensional geodesic hypothesis in terms of a 5-velocity formula_38:
This equation can be recast in several ways, and it has been studied in various forms by authors including Kaluza, Pauli, Gross & Perry, Gegenberg & Kunstatter, and Wesson & Ponce de Leon, 
but it is instructive to convert it back to the usual 4-dimensional length element formula_40, which is related to the 5-dimensional length element formula_9 as given above:
Then the 5D geodesic equation can be written for the spacetime components of the 4velocity, formula_43:
formula_44
The term quadratic in formula_45 provides the 4D geodesic equation plus some electromagnetic terms:
The term linear in formula_45 provides the Lorentz force law:
This is another expression of the "Kaluza miracle". The same hypothesis for the 5D metric that provides electromagnetic stress-energy in the Einstein equations, also provides the Lorentz force law in the equation of motions along with the 4D geodesic equation. Yet correspondence with the Lorentz force law requires that we identify the component of 5-velocity along the 5th dimension with electric charge:
where formula_50 is particle mass and formula_51 is particle electric charge. Thus, electric charge is understood as motion along the 5th dimension. The fact that the Lorentz force law could be understood as a geodesic in 5 dimensions was to Kaluza a primary motivation for considering the 5-dimensional hypothesis, even in the presence of the aesthetically-unpleasing cylinder condition.
Yet there is a problem: the term quadratic in formula_52. 
If there is no gradient in the scalar field, the term quadratic in formula_52 vanishes. But otherwise the expression above implies 
For elementary particles, formula_56. The term quadratic in formula_52 should dominate the equation, perhaps in contradiction to experience. This was the main shortfall of the 5-dimensional theory as Kaluza saw it, and he gives it some discussion in his original article.
The equation of motion for formula_52 is particularly simple under the cylinder condition. Start with the alternate form of the geodesic equation, written for the covariant 5-velocity:
This means that under the cylinder condition, formula_60 is a constant of the 5-dimensional motion:
Kaluza's Hypothesis for the Matter Stress-Energy Tensor.
Kaluza proposed a 5D matter stress tensor formula_62 of the form
where formula_64 is a density and the length element formula_9 is as defined above.
Then, the spacetime component gives a typical "dust" stress energy tensor:
The mixed component provides a 4-current source for the Maxwell equations:
Just as the 5-dimensional metric comprises the 4-D metric framed by the electromagnetic vector potential, the 5-dimensional stress-energy tensor comprises the 4-D stress-energy tensor framed by the vector 4-current.
The Quantum Interpretation of Klein.
Kaluza's original hypothesis was purely classical and extended discoveries of general relativity. By the time of Klein's contribution, the discoveries of Heisenberg, Schroedinger, and de Broglie were receiving a lot of attention. Klein's "Nature" paper suggested that the fifth dimension is closed and periodic, and that the identification of electric charge with motion in the fifth dimension be interpreted as standing waves of wavelength formula_68, much like the electrons around a nucleus in the Bohr model of the atom. The quantization of electric charge could then be nicely understood in terms of integer multiples of fifth-dimensional momentum. Combining the previous Kaluza result for formula_52 in terms of electric charge, and a de Broglie relation for momentum formula_70, Klein obtained an expression for the 0th mode of such waves:
where formula_72 is the Planck constant. Klein found formula_73 cm, and thereby an explanation for the cylinder condition in this small value.
Klein's "Zeitschrift für Physik" paper of the same year, gave a more-detailed treatment that explicitly invoked the techniques of Schroedinger and de Broglie. It recapitulated much of the classical theory of Kaluza described above, and then departed into Klein's quantum interpretation. Klein solved a Schroedinger-like wave equation using an expansion in terms of fifth-dimensional waves resonating in the closed, compact fifth dimension.
Group Theory Interpretation.
 A splitting of five-dimensional spacetime into the Einstein equations and Maxwell equations in four dimensions was first discovered by Gunnar Nordström in 1914, in the context of his theory of gravity, but subsequently forgotten. Kaluza published his derivation in 1921 as an attempt to unify electromagnetism with Einstein's general relativity.
In 1926, Oskar Klein proposed that the fourth spatial dimension is curled up in a circle of a very small radius, so that a particle moving a short distance along that axis would return to where it began. The distance a particle can travel before reaching its initial position is said to be the size of the dimension. This extra dimension is a compact set, and the phenomenon of having a space-time with compact dimensions is referred to as compactification.
In modern geometry, the extra fifth dimension can be understood to be the circle group U(1), as electromagnetism can essentially be formulated as a gauge theory on a fiber bundle, the circle bundle, with gauge group U(1). In Kaluza–Klein theory this group suggests that gauge symmetry is the symmetry of circular compact dimensions. Once this geometrical interpretation is understood, it is relatively straightforward to replace "U"(1) by a general Lie group. Such generalizations are often called Yang–Mills theories. If a distinction is drawn, then it is that Yang–Mills theories occur on a flat space-time, whereas Kaluza–Klein treats the more general case of curved spacetime. The base space of Kaluza–Klein theory need not be four-dimensional space-time; it can be any (pseudo-)Riemannian manifold, or even a supersymmetric manifold or orbifold or even a noncommutative space.
As an approach to the unification of the forces, it is straightforward to apply the Kaluza–Klein theory in an attempt to unify gravity with the strong and electroweak forces by using the symmetry group of the Standard Model, SU(3) × SU(2) × U(1). However, an attempt to convert this interesting geometrical construction into a bona-fide model of reality flounders on a number of issues, including the fact that the fermions must be introduced in an artificial way (in nonsupersymmetric models). Nonetheless, KK remains an important touchstone in theoretical physics and is often embedded in more sophisticated theories. It is studied in its own right as an object of geometric interest in K-theory.
Even in the absence of a completely satisfying theoretical physics framework, the idea of exploring extra, compactified, dimensions is of considerable interest in the experimental physics and astrophysics communities. A variety of predictions, with real experimental consequences, can be made (in the case of large extra dimensions/warped models). For example, on the simplest of principles, one might expect to have standing waves in the extra compactified dimension(s). If a spatial extra dimension is of radius "R", the invariant mass of such standing waves would be "M""n" = "nh"/"Rc" with "n" an integer, "h" being Planck's constant and "c" the speed of light. This set of possible mass values is often called the Kaluza–Klein tower. Similarly, in Thermal quantum field theory a compactification of the euclidean time dimension leads to the Matsubara frequencies and thus to a discretized thermal energy spectrum.
However, Klein's approach to a quantum theory is flawed and, for example, leads to a calculated electron mass of 3x10exp(30) MeV instead of the measured value 0,71 MeV.
Examples of experimental pursuits include work by the CDF collaboration, which has re-analyzed particle collider data for the signature of effects associated with large extra dimensions/warped models.
Brandenberger and Vafa have speculated that in the early universe, cosmic inflation causes three of the space dimensions to expand to cosmological size while the remaining dimensions of space remained microscopic.
Space-time-matter theory.
One particular variant of Kaluza–Klein theory is space-time-matter theory or induced matter theory, chiefly promulgated by Paul Wesson and other members of the so-called Space-Time-Matter Consortium. In this version of the theory, it is noted that solutions to the equation
may be re-expressed so that in four dimensions, these solutions satisfy Einstein's equations
with the precise form of the "T""μν" following from the Ricci-flat condition on the five-dimensional space. In other words, the cylinder condition of the previous development is dropped, and the stress-energy now comes from the derivatives of the 5D metric with respect to the fifth coordinate. Since the energy–momentum tensor is normally understood to be due to concentrations of matter in four-dimensional space, the above result is interpreted as saying that four-dimensional matter is induced from geometry in five-dimensional space.
In particular, the soliton solutions of formula_74 can be shown to contain the Friedmann–Lemaître–Robertson–Walker metric in both radiation-dominated (early universe) and matter-dominated (later universe) forms. The general equations can be shown to be sufficiently consistent with classical tests of general relativity to be acceptable on physical principles, while still leaving considerable freedom to also provide interesting cosmological models.
Geometric interpretation.
The Kaluza–Klein theory is striking because it has a particularly elegant presentation in terms of geometry. In a certain sense, it looks just like ordinary gravity in free space, except that it is phrased in five dimensions instead of four.
The Einstein equations.
The equations governing ordinary gravity in free space can be obtained from an action, by applying the variational principle to a certain action. Let "M" be a (pseudo-)Riemannian manifold, which may be taken as the spacetime of general relativity. If "g" is the metric on this manifold, one defines the action "S"("g") as
where "R"("g") is the scalar curvature and vol("g") is the volume element. By applying the variational principle to the action
one obtains precisely the Einstein equations for free space:
Here, "R""ij" is the Ricci tensor.
The Maxwell equations.
By contrast, the Maxwell equations describing electromagnetism can be understood to be the Hodge equations of a principal U(1)-bundle or circle bundle π: "P" → "M" with fiber U(1). That is, the electromagnetic field "F" is a harmonic 2-form in the space Ω2("M") of differentiable 2-forms on the manifold "M". In the absence of charges and currents, the free-field Maxwell equations are
where * is the Hodge star.
The Kaluza–Klein geometry.
To build the Kaluza–Klein theory, one picks an invariant metric on the circle S1 that is the fiber of the U(1)-bundle of electromagnetism. In this discussion, an "invariant metric" is simply one that is invariant under rotations of the circle. Suppose this metric gives the circle a total length of Λ. One then considers metrics formula_80 on the bundle "P" that are consistent with both the fiber metric, and the metric on the underlying manifold "M". The consistency conditions are:
The Kaluza–Klein action for such a metric is given by
The scalar curvature, written in components, then expands to
where π* is the pullback of the fiber bundle projection π: "P" → "M". The connection "A" on the fiber bundle is related to the electromagnetic field strength as
That there always exists such a connection, even for fiber bundles of arbitrarily complex topology, is a result from homology and specifically, K-theory. Applying Fubini's theorem and integrating on the fiber, one gets
Varying the action with respect to the component "A", one regains the Maxwell equations. Applying the variational principle to the base metric "g", one gets the Einstein equations
with the stress–energy tensor being given by
sometimes called the Maxwell stress tensor.
The original theory identifies Λ with the fiber metric "g"55, and allows Λ to vary from fiber to fiber. In this case, the coupling between gravity and the electromagnetic field is not constant, but has its own dynamical field, the radion.
Generalizations.
In the above, the size of the loop Λ acts as a coupling constant between the gravitational field and the electromagnetic field. If the base manifold is four-dimensional, the Kaluza–Klein manifold "P" is five-dimensional. The fifth dimension is a compact space, and is called the compact dimension. The technique of introducing compact dimensions to obtain a higher-dimensional manifold is referred to as compactification. Compactification does not produce group actions on chiral fermions except in very specific cases: the dimension of the total space must be 2 mod 8 and the G-index of the Dirac operator of the compact space must be nonzero.
The above development generalizes in a more-or-less straightforward fashion to general principal "G"-bundles for some arbitrary Lie group "G" taking the place of U(1). In such a case, the theory is often referred to as a Yang–Mills theory, and is sometimes taken to be synonymous. If the underlying manifold is supersymmetric, the resulting theory is a super-symmetric Yang–Mills theory.
Empirical tests.
Up to now, no experimental or observational signs of extra dimensions have been officially reported. Many theoretical search techniques for detecting Kaluza–Klein resonances have been proposed using the mass couplings of such resonances with the top quark, however until the Large Hadron Collider (LHC) reaches full operational power observation of such resonances are unlikely. An analysis of results from the LHC in December 2010 severely constrains theories with large extra dimensions.
The observation of a Higgs-like boson at the LHC puts a brand new empirical test in the search for Kaluza–Klein resonances and supersymmetric particles. 
The loop Feynman diagrams that exist in the Higgs Interactions allow any particle with electric charge and mass to run in such a loop. Standard Model particles besides the top quark and W boson do not make big contributions to the cross-section observed in the H → γγ decay, but if there are new particles beyond the Standard Model, they could potentially change the ratio of the predicted Standard Model H → γγ cross-section to the experimentally observed cross-section. Hence a measurement of any dramatic change to the H → γγ cross section predicted by the Standard Model is crucial in probing the physics beyond it.

</doc>
<doc id="385846" url="https://en.wikipedia.org/wiki?curid=385846" title="Kaon">
Kaon

In particle physics, a kaon , also called a K meson and denoted , is any of a group of four mesons distinguished by a quantum number called strangeness. In the quark model they are understood to be bound states of a strange quark (or antiquark) and an up or down antiquark (or quark).
Kaons have proved to be a copious source of information on the nature of fundamental interactions since their discovery in cosmic rays in 1947. They were essential in establishing the foundations of the Standard Model of particle physics, such as the quark model of hadrons and the theory of quark mixing (the latter was acknowledged by a Nobel Prize in Physics in 2008). Kaons have played a distinguished role in our understanding of fundamental conservation laws: CP violation, a phenomenon generating the observed matter–antimatter asymmetry of the universe, was discovered in the kaon system in 1964 (which was acknowledged by a Nobel Prize in 1980). Moreover, direct CP violation was also discovered in the kaon decays in the early 2000s.
Basic properties.
The four kaons are :
It is clear from the quark model assignments that the kaons form two doublets of isospin; that is, they belong to the fundamental representation of SU(2) called the 2. One doublet of strangeness +1 contains the and the . The antiparticles form the other doublet (of strangeness −1).
[a] Strong eigenstate. No definite lifetime (see kaon notes below) <br>
[b] Weak eigenstate. Makeup is missing small CP–violating term (see notes on neutral kaons below).
<br>
[c] The mass of the and are given as that of the . However, it is known that a difference between the masses of the and on the order of exists.
Although the and its antiparticle are usually produced via the strong force, they decay weakly. Thus, once created the two are better thought of as superpositions of two weak eigenstates which have vastly different lifetimes:
An experimental observation made in 1964 that K-longs rarely decay into two pions was the discovery of CP violation (see below).
Main decay modes for :
Decay modes for the are charge conjugates of the ones above.
Strangeness.
The discovery of hadrons with the internal quantum number "strangeness" marks the beginning
of a most exciting epoch in particle physics that even now, fifty years later, has not yet
found its conclusion ... by and large experiments have driven the development, and that
major discoveries came unexpectedly or even against expectations expressed by theorists.
 — I.I. Bigi and A.I. Sanda, "CP violation", (ISBN 0-521-44349-0)
In 1947, G. D. Rochester and Clifford Charles Butler of the University of Manchester published two cloud chamber photographs of cosmic ray-induced events, one showing what appeared to be a neutral particle decaying into two charged pions, and one which appeared to be a charged particle decaying into a charged pion and something neutral. The estimated mass of the new particles was very rough, about half a proton's mass. More examples of these "V-particles" were slow in coming.
The first breakthrough was obtained at Caltech, where a cloud chamber was taken up Mount Wilson, for greater cosmic ray exposure. In 1950, 30 charged and 4 neutral V-particles were reported. Inspired by this, numerous mountaintop observations were made over the next several years, and by 1953, the following terminology was adopted: "L-meson" meant muon or pion. "K meson" meant a particle intermediate in mass between the pion and nucleon. "Hyperon" meant any particle heavier than a nucleon.
The decays were extremely slow; typical lifetimes are of the order of . However, production in pion-proton reactions proceeds much faster, with a time scale of . The problem of this mismatch was solved by Abraham Pais who postulated the new quantum number called "strangeness" which is conserved in strong interactions but violated by the weak interactions. Strange particles appear copiously due to "associated production" of a strange and an antistrange particle together. It was soon shown that this could not be a multiplicative quantum number, because that would allow reactions which were never seen in the new synchrotrons which were commissioned in Brookhaven National Laboratory in 1953 and in the Lawrence Berkeley Laboratory in 1955.
Parity violation.
Two different decays were found for charged strange mesons:
The intrinsic parity of a pion is P = −1, and parity is a multiplicative quantum number. Therefore, the two final states have different parity (P = +1 and P = −1, respectively). It was thought that the initial states should also have different parities, and hence be two distinct particles. However, with increasingly precise measurements, no difference was found between the masses and lifetimes of each, respectively, indicating that they are the same particle. This was known as the τ–θ puzzle. It was resolved only by the discovery of parity violation in weak interactions. Since the mesons decay through weak interactions, parity is not conserved, and the two decays are actually decays of the same particle, now called the .
CP violation in neutral meson oscillations.
Initially it was thought that although parity was violated, CP (charge parity) symmetry was conserved. In order to understand the discovery of CP violation, it is necessary to understand the mixing of neutral kaons; this phenomenon does not require CP violation, but it is the context in which CP violation was first observed.
Neutral kaon mixing.
Since neutral kaons carry strangeness, they cannot be their own antiparticles. There must be then two different neutral kaons, differing by two units of strangeness. The question was then how to establish the presence of these two mesons. The solution used a phenomenon called neutral particle oscillations, by which these two kinds of mesons can turn from one into another through the weak interactions, which cause them to decay into pions (see the adjacent figure).
These oscillations were first investigated by Murray Gell-Mann and Abraham Pais together. They considered the CP-invariant time evolution of states with opposite strangeness. In matrix notation one can write
where ψ is a quantum state of the system specified by the amplitudes of being in each of the two basis states (which are "a" and "b" at time "t" = 0). The diagonal elements ("M") of the Hamiltonian are due to strong interaction physics which conserves strangeness. The two diagonal elements must be equal, since the particle and antiparticle have equal masses in the absence of the weak interactions. The off-diagonal elements, which mix opposite strangeness particles, are due to weak interactions; CP symmetry requires them to be real.
The consequence of the matrix "H" being real is that the probabilities of the two states will forever oscillate back and forth. However, if any part of the matrix were imaginary, as is forbidden by CP symmetry, then part of the combination will diminish over time. The diminishing part can be either one component ("a") or the other ("b"), or a mixture of the two.
Mixing.
The eigenstates are obtained by diagonalizing this matrix. This gives new eigenvectors, which we can call K1 which is the difference of the two states of opposite strangeness, and K2, which is the sum. The two are eigenstates of CP with opposite eigenvalues; K1 has CP = +1, and K2 has CP = -1 Since the two-pion final state also has CP = +1, only the K1 can decay this way. The K2 must decay into three pions. Since the mass of K2 is just a little larger than the sum of the masses of three pions, this decay proceeds very slowly, about 600 times slower than the decay of K1 into two pions. These two different modes of decay were observed by Leon Lederman and his coworkers in 1956, establishing the existence of the two weak eigenstates (states with definite lifetimes under decays via the weak force) of the neutral kaons.
These two weak eigenstates are called the (K-long) and (K-short). CP symmetry, which was assumed at the time, implies that  = K1 and  = K2.
Oscillation.
An initially pure beam of will turn into its antiparticle while propagating, which will turn back into the original particle, and so on. This is called particle oscillation. On observing the weak decay "into leptons", it was found that a always decayed into an electron, whereas the antiparticle decayed into the positron. The earlier analysis yielded a relation between the rate of electron and positron production from sources of pure and its antiparticle . Analysis of the time dependence of this semileptonic decay showed the phenomenon of oscillation, and allowed the extraction of the mass splitting between the and . Since this is due to weak interactions it is very small, 10−15 times the mass of each state.
Regeneration.
A beam of neutral kaons decays in flight so that the short-lived disappears, leaving a beam of pure long-lived . If this beam is shot into matter, then the and its antiparticle interact differently with the nuclei. The undergoes quasi-elastic scattering with nucleons, whereas its antiparticle can create hyperons. Due to the different interactions of the two components, quantum coherence between the two particles is lost. The emerging beam then contains different linear superpositions of the and . Such a superposition is a mixture of and ; the is regenerated by passing a neutral kaon beam through matter. Regeneration was observed by Oreste Piccioni and his collaborators at Lawrence Berkeley National Laboratory. Soon thereafter, Robert Adair and his coworkers reported excess regeneration, thus opening a new chapter in this history.
CP violation.
While trying to verify Adair's results, J. Christenson, James Cronin, Val Fitch and Rene Turlay of Princeton University found decays of into two pions (CP = +1)
in an experiment performed in 1964 at the Alternating Gradient Synchrotron at the Brookhaven laboratory. As explained in an earlier section, this required the assumed initial and final states to have different values of CP, and hence immediately suggested CP violation. Alternative explanations such as non-linear quantum mechanics and a new unobserved particle were soon ruled out, leaving CP violation as the only possibility. Cronin and Fitch received the Nobel Prize in Physics for this discovery in 1980.
It turns out that although the and are weak eigenstates (because they have definite lifetimes for decay by way of the weak force), they are "not quite" CP eigenstates. Instead, for small ε (and up to normalization),
and similarly for . Thus occasionally the decays as a K1 with CP = +1, and likewise the can decay with CP = −1. This is known as indirect CP violation, CP violation due to mixing of and its antiparticle. There is also a direct CP violation effect, in which the CP violation occurs during the decay itself. Both are present, because both mixing and decay arise from the same interaction with the W boson and thus have CP violation predicted by the CKM matrix.

</doc>
<doc id="32172638" url="https://en.wikipedia.org/wiki?curid=32172638" title="Kepler's equation">
Kepler's equation

In orbital mechanics, Kepler's equation relates various geometric properties of the orbit of a body subject to a central force.
It was first derived by Johannes Kepler in 1609 in Chapter 60 of his "Astronomia nova", and in book V of his "Epitome of Copernican Astronomy" (1621) Kepler proposed an iterative solution to the equation. The equation has played an important role in the history of both physics and mathematics, particularly classical celestial mechanics.
Equation.
Kepler's equation is
where is the mean anomaly, is the eccentric anomaly, and is the eccentricity.
The 'eccentric anomaly' is useful to compute the position of a point moving in a Keplerian orbit. As for instance, if the body passes the periastron at coordinates , , at time , then to find out the position of the body at any time, you first calculate the mean anomaly from the time and the mean motion by the formula , then solve the Kepler equation above to get , then get the coordinates from:
Kepler's equation is a transcendental equation because sine is a transcendental function, meaning it cannot be solved for algebraically. Numerical analysis and series expansions are generally required to evaluate .
Alternate forms.
There are several forms of Kepler's equation. Each form is associated with a specific type of orbit. The standard Kepler equation is used for elliptic orbits (0 ≤ "e" < 1). The hyperbolic Kepler equation is used for hyperbolic orbits ("e" ≫ 1). The radial Kepler equation is used for linear (radial) orbits ("e" = 1). Barker's equation is used for parabolic orbits ("e" = 1). When "e" = 1, Kepler's equation is not associated with an orbit.
When "e" = 0, the orbit is circular. Increasing "e" causes the circle to flatten into an ellipse. When "e" = 1, the orbit is completely flat, and it appears to be a either a segment if the orbit is closed, or a ray if the orbit is open. An infinitesimal increase to "e" results in a hyperbolic orbit with a turning angle of 180 degrees, and the orbit appears to be a ray. Further increases reduce the turning angle, and as "e" goes to infinity, the orbit becomes a straight line of infinite length.
Hyperbolic Kepler equation.
The Hyperbolic Kepler equation is:
where "H" is the hyperbolic eccentric anomaly.
This equation is derived by multiplying Kepler's equation by the square root of −1; "i" = √ for imaginary unit, and replacing
to obtain
Radial Kepler equation.
The Radial Kepler equation is:
where "t" is time, and "x" is the distance along an "x"-axis.
This equation is derived by multiplying Kepler's equation by 1/2 making the replacement
and setting "e" = 1 gives
Inverse problem.
Calculating "M" for a given value of "E" is straightforward. However, solving for "E" when "M" is given can be considerably more challenging.
Kepler's equation can be solved for "E" analytically by Lagrange inversion. The solution of Kepler's equation given by two Taylor series below.
Confusion over the solvability of Kepler's equation has persisted in the literature for four centuries. Kepler himself expressed doubt at the possibility of ﬁnding a general solution.
Inverse Kepler equation.
The inverse Kepler equation is the solution of Kepler's equation for all real values of e:
Evaluating this yields:
These series can be reproduced in Mathematica with the InverseSeries operation.
These functions are simple Taylor series. Taylor series representations of transcendental functions are considered to be definitions of those functions. Therefore this solution is a formal definition of the inverse Kepler equation. While this solution is the simplest in a certain mathematical sense, for values of "e" near 1 the convergence is very poor, other solutions are preferable for most applications. Alternatively, Kepler's equation can be solved numerically.
The solution for "e" ≠ 1 was discovered by Karl Stumpff in 1968, but its significance wasn't recognized.
Inverse radial Kepler equation.
The inverse radial Kepler equation is:
Evaluating this yields:
<br>
To obtain this result using Mathematica:
Numerical approximation of inverse problem.
For most applications, the inverse problem can be computed numerically by finding the root of the function:
This can be done iteratively via Newton's method:
Note that "E" and "M" are in units of radians in this computation. This iteration is repeated until desired accuracy is obtained (e.g. when "f"("E") < desired accuracy). For most elliptical orbits an initial value of "E"0 = "M"("t") is sufficient. For orbits with "e" > 0.8, an initial value of "E"0 = "π" should be used.
A similar approach can be used for the hyperbolic form of Kepler's equation. In the case of a parabolic trajectory, Barker's equation is used.

</doc>
<doc id="17553" url="https://en.wikipedia.org/wiki?curid=17553" title="Kepler's laws of planetary motion">
Kepler's laws of planetary motion

In astronomy, Kepler's laws of planetary motion are three scientific laws describing the motion of planets around the Sun.
Most planetary orbits are almost circles, and careful observation and calculation is required in order to establish that they are actually ellipses. Using calculations of the orbit of Mars, whose published values are somewhat suspect, which indicated elliptical orbits, Johannes Kepler inferred that other heavenly bodies, including those farther away from the Sun, also have elliptical orbits.
Kepler's work (published between 1609 and 1619) improved the heliocentric theory of Nicolaus Copernicus, explaining how the planets' speeds varied, and using elliptical orbits rather than circular orbits with epicycles.
Isaac Newton showed in 1687 that relationships like Kepler's would apply in the solar system to a good approximation, as consequences of his own laws of motion and law of universal gravitation.
Kepler's laws are part of the foundation of modern astronomy and physics.
Comparison to Copernicus.
Kepler's laws improve the model of Copernicus. If the eccentricities of the planetary orbits are taken as zero, then Kepler basically agrees with Copernicus:
The eccentricities of the orbits of those planets known to Copernicus and Kepler are small, so the foregoing rules give good approximations of planetary motion; but Kepler's laws fit the observations better than Copernicus's.
Kepler's corrections are not at all obvious:
The eccentricity of the orbit of the Earth makes the time from the March equinox to the September equinox, around 186 days, unequal to the time from the September equinox to the March equinox, around 179 days. A diameter would cut the orbit into equal parts, but the plane through the sun parallel to the equator of the earth cuts the orbit into two parts with areas in a 186 to 179 ratio, so the eccentricity of the orbit of the Earth is approximately
which is close to the correct value (0.016710219) (see Earth's orbit).
The calculation is correct when perihelion, the date the Earth is closest to the Sun, falls on a solstice. The current perihelion, near January 4, is fairly close to the solstice of December 21 or 22.
Nomenclature.
It took nearly two centuries for the current formulation of Kepler's work to take on its settled form. Voltaire's "Eléments de la philosophie de Newton" ("Elements of Newton's Philosophy") of 1738 was the first publication to use the terminology of "laws". The "Biographical Encyclopedia of Astronomers" in its article on Kepler (p. 620) states that the terminology of scientific laws for these discoveries was current at least from the time of Joseph de Lalande. It was the exposition of Robert Small, in "An account of the astronomical discoveries of Kepler" (1804) that made up the set of three laws, by adding in the third. Small also claimed, against the history, that these were empirical laws, based on inductive reasoning.
Further, the current usage of "Kepler's Second Law" is something of a misnomer. Kepler had two versions of it, related in a qualitative sense, the "distance law" and the "area law". The "area law" is what became the Second Law in the set of three; but Kepler did himself not privilege it in that way.
History.
Johannes Kepler published his first two laws about planetary motion in 1609, having found them by analyzing the astronomical observations of Tycho Brahe. Kepler's third law was published in 1619.
Kepler in 1621 and Godefroy Wendelin in 1643 noted that Kepler's third law applies to the four brightest moons of Jupiter. The second law, in the "area law" form, was contested by Nicolaus Mercator in a book from 1664; but by 1670 he was publishing in its favour in "Philosophical Transactions", and as the century proceeded it became more widely accepted. The reception in Germany changed noticeably between 1688, the year in which Newton's "Principia" was published and was taken to be basically Copernican, and 1690, by which time work of Gottfried Leibniz on Kepler had been published.
Newton is credited with understanding that the second law is not special to the inverse square law of gravitation, being a consequence just of the radial nature of that law; while the other laws do depend on the inverse square form of the attraction. Carl Runge and Wilhelm Lenz much later identified a symmetry principle in the phase space of planetary motion (the orthogonal group O(4) acting) which accounts for the first and third laws in the case of Newtonian gravitation, as conservation of angular momentum does via rotational symmetry for the second law.
Formulary.
The mathematical model of the kinematics of a planet subject to the laws allows a large range of further calculations.
First law.
Mathematically, an ellipse can be represented by the formula:
where formula_3 is the semi-latus rectum, and "ε" is the eccentricity of the ellipse, and "r" is the distance from the Sun to the planet, and "θ" is the angle to the planet's current position from its closest approach, as seen from the Sun. So ("r", "θ") are polar coordinates.
For an ellipse 0 < "ε" < 1 ; in the limiting case "ε" = 0, the orbit is a circle with the sun at the centre (i.e. where there is no, or nil, eccentricity).
At "θ" = 0°, perihelion, the distance is minimum
At "θ" = 90° and at "θ" = 270° the distance is equal to formula_3.
At "θ" = 180°, aphelion, the distance is maximum (by definition, aphelion is – invariably – perihelion plus 180°)
The semi-major axis "a" is the arithmetic mean between "r"min and "r"max:
The semi-minor axis "b" is the geometric mean between "r"min and "r"max:
The semi-latus rectum "p" is the harmonic mean between "r"min and "r"max:
The eccentricity "ε" is the coefficient of variation between "r"min and "r"max:
The area of the ellipse is
The special case of a circle is "ε" = 0, resulting in "r" = "p" = "r"min = "r"max = "a" = "b" and "A" = "πr"2.
Second law.
The orbital radius and angular velocity of the planet in the elliptical orbit will vary. This is shown in the animation: the planet travels faster when closer to the sun, then slower when farther from the sun. Kepler's second law states that the blue sector has constant area.
In a small time formula_15 the planet sweeps out a small triangle having base line formula_16 and height formula_17 and area
formula_18 and so the constant areal velocity is 
formula_19
The area enclosed by the elliptical orbit is
formula_20
So the period
formula_21
satisfies
and the mean motion of the planet around the Sun
satisfies
Third law.
This captures the relationship between the distance of planets from the Sun, and their orbital periods.
For a brief biography of Kepler and discussion of his third law, see: NASA: Stargaze.
Kepler enunciated in 1619 this third law in a laborious attempt to determine what he viewed as the "music of the spheres" according to precise laws, and express it in terms of musical notation.
So it was known as the "harmonic law".
Mathematically, the law says that the expression
has the same value for all the planets in the solar system. Here "P" is the time taken for a planet to complete an orbit round the sun, and "a" is the mean value between the maximum and minimum distances between the planet and sun (i.e. the semimajor axis).
The modern formulation, with the constant evaluated, reads as:
where
The value formula_27 changes for each planet, so the proportionality constant is not truly the same. Nevertheless, given that "m" is so small relative to "M" for planets in our solar system, the approximation formula_28 is good.
Planetary acceleration.
Isaac Newton computed in his Philosophiæ Naturalis Principia Mathematica the acceleration of a planet moving according to Kepler's first and second law.
This implies that the Sun may be the physical cause of the acceleration of planets.
Newton defined the force acting on a planet to be the product of its mass and the acceleration (see Newton's laws of motion). So:
The Sun plays an unsymmetrical part, which is unjustified. So he assumed, in Newton's law of universal gravitation:
As the planets have small masses compared to that of the Sun, the orbits conform approximately to Kepler's laws. Newton's model improves upon Kepler's model, and fits actual observations more accurately (see two-body problem).
A deviation in the motion of a planet from Kepler's laws due to gravitational attraction by other planets is called a perturbation.
Below comes the detailed calculation of the acceleration of a planet moving according to Kepler's first and second laws.
Acceleration vector.
From the heliocentric point of view consider the vector to the planet formula_29 where formula_30 is the distance to the planet and the direction formula_31 is a unit vector. The direction of a planet changes as it moves:
where formula_33 is the unit vector orthogonal to formula_34 and pointing in the direction of rotation, and formula_35 is the polar angle, and where a dot on top of the variable signifies differentiation with respect to time.
So differentiating the position vector twice to obtain the velocity and the acceleration vectors:
So
where the radial acceleration is
and the transversal acceleration is
The inverse square law.
Kepler's second law says that
is constant.
The transversal acceleration formula_42 is zero:
So the acceleration of a planet obeying Kepler's second law is directed towards the sun.
The radial acceleration formula_44 is
Kepler's first law states that the orbit is described by the equation:
Differentiating with respect to time
or
Differentiating once more
The radial acceleration formula_44 satisfies
Substituting the equation of the ellipse gives
The relation formula_53 gives the simple final result
This means that the acceleration vector formula_55 of any planet obeying Kepler's first and second law satisfies the inverse square law
where
is a constant, and formula_58 is the unit vector pointing from the Sun towards the planet, and formula_16 is the distance between the planet and the Sun.
According to Kepler's third law, formula_60 has the same value for all the planets. So the inverse square law for planetary accelerations applies throughout the entire solar system.
The inverse square law is a differential equation. The solutions to this differential equation include the Keplerian motions, as shown, but they also include motions where the orbit is a hyperbola or parabola or a straight line. See Kepler orbit.
Newton's law of gravitation.
By Newton's second law, the gravitational force that acts on the planet is:
where formula_62 is the mass of the planet and formula_60 has the same value for all planets in the solar system. According to Newton's third Law, the Sun is attracted to the planet by a force of the same magnitude. Since the force is proportional to the mass of the planet, under the symmetric consideration, it should also be proportional to the mass of the Sun, formula_64. So
where formula_66 is the gravitational constant.
The acceleration of solar system body number "i" is, according to Newton's laws:
where formula_68 is the mass of body "j", formula_69 is the distance between body "i" and body "j", formula_70 is the unit vector from body "i" towards body "j", and the vector summation is over all bodies in the world, besides "i" itself.
In the special case where there are only two bodies in the world, Earth and Sun, the acceleration becomes
which is the acceleration of the Kepler motion. So this Earth moves around the Sun according to Kepler's laws.
If the two bodies in the world are Moon and Earth the acceleration of the Moon becomes
So in this approximation the Moon moves around the Earth according to Kepler's laws.
In the three-body case the accelerations are
These accelerations are not those of Kepler orbits, and the three-body problem is complicated. But Keplerian approximation is the basis for perturbation calculations. See Lunar theory.
Position as a function of time.
Kepler used his two first laws to compute the position of a planet as a function of time. His method involves the solution of a transcendental equation called Kepler's equation.
The procedure for calculating the heliocentric polar coordinates ("r","θ") of a planet as a function of the time "t" since perihelion, is the following four steps:
The important special case of circular orbit, "ε" = 0, gives "θ" = "E" = "M". Because the uniform circular motion was considered to be "normal", a deviation from this motion was considered an anomaly.
The proof of this procedure is shown below.
Mean anomaly, "M".
The Keplerian problem assumes an elliptical orbit and the four points:
and
The problem is to compute the polar coordinates ("r","θ") of the planet from the time since perihelion, "t".
It is solved in steps. Kepler considered the circle with the major axis as a diameter, and
The sector areas are related by formula_88
The circular sector area formula_89
The area swept since perihelion, 
is by Kepler's second law proportional to time since perihelion. So the mean anomaly, "M", is proportional to time since perihelion, "t".
where "n" is the mean motion.
Eccentric anomaly, "E".
When the mean anomaly "M" is computed, the goal is to compute the true anomaly "θ". The function "θ" = "f"("M") is, however, not elementary. Kepler's solution is to use
as an intermediate variable, and first compute "E" as a function of "M" by solving Kepler's equation below, and then compute the true anomaly "θ" from the eccentric anomaly "E". Here are the details.
Division by "a"2/2 gives Kepler's equation
This equation gives "M" as a function of "E". Determining "E" for a given "M" is the inverse problem. Iterative numerical algorithms are commonly used.
Having computed the eccentric anomaly "E", the next step is to calculate the true anomaly "θ".
True anomaly, "θ".
Note from the figure that
so that
Dividing by formula_98 and inserting from Kepler's first law
to get
The result is a usable relationship between the eccentric anomaly "E" and the true anomaly "θ".
A computationally more convenient form follows by substituting into the trigonometric identity:
Get
Multiplying by 1 + "ε" gives the result
This is the third step in the connection between time and position in the orbit.
Distance, "r".
The fourth step is to compute the heliocentric distance "r" from the true anomaly "θ" by Kepler's first law:
Using the relation above between "θ" and "E" the final equation for the distance "r" is:

</doc>
<doc id="17327" url="https://en.wikipedia.org/wiki?curid=17327" title="Kinetic energy">
Kinetic energy

In physics, the kinetic energy of an object is the energy that it possesses due to its motion.
It is defined as the work needed to accelerate a body of a given mass from rest to its stated velocity. Having gained this energy during its acceleration, the body maintains this kinetic energy unless its speed changes. The same amount of work is done by the body in decelerating from its current speed to a state of rest.
In classical mechanics, the kinetic energy of a non-rotating object of mass "m" traveling at a speed "v" is . In relativistic mechanics, this is a good approximation only when "v" is much less than the speed of light.
The standard unit of kinetic energy is the joule.
History and etymology.
The adjective "kinetic" has its roots in the Greek word κίνησις "kinesis", meaning "motion". The dichotomy between kinetic energy and potential energy can be traced back to Aristotle's concepts of actuality and potentiality.
The principle in classical mechanics that "E ∝ mv²" was first developed by Gottfried Leibniz and Johann Bernoulli, who described kinetic energy as the "living force", "vis viva". Willem 's Gravesande of the Netherlands provided experimental evidence of this relationship. By dropping weights from different heights into a block of clay, Willem 's Gravesande determined that their penetration depth was proportional to the square of their impact speed. Émilie du Châtelet recognized the implications of the experiment and published an explanation.
The terms "kinetic energy" and "work" in their present scientific meanings date back to the mid-19th century. Early understandings of these ideas can be attributed to Gaspard-Gustave Coriolis, who in 1829 published the paper titled "Du Calcul de l'Effet des Machines" outlining the mathematics of kinetic energy. William Thomson, later Lord Kelvin, is given the credit for coining the term "kinetic energy" c. 1849–51.
Introduction.
Energy occurs in many forms, including chemical energy, thermal energy, electromagnetic radiation, gravitational energy, electric energy, elastic energy, nuclear energy, and rest energy. These can be categorized in two main classes: potential energy and kinetic energy. Kinetic energy is the movement energy of an object.
Kinetic energy may be best understood by examples that demonstrate how it is transformed to and from other forms of energy. For example, a cyclist uses chemical energy provided by food to accelerate a bicycle to a chosen speed. On a level surface, this speed can be maintained without further work, except to overcome air resistance and friction. The chemical energy has been converted into kinetic energy, the energy of motion, but the process is not completely efficient and produces heat within the cyclist.
The kinetic energy in the moving cyclist and the bicycle can be converted to other forms. For example, the cyclist could encounter a hill just high enough to coast up, so that the bicycle comes to a complete halt at the top. The kinetic energy has now largely been converted to gravitational potential energy that can be released by freewheeling down the other side of the hill. Since the bicycle lost some of its energy to friction, it never regains all of its speed without additional pedaling. The energy is not destroyed; it has only been converted to another form by friction. Alternatively the cyclist could connect a dynamo to one of the wheels and generate some electrical energy on the descent. The bicycle would be traveling slower at the bottom of the hill than without the generator because some of the energy has been diverted into electrical energy. Another possibility would be for the cyclist to apply the brakes, in which case the kinetic energy would be dissipated through friction as heat.
Like any physical quantity that is a function of velocity, the kinetic energy of an object depends on the relationship between the object and the observer's frame of reference. Thus, the kinetic energy of an object is not invariant.
Spacecraft use chemical energy to launch and gain considerable kinetic energy to reach orbital velocity. In a perfectly circular orbit, this kinetic energy remains constant because there is almost no friction in near-earth space. However it becomes apparent at re-entry when some of the kinetic energy is converted to heat. If the orbit is elliptical or hyperbolic, then throughout the orbit kinetic and potential energy are exchanged; kinetic energy is greatest and potential energy lowest at closest approach to the earth or other massive body, while potential energy is greatest and kinetic energy the lowest at maximum distance. Without loss or gain, however, the sum of the kinetic and potential energy remains constant.
Kinetic energy can be passed from one object to another. In the game of billiards, the player imposes kinetic energy on the cue ball by striking it with the cue stick. If the cue ball collides with another ball, it slows down dramatically and the ball it collided with accelerates to a speed as the kinetic energy is passed on to it. Collisions in billiards are effectively elastic collisions, in which kinetic energy is preserved. In inelastic collisions, kinetic energy is dissipated in various forms of energy, such as heat, sound, binding energy (breaking bound structures).
Flywheels have been developed as a method of energy storage. This illustrates that kinetic energy is also stored in rotational motion.
Several mathematical descriptions of kinetic energy exist that describe it in the appropriate physical situation. For objects and processes in common human experience, the formula ½mv² given by Newtonian (classical) mechanics is suitable. However, if the speed of the object is comparable to the speed of light, relativistic effects become significant and the relativistic formula is used. If the object is on the atomic or sub-atomic scale, quantum mechanical effects are significant and a quantum mechanical model must be employed.
Newtonian kinetic energy.
Kinetic energy of rigid bodies.
In classical mechanics, the kinetic energy of a "point object" (an object so small that its mass can be assumed to exist at one point), or a non-rotating rigid body depends on the mass of the body as well as its speed. The kinetic energy is equal to 1/2 the product of the mass and the square of the speed. In formula form:
where formula_2 is the mass and formula_3 is the speed (or the velocity) of the body. In SI units (used for most modern scientific work), mass is measured in kilograms, speed in metres per second, and the resulting kinetic energy is in joules.
For example, one would calculate the kinetic energy of an 80 kg mass (about 180 lbs) traveling at 18 metres per second (about 40 mph, or 65 km/h) as
When you throw a ball, you do work on it to give it speed as it leaves your hand. The moving ball can then hit something and push it, doing work on what it hits. The kinetic energy of a moving object is equal to the work required to bring it from rest to that speed, or the work the object can do while being brought to rest: net force × displacement = kinetic energy, i.e.,
Since the kinetic energy increases with the square of the speed, an object doubling its speed has four times as much kinetic energy. For example, a car traveling twice as fast as another requires four times as much distance to stop, assuming a constant braking force. As a consequence of this quadrupling, it takes four times the work to double the speed.
The kinetic energy of an object is related to its momentum by the equation:
where:
For the "translational kinetic energy," that is the kinetic energy associated with rectilinear motion, of a rigid body with constant mass formula_8, whose center of mass is moving in a straight line with speed formula_10, as seen above is equal to
where:
The kinetic energy of any entity depends on the reference frame in which it is measured. However the total energy of an isolated system, i.e. one in which energy can neither enter nor leave, does not change over time in the reference frame in which it is measured. Thus, the chemical energy converted to kinetic energy by a rocket engine is divided differently between the rocket ship and its exhaust stream depending upon the chosen reference frame. This is called the Oberth effect. But the total energy of the system, including kinetic energy, fuel chemical energy, heat, etc., is conserved over time, regardless of the choice of reference frame. Different observers moving with different reference frames would however disagree on the value of this conserved energy.
The kinetic energy of such systems depends on the choice of reference frame: the reference frame that gives the minimum value of that energy is the center of momentum frame, i.e. the reference frame in which the total momentum of the system is zero. This minimum kinetic energy contributes to the invariant mass of the system as a whole.
Derivation.
The work done in accelerating a particle during the infinitesimal time interval "dt" is given by the dot product of "force" and "displacement":
where we have assumed the relationship p = "m" v. (However, also see the special relativistic derivation below.)
Applying the product rule we see that:
Therefore (assuming constant mass so that "dm"=0), the following can be seen:
Since this is a total differential (that is, it only depends on the final state, not how the particle got there), we can integrate it and call the result kinetic energy. Assuming the object was at rest at time 0, we integrate from time 0 to time t because the work done by the force to bring the object from rest to velocity v is equal to the work necessary to do the reverse:
This equation states that the kinetic energy ("E"k) is equal to the integral of the dot product of the velocity (v) of a body and the infinitesimal change of the body's momentum (p). It is assumed that the body starts with no kinetic energy when it is at rest (motionless).
Rotating bodies.
If a rigid body Q is rotating about any line through the center of mass then it has "rotational kinetic energy" (formula_18) which is simply the sum of the kinetic energies of its moving parts, and is thus given by:
where:
(In this equation the moment of inertia must be taken about an axis through the center of mass and the rotation measured by ω must be around that axis; more general equations exist for systems where the object is subject to wobble due to its eccentric shape).
Kinetic energy of systems.
A system of bodies may have internal kinetic energy due to the relative motion of the bodies in the system. For example, in the Solar System the planets and planetoids are orbiting the Sun. In a tank of gas, the molecules are moving in all directions. The kinetic energy of the system is the sum of the kinetic energies of the bodies it contains.
A macroscopic body that is stationary (i.e. a reference frame has been chosen to correspond to the body's center of momentum) may have various kinds of internal energy at the molecular or atomic level, which may be regarded as kinetic energy, due to molecular translation, rotation, and vibration, electron translation and spin, and nuclear spin. These all contribute to the body's mass, as provided by the special theory of relativity. When discussing movements of a macroscopic body, the kinetic energy referred to is usually that of the macroscopic movement only. However all internal energies of all types contribute to body's mass, inertia, and total energy.
Frame of reference.
The speed, and thus the kinetic energy of a single object is frame-dependent (relative): it can take any non-negative value, by choosing a suitable inertial frame of reference. For example, a bullet passing an observer has kinetic energy in the reference frame of this observer. The same bullet is stationary from the point of view of an observer moving with the same velocity as the bullet, and so has zero kinetic energy. By contrast, the total kinetic energy of a system of objects cannot be reduced to zero by a suitable choice of the inertial reference frame, unless all the objects have the same velocity. In any other case the total kinetic energy has a non-zero minimum, as no inertial reference frame can be chosen in which all the objects are stationary. This minimum kinetic energy contributes to the system's invariant mass, which is independent of the reference frame.
The total kinetic energy of a system depends on the inertial frame of reference: it is the sum of the total kinetic energy in a center of momentum frame and the kinetic energy the total mass would have if it were concentrated in the center of mass.
This may be simply shown: let formula_22 be the relative velocity of the center of mass frame "i" in the frame "k".
Since formula_23,
However, let formula_25 the kinetic energy in the center of mass frame, formula_26 would be simply the total momentum that is by definition zero in the center of mass frame, and let the total mass: formula_27. Substituting, we get:
Thus the kinetic energy of a system is lowest with respect to center of momentum reference frames, i.e., frames of reference in which the center of mass is stationary (either the center of mass frame or any other center of momentum frame). In any other frame of reference there is additional kinetic energy corresponding to the total mass moving at the speed of the center of mass. The kinetic energy of the system in the center of momentum frame is a quantity that is invariant (all observers see it to be the same).
Rotation in systems.
It sometimes is convenient to split the total kinetic energy of a body into the sum of the body's center-of-mass translational kinetic energy and the energy of rotation around the center of mass (rotational energy):
where:
Thus the kinetic energy of a tennis ball in flight is the kinetic energy due to its rotation, plus the kinetic energy due to its translation.
Relativistic kinetic energy of rigid bodies.
If a body's speed is a significant fraction of the speed of light, it is necessary to use relativistic mechanics to calculate its kinetic energy. In special relativity theory, the expression for linear momentum is modified.
With "m" being an object's rest mass, v and "v" its velocity and speed, and "c" the speed of light in vacuum, we use the expression for linear momentum formula_30, where formula_31.
Integrating by parts yields
Since formula_33,
formula_35 is a constant of integration for the indefinite integral.
Simplifying the expression we obtain
formula_35 is found by observing that when formula_38 and formula_39, giving
resulting in the formula
This formula shows that the work expended accelerating an object from rest approaches infinity as the velocity approaches the speed of light. Thus it is impossible to accelerate an object across this boundary.
The mathematical by-product of this calculation is the mass-energy equivalence formula—the body at rest must have energy content
At a low speed (formula_3«formula_44), the relativistic kinetic energy is approximated well by the classical kinetic energy. This is done by binomial approximation or by taking the first two terms of the Taylor expansion for the reciprocal square root:
So, the total energy formula_46 can be partitioned into the rest mass energy plus the Newtonian kinetic energy at low speeds.
When objects move at a speed much slower than light (e.g. in everyday phenomena on Earth), the first two terms of the series predominate. The next term in the Taylor series approximation
is small for low speeds. For example, for a speed of the correction to the Newtonian kinetic energy is 0.0417 J/kg (on a Newtonian kinetic energy of 50 MJ/kg) and for a speed of 100 km/s it is 417 J/kg (on a Newtonian kinetic energy of 5 GJ/kg).
The relativistic relation between kinetic energy and momentum is given by
This can also be expanded as a Taylor series, the first term of which is the simple expression from Newtonian mechanics.
What this suggests is that the formulae for energy and momentum are not special and axiomatic, but rather concepts that emerge from the equivalence of mass and energy and the principles of relativity.
General relativity.
Using the convention that
where the four-velocity of a particle is
and formula_51 is the proper time of the particle, there is also an expression for the kinetic energy of the particle in general relativity.
If the particle has momentum
as it passes by an observer with four-velocity "u"obs, then the expression for total energy of the particle as observed (measured in a local inertial frame) is
and the kinetic energy can be expressed as the total energy minus the rest energy:
Consider the case of a metric that is diagonal and spatially isotropic ("g"tt,"g"ss,"g"ss,"g"ss). Since
where "v"α is the ordinary velocity measured w.r.t. the coordinate system, we get
Solving for "u"t gives
Thus for a stationary observer ("v"= 0)
and thus the kinetic energy takes the form
Factoring out the rest energy gives:
This expression reduces to the special relativistic case for the flat-space metric where
In the Newtonian approximation to general relativity
where Φ is the Newtonian gravitational potential. This means clocks run slower and measuring rods are shorter near massive bodies.
Kinetic energy in quantum mechanics.
In quantum mechanics, observables like kinetic energy are represented as operators. For one particle of mass "m", the kinetic energy operator appears as a term in the Hamiltonian and is defined in terms of the more fundamental momentum operator formula_65 as
Notice that this can be obtained by replacing formula_67 by formula_65 in the classical expression for kinetic energy in terms of momentum,
In the Schrödinger picture, formula_65 takes the form formula_71 where the derivative is taken with respect to position coordinates and hence
The expectation value of the electron kinetic energy, formula_73, for a system of "N" electrons described by the wavefunction formula_74 is a sum of 1-electron operator expectation values:
where formula_76 is the mass of the electron and formula_77 is the Laplacian operator acting upon the coordinates of the "i"th electron and the summation runs over all electrons.
The density functional formalism of quantum mechanics requires knowledge of the electron density "only", i.e., it formally does not require knowledge of the wavefunction. Given an electron density formula_78, the exact N-electron kinetic energy functional is unknown; however, for the specific case of a 1-electron system, the kinetic energy can be written as
where formula_80 is known as the von Weizsäcker kinetic energy functional.
References.
 

</doc>
<doc id="209627" url="https://en.wikipedia.org/wiki?curid=209627" title="Klein–Gordon equation">
Klein–Gordon equation

The Klein–Gordon equation (Klein–Fock–Gordon equation or sometimes Klein–Gordon–Fock equation) is a relativistic version of the Schrödinger equation.
Its solutions include a quantum scalar or pseudoscalar field, a field whose quanta are spinless particles. It cannot be straightforwardly interpreted as a Schrödinger equation for a quantum state, because it is second order in time and because it does not admit a positive definite conserved probability density. Still, with the appropriate interpretation, it does describe the quantum amplitude for finding a point particle in various places, the relativistic wavefunction, but the particle propagates both forwards and backwards in time. Any solution to the Dirac equation is automatically a solution to the Klein–Gordon equation, but the converse is not true.
Statement.
The Klein–Gordon equation is
This is often abbreviated as
where and is the d'Alembert operator, defined by
The Klein-Gordon equation is most often written in natural units:
The form is determined by requiring that plane wave solutions of the equation:
obey the energy momentum relation of special relativity:
Unlike the Schrödinger equation, the Klein–Gordon equation admits two values of for each , one positive and one negative. Only by separating out the positive and negative frequency parts does one obtain an equation describing a relativistic wavefunction. For the time-independent case, the Klein–Gordon equation becomes
which is the homogeneous screened Poisson equation.
History.
The equation was named after the physicists Oskar Klein and Walter Gordon, who in 1926 proposed that it describes relativistic electrons. Other authors making similar claims in that same year were Vladimir Fock, Johann Kudar, Théophile de Donder and Frans-H. van den Dungen, and Louis de Broglie. Although it turned out that the Dirac equation describes the spinning electron, the Klein–Gordon equation correctly describes the spinless pion, a composite particle. On July 4, 2012 CERN announced the discovery of the Higgs boson. Since the Higgs boson is a spin-zero particle, it is the first elementary particle that is described by the Klein-Gordon equation. Further experimentation and analysis is required to discern whether the Higgs boson found is that of the Standard Model, or a more exotic form.
The Klein–Gordon equation was first considered as a quantum wave equation by Schrödinger in his search for an equation describing de Broglie waves. The equation is found in his notebooks from late 1925, and he appears to have prepared a manuscript applying it to the hydrogen atom. Yet, because it fails to take into account the electron's spin, the equation predicts the hydrogen atom's fine structure incorrectly, including overestimating the overall magnitude of the splitting pattern by a factor of for the -th energy level. The Dirac result is, however, easily recovered if the orbital momentum quantum number is replaced by total angular momentum quantum number . In January 1926, Schrödinger submitted for publication instead "his" equation, a non-relativistic approximation that predicts the Bohr energy levels of hydrogen without fine structure.
In 1926, soon after the Schrödinger equation was introduced, Vladimir Fock wrote an article about its generalization for the case of magnetic fields, where forces were dependent on velocity, and independently derived this equation. Both Klein and Fock used Kaluza and Klein's method. Fock also determined the gauge theory for the wave equation. The Klein–Gordon equation for a free particle has a simple plane wave solution.
Derivation.
The non-relativistic equation for the energy of a free particle is
By quantizing this, we get the non-relativistic Schrödinger equation for a free particle,
where 
is the momentum operator ( being the del operator), and 
is the energy operator.
The Schrödinger equation suffers from not being relativistically covariant, meaning it does not take into account Einstein's special relativity.
It is natural to try to use the identity from special relativity describing the energy:
Then, just inserting the quantum mechanical operators for momentum and energy yields the equation
This, however, is a cumbersome expression to work with because the differential operator cannot be evaluated while under the square root sign. In addition, this equation, as it stands, is nonlocal (see also Introduction to nonlocal equations).
Klein and Gordon instead began with the square of the above identity, i.e.
which, when quantized, gives
which simplifies to
Rearranging terms yields
Since all reference to imaginary numbers has been eliminated from this equation, it can be applied to fields that are real valued as well as those that have complex values.
Using the inverse of the Minkowski metric , we get
in covariant notation. This is often abbreviated as
where
and
This operator is called the d'Alembert operator. 
Today this form is interpreted as the relativistic field equation for spin-0 particles. Furthermore, any solution to the Dirac equation (for a spin-one-half particle) is automatically a solution to the Klein–Gordon equation, though not all solutions of the Klein–Gordon equation are solutions of the Dirac equation. It is noteworthy that the Klein–Gordon equation is very similar to the Proca equation.
Klein–Gordon equation in a potential.
The Klein–Gordon equation can be generalized to describe a field in some potential as:
Conserved current.
The conserved current associated to the U(1) symmetry of a complex field formula_23 satisfying the Klein Gordon equation reads
The form of the conserved current can be derived systematically by applying Noether's theorem to the U(1) symmetry. We will not do so here, but simply give a proof that this conserved current is correct.
Proof using algebraic manipulations from the KG equation
From the Klein Gordon equation for a complex field formula_25 of mass formula_26 written in covariant notation
and its complex conjugate
we have, multiplying by the left respectively by formula_29 and formula_25 (and omitting for brevity the explicit formula_31 dependence),
Subtracting the former from the latter we obtain
from which we obtain the conservation law for the Klein Gordon field:
Relativistic free particle solution.
The Klein–Gordon equation for a free particle can be written as
We look for plane wave solutions of the form
for some constant angular frequency and wave number . Substitution gives the "dispersion relation":
Energy and momentum are seen to be proportional to and :
So the dispersion relation is just the classic relativistic equation:
For massless particles, we may set , recovering the relationship between energy and momentum for massless particles:
Action.
The Klein–Gordon equation can also be derived via a variational method by considering the action:
where is the Klein–Gordon field and is its mass. The complex conjugate of is written . If the scalar field is taken to be real-valued, then .
Applying the formula for the Hilbert stress–energy tensor to the Lagrangian density (the quantity inside the integral), we can derive the stress–energy tensor of the scalar field. It is
Electromagnetic interaction.
There is a simple way to make any field interact with electromagnetism in a gauge invariant way: replace the derivative operators with the gauge covariant derivative operators. The Klein Gordon equation becomes:
in natural units, where is the vector potential. While it is possible to add many higher order terms, for example,
these terms are not renormalizable in 3+1 dimensions.
The field equation for a charged scalar field multiplies by , which means the field must be complex. In order for a field to be charged, it must have two components that can rotate into each other, the real and imaginary parts.
The action for a charged scalar is the covariant version of the uncharged action:
Gravitational interaction.
In general relativity, we include the effect of gravity and the Klein–Gordon equation becomes (in the mostly pluses signature)
or equivalently
where "g" is the inverse of the metric tensor that is the gravitational potential field, "g" is the determinant of the metric tensor, is the covariant derivative and is the Christoffel symbol that is the gravitational force field.

</doc>
<doc id="198566" url="https://en.wikipedia.org/wiki?curid=198566" title="Lagrangian">
Lagrangian

Lagrangian may refer to:

</doc>
<doc id="23371726" url="https://en.wikipedia.org/wiki?curid=23371726" title="Lagrangian mechanics">
Lagrangian mechanics

Lagrangian mechanics is a reformulation of classical mechanics, introduced by the Italian-French mathematician and astronomer Joseph-Louis Lagrange in 1788. 
In Lagrangian mechanics, the trajectory of a system of particles is derived by solving the Lagrange equations in one of two forms, either the "Lagrange equations of the first kind", which treat constraints explicitly as extra equations, often using Lagrange multipliers; or the "Lagrange equations of the second kind", which incorporate the constraints directly by judicious choice of generalized coordinates. In each case, a mathematical function called the Lagrangian is a function of the generalized coordinates, their time derivatives, and time, and contains the information about the dynamics of the system.
No new physics is introduced by Lagrangian mechanics; it is actually less general than Newtonian mechanics. Newton's laws can include non-conservative forces like friction, however they must include constraint forces explicitly and are best suited to Cartesian coordinates. Lagrangian mechanics is ideal for systems with conservative forces and for bypassing constraint forces, and some (not all) non-conservative forces, in any coordinate system. Generalized coordinates can be chosen by convenience, to exploit symmetries in the system or the geometry of the constraints, which considerably simplifies describing the dynamics of the system. Lagrangian mechanics also reveals conserved quantities and their symmetries in a direct way, although only as a special case of Noether's theorem. The theory connects with the principle of stationary action, although Lagrangian mechanics is less general because it is restricted to equilibrium problems. Also, Lagrangian mechanics can only be applied to systems with holonomic constraints, because the formulation does not work for nonholonomic constraints. Three examples are when the constraint equations are nonintegrable, when the constraints have inequalities, or with complicated non-conservative forces like friction. Nonholonomic constraints require special treatment, and one may revert to Newtonian mechanics, or use other methods.
The Lagrangian formulation of mechanics is important not just for its broad applications, but also for its role in advancing deep understanding of physics. Although Lagrange only sought to describe classical mechanics, "Hamilton's principle" that can be used to derive the Lagrange equation was later recognized to be applicable to much of theoretical physics as well. In quantum mechanics, action and quantum-mechanical phase are related via Planck's constant, and the principle of stationary action can be understood in terms of constructive interference of wave functions. If the Lagrangian is invariant under a symmetry, then the resulting equations of motion are also invariant under that symmetry. This characteristic is very helpful in showing that theories are consistent with either special relativity or general relativity. The action principle, and the Lagrangian formalism, are tied closely to Noether's theorem, which connects physical conserved quantities to continuous symmetries of a physical system. Lagrangian mechanics and Noether's theorem together yield a natural formalism for first quantization by including commutators between certain terms of the Lagrangian equations of motion for a physical system.
Lagrangian mechanics is widely used to solve mechanical problems in physics and engineering when Newton's formulation of classical mechanics is not convenient. Lagrange's equations are also used in optimisation problems of dynamic systems. In mechanics, Lagrange's equations of the second kind are used much more than those of the first kind.
Introduction.
The strength of Lagrangian mechanics is its ability to handle constrained mechanical systems. The following examples motivate the need for the concepts and terminology used to handle such systems.
For a bead sliding on a frictionless wire subject only to gravity in 2d space, the constraint on the bead can be stated in the form "f"(r) = 0, where the position of the bead can be written r = ("x"("s"), "y"("s")), in which "s" is a parameter, the arc length "s" along the curve from some point on the wire. Only "one" coordinate is needed instead of two, because the position of the bead can be parameterized by one number, "s", and the constraint equation connects the two coordinates "x" and "y"; either one is determined from the other. The constraint force is the reaction force the wire exerts on the bead to keep it on the wire, and the non-constraint applied force is gravity acting on the bead.
Suppose the wire changes its shape with time, by flexing. Then the constraint equation and position of the particle are respectively
which now both depend on time "t" due to the changing coordinates as the wire changes its shape. Notice time appears implicitly via the coordinates "and" explicitly in the constraint equations.
Another interesting 2d example is the chaotic double pendulum, again subject to gravity. The length of one pendulum is "L"1 and the length of the other is "L"2. Each pendulum bob has a constraint equation, 
where the positions of the bobs are
and "θ"1 is the angle of pendulum 1 from some reference direction, likewise for pendulum 2. Each pendulum can be described by one coordinate since the constraint equation for each connects the two spatial coordinates.
For a 3d example, a spherical pendulum with constant length "l" free to swing in any angular direction subject to gravity, the constraint on the pendulum bob can be stated in the form 
where the position of the pendulum bob can be written 
in which ("θ", "φ") are the spherical polar angles because the bob moves in the surface of a sphere. A logical choice of variables to describe the motion are the angles ("θ", "φ"). Notice only two coordinates are needed instead of three, because the position of the bob can be parameterized by two numbers, and the constraint equation connects the three coordinates "x", "y", "z" so any one of them is determined from the other two.
For analyzing the small oscillations of multiple coupled simple harmonic oscillators, Lagrangian mechanics is especially natural, since the kinetic and potential energies of the system take a simple form despite the fact there are many particles, and the equations of motion can be derived immediately.
For "N" particles in 3d space, the position vector of each particle can written as a 3-tuple in Cartesian coordinates
so overall, there are 3"N" coordinates to define the configuration of the system. These are all specific points in space to locate the particles, a general point in space is written r = ("x", "y", "z"). If any or all of the particles are subject to a holonomic constraint, described by a constraint equation of the form "f"(r, "t") = 0, then at any instant of time the position coordinates of those particles are linked together and not independent. If there are "C" constraints in the system, then each has a constraint equation,
and one coordinate can be eliminated from each constraint equation. The number of independent coordinates is therefore "n" = 3"N" − "C". We can transform each position vector to a common set of "n" generalized coordinates, conveniently written as an "n"-tuple q = ("q"1, "q"2, ... "qn"), by expressing each position vector, and hence the position coordinates, as functions of the generalized coordinates and time,
The time derivatives of the generalized coordinates are called the generalized velocities, and for each particle the transformation of its velocity vector, the total derivative of its position with respect to time, is
(each overdot indicates a time derivative).
In the previous examples, if one tracks each of the massive objects as a particle (bead, pendulum bob, etc.), calculation of the motion of the particle using Newtonian mechanics would require solving for the time-varying constraint force required to keep the particle in the constrained motion (reaction force exerted by the wire on the bead, or tension in the pendulum rods). For the same problem using Lagrangian mechanics, one looks at the path the particle can take and chooses a convenient set of "independent" generalized coordinates that completely characterize the possible motion of the particle. This choice eliminates the need for the constraint force to enter into the resultant system of equations. There are fewer equations since one is not directly calculating the influence of the constraint on the particle at a given moment.
Definition of the Lagrangian.
The central quantity of Lagrangian mechanics is the Lagrangian, a function which summarizes the dynamics of the entire system. Overall, the Lagrangian has no single expression for all physical systems. Any function which generates the correct equations of motion from the Euler–Lagrange equations, in agreement with physical laws, can be taken as a Lagrangian. However, it is possible to construct general expressions for large classes of applications.
The non-relativistic Lagrangian for a system of particles can be defined by
where "T" is the total kinetic energy of the system, and "V" the potential energy of the system. The dimensions of the Lagrangian are the same as energy.
The total kinetic energy of a system of "N" particles with masses "m"1, "m"2, ..., "mN" is the sum (indicated by Σ) of the kinetic energies of the particles, the energy of the system's motion,
where · is the dot product. The kinetic energy is a function only of the velocities v"k", not the positions r"k" nor time "t", so "T" = "T"(v1, v2, ...). By contrast, the above expression for velocity shows the kinetic energy "in generalized coordinates" depends on the generalized velocities, coordinates, and time if the position vectors depend explicitly on time due to time-varying constraints, so "T" = "T"(q, "d"q/"dt", "t").
The potential energy "V" of the system reflects the energy of interaction between the particles, i.e. how much energy any one particle will have due to all the others and other external influences. For conservative forces, it is a function of the position vectors of the particles only, so "V" = "V"(r1, r2, ...). For those non-conservative forces which can be derived from an appropriate potential, the velocities will appear also, "V" = "V"(r1, r2, ..., v1, v2, ...). If there is some external field changing with time, or external driving force, the potential will change with time, so most generally "V" = "V"(r1, r2, ..., v1, v2, ..., "t").
If the potential or kinetic energy, or both, depend explicitly on time due to time-varying constraints or external influences, the Lagrangian "L"(q, "dq/"dt", "t") is "explicitly time-dependent". If neither the potential nor the kinetic energy depend on time, then the Lagrangian "L"(q, "dq/"dt") is "explicitly independent of time". In either case, the Lagrangian will always have implicit time-dependence through the generalized coordinates.
In addition to the Lagrangian, it is sometimes possible to introduce another function to account for dissipative forces. 
In relativistic mechanics, "L" needs more subtle attention, because it is not the difference between kinetic and potential energy, see relativistic Lagrangian mechanics.
Equations of motion.
In Newtonian mechanics, the equations are given by Newton's laws. The second law F = "m"a applies to each particle, so for an "N" particle system in 3d, there are 3"N" second order ordinary differential equations in the positions of the particles to solve for.
In Lagrangian mechanics, the Lagrangian function generates the equations of motion. Although the equations of motion include partial derivatives denoted by , they are still ordinary differential equations in the position coordinates of the particles. The total time derivative denoted "d"/"dt" may involve implicit differentiation.
There are two kinds, one which accounts for constraint forces by including the constraint equations, and another which excludes constraint forces and only accounts for non-constraint forces in terms of generalized coordinates. By substituting in the Lagrangian and solving the equations, subject to the initial values of the positions and velocities, will give the positions of the particles as functions of time, and one can see how the system evolves.
Lagrange's equations of the first kind.
If a system of "N" particles in 3d is subject to "C" holonomic constraints, given by the equations "f"1, "f"2..., "fC", and the dynamics given by a Lagrangian "L"(r, "d"r/"dt", "t"), Lagrange's equations of the first kind are
_k} + \sum_{i=1}^C \lambda_i\frac{\partial f_i}{\partial \mathbf{r}_k }=0 </math>
where "k" = 1, 2, ..., "N" labels the particles, there is a Lagrange multiplier "λi" for each constraint equation "fi", and
are each shorthands for a vector of derivatives with respect to the indicated variables (not a derivative with respect to the entire vector).
This procedure does increase the number of equations to solve compared to Newton's laws, from 3"N" to 3"N" + "C", because there are 3"N" coupled second order differential equations in the position coordinates and multipliers, plus "C" constraint equations. However, when solved alongside the position coordinates of the particles, the multipliers can yield information about the constraint forces. The coordinates do not need to be eliminated by solving the constraint equations.
Lagrange's equations of the second kind.
The Euler–Lagrange equations, or Lagrange's equations of the second kind
are mathematical results from the calculus of variations, which can also be used in mechanics. Substituting in the Lagrangian "L"(q, "d"q/"dt", "t"), gives the equations of motion of the system.
The number of equations has decreased compared to Newtonian mechanics, from 3"N" to "n" = 3"N" − "C" coupled second order differential equations in the generalized coordinates. These equations do not include constraint forces at all, only non-constraint forces need to be accounted for.
From Newtonian to Lagrangian mechanics.
Newton's laws.
For simplicity, Newton's laws can be illustrated for one particle without much loss of generality (for a system of "N" particles, all of these equations apply to each particle in the system). The equation of motion for particle of mass "m" is Newton's second law of 1687, in modern vector notation
where a is its acceleration and F the resultant force acting "on" it. In three spatial dimensions, this is a system of three coupled second order ordinary differential equations to solve, since there are three components in this vector equation. The solutions are the position vectors r of the particles at time "t", subject to the initial conditions of r and v when "t" = 0.
Newton's laws are easy to use in Cartesian coordinates, but Cartesian coordinates are not always convenient, and for other coordinate systems the equations of motion can become complicated. In a set of curvilinear coordinates ξ = ("ξ"1, "ξ"2, "ξ"3), the law in tensor index notation is the "Lagrangian form"
where "Fa" is the "a"th contravariant components of the resultant force acting on the particle, Γ"abc" are the Christoffel symbols of the second kind,
is the kinetic energy of the particle, and "gbc" the covariant components of the "metric tensor" of the curvilinear coordinate system. All the indices "a", "b", "c", each take the values 1, 2, 3. Curvilinear coordinates are not the same as generalized coordinates.
It may seem like an overcomplication to cast Newton's law in this form, but there are advantages. The acceleration components in terms of the Christoffel symbols can be avoided by evaluating derivatives of the kinetic energy instead, a scalar invariant which takes the same value in all frames of reference. If there is no resultant force acting on the particle, F = 0, it does not accelerate, but moves with constant velocity in a straight line. Mathematically, the solutions of the differential equation are "geodesics", the curves of shortest length between two points in space. In flat 3d real space the geodesics are simply straight lines. So for a free particle, Newton's second law coincides with the geodesic equation, and states free particles follow geodesics, the shortest trajectories it can move along. If the particle is subject to forces, F ≠ 0, the particle accelerates due to forces acting on it, and deviates away from the geodesics it would follow if free. 
The idea of finding the shortest path a particle can follow motivated the first applications of the calculus of variations to mechanical problems, such as the Brachistochrone problem solved by Jean Bernoulli in 1696, as well as Leibniz, Daniel Bernoulli, L'Hôpital around the same time, and Newton the following year. Newton himself was thinking along the lines of the variational calculus, but did not publish. These ideas in turn lead to the variational principles of mechanics, of Fermat, Maupertuis, Euler, Hamilton, and others. With appropriate extensions of the quantities given here in flat 3d space to 4d curved spacetime, the above form of Newton's law also carries over to Einstein's general relativity, in which case free particles follow geodesics in curved spacetime that are no longer "straight lines" in the ordinary sense. 
However, we still need to know the total resultant force F acting on the particle, which in turn requires the resultant non-constraint force N plus the resultant constraint force C,
The constraint forces can be complicated, since they will generally depend on time. Also, if there are constraints, the curvilinear coordinates are not independent but related by one or more constraint equations. 
The constraint forces can either be eliminated from the equations of motion so only the non-constraint forces remain, or included by including the constraint equations in the equations of motion.
D'Alembert's principle.
A fundamental result in analytical mechanics is D'Alembert's principle, introduced in 1708 by Jacques Bernoulli to understand static equilibrium, and developed by D'Alembert in 1743 to solve dynamical problems. The principle asserts for "N" particles
The δr"k" are "virtual displacements", by definition they are infinitesimal changes in the configuration of the system consistent with the constraint forces acting on the system "at an instant of time", i.e. in such a way that the constraint forces maintain the constrained motion. They are not the same as the actual displacements in the system, which are caused by the resultant constraint and non-constraint forces acting on the particle to accelerate and move it. Virtual work is the work done along a virtual displacement for any force (constraint or non-constraint).
Since the constraint forces act perpendicular to the motion of each particle in the system to maintain the constraints, the total virtual work by the constraint forces acting on the system is zero;
so that
Thus D'Alembert's principle allows us to concentrate on only the applied non-constraint forces, and exclude the constraint forces in the equations of motion. The form shown is also independent of the choice of coordinates. However, it is not readily usable to set up the equations of motion and solve for the motion of the system; therefore equations of motion in a set of independent coordinates are sought for.
Equations of motion from D'Alembert's principle.
If there are constraints on particle "k", then since the coordinates of the position r"k" = ("xk", "yk", "zk") are linked together by a constraint equation, so are the those of the virtual displacements "δr"k" = ("δxk", "δyk", "δzk"). Since the generalized coordinates are independent, we can avoid the complications with the "δr"k" by converting to virtual displacements in the generalized coordinates. These are related in the same form as a total differential,
There is no partial time derivative with respect to time multiplied by a time increment, since this is a virtual displacement, one along the constraints in an "instant" of time.
The first term in D'Alembert's principle above is the virtual work done by the non-constraint forces N"k" along the virtual displacements "δ"r"k", and can without loss of generality be converted into the generalized analogues by the definition of generalized forces
so that
This is half of the conversion to generalized coordinates. It remains to convert the acceleration term into generalized coordinates, which is not immediately obvious. Recalling the Lagrange form of Newton's second law, the partial derivatives of the kinetic energy with respect to the generalized coordinates and velocities can be found to give the desired result;
Now D'Alembert's principle is in the generalized coordinates as required,
and since these virtual displacements "δqj" are independent and nonzero, the coefficients can be equated to zero, resulting in Lagrange's equations or the generalized equations of motion,
These equations are equivalent to Newton's laws "for the non-constraint forces". The generalized forces in this equation are derived from the non-constraint forces only - the constraint forces they have been excluded from D'Alembert's principle and do not need to be found. The generalized forces may be non-conservative, provided they satisfy D'Alembert's principle.
Euler–Lagrange equations and Hamilton's principle.
For a non-conservative force which depends on velocity, it "may" be possible to find a potential energy function "V" that depends on positions and velocities. If the generalized forces "Qi" can be derived from a potential "V" such that
equating to Lagrange's equations and defining the Lagrangian as "L" = "T" − "V" obtains Lagrange's equations of the second kind or the Euler–Lagrange equations of motion
However, the Euler–Lagrange equations can only account for non-conservative forces "if" a potential can be found as shown. Lagrange's equations do not involve any potential, only forces; therefore they are more general than the Euler–Lagrange equations. 
These equations also follow from the calculus of variations. The "variation" of the Lagrangian is
which has a similar form to the total differential of "L", but the virtual displacements and their time derivatives replace differentials, and there is no time increment in accordance with the definition of the virtual displacements. An integration by parts with respect to time can transfer the time derivative of "δqj" to the ∂"L"/∂("dqj"/"dt"), in the process exchanging "d"("δqj")/"dt" for "δqj", allowing the independent virtual displacements to be factorized from the derivatives of the Lagrangian,
Now, if the condition "δqj"("t"1) = "δqj"("t"2) = 0 holds for all "j", the terms not integrated are zero. If in addition the entire time integral of "δL" is zero, then because the "δqj" are independent, and the only way for a definite integral to be zero is if the integrand equals zero, each of the coefficients of "δqj" must also be zero. Then we obtain the equations of motion. This can be summarized by Hamilton's principle;
The time integral of the Lagrangian is another quantity called the action, defined as
which is a "functional"; it takes in the Lagrangian function for all times between "t"1 and "t"2 and returns a scalar value. Its dimensions are the same as [ angular momentum ], [energy]·[time], or [length]·[momentum]. With this definition Hamilton's principle is
Thus, instead of thinking about particles accelerating in response to applied forces, one might think of them picking out the path with a stationary action, with the end points of the path in configuration space held fixed at the initial and final times. Hamilton's principle is sometimes referred to as the "principle of least action", however the action functional need only be "stationary", not necessarily a maximum or a minimum value. Any variation of the functional gives an increase in the functional integral of the action. It is not widely stated that Hamilton's principle is a variational principle only with holonomic constraints, if we are dealing with nonholonomic systems then the variational principle should be replaced with one involving d'Alembert principle of virtual work.
Lagrange multipliers and constraints.
We can vary "L" in the Cartesian r"k" coordinates, for "N" particles,
Hamilton's principle is still valid even if the coordinates "L" is expressed in are not independent, here r"k", but the constraints are still assumed to be holonomic. What cannot be done is to simply equate the coefficients of δr"k" to zero because the δr"k" are not independent. Instead, the method of Lagrange multipliers can be used to include the constraints. Multiplying each constraint equation "fi"(r"k", "t") = 0 by a Lagrange multiplier "λi" for "i" = 1, 2, ..., "C", and adding the results to the original Lagrangian, gives the new Lagrangian
The Lagrange multipliers are arbitrary functions of time "t", but not functions of the coordinates r"k", so the multipliers are on equal footing with the position coordinates. With this new Lagrangian the Euler-Lagrange equations recover the constraint equations 
and also obtain the equations of motion in terms of the r"k", which are Lagrange's equations of the first kind
These equations also follow by varying the new Lagrangian and integrating over time;
where as always the end points are fixed, "δr"k"("t"1) = "δr"k"("t"2) = 0 for all "k". The multipliers can be found so that the coefficients of "δ"r"k" are zero, even though the r"k" are not independent. The equations of motion follow.
For the case of a conservative force given by the gradient of some potential energy "V", a function of the rk coordinates only, substituting the Lagrangian "L" = "T" − "V" gives
From the Lagrangian form of Newton's second law above, the derivatives of kinetic energy form the (negative of) resultant force acting on the particle "k", and the derivatives of the potential form the non-constraint force acting on the particle "k",
so it follows the constraint forces are given by
which relates the constraint equations to the constraint forces via the Lagrange multipliers.
Properties of the Euler–Lagrange equation.
In some cases, the Lagrangian has properties which can provide information about the system without solving the equations of motion. These follow from Lagrange's equations of the second kind.
Non uniqueness.
The Lagrangian of a given system is not unique. A Lagrangian "L" can be multiplied by a nonzero constant "a", an arbitrary constant "b" can be added, and the new Lagrangian "aL" + "b" will describe exactly the same motion as "L". A less obvious result is that two Lagrangians describing the same system can differ by the total derivative (not partial) of some function "f"(q, "t") with respect to time;
Each Lagrangian will obtain exactly the same equations of motion.
Invariance under point transformations.
Given a set of generalized coordinates q, if we change these variables to a new set of generalized coordinates s according to a point transformation q = q(s, "t"), the new Lagrangian "L"′ is a function of the new coordinates
and by the chain rule for partial differentiation, Lagrange's equations are invariant under this transformation;
This may simplify the equations of motion. The procedure is analogous to Canonical transformations in Hamiltonian mechanics, which preserves the form of Hamiltonian equations.
Cyclic coordinates and conservation laws.
An important property of the Lagrangian is that conserved quantities can easily be read off from it. The "generalized momentum" "canonically conjugate to" the coordinate "qi" is defined by
If the Lagrangian "L" does "not" depend on some coordinate "qi", then it follows from the Euler–Lagrange equations that the corresponding generalized momentum will be a "conserved" quantity, because its time derivative is zero so the momentum must be a constant of the motion;
This is a special case of Noether's theorem. Such coordinates are called "cyclic" or "ignorable".
For example, a system may have a Lagrangian
where "r" and "z" are lengths along straight lines, "s" is an arc length along some curve, and "θ" and "φ" are angles. Notice "z", "s", and "φ" are all absent in the Lagrangian even though their velocities are not. Then the momenta
are all conserved quantities. The units and nature of each generalized momentum will depend on the corresponding coordinate; in this case "pz" is a translational momentum in the "z" direction, "ps" is also a translational momentum along the curve "s" is measured, and "pφ" is an angular momentum in the plane the angle "φ" is measured in. However complicated the motion of the system is, all the coordinates and velocities will vary in such a way that these momenta are conserved.
Energy conservation.
Taking the total derivative of the Lagrangian "L" = "T" − "V" with respect to time leads to the general result
If the entire Lagrangian is explicitly independent of time, it follows the partial time derivative of the Lagrangian is zero, , so the quantity under the total time derivative in brackets
must be a constant for all times during the motion of the system, and it also follows the kinetic energy is a homogenous function of degree 2 in the generalized velocities. If in addition the potential "V" is only a function of coordinates and independent of velocities, it follows by direct calculation, or use of Euler's theorem for homogenous functions, that
Under all these circumstances, the constant
is the "total conserved energy" of the system. The kinetic and potential energies still change as the system evolves, but the motion of the system will be such that their sum, the total energy, is constant. This is a valuable simplification, since the energy "E" is a constant of integration that counts as an arbitrary constant for the problem, and it may be possible to integrate the velocities from this energy relation to solve for the coordinates. In the case the velocity or kinetic energy or both depends on time, then the energy is "not" conserved.
The Hamiltonian is related to the Lagrangian by a Legendre transformation. By definition it is the above quantity in brackets,
Under the same conditions, the Hamiltonian equals the total energy of the system and is conserved.
Mechanical similarity.
If the potential energy is a homogeneous function of the coordinates and independent of time, and all position vectors are scaled by the same nonzero constant "α", r"k"′ = "α"r"k", so that
and time is scaled by a factor "β", "t"′ = "βt", then the velocities v"k" are scaled by a factor of "α"/"β" and the kinetic energy "T" by ("α"/"β")2. The entire Lagrangian has been scaled by the same factor if
Since the lengths and times have been scaled, the trajectories of the particles in the system follow geometrically similar paths differing in size. The length "l" traversed in time "t" in the original trajectory corresponds to a new length "l′" traversed in time "t′" in the new trajectory, given by the ratios
Interacting particles.
For a given system, if two subsystems "A" and "B" are non-interacting, the Lagrangian "L" of the overall system is the sum of the Lagrangians "LA" and "LB" for the subsystems:
If they do interact this is not possible. In some situations, it may be possible to separate the Lagrangian of the system "L" into the sum of non-interacting Lagrangians, plus another Lagrangian "LAB" containing information about the interaction,
This may be physically motivated by taking the non-interacting Lagrangians to be kinetic energies only, while the interaction Lagrangian is the system's total potential energy. Also, in the limiting case of negligible interaction, "LAB" tends to zero reducing to the non-interacting case above. 
The extension to more than two non-interacting subsystems is straightforwards - the overall Lagrangian is the sum of the separate Lagrangians for each subsystem. If there are interactions, then interaction Lagranians may be added.
Examples in specific coordinate systems.
In the following examples, a particle of mass "m" moves under the influence of a conservative force derived from the gradient ∇ of the a scalar potential,
If there are more particles, in accordance with the above results, the total kinetic energy is a sum over all the particle kinetic energies, and the potential is a function of all the coordinates.
Cartesian coordinates.
The Lagrangian of the particle can be written
The equations of motion for the particle are found by applying the Euler–Lagrange equation, for the "x" coordinate
and similarly for the "y" and "z" coordinates. For the "x" coordinate
hence
and similarly for the "y" and "z" coordinates. Collecting the equations in vector form we find
which is Newton's second law of motion for a particle subject to a conservative force.
Polar coordinates in 2d and 3d.
The Lagrangian for the above problem in spherical coordinates is
so the Euler–Lagrange equations are
The "φ" coordinate is cyclic since it does not appear in the Lagrangian, so the conserved momentum in the system is the angular momentum
in which "r", "θ" and "dφ/dt" can all vary with time, but only in such a way that "pφ" is constant.
Non-relativistic examples.
The following examples apply Lagrange's equations of the second kind to mechanical problems.
Pendulum on a movable support.
Consider a pendulum of mass "m" and length "ℓ", which is attached to a support with mass "M", which can move along a line in the "x"-direction. Let "x" be the coordinate along the line of the support, and let us denote the position of the pendulum by the angle "θ" from the vertical.
The kinetic energy can then be shown to be
and the potential energy of the system is
The Lagrangian is therefore
Since "x" is absent from the Lagrangian, it is a cyclic coordinate. The conserved momentum is
The Lagrange equation for the support coordinate "x" is therefore
or
The Lagrange equation for the angle "θ" is
therefore
These equations may look quite complicated, but finding them with Newton's laws would have required carefully identifying all forces, which would have been much more laborious and prone to errors. By considering limit cases, the correctness of this system can be verified: For example, formula_77 should give the equations of motion for a pendulum that is at rest in some inertial frame, while formula_78 should give the equations for a pendulum in a constantly accelerating system, etc. Furthermore, it is trivial to obtain the results numerically, given suitable starting conditions and a chosen time step, by stepping through the results iteratively.
Two-body central force problem.
The basic problem is that of two bodies of masses "m"1 and "m"2 with position vectors r1 and r2 are in orbit about each other due to an attractive central force "V". We may naïvely write down the Lagrangian in terms of the position coordinates as they are, but it is an established procedure to convert the two-body problem into a one-body problem as follows. Introduce the Jacobi coordinates; the separation of the bodies r = r2 − r1 and the location of the center of mass R = ("m"1r1 + "m"2r2)/("m"1 + "m"2). The Lagrangian is then
where "M" = "m"1 + "m"2 is the total mass, "μ" = "m"1"m"2/("m"1 + "m"2) is the reduced mass, and "V" the potential of the radial force, which depends only on the magnitude of the separation |r| = |r2 − r1|. The Lagrangian is divided into a "center-of-mass" term "L"cm and a "relative motion" term "L"rel. The Euler–Lagrange equation for R is simply
which states the center of mass moves in a straight line at constant velocity. The since the relative motion only depends on the magnitude of the separation, it is ideal to use polar coordinates ("r", "θ") and take "r" = |r|,
which does not depend upon "θ", therefore "θ" is an ignorable coordinate. The conserved momentum corresponding to "θ" is
which will be abbreviated "ℓ". The radial coordinate "r" and angular velocity "dθ"/"dt" can vary with time, but only in such a way that "ℓ" is constant. The Lagrange equation for "r" is
This equation is identical to the radial equation obtained using Newton's laws in a "co-rotating" reference frame, that is, a frame rotating with the reduced mass so it appears stationary. Eliminating the angular velocity "dθ"/"dt" from this radial equation,
which is the equation of motion for a one-dimensional problem in which a particle of mass "μ" is subjected to the inward central force −"dU"/"dr" and a second outward force, called in this context the centrifugal force
Of course, if one remains entirely within the one-dimensional formulation, "ℓ" enters only as some imposed parameter of the external outward force, and its interpretation as angular momentum depends upon the more general two-dimensional problem from which the one-dimensional problem originated.
If one arrives at this equation using Newtonian mechanics in a co-rotating frame, the interpretation is evident as the centrifugal force in that frame due to the rotation of the frame itself. If one arrives at this equation directly by using the generalized coordinates ("r", "θ") and simply following the Lagrangian formulation without thinking about frames at all, the interpretation is that the centrifugal force is an outgrowth of "using polar coordinates". As Hildebrand says:
"Since such quantities are not true physical forces, they are often called "inertia forces". Their presence or absence depends, not upon the particular problem at hand, but "upon the coordinate system chosen"." In particular, if Cartesian coordinates are chosen, the centrifugal force disappears, and the formulation involves only the central force itself, which provides the centripetal force for a curved motion.
This viewpoint, that fictitious forces originate in the choice of coordinates, often is expressed by users of the Lagrangian method. This view arises naturally in the Lagrangian approach, because the frame of reference is (possibly unconsciously) selected by the choice of coordinates. For example, see for a comparison of Lagrangians in an inertial and in a noninertial frame of reference. See also the discussion of "total" and "updated" Lagrangian formulations in. Unfortunately, this usage of "inertial force" conflicts with the Newtonian idea of an inertial force. In the Newtonian view, an inertial force originates in the acceleration of the frame of observation (the fact that it is not an inertial frame of reference), not in the choice of coordinate system. To keep matters clear, it is safest to refer to the Lagrangian inertial forces as "generalized" inertial forces, to distinguish them from the Newtonian vector inertial forces. That is, one should avoid following Hildebrand when he says (p. 155) "we deal "always" with "generalized" forces, velocities accelerations, and momenta. For brevity, the adjective "generalized" will be omitted frequently."
It is known that the Lagrangian of a system is not unique. Within the Lagrangian formalism the Newtonian fictitious forces can be identified by the existence of alternative Lagrangians in which the fictitious forces disappear, sometimes found by exploiting the symmetry of the system.
Non-relativistic test particles in fields.
A test particle is a particle whose mass and charge are assumed to be so small that its effect on external system is insignificant. It is often a hypothetical simplified point particle with no properties other than mass and charge. Real particles like electrons and up quarks are more complex and have additional terms in their Lagrangians.
Non-relativistic test particle in a Newtonian gravitational field.
For a particle with mass "m" in a Newtonian gravitation potential
since the force is conservative, one can follow the same procedure in the Cartesian coordinates example to find
where the mass cancels algebraically, and physically it should because the acceleration of all massive objects due to gravity is independent of the mass.
Non-relativistic test particle in an electromagnetic field.
The case of a charged particle with electrical charge "q" interacting with an electromagnetic field is more complicated. The electric scalar potential "ϕ"(r("t"), "t") and magnetic vector potential A(r("t"), "t") are defined from the electric field
and magnetic field
Notice all the fields depend on the position r of the particle at time "t", and depend explicitly on time as well as implicitly via the position.
The Lagrangian of a massive charged test particle in an electromagnetic field is
which produces the Lorentz force law
An interesting point in this example is the generalized momentum conjugate to r is the ordinary momentum plus a contribution from the A field,
This relation is used in the minimal coupling prescription in quantum mechanics and quantum field theory.
Extensions to include non-conservative forces.
Dissipation (i.e. non-conservative systems) can also be treated with an effective Lagrangian formulated by a certain doubling of the degrees of freedom; see.
In a more general formulation, the forces could be both conservative and viscous. If an appropriate transformation can be found from the Fi, Rayleigh suggests using a dissipation function, "D", of the following form:
where "Cjk" are constants that are related to the damping coefficients in the physical system, though not necessarily equal to them. If "D" is defined this way, then
and
Applications or extensions of Lagrangian mechanics in other contexts.
Relation to other formulations of classical mechanics.
The Hamiltonian, denoted by "H", is obtained by performing a Legendre transformation on the Lagrangian, which introduces new variables, canonically conjugate to the original variables. This doubles the number of variables, but makes differential equations first order. The Hamiltonian is the basis for an alternative formulation of classical mechanics known as Hamiltonian mechanics. It is a particularly ubiquitous quantity in quantum mechanics (see Hamiltonian (quantum mechanics)). 
In the classical view, time is an independent variable and "q""i" (and d"q""i"/d"t") are dependent variables as is often seen in phase space explanations of systems.
Routhian mechanics is a hybrid formulation of Lagrangian and Hamiltonian mechanics, which is not often used in practice but an efficient formulation for cyclic coordinates.
Relativistic formulation.
Lagrangian mechanics can be formulated in special relativity and general relativity. The equations of motion take the same form, but the Lagrangian is not simply the kinetic minus the potential energy.
Applications in quantum mechanics.
In 1948, Feynman discovered the path integral formulation extending the principle of least action to quantum mechanics for electrons and photons. In this formulation, particles travel every possible path between the initial and final states; the probability of a specific final state is obtained by summing over all possible trajectories leading to it. In the classical regime, the path integral formulation cleanly reproduces Hamilton's principle, and Fermat's principle in optics.
Classical field theory.
In Lagrangian mechanics, the generalized coordinates form a discrete set of variables that define the configuration of a system. The continuum analogue for defining a "field" are field variables, say "ϕ"(r, "t"), which represents some density function varying with position and time.
In classical field theory, the physical system is not a set of discrete particles, but rather a continuous field defined over a region of 3d space. Associated with the field is a Lagrangian density
defined in terms of the field and its space and time derivatives at a location r and time "t". The Lagrangian is then the integral of the Lagrangian density over 3d space (see volume integral):
where d3r is a 3d differential volume element, must be used instead. The Lagrangian is a function of time since the Lagrangian density has implicit space dependence via the fields, and may explicit spatial dependence, but these are removed in the integral, leaving only time in as the variable for the Lagrangian.
Uses in engineering.
Circa 1963 Lagrangians were a general part of the engineering curriculum, but a quarter of a century later, even with the ascendency of dynamical systems, they were dropped as requirements for some engineering programs, and are generally considered to be the domain of theoretical dynamics. Circa 2003 this changed dramatically, and Lagrangians are not only a required part of many ME and EE graduate-level curricula, but also find applications in finance, economics, and biology, mainly as the basis of the formulation of various path integral schemes to facilitate the solution of parabolic partial differential equations via random walks.
Circa 2013, Lagrangians find their way into hundreds of direct engineering solutions, including robotics, turbulent flow analysis (Lagrangian and Eulerian specification of the flow field), signal processing, microscopic component contact and nanotechnology (superlinear convergent augmented Lagrangians), gyroscopic forcing and dissipation, semi-infinite supercomputing (which also involve Lagrange multipliers in the subfield of semi-infinite programming), chemical engineering (specific heat linear Lagrangian interpolation in reaction planning), civil engineering (dynamic analysis of traffic flows), optics engineering and design (Lagrangian and Hamiltonian optics) aerospace (Lagrangian interpolation), force stepping integrators, and even airbag deployment (coupled Eulerian-Lagrangians as well as SELM—the stochastic Eulerian Lagrangian method).

</doc>
<doc id="1074997" url="https://en.wikipedia.org/wiki?curid=1074997" title="Lamb shift">
Lamb shift

In physics, the Lamb shift, named after Willis Lamb (1913–2008), is a small difference in energy between two energy levels 2"S"1/2 and 2"P"1/2 (in term symbol notation) of the hydrogen atom in quantum electrodynamics (QED). According to the Dirac equation, the 2"S"1/2 and 2"P"1/2 orbitals should have the same energy. However, the interaction between the electron and the vacuum (which is not accounted for by the Dirac equation) causes a tiny energy shift which is different for states 2"S"1/2 and 2"P"1/2. Lamb and Robert Retherford measured this shift in 1947, and this measurement provided the stimulus for renormalization theory to handle the divergences. It was the harbinger of modern quantum electrodynamics developed by Julian Schwinger, Richard Feynman, Ernst Stueckelberg and Sin-Itiro Tomonaga. Lamb won the Nobel Prize in Physics in 1955 for his discoveries related to the Lamb shift.
Derivation.
This heuristic derivation of the electrodynamic level shift following Welton is from "Quantum Optics".
The fluctuation in the electric and magnetic fields associated with the QED vacuum perturbs the electric potential due to the atomic nucleus. This perturbation causes a fluctuation in the position of the electron, which explains the energy shift. The difference of potential energy is given by
Since the fluctuations are isotropic,
So we can obtain
The classical equation of motion for the electron displacement ("δr") induced by a single mode of the field of wave vector and frequency "ν" is
and this is valid only when the frequency "ν" is greater than "ν"0 in the Bohr orbit, "ν" > "πc"/"a"0. The electron is unable to respond to the fluctuating field if the fluctuations are smaller than the natural orbital frequency in the atom.
For the field oscillating at "ν",
therefore
By the summation over all formula_8,
where formula_10 is some large normalization volume (the volume of the hypothetical "box" containing the hydrogen atom) and
The summation is changed into an integral because of the continuity of , formula_12, so that
This result diverges when no limits about the integral (at both large and small frequencies). As mentioned above, this method is expected to be valid only when "ν" > "πc"/"a"0, or equivalently "k" > "π"/"a"0. It is also valid only for wavelengths longer than the Compton wavelength, or equivalently "k" < "mc"/"ħ". Therefore we can choose the upper and lower limit of the integral and these limits make the result converge.
For the atomic orbital and the Coulomb potential,
since we know that
For "p" orbitals, the nonrelativistic wave function vanishes at the origin, so there is no energy shift. But for "s" orbitals there is some finite value at the origin,
where the Bohr radius is
Therefore
Finally, the difference of the potential energy becomes
where formula_21 is the Fine-structure constant.
This shift is about 1 GHz, very similar with the observed energy shift.
Experimental work.
In 1947 Willis Lamb and Robert Retherford carried out an experiment using microwave techniques to stimulate radio-frequency transitions between
2"S"1/2 and 2"P"1/2 levels of hydrogen. By using lower frequencies than for optical transitions the Doppler broadening could be neglected (Doppler broadening is proportional to the frequency). The energy difference Lamb and Retherford found was a rise of about 1000 MHz of the 2"S"1/2 level above the 2"P"1/2 level.
This particular difference is a one-loop effect of quantum electrodynamics, and can be interpreted as the influence of virtual photons that have been emitted and re-absorbed by the atom. In quantum electrodynamics the electromagnetic field is quantized
and, like the harmonic oscillator in quantum mechanics, its lowest state is not zero. Thus, there exist small zero-point oscillations that cause the electron to execute rapid oscillatory motions. The electron is "smeared out" and the radius is changed
from "r" to "r" + "δr".
The Coulomb potential is therefore perturbed by a small amount and the degeneracy of the two energy levels is removed. The new potential can be approximated (using atomic units) as follows:
The Lamb shift itself is given by
with "k"("n", 0) around 13 varying slightly with "n", and
with "k"("n",) a small number (< 0.05).
For a derivation of Δ"E"Lamb see for example:
Lamb shift in the hydrogen spectrum.
In 1947, Hans Bethe was the first to explain the Lamb shift in the hydrogen spectrum, and he thus laid the foundation for the modern development of quantum electrodynamics. The Lamb shift currently provides a measurement of the fine-structure constant α to better than one part in a million, allowing a precision test of quantum electrodynamics.
A different perspective relates Zitterbewegung to the Lamb shift.

</doc>
<doc id="7141462" url="https://en.wikipedia.org/wiki?curid=7141462" title="Lambda baryon">
Lambda baryon

The Lambda baryons are a family of subatomic hadron particles that have the symbols , , , and and have +1 elementary charge or are neutral. They are baryons containing three different quarks: one up, one down, and one third quark, which can be a strange (), a charm (), a bottom (), or a top () quark. The top Lambda is not expected to be observed as the Standard Model predicts the mean lifetime of top quarks to be roughly . This is about one-twentieth the timescale for strong interactions, and, therefore it does not form hadrons.
The Lambda baryon was first discovered in October 1950, by V. D. Hopper and S. Biswas of the University of Melbourne, as a neutral V particle with a proton as a decay product, thus correctly distinguishing it as a baryon, rather than a meson "i.e.", different in kind from the K meson discovered in 1947 by Rochester and Butler; they were produced by cosmic rays and detected in photographic emulsions flown in a balloon at . Though the particle was expected to live for , it actually survived for . The property that caused it to live so long was dubbed "strangeness" and led to the discovery of the strange quark. Furthermore, these discoveries led to a principle known as the "conservation of strangeness", wherein lightweight particles do not decay as quickly if they exhibit strangeness (because non-weak methods of particle decay must preserve the strangeness of the decaying baryon).
The Lambda baryon has also been observed in atomic nuclei called hypernuclei. These nuclei contain the same number of protons and neutrons as a known nucleus, but also contains one or in rare cases two Lambda particles. In such a scenario, the Lambda slides into the center of the nucleus (it is not a proton or a neutron, and thus is not affected by the Pauli exclusion principle), and it binds the nucleus more tightly together due to its interaction via the strong force. In a lithium isotope (Λ7Li), it made the nucleus 19% smaller.
List.
The symbols encountered in this list are: I ("isospin"), J ("total angular momentum quantum number"), P ("parity"), Q ("charge"), S ("strangeness"), C ("charmness"), B′ ("bottomness"), T ("topness"), B ("baryon number"), u ("up quark"), d ("down quark"), s ("strange quark"), c ("charm quark"), b ("bottom quark"), t ("top quark"), as well as other subatomic particles (hover for name).
Antiparticles are not listed in the table; however, they simply would have all quarks changed to antiquarks, and Q, B, S, C, B′, T, would be of opposite signs. I, J, and P values in red have not been firmly established by experiments, but are predicted by the quark model and are consistent with the measurements. The top lambda () is listed for comparison, but is not expected to be observed, because top quarks decay before they have time to hadronize.
† Particle unobserved, because the top-quark decays before it hadronizes.

</doc>
<doc id="1587123" url="https://en.wikipedia.org/wiki?curid=1587123" title="Landé g-factor">
Landé g-factor

In physics, the Landé "g"-factor is a particular example of a "g"-factor, namely for an electron with both spin and orbital angular momenta. It is named after Alfred Landé, who first described it in 1921.
In atomic physics, the Landé "g"-factor is a multiplicative term appearing in the expression for the energy levels of an atom in a weak magnetic field. The quantum states of electrons in atomic orbitals are normally degenerate in energy, with these degenerate states all sharing the same angular momentum. When the atom is placed in a weak magnetic field, however, the degeneracy is lifted.
Description.
The factor comes about during the calculation of the first-order perturbation in the energy of an atom when a weak uniform magnetic field (that is, weak in comparison to the system's internal magnetic field) is applied to the system. Formally we can write the factor as,
The orbital "g"-factor is equal to 1, and under the approximation formula_2, the above expression simplifies to
Here, "J" is the total electronic angular momentum, "L" is the orbital angular momentum, and "S" is the spin angular momentum. Because "S"=1/2 for electrons, one often sees this formula written with 3/4 in place of "S"("S"+1). The quantities "gL" and "gS" are other "g"-factors of an electron.
If we wish to know the "g"-factor for an atom with total atomic angular momentum F=I+J,
This last approximation is justified because formula_6 is smaller than formula_7 by the ratio of the electron mass to the proton mass.
A derivation.
The following derivation basically follows the line of thought in and.
Both orbital angular momentum and spin angular momentum of electron contribute to the magnetic moment. In particular, each of them alone contributes to the magnetic moment by the following form
where 
Note that negative signs in the above expressions are because an electron carries negative charge, and the value of formula_13 can be derived naturally from Dirac's equation. The total magnetic moment formula_14, as a vector operator, does not lie on the direction of total angular momentum formula_15. However, due to Wigner-Eckart theorem, its expectation value does effectively lie on the direction of formula_16 which can be employed in the determination of the "g"-factor according to the rules of angular momentum coupling. In particular, the "g"-factor is defined as a consequence of the theorem itself
Therefore, 
One gets

</doc>
<doc id="3506553" url="https://en.wikipedia.org/wiki?curid=3506553" title="Laplace expansion">
Laplace expansion

In linear algebra, the Laplace expansion, named after Pierre-Simon Laplace, also called cofactor expansion, is an expression for the determinant |"B"| of an "n" × "n" matrix "B" that is a weighted sum of the determinants of "n" sub-matrices of "B", each of size ("n"−1) × ("n"−1). The Laplace expansion is of theoretical interest as one of several ways to view the determinant, as well as of practical use in determinant computation.
The "i", "j" "cofactor" of "B" is the scalar "Cij" defined by
where "Mij" is the "i", "j" "minor matrix" of "B", that is, the determinant of the ("n" − 1) × ("n" − 1) matrix that results from deleting the "i"-th row and the "j"-th column of "B".
Then the Laplace expansion is given by the following
Then its determinant |"B"| is given by:
Examples.
Consider the matrix
The determinant of this matrix can be computed by using the Laplace expansion along any one of its rows or columns. For instance, an expansion along the first row yields:
Laplace expansion along the second column yields the same result:
It is easy to verify that the result is correct: the matrix is singular because the sum of its first and third column is twice the second column, and hence its determinant is zero.
Proof.
Suppose formula_8 is an "n" × "n" matrix and formula_9 For clarity we also label the entries of formula_8 that compose its formula_11 minor matrix formula_12 as
formula_13 for formula_14
Consider the terms in the expansion of formula_15 that have formula_16 as a factor. Each has the form
for some permutation with formula_18, and a unique and evidently related permutation formula_19 which selects the same minor entries as . Similarly each choice of determines a corresponding i.e. the correspondence formula_20 is a bijection between formula_21 and formula_22 The permutation can be derived from as follows.
Define formula_23 by formula_24 for formula_25 and formula_26. Then formula_27 and
Since the two cycles can be written respectively as formula_29 and formula_30 transpositions,
And since the map formula_20 is bijective,
from which the result follows.
Laplace expansion of a determinant by complementary minors.
Laplaces cofactor expansion can be generalised as follows.
Example.
Consider the matrix
The determinant of this matrix can be computed by using the Laplace's cofactor expansion along the first two rows as follows. Firstly note that there are 6 sets of two distinct numbers in namely let formula_35 be the aforementioned set.
By defining the complementary cofactors to be
and the sign of their permutation to be
The determinant of A can be written out as
where formula_40 is the complementary set to formula_41.
In our explicit example this gives us
As above, It is easy to verify that the result is correct: the matrix is singular because the sum of its first and third column is twice the second column, and hence its determinant is zero.
General statement.
Let formula_43 be an matrix and formula_44 the set of -element subsets of , formula_45 an element in it. Then the determinant of formula_8 can be expanded along the rows identified by formula_45 as follows:
where formula_49 is the sign of the permutation determined by formula_45 and formula_51, equal to formula_52, formula_53 the square submatrix of formula_8 obtained by deleting from formula_8 rows and columns with indices in formula_45 and formula_51 respectively, and formula_58 (called the complement of formula_53) defined to be formula_60 , formula_61 and formula_62 being the complement of formula_45 and formula_51 respectively.
This coincides with the theorem above when formula_65. The same thing holds for any fixed columns.
Computational expense.
The Laplace expansion is computationally inefficient for high dimension because for "N" × "N" matrices, the computational effort scales with "N"!. Therefore, the Laplace expansion is not suitable for large "N". Using a decomposition into triangular matrices as in the LU decomposition, one can determine determinants with effort "N"3/3.

</doc>
<doc id="174706" url="https://en.wikipedia.org/wiki?curid=174706" title="Laplace operator">
Laplace operator

In mathematics, the Laplace operator or Laplacian is a differential operator given by the divergence of the gradient of a function on Euclidean space. It is usually denoted by the symbols ∇·∇, ∇2, or ∆. The Laplacian ∆"f"("p") of a function "f" at a point "p", up to a constant depending on the dimension, is the rate at which the average value of "f" over spheres centered at "p", deviates from "f"("p") as the radius of the sphere grows. In a Cartesian coordinate system, the Laplacian is given by the sum of second partial derivatives of the function with respect to each independent variable. In other coordinate systems such as cylindrical and spherical coordinates, the Laplacian also has a useful form.
The Laplace operator is named after the French mathematician Pierre-Simon de Laplace (1749–1827), who first applied the operator to the study of celestial mechanics, where the operator gives a constant multiple of the mass density when it is applied to a given gravitational potential. Solutions of the equation ∆"f" = 0, now called Laplace's equation, are the so-called harmonic functions, and represent the possible gravitational fields in free space.
The Laplacian occurs in differential equations that describe many physical phenomena, such as electric and gravitational potentials, the diffusion equation for heat and fluid flow, wave propagation, and quantum mechanics. The Laplacian represents the flux density of the gradient flow of a function. For instance, the net rate at which a chemical dissolved in a fluid moves toward or away from some point is proportional to the Laplacian of the chemical concentration at that point; expressed symbolically, the resulting equation is the diffusion equation. For these reasons, it is extensively used in the sciences for modelling all kinds of physical phenomena. The Laplacian is the simplest elliptic operator, and is at the core of Hodge theory as well as the results of de Rham cohomology. In image processing and computer vision, the Laplacian operator has been used for various tasks such as blob and edge detection.
Definition.
The Laplace operator is a second order differential operator in the "n"-dimensional Euclidean space, defined as the divergence (∇·) of the gradient (∇"ƒ"). Thus if "ƒ" is a twice-differentiable real-valued function, then the Laplacian of "ƒ" is defined by
where the latter notations derive from formally writing formula_1 Equivalently, the Laplacian of "ƒ" is the sum of all the "unmixed" second partial derivatives in the Cartesian coordinates formula_2 :
As a second-order differential operator, the Laplace operator maps "C""k"-functions to "C""k"−2-functions for "k" ≥ 2. The expression (or equivalently ) defines an operator , or more generally an operator for any open set Ω.
Motivation.
Diffusion.
In the physical theory of diffusion, the Laplace operator (via Laplace's equation) arises naturally in the mathematical description of equilibrium. Specifically, if "u" is the density at equilibrium of some quantity such as a chemical concentration, then the net flux of "u" through the boundary of any smooth region "V" is zero, provided there is no source or sink within "V":
where n is the outward unit normal to the boundary of "V". By the divergence theorem,
Since this holds for all smooth regions "V", it can be shown that this implies
The left-hand side of this equation is the Laplace operator. The Laplace operator itself has a physical interpretation for non-equilibrium diffusion as the extent to which a point represents a source or sink of chemical concentration, in a sense made precise by the diffusion equation.
Density associated to a potential.
If φ denotes the electrostatic potential associated to a charge distribution "q", then the charge distribution itself is given by the Laplacian of φ:
This is a consequence of Gauss's law. Indeed, if "V" is any smooth region, then by Gauss's law the flux of the electrostatic field E is equal to the charge enclosed (in appropriate units):
where the first equality uses the fact that the electrostatic field is the gradient of the electrostatic potential. The divergence theorem now gives
and since this holds for all regions "V", () follows.
The same approach implies that the Laplacian of the gravitational potential is the mass distribution. Often the charge (or mass) distribution are given, and the associated potential is unknown. Finding the potential function subject to suitable boundary conditions is equivalent to solving Poisson's equation.
Energy minimization.
Another motivation for the Laplacian appearing in physics is that solutions to formula_8 in a region "U" are functions that make the Dirichlet energy functional stationary:
To see this, suppose 
formula_10 is a function, and 
formula_11 is a function that vanishes on the 
boundary of "U". Then
where the last equality follows using Green's first identity.
This calculation shows that if formula_8, then 
"E" is stationary around "f". Conversely, if "E" is stationary 
around f, then formula_14 by the fundamental lemma of calculus of variations.
Coordinate expressions.
Two dimensions.
The Laplace operator in two dimensions is given by
where "x" and "y" are the standard Cartesian coordinates of the "xy"-plane.
In polar coordinates,
Three dimensions.
In three dimensions, it is common to work with the Laplacian in a variety of different coordinate systems.
In Cartesian coordinates,
In cylindrical coordinates,
In spherical coordinates:
(here φ represents the azimuthal angle and θ the zenith angle or co-latitude).
In general curvilinear coordinates (formula_20):
formula_21
where summation over the repeated indices is implied.
"N" dimensions.
In spherical coordinates in "N" dimensions, with the parametrization "x" = "r" θ ∈ R"N" with "r" representing a positive real radius and θ an element of the unit sphere "S""N"−1,
where formula_23 is the Laplace–Beltrami operator on the ("N"−1)-sphere, known as the spherical Laplacian. The two radial derivative terms can be equivalently rewritten as
As a consequence, the spherical Laplacian of a function defined on "S""N"−1 ⊂ R"N" can be computed as the ordinary Laplacian of the function extended to R"N"\{0} so that it is constant along rays, i.e., homogeneous of degree zero.
Spectral theory.
The spectrum of the Laplace operator consists of all eigenvalues λ for which there is a corresponding eigenfunction "ƒ" with
This is known as the Helmholtz equation.
If Ω is a bounded domain in R"n" then the eigenfunctions of the Laplacian are an orthonormal basis for the Hilbert space "L"2(Ω). This result essentially follows from the spectral theorem on compact self-adjoint operators, applied to the inverse of the Laplacian (which is compact, by the Poincaré inequality and Kondrachov embedding theorem). It can also be shown that the eigenfunctions are infinitely differentiable functions. More generally, these results hold for the Laplace–Beltrami operator on any compact Riemannian manifold with boundary, or indeed for the Dirichlet eigenvalue problem of any elliptic operator with smooth coefficients on a bounded domain. When Ω is the "n"-sphere, the eigenfunctions of the Laplacian are the well-known spherical harmonics.
Generalizations.
Laplace–Beltrami operator.
The Laplacian also can be generalized to an elliptic operator called the Laplace–Beltrami operator defined on a Riemannian manifold. The d'Alembert operator generalizes to a hyperbolic operator on pseudo-Riemannian manifolds. The Laplace–Beltrami operator, when applied to a function, is the trace of the function's Hessian:
where the trace is taken with respect to the inverse of the metric tensor. The Laplace–Beltrami operator also can be generalized to an operator (also called the Laplace–Beltrami operator) which operates on tensor fields, by a similar formula.
Another generalization of the Laplace operator that is available on pseudo-Riemannian manifolds uses the exterior derivative, in terms of which the “geometer's Laplacian" is expressed as
Here "d"∗ is the codifferential, which can also be expressed using the Hodge dual. Note that this operator differs in sign from the "analyst's Laplacian" defined
above, a point which must always be kept in mind when reading papers in global analysis. 
More generally, the "Hodge" Laplacian is defined on differential forms α by
This is known as the Laplace–de Rham operator, which is related to the Laplace–Beltrami operator by the Weitzenböck identity.
D'Alembertian.
The Laplacian can be generalized in certain ways to non-Euclidean spaces, where it may be elliptic, hyperbolic, or ultrahyperbolic.
In the Minkowski space the Laplace–Beltrami operator becomes the d'Alembert operator or d'Alembertian:
It is the generalisation of the Laplace operator in the sense that it is the differential operator which is invariant under the isometry group of the underlying space and it reduces to the Laplace operator if restricted to time independent functions. Note that the overall sign of the metric here is chosen such that the spatial parts of the operator admit a negative sign, which is the usual convention in high energy particle physics. The D'Alembert operator is also known as the wave operator, because it is the differential operator appearing in the wave equations and it is also part of the Klein–Gordon equation, which reduces to the wave equation in the massless case.
The additional factor of "c" in the metric is needed in physics if space and time are measured in different units; a similar factor would be required if, for example, the "x" direction were measured in meters while the "y" direction were measured in centimeters. Indeed, theoretical physicists usually work in units such that "c"=1 in order to simplify the equation.

</doc>
