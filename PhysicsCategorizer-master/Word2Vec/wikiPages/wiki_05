<doc id="2784730" url="https://en.wikipedia.org/wiki?curid=2784730" title="Larmor precession">
Larmor precession

In physics, Larmor precession (named after Joseph Larmor) is the precession of the magnetic moment of any object with a magnetic moment about an external magnetic field. Objects with magnetic moments have angular momentum and internal currents of electric charge related to their angular momentum; these include electrons, protons, other fermions, many atomic and nuclear systems, as well as classical macroscopic systems. The magnetic field exerts a torque on the magnetic moment,
where formula_2 is the torque, formula_3 is the magnetic dipole moment, formula_4 is the angular momentum vector, formula_5 is the external magnetic field, formula_6 symbolizes the cross product, and formula_7 is the gyromagnetic ratio which gives the proportionality constant between the magnetic moment and the angular momentum. The phenomenon is similar to the precession of a tilted classical gyroscope in an external gravitational field.
Larmor frequency.
The angular momentum vector formula_4 precesses about the external field axis with an angular frequency known as the Larmor frequency,
where formula_10 is the angular frequency, and
formula_11 is the magnitude of the applied magnetic field.
formula_12 is (for a particle of charge formula_13) the gyromagnetic ratio, equal to formula_14, where formula_15 is the mass of the precessing system, while formula_16 is the g-factor of the system. The g-factor is the unit-less proportionality factor relating the system's angular momentum to the intrinsic magnetic moment; in classical physics it is just 1.
In nuclear physics the g-factor of a given system includes the effect of the nucleon spins, their orbital angular momenta, and their couplings. Generally, the g-factors are very difficult to calculate for such many-body systems, but they have been measured to high precision for most nuclei. The Larmor frequency is important in NMR spectroscopy. The gyromagnetic ratios, which give the Larmor frequencies at a given magnetic field strength, have been measured and tabulated here.
Crucially, the Larmor frequency is independent of the polar angle between the applied magnetic field and the magnetic moment direction. This is what makes it a key concept in fields such as nuclear magnetic resonance (NMR) and electron paramagnetic resonance (EPR), since the precession rate does not depend on the spatial orientation of the spins.
Including Thomas precession.
The above equation is the one that is used in most applications. However, a full treatment must include the effects of Thomas precession, yielding the equation (in CGS units): 
where formula_12 is the relativistic Lorentz factor (not to be confused with the gyromagnetic ratio above). Notably, for the electron g is very close to 2 (2.002..), so if one sets g=2, one arrives at
Bargmann–Michel–Telegdi equation.
The spin precession of an electron in an external electromagnetic field is described by the Bargmann–Michel–Telegdi (BMT) equation 
where formula_21, formula_22, formula_15, and formula_24 are polarization four-vector, charge, mass, and magnetic moment, formula_25 is four-velocity of electron, formula_26, formula_27, and formula_28 is electromagnetic field-strength tensor. Using equations of motion, 
one can rewrite the first term on the right side of the BMT equation as formula_30, where formula_31 is four-acceleration. This term describes Fermi–Walker transport and leads to Thomas precession. The second term is associated with Larmor precession.
When electromagnetic fields are uniform in space or when gradient forces like formula_32 can be neglected, the particle's translational motion is described by
The BMT equation is then written as 
The Beam-Optical version of the Thomas-BMT, from the "Quantum Theory of Charged-Particle Beam Optics", applicable in accelerator optics
Applications.
A 1935 paper published by Lev Landau and Evgeny Lifshitz predicted the existence of ferromagnetic resonance of the Larmor precession, which was independently verified in experiments by J. H. E. Griffiths (UK) and E. K. Zavoiskij (USSR) in 1946.
Larmor precession is important in nuclear magnetic resonance, magnetic resonance imaging, electron paramagnetic resonance and muon spin resonance. It is also important for the alignment of cosmic dust grains, which is a cause of the polarization of starlight.
To calculate the spin of a particle in a magnetic field, one must also take into account Thomas precession.
Precession direction.
The spin angular momentum of an electron precesses counter-clockwise about the direction of the magnetic field. An electron has a negative charge, so the direction of magnetic moment is opposite to that of its spin.

</doc>
<doc id="469246" url="https://en.wikipedia.org/wiki?curid=469246" title="Lattice (group)">
Lattice (group)

In mathematics, especially in geometry and group theory, a lattice in formula_1 is a subgroup of formula_1 which is isomorphic to formula_3, and which spans the real vector space formula_1. In other words, for any basis of formula_1, the subgroup of all linear combinations with integer coefficients forms a lattice. A lattice may be viewed as a regular tiling of a space by a primitive cell.
Lattices have many significant applications in pure mathematics, particularly in connection to Lie algebras, number theory and group theory. They also arise in applied mathematics in connection with coding theory, in cryptography because of conjectured computational hardness of several lattice problems, and are used in various ways in the physical sciences. For instance, in materials science and solid-state physics, a lattice is a synonym for the "frame work" of a crystalline structure, a 3-dimensional array of regularly spaced points coinciding with the atom or molecule positions in a crystal. More generally, lattice models are studied in physics, often by the techniques of computational physics.
Symmetry considerations and examples.
A lattice is the symmetry group of discrete translational symmetry in "n" directions. A pattern with this lattice of translational symmetry cannot have more, but may have less symmetry than the lattice itself. As a group (dropping its geometric structure) a lattice is a finitely-generated free abelian group, and thus isomorphic to formula_3.
A lattice in the sense of a 3-dimensional array of regularly spaced points coinciding with e.g. the atom or molecule positions in a crystal, or more generally, the orbit of a group action under translational symmetry, is a translate of the translation lattice: a coset, which need not contain the origin, and therefore need not be a lattice in the previous sense.
A simple example of a lattice in formula_1 is the subgroup formula_3. More complicated examples include the E8 lattice, which is a lattice in formula_9, and the Leech lattice in formula_10. The period lattice in formula_11 is central to the study of elliptic functions, developed in nineteenth century mathematics; it generalises to higher dimensions in the theory of abelian functions. Lattices called root lattices are important in the theory of simple Lie algebras; for example, the E8 lattice is related to a Lie algebra that goes by the same name.
Dividing space according to a lattice.
A typical lattice formula_12 in formula_1 thus has the form 
where {"v"1, ..., "v""n"} is a basis for formula_1. Different bases can generate the same lattice, but the absolute value of the determinant of the vectors "v""i" is uniquely determined by Λ, and is denoted by d(Λ).
If one thinks of a lattice as dividing the whole of formula_1 into equal polyhedra (copies of an "n"-dimensional parallelepiped, known as the "fundamental region" of the lattice), then d(Λ) is equal to the "n"-dimensional volume of this polyhedron. This is why d(Λ) is sometimes called the covolume of the lattice. If this equals 1, the lattice is called unimodular.
Lattice points in convex sets.
Minkowski's theorem relates the number d(Λ) and the volume of a symmetric convex set "S" to the number of lattice points contained in "S". The number of lattice points contained in a polytope all of whose vertices are elements of the lattice is described by the polytope's Ehrhart polynomial. Formulas for some of the coefficients of this polynomial involve d(Λ) as well.
Computing with lattices.
Lattice basis reduction is the problem of finding a short and nearly orthogonal lattice basis. The Lenstra-Lenstra-Lovász lattice basis reduction algorithm (LLL) approximates such a lattice basis in polynomial time; it has found numerous applications, particularly in public-key cryptography.
Lattices in two dimensions: detailed discussion.
There are five 2D lattice types as given by the crystallographic restriction theorem. Below, the wallpaper group of the lattice is given in IUC notation, Orbifold notation, and Coxeter notation, along with a wallpaper diagram showing the symmetry domains. Note that a pattern with this lattice of translational symmetry cannot have more, but may have less symmetry than the lattice itself. A full list of subgroups is available. For example below the hexagonal/triangular lattice is given twice, with full 6-fold and a half 3-fold reflectional symmetry. If the symmetry group of a pattern contains an "n"-fold rotation then the lattice has "n"-fold symmetry for even "n" and 2"n"-fold for odd "n". 
For the classification of a given lattice, start with one point and take a nearest second point. For the third point, not on the same line, consider its distances to both points. Among the points for which the smaller of these two distances is least, choose a point for which the larger of the two is least. (Not logically equivalent but in the case of lattices giving the same result is just "Choose a point for which the larger of the two is least".)
The five cases correspond to the triangle being equilateral, right isosceles, right, isosceles, and scalene. In a rhombic lattice, the shortest distance may either be a diagonal or a side of the rhombus, i.e., the line segment connecting the first two points may or may not be one of the equal sides of the isosceles triangle. This depends on the smaller angle of the rhombus being less than 60° or between 60° and 90°.
The general case is known as a period lattice. If the vectors p and q generate the lattice, instead of p and q we can also take p and p-q, etc. In general in 2D, we can take "a" p + "b" q and "c" p + "d" q for integers "a","b", "c" and "d" such that "ad-bc" is 1 or -1. This ensures that p and q themselves are integer linear combinations of the other two vectors. Each pair p, q defines a parallelogram, all with the same area, the magnitude of the cross product. One parallelogram fully defines the whole object. Without further symmetry, this parallelogram is a fundamental parallelogram.
The vectors p and q can be represented by complex numbers. Up to size and orientation, a pair can be represented by their quotient. Expressed geometrically: if two lattice points are 0 and 1, we consider the position of a third lattice point. Equivalence in the sense of generating the same lattice is represented by the modular group: formula_17 represents choosing a different third point in the same grid, formula_18 represents choosing a different side of the triangle as reference side 0-1, which in general implies changing the scaling of the lattice, and rotating it. Each "curved triangle" in the image contains for each 2D lattice shape one complex number, the grey area is a canonical representation, corresponding to the classification above, with 0 and 1 two lattice points that are closest to each other; duplication is avoided by including only half of the boundary. The rhombic lattices are represented by the points on its boundary, with the hexagonal lattice as vertex, and "i" for the square lattice. The rectangular lattices are at the imaginary axis, and the remaining area represents the parallelogrammetic lattices, with the mirror image of a parallelogram represented by the mirror image in the imaginary axis.
Lattices in three dimensions.
The 14 lattice types in 3D are called Bravais lattices. They are characterized by their space group. 3D patterns with translational symmetry of a particular type cannot have more, but may have less symmetry than the lattice itself.
Lattices in complex space.
A lattice in formula_19 is a discrete subgroup of formula_19 which spans the 2"n"-dimensional real vector space formula_19.
For example, the Gaussian integers form a lattice in C.
Every lattice in formula_1 is a free abelian group of rank "n"; every lattice in formula_19 is a free abelian group of rank 2"n".
In Lie groups.
More generally, a lattice Γ in a Lie group "G" is a discrete subgroup, such that the quotient "G"/Γ is of finite measure, for the measure on it inherited from Haar measure on "G" (left-invariant, or right-invariant—the definition is independent of that choice). That will certainly be the case when "G"/Γ is compact, but that sufficient condition is not necessary, as is shown by the case of the modular group in "SL"2(R), which is a lattice but where the quotient isn't compact (it has "cusps"). There are general results stating the existence of lattices in Lie groups.
A lattice is said to be uniform or cocompact if "G"/Γ is compact; otherwise the lattice is called non-uniform.
Lattices in general vector-spaces.
Whilst we normally consider formula_24 lattices in formula_1 this concept can be generalized to any finite-dimensional vector space over any field. This can be done as follows:
Let "K" be a field, let "V" be an "n"-dimensional "K"-vector space, let formula_26 be a "K"-basis for "V" and let "R" be a ring contained within "K". Then the "R" lattice formula_27 in "V" generated by "B" is given by:
Different bases "B" will in general generate different lattices. However, if the transition matrix "T" between the bases is in formula_29 - the general linear group of R (in simple terms this means that all the entries of "T" are in "R" and all the entries of formula_30 are in "R" - which is equivalent to saying that the determinant of "T" is in formula_31 - the unit group of elements in "R" with multiplicative inverses) then the lattices generated by these bases will be isomorphic since "T" induces an isomorphism between the two lattices.
Important cases of such lattices occur in number theory with "K" a p-adic field and "R" the p-adic integers.
For a vector space which is also an inner product space, the dual lattice can be concretely described by the set:
or equivalently as,

</doc>
<doc id="1092885" url="https://en.wikipedia.org/wiki?curid=1092885" title="Lattice field theory">
Lattice field theory

In physics, lattice field theory is the study of lattice models of quantum field theory, that is, of field theory on a spacetime that has been discretized onto a lattice. Although most lattice field theories are not exactly solvable, they are of tremendous appeal because they can be studied by simulation on a computer. One hopes that, by performing simulations on larger and larger lattices, while making the lattice spacing smaller and smaller, one will be able to recover the behaviour of the continuum theory. 
Just as in all lattice models, numerical simulation gives access to field configurations that are not accessible to perturbation theory, such as solitons. Likewise, non-trivial vacuum states can be discovered and probed. 
The method is particularly appealing for the quantization of a gauge theory. Most quantization methods keep Poincaré invariance manifest but sacrifice manifest gauge symmetry by requiring gauge fixing. Only after renormalization can gauge invariance be recovered. Lattice field theory differs from these in that it keeps manifest gauge invariance, but sacrifices manifest Poincaré invariance— recovering it only after renormalization. The articles on lattice gauge theory and lattice QCD explore these issues in greater detail.

</doc>
<doc id="671804" url="https://en.wikipedia.org/wiki?curid=671804" title="Lattice gauge theory">
Lattice gauge theory

In physics, lattice gauge theory is the study of gauge theories on a spacetime that has been discretized into a lattice. Gauge theories are important in particle physics, and include the prevailing theories of elementary particles: quantum electrodynamics, quantum chromodynamics (QCD) and the Standard Model. Non-perturbative gauge theory calculations in continuous spacetime formally involve evaluating an infinite-dimensional path integral, which is computationally intractable. By working on a discrete spacetime, the path integral becomes finite-dimensional, and can be evaluated by stochastic simulation techniques such as the Monte Carlo method. When the size of the lattice is taken infinitely large and its sites infinitesimally close to each other, the continuum gauge theory is recovered.
Basics.
In lattice gauge theory, the spacetime is Wick rotated into Euclidean space and discretized into a lattice with sites separated by distance formula_1 and connected by links. In the most commonly considered cases, such as lattice QCD, fermion fields are defined at lattice sites (which leads to fermion doubling), while the gauge fields are defined on the links. That is, an element "U" of the compact Lie group "G" is assigned to each link. Hence to simulate QCD, with Lie group SU(3), a 3×3 unitary matrix, is defined on each link. The link is assigned an orientation, with the inverse element corresponding to the same link with the opposite orientation.
Yang–Mills action.
The Yang–Mills action is written on the lattice using Wilson loops (named after Kenneth G. Wilson), so that the limit formula_2 formally reproduces the original continuum action. Given a faithful irreducible representation ρ of "G", the lattice Yang-Mills action is the sum over all lattice sites of the (real component of the) trace over the "n" links "e"1, ..., "e"n in the Wilson loop,
Here, χ is the character. If ρ is a real (or pseudoreal) representation, taking the real component is redundant, because even if the orientation of a Wilson loop is flipped, its contribution to the action remains unchanged.
There are many possible lattice Yang-Mills actions, depending on which Wilson loops are used in the action. The simplest "Wilson action" uses only the 1×1 Wilson loop, and differs from the continuum action by "lattice artifacts" proportional to the small lattice spacing formula_1. By using more complicated Wilson loops to construct "improved actions", lattice artifacts can be reduced to be proportional to formula_5, making computations more accurate.
Measurements and calculations.
Quantities such as particle masses are stochastically calculated using techniques such as the Monte Carlo method. Gauge field configurations are generated with probabilities proportional to formula_6, where formula_7 is the lattice action and formula_8 is related to the lattice spacing formula_1. The quantity of interest is calculated for each configuration, and averaged. Calculations are often repeated at different lattice spacings formula_1 so that the result can be extrapolated to the continuum, formula_2.
Such calculations are often extremely computationally intensive, and can require the use of the largest available supercomputers. To reduce the computational burden, the so-called quenched approximation can be used, in which the fermionic fields are treated as non-dynamic "frozen" variables. While this was common in early lattice QCD calculations, "dynamical" fermions are now standard. These simulations typically utilize algorithms based upon molecular dynamics or microcanonical ensemble algorithms.
The results of lattice QCD computations show e.g. that in a meson not only the particles (quarks and antiquarks), but also the "fluxtubes" of the gluon fields are important.
Other applications.
Originally, solvable two-dimensional lattice gauge theories had already been introduced in 1971 as models with interesting statistical properties by the theorist Franz Wegner, who worked in the field of phase transitions.
When only 1×1 Wilson loops appear in the action, Lattice gauge theory can be shown to be exactly dual to spin foam models.

</doc>
<doc id="18542" url="https://en.wikipedia.org/wiki?curid=18542" title="Length">
Length

In geometric measurements, length is the most extended dimension of an object. In the International System of Quantities, length is any quantity with dimension distance. In other contexts "length" is the measured dimension of an object. For example, it is possible to cut a length of a wire which is shorter than wire thickness.
Length may be distinguished from height, which is vertical extent, and width or breadth, which are the distance from side to side, measuring across the object at right angles to the length. Length is a measure of one dimension, whereas area is a measure of two dimensions (length squared) and volume is a measure of three dimensions (length cubed). In most systems of measurement, the unit of length is a base unit, from which other units are defined.
History.
Measurement has been important ever since humans settled from nomadic lifestyles and started using building materials, occupying land and trading with neighbours. As society has become more technologically oriented, much higher accuracies of measurement are required in an increasingly diverse set of fields, from micro-electronics to interplanetary ranging.
One of the oldest units of length measurement used in the ancient world was the 'cubit' which was the length of the arm from the tip of the finger to the elbow. This could then be subdivided into shorter units like the foot, hand (which at 4 inches is still used today for expressing the height of horses) or finger, or added together to make longer units like the stride. The cubit could vary considerably due to the different sizes of people.
After Albert Einstein's special relativity, length can no longer be thought of being constant in all reference frames. Thus a ruler that is one meter long in one frame of reference will not be one meter long in a reference frame that is travelling at a velocity relative to the first frame. This means length of an object is variable depending on the observer.
Units.
In the physical sciences and engineering, when one speaks of "units of length", the word "length" is synonymous with "distance". There are several units that are used to measure length. Historically, units of length may have been derived from the lengths of human body parts, the distance travelled in a number of paces, the distance between landmarks or places on the Earth, or arbitrarily on the length of some fixed object.
In the International System of Units (SI), the basic unit of length is the metre and is now defined in terms of the speed of light. The centimetre and the kilometre, derived from the metre, are also commonly used units. In U.S. customary units, English or Imperial system of units, commonly used units of length are the inch, the foot, the yard, and the mile.
Units used to denote distances in the vastness of space, as in astronomy, are much longer than those typically used on Earth and include the astronomical unit, the light-year, and the parsec.

</doc>
<doc id="339024" url="https://en.wikipedia.org/wiki?curid=339024" title="Length contraction">
Length contraction

In physics, length contraction is the phenomenon of a decrease in length of an object as measured by an observer which is traveling at any non-zero velocity relative to the object. This contraction (more formally called Lorentz contraction or Lorentz–FitzGerald contraction after Hendrik Lorentz and George FitzGerald) is usually only noticeable at a substantial fraction of the speed of light. Length contraction is only in the direction parallel to the direction in which the observed body is travelling. This effect is negligible at everyday speeds, and can be ignored for all regular purposes. Only at greater speeds does it become relevant. At a speed of 13,400,000 m/s (30 million mph, 0.0447), the contracted length is 99.9% of the length at rest; at a speed of 42,300,000 m/s (95 million mph, 0.141), the length is still 99%. As the magnitude of the velocity approaches the speed of light, the effect becomes dominant, as can be seen from the formula:
where
and the "Lorentz factor", , is defined as
In this equation it is assumed that the object is parallel with its line of movement. For the observer in relative movement, the length of the object is measured by subtracting the simultaneously measured distances of both ends of the object. For more general conversions, see the Lorentz transformations. An observer at rest viewing an object travelling very close to the speed of light would observe the length of the object in the direction of motion as very near zero.
History.
Length contraction was postulated by George FitzGerald (1889) and Hendrik Antoon Lorentz (1892) to explain the negative outcome of the Michelson-Morley experiment and to rescue the hypothesis of the stationary aether (Lorentz–FitzGerald contraction hypothesis).
Although both FitzGerald and Lorentz alluded to the fact that electrostatic fields in motion were deformed ("Heaviside-Ellipsoid" after Oliver Heaviside, who derived this deformation from electromagnetic theory in 1888), it was considered an ad hoc hypothesis, because at this time there was no sufficient reason to assume that intermolecular forces behave the same way as electromagnetic ones. In 1897 Joseph Larmor developed a model in which all forces are considered to be of electromagnetic origin, and length contraction appeared to be a direct consequence of this model. Yet it was shown by Henri Poincaré (1905) that electromagnetic forces alone cannot explain the electron's stability. So he had to introduce another ad hoc hypothesis: non-electric binding forces (Poincaré stresses) that ensure the electron's stability, give a dynamical explanation for length contraction, and thus hide the motion of the stationary aether.
Eventually, Albert Einstein (1905) was the first to completely remove the ad hoc character from the contraction hypothesis, by demonstrating that this contraction did not require motion through a supposed aether, but could be explained using special relativity, which changed our notions of space, time, and simultaneity. Einstein's view was further elaborated by Hermann Minkowski, who demonstrated the geometrical interpretation of all relativistic effects by introducing his concept of four-dimensional spacetime.
Basis in relativity.
First it is necessary to carefully consider the methods for measuring the lengths of resting and moving objects. Here, "object" simply means a distance with endpoints that are always mutually at rest, "i.e.", that are at rest in the same inertial frame of reference. If the relative velocity between an observer (or his measuring instruments) and the observed object is zero, then the proper length formula_3 of the object can simply be determined by directly superposing a measuring rod. However, if the relative velocity > 0, then one can proceed as follows:
The observer installs a row of clocks that either are synchronized a) by exchanging light signals according to the Poincaré-Einstein synchronization, or b) by "slow clock transport", that is, one clock is transported along the row of clocks in the limit of vanishing transport velocity. Now, when the synchronization process is finished, the object is moved along the clock row and every clock stores the exact time when the left or the right end of the object passes by. After that, the observer only has to look after the position of a clock A that stored the time when the left end of the object was passing by, and a clock B at which the right end of the object was passing by "at the same time". It's clear that distance AB is equal to length formula_4 of the moving object. Using this method, the definition of simultaneity is crucial for measuring the length of moving objects.
Another method is to use a clock indicating its proper time formula_5, which is traveling from one endpoint of the rod to the other in time formula_6 as measured by clocks in the rod's rest frame. The length of the rod can be computed by multiplying its travel time by its velocity, thus formula_7 in the rod's rest frame or formula_8 in the clock's rest frame.
In Newtonian mechanics, simultaneity and time duration are absolute and therefore both methods lead to the equality of formula_4 and formula_3. Yet in relativity theory the constancy of light velocity in all inertial frames in connection with relativity of simultaneity and time dilation destroys this equality. In the first method an observer in one frame claims to have measured the object's endpoints simultaneously, but the observers in all other inertial frames will argue that the object's endpoints were "not" measured simultaneously. In the second method, times formula_6 and formula_5 are not equal due to time dilation, resulting in different lengths too.
The deviation between the measurements in all inertial frames is given by the formulas for Lorentz transformation and time dilation (see Derivation). It turns out, that the proper length remains unchanged and always denotes the greatest length of an object, yet the length of the same object as measured in another inertial frame is shorter than the proper length. This contraction only occurs in the line of motion, and can be represented by the following relation (where formula_13 is the relative velocity and formula_14 the speed of light)
Symmetry.
The principle of relativity (according to which the laws of nature must assume the same form in all inertial reference frames) requires that length contraction is symmetrical: If a rod rests in inertial frame S, it has its proper length in S and its length is contracted in S'. However, if a rod rests in S', it has its proper length in S' and its length is contracted in S. This can be vividly illustrated using symmetric Minkowski diagrams (or Loedel diagrams), because the Lorentz transformation geometrically corresponds to a rotation in four-dimensional spacetime.
"First image": If a rod at rest in S' is given, then its endpoints are located upon the ct' axis and the axis parallel to it. In this frame the simultaneous (parallel to the axis of x') positions of the endpoints are O and B, thus the "proper" length is given by OB. But in S the simultaneous (parallel to the axis of x) positions are O and A, thus the "contracted" length is given by OA.
On the other hand, if another rod is at rest in S, then its endpoints are located upon the ct axis and the axis parallel to it. In this frame the simultaneous (parallel to the axis of x) positions of the endpoints are O and D, thus the "proper" length is given by OD. But in S' the simultaneous (parallel to the axis of x') positions are O and C, thus the "contracted" length is given by OC.
"Second image": A train at rest in S and a station at rest in S' with relative velocity of formula_16 are given. In S a rod with proper length formula_17 is located, so its contracted length formula_18 in S' is given by:
Then the rod will be thrown out of the train in S and will come to rest at the station in S'. Its length has to be measured again according to the methods given above, and now the proper length formula_20 will be measured in S' (the rod has become larger in that system), while in S the rod is in motion and therefore its length is contracted (the rod has become smaller in that system):
Experimental verifications.
Any observer co-moving with the observed object cannot measure the object's contraction, because he can judge himself and the object as at rest in the same inertial frame in accordance with the principle of relativity (as it was demonstrated by the Trouton-Rankine experiment). So Length contraction cannot be measured in the object's rest frame, but only in a frame in which the observed object is in motion. In addition, even in such a non-co-moving frame, "direct" experimental confirmations of Length contraction are hard to achieve, because at the current state of technology, objects of considerable extension cannot be accelerated to relativistic speeds. And the only objects traveling with the speed required are atomic particles, yet whose spatial extensions are too small to allow a direct measurement of contraction.
However, there are "indirect" confirmations of this effect in a non-co-moving frame:
Reality of length contraction.
In 1911 Vladimir Varićak asserted that length contraction is "real" according to Lorentz, while it is "apparent or subjective" according to Einstein. Einstein replied:
Einstein also argued in that paper, that length contraction is not simply the product of "arbitrary" definitions concerning the way clock regulations and length measurements are performed. He presented the following thought experiment: Let A'B' and A"B" be the endpoints of two rods of same proper length. Let them move in opposite directions with same speed with respect to a resting coordinate x-axis. Endpoints A'A" meet at point A*, and B'B" meet at point B*, both points being marked on that axis. Einstein pointed out that length A*B* is shorter than A'B' or A"B", which can also be demonstrated by one of the rods when brought to rest with respect to that axis.
Paradoxes.
Due to superficial application of the contraction formula some paradoxes can occur. For examples see the Ladder paradox or Bell's spaceship paradox. However, those paradoxes can simply be solved by a correct application of relativity of simultaneity. Another famous paradox is the Ehrenfest paradox, which proves that the concept of rigid bodies is not compatible with relativity, reducing the applicability of Born rigidity, and showing that for a co-rotating observer the geometry is in fact non-euclidean.
Visual effects.
Length contraction refers to measurements of position made at simultaneous times according to a coordinate system. This could suggest that if one could take a picture of a fast moving object, that the image would show the object contracted in the direction of motion. However, such visual effects are completely different measurements, as such a photograph is taken from a distance, while length contraction can only directly be measured at the exact location of the object's endpoints. It was shown by several authors such as Roger Penrose and James Terrell that moving objects generally do not appear length contracted on a photograph. For instance, for a small angular diameter, a moving sphere remains circular and is rotated. This kind of visual rotation effect is called Penrose-Terrell rotation.
Derivation.
Lorentz transformation.
Length contraction can be derived from the Lorentz transformation in several ways:
Moving length is known.
In an inertial reference frame S, formula_23 and formula_24 shall denote the endpoints of an object in motion in this frame. There, its length formula_4 was measured according to the above convention by determining the simultaneous positions of its endpoints at formula_26. Now, the proper length of this object in S' shall be calculated by using the Lorentz transformation. Transforming the time coordinates from S into S' results in different times, but this is not problematic, as the object is at rest in S' where it does not matter when the endpoints are measured. Therefore, the transformation of the spatial coordinates suffices, which gives:
Since formula_26, and by setting formula_29 and formula_30, the proper length in S' is given by
with respect to which the measured length in S is contracted by
According to the relativity principle, objects that are at rest in S have to be contracted in S' as well. By exchanging the above signs and primes symmetrically, it follows:
Thus the contracted length as measured in S' is given by:
Proper length is known.
Conversely, if the object rests in S and its proper length is known, the simultaneity of the measurements at the object's endpoints has to be considered in another frame S', as the object constantly changes its position there. Therefore, both spatial and temporal coordinates must be transformed:
With formula_36 and formula_37 this results in non-simultaneous differences:
In order to obtain the simultaneous positions of both endpoints, the distance traveled by the second endpoint with formula_13 during formula_40 must be subtracted from formula_41:
So the moving length in S' is contracted. Likewise, the preceding calculation gives a symmetric result for an object at rest in S':
Time dilation.
Length contraction can also be derived from time dilation, according to which the rate of a single "moving" clock (indicating its proper time formula_5) is lower with respect to two synchronized "resting" clocks (indicating formula_6). Time dilation was experimentally confirmed multiple times, and is represented by the relation:
Suppose a rod of proper length formula_3 at rest in formula_48 and a clock at rest in formula_49 are moving along each other. The respective travel times of the clock between the rod's endpoints are given by formula_50 in formula_48 and formula_52 in formula_49, thus formula_54 and formula_55. By inserting the time dilation formula, the ratio between those lengths is:
Therefore, the length measured in formula_49 is given by
So the effect that the moving clock indicates a lower travel time in formula_48 due to time dilation, is interpreted in formula_49 as due to length contraction of the moving rod. Likewise, if the clock were at rest in formula_48 and the rod in formula_49, the above procedure would give
Geometrical considerations.
Additional geometrical considerations show, that length contraction can be regarded as a "trigonometric" phenomenon, with analogy to parallel slices through a cuboid before and after a "rotation" in E3 (see left half figure at the right). This is the Euclidean analog of "boosting" a cuboid in E1,2. In the latter case, however, we can interpret the boosted cuboid as the "world slab" of a moving plate.
"Image": Left: a "rotated cuboid" in three-dimensional euclidean space E3. The cross section is "longer" in the direction of the rotation than it was before the rotation. Right: the "world slab" of a moving thin plate in Minkowski spacetime (with one spatial dimension suppressed) E1,2, which is a "boosted cuboid". The cross section is "thinner" in the direction of the boost than it was before the boost. In both cases, the transverse directions are unaffected and the three planes meeting at each corner of the cuboids are "mutually orthogonal" (in the sense of E1,2 at right, and in the sense of E3 at left).
In special relativity, Poincaré transformations are a class of affine transformations which can be characterized as the transformations between alternative Cartesian coordinate charts on Minkowski spacetime corresponding to alternative states of inertial motion (and different choices of an origin). Lorentz transformations are Poincaré transformations which are linear transformations (preserve the origin). Lorentz transformations play the same role in Minkowski geometry (the Lorentz group forms the "isotropy group" of the self-isometries of the spacetime) which are played by rotations in euclidean geometry. Indeed, special relativity largely comes down to studying a kind of noneuclidean trigonometry in Minkowski spacetime, as suggested by the following table:

</doc>
<doc id="60828" url="https://en.wikipedia.org/wiki?curid=60828" title="Lepton">
Lepton

A lepton is an elementary, half-integer spin (spin ) particle that does not undergo strong interactions. Two main classes of leptons exist: charged leptons (also known as the "electron-like" leptons), and neutral leptons (better known as neutrinos). Charged leptons can combine with other particles to form various composite particles such as atoms and positronium, while neutrinos rarely interact with anything, and are consequently rarely observed. The best known of all leptons is the electron. 
There are six types of leptons, known as "flavours", forming three "generations". The first generation is the "electronic leptons", comprising the electron () and electron neutrino (); the second is the "muonic leptons", comprising the muon () and muon neutrino (); and the third is the "tauonic leptons", comprising the tau () and the tau neutrino (). Electrons have the least mass of all the charged leptons. The heavier muons and taus will rapidly change into electrons through a process of particle decay: the transformation from a higher mass state to a lower mass state. Thus electrons are stable and the most common charged lepton in the universe, whereas muons and taus can only be produced in high energy collisions (such as those involving cosmic rays and those carried out in particle accelerators).
Leptons have various intrinsic properties, including electric charge, spin, and mass. Unlike quarks however, leptons are not subject to the strong interaction, but they are subject to the other three fundamental interactions: gravitation, electromagnetism (excluding neutrinos, which are electrically neutral), and the weak interaction. For every lepton flavor there is a corresponding type of antiparticle, known as an antilepton, that differs from the lepton only in that some of its properties have equal magnitude but opposite sign. However, according to certain theories, neutrinos may be their own antiparticle, but it is not currently known whether this is the case or not.
The first charged lepton, the electron, was theorized in the mid-19th century by several scientists and was discovered in 1897 by J. J. Thomson. The next lepton to be observed was the muon, discovered by Carl D. Anderson in 1936, which was classified as a meson at the time. After investigation, it was realized that the muon did not have the expected properties of a meson, but rather behaved like an electron, only with higher mass. It took until 1947 for the concept of "leptons" as a family of particle to be proposed. The first neutrino, the electron neutrino, was proposed by Wolfgang Pauli in 1930 to explain certain characteristics of beta decay. It was first observed in the Cowan–Reines neutrino experiment conducted by Clyde Cowan and Frederick Reines in 1956. The muon neutrino was discovered in 1962 by Leon M. Lederman, Melvin Schwartz and Jack Steinberger, and the tau discovered between 1974 and 1977 by Martin Lewis Perl and his colleagues from the Stanford Linear Accelerator Center and Lawrence Berkeley National Laboratory. The tau neutrino remained elusive until July 2000, when the DONUT collaboration from Fermilab announced its discovery.
Leptons are an important part of the Standard Model. Electrons are one of the components of atoms, alongside protons and neutrons. Exotic atoms with muons and taus instead of electrons can also be synthesized, as well as lepton–antilepton particles such as positronium.
Etymology.
The name "lepton" comes from the Greek "leptós", "fine, small, thin" (neuter form: λεπτόν "leptón"); the earliest attested form of the word is the Mycenaean Greek , "re-po-to", written in Linear B syllabic script. "Lepton" was first used by physicist Léon Rosenfeld in 1948:
Following a suggestion of Prof. C. Møller, I adopt — as a pendant to "nucleon" — the denomination "lepton" (from λεπτός, small, thin, delicate) to denote a particle of small mass.
The etymology incorrectly implies that all the leptons are of small mass. When Rosenfeld named them, the only known leptons were electrons and muons, which are in fact of small mass — the mass of an electron () and the mass of a muon (with a value of ) are fractions of the mass of the "heavy" proton (). However, the mass of the tau (discovered in the mid 1970s) () is nearly twice that of the proton, and about 3,500 times that of the electron.
History.
The first lepton identified was the electron, discovered by J.J. Thomson and his team of British physicists in 1897. Then in 1930 Wolfgang Pauli postulated the electron neutrino to preserve conservation of energy, conservation of momentum, and conservation of angular momentum in beta decay. Pauli theorized that an undetected particle was carrying away the difference between the energy, momentum, and angular momentum of the initial and observed final particles. The electron neutrino was simply called the neutrino, as it was not yet known that neutrinos came in different flavours (or different "generations").
Nearly 40 years after the discovery of the electron, the muon was discovered by Carl D. Anderson in 1936. Due to its mass, it was initially categorized as a meson rather than a lepton. It later became clear that the muon was much more similar to the electron than to mesons, as muons do not undergo the strong interaction, and thus the muon was reclassified: electrons, muons, and the (electron) neutrino were grouped into a new group of particles – the leptons. In 1962 Leon M. Lederman, Melvin Schwartz and Jack Steinberger showed that more than one type of neutrino exists by first detecting interactions of the muon neutrino, which earned them the 1988 Nobel Prize, although by then the different flavours of neutrino had already been theorized.
The tau was first detected in a series of experiments between 1974 and 1977 by Martin Lewis Perl with his colleagues at the SLAC LBL group. Like the electron and the muon, it too was expected to have an associated neutrino. The first evidence for tau neutrinos came from the observation of "missing" energy and momentum in tau decay, analogous to the "missing" energy and momentum in beta decay leading to the discovery of the electron neutrino. The first detection of tau neutrino interactions was announced in 2000 by the DONUT collaboration at Fermilab, making it the latest particle of the Standard Model to have been directly observed, apart from the Higgs boson, which probably has been discovered in 2012.
Although all present data is consistent with three generations of leptons, some particle physicists are searching for a fourth generation. The current lower limit on the mass of such a fourth charged lepton is , while its associated neutrino would have a mass of at least .
Properties.
Spin and chirality.
Leptons are spin- particles. The spin-statistics theorem thus implies that they are fermions and thus that they are subject to the Pauli exclusion principle; no two leptons of the same species can be in exactly the same state at the same time. Furthermore, it means that a lepton can have only two possible spin states, namely up or down.
A closely related property is chirality, which in turn is closely related to a more easily visualized property called helicity. The helicity of a particle is the direction of its spin relative to its momentum; particles with spin in the same direction as their momentum are called "right-handed" and otherwise they are called "left-handed". When a particle is mass-less, the direction of its momentum relative to its spin is frame independent, while for massive particles it is possible to 'overtake' the particle by a Lorentz transformation flipping the helicity. Chirality is a technical property (defined through the transformation behaviour under the Poincaré group) that agrees with helicity for (approximately) massless particles and is still well defined for massive particles.
In many quantum field theories—such as quantum electrodynamics and quantum chromodynamics—left and right-handed fermions are identical. However in the Standard Model left-handed and right-handed fermions are treated asymmetrically. Only left-handed fermions participate in the weak interaction, while there are no right-handed neutrinos. This is an example of parity violation. In the literature left-handed fields are often denoted by a capital "L" subscript (e.g. L) and right-handed fields are denoted by a capital "R" subscript.
Electromagnetic interaction.
One of the most prominent properties of leptons is their electric charge, "Q". The electric charge determines the strength of their electromagnetic interactions. It determines the strength of the electric field generated by the particle (see Coulomb's law) and how strongly the particle reacts to an external electric or magnetic field (see Lorentz force). Each generation contains one lepton with "Q" = −"e" (conventionally the charge of a particle is expressed in units of the elementary charge) and one lepton with zero electric charge. The lepton with electric charge is commonly simply referred to as a 'charged lepton' while the neutral lepton is called a neutrino. For example the first generation consists of the electron with a negative electric charge and the electrically neutral electron neutrino .
In the language of quantum field theory the electromagnetic interaction of the charged leptons is expressed by the fact that the particles interact with the quantum of the electromagnetic field, the photon. The Feynman diagram of the electron-photon interaction is shown on the right.
Because leptons possess an intrinsic rotation in the form of their spin, charged leptons generate a magnetic field. The size of their magnetic dipole moment "μ" is given by,
where "m" is the mass of the lepton and "g" is the so-called g-factor for the lepton. First order approximation quantum mechanics predicts that the g-factor is 2 for all leptons. However, higher order quantum effects caused by loops in Feynman diagrams introduce corrections to this value. These corrections, referred to as the anomalous magnetic dipole moment, are very sensitive to the details of a quantum field theory model and thus provide the opportunity for precision tests of the standard model. The theoretical and measured values for the electron anomalous magnetic dipole moment are within agreement within eight significant figures.
Weak interaction.
In the Standard Model, the left-handed charged lepton and the left-handed neutrino are arranged in doublet that transforms in the spinor representation ("T" = ) of the weak isospin SU(2) gauge symmetry. This means that these particles are eigenstates of the isospin projection "T"3 with eigenvalues and − respectively. In the meantime, the right-handed charged lepton transforms as a weak isospin scalar ("T" = 0) and thus does not participate in the weak interaction, while there is no right-handed neutrino at all.
The Higgs mechanism recombines the gauge fields of the weak isospin SU(2) and the weak hypercharge U(1) symmetries to three massive vector bosons (, , ) mediating the weak interaction, and one massless vector boson, the photon, responsible for the electromagnetic interaction. The electric charge "Q" can be calculated from the isospin projection "T"3 and weak hypercharge "Y"W through the Gell-Mann–Nishijima formula,
To recover the observed electric charges for all particles the left-handed weak isospin doublet must thus have "Y"W = −1, while the right-handed isospin scalar e must have "Y"W = −2. The interaction of the leptons with the massive weak interaction vector bosons is shown in the figure on the left.
Mass.
In the Standard Model each lepton starts out with no intrinsic mass. The charged leptons (i.e. the electron, muon, and tau) obtain an effective mass through interaction with the Higgs field, but the neutrinos remain massless. For technical reasons the masslessness of the neutrinos implies that there is no mixing of the different generations of charged leptons as there is for quarks. This is in close agreement with current experimental observations.
However, it is known from experiments – most prominently from observed neutrino oscillations – that neutrinos do in fact have some very small mass, probably less than . This implies the existence of physics beyond the Standard Model. The currently most favoured extension is the so-called seesaw mechanism, which would explain both why the left-handed neutrinos are so light compared to the corresponding charged leptons, and why we have not yet seen any right-handed neutrinos.
Leptonic numbers.
The members of each generation's weak isospin doublet are assigned leptonic numbers that are conserved under the Standard Model. Electrons and electron neutrinos have an "electronic number" of "L"e = 1, while muons and muon neutrinos have a "muonic number" of "L"μ = 1, while tau particles and tau neutrinos have a "tauonic number" of "L"τ = 1. The antileptons have their respective generation's leptonic numbers of −1.
Conservation of the leptonic numbers means that the number of leptons of the same type remains the same, when particles interact. This implies that leptons and antileptons must be created in pairs of a single generation. For example, the following processes are allowed under conservation of leptonic numbers:
but not these:
However, neutrino oscillations are known to violate the conservation of the individual leptonic numbers. Such a violation is considered to be smoking gun evidence for physics beyond the Standard Model. A much stronger conservation law is the conservation of the total number of leptons ("L"), conserved even in the case of neutrino oscillations, but even it is still violated by a tiny amount by the chiral anomaly.
Universality.
The coupling of the leptons to gauge bosons are flavour-independent (i.e., the interactions between leptons and gauge bosons are the same for all leptons). This property is called "lepton universality" and has been tested in measurements of the tau and muon lifetimes and of Z boson partial decay widths, particularly at the Stanford Linear Collider (SLC) and Large Electron-Positron Collider (LEP) experiments.
The decay rate ("Γ") of muons through the process → + + is approximately given by an expression of the form (see muon decay for more details)
where "K"1 is some constant, and "G"F is the Fermi coupling constant. The decay rate of tau particles through the process → + + is given by an expression of the same form
where "K"2 is some constant. Muon–Tauon universality implies that "K"1 = "K"2. On the other hand, electron–muon universality implies
This explains why the branching ratios for the electronic mode (17.85%) and muonic (17.36%) mode of tau decay are equal (within error).
Universality also accounts for the ratio of muon and tau lifetimes. The lifetime of a lepton ("τ"l) is related to the decay rate by
where "B"(x → y) and "Γ"(x → y) denotes the branching ratios and the resonance width of the process x → y.
The ratio of tau and muon lifetime is thus given by
Using the values of the 2008 "Review of Particle Physics" for the branching ratios of muons and tau yields a lifetime ratio of ~, comparable to the measured lifetime ratio of ~. The difference is due to "K"1 and "K"2 not actually being constants; they depend on the mass of leptons.

</doc>
<doc id="1331789" url="https://en.wikipedia.org/wiki?curid=1331789" title="Lepton number">
Lepton number

In particle physics, the lepton number is a conserved quantum number representing the number of leptons minus the number of antileptons in an elementary particle reaction.
In equation form,
so all leptons have assigned a value of +1, antileptons −1, and non-leptonic particles 0. Lepton number (sometimes also called lepton charge) is an additive quantum number, which means that its sum is preserved in interactions (as opposed to multiplicative quantum numbers such as parity, where the product is preserved instead).
Lepton number was introduced in 1953 and was invoked to explain the absence of reactions such as formula_2 in the reactor Cowan–Reines neutrino experiment, which observed formula_3 instead.
Beside the leptonic number, leptonic family numbers are also defined: 
with the same assigning scheme as the leptonic number: +1 for particles of the corresponding family, −1 for the antiparticles, and 0 for leptons of other families or non-leptonic particles.
An example is the muon decay. Like many lepton interactions, muon decay is a Weak Interaction. This is cited as a test for special relativity testing the time dilation effect
Violations of the lepton number conservation laws.
In the Standard Model, leptonic family numbers (LF numbers) would be preserved if neutrinos were massless. Since neutrino oscillations have been observed, neutrinos do have a tiny nonzero mass and conservation laws for LF numbers are therefore only approximate. This means the conservation laws are violated, although because of the smallness of the neutrino mass they still hold to a very large degree for interactions containing charged leptons. However, the (total) lepton number conservation law must still hold (under the Standard Model). Thus, it is possible to see rare muon decays such as µ → eγ or µN→eN:
Because the lepton number conservation law in fact is violated by chiral anomalies, there are problems applying this symmetry universally over all energy scales. However, the quantum number "B" − "L" is much more likely to work and is seen in different models such as the Pati–Salam model.
Experiments such as MEGA and SINDRUM have searched for lepton number violation in muon decays to electrons; MEG set the current branching limit of order 10−13 and plans to lower to limit to 10−14 after 2016. Some BSM theories such as SUSY predict branching ratios of order 10−12 to 10−14. The Mu2e experiment in construction has a planned sensitivity of order 10−17.

</doc>
<doc id="17944" url="https://en.wikipedia.org/wiki?curid=17944" title="Lie algebra">
Lie algebra

In mathematics, a Lie algebra (, not ) is a vector space together with a non-associative multiplication called "Lie bracket" formula_1. It was introduced to study the concept of infinitesimal transformations. Hermann Weyl introduced the term "Lie algebra" (after Sophus Lie) in the 1930s. In older texts, the name "infinitesimal group" is used.
Lie algebras are closely related to Lie groups which are groups that are also smooth manifolds, with the property that the group operations of multiplication and inversion are smooth maps. Any Lie group gives rise to a Lie algebra. Conversely, to any finite-dimensional Lie algebra over real or complex numbers, there is a corresponding connected Lie group unique up to covering (Lie's third theorem). This correspondence between Lie groups and Lie algebras allows one to study Lie groups in terms of Lie algebras.
Definitions.
A Lie algebra is a vector space formula_2 over some field "F" together with a binary operation formula_3 called the Lie bracket that satisfies the following axioms:
Using bilinearity to expand the Lie bracket formula_10 and using alternativity shows that formula_11 for all elements "x", "y" in formula_5, showing that bilinearity and alternativity together imply 
It is customary to express a Lie algebra in lower-case fraktur, like formula_5. If a Lie algebra is associated with a Lie group, then the spelling of the Lie algebra is the same as that Lie group. For example, the Lie algebra of SU("n") is written as formula_15.
Generators and dimension.
Elements of a Lie algebra formula_5 are said to be generators of the Lie algebra if the smallest subalgebra of formula_5 containing them is formula_5 itself. The dimension of a Lie algebra is its dimension as a vector space over "F". The cardinality of a minimal generating set of a Lie algebra is always less than or equal to its dimension.
Subalgebras, ideals and homomorphisms.
The Lie bracket is not associative in general, meaning that formula_19 need not equal formula_20. Nonetheless, much of the terminology that was developed in the theory of associative rings or associative algebras is commonly applied to Lie algebras. A subspace formula_21 that is closed under the Lie bracket is called a Lie subalgebra. If a subspace formula_22 satisfies a stronger condition that
then "I" is called an ideal in the Lie algebra formula_5. A homomorphism between two Lie algebras (over the same base field) is a linear map that is compatible with the respective Lie brackets:
for all elements "x" and "y" in formula_5. As in the theory of associative rings, ideals are precisely the kernels of homomorphisms, given a Lie algebra formula_5 and an ideal "I" in it, one constructs the factor algebra formula_28, and the first isomorphism theorem holds for Lie algebras.
Let "S" be a subset of formula_5. The set of elements "x" such that formula_30 for all "s" in "S" forms a subalgebra called the centralizer of "S". The centralizer of formula_5 itself is called the center of formula_5. Similar to centralizers, if "S" is a subspace, then the set of "x" such that formula_33 is in "S" for all "s" in "S" forms a subalgebra called the normalizer of "S".
Direct sum and semidirect product.
Given two Lie algebras formula_5 and formula_35, their direct sum is the Lie algebra consisting of the vector space
formula_36, of the pairs formula_37, with the operation
Let formula_5 be a Lie algebra and formula_40 its ideal. If the canonical map formula_41 splits (i.e., admits a section), then formula_5 is said to be a semidirect product of formula_40 and formula_44.
Levi's theorem says that a finite-dimensional Lie algebra is a semidirect product of its radical and the complementary subalgebra (Levi subalgebra).
Properties.
Admits an enveloping algebra.
For any associative algebra "A" with multiplication formula_45, one can construct a Lie algebra "L"("A"). As a vector space, "L"("A") is the same as "A". The Lie bracket of two elements of "L"("A") is defined to be their commutator in "A":
The associativity of the multiplication * in "A" implies the Jacobi identity of the commutator in "L"("A"). For example, the associative algebra of "n" × "n" matrices over a field "F" gives rise to the general linear Lie algebra formula_47 The associative algebra "A" is called an enveloping algebra of the Lie algebra "L"("A"). Every Lie algebra can be embedded into one that arises from an associative algebra in this fashion; see universal enveloping algebra.
Representation.
Given a vector space "V", let formula_48 denote the Lie algebra enveloped by the associative algebra of all linear endomorphisms of "V". A representation of a Lie algebra formula_5 on "V" is a Lie algebra homomorphism
A representation is said to be faithful if its kernel is trivial. Every finite-dimensional Lie algebra has a faithful representation on a finite-dimensional vector space (Ado's theorem).
For example,
given by formula_52 is a representation of formula_5 on the vector space formula_5 called the adjoint representation. A derivation on the Lie algebra formula_5 (in fact on any non-associative algebra) is a linear map formula_56 that obeys the Leibniz' law, that is,
for all "x" and "y" in the algebra. For any "x", formula_58 is a derivation; a consequence of the Jacobi identity. Thus, the image of formula_59 lies in the subalgebra of formula_60 consisting of derivations on formula_5. A derivation that happens to be in the image of formula_59 is called an inner derivation. If formula_5 is semisimple, every derivation on formula_5 is inner.
Examples.
Three dimensions.
(The physicist convention for Lie algebras is used in the above equations, hence the factor of .) The Lie algebra formed by these operators have, in fact, representations of all finite dimensions.
Structure theory and classification.
Lie algebras can be classified to some extent. In particular, this has an application to the classification of Lie groups.
Abelian, nilpotent, and solvable.
Analogously to abelian, nilpotent, and solvable groups, defined in terms of the derived subgroups, one can define abelian, nilpotent, and solvable Lie algebras.
A Lie algebra formula_5 is abelian if the Lie bracket vanishes, i.e. ["x","y"] = 0, for all "x" and "y" in formula_5. Abelian Lie algebras correspond to commutative (or abelian) connected Lie groups such as vector spaces formula_82 or tori formula_83 and are all of the form formula_84 meaning an "n"-dimensional vector space with the trivial Lie bracket.
A more general class of Lie algebras is defined by the vanishing of all commutators of given length. A Lie algebra formula_5 is nilpotent if the lower central series
Lie rings need not be Lie groups under addition. Any Lie algebra is an example of a Lie ring. Any associative ring can be made into a Lie ring by defining a bracket operator formula_88. Conversely to any Lie algebra there is a corresponding ring, called the universal enveloping algebra.
Lie rings are used in the study of finite p-groups through the Lazard correspondence. The lower central factors of a "p"-group are finite abelian "p"-groups, so modules over Z/"p"Z. The direct sum of the lower central factors is given the structure of a Lie ring by defining the bracket to be the commutator of two coset representatives. The Lie ring structure is enriched with another module homomorphism, then "p"th power map, making the associated Lie ring a so-called restricted Lie ring.
Lie rings are also useful in the definition of a p-adic analytic groups and their endomorphisms by studying Lie algebras over rings of integers such as the p-adic integers. The definition of finite groups of Lie type due to Chevalley involves restricting from a Lie algebra over the complex numbers to a Lie algebra over the integers, and the reducing modulo "p" to get a Lie algebra over a finite field.

</doc>
<doc id="17945" url="https://en.wikipedia.org/wiki?curid=17945" title="Lie group">
Lie group

In mathematics, a Lie group is a group that is also a differentiable manifold, with the property that the group operations are compatible with the smooth structure. Lie groups are named after Sophus Lie, who laid the foundations of the theory of continuous transformation groups. The term "groupes de Lie" first appeared in French in 1893 in the thesis of Lie’s student Arthur Tresse, page 3.
Lie groups represent the best-developed theory of continuous symmetry of mathematical objects and structures, which makes them indispensable tools for many parts of contemporary mathematics, as well as for modern theoretical physics. They provide a natural framework for analysing the continuous symmetries of differential equations (differential Galois theory), in much the same way as permutation groups are used in Galois theory for analysing the discrete symmetries of algebraic equations. An extension of Galois theory to the case of continuous symmetry groups was one of Lie's principal motivations.
Overview.
Lie groups are smooth differentiable manifolds and as such can be studied using differential calculus, in contrast with the case of more general topological groups. One of the key ideas in the theory of Lie groups is to replace the "global" object, the group, with its "local" or linearized version, which Lie himself called its "infinitesimal group" and which has since become known as its Lie algebra.
Lie groups play an enormous role in modern geometry, on several different levels. Felix Klein argued in his Erlangen program that one can consider various "geometries" by specifying an appropriate transformation group that leaves certain geometric properties invariant. Thus Euclidean geometry corresponds to the choice of the group E(3) of distance-preserving transformations of the Euclidean space R3, conformal geometry corresponds to enlarging the group to the conformal group, whereas in projective geometry one is interested in the properties invariant under the projective group. This idea later led to the notion of a G-structure, where "G" is a Lie group of "local" symmetries of a manifold. On a "global" level, whenever a Lie group acts on a geometric object, such as a Riemannian or a symplectic manifold, this action provides a measure of rigidity and yields a rich algebraic structure. The presence of continuous symmetries expressed via a Lie group action on a manifold places strong constraints on its geometry and facilitates analysis on the manifold. Linear actions of Lie groups are especially important, and are studied in representation theory.
In the 1940s–1950s, Ellis Kolchin, Armand Borel, and Claude Chevalley realised that many foundational results concerning Lie groups can be developed completely algebraically, giving rise to the theory of algebraic groups defined over an arbitrary field. This insight opened new possibilities in pure algebra, by providing a uniform construction for most finite simple groups, as well as in algebraic geometry. The theory of automorphic forms, an important branch of modern number theory, deals extensively with analogues of Lie groups over adele rings; p-adic Lie groups play an important role, via their connections with Galois representations in number theory.
Definitions and examples.
A real Lie group is a group that is also a finite-dimensional real smooth manifold, in which the group operations of multiplication and inversion are smooth maps. Smoothness of the group multiplication
means that μ is a smooth mapping of the product manifold "G"×"G" into "G". These two requirements can be combined to the single requirement that the mapping
be a smooth mapping of the product manifold into "G".
First examples.
All of the previous examples of Lie groups fall within the class of classical groups.
Related concepts.
A complex Lie group is defined in the same way using complex manifolds rather than real ones (example: SL(2, C)), and similarly, using an alternate metric completion of Q, one can define a "p"-adic Lie group over the "p"-adic numbers, a topological group in which each point has a "p"-adic neighborhood. Hilbert's fifth problem asked whether replacing differentiable manifolds with topological or analytic ones can yield new examples. The answer to this question turned out to be negative: in 1952, Gleason, Montgomery and Zippin showed that if "G" is a topological manifold with continuous group operations, then there exists exactly one analytic structure on "G" which turns it into a Lie group (see also Hilbert–Smith conjecture). If the underlying manifold is allowed to be infinite-dimensional (for example, a Hilbert manifold), then one arrives at the notion of an infinite-dimensional Lie group. It is possible to define analogues of many Lie groups over finite fields, and these give most of the examples of finite simple groups.
The language of category theory provides a concise definition for Lie groups: a Lie group is a group object in the category of smooth manifolds. This is important, because it allows generalization of the notion of a Lie group to Lie supergroups.
More examples of Lie groups.
Lie groups occur in abundance throughout mathematics and physics. Matrix groups or algebraic groups are (roughly) groups of matrices (for example, orthogonal and symplectic groups), and these give most of the more common examples of Lie groups.
Constructions.
There are several standard ways to form new Lie groups from old ones:
Related notions.
Some examples of groups that are "not" Lie groups (except in the trivial sense that any group can be viewed as a 0-dimensional Lie group, with the discrete topology), are:
Basic concepts.
The Lie algebra associated with a Lie group.
To every Lie group we can associate a Lie algebra whose underlying vector space is the tangent space of the Lie group at the identity element and which completely captures the local structure of the group. Informally we can think of elements of the Lie algebra as elements of the group that are "infinitesimally close" to the identity, and the Lie bracket of the Lie algebra is related to the commutator of two such infinitesimal elements. Before giving the abstract definition we give a few examples:
The concrete definition given above is easy to work with, but has some minor problems: to use it we first need to represent a Lie group as a group of matrices, but not all Lie groups can be represented in this way, and it is not obvious that the Lie algebra is independent of the representation we use. To get around these problems we give 
the general definition of the Lie algebra of a Lie group (in 4 steps):
This Lie algebra formula_7 is finite-dimensional and it has the same dimension as the manifold "G". The Lie algebra of "G" determines "G" up to "local isomorphism", where two Lie groups are called locally isomorphic if they look the same near the identity element.
Problems about Lie groups are often solved by first solving the corresponding problem for the Lie algebras, and the result for groups then usually follows easily. 
For example, simple Lie groups are usually classified by first classifying the corresponding Lie algebras.
We could also define a Lie algebra structure on "Te" using right invariant vector fields instead of left invariant vector fields. This leads to the same Lie algebra, because the inverse map on "G" can be used to identify left invariant vector fields with right invariant vector fields, and acts as −1 on the tangent space "Te".
The Lie algebra structure on "Te" can also be described as follows:
the commutator operation
on "G" × "G" sends ("e", "e") to "e", so its derivative yields a bilinear operation on "TeG". This bilinear operation is actually the zero map, but the second derivative, under the proper identification of tangent spaces, yields an operation that satisfies the axioms of a Lie bracket, and it is equal to twice the one defined through left-invariant vector fields.
Homomorphisms and isomorphisms.
If "G" and "H" are Lie groups, then a Lie group homomorphism "f" : "G" → "H" is a smooth group homomorphism. In the case of complex Lie groups, such a homomorphism is required to be a holomorphic map. However, these requirements are a bit stringent; over real or complex numbers, every continuous homomorphism between Lie groups turns out to be (real or complex) analytic.
The composition of two Lie homomorphisms is again a homomorphism, and the class of all Lie groups, together with these morphisms, forms a category. Moreover, every Lie group homomorphism induces a homomorphism between the corresponding Lie algebras. Let formula_9 be a Lie group homomorphism and let formula_10 be its derivative at the identity. If we identify the Lie algebras of "G" and "H" with their tangent spaces at the identity elements then formula_10 is a map between the corresponding Lie algebras:
One can show that formula_10 is actually a Lie algebra homomorphism (meaning that it is a linear map which preserves the Lie bracket). In the language of category theory, we then have a covariant functor from the category of Lie groups to the category of Lie algebras which sends a Lie group to its Lie algebra and a Lie group homomorphism to its derivative at the identity.
Two Lie groups are called "isomorphic" if there exists a bijective homomorphism between them whose inverse is also a Lie group homomorphism. Equivalently, it is a diffeomorphism which is also a group homomorphism.
Ado's theorem says every finite-dimensional Lie algebra is isomorphic to a matrix Lie algebra. For every finite-dimensional matrix Lie algebra, there is a linear group (matrix Lie group) with this algebra as its Lie algebra. So every abstract Lie algebra is the Lie algebra of some (linear) Lie group.
The "global structure" of a Lie group is not determined by its Lie algebra; for example, if "Z" is any discrete subgroup of the center of "G" then "G" and "G"/"Z" have the same Lie algebra (see the table of Lie groups for examples). 
A "connected" Lie group is simple, semisimple, solvable, nilpotent, or abelian if and only if its Lie algebra has the corresponding property.
If we require that the Lie group be simply connected, then the global structure is determined by its Lie algebra: for every finite-dimensional Lie algebra formula_7 over F there is a simply connected Lie group "G" with formula_7 as Lie algebra, unique up to isomorphism. Moreover every homomorphism between Lie algebras lifts to a unique homomorphism between the corresponding simply connected Lie groups.
The exponential map.
The exponential map from the Lie algebra M("n", R) of the general linear group GL("n", R) to GL("n", R) is defined by the usual power series:
for matrices "A". If "G" is any subgroup of GL("n", R), then the exponential map takes the Lie algebra of "G" into "G", so we have an exponential map for all matrix groups.
The definition above is easy to use, but it is not defined for Lie groups that are not matrix groups, and it is not clear that the exponential map of a Lie group does not depend on its representation as a matrix group. We can solve both problems using a more abstract definition of the exponential map that works for all Lie groups, as follows.
Every vector "v" in formula_7 determines a linear map from R to formula_7 taking 1 to "v", which can be thought of as a Lie algebra homomorphism. Because R is the Lie algebra of the simply connected Lie group R, this induces a Lie group homomorphism "c" : R → "G" so that
for all "s" and "t". The operation on the right hand side is the group multiplication in "G". The formal similarity of this formula with the one valid for the exponential function justifies the definition
This is called the exponential map, and it maps the Lie algebra formula_7 into the Lie group "G". It provides a diffeomorphism between a neighborhood of 0 in formula_7 and a neighborhood of "e" in "G". This exponential map is a generalization of the exponential function for real numbers (because R is the Lie algebra of the Lie group of positive real numbers with multiplication), for complex numbers (because C is the Lie algebra of the Lie group of non-zero complex numbers with multiplication) and for matrices (because M("n", R) with the regular commutator is the Lie algebra of the Lie group GL("n", R) of all invertible matrices).
Because the exponential map is surjective on some neighbourhood "N" of "e", it is common to call elements of the Lie algebra infinitesimal generators of the group "G". The subgroup of "G" generated by "N" is the identity component of "G".
The exponential map and the Lie algebra determine the "local group structure" of every connected Lie group, because of the Baker–Campbell–Hausdorff formula: there exists a neighborhood "U" of the zero element of formula_7, such that for "u", "v" in "U" we have
where the omitted terms are known and involve Lie brackets of four or more elements. In case "u" and "v" commute, this formula reduces to the familiar exponential law .
The exponential map relates Lie group homomorphisms. That is, if formula_25 is a Lie group homomorphism and formula_26 the induced map on the corresponding Lie algebras, then for all formula_27 we have
In other words the following diagram commutes,
The exponential map from the Lie algebra to the Lie group is not always onto, even if the group is connected (though it does map onto the Lie group for connected groups that are either compact or nilpotent). For example, the exponential map of SL(2, R) is not surjective. Also, exponential map is not surjective nor injective for infinite-dimensional (see below) Lie groups modelled on "C"∞ Fréchet space, even from arbitrary small neighborhood of 0 to corresponding neighborhood of 1.
See also: derivative of the exponential map and normal coordinates.
Lie subgroup.
A Lie subgroup "H" of a Lie group "G" is a Lie group that is a subset of "G" and such that the inclusion map from "H" to "G" is an injective immersion and group homomorphism. According to Cartan's theorem, a closed subgroup of "G" admits a unique smooth structure which makes it an embedded Lie subgroup of "G"—i.e. a Lie subgroup such that the inclusion map is a smooth embedding.
Examples of non-closed subgroups are plentiful; for example take "G" to be a torus of dimension ≥ 2, and let "H" be a one-parameter subgroup of "irrational slope", i.e. one that winds around in "G". Then there is a Lie group homomorphism φ : R → "G" with "H" as its image. The closure of "H" will be a sub-torus in "G".
In terms of the exponential map of "G", in general, only some of the Lie subalgebras of the Lie algebra "g" of "G" correspond to closed Lie subgroups "H" of "G". There is no criterion solely based on the structure of "g" which determines which those are.
Early history.
According to the most authoritative source on the early history of Lie groups (Hawkins, p. 1), Sophus Lie himself considered the winter of 1873–1874 as the birth date of his theory of continuous groups. Hawkins, however, suggests that it was "Lie's prodigious research activity during the four-year period from the fall of 1869 to the fall of 1873" that led to the theory's creation ("ibid"). Some of Lie's early ideas were developed in close collaboration with Felix Klein. Lie met with Klein every day from October 1869 through 1872: in Berlin from the end of October 1869 to the end of February 1870, and in Paris, Göttingen and Erlangen in the subsequent two years ("ibid", p. 2). Lie stated that all of the principal results were obtained by 1884. But during the 1870s all his papers (except the very first note) were published in Norwegian journals, which impeded recognition of the work throughout the rest of Europe ("ibid", p. 76). In 1884 a young German mathematician, Friedrich Engel, came to work with Lie on a systematic treatise to expose his theory of continuous groups. From this effort resulted the three-volume "Theorie der Transformationsgruppen", published in 1888, 1890, and 1893.
Lie's ideas did not stand in isolation from the rest of mathematics. In fact, his interest in the geometry of differential equations was first motivated by the work of Carl Gustav Jacobi, on the theory of partial differential equations of first order and on the equations of classical mechanics. Much of Jacobi's work was published posthumously in the 1860s, generating enormous interest in France and Germany (Hawkins, p. 43). Lie's "idée fixe" was to develop a theory of symmetries of differential equations that would accomplish for them what Évariste Galois had done for algebraic equations: namely, to classify them in terms of group theory. Lie and other mathematicians showed that the most important equations for special functions and orthogonal polynomials tend to arise from group theoretical symmetries. In Lie's early work, the idea was to construct a theory of "continuous groups", to complement the theory of discrete groups that had developed in the theory of modular forms, in the hands of Felix Klein and Henri Poincaré. The initial application that Lie had in mind was to the theory of differential equations. On the model of Galois theory and polynomial equations, the driving conception was of a theory capable of unifying, by the study of symmetry, the whole area of ordinary differential equations. However, the hope that Lie Theory would unify the entire field of ordinary differential equations was not fulfilled. Symmetry methods for ODEs continue to be studied, but do not dominate the subject. There is a differential Galois theory, but it was developed by others, such as Picard and Vessiot, and it provides a theory of quadratures, the indefinite integrals required to express solutions.
Additional impetus to consider continuous groups came from ideas of Bernhard Riemann, on the foundations of geometry, and their further development in the hands of Klein. Thus three major themes in 19th century mathematics were combined by Lie in creating his new theory: the idea of symmetry, as exemplified by Galois through the algebraic notion of a group; geometric theory and the explicit solutions of differential equations of mechanics, worked out by Poisson and Jacobi; and the new understanding of geometry that emerged in the works of Plücker, Möbius, Grassmann and others, and culminated in Riemann's revolutionary vision of the subject.
Although today Sophus Lie is rightfully recognized as the creator of the theory of continuous groups, a major stride in the development of their structure theory, which was to have a profound influence on subsequent development of mathematics, was made by Wilhelm Killing, who in 1888 published the first paper in a series entitled "Die Zusammensetzung der stetigen endlichen Transformationsgruppen" ("The composition of continuous finite transformation groups") (Hawkins, p. 100). The work of Killing, later refined and generalized by Élie Cartan, led to classification of semisimple Lie algebras, Cartan's theory of symmetric spaces, and Hermann Weyl's description of representations of compact and semisimple Lie groups using highest weights.
In 1900 David Hilbert challenged Lie theorists with his Fifth Problem presented at the International Congress of Mathematicians in Paris.
Weyl brought the early period of the development of the theory of Lie groups to fruition, for not only did he classify irreducible representations of semisimple Lie groups and connect the theory of groups with quantum mechanics, but he also put Lie's theory itself on firmer footing by clearly enunciating the distinction between Lie's "infinitesimal groups" (i.e., Lie algebras) and the Lie groups proper, and began investigations of topology of Lie groups. The theory of Lie groups was systematically reworked in modern mathematical language in a monograph by Claude Chevalley.
The concept of a Lie group, and possibilities of classification.
Lie groups may be thought of as smoothly varying families of symmetries. Examples of symmetries include rotation about an axis. What must be understood is the nature of 'small' transformations, e.g., rotations through tiny angles, that link nearby transformations. The mathematical object capturing this structure is called a Lie algebra (Lie himself called them "infinitesimal groups"). It can be defined because Lie groups are manifolds, so have tangent spaces at each point.
The Lie algebra of any compact Lie group (very roughly: one for which the symmetries form a bounded set) can be decomposed as a direct sum of an abelian Lie algebra and some number of simple ones. The structure of an abelian Lie algebra is mathematically uninteresting (since the Lie bracket is identically zero); the interest is in the simple summands. Hence the question arises: what are the simple Lie algebras of compact groups? It turns out that they mostly fall into four infinite families, the "classical Lie algebras" A"n", B"n", C"n" and D"n", which have simple descriptions in terms of symmetries of Euclidean space. But there are also just five "exceptional Lie algebras" that do not fall into any of these families. E8 is the largest of these.
Lie groups are classified according to their algebraic properties (simple, semisimple, solvable, nilpotent, abelian), their connectedness (connected or simply connected) and their compactness.
The identity component of any Lie group is an open normal subgroup, and the quotient group is a discrete group. The universal cover of any connected Lie group is a simply connected Lie group, and conversely any connected Lie group is a quotient of a simply connected Lie group by a discrete normal subgroup of the center. Any Lie group "G" can be decomposed into discrete, simple, and abelian groups in a canonical way as follows. Write 
so that we have a sequence of normal subgroups
Then
This can be used to reduce some problems about Lie groups (such as finding their unitary representations) to the same problems for connected simple groups and nilpotent and solvable subgroups of smaller dimension.
Infinite-dimensional Lie groups.
Lie groups are often defined to be finite-dimensional, but there are many groups that resemble Lie groups, except for being infinite-dimensional. The simplest way to define infinite-dimensional Lie groups is to model them on Banach spaces, and in this case much of the basic theory is similar to that of finite-dimensional Lie groups. However this is inadequate for many applications, because many natural examples of infinite-dimensional Lie groups are not Banach manifolds. Instead one needs to define Lie groups modeled on more general locally convex topological vector spaces. In this case the relation between the Lie algebra and the Lie group becomes rather subtle, and several results about finite-dimensional Lie groups no longer hold.
The literature is not entirely uniform in its terminology as to exactly which properties of infinite-dimensional groups qualify the group for the prefix "Lie" in "Lie group". On the Lie algebra side of affairs, things are simpler since the qualifying criteria for the prefix "Lie" in "Lie algebra" are purely algebraic. For example, an infinite-dimensional Lie algebra may or may not have a corresponding Lie group. That is, there may be a group corresponding to the Lie algebra, but it might not be nice enough to be called a Lie group, or the connection between the group and the Lie algebra might not be nice enough (e.g failure of the exponential map to be onto a neighborhood of the identity). It is the "nice enough" that is not universally defined.
Some of the examples that have been studied include:
The diffeomorphism group of spacetime sometimes appears in attempts to quantize gravity.

</doc>
<doc id="297501" url="https://en.wikipedia.org/wiki?curid=297501" title="Lie superalgebra">
Lie superalgebra

In mathematics, a Lie superalgebra is a generalisation of a Lie algebra to include a Z2-grading. Lie superalgebras are important in theoretical physics where they are used to describe the mathematics of supersymmetry. In most of these theories, the "even" elements of the superalgebra correspond to bosons and "odd" elements to fermions (but this is not always true; for example, the BRST supersymmetry is the other way around).
Definition.
Formally, a Lie superalgebra is a (nonassociative) Z2-graded algebra, or "superalgebra", over a commutative ring (typically R or C) whose product [·, ·], called the Lie superbracket or supercommutator, satisfies the two conditions (analogs of the usual Lie algebra axioms, with grading):
Super skew-symmetry:
The super Jacobi identity:
where "x", "y", and "z" are pure in the Z2-grading. Here, |"x"| denotes the degree of "x" (either 0 or 1). The degree of [x,y] is the sum of degree of x and y modulo 2.
One also sometimes adds the axioms formula_3 for |"x"|=0 (if 2 is invertible this follows automatically) and formula_4 for |"x"|=1 (if 3 is invertible this follows automatically). When the ground ring is the integers or the Lie superalgebra is a free module, these conditions are equivalent to the condition that the [[Poincaré–Birkhoff–Witt theorem]] holds (and, in general, they are necessary conditions for the theorem to hold).
Just as for Lie algebras, the [[universal enveloping algebra]] of the Lie superalgebra can be given a [[Hopf algebra]] structure.
Distinction from graded Lie algebra.
A [[graded Lie algebra]] (say, graded by Z or N) that is anticommutative and Jacobi in the graded sense also has a formula_5 grading (which is called "rolling up" the algebra into odd and even parts), but is not referred to as "super". See [[Graded Lie algebra#note-0|note at graded Lie algebra]] for discussion.
Even and odd parts.
Note that the even subalgebra of a Lie superalgebra forms a (normal) Lie algebra as all the signs disappear, and the superbracket becomes a normal Lie bracket.
One way of thinking about a Lie superalgebra is to consider its even and odd parts, L0 and L1 separately. Then, L0 is a Lie algebra, L1 is a [[representation of a Lie algebra|linear representation]] of L0, and there exists a [[symmetric]] L0-[[equivariant]] [[linear map]] formula_6 such that for all x,y and z in L1,
Involution.
A * Lie superalgebra is a complex Lie superalgebra equipped with an [[Involution (mathematics)|involutive]] [[antilinear]] map from itself to itself which respects the Z2 grading and satisfies
[x,y]*=[y*,x*] for all x and y in the Lie superalgebra. (Some authors prefer the convention [x,y]*=(−1)|x||y|[y*,x*]; changing * to −* switches between the two conventions.) Its [[universal enveloping algebra]] would be an ordinary [[star-algebra|*-algebra]].
Examples.
Given any [[associative superalgebra]] "A" one can define the supercommutator on homogeneous elements by
and then extending by linearity to all elements. The algebra "A" together with the supercommutator then becomes a Lie superalgebra.
The [[Whitehead product]] on homotopy groups gives many examples of Lie superalgebras over the integers.
Classification.
The simple complex finite-dimensional Lie superalgebras were classified by [[Victor Kac]].
The basic classical compact Lie superalgebras (that are not Lie algebras) are: 
SU(m/n) These are the superunitary Lie algebras which have invariants:
This gives two orthosymplectic (see below) invariants if we take the m z variables and n w variables to be non-commuative and we take the real and imaginary parts. Therefore we have
SU(n/n)/U(1) A special case of the superunitary Lie algebras where we remove one U(1) generator to make the algebra simple.
OSp(m/2n) These are the Orthosymplectic groups. They have invariants given by:
for m commutative variables (x) and n pairs of anti-commuative variables (y,z). They are important symmetries in [[supergravity]] theories.
D(2/1;formula_12) This is a set of superalgebras parameterised by the variable formula_12. It has dimension 17 and is a sub-algebra of OSp(9|8). The even part of the group is O(3)xO(3)xO(3). So the invariants are:
for particular constants formula_16.
F(4)
This exceptional Lie superalgebra has dimension 40 and is a sub-algebra of OSp(24|16). The even part of the group is O(3)xSO(7) so three invariants are:
This group is related to the octonions by considering the 16 component spinors as two component octonion spinors and the gamma matrices acting on the upper indices as unit octonions. We then have formula_20 where f is the structure constants of octonion multiplication.
G(3)
This exceptional Lie superalgebra has dimension 31 and is a sub-algebra of OSp(17|14). The even part of the group is O(3)xG2. The invariants are similar to the above (it being a subalgebra of the F(4)?) so the first invariant is:
There are also two so-called strange series called p(n) and q(n).
Classification of infinite-dimensional simple linearly compact Lie superalgebras.
The classification consists of the 10 series W(m, n), S(m, n) ((m, n) ≠ (1, 1)), H(2m, n), K(2m + 1, n), HO(m, m) (m ≥ 2), SHO(m, m) (m ≥ 3), KO(m, m + 1), SKO(m, m + 1; β) (m ≥ 2), SHO∼(2m, 2m), SKO∼(2m + 1, 2m + 3) and the 5 exceptional algebras:
The last two are particularly interesting (according to Kac) because they have the standard model gauge group SU(3)xSU(2)xU(1) as their zero level algebra. Infinite-dimensional (affine) Lie superalgebras are important symmetries in [[superstring theory]].
Category-theoretic definition.
In [[category theory]], a Lie superalgebra can be defined as a nonassociative [[superalgebra]] whose product satisfies
where σ is the cyclic permutation braiding formula_24. In diagrammatic form:
External links.
[[Category:Supersymmetry]]
[[Category:Lie algebras]]

</doc>
<doc id="17939" url="https://en.wikipedia.org/wiki?curid=17939" title="Light">
Light

Light is electromagnetic radiation within a certain portion of the electromagnetic spectrum. The word usually refers to visible light, which is visible to the human eye and is responsible for the sense of sight. Visible light is usually defined as having wavelengths in the range of 400–700 nanometres (nm), or 4.00 × 10−7 to 7.00 × 10−7 m, between the infrared (with longer wavelengths) and the ultraviolet (with shorter wavelengths). This wavelength means a frequency range of roughly 430–750 terahertz (THz).
The main source of light on Earth is the Sun. Sunlight provides the energy that green plants use to create sugars mostly in the form of starches, which release energy into the living things that digest them. This process of photosynthesis provides virtually all the energy used by living things. Historically, another important source of light for humans has been fire, from ancient campfires to modern kerosene lamps. With the development of electric lights and power systems, electric lighting has effectively replaced firelight. Some species of animals generate their own light, a process called bioluminescence. For example, fireflies use light to locate mates, and vampire squids use it to hide themselves from prey.
The primary properties of visible light are intensity, propagation direction, frequency or wavelength spectrum, and polarisation, while its speed in a vacuum, 299 792 458 meters per second, is one of the fundamental constants of nature. Visible light, as with all types of electromagnetic radiation (EMR), is experimentally found to always move at this speed in a vacuum.
In physics, the term "light" sometimes refers to electromagnetic radiation of any wavelength, whether visible or not. In this sense, gamma rays, X-rays, microwaves and radio waves are also light. Like all types of light, visible light is emitted and absorbed in tiny "packets" called photons and exhibits properties of both waves and particles. This property is referred to as the wave–particle duality. The study of light, known as optics, is an important research area in modern physics.
Electromagnetic spectrum and visible light.
Generally, EM radiation, or EMR (the designation "radiation" excludes static electric and magnetic and near fields), is classified by wavelength into radio, microwave, infrared, the visible region that we perceive as light, ultraviolet, X-rays and gamma rays.
The behavior of EMR depends on its wavelength. Higher frequencies have shorter wavelengths, and lower frequencies have longer wavelengths. When EMR interacts with single atoms and molecules, its behavior depends on the amount of energy per quantum it carries.
EMR in the visible light region consists of quanta (called photons) that are at the lower end of the energies that are capable of causing electronic excitation within molecules, which leads to changes in the bonding or chemistry of the molecule. At the lower end of the visible light spectrum, EMR becomes invisible to humans (infrared) because its photons no longer have enough individual energy to cause a lasting molecular change (a change in conformation) in the visual molecule retinal in the human retina, which change triggers the sensation of vision.
There exist animals that are sensitive to various types of infrared, but not by means of quantum-absorption. Infrared sensing in snakes depends on a kind of natural thermal imaging, in which tiny packets of cellular water are raised in temperature by the infrared radiation. EMR in this range causes molecular vibration and heating effects, which is how these animals detect it.
Above the range of visible light, ultraviolet light becomes invisible to humans, mostly because it is absorbed by the cornea below 360 nanometers and the internal lens below 400. Furthermore, the rods and cones located in the retina of the human eye cannot detect the very short (below 360 nm) ultraviolet wavelengths and are in fact damaged by ultraviolet. Many animals with eyes that do not require lenses (such as insects and shrimp) are able to detect ultraviolet, by quantum photon-absorption mechanisms, in much the same chemical way that humans detect visible light.
Various sources define visible light as narrowly as 420 to 680 to as broadly as 380 to 800 nm. Under ideal laboratory conditions, people can see infrared up to at least 1050 nm; children and young adults may perceive ultraviolet wavelengths down to about 310 to 313 nm.
Speed of light.
The speed of light in a vacuum is defined to be exactly 299 792 458 m/s (approx. 186 282 miles per second). The fixed value of the speed of light in SI units results from the fact that the metre is now defined in terms of the speed of light. All forms of electromagnetic radiation move at exactly this same speed in vacuum.
Different physicists have attempted to measure the speed of light throughout history. Galileo attempted to measure the speed of light in the seventeenth century. An early experiment to measure the speed of light was conducted by Ole Rømer, a Danish physicist, in 1676. Using a telescope, Rømer observed the motions of Jupiter and one of its moons, Io. Noting discrepancies in the apparent period of Io's orbit, he calculated that light takes about 22 minutes to traverse the diameter of Earth's orbit. However, its size was not known at that time. If Rømer had known the diameter of the Earth's orbit, he would have calculated a speed of 227,000,000 m/s.
Another, more accurate, measurement of the speed of light was performed in Europe by Hippolyte Fizeau in 1849. Fizeau directed a beam of light at a mirror several kilometers away. A rotating cog wheel was placed in the path of the light beam as it traveled from the source, to the mirror and then returned to its origin. Fizeau found that at a certain rate of rotation, the beam would pass through one gap in the wheel on the way out and the next gap on the way back. Knowing the distance to the mirror, the number of teeth on the wheel, and the rate of rotation, Fizeau was able to calculate the speed of light as 313,000,000 m/s.
Léon Foucault used an experiment which used rotating mirrors to obtain a value of 298,000,000 m/s in 1862. Albert A. Michelson conducted experiments on the speed of light from 1877 until his death in 1931. He refined Foucault's methods in 1926 using improved rotating mirrors to measure the time it took light to make a round trip from Mount Wilson to Mount San Antonio in California. The precise measurements yielded a speed of 299,796,000 m/s.
The effective velocity of light in various transparent substances containing ordinary matter, is less than in vacuum. For example, the speed of light in water is about 3/4 of that in vacuum.
Two independent teams of physicists were said to bring light to a "complete standstill" by passing it through a Bose–Einstein condensate of the element rubidium, one team at Harvard University and the Rowland Institute for Science in Cambridge, Mass., and the other at the Harvard–Smithsonian Center for Astrophysics, also in Cambridge. However, the popular description of light being "stopped" in these experiments refers only to light being stored in the excited states of atoms, then re-emitted at an arbitrary later time, as stimulated by a second laser pulse. During the time it had "stopped" it had ceased to be light.
Optics.
The study of light and the interaction of light and matter is termed optics. The observation and study of optical phenomena such as rainbows and the aurora borealis offer many clues as to the nature of light.
Refraction.
Refraction is the bending of light rays when passing through a surface between one transparent material and another. It is described by Snell's Law:
where formula_2 is the angle between the ray and the surface normal in the first medium, formula_3 is the angle between the ray and the surface normal in the second medium, and n1 and n2 are the indices of refraction, "n" = 1 in a vacuum and "n" > 1 in a transparent substance.
When a beam of light crosses the boundary between a vacuum and another medium, or between two different media, the wavelength of the light changes, but the frequency remains constant. If the beam of light is not orthogonal (or rather normal) to the boundary, the change in wavelength results in a change in the direction of the beam. This change of direction is known as refraction.
The refractive quality of lenses is frequently used to manipulate light in order to change the apparent size of images. Magnifying glasses, spectacles, contact lenses, microscopes and refracting telescopes are all examples of this manipulation.
Light sources.
There are many sources of light. The most common light sources are thermal: a body at a given temperature emits a characteristic spectrum of black-body radiation. A simple thermal source is sunlight, the radiation emitted by the chromosphere of the Sun at around 6,000 Kelvin peaks in the visible region of the electromagnetic spectrum when plotted in wavelength units and roughly 44% of sunlight energy that reaches the ground is visible. Another example is incandescent light bulbs, which emit only around 10% of their energy as visible light and the remainder as infrared. A common thermal light source in history is the glowing solid particles in flames, but these also emit most of their radiation in the infrared, and only a fraction in the visible spectrum. The peak of the blackbody spectrum is in the deep infrared, at about 10 micrometer wavelength, for relatively cool objects like human beings. As the temperature increases, the peak shifts to shorter wavelengths, producing first a red glow, then a white one, and finally a blue-white colour as the peak moves out of the visible part of the spectrum and into the ultraviolet. These colours can be seen when metal is heated to "red hot" or "white hot". Blue-white thermal emission is not often seen, except in stars (the commonly seen pure-blue colour in a gas flame or a welder's torch is in fact due to molecular emission, notably by CH radicals (emitting a wavelength band around 425 nm, and is not seen in stars or pure thermal radiation).
Atoms emit and absorb light at characteristic energies. This produces "emission lines" in the spectrum of each atom. Emission can be spontaneous, as in light-emitting diodes, gas discharge lamps (such as neon lamps and neon signs, mercury-vapor lamps, etc.), and flames (light from the hot gas itself—so, for example, sodium in a gas flame emits characteristic yellow light). Emission can also be stimulated, as in a laser or a microwave maser.
Deceleration of a free charged particle, such as an electron, can produce visible radiation: cyclotron radiation, synchrotron radiation, and bremsstrahlung radiation are all examples of this. Particles moving through a medium faster than the speed of light in that medium can produce visible Cherenkov radiation.
Certain chemicals produce visible radiation by chemoluminescence. In living things, this process is called bioluminescence. For example, fireflies produce light by this means, and boats moving through water can disturb plankton which produce a glowing wake.
Certain substances produce light when they are illuminated by more energetic radiation, a process known as fluorescence. Some substances emit light slowly after excitation by more energetic radiation. This is known as phosphorescence.
Phosphorescent materials can also be excited by bombarding them with subatomic particles. Cathodoluminescence is one example. This mechanism is used in cathode ray tube television sets and computer monitors.
Certain other mechanisms can produce light:
When the concept of light is intended to include very-high-energy photons (gamma rays), additional generation mechanisms include:
Units and measures.
Light is measured with two main alternative sets of units: radiometry consists of measurements of light power at all wavelengths, while photometry measures light with wavelength weighted with respect to a standardised model of human brightness perception. Photometry is useful, for example, to quantify Illumination (lighting) intended for human use. The SI units for both systems are summarised in the following tables.
The photometry units are different from most systems of physical units in that they take into account how the human eye responds to light. The cone cells in the human eye are of three types which respond differently across the visible spectrum, and the cumulative response peaks at a wavelength of around 555 nm. Therefore, two sources of light which produce the same intensity (W/m2) of visible light do not necessarily appear equally bright. The photometry units are designed to take this into account, and therefore are a better representation of how "bright" a light appears to be than raw intensity. They relate to raw power by a quantity called luminous efficacy, and are used for purposes like determining how to best achieve sufficient illumination for various tasks in indoor and outdoor settings. The illumination measured by a photocell sensor does not necessarily correspond to what is perceived by the human eye, and without filters which may be costly, photocells and charge-coupled devices (CCD) tend to respond to some infrared, ultraviolet or both.
Light pressure.
Light exerts physical pressure on objects in its path, a phenomenon which can be deduced by Maxwell's equations, but can be more easily explained by the particle nature of light: photons strike and transfer their momentum. Light pressure is equal to the power of the light beam divided by "c", the speed of light. Due to the magnitude of "c", the effect of light pressure is negligible for everyday objects. For example, a one-milliwatt laser pointer exerts a force of about 3.3 piconewtons on the object being illuminated; thus, one could lift a U.S. penny with laser pointers, but doing so would require about 30 billion 1-mW laser pointers. However, in nanometre-scale applications such as nanoelectromechanical systems (|NEMS), the effect of light pressure is more significant, and exploiting light pressure to drive NEMS mechanisms and to flip nanometre-scale physical switches in integrated circuits is an active area of research.
At larger scales, light pressure can cause asteroids to spin faster, acting on their irregular shapes as on the vanes of a windmill. The possibility of making solar sails that would accelerate spaceships in space is also under investigation.
Although the motion of the Crookes radiometer was originally attributed to light pressure, this interpretation is incorrect; the characteristic Crookes rotation is the result of a partial vacuum. This should not be confused with the Nichols radiometer, in which the (slight) motion caused by torque (though not enough for full rotation against friction) "is" directly caused by light pressure.
Historical theories about light, in chronological order.
Classical Greece and Hellenism.
In the fifth century BC, Empedocles postulated that everything was composed of four elements; fire, air, earth and water. He believed that Aphrodite made the human eye out of the four elements and that she lit the fire in the eye which shone out from the eye making sight possible. If this were true, then one could see during the night just as well as during the day, so Empedocles postulated an interaction between rays from the eyes and rays from a source such as the sun.
In about 300 BC, Euclid wrote "Optica", in which he studied the properties of light. Euclid postulated that light travelled in straight lines and he described the laws of reflection and studied them mathematically. He questioned that sight is the result of a beam from the eye, for he asks how one sees the stars immediately, if one closes one's eyes, then opens them at night. Of course if the beam from the eye travels infinitely fast this is not a problem.
In 55 BC, Lucretius, a Roman who carried on the ideas of earlier Greek atomists, wrote:
"The light & heat of the sun; these are composed of minute atoms which, when they are shoved off, lose no time in shooting right across the interspace of air in the direction imparted by the shove." – "On the nature of the Universe"
Despite being similar to later particle theories, Lucretius's views were not generally accepted.
Ptolemy (c. 2nd century) wrote about the refraction of light in his book "Optics".
Classical India.
In ancient India, the Hindu schools of Samkhya and Vaisheshika, from around the early centuries AD developed theories on light. According to the Samkhya school, light is one of the five fundamental "subtle" elements ("tanmatra") out of which emerge the gross elements. The atomicity of these elements is not specifically mentioned and it appears that they were actually taken to be continuous.
On the other hand, the Vaisheshika school gives an atomic theory of the physical world on the non-atomic ground of ether, space and time. (See "Indian atomism".) The basic atoms are those of earth ("prthivi"), water ("pani"), fire ("agni"), and air ("vayu") Light rays are taken to be a stream of high velocity of "tejas" (fire) atoms. The particles of light can exhibit different characteristics depending on the speed and the arrangements of the "tejas" atoms.
The "Vishnu Purana" refers to sunlight as "the seven rays of the sun".
The Indian Buddhists, such as Dignāga in the 5th century and Dharmakirti in the 7th century, developed a type of atomism that is a philosophy about reality being composed of atomic entities that are momentary flashes of light or energy. They viewed light as being an atomic entity equivalent to energy.
Descartes.
René Descartes (1596–1650) held that light was a mechanical property of the luminous body, rejecting the "forms" of Ibn al-Haytham and Witelo as well as the "species" of Bacon, Grosseteste, and Kepler. In 1637 he published a theory of the refraction of light that assumed, incorrectly, that light travelled faster in a denser medium than in a less dense medium. Descartes arrived at this conclusion by analogy with the behaviour of sound waves. Although Descartes was incorrect about the relative speeds, he was correct in assuming that light behaved like a wave and in concluding that refraction could be explained by the speed of light in different media.
Descartes is not the first to use the mechanical analogies but because he clearly asserts that light is only a mechanical property of the luminous body and the transmitting medium, Descartes' theory of light is regarded as the start of modern physical optics.
Particle theory.
Pierre Gassendi (1592–1655), an atomist, proposed a particle theory of light which was published posthumously in the 1660s. Isaac Newton studied Gassendi's work at an early age, and preferred his view to Descartes' theory of the "plenum". He stated in his "Hypothesis of Light" of 1675 that light was composed of corpuscles (particles of matter) which were emitted in all directions from a source. One of Newton's arguments against the wave nature of light was that waves were known to bend around obstacles, while light travelled only in straight lines. He did, however, explain the phenomenon of the diffraction of light (which had been observed by Francesco Grimaldi) by allowing that a light particle could create a localised wave in the aether.
Newton's theory could be used to predict the reflection of light, but could only explain refraction by incorrectly assuming that light accelerated upon entering a denser medium because the gravitational pull was greater. Newton published the final version of his theory in his "Opticks" of 1704. His reputation helped the particle theory of light to hold sway during the 18th century. The particle theory of light led Laplace to argue that a body could be so massive that light could not escape from it. In other words, it would become what is now called a black hole. Laplace withdrew his suggestion later, after a wave theory of light became firmly established as the model for light (as has been explained, neither a particle or wave theory is fully correct). A translation of Newton's essay on light appears in "The large scale structure of space-time," by Stephen Hawking and George F. R. Ellis.
The fact that light could be polarized was for the first time qualitatively explained by Newton using the particle theory. Étienne-Louis Malus in 1810 created a mathematical particle theory of polarization. Jean-Baptiste Biot in 1812 showed that this theory explained all known phenomena of light polarization. At that time the polarization was considered as the proof of the particle theory.
Wave theory.
To explain the origin of colors, Robert Hooke (1635-1703) developed a "pulse theory" and compared the spreading of light to that of waves in water in his 1665 Micrographia ("Observation XI"). In 1672 Hooke suggested that light's vibrations could be perpendicular to the direction of propagation. Christiaan Huygens (1629-1695) worked out a mathematical wave theory of light in 1678, and published it in his "Treatise on light" in 1690. He proposed that light was emitted in all directions as a series of waves in a medium called the "Luminiferous ether". As waves are not affected by gravity, it was assumed that they slowed down upon entering a denser medium.
The wave theory predicted that light waves could interfere with each other like sound waves (as noted around 1800 by Thomas Young). Young showed by means of a diffraction experiment that light behaved as waves. He also proposed that different colours were caused by different wavelengths of light, and explained colour vision in terms of three-coloured receptors in the eye.
Another supporter of the wave theory was Leonhard Euler. He argued in "Nova theoria lucis et colorum" (1746) that diffraction could more easily be explained by a wave theory.
In 1815 Ampere gave Fresnel an idea that the polarization of light can be explained by the wave theory if light were a transverse wave.
Later, Augustin-Jean Fresnel independently worked out his own wave theory of light, and presented it to the Académie des Sciences in 1817. Siméon Denis Poisson added to Fresnel's mathematical work to produce a convincing argument in favour of the wave theory, helping to overturn Newton's corpuscular theory. By the year 1821, Fresnel was able to show via mathematical methods that polarisation could be explained by the wave theory of light and only if light was entirely transverse, with no longitudinal vibration whatsoever.
The weakness of the wave theory was that light waves, like sound waves, would need a medium for transmission. The existence of the hypothetical substance "luminiferous aether" proposed by Huygens in 1678 was cast into strong doubt in the late nineteenth century by the Michelson–Morley experiment.
Newton's corpuscular theory implied that light would travel faster in a denser medium, while the wave theory of Huygens and others implied the opposite. At that time, the speed of light could not be measured accurately enough to decide which theory was correct. The first to make a sufficiently accurate measurement was Léon Foucault, in 1850. His result supported the wave theory, and the classical particle theory was finally abandoned, only to partly re-emerge in the 20th century.
Electromagnetic theory as explanation for all types of visible light and all EM radiation.
In 1845, Michael Faraday discovered that the plane of polarisation of linearly polarised light is rotated when the light rays travel along the magnetic field direction in the presence of a transparent dielectric, an effect now known as Faraday rotation. This was the first evidence that light was related to electromagnetism. In 1846 he speculated that light might be some form of disturbance propagating along magnetic field lines. Faraday proposed in 1847 that light was a high-frequency electromagnetic vibration, which could propagate even in the absence of a medium such as the ether.
Faraday's work inspired James Clerk Maxwell to study electromagnetic radiation and light. Maxwell discovered that self-propagating electromagnetic waves would travel through space at a constant speed, which happened to be equal to the previously measured speed of light. From this, Maxwell concluded that light was a form of electromagnetic radiation: he first stated this result in 1862 in "On Physical Lines of Force". In 1873, he published "A Treatise on Electricity and Magnetism", which contained a full mathematical description of the behaviour of electric and magnetic fields, still known as Maxwell's equations. Soon after, Heinrich Hertz confirmed Maxwell's theory experimentally by generating and detecting radio waves in the laboratory, and demonstrating that these waves behaved exactly like visible light, exhibiting properties such as reflection, refraction, diffraction, and interference. Maxwell's theory and Hertz's experiments led directly to the development of modern radio, radar, television, electromagnetic imaging, and wireless communications.
In the quantum theory, photons are seen as wave packets of the waves described in the classical theory of Maxwell. The quantum theory was needed to explain effects even with visual light that Maxwell's classical theory could not (such as spectral lines).
Quantum theory.
In 1900 Max Planck, attempting to explain black body radiation suggested that although light was a wave, these waves could gain or lose energy only in finite amounts related to their frequency. Planck called these "lumps" of light energy "quanta" (from a Latin word for "how much"). In 1905, Albert Einstein used the idea of light quanta to explain the photoelectric effect, and suggested that these light quanta had a "real" existence. In 1923 Arthur Holly Compton showed that the wavelength shift seen when low intensity X-rays scattered from electrons (so called Compton scattering) could be explained by a particle-theory of X-rays, but not a wave theory. In 1926 Gilbert N. Lewis named these light quanta particles photons.
Eventually the modern theory of quantum mechanics came to picture light as (in some sense) "both" a particle and a wave, and (in another sense), as a phenomenon which is "neither" a particle nor a wave (which actually are macroscopic phenomena, such as baseballs or ocean waves). Instead, modern physics sees light as something that can be described sometimes with mathematics appropriate to one type of macroscopic metaphor (particles), and sometimes another macroscopic metaphor (water waves), but is actually something that cannot be fully imagined. As in the case for radio waves and the X-rays involved in Compton scattering, physicists have noted that electromagnetic radiation tends to behave more like a classical wave at lower frequencies, but more like a classical particle at higher frequencies, but never completely loses all qualities of one or the other. Visible light, which occupies a middle ground in frequency, can easily be shown in experiments to be describable using either a wave or particle model, or sometimes both.

</doc>
<doc id="285773" url="https://en.wikipedia.org/wiki?curid=285773" title="Limit of a sequence">
Limit of a sequence

As the positive integer "n" becomes larger and larger, the value "n" sin(1/"n") becomes arbitrarily close to 1. We say that "the limit of the sequence "n" sin(1/"n") equals 1."
In mathematics, the limit of a sequence is the value that the terms of a sequence "tend to". If such a limit exists, the sequence is called convergent. A sequence which does not converge is said to be divergent. The limit of a sequence is said to be the fundamental notion on which the whole of analysis ultimately rests.
Limits can be defined in any metric or topological space, but are usually first encountered in the real numbers.
History.
The Greek philosopher Zeno of Elea is famous for formulating paradoxes that involve limiting processes.
Leucippus, Democritus, Antiphon, Eudoxus and Archimedes developed the method of exhaustion, which uses an infinite sequence of approximations to determine an area or a volume. Archimedes succeeded in summing what is now called a geometric series.
Newton dealt with series in his works on "Analysis with infinite series" (written in 1669, circulated in manuscript, published in 1711), "Method of fluxions and infinite series" (written in 1671, published in English translation in 1736, Latin original published much later) and "Tractatus de Quadratura Curvarum" (written in 1693, published in 1704 as an Appendix to his "Optiks"). In the latter work, Newton considers the binomial expansion of ("x"+"o")"n" which he then linearizes by "taking limits" (letting "o"→0).
In the 18th century, mathematicians such as Euler succeeded in summing some "divergent" series by stopping at the right moment; they did not much care whether a limit existed, as long as it could be calculated. At the end of the century, Lagrange in his "Théorie des fonctions analytiques" (1797) opined that the lack of rigour precluded further development in calculus. Gauss in his etude of hypergeometric series (1813) for the first time rigorously investigated under which conditions a series converged to a limit.
The modern definition of a limit (for any ε there exists an index "N" so that ...) was given by Bernhard Bolzano ("Der binomische Lehrsatz", Prague 1816, little noticed at the time) and by Karl Weierstrass in the 1870s.
Real numbers.
In the real numbers, a number formula_1 is the limit of the sequence formula_2 if the numbers in the sequence become closer and closer to formula_1 and not to any other number.
Formal definition.
We call formula_20 the limit of the sequence formula_2 if the following condition holds:
In other words, for every measure of closeness formula_26, the sequence's terms are eventually that close to the limit. The sequence formula_2 is said to converge to or tend to the limit formula_20, written formula_29 or formula_30.
If a sequence converges to some limit, then it is convergent; otherwise it is divergent.
Properties.
Limits of sequences behave well with respect to the usual arithmetic operations. If formula_31 and formula_32, then formula_33, formula_34 and, if neither "b" nor any formula_35 is zero, formula_36.
For any continuous function "f", if formula_29 then formula_38. In fact, any real-valued function "f" is continuous if and only if it preserves the limits of sequences (though this is not necessarily true when using more general notions of continuity).
Some other important properties of limits of real sequences include the following.
These properties are extensively used to prove limits without the need to directly use the cumbersome formal definition. Once proven that formula_53 it becomes easy to show that formula_54, (formula_55), using the properties above.
Infinite limits.
A sequence formula_2 is said to tend to infinity, written formula_57 or formula_58 if, for every "K", there is an "N" such that, for every formula_59, formula_60; that is, the sequence terms are eventually larger than any fixed "K". Similarly, formula_61 if, for every "K", there is an "N" such that, for every formula_59, formula_63. If a sequence tends to infinity, or to minus infinity, then it is divergent (however, a divergent sequence need not tend to plus or minus infinity).
Metric spaces.
Definition.
A point "x" of the metric space ("X", "d") is the limit of the sequence ("xn") if, for all ε > 0, there is an "N" such that, for every formula_59, formula_65. This coincides with the definition given for real numbers when formula_66 and formula_67.
Properties.
For any continuous function "f", if formula_29 then formula_38. In fact, a function "f" is continuous if and only if it preserves the limits of sequences.
Limits of sequences are unique when they exist, as distinct points are separated by some positive distance, so for formula_26 less than half this distance, sequence terms cannot be within a distance formula_26 of both points.
Topological spaces.
Definition.
A point "x" of the topological space ("X", τ) is the limit of the sequence ("xn") if, for every neighbourhood "U" of "x", there is an "N" such that, for every formula_59, formula_73. This coincides with the definition given for metric spaces if ("X","d") is a metric space and formula_74 is the topology generated by "d".
The limit of a sequence of points formula_75 in a topological space "T" is a special case of the limit of a function: the domain is formula_76 in the space formula_77 with the induced topology of the affinely extended real number system, the range is "T", and the function argument "n" tends to +∞, which in this space is a limit point of formula_76.
Properties.
If "X" is a Hausdorff space then limits of sequences are unique where they exist. Note that this need not be the case in general; in particular, if two points "x" and "y" are topologically indistinguishable, any sequence that converges to "x" must converge to "y" and vice versa.
Cauchy sequences.
A Cauchy sequence is a sequence whose terms become arbitrarily close together as n gets very large. The notion of a Cauchy sequence is important in the study of sequences in metric spaces, and, in particular, in real analysis. One particularly important result in real analysis is "Cauchy characterization of convergence for sequences": 
Definition in hyperreal numbers.
The definition of the limit using the hyperreal numbers formalizes the intuition that for a "very large" value of the index, the corresponding term is "very close" to the limit. More precisely, a real sequence formula_2 tends to "L" if for every infinite hypernatural "H", the term "x""H" is infinitely close to "L", i.e., the difference "x""H" - "L" is infinitesimal. Equivalently, "L" is the standard part of "x""H"
Thus, the limit can be defined by the formula
where the limit exists if and only if the righthand side is independent of the choice of an infinite "H".

</doc>
<doc id="21245414" url="https://en.wikipedia.org/wiki?curid=21245414" title="Line integral">
Line integral

In mathematics, a line integral is an integral where the function to be integrated is evaluated along a curve. The terms path integral, curve integral, and curvilinear integral are also used; contour integral as well, although that is typically reserved for line integrals in the complex plane.
The function to be integrated may be a scalar field or a vector field. The value of the line integral is the sum of values of the field at all points on the curve, weighted by some scalar function on the curve (commonly arc length or, for a vector field, the scalar product of the vector field with a differential vector in the curve). This weighting distinguishes the line integral from simpler integrals defined on intervals. Many simple formulae in physics (for example, ) have natural continuous analogs in terms of line integrals (). The line integral finds the work done on an object moving through an electric or gravitational field, for example.
Vector calculus.
In qualitative terms, a line integral in vector calculus can be thought of as a measure of the total effect of a given field along a given curve. More specifically, the line integral over a scalar field can be interpreted as the area under the field carved out by a particular curve. This can be visualized as the surface created by "z" = "f"("x","y") and a curve "C" in the "x"-"y" plane. The line integral of "f" would be the area of the "curtain" created when the points of the surface that are directly over "C" are carved out.
Line integral of a scalar field.
Definition.
For some scalar field "f" : "U" ⊆ R"n" → R, the line integral along a piecewise smooth curve "C" ⊂ "U" is defined as
where r: [a, b] → "C" is an arbitrary bijective parametrization of the curve "C" such that r("a") and r("b") give the endpoints of "C" and formula_2.
The function "f" is called the integrand, the curve "C" is the domain of integration, and the symbol "ds" may be intuitively interpreted as an elementary arc length. Line integrals of scalar fields over a curve "C" do not depend on the chosen parametrization r of "C".
Geometrically, when the scalar field "f" is defined over a plane ("n"=2), its graph is a surface "z"="f"("x","y") in space, and the line integral gives the (signed) cross-sectional area bounded by the curve "C" and the graph of "f". See the animation to the right.
Derivation.
For a line integral over a scalar field, the integral can be constructed from a Riemann sum using the above definitions of "f", "C" and a parametrization r of "C". This can be done by partitioning the interval ["a","b"] into "n" sub-intervals ["t""i-1", "t""i"] of length Δ"t" = ("b" − "a")/"n", then r("t""i") denotes some point, call it a sample point, on the curve "C". We can use the set of sample points {r("t""i") : 1 ≤ i ≤ n} to approximate the curve "C" by a polygonal path by introducing a straight line piece between each of the sample points r("t""i-1") and r("t""i"). We then label the distance between each of the sample points on the curve as Δ"s""i". The product of "f"(r("t""i")) and Δ"s""i" can be associated with the signed area of a rectangle with a height and width of "f"(r("t""i")) and Δ"s""i" respectively. Taking the limit of the sum of the terms as the length of the partitions approaches zero gives us formula_3
We note that, by the mean value theorem, the distance between subsequent points on the curve, is
Substituting this in the above Riemann sum yields
which is the Riemann sum for the integral
Line integral of a vector field.
Definition.
For a vector field F : "U" ⊆ R"n" → R"n", the line integral along a piecewise smooth curve "C" ⊂ "U", in the direction of r, is defined as
where · is the dot product and r: [a, b] → "C" is a bijective parametrization of the curve "C" such that r("a") and r("b") give the endpoints of "C".
A line integral of a scalar field is thus a line integral of a vector field where the vectors are always tangential to the line.
Line integrals of vector fields are independent of the parametrization r in absolute value, but they do depend on its orientation. Specifically, a reversal in the orientation of the parametrization changes the sign of the line integral.
The line integral of a vector field along a curve is the integral of the corresponding 1-form under the musical isomorphism over the curve considered as an immersed
1-manifold.
Derivation.
The line integral of a vector field can be derived in a manner very similar to the case of a scalar field. However this time with the inclusion of a dot product. Again using the above definitions of F, "C" and its parametrization r("t"), we construct the integral from a Riemann sum. Partition the interval ["a","b"] into "n" intervals of length Δ"t" = ("b" − "a")/"n". Letting "ti" be the "i"th point on ["a","b"], then r("t""i") gives us the position of the "i"th point on the curve. However, instead of calculating up the distances between subsequent points, we need to calculate their displacement vectors, Δr"i". As before, evaluating F at all the points on the curve and taking the dot product with each displacement vector gives us the infinitesimal contribution of each partition of F on "C". Letting the size of the partitions go to zero gives us a sum
By the mean value theorem, we see that the displacement vector between adjacent points on the curve is
Substituting this in the above Riemann sum yields
which is the Riemann sum for the integral defined above.
Path independence.
If a vector field F is the gradient of a scalar field "G" (i.e. if F is conservative), that is,
then the derivative of the composition of "G" and r("t") is
which happens to be the integrand for the line integral of F on r("t"). It follows that, given a path "C ", then
In other words, the integral of F over "C" depends solely on the values of "G" in the points r("b") and r("a") and is thus independent of the path between them.
For this reason, a line integral of a conservative vector field is called "path independent".
Applications.
The line integral has many uses in physics. For example, the work done on a particle traveling on a curve "C" inside a force field represented as a vector field F is the line integral of F on "C".
Flow across a curve.
For a vector field F : "U" ⊆ R"2" → R"2", such as formula_15 the line integral across a piecewise smooth curve "C" ⊂ "U", is defined as
where · is the dot product and r: [a, b] → "C", formula_17 is a bijective parametrization of the curve "C" such that r("a") and r("b") give the endpoints of "C".
Complex line integral.
In complex analysis, the line integral is defined in terms of multiplication and addition of complex numbers. Suppose "U" is an open subset of the complex plane C, "f" : "U" → C is a function, and formula_18 is a curve of finite length, parametrized by formula_19, where formula_20 The line integral
may be defined by subdividing the interval ["a", "b"] into "a" = "t"0 < "t"1 < ... < "t""n" = "b" and considering the expression
The integral is then the limit of this Riemann sum as the lengths of the subdivision intervals approach zero.
If the parametrization formula_23 is continuously differentiable, the line integral can be evaluated as an integral of a function of a real variable:
When formula_25 is a closed curve, that is, its initial and final points coincide, the notation
is often used for the line integral of "f" along formula_25.
A closed curve line integral is sometimes referred to as a cyclic integral in engineering applications.
The line integral with respect to the conjugate complex differential formula_28 is defined to be
The line integrals of complex functions can be evaluated using a number of techniques: the integral may be split into real and imaginary parts reducing the problem to that of evaluating two real-valued line integrals, the Cauchy integral formula may be used in other circumstances. If the line integral is a closed curve in a region where the function is analytic and containing no singularities, then the value of the integral is simply zero; this is a consequence of the Cauchy integral theorem. The residue theorem allows contour integrals to be used in the complex plane to find integrals of real-valued functions of a real variable (see residue theorem for an example).
Example.
Consider the function "f"("z")=1/"z", and let the contour "L" be the unit circle about 0, parametrized by γ("t")="e""it" with "t" in [0, 2π] (which generates the circle counterclockwise). Substituting, we find
Here we have used the fact that any complex number "z" can be written as "reit" where "r" is the modulus of "z". On the unit circle this is fixed to 1, so the only variable left is the angle, which is denoted by "t". This answer can be also verified by the Cauchy integral formula.
Relation between the line integral of a vector field and the complex line integral.
Viewing complex numbers as 2-dimensional vectors, the line integral of a 2-dimensional vector field corresponds to the real part of the line integral of the conjugate of the corresponding complex function of a complex variable. More specifically, if formula_31 is a parameterization of "L" and formula_32, then:
provided that both integrals on the right hand side exist, and that the parametrization formula_23 of "L" has the same orientation as formula_35 (just expand the Riemann sum for the lefthand integral and take the limit).
By Green's theorem, the area of a region enclosed by a smooth, closed, positively oriented curve formula_25 is given by the integral
This fact is used, for example, in the proof of the area theorem.
Due to the Cauchy-Riemann equations the curl of the vector field corresponding to the conjugate of a holomorphic function is zero. This relates through Stokes' theorem both types of line integral being zero.
Quantum mechanics.
The "path integral formulation" of quantum mechanics actually refers not to path integrals in this sense but to functional integrals, that is, integrals over a space of paths, of a function "of" a possible path. However, path integrals in the sense of this article are important in quantum mechanics; for example, complex contour integration is often used in evaluating probability amplitudes in quantum scattering theory.

</doc>
<doc id="18422" url="https://en.wikipedia.org/wiki?curid=18422" title="Linear algebra">
Linear algebra

Linear algebra is the branch of mathematics concerning vector spaces and linear mappings between such spaces. It includes the study of lines, planes, and subspaces, but is also concerned with properties common to all vector spaces.
The set of points with coordinates that satisfy a linear equation forms a hyperplane in an "n"-dimensional space. The conditions under which a set of "n" hyperplanes intersect in a single point is an important focus of study in linear algebra. Such an investigation is initially motivated by a system of linear equations containing several unknowns. Such equations are naturally represented using the formalism of matrices and vectors.
Linear algebra is central to both pure and applied mathematics. For instance, abstract algebra arises by relaxing the axioms of a vector space, leading to a number of generalizations. Functional analysis studies the infinite-dimensional version of the theory of vector spaces. Combined with calculus, linear algebra facilitates the solution of linear systems of differential equations.
Techniques from linear algebra are also used in analytic geometry, engineering, physics, natural sciences, computer science, computer animation, and the social sciences (particularly in economics). Because linear algebra is such a well-developed theory, nonlinear mathematical models are sometimes approximated by linear models.
History.
The study of linear algebra first emerged from the study of determinants, which were used to solve systems of linear equations. Determinants were used by Leibniz in 1693, and subsequently, Gabriel Cramer devised Cramer's Rule for solving linear systems in 1750. Later, Gauss further developed the theory of solving linear systems by using Gaussian elimination, which was initially listed as an advancement in geodesy.
The study of matrix algebra first emerged in England in the mid-1800s. In 1844 Hermann Grassmann published his “Theory of Extension” which included foundational new topics of what is today called linear algebra. In 1848, James Joseph Sylvester introduced the term matrix, which is Latin for "womb". While studying compositions of linear transformations, Arthur Cayley was led to define matrix multiplication and inverses. Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants, and wrote "There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants".
In 1882, Hüseyin Tevfik Pasha wrote the book titled "Linear Algebra". The first modern and more precise definition of a vector space was introduced by Peano in 1888; by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged. Linear algebra first took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as abstract algebra. The use of matrices in quantum mechanics, special relativity, and statistics helped spread the subject of linear algebra beyond pure mathematics. The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations.
The origin of many of these ideas is discussed in the articles on determinants and Gaussian elimination.
Educational history.
Linear algebra first appeared in graduate textbooks in the 1940s and in undergraduate textbooks in the 1950s. Following work by the School Mathematics Study Group, U.S. high schools asked 12th grade students to do "matrix algebra, formerly reserved for college" in the 1960s. In France during the 1960s, educators attempted to teach linear algebra through affine dimensional vector spaces in the first year of secondary school. This was met with a backlash in the 1980s that removed linear algebra from the curriculum. In 1993, the U.S.-based Linear Algebra Curriculum Study Group recommended that undergraduate linear algebra courses be given an application-based "matrix orientation" as opposed to a theoretical orientation.
Scope of study.
Vector spaces.
The main structures of linear algebra are vector spaces. A vector space over a field "F" is a set "V" together with two binary operations. Elements of "V" are called "vectors" and elements of "F" are called "scalars". The first operation, "vector addition", takes any two vectors "v" and "w" and outputs a third vector . The second operation, "scalar multiplication", takes any scalar "a" and any vector "v" and outputs a new . The operations of addition and multiplication in a vector space must satisfy the following axioms. In the list below, let "u", "v" and "w" be arbitrary vectors in "V", and "a" and "b" scalars in "F".
The first four axioms are those of "V" being an abelian group under vector addition. Vector spaces may be diverse in nature, for example, containing functions, polynomials or matrices. Linear algebra is concerned with properties common to all vector spaces.
Linear transformations.
Similarly as in the theory of other algebraic structures, linear algebra studies mappings between vector spaces that preserve the vector-space structure. Given two vector spaces "V" and "W" over a field F, a linear transformation (also called linear map, linear mapping or linear operator) is a map
that is compatible with addition and scalar multiplication:
for any vectors "u","v" ∈ "V" and a scalar "a" ∈ F.
Additionally for any vectors "u", "v" ∈ "V" and scalars "a", "b" ∈ F:
When a bijective linear mapping exists between two vector spaces (that is, every vector from the second space is associated with exactly one in the first), we say that the two spaces are isomorphic. Because an isomorphism preserves linear structure, two isomorphic vector spaces are "essentially the same" from the linear algebra point of view. One essential question in linear algebra is whether a mapping is an isomorphism or not, and this question can be answered by checking if the determinant is nonzero. If a mapping is not an isomorphism, linear algebra is interested in finding its range (or image) and the set of elements that get mapped to zero, called the kernel of the mapping.
Linear transformations have geometric significance. For example, 2 × 2 real matrices denote standard planar mappings that preserve the origin.
Subspaces, span, and basis.
Again, in analogue with theories of other algebraic objects, linear algebra is interested in subsets of vector spaces that are themselves vector spaces; these subsets are called linear subspaces. For example, both the range and kernel of a linear mapping are subspaces, and are thus often called the range space and the nullspace; these are important examples of subspaces. Another important way of forming a subspace is to take a linear combination of a set of vectors "v"1, "v"2, ..., "vk":
where "a"1, "a"2, ..., "a""k" are scalars. The set of all linear combinations of vectors "v"1, "v"2, ..., "vk" is called their span, which forms a subspace.
A linear combination of any system of vectors with all zero coefficients is the zero vector of "V". If this is the only way to express the zero vector as a linear combination of "v"1, "v"2, ..., "vk" then these vectors are linearly independent. Given a set of vectors that span a space, if any vector "w" is a linear combination of other vectors (and so the set is not linearly independent), then the span would remain the same if we remove "w" from the set. Thus, a set of linearly dependent vectors is redundant in the sense that there will be a linearly independent subset which will span the same subspace. Therefore, we are mostly interested in a linearly independent set of vectors that spans a vector space "V", which we call a basis of "V". Any set of vectors that spans "V" contains a basis, and any linearly independent set of vectors in "V" can be extended to a basis. It turns out that if we accept the axiom of choice, every vector space has a basis; nevertheless, this basis may be unnatural, and indeed, may not even be constructible. For instance, there exists a basis for the real numbers considered as a vector space over the rationals, but no explicit basis has been constructed.
Any two bases of a vector space "V" have the same cardinality, which is called the dimension of "V". The dimension of a vector space is well-defined by the dimension theorem for vector spaces. If a basis of "V" has finite number of elements, "V" is called a finite-dimensional vector space. If "V" is finite-dimensional and "U" is a subspace of "V", then dim "U" ≤ dim "V". If "U"1 and "U"2 are subspaces of "V", then
One often restricts consideration to finite-dimensional vector spaces. A fundamental theorem of linear algebra states that all vector spaces of the same dimension are isomorphic, giving an easy way of characterizing isomorphism.
Matrix theory.
A particular basis {"v"1, "v"2, ..., "vn"} of "V" allows one to construct a coordinate system in "V": the vector with coordinates ("a"1, "a"2, ..., "an") is the linear combination
The condition that "v"1, "v"2, ..., "vn" span "V" guarantees that each vector "v" can be assigned coordinates, whereas the linear independence of "v"1, "v"2, ..., "vn" assures that these coordinates are unique (i.e. there is only one linear combination of the basis vectors that is equal to "v"). In this way, once a basis of a vector space "V" over F has been chosen, "V" may be identified with the coordinate "n"-space F"n". Under this identification, addition and scalar multiplication of vectors in "V" correspond to addition and scalar multiplication of their coordinate vectors in F"n". Furthermore, if "V" and "W" are an "n"-dimensional and "m"-dimensional vector space over F, and a basis of "V" and a basis of "W" have been fixed, then any linear transformation "T": "V" → "W" may be encoded by an "m" × "n" matrix "A" with entries in the field F, called the matrix of "T" with respect to these bases. Two matrices that encode the same linear transformation in different bases are called similar. Matrix theory replaces the study of linear transformations, which were defined axiomatically, by the study of matrices, which are concrete objects. This major technique distinguishes linear algebra from theories of other algebraic structures, which usually cannot be parameterized so concretely.
There is an important distinction between the coordinate "n"-space R"n" and a general finite-dimensional vector space "V". While R"n" has a standard basis {"e"1, "e"2, ..., "en"}, a vector space "V" typically does not come equipped with such a basis and many different bases exist (although they all consist of the same number of elements equal to the dimension of "V").
One major application of the matrix theory is calculation of determinants, a central concept in linear algebra. While determinants could be defined in a basis-free manner, they are usually introduced via a specific representation of the mapping; the value of the determinant does not depend on the specific basis. It turns out that a mapping has an inverse if and only if the determinant has an inverse (every non-zero real or complex number has an inverse). If the determinant is zero, then the nullspace is nontrivial. Determinants have other applications, including a systematic way of seeing if a set of vectors is linearly independent (we write the vectors as the columns of a matrix, and if the determinant of that matrix is zero, the vectors are linearly dependent). Determinants could also be used to solve systems of linear equations (see Cramer's rule), but in real applications, Gaussian elimination is a faster method.
Eigenvalues and eigenvectors.
In general, the action of a linear transformation may be quite complex. Attention to low-dimensional examples gives an indication of the variety of their types. One strategy for a general n-dimensional transformation "T" is to find "characteristic lines" that are invariant sets under "T". If "v" is a non-zero vector such that "Tv" is a scalar multiple of "v", then the line through 0 and "v" is an invariant set under "T" and "v" is called a characteristic vector or eigenvector. The scalar λ such that "Tv" = λ"v" is called a characteristic value or eigenvalue of "T".
To find an eigenvector or an eigenvalue, we note that
where I is the identity matrix. For there to be nontrivial solutions to that equation, det("T" − λ I) = 0. The determinant is a polynomial, and so the eigenvalues are not guaranteed to exist if the field is R. Thus, we often work with an algebraically closed field such as the complex numbers when dealing with eigenvectors and eigenvalues so that an eigenvalue will always exist. It would be particularly nice if given a transformation "T" taking a vector space "V" into itself we can find a basis for "V" consisting of eigenvectors. If such a basis exists, we can easily compute the action of the transformation on any vector: if "v"1, "v"2, ..., "vn" are linearly independent eigenvectors of a mapping of "n"-dimensional spaces "T" with (not necessarily distinct) eigenvalues λ1, λ2, ..., λ"n", and if "v" = "a"1"v"1 + ... + "an vn", then,
Such a transformation is called a diagonalizable matrix since in the eigenbasis, the transformation is represented by a diagonal matrix. Because operations like matrix multiplication, matrix inversion, and determinant calculation are simple on diagonal matrices, computations involving matrices are much simpler if we can bring the matrix to a diagonal form. Not all matrices are diagonalizable (even over an algebraically closed field).
Inner-product spaces.
Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product. The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles. Formally, an "inner product" is a map
that satisfies the following three axioms for all vectors "u", "v", "w" in "V" and all scalars "a" in "F":
Note that in R, it is symmetric.
We can define the length of a vector "v" in "V" by
and we can prove the Cauchy–Schwarz inequality:
In particular, the quantity
and so we can call this quantity the cosine of the angle between the two vectors.
Two vectors are orthogonal if formula_17. An orthonormal basis is a basis where all basis vectors have length 1 and are orthogonal to each other. Given any finite-dimensional vector space, an orthonormal basis could be found by the Gram–Schmidt procedure. Orthonormal bases are particularly nice to deal with, since if "v" = "a"1 "v"1 + ... + "an vn", then formula_18.
The inner product facilitates the construction of many useful concepts. For instance, given a transform "T", we can define its Hermitian conjugate "T*" as the linear transform satisfying
If "T" satisfies "TT*" = "T*T", we call "T" normal. It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span "V".
Applications.
Because of the ubiquity of vector spaces, linear algebra is used in many fields of mathematics, natural sciences, computer science, and social science. Below are just some examples of applications of linear algebra.
Solution of linear systems.
Linear algebra provides the formal setting for the linear combination of equations used in the Gaussian method. Suppose the goal is to find and describe the solution(s), if any, of the following system of linear equations:
The Gaussian-elimination algorithm is as follows: eliminate "x" from all equations below "L"1, and then eliminate "y" from all equations below "L"2. This will put the system into triangular form. Then, using back-substitution, each unknown can be solved for.
In the example, "x" is eliminated from "L"2 by adding (3/2)"L"1 to "L"2. "x" is then eliminated from "L"3 by adding "L"1 to "L"3. Formally:
The result is:
Now "y" is eliminated from "L"3 by adding −4"L"2 to "L"3:
The result is:
This result is a system of linear equations in triangular form, and so the first part of the algorithm is complete.
The last part, back-substitution, consists of solving for the known in reverse order. It can thus be seen that
Then, "z" can be substituted into "L"2, which can then be solved to obtain
Next, "z" and "y" can be substituted into "L"1, which can be solved to obtain
The system is solved.
We can, in general, write any system of linear equations as a matrix equation:
The solution of this system is characterized as follows: first, we find a particular solution "x"0 of this equation using Gaussian elimination. Then, we compute the solutions of "Ax" = 0; that is, we find the null space "N" of "A". The solution set of this equation is given by formula_30. If the number of variables is equal to the number of equations, then we can characterize when the system has a unique solution: since "N" is trivial if and only if det "A" ≠ 0, the equation has a unique solution if and only if det "A" ≠ 0.
Least-squares best fit line.
The least squares method is used to determine the best fit line for a set of data. This line will minimize the sum of the squares of the residuals.
Fourier series expansion.
Fourier series are a representation of a function "f": [−π, π] → R as a trigonometric series:
This series expansion is extremely useful in solving partial differential equations. In this article, we will not be concerned with convergence issues; it is nice to note that all Lipschitz-continuous functions have a converging Fourier series expansion, and nice enough discontinuous functions have a Fourier series that converges to the function value at most points.
The space of all functions that can be represented by a Fourier series form a vector space (technically speaking, we call functions that have the same Fourier series expansion the "same" function, since two different discontinuous functions might have the same Fourier series). Moreover, this space is also an inner product space with the inner product
The functions "gn"("x") = sin("nx") for "n" > 0 and "hn"("x") = cos("nx") for "n" ≥ 0 are an orthonormal basis for the space of Fourier-expandable functions. We can thus use the tools of linear algebra to find the expansion of any function in this space in terms of these basis functions. For instance, to find the coefficient "ak", we take the inner product with "hk":
and by orthonormality, formula_34; that is,
Quantum mechanics.
Quantum mechanics is highly inspired by notions in linear algebra. In quantum mechanics, the physical state of a particle is represented by a vector, and observables (such as momentum, energy, and angular momentum) are represented by linear operators on the underlying vector space. More concretely, the wave function of a particle describes its physical state and lies in the vector space L2 (the functions φ: R3 → C such that formula_36 is finite), and it evolves according to the Schrödinger equation. Energy is represented as the operator formula_37, where "V" is the potential energy. "H" is also known as the Hamiltonian operator. The eigenvalues of "H" represents the possible energies that can be observed. Given a particle in some state φ, we can expand φ into a linear combination of eigenstates of "H". The component of "H" in each eigenstate determines the probability of measuring the corresponding eigenvalue, and the measurement forces the particle to assume that eigenstate (wave function collapse).
Geometric introduction.
Many of the principles and techniques of linear algebra can be seen in the geometry of lines in a real two dimensional plane "E". When formulated using vectors and matrices the geometry of points and lines in the plane can be extended to the geometry of points and hyperplanes in high-dimensional spaces.
Point coordinates in the plane "E" are ordered pairs of real numbers, ("x","y"), and a line is defined as the set of points ("x","y") that satisfy the linear equation
where "a", "b" and "c" are not all zero.
Then,
or
where x = ("x", "y", 1) is the 3 × 1 set of homogeneous coordinates associated with the point ("x", "y").
Homogeneous coordinates identify the plane "E" with the "z" = 1 plane in three dimensional space. The x−y coordinates in "E" are obtained from homogeneous coordinates y = ("y"1, "y"2, "y"3) by dividing by the third component (if it is nonzero) to obtain y = ("y"1/"y"3, "y"2/"y"3, 1).
The linear equation, λ, has the important property, that if x1 and x2 are homogeneous coordinates of points on the line, then the point "αx1 + "βx2 is also on the line, for any real "α" and "β".
Now consider the equations of the two lines "λ"1 and "λ"2,
which forms a system of linear equations. The intersection of these two lines is defined by x = ("x", "y", 1) that satisfy the matrix equation,
or using homogeneous coordinates,
The point of intersection of these two lines is the unique non-zero solution of these equations. In homogeneous coordinates,
the solutions are multiples of the following solution:
if the rows of B are linearly independent (i.e., "λ"1 and "λ"2 represent distinct lines).
Divide through by x3 to get Cramer's rule for the solution of a set of two linear equations in two unknowns. Notice that this yields a point in the "z" = 1 plane only when the 2 × 2 submatrix associated with "x"3 has a non-zero determinant.
It is interesting to consider the case of three lines, λ1, λ2 and λ3, which yield the matrix equation,
which in homogeneous form yields,
Clearly, this equation has the solution x = (0,0,0), which is not a point on the "z" = 1 plane "E". For a solution to exist in the plane "E", the coefficient matrix "C" must have rank 2, which means its determinant must be zero. Another way to say this is that the columns of the matrix must be linearly dependent.
Introduction to linear transformations.
Another way to approach linear algebra is to consider linear functions on the two dimensional real plane "E"=R2. Here R denotes the set of real numbers. Let x=(x, y) be an arbitrary vector in "E" and consider the linear function λ: "E"→R, given by
or
This transformation has the important property that if Ay=d, then
This shows that the sum of vectors in "E" map to the sum of their images in R. This is the defining characteristic of a linear map, or linear transformation. For this case, where the image space is a real number the map is called a linear functional.
Consider the linear functional a little more carefully. Let i=(1,0) and j =(0,1) be the natural basis vectors on "E", so that x=xi+yj. It is now possible to see that
Thus, the columns of the matrix A are the image of the basis vectors of "E" in R.
This is true for any pair of vectors used to define coordinates in "E". Suppose we select a non-orthogonal non-unit vector basis v and w to define coordinates of vectors in "E". This means a vector x has coordinates (α,β), such that x=αv+βw. Then, we have the linear functional
where Av=d and Aw=e are the images of the basis vectors v and w. This is written in matrix form as
Coordinates relative to a basis.
This leads to the question of how to determine the coordinates of a vector x relative to a general basis v and w in "E". Assume that we know the coordinates of the vectors, x, v and w in the natural basis i=(1,0) and j =(0,1). Our goal is two find the real numbers α, β, so that x=αv+βw, that is
To solve this equation for α, β, we compute the linear coordinate functionals σ and τ for the basis v, w, which are given by,
The functionals σ and τ compute the components of x along the basis vectors v and w, respectively, that is,
which can be written in matrix form as
These coordinate functionals have the properties,
These equations can be assembled into the single matrix equation,
Thus, the matrix formed by the coordinate linear functionals is the inverse of the matrix formed by the basis vectors.
Inverse image.
The set of points in the plane "E" that map to the same image in R under the linear functional λ define a line in "E". This line is the image of the inverse map, λ−1: R→"E". This inverse image is the set of the points x=(x, y) that solve the equation,
Notice that a linear functional operates on known values for x=(x, y) to compute a value "c" in R, while the inverse image seeks the values for x=(x, y) that yield a specific value "c".
In order to solve the equation, we first recognize that only one of the two unknowns (x,y) can be determined, so we select y to be determined, and rearrange the equation
Solve for y and obtain the inverse image as the set of points,
For convenience the free parameter x has been relabeled t.
The vector p defines the intersection of the line with the y-axis, known as the y-intercept. The vector h satisfies the homogeneous equation,
Notice that if h is a solution to this homogeneous equation, then "t" h is also a solution.
The set of points of a linear functional that map to zero define the "kernel" of the linear functional. The line can be considered to be the set of points h in the kernel translated by the vector p.
Generalizations and related topics.
Since linear algebra is a successful theory, its methods have been developed and generalized in other parts of mathematics. In module theory, one replaces the field of scalars by a ring. The concepts of linear independence, span, basis, and dimension (which is called rank in module theory) still make sense. Nevertheless, many theorems from linear algebra become false in module theory. For instance, not all modules have a basis (those that do are called free modules), the rank of a free module is not necessarily unique, not every linearly independent subset of a module can be extended to form a basis, and not every subset of a module that spans the space contains a basis.
In multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables. This line of inquiry naturally leads to the idea of the dual space, the vector space "V"∗ consisting of linear maps where "F" is the field of scalars. Multilinear maps can be described via tensor products of elements of "V"∗.
If, in addition to vector addition and scalar multiplication, there is a bilinear vector product , the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials).
Functional analysis mixes the methods of linear algebra with those of mathematical analysis and studies various function spaces, such as L"p" spaces.
Representation theory studies the actions of algebraic objects on vector spaces by representing these objects as matrices. It is interested in all the ways that this is possible, and it does so by finding subspaces invariant under all transformations of the algebra. The concept of eigenvalues and eigenvectors is especially important.
Algebraic geometry considers the solutions of systems of polynomial equations.
There are several related topics in the field of Computer Programming that utilizes much of the techniques and theorems Linear Algebra encompasses and refers to.

</doc>
<doc id="395885" url="https://en.wikipedia.org/wiki?curid=395885" title="Linear algebraic group">
Linear algebraic group

In mathematics, a linear algebraic group is a subgroup of the group of invertible "n"×"n" matrices (under matrix multiplication) that is defined by polynomial equations. An example is the orthogonal group, defined by the relation MTM = I where MT is the transpose of M.
The main examples of linear algebraic groups are certain Lie groups, where the underlying field is the real or complex field. (For example, every compact Lie group can be regarded as the group of points of a real linear algebraic group, essentially by the Peter–Weyl theorem.)
These were the first algebraic groups to be extensively studied. Such groups were known for a long time before their abstract algebraic theory was developed according to the needs of major applications. Compact Lie groups were considered by Élie Cartan, Ludwig Maurer, Wilhelm Killing, and Sophus Lie in the 1880s and 1890s in the context of differential equations and Galois theory. However, a purely algebraic theory was first developed by , with Armand Borel as one of its pioneers. The Picard–Vessiot theory did lead to algebraic groups.
The first basic theorem of the subject is that any "affine" algebraic group is a linear algebraic group: that is, any affine variety V that has an algebraic group law has a "faithful" linear representation, over the same field, which is also a morphism of varieties. For example the "additive group" of an "n"-dimensional vector space has a faithful representation as ("n"+1)×("n"+1) matrices.
One can define the Lie algebra of an algebraic group purely algebraically (it consists of the dual number points based at the identity element); and this theorem shows that we get a matrix Lie algebra. A linear algebraic group G consists of a finite number of irreducible components, that are in fact also the connected components: the one Go containing the identity will be a normal subgroup of G.
One of the first uses for the theory was to define the Chevalley groups.
Examples.
Since formula_1, formula_2 is a linear algebraic group. The embedding formula_3
shows that formula_4 is a unipotent group.
The deeper structure theory applies to connected linear algebraic groups G, and begins with the definition of Borel subgroups B. These turn out to be maximal as connected solvable subgroups (i.e., subgroups with composition series having as factors one-dimensional subgroups, all of which are groups of additive or multiplicative type); and also minimal such that G/B is a projective variety.
The most important subgroups of a linear algebraic group, besides its Borel subgroups, are its tori, especially the maximal ones (similar to the study of maximal tori in Lie groups). If there is a maximal torus which "splits" (i.e. is isomorphic to a product of multiplicative groups), one calls the linear group "split" as well. If there is no splitting maximal torus, one studies the splitting tori and the maximal ones of them. If there is a rank at least 1 split torus in the group, the group is called "isotropic" and "anisotropic" if this is not the case. Any anisotropic or isotropic linear algebraic group over a field becomes split over the algebraic closure, so this distinction is interesting from the point of view of Algebraic number theory.
Group actions.
Let "G" be a unipotent group acting on an affine variety. Then every "G"-orbit in the variety is closed.
The Borel fixed-point theorem states that a connected solvable group acting on a non-empty complete variety admits a fixed point. The classical Lie–Kolchin theorem follows from the theorem applied to the flag variety.
Non-algebraic Lie groups.
There are several classes of examples of Lie groups that aren't the real or complex points of an algebraic group.

</doc>
<doc id="55632" url="https://en.wikipedia.org/wiki?curid=55632" title="Linear combination">
Linear combination

In mathematics, a linear combination is an expression constructed from a set of terms by multiplying each term by a constant and adding the results (e.g. a linear combination of "x" and "y" would be any expression of the form "ax" + "by", where "a" and "b" are constants). The concept of linear combinations is central to linear algebra and related fields of mathematics.
Most of this article deals with linear combinations in the context of a vector space over a field, with some generalizations given at the end of the article.
Definition.
Suppose that "K" is a field (for example, the real numbers) and "V" is a vector space over "K". As usual, we call elements of "V" "vectors" and call elements of "K" "scalars".
If "v"1...,"v""n" are vectors and "a"1...,"a""n" are scalars, then the "linear combination of those vectors with those scalars as coefficients" is
There is some ambiguity in the use of the term "linear combination" as to whether it refers to the expression or to its value. In most cases the value is emphasized, like in the assertion "the set of all linear combinations of "v"1...,"v""n" always forms a subspace". However, one could also say "two different linear combinations can have the same value" in which case the expression must have been meant. The subtle difference between these uses is the essence of the notion of linear dependence: a family "F" of vectors is linearly independent precisely if any linear combination of the vectors in "F" (as value) is uniquely so (as expression). In any case, even when viewed as expressions, all that matters about a linear combination is the coefficient of each "v""i"; trivial modifications such as permuting the terms or adding terms with zero coefficient do not give distinct linear combinations.
In a given situation, "K" and "V" may be specified explicitly, or they may be obvious from context. In that case, we often speak of "a linear combination of the vectors" "v"1...,"v""n", with the coefficients unspecified (except that they must belong to "K"). Or, if "S" is a subset of "V", we may speak of "a linear combination of vectors in S", where both the coefficients and the vectors are unspecified, except that the vectors must belong to the set "S" (and the coefficients must belong to "K"). Finally, we may speak simply of "a linear combination", where nothing is specified (except that the vectors must belong to "V" and the coefficients must belong to "K"); in this case one is probably referring to the expression, since every vector in "V" is certainly the value of some linear combination.
Note that by definition, a linear combination involves only finitely many vectors (except as described in Generalizations below).
However, the set "S" that the vectors are taken from (if one is mentioned) can still be infinite; each individual linear combination will only involve finitely many vectors.
Also, there is no reason that "n" cannot be zero; in that case, we declare by convention that the result of the linear combination is the zero vector in "V".
Examples and counterexamples.
Euclidean vectors.
Let the field "K" be the set R of real numbers, and let the vector space "V" be the Euclidean space R3.
Consider the vectors "e"1 = (1,0,0), "e"2 = (0,1,0) and "e"3 = (0,0,1).
Then "any" vector in R3 is a linear combination of "e"1, "e"2 and "e"3.
To see that this is so, take an arbitrary vector ("a"1,"a"2,"a"3) in R3, and write:
Functions.
Let "K" be the set C of all complex numbers, and let "V" be the set CC("R") of all continuous functions from the real line R to the complex plane C.
Consider the vectors (functions) "f" and "g" defined by "f"("t") := "e""it" and "g"("t") := "e"−"it".
Some linear combinations of "f" and "g" are:
On the other hand, the constant function 3 is "not" a linear combination of "f" and "g". To see this, suppose that 3 could be written as a linear combination of "e""it" and "e"−"it". This means that there would exist complex scalars "a" and "b" such that "ae""it" + "be"−"it" = 3 for all real numbers "t". Setting "t" = 0 and "t" = π gives the equations "a" + "b" = 3 and "a" + "b" = −3, and clearly this cannot happen. See Euler's identity.
Polynomials.
Let "K" be R, C, or any field, and let "V" be the set "P" of all polynomials with coefficients taken from the field "K".
Consider the vectors (polynomials) "p"1 := 1, "p"2 := "x" + 1, and "p"3 := "x"2 + "x" + 1.
Is the polynomial "x"2 − 1 a linear combination of "p"1, "p"2, and "p"3?
To find out, consider an arbitrary linear combination of these vectors and try to see when it equals the desired vector "x"2 − 1.
Picking arbitrary coefficients "a"1, "a"2, and "a"3, we want
Multiplying the polynomials out, this means
and collecting like powers of "x", we get
Two polynomials are equal if and only if their corresponding coefficients are equal, so we can conclude
This system of linear equations can easily be solved.
First, the first equation simply says that "a"3 is 1.
Knowing that, we can solve the second equation for "a"2, which comes out to −1.
Finally, the last equation tells us that "a"1 is also −1.
Therefore, the only possible way to get a linear combination is with these coefficients.
Indeed,
so "x"2 − 1 "is" a linear combination of "p"1, "p"2, and "p"3.
On the other hand, what about the polynomial "x"3 − 1?
If we try to make this vector a linear combination of "p"1, "p"2, and "p"3, then following the same process as before, we’ll get the equation
However, when we set corresponding coefficients equal in this case, the equation for "x"3 is
which is always false.
Therefore, there is no way for this to work, and "x"3 − 1 is "not" a linear combination of "p"1, "p"2, and "p"3.
The linear span.
"Main article: linear span"
Take an arbitrary field "K", an arbitrary vector space "V", and let "v"1...,"v""n" be vectors (in "V").
It’s interesting to consider the set of "all" linear combinations of these vectors.
This set is called the "linear span" (or just "span") of the vectors, say S ={"v"1...,"v""n"}. We write the span of S as span(S) or sp(S):
Linear independence.
For some sets of vectors "v"1...,"v""n",
a single vector can be written in two different ways as a linear combination of them:
Equivalently, by subtracting these (formula_17) a non-trivial combination is zero:
If that is possible, then "v"1...,"v""n" are called "linearly dependent"; otherwise, they are "linearly independent".
Similarly, we can speak of linear dependence or independence of an arbitrary set "S" of vectors.
If "S" is linearly independent and the span of "S" equals "V", then "S" is a basis for "V".
Affine, conical, and convex combinations.
By restricting the coefficients used in linear combinations, one can define the related concepts of affine combination, conical combination, and convex combination, and the associated notions of sets closed under these operations.
Because these are more "restricted" operations, more subsets will be closed under them, so affine subsets, convex cones, and convex sets are "generalizations" of vector subspaces: a vector subspace is also an affine subspace, a convex cone, and a convex set, but a convex set need not be a vector subspace, affine, or a convex cone.
These concepts often arise when one can take certain linear combinations of objects, but not any: for example, probability distributions are closed under convex combination (they form a convex set), but not conical or affine combinations (or linear), and positive measures are closed under conical combination but not affine or linear – hence one defines signed measures as the linear closure.
Linear and affine combinations can be defined over any field (or ring), but conical and convex combination require a notion of "positive", and hence can only be defined over an ordered field (or ordered ring), generally the real numbers.
If one allows only scalar multiplication, not addition, one obtains a (not necessarily convex) cone; one often restricts the definition to only allowing multiplication by positive scalars.
All of these concepts are usually defined as subsets of an ambient vector space (except for affine spaces, which are also considered as "vector spaces forgetting the origin"), rather than being axiomatized independently.
Operad theory.
More abstractly, in the language of operad theory, one can consider vector spaces to be algebras over the operad formula_19 (the infinite direct sum, so only finitely many terms are non-zero; this corresponds to only taking finite sums), which parametrizes linear combinations: the vector formula_20 for instance corresponds to the linear combination formula_21. Similarly, one can consider affine combinations, conical combinations, and convex combinations to correspond to the sub-operads where the terms sum to 1, the terms are all non-negative, or both, respectively. Graphically, these are the infinite affine hyperplane, the infinite hyper-octant, and the infinite simplex. This formalizes what is meant by formula_22 being or the standard simplex being model spaces, and such observations as that every bounded convex polytope is the image of a simplex. Here suboperads correspond to more restricted operations and thus more general theories.
From this point of view, we can think of linear combinations as the most general sort of operation on a vector space – saying that a vector space is an algebra over the operad of linear combinations is precisely the statement that "all possible" algebraic operations in a vector space are linear combinations.
The basic operations of addition and scalar multiplication, together with the existence of an additive identity and additive inverses, cannot be combined in any more complicated way than the generic linear combination: the basic operations are a generating set for the operad of all linear combinations.
Ultimately, this fact lies at the heart of the usefulness of linear combinations in the study of vector spaces.
Generalizations.
If "V" is a topological vector space, then there may be a way to make sense of certain "infinite" linear combinations, using the topology of "V".
For example, we might be able to speak of "a"1"v"1 + "a"2"v"2 + "a"3"v"3 + ..., going on forever.
Such infinite linear combinations do not always make sense; we call them "convergent" when they do.
Allowing more linear combinations in this case can also lead to a different concept of span, linear independence, and basis.
The articles on the various flavours of topological vector spaces go into more detail about these.
If "K" is a commutative ring instead of a field, then everything that has been said above about linear combinations generalizes to this case without change.
The only difference is that we call spaces like this "V" modules instead of vector spaces.
If "K" is a noncommutative ring, then the concept still generalizes, with one caveat:
Since modules over noncommutative rings come in left and right versions, our linear combinations may also come in either of these versions, whatever is appropriate for the given module.
This is simply a matter of doing scalar multiplication on the correct side.
A more complicated twist comes when "V" is a bimodule over two rings, "K"L and "K"R.
In that case, the most general linear combination looks like
where "a"1...,"a""n" belong to "K"L, "b"1...,"b""n" belong to "K"R, and "v"1...,"v""n" belong to "V".

</doc>
<doc id="379868" url="https://en.wikipedia.org/wiki?curid=379868" title="Linear differential equation">
Linear differential equation

In mathematics, linear differential equations are differential equations having solutions which can be added together in particular linear combinations to form further solutions. They can be ordinary (ODEs) or partial (PDEs). The solutions to (homogeneous) linear differential equations form a vector space (unlike non-linear differential equations).
Introduction.
Linear differential equations are of the form
where the differential operator "L" is a linear operator, "y" is the unknown function (such as a function of time "y"("t")), and the right hand side "f" is a given function of the same nature as "y" (called the source term). For a function dependent on time we may write the equation more expressly as
and, even more precisely by bracketing
The linear operator "L" may be considered to be of the form
The linearity condition on "L" rules out operations such as taking the square of the derivative of "y"; but permits, for example, taking the second derivative of "y". 
It is convenient to rewrite this equation in an operator form
where "D" is the differential operator "d/dt" (i.e. "Dy = y' ", "D"2"y = y"... "), and the "An" are given functions. 
Such an equation is said to have order "n", the index of the highest derivative of "y" that is involved. 
A typical simple example is the linear differential equation used to model radioactive decay. Let "N"("t") denote the number of radioactive atoms in some sample of material at time "t". Then for some constant "k" > 0, the number of radioactive atoms which decay can be modelled by
If "y" is assumed to be a function of only one variable, one speaks about an ordinary differential equation, else the derivatives and their coefficients must be understood as (contracted) vectors, matrices or tensors of higher rank, and we have a (linear) partial differential equation.
The case where "f" = 0 is called a homogeneous equation and its solutions are called complementary functions. It is particularly important to the solution of the general case, since any complementary function can be added to a solution of the inhomogeneous equation to give another solution (by a method traditionally called "particular integral and complementary function"). When the "Ai" are numbers, the equation is said to have "constant coefficients".
Homogeneous equations with constant coefficients.
The first method of solving linear homogeneous ordinary differential equations with constant coefficients is due to Euler, who realized that solutions have the form "ezx", for possibly-complex values of "z". The exponential function is one of the few functions to keep its shape after differentiation, allowing the sum of its multiple derivatives to cancel out to zero, as required by the equation. Thus, for constant values "A"1..., "A"n, to solve:
we set "y" = "ezx", leading to
Division by "ezx" gives the "n"th-order polynomial:
This algebraic equation "F"("z") = 0 is the characteristic equation considered later by Gaspard Monge and Augustin-Louis Cauchy.
Formally, the terms
of the original differential equation are replaced by "zk". Solving the polynomial gives "n" values of "z", "z"1, ..., "zn". Substitution of any of those values for "z" into "ezx" gives a solution "ezix". Since homogeneous linear differential equations obey the superposition principle, any linear combination of these functions also satisfies the differential equation.
When these roots are all distinct, we have "n" distinct solutions to the differential equation. It can be shown that these are linearly independent, by applying the Vandermonde determinant, and together they form a basis of the space of all solutions of the differential equation.
The preceding gave a solution for the case when all zeros are distinct, that is, each has multiplicity 1. For the general case, if "z" is a (possibly complex) zero (or root) of "F"("z") having multiplicity "m", then, for formula_11, formula_12 is a solution of the ODE. Applying this to all roots gives a collection of "n" distinct and linearly independent functions, where "n" is the degree of "F"("z"). As before, these functions make up a basis of the solution space.
If the coefficients "Ai" of the differential equation are real, then real-valued solutions are generally preferable. Since non-real roots "z" then come in conjugate pairs, so do their corresponding basis functions , and the desired result is obtained by replacing each pair with their real-valued linear combinations Re("y") and Im("y"), where "y" is one of the pair.
A case that involves complex roots can be solved with the aid of Euler's formula.
Examples.
Given formula_13. The characteristic equation is formula_14 which has roots "2±i". Thus the solution basis formula_15 is formula_16. Now "y" is a solution if and only if formula_17 for formula_18.
Because the coefficients are real,
The linear combinations
will give us a real basis in formula_21.
Simple harmonic oscillator.
The second order differential equation
which represents a simple harmonic oscillator, can be restated as
The expression in parenthesis can be factored out, yielding
which has a pair of linearly independent solutions:
The solutions are, respectively,
and
These solutions provide a basis for the two-dimensional solution space of the second order differential equation: meaning that linear combinations of these solutions will also be solutions. In particular, the following solutions can be constructed
and
These last two trigonometric solutions are linearly independent, so they can serve as another basis for the solution space, yielding the following general solution:
Damped harmonic oscillator.
Given the equation for the damped harmonic oscillator:
the expression in parentheses can be factored out: first obtain the characteristic equation by replacing "D" with λ. This equation must be satisfied for all "y", thus:
Solve using the quadratic formula:
Use these data to factor out the original differential equation:
This implies a pair of solutions, one corresponding to
The solutions are, respectively,
where ω = "b"/2"m". From this linearly independent pair of solutions can be constructed another linearly independent pair which thus serve as a basis for the two-dimensional solution space:
However, if |ω| < |ω0| then it is preferable to get rid of the consequential imaginaries, expressing the general solution as
This latter solution corresponds to the underdamped case, whereas the former one corresponds to the overdamped case: the solutions for the underdamped case oscillate whereas the solutions for the overdamped case do not.
Nonhomogeneous equation with constant coefficients.
To obtain the solution to the nonhomogeneous equation (sometimes called inhomogeneous equation), find a particular integral "yP"("x") by either the method of undetermined coefficients or the method of variation of parameters; the general solution to the linear differential equation is the sum of the general solution of the related homogeneous equation and the particular integral. Or, when the initial conditions are set, use Laplace transform to obtain the particular solution directly.
Suppose we face
For later convenience, define the characteristic polynomial
We find a solution basis formula_44 for the homogeneous ("f"("x") = 0) case. We now seek a particular integral "yp"("x") by the variation of parameters method. Let the coefficients of the linear combination be functions of "x":
For ease of notation we will drop the dependency on "x" (i.e. the various "(x)"). Using the operator notation "D" = "d/dx", the ODE in question is "P"("D")"y" = "f"; so
With the constraints
the parameters commute out,
But "P"("D")"yj" = 0, therefore
This, with the constraints, gives a linear system in the "u′j". This much can always be solved; in fact, combining Cramer's rule with the Wronskian,
In the very non-standard notation used above, one should take the i,n-minor of W and multiply it by f. That's why we get a minus-sign. Alternatively, forget about the minus sign and just compute the determinant of the matrix obtained by substituting the j-th W column with (0, 0, ..., f).
The rest is a matter of integrating "u′j".
The particular integral is not unique; formula_54 also satisfies the ODE for any set of constants "cj".
Example.
Suppose formula_55. We take the solution basis found above formula_56.
Using the list of integrals of exponential functions
And so
For interest's sake, this ODE has a physical interpretation as a driven damped harmonic oscillator; "yp" represents the steady state, and formula_61 is the transient.
Equation with variable coefficients.
A linear ODE of order "n" with variable coefficients has the general form
Examples.
A simple example is the Cauchy–Euler equation often used in engineering
First-order equation with variable coefficients.
A linear ODE of order 1 with variable coefficients has the general form
Where D is the differential operator. Equations of this form can be solved by multiplying the integrating factor
throughout to obtain
which simplifies due to the product rule (applied backwards) to
which, on integrating both sides and solving for "y"("x") gives:
In other words: The solution of a first-order linear ODE
with coefficients that may or may not vary with "x", is:
where κ is the constant of integration, and
A compact form of the general solution based on a Green's function is (see J. Math. Chem. 48 (2010) 175):
where δ("x") is the generalized Dirac delta function.
Examples.
Consider a first order differential equation with constant coefficients:
This equation is particularly relevant to first order systems such as RC circuits and mass-damper systems.
In this case, "f"("x") = "b", "g"("x") = 1.
Hence its solution is
Systems of linear differential equations.
An arbitrary linear ordinary differential equation or even a system of such equations can be converted into a first order system of linear differential equations by adding variables for all but the highest order derivatives. A linear system can be viewed as a single equation with a vector-valued variable. The general treatment is analogous to the treatment above of ordinary first order linear differential equations, but with complications stemming from noncommutativity of matrix multiplication.
To solve
(here formula_76 is a vector or matrix, and formula_77 is a matrix),
let formula_78 be the solution of formula_79 with formula_80 (the identity matrix). formula_81 is a fundamental matrix for the equation — the columns of formula_81 form a complete linearly independent set of solutions for the homogeneous equation. After substituting formula_83, the equation formula_84 simplifies to formula_85 Thus, 
If formula_87 commutes with formula_88 for all formula_89 and formula_90, then 
and thus 
but in the general case there is no closed form solution, and an approximation method such as Magnus expansion may have to be used. Note that the exponentials are matrix exponentials.

</doc>
<doc id="152111" url="https://en.wikipedia.org/wiki?curid=152111" title="Linear function">
Linear function

In mathematics, the term linear function refers to two distinct but related notions:
As a polynomial function.
In calculus, analytic geometry and related areas, a linear function is a polynomial of degree one or less, including the zero polynomial (the latter not being considered to have degree zero).
When the function is of only one variable, it is of the form
where and are constants, often real numbers. The graph of such a function of one variable is a nonvertical line. is frequently referred to as the slope of the line, and as the intercept.
For a function formula_2 of any finite number of independent variables, the general formula is
and the graph is a hyperplane of dimension .
A constant function is also considered linear in this context, as it is a polynomial of degree zero or is the zero polynomial. Its graph, when there is only one independent variable, is a horizontal line.
In this context, the other meaning (a linear map) may be referred to as a homogeneous linear function or a linear form. In the context of linear algebra, this meaning (polynomial functions of degree 0 or 1) is a special kind of affine map.
As a linear map.
In linear algebra, a linear function is a map "f" between two vector spaces that preserves vector addition and scalar multiplication:
Here denotes a constant belonging to some field of scalars (for example, the real numbers) and and are elements of a vector space, which might be itself.
Some authors use "linear function" only for linear maps that take values in the scalar field; these are also called linear functionals.
The "linear functions" of calculus qualify as "linear maps" when (and only when) formula_6, or, equivalently, when the constant formula_7. Geometrically, the graph of the function must pass through the origin.

</doc>
<doc id="101863" url="https://en.wikipedia.org/wiki?curid=101863" title="Linear independence">
Linear independence

In the theory of vector spaces the concept of linear dependence and linear independence of the vectors in a subset of the vector space is central to the definition of dimension. A set of vectors is said to be linearly dependent if one of the vectors in the set can be defined as a linear combination of the other vectors. If no vector in the set can be written in this way, then the vectors are said to be linearly independent.
A vector space can be of finite dimension or infinite dimension depending on the number of linearly independent basis vectors. The definition of linear dependence and the ability to determine whether a subset of vectors in a vector space are linearly dependent are central to determining a set of basis vectors for a vector space.
Definition.
The vectors in a subset "S"={"v"1,"v"2...,"v""n"} of a vector space "V" are said to be "linearly dependent", if there exist a "finite" number of "distinct" vectors "v"1, "v"2, ..., "vk" in "S" and scalars "a"1, "a"2, ..., "ak", not all zero, such that
where zero denotes the zero vector.
Notice that if not all of the scalars are zero, then at least one is non-zero, say "a"1, in which case this equation can be written in the form
Thus, v1 is shown to be a linear combination of the remaining vectors.
The vectors in a set "T"={v1,v2...,vn} are said to be "linearly independent" if the equation
can only be satisfied by "a"i=0 for i=1..., n. This implies that no vector in the set can be represented as a linear combination of the remaining vectors in the set. In other words, a set of vectors is linearly independent if the only representations of 0 as a linear combination of its vectors is the trivial representation in which all the scalars "a"i are zero.
Infinite dimensions.
In order to allow the number of linearly independent vectors in a vector space to be countably infinite, it is useful to define linear dependence as follows. More generally, let "V" be a vector space over a field "K", and let {"v""i" | "i"∈"I"} be a family of elements of "V". The family is "linearly dependent" over "K" if there exists a family {"a""j" | "j"∈"J"} of elements of "K", not all zero, such that
where the index set "J" is a nonempty, finite subset of "I".
A set "X" of elements of "V" is "linearly independent" if the corresponding family {"x"}x∈"X" is linearly independent. Equivalently, a family is dependent if a member is in the linear span of the rest of the family, i.e., a member is a linear combination of the rest of the family. The trivial case of the empty family must be regarded as linearly independent for theorems to apply.
A set of vectors which is linearly independent and spans some vector space, forms a basis for that vector space. For example, the vector space of all polynomials in "x" over the reals has the (infinite) subset {1, "x", "x"2, ...} as a basis.
Geometric meaning.
A geographic example may help to clarify the concept of linear independence. A person describing the location of a certain place might say, "It is 3 miles north and 4 miles east of here." This is sufficient information to describe the location, because the geographic coordinate system may be considered as a 2-dimensional vector space (ignoring altitude and the curvature of the Earth's surface). The person might add, "The place is 5 miles northeast of here." Although this last statement is "true", it is not necessary.
In this example the "3 miles north" vector and the "4 miles east" vector are linearly independent. That is to say, the north vector cannot be described in terms of the east vector, and vice versa. The third "5 miles northeast" vector is a linear combination of the other two vectors, and it makes the set of vectors "linearly dependent", that is, one of the three vectors is unnecessary.
Also note that if altitude is not ignored, it becomes necessary to add a third vector to the linearly independent set. In general, "n" linearly independent vectors are required to describe any location in "n"-dimensional space.
Evaluating linear independence.
Vectors in R2.
Three vectors: Consider the set of vectors "v"1 = (1, 1), "v"2 = (−3, 2) and "v"3 = (2, 4), then the condition for linear dependence seeks a set of non-zero scalars, such that
or
Row reduce this matrix equation by subtracting the first equation from the second to obtain,
Continue the row reduction by (i) dividing the second equation by 5, and then (ii) multiplying by 3 and adding to the first equation, that is
We can now rearrange this equation to obtain 
which shows that non-zero "a"i exist "v"3 = (2, 4) can be defined in terms of "v"1 = (1, 1), "v"2 = (−3, 2). Thus, the three vectors are linearly dependent.
Two vectors: Now consider the linear dependence of the two vectors "v"1 = (1, 1), "v"2 = (−3, 2), and check, 
or
The same row reduction presented above yields,
This shows that "a"i = 0, which means that the vectors "v"1 = (1, 1) and "v"2 = (−3, 2) are linearly independent.
Vectors in R4.
In order to determine if the three vectors in R4,
are linearly dependent, form the matrix equation,
Row reduce this equation to obtain,
Rearrange to solve for v3 and obtain,
This equation is easily solved to define non-zero "a"i,
where "a"3 can be chosen arbitrarily. Thus, the vectors "v"1, "v"2 and "v"3 are linearly dependent.
Alternative method using determinants.
An alternative method uses the fact that "n" vectors in formula_18 are linearly independent if and only if the determinant of the matrix formed by taking the vectors as its columns is non-zero.
In this case, the matrix formed by the vectors is
We may write a linear combination of the columns as
We are interested in whether "A"Λ = 0 for some nonzero vector Λ. This depends on the determinant of "A", which is
Since the determinant is non-zero, the vectors (1, 1) and (−3, 2) are linearly independent.
Otherwise, suppose we have "m" vectors of "n" coordinates, with "m" < "n". Then "A" is an "n"×"m" matrix and Λ is a column vector with "m" entries, and we are again interested in "A"Λ = 0. As we saw previously, this is equivalent to a list of "n" equations. Consider the first "m" rows of "A", the first "m" equations; any solution of the full list of equations must also be true of the reduced list. In fact, if 〈"i"1...,"i""m"〉 is any list of "m" rows, then the equation must be true for those rows.
Furthermore, the reverse is true. That is, we can test whether the "m" vectors are linearly dependent by testing whether
for all possible lists of "m" rows. (In case "m" = "n", this requires only one determinant, as above. If "m" > "n", then it is a theorem that the vectors must be linearly dependent.) This fact is valuable for theory; in practical calculations more efficient methods are available.
Natural basis vectors.
Let "V" = R"n" and consider the following elements in "V", known as the natural basis vectors:
Then e1, e2, ..., en are linearly independent.
Proof.
Suppose that "a"1, "a"2, ..., "an" are elements of R such that
Since
then "ai" = 0 for all "i" in {1, ..., "n"}.
Linear independence of basis functions.
Let "V" be the vector space of all functions of a real variable "t". Then the functions "et" and "e"2"t" in "V" are linearly independent.
Proof.
Suppose "a" and "b" are two real numbers such that
for "all" values of "t". We need to show that "a" = 0 and "b" = 0. In order to do this, we divide through by "e""t" (which is never zero) and subtract to obtain
In other words, the function "be""t" must be independent of "t", which only occurs when "b" = 0. It follows that "a" is also zero.
Projective space of linear dependences.
A linear dependence among vectors v1, ..., v"n" is a tuple ("a"1, ..., "a""n") with "n" scalar components, not all zero, such that
If such a linear dependence exists, then the "n" vectors are linearly dependent. It makes sense to identify two linear dependences if one arises as a non-zero multiple of the other, because in this case the two describe the same linear relationship among the vectors. Under this identification, the set of all linear dependences among v1, ..., v"n" is a projective space.
Linear dependence between random variables.
The covariance is sometimes called a measure of "linear dependence" between two random variables. That does not mean the same thing as in the context of linear algebra. When the covariance is normalized, one obtains the correlation matrix. From it, one can obtain the Pearson coefficient, which gives the goodness of the fit for the best possible linear function describing the relation between the variables. In this sense covariance is a linear gauge of dependence.

</doc>
<doc id="484872" url="https://en.wikipedia.org/wiki?curid=484872" title="Linear least squares (mathematics)">
Linear least squares (mathematics)

In statistics and mathematics, linear least squares is an approach fitting a mathematical or statistical model to data in cases where the idealized value provided by the model for any data point is expressed linearly in terms of the unknown parameters of the model. The resulting fitted model can be used to summarize the data, to predict unobserved values from the same system, and to understand the mechanisms that may underlie the system.
Mathematically, linear least squares is the problem of approximately solving an overdetermined system of linear equations, where the best approximation is defined as that which minimizes the sum of squared differences between the data values and their corresponding modeled values. The approach is called "linear" least squares since the assumed function is linear in the parameters to be estimated. Linear least squares problems are convex and have a closed-form solution that is unique, provided that the number of data points used for fitting equals or exceeds the number of unknown parameters, except in special degenerate situations. In contrast, non-linear least squares problems generally must be solved by an iterative procedure, and the problems can be non-convex with multiple optima for the objective function. If prior distributions are available, then even an underdetermined system can be solved using the Bayesian MMSE estimator.
In statistics, linear least squares problems correspond to a particularly important type of statistical model called linear regression which arises as a particular form of regression analysis. One basic form of such a model is an ordinary least squares model. The present article concentrates on the mathematical aspects of linear least squares problems, with discussion of the formulation and interpretation of statistical regression models and statistical inferences related to these being dealt with in the articles just mentioned. See outline of regression analysis for an outline of the topic.
Example.
As a result of an experiment, four formula_1 data points were obtained, formula_2 formula_3 formula_4 and formula_5 (shown in red in the picture on the right). We hope to find a line formula_6 that best fits these four points. In other words, we would like to find the numbers formula_7 and formula_8 that approximately solve the overdetermined linear system
of four equations in two unknowns in some "best" sense.
The "error", at each point, between the curve fit and the data is the difference between the right- and left-hand sides of the equations above. The least squares approach to solving this problem is to try to make as small as possible the sum of the squares of these errors; that is, to find the minimum of the function
The minimum is determined by calculating the partial derivatives of formula_11 with respect to formula_7 and formula_8 and setting them to zero
This results in a system of two equations in two unknowns, called the normal equations, which give, when solved
and the equation formula_18 of the line of best fit. The residuals, that is, the discrepancies between the formula_19 values from the experiment and the formula_19 values calculated using the line of best fit are then found to be formula_21 formula_22 formula_23 and formula_24 (see the picture on the right). The minimum value of the sum of squares of the residuals is formula_25
More generally, one can have formula_26 regressors formula_27, and a linear model
Using a quadratic model.
Importantly, in "linear least squares", we are not restricted to using a line as the model as in the above example. For instance, we could have chosen the restricted quadratic model formula_29. This model is still linear in the formula_7 parameter, so we can still perform the same analysis, constructing a system of equations from the data points:
The partial derivatives with respect to the parameters (this time there is only one) are again computed and set to 0:
formula_32
and solved
formula_33
leading to the resulting best fit model formula_34
The general problem.
Consider an overdetermined system 
of "m" linear equations in "n" unknown coefficients, "β"1,"β"2,…,"β""n", with "m" > "n". (Note: for a linear model as above, not all of formula_36 contains information on the data points. The first column is populated with ones, formula_37, only the other columns contain actual data, and "n" = number of regressors + 1).
This can be written in matrix form as 
where
Such a system usually has no solution, so the goal is instead to find the coefficients β which fit the equations "best," in the sense of solving the quadratic minimization problem
where the objective function "S" is given by
A justification for choosing this criterion is given in properties below. This minimization problem has a unique solution, provided that the "n" columns of the matrix "X" are linearly independent, given by solving the normal equations
The matrix formula_43 is known as the Gramian matrix of formula_44, which possesses several nice properties such as being a positive semi-definite matrix.
Following is one generalized example which shows how to find a best fit line using least squares method (in two dimension). Later, a MATLAB code implementation of the same is also written which can help one visualize graphically.
Let us consider 3 points (x1,y1), (x2,y2) and (x3,y3) in a two dimensional frame.
Now, assume that the best fit line is in the form: y = C + Dx. Substituting the three points in this equation will give us three equations.
These three equations can be written in the form of a matrix equation of the type Ax=b.
Projection of matrix A on b gives us x. This can be represented by the formula : 
We get beta from this expression; i.e. formula_46 is x. Hence, this follows that we have obtained C and D of the equation 
y = C + Dx.
Hence, the best fit line obtained.
Matlab code implementation for better understanding in visuals:
Python code using the same variable naming as the Matlab code:
Derivation of the normal equations.
Define the formula_47th residual to be 
Then formula_49 can be rewritten
"S" is minimized when its gradient vector is zero. (This follows by definition: if the gradient vector is not zero, there is a direction in which we can move to minimize it further - see maxima and minima.) The elements of the gradient vector are the partial derivatives of "S" with respect to the parameters:
The derivatives are
Substitution of the expressions for the residuals and the derivatives into the gradient equations gives
Thus if formula_54 minimizes "S", we have
Upon rearrangement, we obtain the normal equations:
The normal equations are written in matrix notation as
The solution of the normal equations yields the vector formula_46 of the optimal parameter values.
Derivation directly in terms of matrices.
The normal equations can be derived directly from a matrix representation of the problem as follows. The objective is to minimize
Note that :formula_60 has the dimension 1x1 (the number of columns of formula_61), so it is a scalar and equal to its own transpose, hence formula_62
and the quantity to minimize becomes 
Differentiating this with respect to formula_64 and equating to zero to satisfy the first-order conditions gives
which is equivalent to the above-given normal equations. A sufficient condition for satisfaction of the second-order conditions for a minimum is that formula_44 have full column rank, in which case formula_43 is positive definite.
Derivation without calculus.
When formula_43 is positive definite, the formula for the minimizing value of formula_69 can be derived without the use of derivatives. The quantity
can be written as 
where formula_72 depends only on formula_73 and formula_74, and formula_75 is the inner product defined by 
It follows that formula_77 is equal to 
and therefore minimized exactly when
Computation.
A general approach to the least squares problem formula_80 can be described as follows. Suppose that we can find an "n" by "m" matrix S 
such that XS is an 
orthogonal projection onto the image of X. Then a solution to our minimization problem is given by 
simply because 
is exactly a sought for orthogonal projection of formula_83 onto an image of X 
(see the picture below and note that as explained in the
next section the image of X is just a subspace generated by column vectors of X). 
A few popular ways to find such a matrix "S" are described below.
Inverting the matrix of the normal equations.
The algebraic solution of the normal equations can be written as
where "X" + is the Moore–Penrose pseudoinverse of "X". Although this equation is correct, and can work in many applications, it is not computationally efficient to invert the normal equations matrix (the Gramian matrix). An exception occurs in numerical smoothing and differentiation where an analytical expression is required.
If the matrix "X"T"X" is well-conditioned and positive definite, implying that it has full rank, the normal equations can be solved directly by using the Cholesky decomposition "R"T"R", where "R" is an upper triangular matrix, giving:
The solution is obtained in two stages, a forward substitution step, solving for z:
followed by a backward substitution, solving for formula_46
Both substitutions are facilitated by the triangular nature of "R".
See example of linear regression for a worked-out numerical example with three parameters.
Orthogonal decomposition methods.
Orthogonal decomposition methods of solving the least squares problem are slower than the normal equations method but are more numerically stable because they avoid forming the product "X"T"X".
The residuals are written in matrix notation as
The matrix "X" is subjected to an orthogonal decomposition, e.g., the QR decomposition as follows. 
where "Q" is an "m"×"n" orthogonal matrix ("Q"T"Q=I") and "R" is an "n"×"n" upper triangular matrix with formula_91.
The residual vector is left-multiplied by "Q"T. 
Because "Q" is orthogonal, the sum of squares of the residuals, "s", may be written as:
Since v doesn't depend on β, the minimum value of "s" is attained when the upper block, u, is zero. Therefore the parameters are found by solving:
These equations are easily solved as "R" is upper triangular.
An alternative decomposition of "X" is the singular value decomposition (SVD)
where "U" is "m" by "m" orthogonal matrix, "V" is "n" by "n" orthogonal matrix and formula_96 is an "m" by "n" matrix with all its elements outside of the main diagonal equal to "0". The pseudoinverse of formula_96 is easily obtained by inverting its non-zero diagonal elements and transposing. Hence,
where "P" is obtained from formula_96 by replacing its non-zero diagonal elements with ones. Since formula_100 (the property of pseudoinverse), the matrix formula_101 is an orthogonal projection onto the image (column-space) of "X". In accordance with a general approach described in the introduction above (find XS which is an orthogonal projection), 
and thus,
is a solution of a least squares problem. This method is the most computationally intensive, but is particularly useful if the normal equations matrix, "X"T"X", is very ill-conditioned (i.e. if its condition number multiplied by the machine's relative round-off error is appreciably large). In that case, including the smallest singular values in the inversion merely adds numerical noise to the solution. This can be cured with the truncated SVD approach, giving a more stable and exact answer, by explicitly setting to zero all singular values below a certain threshold and so ignoring them, a process closely related to factor analysis.
Properties of the least-squares estimators.
The gradient equations at the minimum can be written as
A geometrical interpretation of these equations is that the vector of residuals, formula_105 is orthogonal to the column space of "X", since the dot product formula_106 is equal to zero for "any" conformal vector, v. This means that formula_107 is the shortest of all possible vectors formula_108, that is, the variance of the residuals is the minimum possible. This is illustrated at the right. 
Introducing formula_109 and a matrix "K" with the assumption that a matrix formula_110 is non-singular and "K"T "X" = 0 (cf. Orthogonal projections), the residual vector should satisfy the following equation:
The equation and solution of linear least squares are thus described as follows:
If the experimental errors, formula_114, are uncorrelated, have a mean of zero and a constant variance, formula_115, the Gauss-Markov theorem states that the least-squares estimator, formula_46, has the minimum variance of all estimators that are linear combinations of the observations. In this sense it is the best, or optimal, estimator of the parameters. Note particularly that this property is independent of the statistical distribution function of the errors. In other words, "the distribution function of the errors need not be a normal distribution". However, for some probability distributions, there is no guarantee that the least-squares solution is even possible given the observations; still, in such cases it is the best estimator that is both linear and unbiased.
For example, it is easy to show that the arithmetic mean of a set of measurements of a quantity is the least-squares estimator of the value of that quantity. If the conditions of the Gauss-Markov theorem apply, the arithmetic mean is optimal, whatever the distribution of errors of the measurements might be.
However, in the case that the experimental errors do belong to a normal distribution, the least-squares estimator is also a maximum likelihood estimator.
These properties underpin the use of the method of least squares for all types of data fitting, even when the assumptions are not strictly valid.
Limitations.
An assumption underlying the treatment given above is that the independent variable, "x", is free of error. In practice, the errors on the measurements of the independent variable are usually much smaller than the errors on the dependent variable and can therefore be ignored. When this is not the case, total least squares or more generally errors-in-variables models, or "rigorous least squares", should be used. This can be done by adjusting the weighting scheme to take into account errors on both the dependent and independent variables and then following the standard procedure.
In some cases the (weighted) normal equations matrix "X"T"X" is ill-conditioned. When fitting polynomials the normal equations matrix is a Vandermonde matrix. Vandermonde matrices become increasingly ill-conditioned as the order of the matrix increases. In these cases, the least squares estimate amplifies the measurement noise and may be grossly inaccurate. Various regularization techniques can be applied in such cases, the most common of which is called ridge regression. If further information about the parameters is known, for example, a range of possible values of formula_117, then various techniques can be used to increase the stability of the solution. For example, see constrained least squares.
Another drawback of the least squares estimator is the fact that the norm of the residuals, formula_118 is minimized, whereas in some cases one is truly interested in obtaining small error in the parameter formula_117, e.g., a small value of formula_120. However, since the true parameter formula_121 is necessarily unknown, this quantity cannot be directly minimized. If a prior probability on formula_46 is known, then a Bayes estimator can be used to minimize the mean squared error, formula_123. The least squares method is often applied when no prior is known. Surprisingly, when several parameters are being estimated jointly, better estimators can be constructed, an effect known as Stein's phenomenon. For example, if the measurement error is Gaussian, several estimators are known which dominate, or outperform, the least squares technique; the best known of these is the James–Stein estimator. This is an example of more general shrinkage estimators that have been applied to regression problems.
Weighted linear least squares.
In some cases the observations may be weighted—for example, they may not be equally reliable. In this case, one can minimize the weighted sum of squares:
where "w""i" > 0 is the weight of the "i"th observation, and "W" is the diagonal matrix of such weights.
The weights should, ideally, be equal to the reciprocal of the variance of the measurement. 
The normal equations are then:
This method is used in iteratively reweighted least squares.
Parameter errors and correlation.
The estimated parameter values are linear combinations of the observed values
Therefore an expression for the residuals (i.e., the "estimated" errors in the parameters) can be obtained by error propagation from the errors in the observations. Let the variance-covariance matrix for the observations be denoted by "M" and that of the parameters by "Mβ". Then,
When "W" = "M" −1 this simplifies to
When unit weights are used ("W" = "I") it is implied that the experimental errors are uncorrelated and all equal: "M" = "σ"2"I", where "σ"2 is the variance of an observation, and "I" is the identity matrix. In this case "σ"2 is approximated by formula_129, where "S" is the minimum value of the objective function
The denominator, "m" − "n", is the number of degrees of freedom; see effective degrees of freedom for generalizations for the case of correlated observations. In all cases, the variance of the parameter formula_131 is given by formula_132 and the covariance between parameters formula_131 and formula_134 is given by formula_135. Standard deviation is the square root of variance, and the correlation coefficient is given by formula_136. These error estimates reflect only random errors in the measurements. The true uncertainty in the parameters is larger due to the presence of systematic errors which, by definition, cannot be quantified.
Note that even though the observations may be un-correlated, the parameters are typically correlated.
Parameter confidence limits.
It is often "assumed", for want of any concrete evidence but often appealing to the central limit theorem -- see Normal distribution#Occurrence -- that the error on each observation belongs to a normal distribution with a mean of zero and standard deviation formula_115. Under that assumption the following probabilities can be derived for a single scalar parameter estimate in terms of its estimated standard error formula_138 (given here):
The assumption is not unreasonable when "m" » "n". If the experimental errors are normally distributed the parameters will belong to a Student's t-distribution with "m" − "n" degrees of freedom. When "m" » "n" Student's t-distribution approximates a normal distribution. Note, however, that these confidence limits cannot take systematic error into account. Also, parameter errors should be quoted to one significant figure only, as they are subject to sampling error.
When the number of observations is relatively small, Chebychev's inequality can be used for an upper bound on probabilities, regardless of any assumptions about the distribution of experimental errors: the maximum probabilities that a parameter will be more than 1, 2 or 3 standard deviations away from its expectation value are 100%, 25% and 11% respectively.
Residual values and correlation.
The residuals are related to the observations by
where "H" is the idempotent matrix known as the hat matrix:
and "I" is the identity matrix. The variance-covariance matrix of the residuals, Mr is given by
Thus the residuals are correlated, even if the observations are not.
When formula_145,
The sum of residual values is equal to zero whenever the model function contains a constant term. Left-multiply the expression for the residuals by "X"T:
Say, for example, that the first term of the model is a constant, so that formula_148 for all "i". In that case it follows that
Thus, in the motivational example, above, the fact that the sum of residual values is equal to zero it is not accidental but is a consequence of the presence of the constant term, α, in the model. 
If experimental error follows a normal distribution, then, because of the linear relationship between residuals and observations, so should residuals, but since the observations are only a sample of the population of all possible observations, the residuals should belong to a Student's t-distribution. Studentized residuals are useful in making a statistical test for an outlier when a particular residual appears to be excessively large.
Objective function.
The optimal value of the objective function, found by substituting in the optimal expression for the coefficient vector, can be written as
the latter equality holding since ("I" – "H") is symmetric and idempotent. It can be shown from this that under an appropriate assignment of weights the expected value of "S" is "m-n". If instead unit weights are assumed, the expected value of "S" is formula_151, where formula_152 is the variance of each observation.
If it is assumed that the residuals belong to a normal distribution, the objective function, being a sum of weighted squared residuals, will belong to a chi-squared (formula_153) distribution with "m-n" degrees of freedom. Some illustrative percentile values of formula_153 are given in the following table.
These values can be used for a statistical criterion as to the goodness-of-fit. When unit weights are used, the numbers should be divided by the variance of an observation.
Constrained linear least squares.
Often it is of interest to solve a linear least squares problem with an additional constraint on the solution. With constrained linear least squares, the original equation
must be fit as closely as possible (in the least squares sense) while ensuring that some other property of formula_156 is maintained. There are often special purpose algorithms for solving such problems efficiently. Some examples of constraints are given below:
When the constraint only applies to some of the variables, the mixed problem may be solved using separable least squares by letting formula_167 and formula_168 represent the unconstrained (1) and constrained (2) components. Then substituting the least squares solution for formula_169, i.e.
back into the original expression gives (following some rearrangement) an equation that can be solved as a purely constrained problem in formula_171.
where formula_173 is a projection matrix. Following the constrained estimation of formula_174 the vector formula_175 is obtained from the expression above.
Typical uses and applications.
Uses in data fitting.
The primary application of linear least squares is in data fitting. Given a set of "m" data points formula_178 consisting of experimentally measured values taken at "m" values formula_179 of an independent variable (formula_180 may be scalar or vector quantities), and given a model function formula_181 with formula_182 it is desired to find the parameters formula_134 such that the model function "best" fits the data. In linear least squares, linearity is meant to be with respect to parameters formula_184 so 
Here, the functions formula_186 may be nonlinear with respect to the variable x. 
Ideally, the model function fits the data exactly, so 
for all formula_188 This is usually not possible in practice, as there are more data points than there are parameters to be determined. The approach chosen then is to find the minimal possible value of the sum of squares of the residuals
so to minimize the function 
After substituting for formula_191 and then for formula_192, this minimization problem becomes the quadratic minimization problem above with
and the best fit can be found by solving the normal equations.
Further discussion.
The "numerical methods for linear least squares" are important because linear regression models are among the most important types of model, both as formal statistical models and for exploration of data-sets. The majority of statistical computer packages contain facilities for regression analysis that make use of linear least squares computations. Hence it is appropriate that considerable effort has been devoted to the task of ensuring that these computations are undertaken efficiently and with due regard to round-off error.
Individual statistical analyses are seldom undertaken in isolation, but rather are part of a sequence of investigatory steps. Some of the topics involved in considering numerical methods for linear least squares relate to this point. Thus important topics can be
Fitting of linear models by least squares often, but not always, arise in the context of statistical analysis. It can therefore be important that considerations of computation efficiency for such problems extend to all of the auxiliary quantities required for such analyses, and are not restricted to the formal solution of the linear least squares problem.
Rounding errors.
Matrix calculations, like any other, are affected by rounding errors. An early summary of these effects, regarding the choice of computation methods for matrix inversion, was provided by Wilkinson.

</doc>
<doc id="18102" url="https://en.wikipedia.org/wiki?curid=18102" title="Linear map">
Linear map

In mathematics, a linear map (also called a linear mapping, linear transformation or, in some contexts, linear function) is a mapping between two modules (including vector spaces) that preserves (in the sense defined below) the operations of addition and scalar multiplication. Linear maps can generally be represented as matrices, and simple examples include rotation and reflection linear transformations.
An important special case is when , in which case the map is called a linear operator, or an endomorphism of . Sometimes the term "linear function" has the same meaning as "linear map", while in analytic geometry it does not.
A linear map always maps linear subspaces onto linear subspaces (possibly of a lower dimension); for instance it maps a plane through the origin to a plane, straight line or point.
In the language of abstract algebra, a linear map is a module homomorphism. In the language of category theory it is a morphism in the category of modules over a given ring.
Definition and first consequences.
Let "V" and "W" be vector spaces over the same field "K". A function is said to be a "linear map" if for any two vectors x and y in "V" and any scalar "α" in "K", the following two conditions are satisfied:
This is equivalent to requiring the same for any linear combination of vectors, i.e. that for any vectors and scalars , the following equality holds:
Denoting the zero elements of the vector spaces "V" and "W" by 0"V" and 0"W" respectively, it follows that because letting in the equation for homogeneity of degree 1,
Occasionally, "V" and "W" can be considered to be vector spaces over different fields. It is then necessary to specify which of these ground fields is being used in the definition of "linear". If "V" and "W" are considered as spaces over the field "K" as above, we talk about "K"-linear maps. For example, the conjugation of complex numbers is an R-linear map , but it is not C-linear.
A linear map from "V" to "K" (with "K" viewed as a vector space over itself) is called a linear functional.
These statements generalize to any left-module "R""M" over a ring "R" without modification, and to any right-module upon reversing of the scalar multiplication.
Matrices.
If "V" and "W" are finite-dimensional vector spaces and a basis is defined for each vector space, then every linear map from "V" to "W" can be represented by a matrix. This is useful because it allows concrete calculations. Matrices yield examples of linear maps: if "A" is a real matrix, then describes a linear map (see Euclidean space).
Let {v1, ..., v"n"} be a basis for "V". Then every vector v in "V" is uniquely determined by the coefficients "c"1, ..., "c""n" in the field R:
If is a linear map,
which implies that the function "f" is entirely determined by the vectors "f"(v1), ..., "f"(v"n"). Now let be a basis for "W". Then we can represent each vector "f"(v"j") as
Thus, the function "f" is entirely determined by the values of "a""ij". If we put these values into an matrix "M", then we can conveniently use it to compute the vector output of "f" for any vector in "V". To get "M", every column "j" of "M" is a vector
corresponding to "f"(v"j") as defined above. To define it more clearly, for some column "j" that corresponds to the mapping "f"(v"j"),
where M is the matrix of "f". The symbol ∗ denotes that there are other columns which together with column "j" make up a total of "n" columns of "M". In other words, every column has a corresponding vector "f"(v"j") whose coordinates "a"1"j", ..., "a""mj" are the elements of column "j". A single linear map may be represented by many matrices. This is because the values of the elements of a matrix depend on the bases chosen.
The matrices of a linear transformation can be represented visually:
Such that starting in the bottom left corner formula_18 and looking for the bottom right corner formula_19, one would left-multiply—that is, formula_20. The equivalent method would be the "longer" method going clockwise from the same point such that formula_21 is left-multiplied with formula_22, or formula_23.
Examples of linear transformation matrices.
In two-dimensional space R2 linear maps are described by 2 × 2 real matrices. These are some examples:
Forming new linear maps from given ones.
The composition of linear maps is linear: if and are linear, then so is their composition . It follows from this that the class of all vector spaces over a given field "K", together with "K"-linear maps as morphisms, forms a category.
The inverse of a linear map, when defined, is again a linear map.
If and are linear, then so is their pointwise sum (which is defined by .
If is linear and "a" is an element of the ground field "K", then the map "af", defined by , is also linear.
Thus the set of linear maps from "V" to "W" itself forms a vector space over "K", sometimes denoted . Furthermore, in the case that , this vector space (denoted End("V")) is an associative algebra under composition of maps, since the composition of two linear maps is again a linear map, and the composition of maps is always associative. This case is discussed in more detail below.
Given again the finite-dimensional case, if bases have been chosen, then the composition of linear maps corresponds to the matrix multiplication, the addition of linear maps corresponds to the matrix addition, and the multiplication of linear maps with scalars corresponds to the multiplication of matrices with scalars.
Endomorphisms and automorphisms.
A linear transformation "f": "V" → "V" is an endomorphism of "V"; the set of all such endomorphisms End("V") together with addition, composition and scalar multiplication as defined above forms an associative algebra with identity element over the field "K" (and in particular a ring). The multiplicative identity element of this algebra is the identity map id: "V" → "V".
An endomorphism of "V" that is also an isomorphism is called an automorphism of "V". The composition of two automorphisms is again an automorphism, and the set of all automorphisms of "V" forms a group, the automorphism group of "V" which is denoted by Aut("V") or GL("V"). Since the automorphisms are precisely those endomorphisms which possess inverses under composition, Aut("V") is the group of units in the ring End("V").
If "V" has finite dimension "n", then End("V") is isomorphic to the associative algebra of all "n" × "n" matrices with entries in "K". The automorphism group of "V" is isomorphic to the general linear group GL("n", "K") of all "n" × "n" invertible matrices with entries in "K".
Kernel, image and the rank–nullity theorem.
If "f" : "V" → "W" is linear, we define the kernel and the image or range of "f" by
ker("f") is a subspace of "V" and im("f") is a subspace of "W". The following dimension formula is known as the rank–nullity theorem:
The number dim(im("f")) is also called the "rank of f" and written as rank("f"), or sometimes, ρ("f"); the number dim(ker("f")) is called the "nullity of f" and written as null("f") or ν("f"). If "V" and "W" are finite-dimensional, bases have been chosen and "f" is represented by the matrix "A", then the rank and nullity of "f" are equal to the rank and nullity of the matrix "A", respectively.
Cokernel.
A subtler invariant of a linear transformation is the "co"kernel, which is defined as
This is the "dual" notion to the kernel: just as the kernel is a "sub"space of the "domain," the co-kernel is a "quotient" space of the "target."
Formally, one has the exact sequence
These can be interpreted thus: given a linear equation "f"(v) = w to solve,
The dimension of the co-kernel and the dimension of the image (the rank) add up to the dimension of the target space. For finite dimensions, this means that the dimension of the quotient space "W"/"f"("V") is the dimension of the target space minus the dimension of the image.
As a simple example, consider the map "f": R2 → R2, given by "f"("x", "y") = (0, "y"). Then for an equation "f"("x", "y") = ("a", "b") to have a solution, we must have "a" = 0 (one constraint), and in that case the solution space is ("x", "b") or equivalently stated, (0, "b") + ("x", 0), (one degree of freedom). The kernel may be expressed as the subspace ("x", 0) < "V": the value of "x" is the freedom in a solution – while the cokernel may be expressed via the map "W" → R, formula_37 given a vector ("a", "b"), the value of "a" is the "obstruction" to there being a solution.
An example illustrating the infinite-dimensional case is afforded by the map "f": R∞ → R∞, formula_38 with "b"1 = 0 and "b""n" + 1 = "an" for "n" > 0. Its image consists of all sequences with first element 0, and thus its cokernel consists of the classes of sequences with identical first element. Thus, whereas its kernel has dimension 0 (it maps only the zero sequence to the zero sequence), its co-kernel has dimension 1. Since the domain and the target space are the same, the rank and the dimension of the kernel add up to the same sum as the rank and the dimension of the co-kernel ( formula_39 ), but in the infinite-dimensional case it cannot be inferred that the kernel and the co-kernel of an endomorphism have the same dimension (0 ≠ 1). The reverse situation obtains for the map "h": R∞ → R∞, formula_40 with "cn" = "a""n" + 1. Its image is the entire target space, and hence its co-kernel has dimension 0, but since it maps all sequences in which only the first element is non-zero to the zero sequence, its kernel has dimension 1.
Index.
For a linear operator with finite-dimensional kernel and co-kernel, one may define "index" as:
namely the degrees of freedom minus the number of constraints.
For a transformation between finite-dimensional vector spaces, this is just the difference dim("V") − dim("W"), by rank–nullity. This gives an indication of how many solutions or how many constraints one has: if mapping from a larger space to a smaller one, the map may be onto, and thus will have degrees of freedom even without constraints. Conversely, if mapping from a smaller space to a larger one, the map cannot be onto, and thus one will have constraints even without degrees of freedom.
The index of an operator is precisely the Euler characteristic of the 2-term complex 0 → "V" → "W" → 0. In operator theory, the index of Fredholm operators is an object of study, with a major result being the Atiyah–Singer index theorem.
Algebraic classifications of linear transformations.
No classification of linear maps could hope to be exhaustive. The following incomplete list enumerates some important classifications that do not require any additional structure on the vector space.
Let "V" and "W" denote vector spaces over a field, "F". Let "T": "V" → "W" be a linear map.
Change of basis.
Given a linear map whose matrix is "A", in the basis "B" of the space it transforms vectors coordinates [u] as [v] = "A"[u]. As vectors change with the inverse of "B", its inverse transformation is [v] = "B"[v'].
Substituting this in the first expression
hence
Therefore the matrix in the new basis is "A′" = "B"−1"AB", being "B" the matrix of the given basis.
Therefore linear maps are said to be 1-co 1-contra -variant objects, or type (1, 1) tensors.
Continuity.
A "linear transformation" between topological vector spaces, for example normed spaces, may be continuous. If its domain and codomain are the same, it will then be a continuous linear operator. A linear operator on a normed linear space is continuous if and only if it is bounded, for example, when the domain is finite-dimensional. An infinite-dimensional domain may have discontinuous linear operators. 
An example of an unbounded, hence discontinuous, linear transformation is differentiation on the space of smooth functions equipped with the supremum norm (a function with small values can have a derivative with large values, while the derivative of 0 is 0). For a specific example, sin("nx")/"n" converges to 0, but its derivative cos("nx") does not, so differentiation is not continuous at 0 (and by a variation of this argument, it is not continuous anywhere).
Applications.
A specific application of linear maps is for geometric transformations, such as those performed in computer graphics, where the translation, rotation and scaling of 2D or 3D objects is performed by the use of a transformation matrix. Linear mappings also are used as a mechanism for describing change: for example in calculus correspond to derivatives; or in relativity, used as a device to keep track of the local transformations of reference frames. 
Another application of these transformations is in compiler optimizations of nested-loop code, and in parallelizing compiler techniques.

</doc>
<doc id="2485377" url="https://en.wikipedia.org/wiki?curid=2485377" title="List of baryons">
List of baryons

Baryons are composite particles made of three quarks, as opposed to mesons, which are composite particles made of one quark and one antiquark. Baryons and mesons are both hadrons, which are particles composed solely of quarks or both quarks and antiquarks. The term "baryon" is derived from the Greek "βαρύς" ("barys"), meaning "heavy", because, at the time of their naming, it was believed that baryons were characterized by having greater masses than other particles that were classed as matter.
Until a few years ago, it was believed that some experiments showed the existence of pentaquarks – baryons made of four quarks and one antiquark. The particle physics community as a whole did not view their existence as likely by 2006. On 13 July 2015, the LHCb collaboration at CERN reported results consistent with pentaquark states in the decay of bottom Lambda baryons (Λ).
Since baryons are composed of quarks, they participate in the strong interaction. Leptons, on the other hand, are not composed of quarks and as such do not participate in the strong interaction. The most famous baryons are the protons and neutrons that make up most of the mass of the visible matter in the universe, whereas electrons, the other major component of atoms, are leptons. Each baryon has a corresponding antiparticle known as an antibaryon in which quarks are replaced by their corresponding antiquarks. For example, a proton is made of two up quarks and one down quark, while its corresponding antiparticle, the antiproton, is made of two up antiquarks and one down antiquark.
Lists of baryons.
These lists detail all known and predicted baryons in total angular momentum "J" =  and "J" =  configurations with positive parity.
The symbols encountered in these lists are: "I" (isospin), "J" (total angular momentum), "P" (parity), u (up quark), d (down quark), s (strange quark), c (charm quark), b (bottom quark), "Q" (charge), "B" (baryon number), "S" (strangeness), "C" (charm), "B′" (bottomness), as well as a wide array of subatomic particles (hover for name). (See the "baryon" article for a detailed explanation of these symbols.)
Antibaryons are not listed in the tables; however, they simply would have all quarks changed to antiquarks, and "Q", "B", "S", "C", "B′", would be of opposite signs. Particles with † next to their names have been predicted by the Standard Model but not yet observed. Values in red have not been firmly established by experiments, but are predicted by the quark model and are consistent with the measurements.
"J""P" = + baryons.
† Particle has not yet been observed.<br>
[a] The masses of the proton and neutron are known with much better precision in atomic mass units (u) than in MeV/c2, due to the relatively poorly known value of the elementary charge. In atomic mass unit, the mass of the proton is whereas that of the neutron is . <br>
[b] At least 1035 years. See proton decay.<br>
[c] For free neutrons; in most common nuclei, neutrons are stable.<br>
[d] PDG reports the resonance width (Γ). Here the conversion τ =  is given instead. <br>
[e] Some controversy exists about this data.
"J""P" = + baryons.
† Particle has not yet been observed.<br>
[h] PDG reports the resonance width (Γ). Here the conversion τ =  is given instead.
Baryon resonance particles.
This table gives the name, quantum numbers (where known), and experimental status of baryons resonances confirmed by the PDG. Baryon resonance particles are excited baryon states with short half lives and higher masses. Despite significant research, the fundamental degrees of freedom behind baryon excitation spectra are still poorly understood. The spin-parity JP (when known) is given with each particle. For the strongly decaying particles, the JP values are considered to be part of the names, as is the mass for all resonances.

</doc>
<doc id="1749665" url="https://en.wikipedia.org/wiki?curid=1749665" title="List of finite simple groups">
List of finite simple groups

In mathematics, the classification of finite simple groups states that every finite simple group is cyclic, or alternating, or in one of 16 families of groups of Lie type, or one of 26 sporadic groups.
The list below gives all finite simple groups, together with their order, the size of the Schur multiplier, the size of the outer automorphism group, usually some small representations, and lists of all duplicates.
Summary.
The following table is a complete list of the 18 families of finite simple groups and the 26 sporadic simple groups, along with their orders. Any non-simple members of each family are listed, as well as any members duplicated within a family or between families. (In removing duplicates it is useful to note that no two finite simple groups have the same order, except that the group A8 = "A"3(2) and "A"2(4) both have order 20160, and that the group "Bn"("q") has the same order as "Cn"("q") for "q" odd, "n" > 2. The smallest of the latter pairs of groups are "B"3(3) and "C"3(3) which both have order 4585351680.)
There is an unfortunate conflict between the notations for the alternating groups A"n" and the groups of Lie type "An"("q"). Some authors use various different fonts for A"n" to distinguish them. In particular,
in this article we make the distinction by setting the alternating groups A"n" in Roman font and the Lie-type groups "An"("q") in italic.
In what follows, "n" is a positive integer, and "q" is a positive power of a prime number "p", with the restrictions noted.
Cyclic groups, "Zp".
Simplicity: Simple for "p" a prime number.
Order: "p"
Schur multiplier: Trivial.
Outer automorphism group: Cyclic of order "p" − 1.
Other names: "Z/pZ"
Remarks: These are the only simple groups that are not perfect.
Alternating groups, A"n", "n" > 4.
Simplicity: Solvable for "n" < 5, otherwise simple.
Order: "n"!/2 when "n" > 1.
Schur multiplier: 2 for "n" = 5 or "n" > 7, 6 for "n" = 6 or 7; see Covering groups of the alternating and symmetric groups
Outer automorphism group: In general 2. Exceptions: for "n" = 1, "n" = 2, it is trivial, and for "n" = 6, it has order 4 (elementary abelian).
Other names: "Altn".
Isomorphisms: A1 and A2 are trivial. A3 is cyclic of order 3. A4 is isomorphic to "A"1(3) (solvable). A5 is isomorphic to "A"1(4) and to "A"1(5). A6 is isomorphic to "A"1(9) and to the derived group "B"2(2)'. A8 is isomorphic to "A"3(2).
Remarks: An index 2 subgroup of the symmetric group of permutations of "n" points when "n" > 1.
Groups of Lie type.
Notation: "n" is a positive integer, "q" > 1 is a power of a prime number "p", and is the order of some underlying finite field. The order of the outer automorphism group is written as "d"·"f"·"g", where "d" is the order of the group of "diagonal automorphisms", "f" is the order of the (cyclic) group of "field automorphisms" (generated by a Frobenius automorphism), and "g" is the order of the group of "graph automorphisms" (coming from automorphisms of the Dynkin diagram).
Suzuki groups, 2"B"2(22"n"+1).
Simplicity: Simple for "n" ≥ 1. The group
2"B"2(2) is solvable.
Order:
"q"2
where
"q" = 22"n"+1.
Schur multiplier: Trivial for "n" ≠ 1, elementary abelian of order 4
for 2"B"2(8).
Outer automorphism group:
where "f" = 2"n" + 1.
Other names: Suz(22"n"+1), Sz(22"n"+1).
Isomorphisms: 2"B"2(2) is the Frobenius group of order 20.
Remarks: Suzuki group are Zassenhaus groups acting on sets of size (22"n"+1)2 + 1, and have 4 dimensional representations over the field with 22"n"+1 elements. They are the only non-cyclic simple groups whose order is not divisible by 3. They are not related to the sporadic Suzuki group.
Ree groups and Tits group, 2"F"4(22"n"+1).
Simplicity: Simple for "n" ≥ 1. The derived group 2"F"4(2)′ is simple of index 2
in 2"F"4(2), and is called the Tits group,
named for the Belgian mathematician Jacques Tits.
Order:
"q"12
where
"q" = 22"n"+1.
The Tits group has order 17971200 = 211 · 33 · 52 · 13.
Schur multiplier: Trivial for "n" ≥ 1 and for the Tits group.
Outer automorphism group:
where "f" = 2"n" + 1. Order 2 for the Tits group.
Remarks: Unlike the other simple groups of Lie type, the Tits group does not have a BN pair, though its automorphism group does so most authors count it as a sort of honorary group of Lie type.
Ree groups, 2"G"2(32"n"+1).
Simplicity: Simple for "n" ≥ 1. The group "2G2"(3) is not simple, but its derived group "2G2"(3)′ is a simple subgroup of index 3.
Order:
"q"3
where
"q" = 32"n"+1
Schur multiplier: Trivial for "n"≥1 and for 2"G"2(3)′.
Outer automorphism group:
where "f" = 2"n" + 1.
Other names: Ree(32"n"+1), R(32"n"+1), E2*(32"n"+1) .
Isomorphisms: The derived group 2"G"2(3)′ is isomorphic to "A"1(8).
Remarks: 2"G"2(32"n"+1) has a doubly transitive permutation representation on 33(2"n"+1) + 1 points and acts on a 7-dimensional vector space over the field with 32"n"+1 elements.
Sporadic groups.
Higman–Sims group, "HS".
Order: 29 · 32 · 53· 7 · 11 = 44352000
Schur multiplier: Order 2.
Outer automorphism group: Order 2.
Remarks: It acts as a rank 3 permutation group on the Higman Sims graph with 100 points, and is contained in "Co"3.
McLaughlin group, "McL".
Order: 27 · 36 · 53· 7 · 11 = 898128000
Schur multiplier: Order 3.
Outer automorphism group: Order 2.
Remarks: Acts as a rank 3 permutation group on the McLaughlin graph with 275 points, and is contained in "Co"3.
Held group, "He".
Order:
210 · 33 · 52· 73· 17 = 4030387200
Schur multiplier: Trivial.
Outer automorphism group: Order 2.
Other names: Held–Higman–McKay group, HHM, "F"7, HTH
Remarks: Centralizes an element of order 7 in the monster group.
Rudvalis group, "Ru".
Order:
214 · 33 · 53· 7 · 13 · 29 = 145926144000
Schur multiplier: Order 2.
Outer automorphism group: Trivial.
Remarks: The double cover acts on a 28 dimensional lattice over the Gaussian integers.
Suzuki sporadic group, "Suz".
Order: 213 · 37 · 52· 7 · 11 · 13 = 448345497600
Schur multiplier: Order 6.
Outer automorphism group: Order 2.
Other names: "Sz"
Remarks: The 6 fold cover acts on a 12 dimensional lattice over the Eisenstein integers. It is not related to the Suzuki groups of Lie type.
O'Nan group, "O'N".
Order:
29 · 34 · 5 · 73 · 11 · 19 · 31 = 460815505920
Schur multiplier: Order 3.
Outer automorphism group: Order 2.
Other names: O'Nan–Sims group, O'NS, O–S
Remarks:
The triple cover has two 45-dimensional representations over the field with 7 elements, exchanged by an outer automorphism.
Harada–Norton group, "HN".
Order:
214 · 36 · 56 · 7 · 11 · 19 = 273030912000000
Schur multiplier: Trivial.
Outer automorphism group: Order 2.
Other names: "F"5, "D"
Remarks: Centralizes an element of order 5 in the monster group.
Lyons group, "Ly".
Order:
28 · 37 · 56 · 7 · 11 · 31 · 37 · 67 = 51765179004000000
Schur multiplier: Trivial.
Outer automorphism group: Trivial.
Other names: Lyons–Sims group, "LyS"
Remarks: Has a 111 dimensional representation over the field with 5 elements.
Thompson group, "Th".
Order: 215 · 310 · 53 · 72 · 13 · 19 · 31 = 90745943887872000
Schur multiplier: Trivial.
Outer automorphism group: Trivial.
Other names: "F"3, "E"
Remarks: Centralizes an element of order 3 in the monster, and is contained in "E"8(3), so has a 248-dimensional representation over the field with 3 elements.
Baby Monster group, "B".
Order:
Schur multiplier: Order 2.
Outer automorphism group: Trivial.
Other names: "F"2
Remarks: The double cover is contained in the monster group. It has a representation of dimension 4371 over the complex numbers (with no nontrivial invariant product), and a representation of dimension 4370 over the field with 2 elements preserving a commutative but non-associative product.
Fischer–Griess Monster group, "M".
Order:
Schur multiplier: Trivial.
Outer automorphism group: Trivial.
Other names: "F"1, "M"1, Monster group, Friendly giant, Fischer's monster.
Remarks: Contains all but 6 of the other sporadic groups as subquotients. Related to monstrous moonshine. The monster is the automorphism group of the 196,883-dimensional Griess algebra and the infinite dimensional monster vertex operator algebra, and acts naturally on the monster Lie algebra.
Non-cyclic simple groups of small order.
 lists the 56 non-cyclic simple groups of order less than a million.

</doc>
<doc id="234969" url="https://en.wikipedia.org/wiki?curid=234969" title="List of integrals of inverse trigonometric functions">
List of integrals of inverse trigonometric functions

The following is a list of indefinite integrals (antiderivatives) of expressions involving the inverse trigonometric functions. For a complete list of integral formulas, see lists of integrals.

</doc>
<doc id="234906" url="https://en.wikipedia.org/wiki?curid=234906" title="List of integrals of trigonometric functions">
List of integrals of trigonometric functions

The following is a list of integrals (antiderivative functions) of trigonometric functions. For antiderivatives involving both exponential and trigonometric functions, see List of integrals of exponential functions. For a complete list of antiderivative functions, see Lists of integrals. For the special antiderivatives involving trigonometric functions, see Trigonometric integral.
Generally, if the function formula_1 is any trigonometric function, and formula_2 is its derivative,
In all formulas the constant "a" is assumed to be nonzero, and "C" denotes the constant of integration.

</doc>
<doc id="193837" url="https://en.wikipedia.org/wiki?curid=193837" title="List of matrices">
List of matrices

This page lists some important classes of matrices used in mathematics, science and engineering. A matrix (plural matrices, or less commonly matrixes) is a rectangular array of numbers called "entries". Matrices have a long history of both study and application, leading to diverse ways of classifying matrices. A first group is matrices satisfying concrete conditions of the entries, including constant matrices. An important example is the identity matrix given by
Further ways of classifying matrices are according to their eigenvalues or by imposing conditions on the product of the matrix with other matrices. Finally, many domains, both in mathematics and other sciences including physics and chemistry have particular matrices that are applied chiefly in these areas.
Matrices with explicitly constrained entries.
The following lists matrices whose entries are subject to certain conditions. Many of them apply to "square matrices" only, that is matrices with the same number of columns and rows. The main diagonal of a square matrix is the diagonal joining the upper left corner and the lower right one or equivalently the entries "a""i","i". The other diagonal is called anti-diagonal (or counter-diagonal).
Constant matrices.
The list below comprises matrices whose elements are constant for any given dimension (size) of matrix. The matrix entries will be denoted "aij". The table below uses the Kronecker delta δ"ij" for two integers "i" and "j" which is 1 if "i" = "j" and 0 else.
Matrices satisfying conditions on products or inverses.
A number of matrix-related notions is about properties of products or inverses of the given matrix. The matrix product of a "m"-by-"n" matrix "A" and a "n"-by-"k" matrix "B" is the "m"-by-"k" matrix "C" given by
This matrix product is denoted "AB". Unlike the product of numbers, matrix products are not commutative, that is to say "AB" need not be equal to "BA". A number of notions are concerned with the failure of this commutativity. An inverse of square matrix "A" is a matrix "B" (necessarily of the same dimension as "A") such that "AB" = "I". Equivalently, "BA" = "I". An inverse need not exist. If it exists, "B" is uniquely determined, and is also called "the" inverse of "A", denoted "A"−1.
Matrices used in statistics.
The following matrices find their main application in statistics and probability theory.
Matrices used in graph theory.
The following matrices find their main application in graph and network theory.

</doc>
<doc id="2377848" url="https://en.wikipedia.org/wiki?curid=2377848" title="List of mesons">
List of mesons

Mesons are unstable subatomic particles composed of one quark and one antiquark. They are part of the hadron particle family – particles made of quarks. The other members of the hadron family are the baryons – subatomic particles composed of three quarks. The main difference between mesons and baryons is that mesons have integer spin (thus are bosons) while baryons are fermions (half-integer spin). Because mesons are bosons, the Pauli exclusion principle does not apply to them. Because of this, they can act as force mediating particles on short distances, and thus play a part in processes such as the nuclear interaction.
Since mesons are composed of quarks, they participate in both the weak and strong interactions. Mesons with net electric charge also participate in the electromagnetic interaction. They are classified according to their quark content, total angular momentum, parity, and various other properties such as C-parity and G-parity. While no meson is stable, those of lower mass are nonetheless more stable than the most massive mesons, and are easier to observe and study in particle accelerators or in cosmic ray experiments. They are also typically less massive than baryons, meaning that they are more easily produced in experiments, and will exhibit higher-energy phenomena sooner than baryons would. For example, the charm quark was first seen in the J/Psi meson () in 1974, and the bottom quark in the upsilon meson () in 1977.
Each meson has a corresponding antiparticle (antimeson) where quarks are replaced by their corresponding antiquarks and vice versa. For example, a positive pion () is made of one up quark and one down antiquark; and its corresponding antiparticle, the negative pion (), is made of one up antiquark and one down quark. Some experiments show the evidence of "tetraquarks" – "exotic" mesons made of two quarks and two antiquarks, but the particle physics community as a whole does not view their existence as likely, although still possible.
The symbols encountered in these lists are: I ("isospin"), J ("total angular momentum"), P ("parity"), C ("C-parity"), G ("G-parity"), u ("up quark"), d ("down quark"), s ("strange quark"), c ("charm quark"), b ("bottom quark"), Q ("charge"), B ("baryon number"), S ("strangeness"), C ("charm"), and B′ ("bottomness"), as well as a wide array of subatomic particles (hover for name).
Summary table.
Because this table was initially derived from published results and many of those results were preliminary, as many as 64 of the mesons in the following table may not exist or have the wrong mass or quantum numbers.
Meson properties.
The following lists detail all known and predicted pseudoscalar (JP = 0−) and vector (JP = 1−) mesons.
The properties and quark content of the particles are tabulated below; for the corresponding antiparticles, simply change quarks into antiquarks (and vice versa) and flip the sign of Q, B, S, C, and B′. Particles with † next to their names have been predicted by the standard model but not yet observed. Values in red have not been firmly established by experiments, but are predicted by the quark model and are consistent with the measurements.
Pseudoscalar mesons.
[a] Makeup inexact due to non-zero quark masses.<br>
[b] PDG reports the resonance width (Γ). Here the conversion τ =  is given instead.<br>
[c] Strong eigenstate. No definite lifetime (see kaon notes below) <br>
[d] The mass of the and are given as that of the . However, it is known that a difference between the masses of the and on the order of exists.<br>
[e] Weak eigenstate. Makeup is missing small CP–violating term (see notes on neutral kaons below).
Vector mesons.
[f] PDG reports the resonance width (Γ). Here the conversion τ =  is given instead. <br>
[g] The exact value depends on the method used. See the given reference for detail.
Notes on neutral kaons.
There are two complications with neutral kaons:
Note that these issues also exist in principle for other neutral flavored mesons; however, the weak eigenstates are considered separate particles only for kaons because of their dramatically different lifetimes.

</doc>
<doc id="385334" url="https://en.wikipedia.org/wiki?curid=385334" title="List of particles">
List of particles

This is a list of the different types of particles found or believed to exist in the whole of the universe. For individual lists of the different particles, see the list below...
Elementary particles.
Elementary particles are particles with no measurable internal structure; that is, they are not composed of other particles. They are the fundamental objects of quantum field theory. Many families and sub-families of elementary particles exist. Elementary particles are classified according to their spin. Fermions have half-integer spin while bosons have integer spin. All the particles of the Standard Model have been experimentally observed, recently including the Higgs boson.
Fermions.
Fermions are one of the two fundamental classes of particles, the other being bosons. Fermion particles are described by Fermi–Dirac statistics and have quantum numbers described by the Pauli exclusion principle. They include the quarks and leptons, as well as any composite particles consisting of an odd number of these, such as all baryons and many atoms and nuclei.
Fermions have half-integer spin; for all known elementary fermions this is . All known fermions, except neutrinos, are also Dirac fermions; that is, each known fermion has its own distinct antiparticle. It is not known whether the neutrino is a Dirac fermion or a Majorana fermion. Fermions are the basic building blocks of all matter. They are classified according to whether they interact via the color force or not. In the Standard Model, there are 12 types of elementary fermions: six quarks and six leptons.
Quarks.
Quarks are the fundamental constituents of hadrons and interact via the strong interaction. Quarks are the only known carriers of fractional charge, but because they combine in groups of three (baryons) or in groups of two with antiquarks (mesons), only integer charge is observed in nature. Their respective antiparticles are the antiquarks, which are identical except for the fact that they carry the opposite electric charge (for example the up quark carries charge +, while the up antiquark carries charge −), color charge, and baryon number. There are six flavors of quarks; the three positively charged quarks are called "up-type quarks" and the three negatively charged quarks are called "down-type quarks".
Leptons.
Leptons do not interact via the strong interaction. Their respective antiparticles are the antileptons which are identical, except for the fact that they carry the opposite electric charge and lepton number. The antiparticle of an electron is an antielectron, which is nearly always called a "positron" for historical reasons. There are six leptons in total; the three charged leptons are called "electron-like leptons", while the neutral leptons are called "neutrinos". Neutrinos are known to oscillate, so that neutrinos of definite flavor do not have definite mass, rather they exist in a superposition of mass eigenstates. The hypothetical heavy right-handed neutrino, called a "sterile neutrino", has been left off the list. 
Bosons.
Bosons are one of the two fundamental classes of particles, the other being fermions. Bosons are characterized by Bose–Einstein statistics and all have integer spins. Bosons may be either elementary, like photons and gluons, or composite, like mesons.
The fundamental forces of nature are mediated by gauge bosons, and mass is believed to be created by the Higgs field. According to the Standard Model the elementary bosons are:
The graviton is added to the list although it is not predicted by the Standard Model, but by other theories in the framework of quantum field theory. Furthermore, gravity is non-renormalizable. There are a total of eight independent gluons. The Higgs boson is postulated by the electroweak theory primarily to explain the origin of particle masses. In a process known as the "Higgs mechanism", the Higgs boson and the other gauge bosons in the Standard Model acquire mass via spontaneous symmetry breaking of the SU(2) gauge symmetry. The Minimal Supersymmetric Standard Model (MSSM) predicts several Higgs bosons. A new particle expected to be the Higgs boson was observed at the CERN/LHC on March 14, 2013, around the energy of 126.5GeV with an accuracy of close to five sigma (99.9999%, which is accepted as definitive). The Higgs mechanism giving mass to other particles has not been observed yet.
Hypothetical particles.
Supersymmetric theories predict the existence of more particles, none of which have been confirmed experimentally as of 2014:
Note: just as the photon, Z boson and W± bosons are superpositions of the B0, W0, W1, and W2 fields – the photino, zino, and wino± are superpositions of the bino0, wino0, wino1, and wino2 by definition.<br>No matter if one uses the original gauginos or this superpositions as a basis, the only predicted physical particles are neutralinos and charginos as a superposition of them together with the Higgsinos.
Other theories predict the existence of additional bosons:
Mirror particles are predicted by theories that restore parity symmetry.
"Magnetic monopole" is a generic name for particles with non-zero magnetic charge. They are predicted by some GUTs.
"Tachyon" is a generic name for hypothetical particles that travel faster than the speed of light and have an imaginary rest mass.
Preons were suggested as subparticles of quarks and leptons, but modern collider experiments have all but ruled out their existence.
Kaluza–Klein towers of particles are predicted by some models of extra dimensions. The extra-dimensional momentum is manifested as extra mass in four-dimensional spacetime.
Composite particles.
Hadrons.
Hadrons are defined as strongly interacting composite particles. Hadrons are either:
Quark models, first proposed in 1964 independently by Murray Gell-Mann and George Zweig (who called quarks "aces"), describe the known hadrons as composed of valence quarks and/or antiquarks, tightly bound by the color force, which is mediated by gluons. A "sea" of virtual quark-antiquark pairs is also present in each hadron.
Baryons.
Ordinary baryons (composite fermions) contain three valence quarks or three valence antiquarks each.
Some hints at the existence of exotic baryons have been found recently; however, negative results have also been reported. Their existence is uncertain.
Mesons.
Ordinary mesons are made up of a valence quark and a valence antiquark. Because mesons have spin of 0 or 1 and are not themselves elementary particles, they are "composite" bosons. Examples of mesons include the pion, kaon, and the J/ψ. In quantum hydrodynamic models, mesons mediate the residual strong force between nucleons.
At one time or another, positive signatures have been reported for all of the following exotic mesons but their existences have yet to be confirmed.
Atomic nuclei.
Atomic nuclei consist of protons and neutrons. Each type of nucleus contains a specific number of protons and a specific number of neutrons, and is called a "nuclide" or "isotope". Nuclear reactions can change one nuclide into another. See table of nuclides for a complete list of isotopes.
Atoms.
Atoms are the smallest neutral particles into which matter can be divided by chemical reactions. An atom consists of a small, heavy nucleus surrounded by a relatively large, light cloud of electrons. Each type of atom corresponds to a specific chemical element. To date, 118 elements have been discovered, while only the elements 1–112,114, and 116 have received official names.
The atomic nucleus consists of protons and neutrons. Protons and neutrons are, in turn, made of quarks.
Molecules.
Molecules are the smallest particles into which a non-elemental substance can be divided while maintaining the physical properties of the substance. Each type of molecule corresponds to a specific chemical compound. Molecules are a composite of two or more atoms. See list of compounds for a list of molecules.
Condensed matter.
The field equations of condensed matter physics are remarkably similar to those of high energy particle physics. As a result, much of the theory of particle physics applies to condensed matter physics as well; in particular, there are a selection of field excitations, called quasi-particles, that can be created and explored. These include:

</doc>
<doc id="18242141" url="https://en.wikipedia.org/wiki?curid=18242141" title="List of quasiparticles">
List of quasiparticles

This is a list of quasiparticles.

</doc>
<doc id="3655316" url="https://en.wikipedia.org/wiki?curid=3655316" title="Local symmetry">
Local symmetry

In physics, a local symmetry is symmetry of some physical quantity, which smoothly depends on the point of the base manifold. Such quantities can be for example an observable, a tensor or the Lagrangian of a theory.
If a symmetry is local in this sense, then one can apply a local transformation (resp. local gauge transformation), which means that the representation of the symmetry group is a function of the manifold and can thus be taken to act differently on different points of spacetime.
The diffeomorphism group is a local symmetry and thus every geometrical or generally covariant theory (i.e. a theory whose equations are tensor equations, for example general relativity) has local symmetries.
Often the term local symmetry is specifically associated with local gauge symmetries in Yang–Mills theory (see also standard model) where the Lagrangian is locally symmetric under some compact Lie group. Local gauge symmetries always come together with some bosonic gauge fields, like the photon or gluon field, which induce a force in addition to requiring conservation laws.

</doc>
<doc id="4258398" url="https://en.wikipedia.org/wiki?curid=4258398" title="Locally integrable function">
Locally integrable function

In mathematics, a locally integrable function (sometimes also called locally summable function) is a function which is integrable (so its integral is finite) on every compact subset of its domain of definition. The importance of such functions lies in the fact that their function space is similar to spaces, but its members are not required to satisfy any growth restriction on their behavior at infinity: in other words, locally integrable functions can grow arbitrarily fast at infinity, but are still manageable in a way similar to ordinary integrable functions.
Definition.
Standard definition.
. Let be an open set in the Euclidean space and be a Lebesgue measurable function. If on is such that 
i.e. its Lebesgue integral is finite on all compact subsets of , then   is called "locally integrable". The set of all such functions is denoted by :
where denotes the restriction of   to the set . The classical definition of a locally integrable function involves only measure theoretic and topological concepts and can be carried over abstract to complex-valued functions on a topological measure space : however, since the most common application of such functions is to distribution theory on Euclidean spaces, all the definitions in this and the following sections deal explicitly only with this important case.
An alternative definition.
. Let be an open set in the Euclidean space . Then a function such that 
for each test function is called "locally integrable", and the set of such functions is denoted by . Here denotes the set of all infinitely differentiable functions with compact support contained in .
This definition has its roots in the approach to measure and integration theory based on the concept of continuous linear functional on a topological vector space, developed by Nicolas Bourbaki and his school: it is also the one adopted by and by . This "distribution theoretic" definition is equivalent to the standard one, as the following lemma proves:
. A given function is locally integrable according to if and only if it is locally integrable according to , i.e.
Proof of 
If part: Let be a test function. It is bounded by its supremum norm , measurable, and has a compact support, let's call it . Hence
by . 
Only if part: Let be a compact subset of the open set . We will first construct a test function which majorises the indicator function of .
The usual set distance between and the boundary is strictly greater than zero, i.e.
hence it is possible to choose a real number such that (if is the empty set, take ). Let and denote the closed -neighborhood and -neighborhood of , respectively. They are likewise compact and satisfy
Now use convolution to define the function by
where is a mollifier constructed by using the standard positive symmetric one. Obviously is non-negative in the sense that , infinitely differentiable, and its support is contained in , in particular it is a test function. Since for all , we have that .
Let   be a locally integrable function according to . Then 
Since this holds for every compact subset of , the function   is locally integrable according to . □
Generalization: locally "p"-integrable functions.
. Let be an open set in the Euclidean space ℝ"n" and ℂ be a Lebesgue measurable function. If, for a given with , satisfies
i.e., it belongs to for all compact subsets of , then is called "locally" -"integrable" or also -"locally integrable". The set of all such functions is denoted by :
An alternative definition, completely analogous to the one given for locally integrable functions, can also be given for locally -integrable functions: it can also be and proven equivalent to the one in this section. Despite their apparent higher generality, locally -integrable functions form a subset of locally integrable functions for every such that .
Notation.
Apart from the different glyphs which may be used for the uppercase "L", there are few variants for the notation of the set of locally integrable functions
Properties.
"L""p",loc is a complete metric space for all "p" ≥ 1.
. is a complete metrizable space: its topology can be generated by the following metric:
where is a family of non empty open sets such that
In references , , and , this theorem is stated but not proved on a formal basis: a complete proof of a more general result, which includes it, is found in .
"L""p" is a subspace of "L"1,loc for all "p" ≥ 1.
. Every function belonging to , , where is an open subset of ℝ"n", is locally integrable. 
Proof. The case is trivial, therefore in the sequel of the proof it is assumed that . Consider the characteristic function of a compact subset of : then, for ,
where
Then by Hölder's inequality, the product is integrable i.e. belongs to and 
therefore 
Note that since the following inequality is true
the theorem is true also for functions belonging only to the space of locally -integrable functions, therefore the theorem implies also the following result.
. Every function in , , is locally integrable, i. e. belongs to .
"L"1,loc is the space of densities of absolutely continuous measures.
. A function is the density of an absolutely continuous measure if and only if .
The proof of this result is sketched by . Rephrasing its statement, this theorem asserts that every locally integrable function defines an absolutely continuous measure and conversely that every absolutely continuous measures defines a locally integrable function: this is also, in the abstract measure theory framework, the form of the important Radon–Nikodym theorem given by Stanisław Saks in his treatise.
Applications.
Locally integrable functions play a prominent role in distribution theory and they occur in the definition of various classes of functions and function spaces, like functions of bounded variation. Moreover, they appear in the Radon–Nikodym theorem by characterizing the absolutely continuous part of every measure.

</doc>
<doc id="102476" url="https://en.wikipedia.org/wiki?curid=102476" title="Log-normal distribution">
Log-normal distribution

\ e^{-\frac{\left(\ln x-\mu\right)^2}{2\sigma^2}}</math>
 | cdf = formula_1
 | mean = formula_2
 | median = formula_3
 | mode = formula_4
 | variance = formula_5
 | skewness = formula_6
 | kurtosis = formula_7
 | entropy = formula_8
 | mgf = defined only on the negative half-axis, see text
 | char = representation formula_9 is asymptotically divergent but sufficient for numerical purposes
 | fisher = formula_10
In probability theory, a log-normal (or lognormal) distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed. Thus, if the random variable formula_11 is log-normally distributed, then formula_12 has a normal distribution. Likewise, if formula_13 has a normal distribution, then formula_14 has a log-normal distribution. A random variable which is log-normally distributed takes only positive real values. The distribution is occasionally referred to as the Galton distribution or Galton's distribution, after Francis Galton. The log-normal distribution also has been associated with other names, such as McAlister, Gibrat and Cobb–Douglas.
A log-normal process is the statistical realization of the multiplicative product of many independent random variables, each of which is positive. This is justified by considering the central limit theorem in the log domain. The log-normal distribution is the maximum entropy probability distribution for a random variate formula_11 for which the mean and variance of formula_16 are specified.
Notation.
Given a log-normally distributed random variable formula_11 and two parameters formula_18 and formula_19 that are, respectively, the mean and standard deviation of the variable’s natural logarithm, then the logarithm of formula_11 is normally distributed, and we can write formula_11 as
with formula_23 a standard normal variable.
This relationship is true regardless of the base of the logarithmic or exponential function. If formula_24 is normally distributed, then so is formula_25, for any two positive numbers formula_26. Likewise, if formula_27 is log-normally distributed, then so is formula_28, where formula_29 is a positive number formula_30.
On a logarithmic scale, formula_18 and formula_19 can be called the "location parameter" and the "scale parameter", respectively.
In contrast, the mean, standard deviation, and variance of the non-logarithmized sample values are respectively denoted formula_33, "s.d.", and formula_34 in this article. The two sets of parameters can be related as (see also Arithmetic moments below)
Characterization.
Probability density function.
A random positive variable formula_36 is log-normally distributed if the logarithm of formula_36 is normally distributed,
A change of variables must conserve differential probability. In particular,
where
is the log-normal probability density function.
Cumulative distribution function.
The cumulative distribution function is
where erfc is the complementary error function, and Φ is the cumulative distribution function of the standard normal distribution.
Characteristic function and moment generating function.
All moments of the log-normal distribution exist and it holds that: formula_42 (which can be derived by letting formula_43 within the integral). However, the expected value formula_44 is not defined for any positive value of the argument formula_45 as the defining integral diverges. In consequence
the moment generating function is not defined. The last is related to the fact that the lognormal distribution is not uniquely determined by its moments.
Similarly, the characteristic function formula_46 is not defined in the half complex plane and therefore it is not analytic in the origin. In consequence, the characteristic function of the log-normal distribution cannot be represented as an infinite convergent series. In particular, its Taylor formal series formula_47 diverges. However, a number of alternative divergent series representations have been obtained
A closed-form formula for the characteristic function formula_48 with formula_45 in the domain of convergence is not known. A relatively simple approximating formula is available in closed form and given by
formula_50
where formula_51 is the Lambert W function. This approximation is derived via an asymptotic method but it stays sharp all over the domain of convergence of formula_52.
Properties.
Location and scale.
The location and scale parameters of a log-normal distribution, i.e. formula_18 and formula_19, are more readily treated using the geometric mean, formula_55, and the geometric standard deviation, formula_56, rather than the arithmetic mean, formula_57, and the arithmetic standard deviation, formula_58.
Geometric moments.
The geometric mean of the log-normal distribution is formula_59, and the geometric standard deviation is formula_60. By analogy with the arithmetic statistics, one can define a geometric variance, formula_61, and a geometric coefficient of variation, formula_62.
Because the log-transformed variable formula_63 is symmetric and quantiles are preserved under monotonic transformations, the geometric mean of a log-normal distribution is equal to its median, formula_64.
Note that the geometric mean is "less" than the arithmetic mean. This is due to the AM–GM inequality, and corresponds to the logarithm being convex down. In fact,
In finance the term formula_66 is sometimes interpreted as a convexity correction. From the point of view of stochastic calculus, this is the same correction term as in Itō's lemma for geometric Brownian motion.
Arithmetic moments.
The arithmetic mean, arithmetic variance, and arithmetic standard deviation of a log-normally distributed variable formula_11 are given by
respectively.
The location (formula_18) and scale (formula_19) parameters can be obtained if the arithmetic mean and the arithmetic variance are known; it is simpler if formula_19 is computed first:
For any real or complex number formula_73, the formula_73th moment of a log-normally distributed variable formula_11 is given by
A log-normal distribution is not uniquely determined by its moments formula_77 for formula_78, that is, there exists some other distribution with the same moments for all formula_79. In fact, there is a whole family of distributions with the same moments as the log-normal distribution.
Mode and median.
The mode is the point of global maximum of the probability density function. In particular, it solves the equation formula_80:
The median is such a point where formula_82:
Arithmetic coefficient of variation.
The arithmetic coefficient of variation formula_84 is the ratio formula_85 (on the natural scale). For a log-normal distribution it is equal to
Contrary to the arithmetic standard deviation, the arithmetic coefficient of variation is independent of the arithmetic mean.
Partial expectation.
The partial expectation of a random variable formula_11 with respect to a threshold formula_79 is defined as formula_89 where formula_90 is the probability density function of formula_11. Alternatively, and using the definition of conditional expectation, it can be written as formula_92. For a log-normal random variable the partial expectation is given by:
Where Phi is the normal cumulative distribution function. The derivation of the formula is provided in the discussion of this Wikipedia entry. The partial expectation formula has applications in insurance and economics, it is used in solving the partial differential equation leading to the Black–Scholes formula.
Conditional expectation.
The conditional expectation of a lognormal random variable X with respect to a threshold k is its partial expectation divided by the cumulative probability of being in that range:
Other.
A set of data that arises from the log-normal distribution has a symmetric Lorenz curve (see also Lorenz asymmetry coefficient).
The harmonic formula_96, geometric formula_97 and arithmetic formula_98 means of this distribution are related; such relation is given by
Log-normal distributions are infinitely divisible, but they are not stable distributions, which can be easily drawn from.
Occurrence.
The log-normal distribution is important in the description of natural phenomena. 
The reason is that for many natural processes of growth, relative growth rate is independent of size. This is also known as Gibrat's law, after Robert Gibrat (1904–1980) who formulated it for companies. It can be shown that a growth process following Gibrat's law will result in entity sizes with a log-normal distribution. Examples include:
Maximum likelihood estimation of parameters.
For determining the maximum likelihood estimators of the log-normal distribution parameters μ and σ, we can use the same procedure as for the normal distribution. To avoid repetition, we observe that
where by formula_102 we denote the probability density function of the log-normal distribution and by formula_103 that of the normal distribution. Therefore, using the same indices to denote distributions, we can write the log-likelihood function thus:
Since the first term is constant with regard to μ and σ, both logarithmic likelihood functions, formula_105 and formula_106, reach their maximum with the same formula_18 and formula_19. Hence, using the formulas for the normal distribution maximum likelihood parameter estimators and the equality above, we deduce that for the log-normal distribution it holds that
Multivariate log-normal.
If formula_110 is a multivariate normal distribution then formula_111 has a multivariate log-normal distribution with mean
and covariance matrix
Related distributions.
In the case that all formula_130 have the same variance parameter formula_131, these formulas simplify to
Similar distributions.
A substitute for the log-normal whose integral can be expressed in terms of more elementary functions can be obtained based on the logistic distribution to get an approximation for the CDF
This is a log-logistic distribution.

</doc>
<doc id="1390288" url="https://en.wikipedia.org/wiki?curid=1390288" title="Loop group">
Loop group

In mathematics, a loop group is a group of loops in a topological group "G" with multiplication defined pointwise. 
Definition.
In its most general form a loop group is a group of mappings from a manifold to a topological group .
More specifically, let , the circle in the complex plane, and let denote the space of continuous maps , i.e.
equipped with the compact-open topology. An element of is called a "loop" in . 
Pointwise multiplication of such loops gives the structure of a topological group. Parametrize with ,
and define multiplication in by
Associativity follows from associativity in . The inverse is given by
and the identity by
The space is called the free loop group on . A loop group is any subgroup of the free loop group .
Examples.
An important example of a loop group is the group 
of based loops on . It is defined to be the kernel of the evaluation map 
and hence is a closed normal subgroup of . (Here, is the map that sends a loop to its value at .) Note that we may embed into as the subgroup of constant loops. Consequently, we arrive at a split exact sequence 
The space splits as a semi-direct product, 
We may also think of as the loop space on . From this point of view, is a "H"-space with respect to concatenation of loops. On the face of it, this seems to provide with two very different product maps. However, it can be shown that concatenation and pointwise multiplication are homotopic. Thus, in terms of the homotopy theory of , these maps are interchangeable.
Loop groups were used to explain the phenomenon of Bäcklund transforms in soliton equations by Chuu-Lian Terng and Karen Uhlenbeck.

</doc>
<doc id="152664" url="https://en.wikipedia.org/wiki?curid=152664" title="Loop quantum gravity">
Loop quantum gravity

Loop quantum gravity (LQG) is a theory that attempts to describe the quantum properties of the universe and gravity. It is also a theory of quantum spacetime because, according to general relativity, gravity is a manifestation of the geometry of spacetime. LQG is an attempt to merge quantum mechanics and general relativity. The main output of the theory is a physical picture of space where space is granular. The granularity is a direct consequence of the quantization. It has the same nature as the granularity of the photons in the quantum theory of electromagnetism and the discrete levels of the energy of the atoms. Here, it is space itself that is discrete. In other words, there is a minimum distance possible to travel through it.
More precisely, space can be viewed as an extremely fine fabric or network "woven" of finite loops. These networks of loops are called spin networks. The evolution of a spin network over time is called a spin foam. The predicted size of this structure is the Planck length, which is approximately 10−35 meters. According to the theory, there is no meaning to distance at scales smaller than the Planck scale. Therefore, LQG predicts that not just matter, but space itself, has an atomic structure.
Today LQG is a vast area of research, developing in several directions, which involves about 30 research groups worldwide. They all share the basic physical assumptions and the mathematical description of quantum space. The full development of the theory is being pursued in two directions: the more traditional canonical loop quantum gravity, and the newer covariant loop quantum gravity, more commonly called spin foam theory.
Research into the physical consequences of the theory is proceeding in several directions. Among these, the most well-developed is the application of LQG to cosmology, called loop quantum cosmology (LQC). LQC applies LQG ideas to the study of the early universe and the physics of the Big Bang. Its most spectacular consequence is that the evolution of the universe can be continued beyond the Big Bang. The Big Bang appears thus to be replaced by a sort of cosmic Big Bounce.
History.
In 1986, Abhay Ashtekar reformulated Einstein's general relativity in a language closer to that of the rest of fundamental physics. Shortly after, Ted Jacobson and Lee Smolin realized that the formal equation of quantum gravity, called the Wheeler–DeWitt equation, admitted solutions labelled by loops, when rewritten in the new Ashtekar variables, and Carlo Rovelli and Lee Smolin defined a nonperturbative and background-independent quantum theory of gravity in terms of these loop solutions. Jorge Pullin and Jerzy Lewandowski understood that the intersections of the loops are essential for the consistency of the theory, and the theory should be formulated in terms of intersecting loops, or graphs.
In 1994, Rovelli and Smolin showed that the quantum operators of the theory associated to area and volume have a discrete spectrum. That is, geometry is quantized. This result defines an explicit basis of states of quantum geometry, which turned out to be labelled by Roger Penrose's spin networks, which are graphs labelled by spins.
The canonical version of the dynamics was put on firm ground by Thomas Thiemann, who defined an anomaly-free Hamiltonian operator, showing the existence of a mathematically consistent background-independent theory. The covariant or spinfoam version of the dynamics developed during several decades, and crystallized in 2008, from the joint work of research groups in France, Canada, UK, Poland, and Germany, lead to the definition of a family of transition amplitudes, which in the classical limit can be shown to be related to a family of truncations of general relativity. The finiteness of these amplitudes was proven in 2011. It requires the existence of a positive cosmological constant, and this is consistent with observed acceleration in the expansion of the Universe.
General covariance and background independence.
In theoretical physics, general covariance is the invariance of the form of physical laws under arbitrary differentiable coordinate transformations. The essential idea is that coordinates are only artifices used in describing nature, and hence should play no role in the formulation of fundamental physical laws. A more significant requirement is the principle of general relativity that states that the laws of physics take the same form in all reference systems. This is a generalization of the principle of special relativity which states that the laws of physics take the same form in all inertial frames.
In mathematics, a diffeomorphism is an isomorphism in the category of smooth manifolds. It is an invertible function that maps one differentiable manifold to another, such that both the function and its inverse are smooth. These are the defining symmetry transformations of General Relativity since the theory is formulated only in terms of a differentiable manifold.
In general relativity, general covariance is intimately related to "diffeomorphism invariance". This symmetry is one of the defining features of the theory. However, it is a common misunderstanding that "diffeomorphism invariance" refers to the invariance of the physical predictions of a theory under arbitrary coordinate transformations; this is untrue and in fact every physical theory is invariant under coordinate transformations this way. Diffeomorphisms, as mathematicians define them, correspond to something much more radical; intuitively a way they can be envisaged is as simultaneously dragging all the physical fields (including the gravitational field) over the bare differentiable manifold while staying in the same coordinate system. Diffeomorphisms are the true symmetry transformations of general relativity, and come about from the assertion that the formulation of the theory is based on a bare differentiable manifold, but not on any prior geometry — the theory is background-independent (this is a profound shift, as all physical theories before general relativity had as part of their formulation a prior geometry). What is preserved under such transformations are the coincidences between the values the gravitational field take at such and such a "place" and the values the matter fields take there. From these relationships one can form a notion of matter being located with respect to the gravitational field, or vice versa. This is what Einstein discovered: that physical entities are located with respect to one another only and not with respect to the spacetime manifold. As Carlo Rovelli puts it: "No more fields on spacetime: just fields on fields.". This is the true meaning of the saying "The stage disappears and becomes one of the actors"; space-time as a "container" over which physics takes place has no objective physical meaning and instead the gravitational interaction is represented as just one of the fields forming the world. This is known as the relationalist interpretation of space-time. The realization by Einstein that general relativity should be interpreted this way is the origin of his remark "Beyond my wildest expectations".
In LQG this aspect of general relativity is taken seriously and this symmetry is preserved by requiring that the physical states remain invariant under the generators of diffeomorphisms. The interpretation of this condition is well understood for purely spatial diffeomorphisms. However, the understanding of diffeomorphisms involving time (the Hamiltonian constraint) is more subtle because it is related to dynamics and the so-called "problem of time" in general relativity. A generally accepted
calculational framework to account for this constraint has yet to be found. A plausible candidate for the quantum hamiltonian constraint is the operator introduced by Thiemann.
LQG is formally background independent. The equations of LQG are not embedded in, or dependent on, space and time (except for its invariant topology). Instead, they are expected to give rise to space and time at distances which are large compared to the Planck length. The issue of background independence in LQG still has some unresolved subtleties. For example, some derivations require a fixed choice of the topology, while any consistent quantum theory of gravity should include topology change as a dynamical process.
Constraints and their Poisson bracket algebra.
The constraints of classical canonical general relativity.
In the Hamiltonian formulation of ordinary classical mechanics the Poisson bracket is an important concept. A "canonical coordinate system" consists of canonical position and momentum variables that satisfy canonical Poisson-bracket relations,
formula_1
where the Poisson bracket is given by
for arbitrary phase space functions formula_3 and formula_4. With the use of Poisson brackets, the Hamilton's equations can be rewritten as,
formula_5,
formula_6.
These equations describe a ``flow" or orbit in phase space generated by the Hamiltonian formula_7. Given any phase space function formula_8, we have
formula_9
Let us consider constrained systems, of which General relativity is an example. In a similar way the Poisson bracket between a constraint and the phase space variables generates a flow along an orbit in (the unconstrained) phase space generated by the constraint. There are three types of constraints in Ashtekar's reformulation of classical general relativity:
formula_10 Gauss gauge constraints.
The Gauss constraints
formula_11.
This represents an infinite number of constraints one for each value of formula_12. These come about from re-expressing General relativity as an formula_13 Yang–Mills type gauge theory (Yang–Mills is a generalization of Maxwell's theory where the gauge field transforms as a vector under Gauss transformations, that is, the Gauge field is of the form formula_14 where formula_15 is an internal index. See Ashtekar variables). These infinite number of Gauss gauge constraints can be smeared with test fields with internal indices, formula_16,
formula_17.
which we demand vanish for any such function. These smeared constraints defined with respect to a suitable space of smearing functions give an equivalent description to the original constraints.
In fact Ashtekar's formulation may be thought of as ordinary formula_13 Yang–Mills theory together with the following special constraints, resulting from diffeomorphism invariance, and a Hamiltonian that vanishes. The dynamics of such a theory are thus very different from that of ordinary Yang–Mills theory.
Spatial diffeomorphisms constraints.
The spatial diffeomorphism constraints
formula_19
can be smeared by the so-called shift functions formula_20 to give an equivalent set of smeared spatial diffeomorphism constraints,
formula_21.
These generate spatial diffeomorphisms along orbits defined by the shift function formula_22.
Hamiltonian constraints.
The Hamiltonian
formula_23
can be smeared by the so-called lapse functions formula_24 to give an equivalent set of smeared Hamiltonian constraints,
formula_25.
These generate time diffeomorphisms along orbits defined by the lapse function formula_24.
In Ashtekar formulation the gauge field formula_14 is the configuration variable (the configuration variable being analogous to formula_28 in ordinary mechanics) and its conjugate momentum is the (densitized) triad (electrical field) formula_29. The constraints are certain functions of these phase space variables.
We consider the action of the constraints on arbitrary phase space functions. An important notion here is the Lie derivative, formula_30, which is basically a derivative operation that infinitesimally "shifts" functions along some orbit with tangent vector formula_31.
The Poisson bracket algebra.
Of particular importance is the Poisson bracket algebra formed between the (smeared) constraints themselves as it completely determines the theory. In terms of the above smeared constraints the constraint algebra amongst the Gauss' law reads,
formula_32
where formula_33. And so we see that the Poisson bracket of two Gauss' law is equivalent to a single Gauss' law evaluated on the commutator of the smearings. The Poisson bracket amongst spatial diffeomorphisms constraints reads
formula_34
and we see that its effect is to "shift the smearing". The reason for this is that the smearing functions are not functions of the canonical variables and so the spatial diffeomorphism does not generate diffeomorphims on them. They do however generate diffeomorphims on everything else. This is equivalent to leaving everything else fixed while shifting the smearing .The action of the spatial diffeomorphism on the Gauss law is
formula_35,
again, it shifts the test field formula_36. The Gauss law has vanishing Poisson bracket with the Hamiltonian constraint. The spatial diffeomorphism constraint with a Hamiltonian gives a Hamiltonian with its smearing shifted,
formula_37.
Finally, the poisson bracket of two Hamiltonians is a spatial diffeomorphism,
formula_38
where formula_39 is some phase space function. That is, it is a sum over infinitesimal spatial diffeomorphisms constraints where the coefficients of proportionality are not constants but have non-trivial phase space dependence.
A (Poisson bracket) Lie algebra, with constraints formula_40, is of the form
formula_41
where formula_42 are constants (the so-called structure constants). The above Poisson bracket algebra for General relativity does not form a true Lie algebra as we have structure functions rather than structure constants for the Poisson bracket between two Hamiltonians. This leads to difficulties.
Dirac observables.
The constraints define a constraint surface in the original phase space. The gauge motions of the constraints apply to all phase space but have the feature that they leave the constraint surface where it is, and thus the orbit of a point in the hypersurface under gauge transformations will be an orbit entirely within it. Dirac observables are defined as phase space functions, formula_43, that Poisson commute with all the constraints when the constraint equations are imposed,
formula_44,
that is, they are quantities defined on the constraint surface that are invariant under the gauge transformations of the theory.
Then, solving only the constraint formula_45 and determining the Dirac observables with respect to it leads us back to the ADM phase space with constraints formula_46. The dynamics of general relativity is generated by the constraints, it can be shown that six Einstein equations describing time evolution (really a gauge transformation) can be obtained by calculating the Poisson brackets of the three-metric and its conjugate momentum with a linear combination of the spatial diffeomorphism and 
Hamiltonian constraint. The vanishing of the constraints, giving the physical phase space, are the four other Einstein equations.
Quantization of the constraints – the equations of quantum general relativity.
Pre-history and Ashtekar new variables.
Many of the technical problems in canonical quantum gravity revolve around the constraints. Canonical general relativity was originally formulated in terms of metric variables, but there seemed to be insurmountable mathematical difficulties in promoting the constraints to quantum operators because of their highly non-linear dependence on the canonical variables. The equations were much simplified with the introduction of Ashtekars new variables. Ashtekar variables describe canonical general relativity in terms of a new pair canonical variables closer to that of gauge theories. The first step consists of using densitized triads formula_47 (a triad formula_48 is simply three orthogonal vector fields labeled by formula_49 and the densitized triad is defined by formula_50) to encode information about the spatial metric,
formula_51.
(where formula_52 is the flat space metric, and the above equation expresses that formula_53, when written in terms of the basis formula_48, is locally flat). (Formulating general relativity with triads instead of metrics was not new.) The densitized triads are not unique, and in fact one can perform a local in space rotation with respect to the internal indices formula_15. The canonically conjugate variable is related to the extrinsic curvature by formula_56. But problems similar to using the metric formulation arise when one tries to quantize the theory. Ashtekar's new insight was to introduce a new configuration variable,
formula_57
that behaves as a complex formula_58 connection where formula_59 is related to the so-called spin connection via formula_60. Here formula_61 is called the chiral spin connection. It defines a covariant derivative formula_62. It turns out that formula_63 is the conjugate momentum of formula_61, and together these form Ashtekar's new variables.
The expressions for the constraints in Ashtekar variables; the Gauss's law, the spatial diffeomorphism constraint and the (densitized) Hamiltonian constraint then read:
formula_65
formula_66,
formula_67
respectively, where formula_68 is the field strength tensor of the connection formula_61 and where formula_70 is referred to as the vector constraint. The above-mentioned local in space rotational invariance is the original of the formula_58 gauge invariance here expressed by the Gauss law. Note that these constraints are polynomial in the fundamental variables, unlike as with the constraints in the metric formulation. This dramatic simplification seemed to open up the way to quantizing the constraints. (See the article Self-dual Palatini action for a derivation of Ashtekar's formulism).
With Ashtekar's new variables, given the configuration variable formula_72, it is natural to consider wavefunctions formula_73. This is the connection representation. It is analogous to ordinary quantum mechanics with configuration variable formula_28 and wavefunctions formula_75. The configuration variable gets promoted to a quantum operator via:
formula_76,
(analogous to formula_77) and the triads are (functional) derivatives,
formula_78.
(analogous to formula_79). In passing over to the quantum theory the constraints become operators on a kinematic Hilbert space (the unconstrained formula_58 Yang–Mills Hilbert space). Note that different ordering of the formula_81's and formula_82's when replacing the formula_82's with derivatives give rise to different operators - the choice made is called the factor ordering and should be chosen via physical reasoning. Formally they read
formula_84
formula_85
formula_86.
There are still problems in properly defining all these equations and solving them. For example the Hamiltonian constraint Ashtekar worked with was the densitized version instead of the original Hamiltonian, that is, he worked with formula_87. There were serious difficulties in promoting this quantity to a quantum operator. Moreover, although Ashtekar variables had the virtue of simplifying the Hamiltonian, they are complex. When one quantizes the theory, it is difficult to ensure that one recovers real general relativity as opposed to complex general relativity.
Quantum constraints as the equations of quantum general relativity.
We now move on to demonstrate an important aspect of the quantum constraints. We consider Gauss' law only. First we state the classical result that the Poisson bracket of the smeared Gauss' law formula_88 with the connections is
formula_89
The quantum Gauss' law reads
formula_90
If one smears the quantum Gauss' law and study its action on the quantum state one finds that the action of the constraint on the quantum state is equivalent to shifting the argument of formula_91 by an infinitesimal (in the sense of the parameter formula_36 small) gauge transformation,
formula_93
and the last identity comes from the fact that the constraint annihilates the state. So the constraint, as a quantum operator, is imposing the same symmetry that its vanishing imposed classically: it is telling us that the functions formula_94 have to be gauge invariant functions of the connection. The same idea is true for the other constraints.
Therefore the two step process in the classical theory of solving the constraints formula_95 (equivalent to solving the admissibility conditions for the initial data) and looking for the gauge orbits (solving the `evolution' equations) is replaced by a one step process in the quantum theory, namely looking for solutions formula_91 of the quantum equations formula_97. This is because it obviously solves the constraint at the quantum level and it simultaneously looks for states that are gauge invariant because formula_98 is the quantum generator of gauge transformations (gauge invariant functions are constant along the gauge orbits and thus characterize them). Recall that, at the classical level, solving the admissibility conditions and evolution equations was equivalent to solving all of Einstein's field equations, this underlines the central role of the quantum constraint equations in canonical quantum gravity.
Introduction of the loop representation.
It was in particular the inability to have good control over the space of solutions to the Gauss' law and spacial diffeomorphism constraints that led Rovelli and Smolin to consider a new representation - the loop representation in gauge theories and quantum gravity.
We need the notion of a holonomy. A holonomy is a measure of how much the initial and final values of a spinor or vector differ after parallel transport around a closed loop; it is denoted
formula_99.
Knowledge of the holonomies is equivalent to knowledge of the connection, up to gauge equivalence. Holonomies can also be associated with an edge; under a Gauss Law these transform as
formula_100.
For a closed loop formula_101 if we take the trace of this, that is, putting formula_102 and summing we obtain
formula_103
or
formula_104.
The trace of an holonomy around a closed loop is written
formula_105
and is called a Wilson loop. Thus Wilson loops are gauge invariant. The explicit form of the Holonomy is
formula_106
where formula_107 is the curve along which the holonomy is evaluated, and formula_108 is a parameter along the curve, formula_109 denotes path ordering meaning factors for smaller values of formula_108 appear to the left, and formula_111 are matrices that satisfy the formula_58 algebra
formula_113.
The Pauli matrices satisfy the above relation. It turns out that there are infinitely many more examples of sets of matrices that satisfy these relations, where each set comprises formula_114 matrices with formula_115, and where none of these can be thought to `decompose' into two or more examples of lower dimension. They are called different irreducible representations of the formula_58 algebra. The most fundamental representation being the Pauli matrices. The holonomy is labelled by a half integer formula_117 according to the irreducible representation used.
The use of Wilson loops explicitly solves the Gauss gauge constraint. To handle the spatial diffeomorphism constraint we need to go over to the loop representation. As Wilson loops form a basis we can formally expand any Gauss gauge invariant function as,
formula_118.
This is called the loop transform. We can see the analogy with going to the momentum representation in quantum mechanics(see Position and momentum space). There one has a basis of states formula_119 labelled by a number formula_120 and one expands
formula_121.
and works with the coefficients of the expansion formula_122.
The inverse loop transform is defined by
formula_123.
This defines the loop representation. Given an operator formula_124 in the connection representation,
formula_125,
one should define the corresponding operator formula_126 on formula_127 in the loop representation via,
formula_128,
where formula_129 is defined by the usual inverse loop transform,
formula_130.
A transformation formula giving the action of the operator formula_126 on formula_127 in terms of the action of the operator formula_124 on formula_94 is then obtained by equating the R.H.S. of formula_135 with the R.H.S. of formula_136 with formula_137 substituted into formula_136, namely
formula_139,
or
formula_140,
where by formula_141 we mean the operator formula_124 but with the reverse factor ordering (remember from simple quantum mechanics where the product of operators is reversed under conjugation). We evaluate the action of this operator on the Wilson loop as a calculation in the connection representation and rearranging the result as a manipulation purely in terms of loops (one should remember that when considering the action on the Wilson loop one should choose the operator one wishes to transform with the opposite factor ordering to the one chosen for its action on wavefunctions formula_94). This gives the physical meaning of the operator formula_126. For example if formula_141 corresponded to a spatial diffeomorphism, then this can be thought of as keeping the connection field formula_81 of formula_105 where it is while performing a spatial diffeomorphism on formula_107 instead. Therefore the meaning of formula_126 is a spatial diffeomorphism on formula_107, the argument of formula_127.
In the loop representation we can then solve the spatial diffeomorphism constraint by considering functions of loops formula_127 that are invariant under spatial diffeomorphisms of the loop formula_107. That is, we construct what mathematicians call knot invariants. This opened up an unexpected connection between knot theory and quantum gravity.
What about the Hamiltonian constraint? Let us go back to the connection representation. Any collection of non-intersecting Wilson loops satisfy Ashtekar's quantum Hamiltonian constraint. This can be seen from the following. With a particular ordering of terms and replacing formula_63 by a derivative, the action of the quantum Hamiltonian constraint on a Wilson loop is
formula_155.
When a derivative is taken it brings down the tangent vector, formula_156, of the loop, formula_107. So we have something like
formula_158.
However, as formula_68 is anti-symmetric in the indices formula_160 and formula_161 this vanishes (this assumes that formula_107 is not discontinuous anywhere and so the tangent vector is unique). Now let us go back to the loop representation.
We consider wavefunctions formula_127 that vanish if the loop has discontinuities and that are knot invariants. Such functions solve the Gauss law, the spatial diffeomorphism constraint and (formally) the Hamiltonian constraint. Thus we have identified an infinite set of exact (if only formal) solutions to all the equations of quantum general relativity! This generated a lot of interest in the approach and eventually led to LQG.
Geometric operators, the need for intersecting Wilson loops and spin network states.
The easiest geometric quantity is the area. Let us choose coordinates so that the surface formula_164 is characterized by formula_165. The area of small parallelogram of the surface formula_164 is the product of length of each side times formula_167 where formula_168 is the angle between the sides. Say one edge is given by the vector formula_169 and the other by formula_170 then,
formula_171
formula_172
In the space spanned by formula_173 and formula_174 we have an infinitesimal parallelogram described by formula_175 and formula_176. Using formula_177 (where the indices formula_81 and formula_179 run from 1 to 2), we get the area of the surface formula_164 to be given by
formula_181
where formula_182 and is the determinant of the metric induced on formula_164. The latter can be rewritten formula_184 where the indices formula_185 go from 1 to 2. This can be further rewritten as
formula_186.
The standard formula for an inverse matrix is
formula_187
Note the similarity between this and the expression for formula_188. But in Ashtekar variables we have formula_189. Therefore
formula_190.
According to the rules of canonical quantization we should promote the triads formula_191 to quantum operators,
formula_192.
It turns out that the area formula_193 can be promoted to a well defined quantum operator despite the fact that we are dealing with product of two functional derivatives and worse we have a square-root to contend with as well. Putting formula_194, we talk of being in the formula_195-th representation. We note that formula_196. This quantity is important in the final formula for the area spectrum. We simply state the result below,
formula_197
where the sum is over all edges formula_198 of the Wilson loop that pierce the surface formula_164.
The formula for the volume of a region formula_200 is given by
formula_201.
The quantization of the volume proceeds the same way as with the area. As we take the derivative, and each time we do so we bring down the tangent vector formula_156, when the volume operator acts on non-intersecting Wilson loops the result vanishes. Quantum states with non-zero volume must therefore involve intersections. Given that the anti-symmetric summation is taken over in the formula for the volume we would need at least intersections with three non-coplanar lines. Actually it turns out that one needs at least four-valent vertices for the volume operator to be non-vanishing.
We now consider Wilson loops with intersections. We assume the real representation where the gauge group is formula_58. Wilson loops are an over complete basis as there are identities relating different Wilson loops. These come about from the fact that Wilson loops are based on matrices (the holonomy) and these matrices satisfy identities. Given any two formula_58 matrices formula_205 and formula_206 it is easy to check that,
formula_207.
This implies that given two loops formula_107 and formula_209 that intersect, we will have,
formula_210
where by formula_211 we mean the loop formula_209 traversed in the opposite direction and formula_213 means the loop obtained by going around the loop formula_107 and then along formula_209. See figure below. Given that the matrices are unitary one has that formula_216. Also given the cyclic property of the matrix traces (i.e. formula_217) one has that formula_218. These identities can be combined with each other into further identities of increasing complexity adding more loops. These identities are the so-called Mandelstam identities. Spin networks certain are linear combinations of intersecting Wilson loops designed to address the over completeness introduced by the Mandelstam identities (for trivalent intersections they eliminate the over-compleness entirely) and actually constitute a basis for all gauge invariant functions.
As mentioned above the holonomy tells you how to propagate test spin half particles. A spin network state assigns an amplitude to a set of spin half particles tracing out a path in space, merging and splitting. These are described by spin networks formula_107: the edges are labelled by spins together with `intertwiners' at the vertices which are prescription for how to sum over different ways the spins are rerouted. The sum over rerouting are chosen as such to make the form of the intertwiner invariant under Gauss gauge transformations.
Real variables, modern analysis and LQG.
Let us go into more detail about the technical difficulties associated with using Ashtekar's variables:
With Ashtekar's variables one uses a complex connection and so the relevant gauge group as actually formula_220 and not formula_58. As formula_220 is non-compact it creates serious problems for the rigorous construction of the necessary mathematical machinery. The group formula_58 is on the other hand is compact and the relevant constructions needed have been developed.
As mentioned above, because Ashtekar's variables are complex it results in complex general relativity. To recover the real theory one has to impose what are known as the reality conditions. These require that the densitized triad be real and that the real part of the Ashtekar connection equals the compatible spin connection (the compatibility condition being formula_224) determined by the desitized triad. The expression for compatible connection formula_59 is rather complicated and as such non-polynomial formula enters through the back door.
Before we state the next difficulty we should give a definition; a tensor density of weight formula_226 transforms like an ordinary tensor, except that in additional the formula_226th power of the Jacobian,
formula_228
appears as a factor, i.e.
formula_229.
It turns out that it is impossible, on general grounds, to construct a UV-finite, diffeomorphism non-violating operator corresponding to formula_230. The reason is that the rescaled Hamiltonian constraint is a scalar density of weight two while it can be shown that only scalar densities of weight one have a chance to result in a well defined operator. Thus, one is forced to work with the original unrescaled, density one-valued, Hamiltonian constraint. However, this is non-polynomial and the whole virtue of the complex variables is questioned. In fact, all the solutions constructed for Ashtekar's Hamiltonian constraint only vanished for finite regularization (physics), however, this violates spatial diffeomorphism invariance.
Without the implementation and solution of the Hamiltonian constraint no progress can be made and no reliable predictions are possible!
To overcome the first problem one works with the configuration variable
formula_231
where formula_232 is real (as pointed out by Barbero, who introduced real variables some time after Ashtekar's variables). The Guass law and the spatial diffeomorphism constraints are the same. In real Ashtekar variables the Hamiltonian is
formula_233.
The complicated relationship between formula_59 and the desitized triads causes serious problems upon quantization. It is with the choice formula_235 that the second more complicated term is made to vanish. However, as mentioned above formula_59 reappears in the reality conditions. Also we still have the problem of the formula_237 factor.
Thiemann was able to make it work for real formula_232. First he could simplify the troublesome formula_237 by using the identity
formula_240
where formula_31 is the volume. The formula_242 and formula_31 can be promoted to well defined operators in the loop representation and the Poisson bracket is replaced by a commutator upon quantization; this takes care of the first term. It turns out that a similar trick can be used to treat the second term. One introduces the quantity
formula_244
and notes that
formula_245.
We are then able to write
formula_246.
The reason the quantity formula_39 is easier to work with at the time of quantization is that it can be written as
formula_248
where we have used that the integrated densitized trace of the extrinsic curvature, formula_39, is the``time derivative of the volume".
In the long history of canonical quantum gravity formulating the Hamiltonian constraint as a quantum operator (Wheeler–DeWitt equation) in a mathematically rigorous manner has been a formidable problem. It was in the loop representation that a mathematically well defined Hamiltonian constraint was finally formulated in 1996. We leave more details of its construction to the article Hamiltonian constraint of LQG. This together with the quantum versions of the Gauss law and spatial diffeomorphism constrains written in the loop representation are the central equations of LQG (modern canonical quantum General relativity).
Finding the states that are annihilated by these constraints (the physical states), and finding the corresponding physical inner product, and observables is the main goal of the technical side of LQG.
A very important aspect of the Hamiltonian operator is that it only acts at vertices (a consequence of this is that Thiemann's Hamiltonian operator, like Ashtekar's operator, annihilates non-intersecting loops except now it is not just formal and has rigorous mathematical meaning). More precisely, its action is non-zero on at least vertices of valence three and greater and results in a linear combination of new spin networks where the original graph has been modified by the addition of lines at each vertex together and a change in the labels of the adjacent links of the vertex.
Implementation and solution the quantum constraints.
We solve, at least approximately, all the quantum constraint equations and for the physical inner product to make physical predictions.
Before we move on to the constraints of LQG, lets us consider certain cases. We start with a kinematic Hilbert space formula_250 as so is equipped with an inner product—the kinematic inner product formula_251.
i) Say we have constraints formula_98 whose zero eigenvalues lie in their discrete spectrum.
Solutions of the first constraint, formula_253, correspond to a subspace of the kinematic Hilbert space, formula_254. There will be a projection operator formula_255 mapping formula_250 onto formula_257. The kinematic inner product structure is easily employed to provide the inner product structure after solving this first constraint; the new inner product formula_258 is simply
formula_259
They are based on the same inner product and are states normalizable with respect to it.
ii) The zero point is not contained in the point spectrum of all the formula_98, there is then no non-trivial solution formula_261 to the system of quantum constraint equations formula_97 for all formula_198.
For example the zero eigenvalue of the operator
formula_264
on formula_265 lies in the continuous spectrum formula_266 but the formal ``eigenstate" formula_267 is not normalizable in the kinematic inner product,
formula_268
and so does not belong to the kinematic Hilbert space formula_250. In these cases we take a dense subset formula_270 of formula_250 (intuitively this means either any point in formula_270 is either in formula_250 or arbitrarily close to a point in formula_250) with very good convergence properties and consider its dual space formula_275 (intuitively these map elements of formula_270 onto finite complex numbers in a linear manner), then formula_277 (as formula_275 contains distributional functions). The constraint operator is then implemented on this larger dual space, which contains distributional functions, under the adjoint action on the operator. One looks for solutions on this larger space. This comes at the price that the solutions must be given a new Hilbert space inner product with respect to which they are normalizable (see article on rigged Hilbert space). In this case we have a generalized projection operator on the new space of states. We cannot use the above formula for the new inner product as it diverges, instead the new inner product is given by the simply modification of the above,
formula_279
The generalized projector formula_280 is known as a rigging map.
Implementation and solution the quantum constraints of LQG.
Let us move to LQG, additional complications will arise from that one cannot define an operator for the quantum spatial diffeomorphism constraint as the infinitesimal generator of finite diffeomorphism transformations and the fact the constraint algebra is not a Lie algebra due to the bracket between two Hamiltonian constraints.
Implementation and solution the Gauss constraint:
One does not actually need to promote the Gauss constraint to an operator since we can work directly with Gauss-gauge-invariant functions (that is, one solves the constraint classically and quantizes only the phase space reduced with respect to the Gauss constraint). The Gauss law is solved by the use of spin network states. They provide a basis for the Kinematic Hilbert space formula_250. 
Implementation of the quantum spatial diffeomorphism constraint:
It turns out that one cannot define an operator for the quantum spatial diffeomorphism constraint as the infinitesimal generator of finite diffeomorphism transformations, represented on formula_250. The representation of finite diffeomorphisms is a family of unitary operators formula_283 acting on a spin-network state formula_284 by
formula_285
for any spatial diffeomorphism formula_286 on formula_164. To understand why one cant define an operator for the quantum spatial diffeomorphism constraint consider what is called a 1-parameter subgroup formula_288 in the group of spatial diffeomorphisms, this is then represented as a 1-parameter unitary group formula_289 on formula_250. However, formula_289 is not weakly continuous since the subspace formula_292 belongs to and the subspace formula_284 belongs to are orthogonal to each other no matter how small the parameter formula_294 is. So one always has
formula_295
even in the limit when formula_294 goes to zero. Therefore, the infinitesimal generator of formula_289 does not exist.
Solution of the spatial diffeomorphism constraint.
The spatial diffeomorphism constraint has been solved. The induced inner product formula_298 on formula_299 (we do not pursue the details) has a very simple description in terms of spin network states; given two spin networks formula_108 and formula_301, with associated spin network states formula_302 and formula_303, the inner product is 1 if formula_108 and formula_301 are related to each other by a spatial diffeomorphism and zero otherwise.
We have provided a description of the implemented and complete solution of the kinematic constraints, the Gauss and spatial diffeomorphisms constraints which will be the same for any background-independent gauge field theory. The feature that distinguishes such different theories is the Hamiltonian constraint which is the only one that depends on the Lagrangian of the classical theory.
Problem arising from the Hamiltonian constraint.
Details of the implementation the quantum Hamiltonian constraint and solutions are treated in a different article Hamiltonian constraint of LQG. However, in this article we introduce an approximation scheme for the formal solution of the Hamiltonian constraint operator given in the section below on spinfoams. Here we just mention issues that arises with the Hamiltonian constraint. 
The Hamiltonian constraint maps diffeomorphism invariant states onto non-diffeomorphism invaiant states as so does not preserve the diffeomorphism Hilbert space formula_299. This is an unavoidable consequence of the operator algebra, in particular the commutator:
formula_307
as can be seen by applying this to formula_308,
formula_309
and using formula_310 to obtain
formula_311
and so formula_312 is not in formula_313.
This means that you can't just solve the spatial diffeomorphism constraint and then the Hamiltonian constraint. This problem can be circumvented by the introduction of the master constraint, with its trivial operator algebra, one is then able in principle to construct the physical inner product from formula_299.
Spin foams.
In loop quantum gravity (LQG), a spin network represents a "quantum state" of the gravitational field on a 3-dimensional hypersurface. The set of all possible spin networks (or, more accurately, "s-knots" - that is, equivalence classes of spin networks under diffeomorphisms) is countable; it constitutes a basis of LQG Hilbert space.
In physics, a spin foam is a topological structure made out of two-dimensional faces that represents one of the configurations that must be summed to obtain a Feynman's path integral (functional integration) description of quantum gravity. It is closely related to loop quantum gravity.
Spin foam derived from the Hamiltonian constraint operator.
The Hamiltonian constraint generates `time' evolution. Solving the Hamiltonian constraint should tell us how quantum states evolve in `time' from an initial spin network state to a final spin network state. One approach to solving the Hamiltonian constraint starts with what is called the Dirac delta function. This is a rather singular function of the real line, denoted formula_315, that is zero everywhere except at formula_316 but whose integral is finite and nonzero. It can be represented as a Fourier integral,
formula_317.
One can employ the idea of the delta function to impose the condition that the Hamiltonian constraint should vanish. It is obvious that
formula_318
is non-zero only when formula_319 for all formula_12 in formula_164. Using this we can `project' out solutions to the Hamiltonian constraint. With analogy to the Fourier integral given above, this (generalized) projector can formally be written as
formula_322.
Interestingly, this is formally spatially diffeomorphism-invariant. As such it can be applied at the spatially diffeomorphism-invariant level. Using this the physical inner product is formally given by
formula_323
where formula_324 are the initial spin network and formula_325 is the final spin network.
The exponential can be expanded
formula_326
and each time a Hamiltonian operator acts it does so by adding a new edge at the vertex. The summation over different sequences of actions of formula_327 can be visualized as a summation over different histories of `interaction vertices' in the `time' evolution sending the initial spin network to the final spin network. This then naturally gives rise to the two-complex (a combinatorial set of faces that join along edges, which in turn join on vertices) underlying the spin foam description; we evolve forward an initial spin network sweeping out a surface, the action of the Hamiltonian constraint operator is to produce a new planar surface starting at the vertex. We are able to use the action of the Hamiltonian constraint on the vertex of a spin network state to associate an amplitude to each "interaction" (in analogy to Feynman diagrams). See figure below. This opens up a way of trying to directly link canonical LQG to a path integral description. Now just as a spin networks describe quantum space, each configuration contributing to these path integrals, or sums over history, describe `quantum space-time'. Because of their resemblance to soap foams and the way they are labeled John Baez gave these `quantum space-times' the name `spin foams'.
There are however severe difficulties with this particular approach, for example the Hamiltonian operator is not self-adjoint, in fact it is not even a normal operator (i.e. the operator does not commute with its adjoint) and so the spectral theorem cannot be used to define the exponential in general. The most serious problem is that the formula_328's are not mutually commuting, it can then be shown the formal quantity formula_322 cannot even define a (generalized) projector. The master constraint (see below) does not suffer from these problems and as such offers a way of connecting the canonical theory to the path integral formulation.
Spin foams from BF theory.
It turns out there are alternative routes to formulating the path integral, however their connection to the Hamiltonian formalism is less clear. One way is to start with the BF theory. This is a simpler theory to general relativity. It has no local degrees of freedom and as such depends only on topological aspects of the fields. BF theory is what is known as a topological field theory. Surprisingly, it turns out that general relativity can be obtained from BF theory by imposing a constraint, BF theory involves a field formula_330 and if one chooses the field formula_179 to be the (anti-symmetric) product of two tetrads
formula_332
(tetrads are like triads but in four spacetime dimensions), one recovers general relativity. The condition that the formula_179 field be given by the product of two tetrads is called the simplicity constraint. The spin foam dynamics of the topological field theory is well understood. Given the spin foam `interaction' amplitudes for this simple theory, one then tries to implement the simplicity conditions to obtain a path integral for general relativity. The non-trivial task of constructing a spin foam model is then reduced to the question of how this simplicity constraint should be imposed in the quantum theory. The first attempt at this was the famous Barrett–Crane model. However this model was shown to be problematic, for example there did not seem to be enough degrees of freedom to ensure the correct classical limit. It has been argued that the simplicity constraint was imposed too strongly at the quantum level and should only be imposed in the sense of expectation values just as with the Lorenz gauge condition formula_334 in the Gupta–Bleuler formalism of quantum electrodynamics. New models have now been put forward, sometimes motivated by imposing the simplicity conditions in a weaker sense.
Another difficulty here is that spin foams are defined on a discretization of spacetime. While this presents no problems for a topological field theory as it has no local degrees of freedom, it presents problems for GR. This is known as the problem triangularization dependence.
Modern formulation of spin foams.
Just as imposing the classical simplicity constraint recovers general relativity from BF theory, one expects an appropriate quantum simplicity constraint will recover quantum gravity from quantum BF theory.
Much progress has been made with regard to this issue by Engle, Pereira, and Rovelli and Freidal and Krasnov in defining spin foam interaction amplitudes with much better behaviour.
An attempt to make contact between EPRL-FK spin foam and the canonical formulation of LQG has been made.
Spin foam derived from the master constraint operator.
See below.
The semi-classical limit.
What is the semiclassical limit?
The classical limit or correspondence limit is the ability of a physical theory to approximate or "recover" classical mechanics when considered over special values of its parameters. The classical limit is used with physical theories that predict non-classical behavior.
In physics, the correspondence principle states that the behavior of systems described by the theory of quantum mechanics (or by the old quantum theory) reproduces classical physics in the limit of large quantum numbers. In other words, it says that for large orbits and for large energies, quantum calculations must agree with classical calculations.
The principle was formulated by Niels Bohr in 1920, though he had previously made use of it as early as 1913 in developing his model of the atom.
There are two basic requirements in establishing the semi-classical limit of any quantum theory:
i) reproduction of the Poisson brackets (of the diffeomorphism constraints in the case of general relativity). This is extremely important because, as noted above, the Poisson bracket algebra formed between the (smeared) constraints themselves completely determines the classical theory. This is analogous to establishing Ehrenfest's theorem;
ii) the specification of a complete set of classical observables whose corresponding operators (see complete set of commuting observables for the quantum mechanical definition of a complete set of observables) when acted on by appropriate semi-classical states reproduce the same classical variables with small quantum corrections (a subtle point is that states that are semi-classical for one class of observables may not be semi-classical for a different class of observables).
This may be easily done, for example, in ordinary quantum mechanics for a particle but in general relativity this becomes a highly non-trivial problem as we will see below.
Why might LQG not have general relativity as its semiclassical limit?
Any candidate theory of quantum gravity must be able to reproduce Einstein's theory of general relativity as a classical limit of a quantum theory. This is not guaranteed because of a feature of quantum field theories which is that they have different sectors, these are analogous to the different phases that come about in the thermodynamical limit of statistical systems. Just as different phases are physically different, so are different sectors of a quantum field theory. It may turn out that LQG belongs to an unphysical sector - one in which you do not recover general relativity in the semi classical limit (in fact there might not be any physical sector at all).
Moreover, the physical Hilbert space formula_335 must contain enough semi-classical states to guarantee that the quantum theory one obtains can return to the classical theory when
formula_336. In order to guarantee this one must avoid quantum anomalies at all cost, because if we do not there will be restrictions on the physical Hilbert space that have no counterpart in the classical theory, implying that the quantum theory has less degrees of freedom than the classical theory.
Theorems establishing the uniqueness of the loop representation as defined by Ashtekar et al. (i.e. a certain concrete realization of a Hilbert space and associated operators reproducing the correct loop algebra - the realization that everybody was using) have been given by two groups (Lewandowski, Okolow, Sahlmann and Thiemann; and Christian Fleischhack). Before this result was established it was not known whether there could be other examples of Hilbert spaces with operators invoking the same loop algebra, other realizations, not equivalent to the one that had been used so far. These uniqueness theorems imply no others exist and so if LQG does not have the correct semiclassical limit then this would mean the end of the loop representation of quantum gravity altogether.
Difficulties checking the semiclassical limit of LQG.
There are difficulties in trying to establish LQG gives Einstein's theory of general relativity in the semi classical limit. There are a number of particular difficulties in establishing the semi-classical limit
Difficulties in trying to examine the semi classical limit of the theory should not be confused with it having the wrong semi classical limit.
Progress in demonstrating LQG has the correct semiclassical limit.
Much details here to be written up...
Concerning issue number 2 above one can consider so-called weave states. Ordinary measurements of geometric quantities are macroscopic, and planckian discreteness is smoothed out. The fabric of a T-shirt is analogous. At a distance it is a smooth curved two-dimensional surface. But a closer inspection we see that it is actually composed of thousands of one-dimensional linked threads. The image of space given in LQG is similar, consider a very large spin network formed by a very large number of nodes and links, each of Planck scale. But probed at a macroscopic scale, it appears as a three-dimensional continuous metric geometry.
As far as the editor knows problem 4 of having semi-classical machinery for non-graph changing operators is as the moment still out of reach.
To make contact with familiar low energy physics it is mandatory to have to develop approximation schemes both for the physical inner product and for Dirac observables.
The spin foam models have been intensively studied can be viewed as avenues toward approximation schemes for the physical inner product.
Markopoulou et al. adopted the idea of noiseless subsystems in an attempt to solve the problem of the low
energy limit in background independent quantum gravity theories The idea has even led to the intriguing possibility of matter of the standard model being identified with emergent degrees of freedom from some versions of LQG (see section below: "LQG and related research programs").
As Wightman emphasized in the 1950s, in Minkowski QFTs the formula_339 point functions
formula_340,
completely determine the theory. In particular, one can calculate the scattering amplitudes from these quantities. As explained below in the section on the "Background independent scattering amplitudes", in the background-independent context, the formula_339 point functions refer to a state and in gravity that state can naturally encode information about a specific geometry which can then appear in the expressions of these quantities. To leading order LQG calculations have been shown to agree in an appropriate sense with the formula_339point functions calculated in the effective low energy quantum general relativity.
Improved dynamics and the master constraint.
The master constraint.
Thiemann's master constraint should not be confused with the master equation which has to do with random processes. The Master Constraint Programme for Loop Quantum Gravity (LQG) was proposed as a classically equivalent way to impose the infinite number of Hamiltonian constraint equations
formula_23
(formula_12 being a continuous index) in terms of a single master constraint,
formula_345.
which involves the square of the constraints in question. Note that formula_346 were infinitely many whereas the master constraint is only one. It is clear that if formula_347 vanishes then so do the infinitely many formula_346's. Conversely, if all the formula_346's vanish then so does formula_347, therefore they are equivalent. The master constraint formula_347 involves an appropriate averaging over all space and so is invariant under spatial diffeomorphisms (it is invariant under spatial "shifts" as it is a summation over all such spatial "shifts" of a quantity that transforms as a scalar). Hence its Poisson bracket with the (smeared) spacial diffeomorphism constraint, formula_352, is simple:
formula_353.
(it is formula_354 invariant as well). Also, obviously as any quantity Poisson commutes with itself, and the master constraint being a single constraint, it satisfies
formula_355.
We also have the usual algebra between spatial diffeomorphisms. This represents a dramatic simplification of the Poisson bracket structure, and raises new hope in understanding the dynamics and establishing the semi-classical limit.
An initial objection to the use of the master constraint was that on first sight it did not seem to encode information about the observables; because the Mater constraint is quadratic in the constraint, when you compute its Poisson bracket with any quantity, the result is proportional to the constraint, therefore it always vanishes when the constraints are imposed and as such does not select out particular phase space functions. However, it was realized that the condition
formula_356
is equivalent to formula_43 being a Dirac observable. So the master constraint does capture information about the observables. Because of its significance this is known as the Master equation.
That the master constraint Poisson algebra is an honest Lie algebra opens up the possibility of using a certain method, known as group averaging, in order to construct solutions of the infinite number of Hamiltonian constraints, a physical inner product thereon and Dirac observables via what is known as refined algebraic quantization RAQ
The quantum master constraint.
Define the quantum master constraint (regularisation issues aside) as
formula_358.
Obviously,
formula_359
for all formula_12 implies formula_361. Conversely, if formula_361 then
formula_363
implies
formula_359 .
What is done first is, we are able to compute the matrix elements of the would-be operator formula_365, that is, we compute the quadratic form formula_366. It turns out that as formula_366 is a graph changing, diffeomorphism invariant quadratic form it cannot exist on the kinematic Hilbert space formula_368, and must be defined on formula_369. The fact that the master constraint operator formula_365 is densely defined on formula_371, it is obvious that formula_365 is a positive and symmetric operator in formula_371. Therefore, the quadratic form formula_366 associated with formula_365 is closable. The closure of formula_366 is the quadratic form of a unique self-adjoint operator formula_377, called the Friedrichs extension of formula_365. We relabel formula_377 as formula_365 for simplicity. (Note that the presence of an inner product, viz Eq 4, means there are no superfluous solutions i.e. there are no formula_91 such that formula_382 but for which formula_361).
It is also possible to construct a quadratic form formula_384 for what is called the extended master constraint (discussed below) on formula_368 which also involves the weighted integral of the square of the spatial diffeomorphism constraint (this is possible because formula_384 is not graph changing).
The spectrum of the master constraint may not contain zero due to normal or factor ordering effects which are finite but similar in nature to the infinite vacuum energies of background-dependent quantum field theories. In this case it turns out to be physically correct to replace formula_365 with formula_388 provided that the "normal ordering constant" vanishes in the classical limit, that is, formula_389, so that formula_390 is a valid quantisation of formula_347.
Testing the master constraint.
The constraints in their primitive form are rather singular, this was the reason for integrating them over test functions to obtain smeared constraints. However, it would appear that the equation for the master constraint, given above, is even more singular involving the product of two primitive constraints (although integrated over space). Squaring the constraint is dangerous as it could lead to worsened ultraviolent behaviour of the corresponding operator and hence the master constraint programme must be approached with due care.
In doing so the master constraint programme has been satisfactorily tested in a number of model systems with non-trivial constraint algebras, free and interacting field theories. The master constraint for LQG was established as a genuine positive self-adjoint operator and the physical Hilbert space of LQG was shown to be non-empty, an obvious consistency test LQG must pass to be a viable theory of quantum General relativity.
Applications of the master constraint.
The master constraint has been employed in attempts to approximate the physical inner product and define more rigorous path integrals.
The Consistent Discretizations approach to LQG, is an application of the master constraint program to construct the physical Hilbert space of the canonical theory.
Spin foam from the master constraint.
It turns out that the master constraint is easily generalized to incorporate the other constraints. It is then referred to as the extended master constraint, denoted formula_392. We can define the extended master constraint which imposes both the Hamiltonian constraint and spatial diffeomorphism constraint as a single operator,
formula_393.
Setting this single constraint to zero is equivalent to formula_394 and formula_395 for all formula_12 in formula_164. This constraint implements the spatial diffeomorphism and Hamiltonian constraint at the same time on the Kinematic Hilbert space. The physical inner product is then defined as
formula_398
(as formula_399). A spin foam representation of this expression is obtained by splitting the formula_294-parameter in discrete steps and writing
formula_401
The spin foam description then follows from the application of formula_402 on a spin network resulting in a linear combination of new spin networks whose graph and labels have been modified. Obviously an approximation is made by truncating the value of formula_403 to some finite integer. An advantage of the extended master constraint is that we are working at the kinematic level and so far it is only here we have access semi-classical coherent states. Moreover, one can find none graph changing versions of this master constraint operator, which are the only type of operators appropriate for these coherent states.
Algebraic quantum gravity.
The master constraint programme has evolved into a fully combinatorial treatment of gravity known as Algebraic Quantum Gravity (AQG). The non-graph changing master constraint operator is adapted in the framework of algebraic quantum gravity. While AQG is inspired by LQG, it differs drastically from it because in AQG there is fundamentally no topology or differential structure - it is background independent in a more generalized sense and could possibly have something to say about topology change. In this new formulation of quantum gravity AQG semiclassical states always control the fluctuations of all present degrees of freedom. This makes the AQG semiclassical analysis superior over that of LQG, and progress has been made in establishing it has the correct semiclassical limit and providing contact with familiar low energy physics. See Thiemann's book for details.
Physical applications of LQG.
Black hole entropy.
The Immirzi parameter (also known as the Barbero-Immirzi parameter) is a numerical coefficient appearing in loop quantum gravity. It may take real or imaginary values.
Black hole thermodynamics is the area of study that seeks to reconcile the laws of thermodynamics with the existence of black hole event horizons. The no hair conjecture of general relativity states that a black hole is characterized only by its mass, its charge, and its angular momentum; hence, it has no entropy. It appears, then, that one can violate the second law of thermodynamics by dropping an object with nonzero entropy into a black hole. Work by Stephen Hawking and Jacob Bekenstein showed that one can preserve the second law of thermodynamics by assigning to each black hole a "black-hole entropy"
where formula_81 is the area of the hole's event horizon, formula_406 is the Boltzmann constant, and formula_407 is the Planck length. The fact that the black hole entropy is also the maximal entropy that can be obtained by the Bekenstein bound (wherein the Bekenstein bound becomes an equality) was the main observation that led to the holographic principle.
An oversight in the application of the no-hair theorem is the assumption that the relevant degrees of freedom accounting for the entropy of the black hole must be classical in nature; what if they were purely quantum mechanical instead and had non-zero entropy? Actually, this is what is realized in the LQG derivation of black hole entropy, and can be seen as a consequence of its background-independence – the classical black hole spacetime comes about from the semi-classical limit of the quantum state of the gravitational field, but there are many quantum states that have the same semiclassical limit. Specifically, in LQG it is possible to associate a quantum geometrical interpretation to the microstates: These are the quantum geometries of the horizon which are consistent with the area, formula_81, of the black hole and the topology of the horizon (i.e. spherical). LQG offers a geometric explanation of the finiteness of the entropy and of the proportionality of the area of the horizon. These calculations have been generalized to rotating black holes.
It is possible to derive, from the covariant formulation of full quantum theory (Spinfoam) the correct relation between energy and area (1st law), the Unruh temperature and the distribution that yields Hawking entropy. The calculation makes use of the notion of dynamical horizon and is done for non-extremal black holes.
A recent success of the theory in this direction is the computation of the entropy of all non singular black holes directly from theory and independent of Immirzi parameter. The result is the expected formula formula_409, where formula_410 is the entropy and formula_81 the area of the black hole, derived by Bekenstein and Hawking on heuristic grounds. This is the only known derivation of this formula from a fundamental theory, for the case of generic non singular black holes. Older attempts at this calculation had difficulties. The problem was that although Loop quantum gravity predicted that the entropy of a black hole is proportional to the area of the event horizon, the result depended on a crucial free parameter in the theory, the above-mentioned Immirzi parameter. However, there is no known computation of the Immirzi parameter, so it had to be fixed by demanding agreement with Bekenstein and Hawking's calculation of the black hole entropy.
Loop quantum cosmology.
The popular and technical literature makes extensive references to LQG-related topic of loop quantum cosmology. LQC was mainly developed by Martin Bojowald, it was popularized Loop quantum cosmology in "Scientific American" for predicting a Big Bounce prior to the Big Bang. Loop quantum cosmology (LQC) is a symmetry-reduced model of classical general relativity quantized using methods that mimic those of loop quantum gravity (LQG) that predicts a "quantum bridge" between contracting and expanding cosmological branches.
Achievements of LQC have been the resolution of the big bang singularity, the prediction of a Big Bounce, and a natural mechanism for inflation (cosmology).
LQC models share features of LQG and so is a useful toy model. However, the results obtained are subject to the usual restriction that a truncated classical theory, then quantized, might not display the true behaviour of the full theory due to artificial suppression of degrees of freedom that might have large quantum fluctuations in the full theory. It has been argued that singularity avoidance in LQC are by mechanisms only available in these restrictive models and that singularity avoidance in the full theory can still be obtained but by a more subtle feature of LQG.
Loop quantum gravity phenomenology.
Quantum gravity effects are notoriously difficult to measure because the Planck length is so incredibly small. However recently physicists have started to consider the possibility of measuring quantum gravity effects mostly from astrophysical observations and gravitational wave detectors.The energy of those fluctuations at scales this small cause space-perturbations which are visible at higher scales.
Background independent scattering amplitudes.
Loop quantum gravity is formulated in a background-independent language. No spacetime is assumed a priori, but rather it is built up by the states of theory themselves - however scattering amplitudes are derived from formula_403-point functions (Correlation function (quantum field theory)) and these, formulated in conventional quantum field theory, are functions of points of a background space-time. The relation between the background-independent formalism and the conventional formalism of quantum field theory on a given spacetime is far from obvious, and it is far from obvious how to recover low-energy quantities from the full background-independent theory. One would like to derive the formula_403-point functions of the theory from the background-independent formalism, in order to compare them with the standard perturbative expansion of quantum general relativity and therefore check that loop quantum gravity yields the correct low-energy limit.
A strategy for addressing this problem has been suggested; the idea is to study the boundary amplitude, namely a path integral over a finite space-time region, seen as a function of the boundary value of the field. In conventional quantum field theory, this boundary amplitude is well–defined and codes the physical information of the theory; it does so in quantum gravity as well, but in a fully background–independent manner. A generally covariant definition of formula_403-point functions can then be based on the idea that the distance between physical points –arguments of the formula_403-point function is determined by the state of the gravitational field on the boundary of the spacetime region considered.
Progress has been made in calculating background independent scattering amplitudes this way with the use of spin foams. This is a way to extract physical information from the theory. Claims to have reproduced the correct behaviour for graviton scattering amplitudes and to have recovered classical gravity have been made. "We have calculated Newton's law starting from a world with no space and no time." - Carlo Rovelli.
Gravitons, string theory, supersymmetry, extra dimensions in LQG.
Some quantum theories of gravity posit a spin-2 quantum field that is quantized, giving rise to gravitons. In string theory one generally starts with quantized excitations on top of a classically fixed background. This theory is thus described as background dependent. Particles like photons as well as changes in the spacetime geometry (gravitons) are both described as excitations on the string worldsheet. The background dependence of string theory can have important physical consequences, such as determining the number of quark generations. In contrast, loop quantum gravity, like general relativity, is manifestly background independent, eliminating the background required in string theory. Loop quantum gravity, like string theory, also aims to overcome the nonrenormalizable divergences of quantum field theories.
LQG never introduces a background and excitations living on this background, so LQG does not use gravitons as building blocks. Instead one expects that one may recover a kind of semiclassical limit or weak field limit where something like "gravitons" will show up again. In contrast, gravitons play a key role in string theory where they are among the first (massless) level of excitations of a superstring.
LQG differs from string theory in that it is formulated in 3 and 4 dimensions and without supersymmetry or Kaluza-Klein extra dimensions, while the latter requires both to be true. There is no experimental evidence to date that confirms string theory's predictions of supersymmetry and Kaluza–Klein extra dimensions. In a 2003 paper A dialog on quantum gravity, Carlo Rovelli regards the fact LQG is formulated in 4 dimensions and without supersymmetry as a strength of the theory as it represents the most parsimonious explanation, consistent with current experimental results, over its rival string/M-theory. Proponents of string theory will often point to the fact that, among other things, it demonstrably reproduces the established theories of general relativity and quantum field theory in the appropriate limits, which Loop Quantum Gravity has struggled to do. In that sense string theory's connection to established physics may be considered more reliable and less speculative, at the mathematical level. Peter Woit in Not Even Wrong and Lee Smolin in The Trouble with Physics regard string/M-theory to be in conflict with current known experimental results.
Since LQG has been formulated in 4 dimensions (with and without supersymmetry), and M-theory requires supersymmetry and 11 dimensions, a direct comparison between the two has not been possible. It is possible to extend mainstream LQG formalism to higher-dimensional supergravity, general relativity with supersymmetry and Kaluza–Klein extra dimensions should experimental evidence establish their existence. It would therefore be desirable to have higher-dimensional Supergravity loop quantizations at one's
disposal in order to compare these approaches. In fact a series of recent papers have been published attempting just this. Most recently, Thiemann (and alumni) have made progress toward calculating black hole entropy for supergravity in higher dimensions. It will be interesting to compare these results to the corresponding super string calculations.
As of April 2013 LHC has failed to find evidence of supersymmetry or Kaluza–Klein extra dimensions, which has encouraged LQG researchers. Shaposhnikov in his paper "Is there a new physics between electroweak and Planck scales?" has proposed the neutrino minimal standard model, which claims the most parsimonious theory is a standard model extended with neutrinos, plus gravity, and that extra dimensions, GUT physics, and supersymmetry, string/M-theory physics are unrealized in nature, and that any theory of quantum gravity must be four dimensional, like loop quantum gravity.
LQG and related research programs.
Several research groups have attempted to combine LQG with other research programs: Johannes Aastrup, Jesper M. Grimstrup et al. research combines noncommutative geometry with loop quantum gravity, Laurent Freidel, Simone Speziale, et al., spinors and twistor theory with loop quantum gravity, and Lee Smolin et al. with Verlinde entropic gravity and loop gravity. Stephon Alexander, Antonino Marciano and Lee Smolin have attempted to explain the origins of weak force chirality in terms of Ashketar's variables, which describe gravity as chiral, and LQG with Yang–Mills theory fields in four dimensions. Sundance Bilson-Thompson, Hackett et al., has attempted to introduce standard model via LQG"s degrees of freedom as an emergent property (by employing the idea noiseless subsystems a useful notion introduced in more general situation for constrained systems by Fotini Markopoulou-Kalamara et al.) LQG has also drawn philosophical comparisons with causal dynamical triangulation and asymptotically safe gravity, and the spinfoam with group field theory and AdS/CFT correspondence. Smolin and Wen have suggested combining LQG with String-net liquid, tensors, and Smolin and Fotini Markopoulou-Kalamara Quantum Graphity. There is the consistent discretizations approach. In addition to what has already mentioned above, Pullin and Gambini provide a framework to connect the path integral and canonical approaches to quantum gravity. They may help reconcile the spin foam and canonical loop representation approaches. Recent research by Chris Duston and Matilde Marcolli introduces topology change via topspin networks.
Problems and comparisons with alternative approaches.
Some of the major unsolved problems in physics are theoretical, meaning that existing theories seem incapable of explaining a certain observed phenomenon or experimental result. The others are experimental, meaning that there is a difficulty in creating an experiment to test a proposed theory or investigate a phenomenon in greater detail.
Can quantum mechanics and general relativity be realized as a fully consistent theory (perhaps as a quantum field theory)? Is spacetime fundamentally continuous or discrete? Would a consistent theory involve a force mediated by a hypothetical graviton, or be a product of a discrete structure of spacetime itself (as in loop quantum gravity)? Are there deviations from the predictions of general relativity at very small or very large scales or in other extreme circumstances that flow from a quantum gravity theory?
The theory of LQG is one possible solution to the problem of quantum gravity, as is string theory. There are substantial differences however. For example, string theory also addresses unification, the understanding of all known forces and particles as manifestations of a single entity, by postulating extra dimensions and so-far unobserved additional particles and symmetries. Contrary to this, LQG is based only on quantum theory and general relativity and its scope is limited to understanding the quantum aspects of the gravitational interaction. On the other hand, the consequences of LQG are radical, because they fundamentally change the nature of space and time and provide a tentative but detailed physical and mathematical picture of quantum spacetime.
Presently, no semiclassical limit recovering general relativity has been shown to exist. This means it remains unproven that LQG's description of spacetime at the Planck scale has the right continuum limit (described by general relativity with possible quantum corrections). Specifically, the dynamics of the theory is encoded in the Hamiltonian constraint, but there is no candidate Hamiltonian. Other technical problems include finding off-shell closure of the constraint algebra and physical inner product vector space, coupling to matter fields of Quantum field theory, fate of the renormalization of the graviton in perturbation theory that lead to ultraviolet divergence beyond 2-loops (see One-loop Feynman diagram in Feynman diagram).
While there has been a recent proposal relating to observation of naked singularities, and doubly special relativity as a part of a program called loop quantum cosmology, there is no experimental observation for which loop quantum gravity makes a prediction not made by the Standard Model or general relativity (a problem that plagues all current theories of quantum gravity). Because of the above-mentioned lack of a semiclassical limit, LQG has not yet even reproduced the predictions made by general relativity.
An alternative criticism is that general relativity may be an effective field theory, and therefore quantization ignores the fundamental degrees of freedom.

</doc>
<doc id="543568" url="https://en.wikipedia.org/wiki?curid=543568" title="Lorentz covariance">
Lorentz covariance

In physics, Lorentz symmetry, named for Hendrik Lorentz, is "the feature of nature that says experimental results are independent of the orientation or the boost velocity of the laboratory through space". Lorentz covariance, a related concept, is a key property of spacetime following from the special theory of relativity. Lorentz covariance has two distinct, but closely related meanings:
This usage of the term "covariant" should not be confused with the related concept of a "covariant vector". On manifolds, the words "covariant" and "contravariant" refer to how objects transform under general coordinate transformations. Confusingly, both covariant and contravariant four-vectors can be Lorentz covariant quantities.
Local Lorentz covariance, which follows from general relativity, refers to Lorentz covariance applying only "locally" in an infinitesimal region of spacetime at every point. There is a generalization of this concept to cover Poincaré covariance and Poincaré invariance.
Examples.
In general, the nature of a Lorentz tensor can be identified by its tensor order, which is the number of indices it has. No indices implies it is a scalar, one implies that it is a vector, etc. Furthermore, any number of new scalars, vectors etc. can be made by contracting any kinds of tensors together, but many of these may not have any real physical meaning. Some of those tensors that do have a physical interpretation are listed (by no means exhaustively) below.
Please note, the metric sign convention such that η = diag (1, −1, −1, −1) is used throughout the article.
Scalars.
Spacetime interval:
Proper time (for timelike intervals):
Proper distance (for spacelike intervals):
Rest mass:
Electromagnetism invariants:
D'Alembertian/wave operator:
Four-vectors.
4-Displacement:
4-Position:
4-Gradient: with is the 4D Partial derivative:
4-Velocity:
where formula_12
4-Momentum:
where formula_14
4-Current:
where formula_16
Four-tensors.
The Kronecker delta:
The Minkowski metric (the metric of flat space according to general relativity):
The Levi-Civita symbol:
Electromagnetic field tensor (using a metric signature of + − − − ):
Dual electromagnetic field tensor:
Lorentz violating models.
In standard field theory, there are very strict and severe constraints on marginal and relevant Lorentz violating operators within both QED and the Standard Model. Irrelevant Lorentz violating operators may be suppressed by a high cutoff scale, but they typically induce marginal and relevant Lorentz violating operators via radiative corrections. So, we also have very strict and severe constraints on irrelevant Lorentz violating operators.
Since some approaches to quantum gravity lead to violations of Lorentz invariance, these studies are part of Phenomenological Quantum Gravity.
Lorentz violating models typically fall into four classes:
Models belonging to the first two classes can be consistent with experiment if Lorentz breaking happens at Planck scale or beyond it, or even before it in suitable preonic models, and if Lorentz symmetry violation is governed by a suitable energy-dependent parameter. One then has a class of models which deviate from Poincaré symmetry near the Planck scale but still flows towards an exact Poincaré group at very large length scales. This is also true for the third class, which is furthermore protected from radiative corrections as one still has an exact (quantum) symmetry.
Even though there is no evidence of the violation of Lorentz invariance, several experimental searches for such violations have been performed during recent years. A detailed summary of the results of these searches is given in the Data Tables for Lorentz and CPT Violation.

</doc>
<doc id="732446" url="https://en.wikipedia.org/wiki?curid=732446" title="Lorentz factor">
Lorentz factor

The Lorentz factor or Lorentz term is the factor by which time, length, and relativistic mass change for an object while that object is moving. The expression appears in several equations in special relativity, and it arises in derivations of the Lorentz transformations. The name originates from its earlier appearance in Lorentzian electrodynamics – named after the Dutch physicist Hendrik Lorentz.
Due to its ubiquity, it is generally denoted with the symbol "γ" (Greek lowercase gamma). Sometimes (especially in discussion of superluminal motion) the factor is written as "Γ" (Greek uppercase-gamma) rather than "γ".
Definition.
The Lorentz factor is defined as:
where:
This is the most frequently used form in practice, though not the only one (see below for alternative forms).
To complement the definition, some authors define the reciprocal:
see velocity addition formula.
Occurrence.
Following is a list of formulae from Special relativity which use "γ" as a shorthand:
Corollaries of the above transformations are the results:
Applying conservation of momentum and energy leads to these results:
Numerical values.
In the table below, the left-hand column shows speeds as different fractions of the speed of light (i.e. in units of "c"). The middle column shows the corresponding Lorentz factor, the final is the reciprocal. Values in bold are exact.
Alternative representations.
There are other ways to write the factor. Above, velocity "v" was used, but related variables such as momentum and rapidity may also be convenient.
Momentum.
Solving the previous relativistic momentum equation for "γ" leads to:
This form is rarely used, it does however appear in the Maxwell–Jüttner distribution.
Rapidity.
Applying the definition of rapidity as the following hyperbolic angle "φ":
also leads to "γ" (by use of hyperbolic identities):
Using the property of Lorentz transformation, it can be shown that rapidity is additive, a useful property that velocity does not have. Thus the rapidity parameter forms a one-parameter group, a foundation for physical models.
Series expansion (velocity).
The Lorentz factor has the Maclaurin series:
which is a special case of a binomial series.
The approximation "γ" ≈ 1 + 1/2 "β"2 may be used to calculate relativistic effects at low speeds. It holds to within 1% error for "v" < 0.4 c ("v" < 120,000 km/s), and to within 0.1% error for "v" < 0.22 "c" ("v" < 66,000 km/s).
The truncated versions of this series also allow physicists to prove that special relativity reduces to Newtonian mechanics at low speeds. For example, in special relativity, the following two equations hold:
For "γ" ≈ 1 and "γ" ≈ 1 + 1/2 "β"2, respectively, these reduce to their Newtonian equivalents:
The Lorentz factor equation can also be inverted to yield:
This has an asymptotic form of:
The first two terms are occasionally used to quickly calculate velocities from large "γ" values. The approximation "β" ≈ 1 - 1/2 "γ"−2 holds to within 1% tolerance for "γ" > 2, and to within 0.1% tolerance for "γ" > 3.5.
Applications in astronomy.
The standard model of long-duration gamma-ray bursts (GRBs) holds that these explosions are ultra-relativistic (initial formula_7 greater than approximately 100), which is invoked to explain the so-called "compactness" problem: absent this ultra-relativistic expansion, the ejecta would be optically thick to pair production at typical peak spectral energies of a few 100 keV, whereas the prompt emission is observed to be non-thermal.

</doc>
<doc id="18631" url="https://en.wikipedia.org/wiki?curid=18631" title="Lorentz force">
Lorentz force

In physics, particularly electromagnetism, the Lorentz force is the combination of electric and magnetic force on a point charge due to electromagnetic fields. If a particle of charge "q" moves with velocity v in the presence of an electric field E and a magnetic field B, then it will experience a force
(in SI units). Variations on this basic formula describe the magnetic force on a current-carrying wire (sometimes called "Laplace force"), the electromotive force in a wire loop moving through a magnetic field (an aspect of Faraday's law of induction), and the force on a charged particle which might be travelling near the speed of light (relativistic form of the Lorentz force).
The first derivation of the Lorentz force is commonly attributed to Oliver Heaviside in 1889, although other historians suggest an earlier origin in an 1865 paper by James Clerk Maxwell. Hendrik Lorentz derived it a few years after Heaviside.
Equation (SI units).
Charged particle.
The force F acting on a particle of electric charge "q" with instantaneous velocity v, due to an external electric field E and magnetic field B, is given by:
where × is the vector cross product. All boldface quantities are vectors. More explicitly stated:
in which r is the position vector of the charged particle, "t" is time, and the overdot is a time derivative.
A positively charged particle will be accelerated in the "same" linear orientation as the E field, but will curve perpendicularly to both the instantaneous velocity vector v and the B field according to the right-hand rule (in detail, if the thumb of the right hand points along v and the index finger along B, then the middle finger points along F).
The term "qE is called the electric force, while the term "qv × B is called the magnetic force. According to some definitions, the term "Lorentz force" refers specifically to the formula for the magnetic force, with the "total" electromagnetic force (including the electric force) given some other (nonstandard) name. This article will "not" follow this nomenclature: In what follows, the term "Lorentz force" will refer only to the expression for the total force.
The magnetic force component of the Lorentz force manifests itself as the force that acts on a current-carrying wire in a magnetic field. In that context, it is also called the Laplace force.
Continuous charge distribution.
For a continuous charge distribution in motion, the Lorentz force equation becomes:
where "d"F is the force on a small piece of the charge distribution with charge "dq". If both sides of this equation are divided by the volume of this small piece of the charge distribution "dV", the result is:
where f is the "force density" (force per unit volume) and "ρ" is the charge density (charge per unit volume). Next, the current density corresponding to the motion of the charge continuum is
so the continuous analogue to the equation is
The total force is the volume integral over the charge distribution:
By eliminating ρ and J, using Maxwell's equations, and manipulating using the theorems of vector calculus, this form of the equation can be used to derive the Maxwell stress tensor σ, in turn this can be combined with the Poynting vector S to obtain the electromagnetic stress–energy tensor T used in general relativity.
In terms of σ and S, another way to write the Lorentz force (per unit 3D volume) is
where "c" is the speed of light and ∇· denotes the divergence of a tensor field. Rather than the amount of charge and its velocity in electric and magnetic fields, this equation relates the energy flux (flow of "energy" per unit time per unit distance) in the fields to the force exerted on a charge distribution. See Covariant formulation of classical electromagnetism for more details.
History.
Early attempts to quantitatively describe the electromagnetic force were made in the mid-18th century. It was proposed that the force on magnetic poles, by Johann Tobias Mayer and others in 1760, and electrically charged objects, by Henry Cavendish in 1762, obeyed an inverse-square law. However, in both cases the experimental proof was neither complete nor conclusive. It was not until 1784 when Charles-Augustin de Coulomb, using a torsion balance, was able to definitively show through experiment that this was true. Soon after the discovery in 1820 by H. C. Ørsted that a magnetic needle is acted on by a voltaic current, André-Marie Ampère that same year was able to devise through experimentation the formula for the angular dependence of the force between two current elements. In all these descriptions, the force was always given in terms of the properties of the objects involved and the distances between them rather than in terms of electric and magnetic fields.
The modern concept of electric and magnetic fields first arose in the theories of Michael Faraday, particularly his idea of lines of force, later to be given full mathematical description by Lord Kelvin and James Clerk Maxwell. From a modern perspective it is possible to identify in Maxwell's 1865 formulation of his field equations a form of the Lorentz force equation in relation to electric currents, however, in the time of Maxwell it was not evident how his equations related to the forces on moving charged objects. J. J. Thomson was the first to attempt to derive from Maxwell's field equations the electromagnetic forces on a moving charged object in terms of the object's properties and external fields. Interested in determining the electromagnetic behavior of the charged particles in cathode rays, Thomson published a paper in 1881 wherein he gave the force on the particles due to an external magnetic field as
Thomson derived the correct basic form of the formula, but, because of some miscalculations and an incomplete description of the [[displacement current]], included an incorrect scale-factor of a half in front of the formula. It was [[Oliver Heaviside]], who had invented the modern vector notation and applied them to Maxwell's field equations, that in 1885 and 1889 fixed the mistakes of Thomson's derivation and arrived at the correct form of the magnetic force on a moving charged object. Finally, in 1892, [[Hendrik Lorentz]] derived the modern form of the formula for the electromagnetic force which includes the contributions to the total force from both the electric and the magnetic fields. Lorentz began by abandoning the Maxwellian descriptions of the ether and conduction. Instead, Lorentz made a distinction between matter and the [[luminiferous aether]] and sought to apply the Maxwell equations at a microscopic scale. Using Heaviside's version of the Maxwell equations for a stationary ether and applying [[Lagrangian mechanics]] (see below), Lorentz arrived at the correct and complete form of the force law that now bears his name.
Trajectories of particles due to the Lorentz force.
[[File:charged-particle-drifts.svg|300px|thumbnail|right|Charged particle drifts in a homogeneous magnetic field. (A) No disturbing force (B) With an electric field, E (C) With an independent force, F (e.g. gravity) (D) In an inhomogeneous magnetic field, grad H]]
In many cases of practical interest, the motion in a [[magnetic field]] of an [[electric charge|electrically charged]] particle (such as an [[electron]] or [[ion]] in a [[Plasma (physics)|plasma]]) can be treated as the superposition of a relatively fast circular motion around a point called the guiding center and a relatively slow drift of this point. The drift speeds may differ for various species depending on their charge states, masses, or temperatures, possibly resulting in electric currents or chemical separation.
Significance of the Lorentz force.
While the modern Maxwell's equations describe how electrically charged particles and currents or moving charged particles give rise to electric and magnetic fields, the Lorentz force law completes that picture by describing the force acting on a moving point charge "q" in the presence of electromagnetic fields. The Lorentz force law describes the effect of E and B upon a point charge, but such electromagnetic forces are not the entire picture. Charged particles are possibly coupled to other forces, notably gravity and nuclear forces. Thus, Maxwell's equations do not stand separate from other physical laws, but are coupled to them via the charge and current densities. The response of a point charge to the Lorentz law is one aspect; the generation of E and B by currents and charges is another.
In real materials the Lorentz force is inadequate to describe the behavior of charged particles, both in principle and as a matter of computation. The charged particles in a material medium both respond to the E and B fields and generate these fields. Complex transport equations must be solved to determine the time and spatial response of charges, for example, the [[Boltzmann equation]] or the [[Fokker–Planck equation]] or the [[Navier–Stokes equations]]. For example, see [[magnetohydrodynamics]], [[fluid dynamics]], [[electrohydrodynamics]], [[superconductivity]], [[stellar evolution]]. An entire physical apparatus for dealing with these matters has developed. See for example, [[Green–Kubo relations]] and [[Green's function (many-body theory)]].
Lorentz force law as the definition of E and B.
In many textbook treatments of classical electromagnetism, the Lorentz force Law is used as the "definition" of the electric and magnetic fields E and B. To be specific, the Lorentz force is understood to be the following empirical statement:
This "is" valid; countless experiments have shown that it is, even for particles approaching the speed of light (that is, [[Norm (mathematics)#Euclidean norm|magnitude]] of v = |v| = "c"). So the two [[vector field]]s E and B are thereby defined throughout space and time, and these are called the "electric field" and "magnetic field". Note that the fields are defined everywhere in space and time with respect to what force a test charge would receive regardless of whether a charge is present to experience the force.
Note also that as a definition of E and B, the Lorentz force is only a definition in principle because a real particle (as opposed to the hypothetical "test charge" of infinitesimally-small mass and charge) would generate its own finite E and B fields, which would alter the electromagnetic force that it experiences. In addition, if the charge experiences acceleration, as if forced into a curved trajectory by some external agency, it emits radiation that causes braking of its motion. See for example [[Bremsstrahlung]] and [[synchrotron light]]. These effects occur through both a direct effect (called the [[Abraham–Lorentz force|radiation reaction force]]) and indirectly (by affecting the motion of nearby charges and currents). Moreover, net force must include [[gravity]], [[Electroweak interaction|electroweak]], and any other forces aside from electromagnetic force.
Force on a current-carrying wire.
[[File:Regla mano derecha Laplace.svg|right|thumb|250px|Right-hand rule for a current-carrying wire in a magnetic field B]]
When a wire carrying an electric current is placed in a magnetic field, each of the moving charges, which comprise the current, experiences the Lorentz force, and together they can create a macroscopic force on the wire (sometimes called the Laplace force). By combining the Lorentz force law above with the definition of electric current, the following equation results, in the case of a straight, stationary wire:
where ℓ is a vector whose magnitude is the length of wire, and whose direction is along the wire, aligned with the direction of [[conventional current]] flow "I".
If the wire is not straight but curved, the force on it can be computed by applying this formula to each [[infinitesimal]] segment of wire "d"ℓ, then adding up all these forces by [[integration (calculus)|integration]]. Formally, the net force on a stationary, rigid wire carrying a steady current "I" is
This is the net force. In addition, there will usually be [[torque]], plus other effects if the wire is not perfectly rigid.
One application of this is [[Ampère's force law]], which describes how two current-carrying wires can attract or repel each other, since each experiences a Lorentz force from the other's magnetic field. For more information, see the article: [[Ampère's force law]].
EMF.
The magnetic force ("q" v × B) component of the Lorentz force is responsible for "motional" [[electromotive force]] (or "motional EMF"), the phenomenon underlying many electrical generators. When a conductor is moved through a magnetic field, the magnetic force tries to push electrons through the wire, and this creates the EMF. The term "motional EMF" is applied to this phenomenon, since the EMF is due to the "motion" of the wire.
In other electrical generators, the magnets move, while the conductors do not. In this case, the EMF is due to the electric force ("q"E) term in the Lorentz Force equation. The electric field in question is created by the changing magnetic field, resulting in an "induced" EMF, as described by the [[Electromagnetic induction#Maxwell–Faraday equation|Maxwell–Faraday equation]] (one of the four modern [[Maxwell's equations]]).
Both of these EMF's, despite their different origins, can be described by the same equation, namely, the EMF is the rate of change of [[magnetic flux]] through the wire. (This is Faraday's law of induction, see [[Lorentz force#Lorentz force and Faraday.27s law of induction|below]].) Einstein's [[special theory of relativity]] was partially motivated by the desire to better understand this link between the two effects. In fact, the electric and magnetic fields are different faces of the same electromagnetic field, and in moving from one inertial frame to another, the [[solenoidal vector field]] portion of the "E"-field can change in whole or in part to a "B"-field or "vice versa".
Lorentz force and Faraday's law of induction.
Given a loop of wire in a [[magnetic field]], Faraday's law of induction states the induced [[electromotive force]] (EMF) in the wire is:
where
is the [[magnetic flux]] through the loop, B is the magnetic field, Σ("t") is a surface bounded by the closed contour ∂Σ("t"), at all at time "t", dA is an infinitesimal [[vector area]] element of Σ("t") (magnitude is the area of an infinitesimal patch of surface, direction is [[orthogonal]] to that surface patch).
The "sign" of the EMF is determined by [[Lenz's law]]. Note that this is valid for not only a "stationary" wire — but also for a "moving" wire.
From [[Faraday's law of induction]] (that is valid for a moving wire, for instance in a motor) and the [[Maxwell Equations]], the Lorentz Force can be deduced. The reverse is also true, the Lorentz force and the [[Maxwell Equations]] can be used to derive the [[Faraday's law of induction|Faraday Law]].
Let Σ("t") be the moving wire, moving together without rotation and with constant velocity v and Σ("t") be the internal surface of the wire. The EMF around the closed path ∂Σ("t") is given by:
where
is the electric field and dℓ is an [[infinitesimal]] vector element of the contour ∂Σ("t").
NB: Both dℓ and dA have a sign ambiguity; to get the correct sign, the [[right-hand rule]] is used, as explained in the article [[Kelvin-Stokes theorem]].
The above result can be compared with the version of Faraday's law of induction that appears in the modern Maxwell's equations, called here the "Maxwell-Faraday equation":
The Maxwell-Faraday equation also can be written in an "integral form" using the [[Kelvin-Stokes theorem]].
So we have, the Maxwell Faraday equation:
The two are equivalent if the wire is not moving. Using the [[Leibniz integral rule]] and that "div" B = 0, results in,
and using the Maxwell Faraday equation,
since this is valid for any wire position it implies that,
Faraday's law of induction holds whether the loop of wire is rigid and stationary, or in motion or in process of deformation, and it holds whether the magnetic field is constant in time or changing. However, there are cases where Faraday's law is either inadequate or difficult to use, and application of the underlying Lorentz force law is necessary. See [[Faraday paradox#Inapplicability of Faraday's law|inapplicability of Faraday's law]].
If the magnetic field is fixed in time and the conducting loop moves through the field, the magnetic flux Φ"B" linking the loop can change in several ways. For example, if the B-field varies with position, and the loop moves to a location with different B-field, Φ"B" will change. Alternatively, if the loop changes orientation with respect to the B-field, the B • dA differential element will change because of the different angle between B and dA, also changing Φ"B". As a third example, if a portion of the circuit is swept through a uniform, time-independent B-field, and another portion of the circuit is held stationary, the flux linking the entire closed circuit can change due to the shift in relative position of the circuit's component parts with time (surface ∂Σ("t") time-dependent). In all three cases, Faraday's law of induction then predicts the EMF generated by the change in Φ"B".
Note that the Maxwell Faraday's equation implies that the Electric Field E is non conservative when the Magnetic Field B varies in time, and is not expressible as the gradient of a [[scalar field]], and not subject to the [[gradient theorem]] since its rotational is not zero.
Lorentz force in terms of potentials.
The E and B fields can be replaced by the [[magnetic vector potential]] A and ([[Scalar (mathematics)|scalar]]) [[electrostatic potential]] "ϕ" by
where ∇ is the gradient, ∇• is the divergence, ∇ × is the [[Curl (mathematics)|curl]].
The force becomes
and using an identity for the [[Triple product#Vector triple product|triple product]] simplifies to
using the chain rule, the [[total derivative]] of A is:
so the above expression can be rewritten as;
which can take the convenient Euler–Lagrange form
(\phi-\dot{\mathbf{x}}\cdot\mathbf{A})+ \frac{\mathrm{d}}{\mathrm{d}t}\nabla_{\dot{\mathbf{x}}}(\phi-\dot{\mathbf{x}}\cdot\mathbf{A})\right]</math>
Lorentz force and analytical mechanics.
The [[Lagrangian mechanics|Lagrangian]] for a charged particle of mass "m" and charge "q" in an electromagnetic field equivalently describes the dynamics of the particle in terms of its "energy", rather than the force exerted on it. The classical expression is given by:
where A and "ϕ" are the potential fields as above. Using [[Lagrangian mechanics|Lagrange's equations]], the equation for the Lorentz force can be obtained.
The potential energy depends on the velocity of the particle, so the force is velocity dependent, so it is not conservative.
The relativistic Lagrangian is
The action is the relativistic [[arclength]] of the path of the particle in [[space time]], minus the potential energy contribution, plus an extra contribution which [[Quantum Mechanics|quantum mechanically]] is an extra [[phase (waves)|phase]] a charged particle gets when it is moving along a vector potential.
Equation (cgs units).
The above-mentioned formulae use [[SI units]] which are the most common among experimentalists, technicians, and engineers. In [[Gaussian units|cgs-Gaussian units]], which are somewhat more common among theoretical physicists, one has instead
where "c" is the [[speed of light]]. Although this equation looks slightly different, it is completely equivalent, since
one has the following relations:
where ε0 is the [[vacuum permittivity]] and μ0 the [[vacuum permeability]]. In practice, the subscripts "cgs" and "SI" are always omitted, and the unit system has to be assessed from context.
Relativistic form of the Lorentz force.
Covariant form of the Lorentz force.
Field tensor.
Using the [[metric signature]] , The Lorentz force for a charge "q" can be written in [[Lorentz covariance|covariant form]]:
where "pα" is the [[four-momentum]], defined as
"τ" the [[proper time]] of the particle, "Fαβ" the contravariant [[electromagnetic tensor]]
and "U" is the covariant [[four-velocity|4-velocity]] of the particle, defined as:
in which 
is the [[Lorentz factor]].
The fields are transformed to a frame moving with constant relative velocity by:
where Λ"μα" is the [[Lorentz transformation]] tensor.
Translation to vector notation.
The α = 1 component ("x"-component) of the force is
Substituting the components of the covariant electromagnetic tensor "F" yields
Using the components of covariant [[four-velocity]] yields
The calculation for α = 2, 3 (force components in the "y" and "z" directions) yields similar results, so collecting the 3 equations into one:
and since differentials in coordinate time "dt" and proper time "dτ" are related by the Lorentz factor,
so we arrive at
This is precisely the Lorentz force law, however, it is important to note that p is the relativistic expression, 
STA form of the Lorentz force.
The electric and magnetic fields are [[Classical electromagnetism and special relativity|dependent on the velocity of an observer]], so the relativistic form of the Lorentz force law can best be exhibited starting from a coordinate-independent expression for the electromagnetic and magnetic fields, formula_42, and an arbitrary time-direction, formula_43, where
and
formula_46 is a space-time bivector (an oriented plane segment, just like a vector is an oriented line segment), which has six degrees of freedom corresponding to boosts (rotations in space-time planes) and rotations (rotations in space-space planes). The dot product with the vector formula_43 pulls a vector (in the space algebra) from the translational part, while the wedge-product creates a trivector (in the space algebra) who is dual to a vector which is the usual magnetic field vector.
The relativistic velocity is given by the (time-like) changes in a time-position vector formula_48, where
(which shows our choice for the metric) and the velocity is
The proper (invariant is an inadequate term because no transformation has been defined) form of the Lorentz force law is simply
Note that the order is important because between a bivector and a vector the dot product is anti-symmetric. Upon a space time split like one can obtain the velocity, and fields as above yielding the usual expression.
Applications.
The Lorentz force occurs in many devices, including:
In its manifestation as the Laplace force on an electric current in a conductor, this force occurs in many devices including:
References.
The numbered references refer in part to the list immediately below.
External links.
[[Category:Concepts in physics]]
[[Category:Electromagnetism]]
[[Category:Maxwell's equations]]

</doc>
<doc id="230489" url="https://en.wikipedia.org/wiki?curid=230489" title="Lorentz group">
Lorentz group

In physics and mathematics, the Lorentz group is the group of all Lorentz transformations of Minkowski spacetime, the classical setting for all (nongravitational) physical phenomena. The Lorentz group is named for the Dutch physicist Hendrik Lorentz.
Under the Lorentz transformations, these laws and equations are invariant: 
Therefore, the Lorentz group expresses the fundamental symmetry of many known fundamental laws of nature.
Basic properties.
The Lorentz group is a subgroup of the Poincaré group—the group of all isometries of Minkowski spacetime. Lorentz transformations are, precisely, isometries that leave the origin fixed. Thus, the Lorentz group is an isotropy subgroup of the isometry group of Minkowski spacetime. For this reason, the Lorentz group is sometimes called the homogeneous Lorentz group while the Poincaré group is sometimes called the "inhomogeneous Lorentz group". Lorentz transformations are examples of linear transformations; general isometries of Minkowski spacetime are affine transformations.
Mathematically, the Lorentz group may be described as the generalized orthogonal group O(1,3), the matrix Lie group that preserves the quadratic form
on R4. This quadratic form is, when put on matrix form (see classical orthogonal group), interpreted in physics as the metric tensor of Minkowski spacetime.
The Lorentz group is a six-dimensional noncompact non-abelian real Lie group that is not connected. All four of its connected components are not simply connected. The identity component (i.e., the component containing the identity element) of the Lorentz group is itself a group, and is often called the restricted Lorentz group, and is denoted SO+(1,3). The restricted Lorentz group consists of those Lorentz transformations that preserve the orientation of space and direction of time. The restricted Lorentz group has often been presented through a facility of biquaternion algebra.
The restricted Lorentz group arises in other ways in pure mathematics. For example, it arises as the point symmetry group of a certain ordinary differential equation. This fact also has physical significance.
Connected components.
Because it is a Lie group, the Lorentz group O(1,3) is both a group and a smooth manifold. As a manifold, it has four connected components. Intuitively, this means that it consists of four topologically separated pieces.
Each of the four connected components can be categorized by which of these two properties its elements have:
Lorentz transformations that preserve the direction of time are called . The subgroup of orthochronous transformations is often denoted O+(1,3). Those that preserve orientation are called proper, and as linear transformations they have determinant +1. (The improper Lorentz transformations have determinant −1.) The subgroup of proper Lorentz transformations is denoted SO(1,3). 
The subgroup of all Lorentz transformations preserving both orientation and direction of time is called the proper, orthochronous Lorentz group or restricted Lorentz group, and is denoted by SO+(1, 3). (Note that some authors refer to SO(1,3) or even O(1,3) when they actually mean SO+(1, 3).)
The set of the four connected components can be given a group structure as the quotient group O(1,3)/SO+(1,3), which is isomorphic to the Klein four-group. Every element in O(1,3) can be written as the semidirect product of a proper, orthochronous transformation and an element of the discrete group
where "P" and "T" are the space inversion and time reversal operators:
Thus an arbitrary Lorentz transformation can be specified as a proper, orthochronous Lorentz transformation along with a further two 
bits of information, which pick out one of the four connected components. This pattern is typical of finite-dimensional Lie groups.
Restricted Lorentz group.
The restricted Lorentz group is the identity component of the Lorentz group, which means that it consists of all Lorentz transformations that can be connected to the identity by a continuous curve lying in the group. The restricted Lorentz group is a connected normal subgroup of the full Lorentz group with the same dimension, in this case with dimension six.
The restricted Lorentz group is generated by ordinary spatial rotations and Lorentz boosts (which can be thought of as hyperbolic rotations in a plane that includes a time-like direction). Since every proper, orthochronous Lorentz transformation can be written as a product of a rotation (specified by 3 real parameters) and a boost (also specified by 3 real parameters), it takes 6 real parameters to specify an arbitrary proper orthochronous Lorentz transformation. This is one way to understand why the restricted Lorentz group is six-dimensional. (See also the Lie algebra of the Lorentz group.)
The set of all rotations forms a Lie subgroup isomorphic to the ordinary rotation group SO(3). The set of all boosts, however, does "not" form a subgroup, since composing two boosts does not, in general, result in another boost. (Rather, a pair of non-colinear boosts is equivalent to a boost and a rotation, and this relates to Thomas rotation.) A boost in some direction, or a rotation about some axis, generates a one-parameter subgroup.
Surfaces of transitivity.
If a group acts on a space , then a surface is a surface of transitivity if is invariant under , i.e., , and for any two points there is a such that . By definition of the Lorentz group, it preserves the quadratic form 
The surfaces of transitivity of the orthochronous Lorentz group , of spacetime are the following:
These surfaces are , so the images are not faithful, but they are faithful for the corresponding facts about . For the full Lorentz group, the surfaces of transitivity are only four since the transformation takes an upper branch of a hyperboloid (cone) to a lower one and vice versa. 
These observations constitute a good starting point for finding all infinite-dimensional unitary representations of the Lorentz group, in fact, of the Poincaré group, using the method of induced representations. One begins with a "standard vector", one for each surface of transitivity, and then ask which subgroup preserves these vectors. These subgroups are called little groups by physicists. The problem is then essentially reduced to the easier problem of finding representations of the little groups. For example, a standard vector in one of the hyperbolas of two sheets could be suitably chosen as . For each , the vector pierces exactly one sheet. In this case the little group is , the rotation group, all of whose representations are known. The precise infinite-dimensional unitary representation under which a particle transform is part of its classification. Not all representations can correspond to physical particles (as far as is known). Standard vectors on the one-sheeted hyperbolas would correspond to tachyons. Particles on the light cone are photons, and more hypothetically, gravitons. The "particle" corresponding to the origin is the vacuum.
Relation to the Möbius group.
The restricted Lorentz group SO+(1, 3) is isomorphic to the projective special linear group PSL(2,C), which is in turn isomorphic to the Möbius group, the symmetry group of conformal geometry on the Riemann sphere. (This observation was utilized by Roger Penrose as the starting point of twistor theory.)
This may be shown by constructing a surjective homomorphism of Lie groups from SL(2,C) to SO+(1,3), which we will call the spinor map. This proceeds as follows:
We can define an action of SL(2,C) on Minkowski spacetime by writing a point of spacetime as a two-by-two Hermitian matrix in the form
This presentation has the pleasant feature that
Therefore, we have identified the space of Hermitian matrices (which is four-dimensional, as a "real" vector space)
with Minkowski spacetime in such a way that the determinant of a Hermitian matrix is the squared length of the corresponding vector in Minkowski spacetime.
SL(2,C) acts on the space of Hermitian matrices via
where formula_6 is the Hermitian transpose of formula_7, and this action preserves the determinant. Therefore, SL(2,C) acts on Minkowski spacetime by (linear) isometries, and so is homomorphic to a subgroup of the Lorentz group (by the definition of the Lorentz group.)
This completes the proof that there is a homomorphism from SL(2,C) to SO+(1,3). The kernel of the spinor map is the two element subgroup ±"I", and it happens that the map is surjective. By the first isomorphism theorem, the quotient group PSL(2,C) is isomorphic to SO+(1,3).
In optics, this construction is known as the Poincaré sphere.
Appearance of the night sky.
This isomorphism has the consequence that Möbius transformations of the Riemann sphere represent the way that Lorentz transformations change the appearance of the night sky, as seen by an observer who is maneuvering at relativistic velocities relative to the "fixed stars". 
Suppose the "fixed stars" live in Minkowski spacetime and are modeled by points on the celestial sphere. Then a given point on the celestial sphere can be associated with , a complex number that corresponds to the point on the Riemann sphere, and can be identified with a null vector (a light-like vector) in Minkowski space
or the Hermitian matrix
The set of real scalar multiples of this null vector, called a "null line" through the origin, represents a "line of sight" from an observer at a particular place and time (an arbitrary event we can identify with the origin of Minkowski spacetime) to various distant objects, such as stars. Then the points of the celestial sphere (equivalently, lines of sight) are identified with certain Hermitian matrices.
Conjugacy classes.
Because the restricted Lorentz group SO+(1, 3) is isomorphic to the Möbius group PSL(2,C), its conjugacy classes also fall into five classes:
In the article on Möbius transformations, it is explained how this classification arises by considering the fixed points of Möbius transformations in their action on the Riemann sphere, which corresponds here to null eigenspaces of restricted Lorentz transformations in their action on Minkowski spacetime.
An example of each type is given in the subsections below, along with the effect of the one-parameter subgroup it generates (e.g., on the appearance of the night sky).
The Möbius transformations are the conformal transformations of the Riemann sphere (or celestial sphere). Then conjugating with an arbitrary element of SL(2,C) obtains the following examples of arbitrary elliptic, hyperbolic, loxodromic, and parabolic (restricted) Lorentz transformations, respectively. The effect on the flow lines of the corresponding one-parameter subgroups is to transform the pattern seen in the examples by some conformal transformation. For example, an elliptic Lorentz transformation can have any two distinct fixed points on the celestial sphere, but points still flow along circular arcs from one fixed point toward the other. The other cases are similar.
Elliptic.
An elliptic element of SL(2,C) is
and has fixed points = 0, ∞. Writing the action as and collecting terms, the spinor map converts this to the (restricted) Lorentz transformation
This transformation then represents a rotation about the axis, exp(). The one-parameter subgroup it generates is obtained by taking to be a real variable, the rotation angle, instead of a constant. 
The corresponding continuous transformations of the celestial sphere (except for the identity) all share the same two fixed points, the North and South poles. The transformations move all other points around latitude circles so that this group yields a continuous counterclockwise rotation about the axis as increases. The "angle doubling" evident in the spinor map is a characteristic feature of "spinorial double coverings".
Hyperbolic.
A hyperbolic element of SL(2,C) is
and has fixed points = 0, ∞. Under stereographic projection from the Riemann sphere to the Euclidean plane, the effect of this Möbius transformation is a dilation from the origin. 
The spinor map converts this to the Lorentz transformation
This transformation represents a boost along the axis with rapidity . The one-parameter subgroup it generates is obtained by taking to be a real variable, instead of a constant. The corresponding continuous transformations of the celestial sphere (except for the identity) all share the same fixed points (the North and South poles), and they move all other points along longitudes away from the South pole and toward the North pole.
Loxodromic.
A loxodromic element of SL(2,C) is
and has fixed points = 0, ∞. The spinor map converts this to the Lorentz transformation
The one-parameter subgroup this generates is obtained by replacing "β+iθ" with any real multiple of this complex constant. (If "β, θ" vary independently, then a "two-dimensional" abelian subgroup is obtained, consisting of simultaneous rotations about the axis and boosts along the -axis; in contrast, the "one-dimensional" subgroup discussed here consists of those elements of this two-dimensional subgroup such that the rapidity of the boost and angle of the rotation have a "fixed ratio".) 
The corresponding continuous transformations of the celestial sphere (excepting the identity) all share the same two fixed points (the North and South poles). They move all other points away from the South pole and toward the North pole (or vice versa), along a family of curves called loxodromes. Each loxodrome spirals infinitely often around each pole.
Parabolic.
A parabolic element of SL(2,C) is
and has the single fixed point = ∞ on the Riemann sphere. Under stereographic projection, it appears as an ordinary translation along the real axis. 
The spinor map converts this to the matrix (representing a Lorentz transformation)
This generates a two-parameter abelian subgroup, which is obtained by considering a complex variable rather than a constant. The corresponding continuous transformations of the celestial sphere (except for the identity transformation) move points along a family of circles that are all tangent at the North pole to a certain great circle. All points other than the North pole itself move along these circles. 
Parabolic Lorentz transformations are often called null rotations, since they preserve null vectors, just as rotations preserve timelike vectors and boosts preserve spacelike vectors. Since these are likely to be the least familiar of the four types of nonidentity Lorentz transformations (elliptic, hyperbolic, loxodromic, parabolic), it is illustrated here how to determine the effect of an example of a parabolic Lorentz transformation on Minkowski spacetime. 
The matrix given above yields the transformation
Now, without loss of generality, pick =0. Differentiating this transformation with respect to the now real group parameter and evaluating at "α"=0 produces the corresponding vector field (first order linear partial differential operator),
Apply this to a function , and demand that it stays invariant, i.e., it is annihilated by this transformation. The solution of the resulting first order linear partial differential equation can be expressed in the form
where is an "arbitrary" smooth function. The arguments of give three "rational invariants" describing how points (events) move under this parabolic transformation, as they themselves do not move,
Choosing real values for the constants on the right hand sides yields three conditions, and thus specifies a curve in Minkowski spacetime. This curve is an orbit of the transformation. 
The form of the rational invariants shows that these flowlines (orbits) have a simple description: suppressing the inessential coordinate , each orbit is the intersection of a "null plane", , with a "hyperboloid", . The case 3 = 0 has the hyperboloid degenerate to a light cone with the orbits becoming parabolas lying in corresponding null planes.
A particular null line lying on the light cone is left "invariant"; this corresponds to the unique (double) fixed point on the Riemann sphere mentioned above. The other null lines through the origin are "swung around the cone" by the transformation. Following the motion of one such null line as increases corresponds to following the motion of a point along one of the circular flow lines on the celestial sphere, as described above.
A choice =0 instead, produces similar orbits, now with the roles of and interchanged.
Parabolic transformations lead to the gauge symmetry of massless particles (like photons) with helicity || ≥ 1. In the above explicit example, a massless particle moving in the direction, so with 4-momentum P=("p",0,0,"p"), is not affected at all by the -boost and -rotation combination displayed above, in the "little group" of its motion. This is evident from the explicit transformation law discussed: like any light-like vector, P itself is now invariant, i.e., all traces or effects of have disappeared. 1 = 2 = 3 = 0, in the special case discussed. (The other similar generator, as well as it and "z" comprise altogether the little group of the lightlike vector, isomorphic to (2).)
Lie algebra.
As with any Lie group, the best way to study many aspects of the Lorentz group is via its Lie algebra. The Lorentz group is a subgroup of the diffeomorphism group of R4 and therefore its Lie algebra can be identified with vector fields on R4. In particular, the vectors that generate isometries on a space are its Killing vectors, which provides a convenient alternative to the left-invariant vector field for calculating the Lie algebra. We can write down a set of six generators:
It may be helpful to briefly recall here how to obtain a one-parameter group from a vector field, written in the form of a first order linear partial differential operator such as
The corresponding initial value problem is
The solution can be written
or
where we easily recognize the one-parameter matrix group of rotations exp("i λ Jz") about the z axis.
Differentiating with respect to the group parameter and setting it "λ"=0 in that result, we recover the standard matrix,
which corresponds to the vector field we started with. This illustrates how to pass between matrix and vector field representations of elements of the Lie algebra.
Reversing the procedure in the previous section, we see that the Möbius transformations that correspond to our six generators arise from exponentiating respectively "β"/2 (for the three boosts) or "iθ"/2 (for the three rotations) times the three Pauli matrices
For our purposes, another generating set is more convenient. The following table lists the six generators, in which
Notice that the generators consist of
Let's verify one line in this table. Start with
Exponentiate:
This element of SL(2,C) represents the one-parameter subgroup of (elliptic) Möbius transformations:
Next,
The corresponding vector field on C (thought of as the image of S2 under stereographic projection) is
Writing formula_36, this becomes the vector field on R2
Returning to our element of SL(2,"C"), writing out the action formula_5 and collecting terms, we find that the image under the spinor map is the element of SO+(1,3)
Differentiating with respect to at =0, yields the corresponding vector field on R4, 
This is evidently the generator of counterclockwise rotation about the axis.
Subgroups of the Lorentz group.
The subalgebras of the Lie algebra of the Lorentz group can be enumerated, up to conjugacy, from which we can list the closed subgroups of the restricted Lorentz group, up to conjugacy. (See the book by Hall cited below for the details.) We can readily express the result in terms of the generating set given in the table above.
The one-dimensional subalgebras of course correspond to the four conjugacy classes of elements of the Lorentz group:
The two-dimensional subalgebras are:
The three-dimensional subalgebras are:
The four-dimensional subalgebras are all conjugate to
The subalgebras form a lattice (see the figure), and each subalgebra generates by exponentiation a closed subgroup of the restricted Lie group. From these, all subgroups of the Lorentz group can be constructed, up to conjugation, by multiplying by one of the elements of the Klein four-group.
As with any connected Lie group, the coset spaces of the closed subgroups of the restricted Lorentz group, or homogeneous spaces, have considerable mathematical interest. A few, brief descriptions:
Covering groups.
In a previous section, we constructed a homomorphism , which we called the spinor map. Since is simply connected, it is the covering group of the restricted Lorentz group . By restriction we obtain a homomorphism . Here, the special unitary group SU(2), which is isomorphic to the group of unit norm quaternions, is also simply connected, so it is the covering group of the rotation group SO(3). Each of these covering maps are twofold covers in the sense that precisely two elements of the covering group map to each element of the quotient. One often says that the restricted Lorentz group and the rotation group are doubly connected. This means that the fundamental group of the each group is isomorphic to the two-element cyclic group Z2.
Twofold coverings are characteristic of spin groups. Indeed, in addition to the double coverings
we have the double coverings
These spinorial double coverings are all closely related to Clifford algebras.
Topology.
The left and right groups in the double covering
are deformation retracts of the left and right groups, respectively, in the double covering
But the homogeneous space SO+(1,3)/SO(3) is homeomorphic to hyperbolic 3-space H3, so we have exhibited the restricted Lorentz group as a principal fiber bundle with fibers SO(3) and base H3. Since the latter is homeomorphic to R3, while SO(3) is homeomorphic to three-dimensional real projective space RP3, we see that the restricted Lorentz group is "locally" homeomorphic to the product of RP3 with R3. Since the base space is contractible, this can be extended to a global homeomorphism.
Generalization to higher dimensions.
The concept of the Lorentz group has a natural generalization to spacetime of any number of dimensions. Mathematically, the Lorentz group of "n"+1-dimensional Minkowski space is the group O("n",1) (or O(1,"n")) of linear transformations of R"n"+1 that preserves the quadratic form
Many of the properties of the Lorentz group in four dimensions (where ) generalize straightforwardly to arbitrary "n". For instance, the Lorentz group O("n",1) has four connected components, and it acts by conformal transformations on the celestial ("n"−1)-sphere in "n"+1-dimensional Minkowski space. The identity component SO+("n",1) is an SO("n")-bundle over hyperbolic "n"-space H"n".
The low-dimensional cases and are often useful as "toy models" for the physical case , while higher-dimensional Lorentz groups are used in physical theories such as string theory that posit the existence of hidden dimensions. The Lorentz group O("n",1) is also the isometry group of "n"-dimensional de Sitter space dS"n", which may be realized as the homogeneous space O("n",1)/O("n"−1,1). In particular O(4,1) is the isometry group of the de Sitter universe dS4, a cosmological model.

</doc>
<doc id="18404" url="https://en.wikipedia.org/wiki?curid=18404" title="Lorentz transformation">
Lorentz transformation

In physics, the Lorentz transformation (or transformations) are coordinate transformations between two coordinate frames that move at constant velocity relative to each other. 
Frames of reference can be divided into two groups, inertial (relative motion with constant velocity) and non-inertial (accelerating in curved paths, rotational motion with constant angular velocity, etc.). The term "Lorentz transformations" only refers to transformations between "inertial" frames, usually in the context of special relativity.
In each reference frame, an observer can use a local coordinate system (most exclusively Cartesian coordinates in this context) to measure lengths, and a clock to measure time intervals. An observer is a real or imaginary entity that can take measurements, say humans, or any other living organism—or even robots and computers. An event is something that happens at a point in space at an instant of time, or more formally a point in spacetime. The transformations connect the space and time coordinates of an event as measured by an observer in each frame.
They supersede the Galilean transformation of Newtonian physics, which assumes an absolute space and time (see Galilean relativity). The Galilean transformation is a good approximation only at relative speeds much smaller than the speed of light. Lorentz transformations have a number of unintuitive features that do not appear in Galilean transformations. For example, they reflect the fact that observers moving at different velocities may measure different distances, elapsed times, and even different orderings of events, but always such that the speed of light is the same in all inertial reference frames. The invariance of light speed is one of the postulates of special relativity.
Historically, the transformations were the result of attempts by Lorentz and others to explain how the speed of light was observed to be independent of the reference frame, and to understand the symmetries of the laws of electromagnetism. The Lorentz transformation is in accordance with special relativity, but was derived before special relativity. The transformations are named after the Dutch physicist Hendrik Lorentz.
The Lorentz transformation is a linear transformation. It may include a rotation of space; a rotation-free Lorentz transformation is called a Lorentz boost. In Minkowski space, the mathematical model of spacetime in special relativity, the Lorentz transformations preserve the spacetime interval between any two events. This property is the defining property of a Lorentz transformation. They describe only the transformations in which the spacetime event at the origin is left fixed. They can be considered as a hyperbolic rotation of Minkowski space. The more general set of transformations that also includes translations is known as the Poincaré group.
History.
Many physicists—including Woldemar Voigt, George FitzGerald, Joseph Larmor, and Hendrik Lorentz himself—had been discussing the physics implied by these equations since 1887. Early in 1889, Oliver Heaviside had shown from Maxwell's equations that the electric field surrounding a spherical distribution of charge should cease to have spherical symmetry once the charge is in motion relative to the ether. FitzGerald then conjectured that Heaviside’s distortion result might be applied to a theory of intermolecular forces. Some months later, FitzGerald published the conjecture that bodies in motion are being contracted, in order to explain the baffling outcome of the 1887 ether-wind experiment of Michelson and Morley. In 1892, Lorentz independently presented the same idea in a more detailed manner, which was subsequently called FitzGerald–Lorentz contraction hypothesis. Their explanation was widely known before 1905.
Lorentz (1892–1904) and Larmor (1897–1900), who believed the luminiferous ether hypothesis, also looked for the transformation under which Maxwell's equations are invariant when transformed from the ether to a moving frame. They extended the FitzGerald–Lorentz contraction hypothesis and found out that the time coordinate has to be modified as well ("local time"). Henri Poincaré gave a physical interpretation to local time (to first order in v/c) as the consequence of clock synchronization, under the assumption that the speed of light is constant in moving frames. Larmor is credited to have been the first to understand the crucial time dilation property inherent in his equations.
In 1905, Poincaré was the first to recognize that the transformation has the properties of a mathematical group,
and named it after Lorentz.
Later in the same year Albert Einstein published what is now called special relativity, by deriving the Lorentz transformation under the assumptions of the principle of relativity and the constancy of the speed of light in any inertial reference frame, and by abandoning the mechanical aether.
Derivation.
An "event" is something that happens at a certain point in spacetime, or more generally, the point in spacetime itself. In any inertial frame an event is specified by a time coordinate "t" and a set of Cartesian coordinates to specify position in space in that frame. Subscripts label individual events.
From Einstein's second postulate of relativity follows immediately
in all inertial frames for events connected by "light signals". The quantity on the left is called the "spacetime interval" between events and . The interval between "any two" events, not necessarily separated by light signals, is in fact invariant, i.e., independent of the state of relative motion of observers in different inertial frames, as is shown here (where one can also find several more explicit derivations than presently given). The transformation sought after thus must possess the property that
where are the spacetime coordinates used to define events in one frame, and are the coordinates in another frame. Now one observes that a "linear" solution to the simpler problem
solves the general problem too. Finding the solution to the simpler problem is just a matter of look-up in the theory of classical groups that preserve bilinear forms of various signature. The Lorentz transformation is thus an element of the group O(3, 1) or, for those that prefer the other metric signature, .
Generalities.
The relations between the primed and unprimed spacetime coordinates are the Lorentz transformations, each coordinate in one frame is a linear function of all the coordinates in the other frame, and the inverse functions are the inverse transformation. Depending on how the frames move relative to each other, and how they are oriented in space relative to each other, other parameters that describe direction, speed, and orientation enter the transformation equations. 
Transformations describing relative motion with constant (uniform) velocity and without rotation of the space coordinate axes are called a "boosts", and the relative velocity between the frames is the parameter of the transformation. The other basic type of Lorentz transformations is rotations in the spatial coordinates only, these are also inertial frames since there is no relative motion, the frames are simply tilted (and not continuously rotating), and in this case quantities defining the rotation are the parameters of the transformation (e.g., axis–angle representation, or Euler angles, etc.). A combination of a rotation and boost is a "homogenous transformation", which transforms the origin back to the origin. 
Boosts should not be conflated with mere displacements in spacetime: in this case, the coordinate systems are simply shifted and there is no relative motion. However, these also count as Lorentz transformations since they leave the spacetime interval invariant. A combination of a rotation with a boost, followed by a shift in spacetime, is an "inhomogenous transformation".
Boosts.
Coordinate transformation.
A "stationary" observer in frame defines events with coordinates . Another frame moves with velocity relative to , and an observer in this "moving" frame defines events using the coordinates .
The coordinate axes in each frame are parallel (the and axes are parallel, the and axes are parallel, and the and axes are parallel), remain mutually perpendicular, and relative motion is along the coincident axes. At , the origins of both coordinate systems are the same, . In other words, the times and positions are coincident at this event. If all these hold, then the coordinate systems are said to be in standard configuration, or synchronized.
What is the conversion between these coordinate systems? If an observer in records an event , then an observer in records the "same" event with coordinates
where is the relative velocity between frames in the -direction, is the speed of light, and
(lowercase gamma) is the Lorentz factor.
Here, is the "parameter" of the transformation, for a given boost it is a constant number, but in general can take a continuous range of values. In the setup used here, positive relative velocity is motion along the standard setup (positive directions of the axes), zero relative velocity is no relative motion, while negative relative velocity is relative motion in the opposite direction to the standard configuration, i.e., along the negative directions of the axes. The magnitude of relative velocity cannot equal or exceed , in other words only subluminal speeds are allowed. The corresponding range of is . The transformations are well-defined if these ranges hold.
The transformations are not defined if is outside these limits. At luminal speed () is infinite, and for superluminal speeds () is a complex number, each of which make the transformations unphysical. The space and time coordinates are measurable quantities and numerically must be real numbers, not complex.
A critical requirement of the Lorentz transformations is the invariance of the speed of light, a fact used in their derivation, and contained in the transformations themselves. If in the equation for a pulse of light along the direction is , then in the Lorentz transformations give , and vice versa, for any . 
Another important property is for relative speeds much less than the speed of light, the Lorentz transformations reduce to the Galilean transformation in accordance with the correspondence principle. Mathematically, as , so that
In words, as relative velocity approaches 0, the speed of light (seems to) approach infinity. Hence, it is sometimes said that nonrelativistic physics is a physics of "instantaneous action at a distance".
The Lorentz transformation can be viewed as an active and passive transformation. An observer in F′ notices the coordinates of the event to be "boosted" in the negative directions of the axes, because of the in the transformations (an active transformation). This has the equivalent effect of the "coordinate system" F′ boosted in the positive directions of the axes, while the event does not change and is simply represented in another coordinate system. Here the Lorentz transformations are employed as passive transformations.
The inverse relations ( in terms of ) can be found by algebraically solving the original set of equations, but it's very tedious. A much more efficient way is to use physical principles. Here is the "stationary" frame while is the "moving" frame. According to the principle of relativity, there is no privileged frame of reference, so the transformations from to must take exactly the same form as the transformations from to . The only difference is moves with velocity relative to (i.e., the relative velocity has the same magnitude but is oppositely directed). Thus if an observer in notes an event , then an observer in notes the "same" event with coordinates
and the value of remains unchanged. This "trick" of simply reversing the direction of relative velocity while preserving its magnitude, and exchanging primed and unprimed variables, always applies to finding the inverse transformation of every boost in any direction.
The above Lorentz transformations apply to "one event". If there are two events, there is a spatial separation and time interval between them. It follows from the linearity of the Lorentz transformations that two values of space and time coordinates can be chosen, the Lorentz transformations can be applied to each, then subtracted to get the Lorentz transformations of the differences;
with inverse relations
where (capital Delta) indicates a difference of quantities, e.g., for two values of coordinates, and so on.
These transformations on "differences" rather than spatial points or instants of time are useful for a number of reasons:
Two unintuitive, but correct, predictions of the transformations are:
Sometimes it is more convenient to use (lowercase beta) instead of , so that
which shows clearer the symmetry in the transformation. From the allowed ranges of and the definition of , it follows . The use of and is standard throughout the literature.
Vector transformations.
Only time and the coordinates parallel to the direction of relative motion change, while those coordinates perpendicular do not. With this in mind, split the spatial position vector as measured in , and as measured in , each into components perpendicular (⊥) and parallel ( ‖ ) to ,
The transition from the boost in any of the Cartesian directions, say the direction, to a boost in any direction can be made from the identifications
where are the Cartesian basis vectors, a set of mutually perpendicular unit vectors along their indicated directions. Then using the dot product the Lorentz transformations take the form
The Lorentz factor retains its definition for a boost in any direction, since it depends only on the magnitude of the relative velocity. The definition with magnitude is also used by some authors.
These transformations are vector equations and therefore true in "any" direction. The transformations between the entire position vectors and for a boost in any direction can be constructed from these also. A single boost in any direction depends on the full relative velocity vector with a magnitude that cannot equal or exceed , so that . An observer in frame observes to move with relative velocity . The coordinate axes of each frame are still parallel and orthogonal. Here the frames are still in standard configuration.
Introducing , a unit vector in the direction of relative motion, the parallel component can be found by vector projection into the direction of relative motion
and the perpendicular component by vector rejection from the direction of relative motion
The procedure for is identical. The unit vector has the advantage of simplifying equations for a single boost, allows either or to be reinstated when convenient, and makes alternative parametrizations easier. It is not convenient for multiple boosts. The relative velocity is with magnitude and direction . Combining the results gives
For the inverse transformations, exchange primed for unprimed coordinates to switch coordinate frames, and since an observer in observes to move with relative velocity , negate the relative velocity (or simply the unit vector since the magnitude is always positive) to obtain
Matrix transformations.
Since the Lorentz transformations are a linear transformation, they can be written in a single matrix equation (see matrix product for the multiplication of these matrices). The separate algebraic equations are often used in practical calculations, but for theoretical purposes it is useful to collect all the separate equations into one matrix equation;
The boost matrix is a symmetric matrix, it equals its transpose. Abbreviating the column vector of spacetime coordinates in F by , likewise for those of F′ by , and the boost matrix by , the boost is 
In the inverse transformations the transformation matrix is the matrix inverse of the original transformation. Instead of explicitly calculating the inverse matrix by brute force, the simple change suffices, and the inverse transformation is
The boosts along the Cartesian directions can be readily obtained, for example the unit vector in the x direction has components and . 
Looking at the patterns in the boost matrices along the Cartesian directions, the general boost matrix can be systematically rewritten by introducing
Collecting these into a vector of matrices , the matrix and its square allow the compact expression
The matrices make one or more successive transformations easier to handle, rather than rotely iterating the transformations to obtain the result of more than one transformation, examples of this procedure are given later. For two or more consecutive boosts that are not collinear but in different directions, the result is still a Lorentz transformation, but not a single boost. Moreover, Lorentz boosts along different directions do not commute, changing their order changes the resultant transformation. 
The non-commutativity of Lorentz boosts is another unintuitive feature of special relativity that is unlike Galilean relativity. In Newtonian mechanics any pair of Galilean boosts can be performed in either order, and the result is the same Galilean transformation. The general problem of two boosts each in any direction is outlined later.
Rapidity parametrization.
The Lorentz transformations can also be derived in a way that resembles circular rotations in 3d space using the hyperbolic functions. For the boost in the direction, the results are
where (lowercase zeta) is a parameter called "rapidity" (many other symbols are used, including ). Given the strong resemblance to rotations of spatial coordinates in 3d space in the Cartesian xy, yz, and zx planes, the Lorentz transformation can be thought of as a hyperbolic rotation of spacetime coordinates in 4d Minkowski space in the Cartesian-time planes, here the plane (boosts in the other directions would be hyperbolic rotations in the yt or zt planes). The parameter represents the hyperbolic angle of rotation, analogous to the ordinary angle for circular rotations. This transformation can be illustrated with a Minkowski diagram.
The hyperbolic functions arise from the "difference" between the squares of the time and spatial coordinates in the equation for a light pulse, according to the identity 
Using the definition
a consequence these two hyperbolic formulae is an identity that matches the Lorentz factor
Comparing the Lorentz transformations in terms of the relative velocity and rapidity, or using the above formulae, the connections between , , and are
Taking the inverse hyperbolic tangent gives the rapidity
Since , it follows . From the relation between and , positive rapidity is motion according to the standard setup (along the positive directions of the axes), zero rapidity is no relative motion, while negative rapidity is relative motion in the opposite direction (along the negative directions of the axes).
The geometric significance of the hyperbolic functions can be visualized by taking or in the transformations, see diagram (the coordinates are suppressed). Squaring and subtracting the results, one can derive hyperbolic curves of constant coordinate values but varying , which parametrizes the curves. Conversely the and axes can be constructed for varying coordinates but constant . 
Exchanging primed and unprimed quantities to switch the coordinate frames and observers, and negating rapidity is equivalent to negating the relative velocity as the motion is in the opposite direction. Therefore,
The inverse transformations can be similarly visualized by considering the cases when and .
The rapidity relations and can be substituted into all the velocity parametrized formulae above. The following are additional details.
Rapidities can be added to obtain the overall rapidity, unlike relative velocities. If a frame is boosted with rapidity relative to frame along the axes, and another frame is boosted with rapidity relative to along the axes, so that (suppressing the irrelevant coordinates)
then is the rapidity of the overall boost of relative to ,
and the relative velocities are related to the rapidities by
This holds if the boosts are collinear as they are here (not just along the common directions of each frame, but any direction). The relative velocities can be in the same or opposite directions, but must be collinear. Moreover, the hyperbolic identity
coincides with the resultant relative velocity of the two relative velocities along the same direction.
Using the same unit vector , the vectorial relation between relative velocity and rapidity is
and the "rapidity vector" can be defined as
each of which serves as a useful abbreviation in some contexts. The magnitude of is the absolute value of the rapidity scalar confined to , which agrees with the range . 
Transformation of other physical quantities.
Transformation of velocities.
So far only the Lorentz transformation of position and time coordinates have been given. Since the concept of a Lorentz transformation depends on specifying inertial frames, and the constant relative velocity between any two, it is instructive to look at the transformation of velocities between inertial frames.
For the boost in the x direction, the differentials in the coordinates and time are
and defining the Cartesian components of the coordinate velocity as measured in F by
the corresponding definitions in F′ are found by dividing the coordinate differentials by the time differentials
This is the Lorentz transformation of the velocity components for an x-boost only. The Lorentz factor applies to the relative velocity between the frames, not the coordinate velocities. The inverse transformations follow by negating and exchange the primed and unprimed quantities, just as for the coordinate transformations. 
The full 3d result can be similarly obtained by defining
and repeating the process of taking differentials and dividing equations leads to
Negating and exchanging the primed and unprimed vectors gives the inverse transformation. In the nonrelativistic limit , , and the Galilean transformations of velocity are recovered
Only in the non-relativistic limit can we use ordinary vector addition of two velocities to obtain the resultant velocity. The general rule for combining velocities in relativistic mechanics can be inferred from the inverse transformation. The velocities and are the velocity of some massive object, or a third inertial frame (say F′′) provided for this case they are "constant". Denote either entity by X. Then X moves with velocity relative to F, or equivalently with velocity relative to F′, in turn F′ moves with velocity relative to F. This can motivate the operation and notation for the relativistic velocity addition formula of two 3-velocites using the inverse transformation
The "inverse transformation" is used since this gives the desired velocity (that of X relative to F), and correctly reduces to the Galilean "sum" of velocities for speeds much less than . If the original transformation was used, a difference in velocities would be obtained and we would have instead. Although the relativistic velocity addition has the unfortunate properties of being nonlinear, non-commutative, and non-associative, it does correctly obtain a velocity with a magnitude less than . If ordinary vector addition was used, then it would be possible to obtain a velocity with magnitude larger than , which is unphysical for Lorentz transformations.
If X is a third frame F′′ and all three are in standard configuration, then the trajectory of F′′ must pass through the origins of F and F′ at the initial time as measured in all of these frames (which synchronizes them), i.e., at . 
Applications of relativistic velocity addition include stellar aberration, the Fizeau experiment, and the relativistic Doppler effect.
The Lorentz transformations of acceleration can be obtained by taking differentials in the velocity components and dividing these by the time differential. 
Transformation of energy, momentum, and angular momentum.
The transformation of dynamic quantities, such as momentum, energy, angular momentum, can be obtained using the invariant spacetime interval and conservation laws. In particular, for a boost in the x direction, the transformation of energy and momentum combine as follows;
which takes an almost identical form to the transformation of spacetime coordinates, where are replaced by . These transformations leave the quantity 
Length contraction has an effect on charge density and current density , and time dilation has an effect on the rate of flow of charge (current), so charge and current distributions must transform somehow under a boost. The Lorentz transformations are
Maxwell's equations are invariant under these Lorentz transformations. 
Spatial rotations.
A rotation on the spatial coordinates only, leaving the time coordinate alone, leaves the spacetime interval invariant. Therefore spatial rotations are also Lorentz transformations. The 4d matrix is simply
where is a 3d rotation matrix. For the purposes of this article the axis-angle representation will be used here, and the "axis-angle vector" is a useful definition; the angle multiplied by a unit vector parallel to the axis. The inverse of corresponds to rotations using the same axis and angle, but in the opposite sense. The rotation matrix is orthogonal, so the transpose equals the inverse;
Looking at the patterns in the rotation matrices about the Cartesian axes, it is useful to introduce the matrices
Collecting these into a vector , these matrices allow the 4d rotation matrix to be written
In this article the "right-handed" convention for the spatial coordinates is used (see orientation (vector space)), so that rotations are positive in the anticlockwise sense according to the right-hand rule, and negative in the clockwise sense. This matrix rotates any 3d vector about the axis through angle anticlockwise (an active transformation), which has the equivalent effect of rotating the coordinate frame clockwise about the same axis through the same angle (a passive transformation).
The general Lorentz transformation.
So far, the Lorentz transformations have been restricted to an individual boost between two frames whose , , and axis are parallel and whose spacetime origins coincide, and rotations on the spatial coordinates are also allowed. The most general proper Lorentz transformation will be outlined below. 
The calculations can be simplified by compressing everything into block matrix form, 
in which are arranged into column vectors (the matrix transpose of these are row vectors).
Two general boosts.
Suppose again there are three frames all in standard configuration. If F′′ is boosted with velocity relative to F', and F' boosted with velocity relative to F, then the separate boosts are
and the transformation from F to F′′ is
where the general transformation is
and in turn, the Lorentz factor of the composite transformation is determined from the individual velocities and their Lorentz factors,
while the two possible composite relative velocities are 3d row or column vectors,
The 3×3 matrix 
has little physical interpretation. The magnitudes of and are equal
but clearly the vectors are different, so the directions are different, suggesting one is a rotated copy of the other. 
For the inverse transformation, F is boosted with velocity − relative to F′, and F′ boosted with velocity − relative to F′′, so that
Combining these 
and carrying out the block matrix multiplication, using the previous abbreviations the result is
For a single boost between two inertial frames, there is only one unambiguous relative velocity (or its negative). For two boosts, the peculiar result of "two" inequivalent relative velocities instead of one raises the question - which is the correct relative velocity between F and F′′ ? This seems to contradict the symmetry of relative motion between any two frames. Also, the overall Lorentz transformation cannot be a single boost alone as the matrix is not symmetric, and the next simplest assumption is that it equals a boost followed or preceded by a rotation.
Altogether, this means two boosts equals a boost combined with a rotation. It turns out this equality is correct: the rotation of frames matches the angular separation of the composite velocities, and explains how one composite velocity applies to one frame, while the other applies to the rotated frame. The rotation also breaks the symmetry in the overall Lorentz transformation making it nonsymmetric. In addition, the rotation gives rise to Thomas precession. 
For this specific rotation, let the angle be and the axis be defined by the unit vector .
Boost followed by a rotation.
Split the Lorentz transformation into a boost followed by a rotation,
and it follows the composite velocity , the velocity of F′′ relative to F, is found from ,
Is the velocity of F relative to F′′ simply the negative of ? The answer is no.
To see this, look at the inverse configuration; the frame F is rotated anticlockwise through the same angle and axis. The inverse decomposition is thus a boost (the exact nature to be determined, owing to the complication of relativistic velocity addition) followed by an inverse rotation 
so this time the composite velocity , the velocity of F relative to F′′, is this time found from ,
Overall, an observer in F notices F′′ to move with velocity and F′ to move with velocity . An observer in F′′ notices F to move with velocity and F′ to move with velocity . An observer in F′ notices F to move with velocity and F′′ to move with velocity .
Rotation followed by a boost.
It is also possible to decompose the Lorentz transformation into a rotation followed by a boost,
so the composite velocity , the velocity of F′′ relative to F, is this time found from ,
The inverse transformation follows a similar decomposition procedure,
and the composite velocity , the velocity of F relative to F′′, is found from ,
The key effect in exchanging the rotation and boost is the separate boost velocities are exchanged in the final composite velocity. This time, an observer in F notices F′′ to move with velocity and F′ to move with velocity . An observer in F′′ notices F to move with velocity and F′ to move with velocity . An observer in F′ notices F to move with velocity and F′′ to move with velocity .
Thomas rotation.
The above formulae constitute the relativistic velocity addition and the Thomas rotation explicitly in the general Lorentz transformations. Throughout, the important formula
holds, allowing the rotation matrix to be defined completely in terms of the relative velocities and . The angle of a rotation matrix in the axis–angle representation can be found from the trace of the rotation matrix, the general result for "any" axis is . Taking the trace of the equation gives
The angle between and is "not" the same as the angle between and . 
Also for all cases, 
so the vectors and are indeed related by a rotation, in fact by the same rotation matrix which rotates the coordinate frames. Starting from , the matrix rotates this into anticlockwise, it follows their cross product (in the right-hand convention)
defines the axis correctly, therefore the axis is also parallel to , geometrically this is perpendicular to the plane of the boost velocities. Since the magnitude of is neither interesting nor important, only the direction is, it is customary to normalize the vector into the unit vector above thus
which still completely defines the direction of the axis without loss of information. 
The rotation is simply a "static" rotation and there is no relative rotational motion between the frames, there is relative translational motion in the boost. However, if the frames accelerate, then the rotated frame rotates with an angular velocity. This effect is known as the Thomas precession, and arises purely from the kinematics of successive Lorentz boosts.
Introduction to the Lorentz group.
The Lorentz transformations form a "group". The operation of composition can be defined (here matrix multiplication). Boosts and rotations themselves are Lorentz transformations, and the product of any two is also a Lorentz transformation, so the set of these matrices is closed under this operation of composition. Specifically, two pure rotations (without boosts) are a rotation, but two pure boosts (without rotations) is not a single boost, but a boost and rotation combined. The identity element is the 4×4 identity matrix, which has the effect of not changing the coordinates, and the boost and rotation matrices reduce to this for no relative motion and/or no rotation,
The inverse group elements are easily obtained. For three or more transformations, the composition is associative because matrix multiplication is also associative. 
It is a result of special relativity that the following quantity, the spacetime interval "s"2, is an invariant,
where is the Minkowski metric as a square matrix and a column vector of the spacetime coordinates,
See invariance of interval. For this to hold, it can be shown that it is necessary and sufficient for the coordinate transformation to be of the form of a boost and/or rotation, plus an additional displacement in spacetime. These additional displacements are considered later.
The interval above is a quadratic form of signature (3,1). The boost and rotation matrices have determinant +1 and are continuous transformations on the spacetime coordinates that leave the interval invariant, making them elements of the Lie group SO(3,1). As presented in this article, any Lie groups mentioned are matrix Lie groups.
Other Lorentz transformations may have a determinant of opposite sign and other properties, for example any of the boosts and/or rotations, combined with parity inversion and/or time reversal, will also leave the above quadratic form invariant. Therefore SO(3,1) is not the full Lorentz group, but an important yet restricted part of it. The general cases are outlined later.
Generators and parameters of the Lorentz group.
The axis-angle vector and rapidity vector are altogether six continuous variables which make up the group parameters, and and are the six group generators.
Physically, the generators of the Lorentz group are operators that correspond to important symmetries in spacetime: are the "rotation generators" which correspond to angular momentum, and are the "boost generators" which correspond to the motion of the system in spacetime. 
Lorentz generators can be added together, or multiplied by real numbers, to get more Lorentz generators. For example
is a generator. Therefore the set of all Lorentz generators 
together with the operations of ordinary matrix addition and multiplication of a matrix by a number, forms a vector space over the real numbers. The generators form a basis set of "V", and the components of the axis-angle and rapidity vectors, , are the coordinates of a Lorentz generator with respect to this basis. 
Three of the commutation relations of the Lorentz generators are
where the bracket is a binary operation known as the "commutator", and the other relations can be found by taking cyclic permutations of x, y, z components (i.e. change x to y, y to z, and z to x, repeat).
These commutation relations, and the vector space of generators, fulfill the definition of the Lie algebra formula_89. In summary, a Lie algebra is defined as a vector space "V" over a field of numbers, and with a binary operation [ , ] (called a Lie bracket in this context) on the elements of the vector space, satisfying the axioms of bilinearity, alternatization, and the Jacobi identity. Here the operation [ , ] is the commutator which satisfies all of these axioms, the vector space is the set of Lorentz generators "V" as given previously, and the field is the set of real numbers.
The group elements are found from the exponential map (Lie theory) from the Lie algebra to the Lie group;
To see the exponential mapping heuristically, consider the infinitesimal Lorentz boost in the x direction for simplicity (the generalization to any direction follows an almost identical procedure). The infinitesimal transformation a small boost away from the identity, obtained by the Taylor expansion of the boost matrix to first order about ,
where the higher order terms not shown are negligible because is small. The derivative of the matrix is the matrix of the entries differentiated with respect to the same variable (see matrix calculus), and it is understood the derivatives are found first then evaluated at , which turn out to give
The derivative of a group element with respect to a group parameter serves as a definition of the corresponding group generator, and this reflects an infinitesimal transformation away from the identity. 
In the limit of an infinite number of infinitely small steps, the finite boost transformation in the form of a matrix exponential is obtained
where the limit definition of the exponential has been used (see also characterizations of the exponential function). 
Almost identical results appear for the other Cartesian directions, and the general boost matrix is
similarly the general rotation matrix is
and the general Lorentz transformation is 
This is in general a product of a rotation and a boost, but the decomposition of a general Lorentz transformation into such factors is nontrivial. In particular,
because the generators do not commute. For a description of how to find the factors of a general Lorentz transformation in terms of a boost and a rotation "in principle" (this usually does not yield an intelligible expression in terms of generators and ), see finding the Thomas rotation. If, on the other hand, "the decomposition is given" in terms of the generators, and one wants to find the product in terms of the generators, then the Baker–Campbell–Hausdorff formula applies.
The homogenous Lorentz group.
Given two four vectors and , each written as column vectors and the same Minkowski metric tensor above,
the inner product of these can be defined as the scalar
which takes the same value in every inertial frame. A quantity invariant under Lorentz transformations is known as a Lorentz scalar. Making a Lorentz transformation on the vectors gives the same scalar,
The inner product of two four vectors is a bilinear form of signature (3,1), and is invariant under Lorentz transformations which form a group. The set of all Lorentz transformations, the full Lorentz group (denoted formula_101), is the Lie group O(3,1). From the invariance it follows
and this matrix equation will give the general conditions on the Lorentz transformation to ensure invariance of the inner product. Taking the determinant of the equation using the product rule gives immediately
Writing the general Lorentz transformation in a slightly more generalized form,
here has replaced the Lorentz factor for generality, and the conditions on it are to be found. Carrying out the block matrix multiplications obtains general conditions on , and not much information can be directly extracted from them. However, one of the results 
is more interesting and useful; since always, it follows that
The negative inequality may be unexpected, because multiplies the time coordinate and this has an effect on time symmetry. If the positive equality holds, then is the Lorentz factor.
The determinant and inequality provide four ways to classify Lorentz transformations (herein LTs for brevity). Any particular LT has only one determinant sign, and only one inequality. There are four sets which include every possible pair given by the intersections ("n"-shaped symbol meaning "and") of these classifying sets.
where "+" and "−" indicate the determinant sign, and "↑" for ≥ and "↓" for ≤ denote the inequalites. 
The full Lorentz group splits into the union ("u"-shaped symbol meaning "or") of four disjoint sets
A subgroup of a group must be closed under the same operation of the group (here matrix multiplication). In other words, for two Lorentz transformations Λ and L from a particular set, the composite Lorentz transformations ΛL and LΛ must return to the same set Λ and L came from. This will not always be the case; it can be shown that the composition of "any" two Lorentz transformations always has the positive determinant and positive inequality, a proper orthochronous transformation.
The orthochronous, proper, proper orthochronous sets of LTs are all subgroups. Another subgroup is the union of proper orthochronous and improper antichronous sets, formula_108.
Rotations and boosts are elements of the proper orthochronous Lorentz group. 
The other sets involving the improper and/or antichronous properties do not form subgroups, because the composite transformation always has a positive determinant or inequality, whereas the original separate transformations will have negative determinants and/or inequalities. However, the elements of these sets can be expressed in terms of proper orthochronous transformations with appropriate parity inversion and/or time reversal . These are in matrix form
so if Λ is a proper orthochronous LT, then TΛ is improper antichronous, PΛ is improper orthochronous, and TPΛ = PTΛ is proper antichronous.
There are four extra symmetries in spacetime; a translation in time, and three translations in space, therefore the homogenous Lorentz group does not contain the full symmetry of special relativity.
The inhomogenous Lorentz group.
The inhomogenous Lorentz transformation or Poincaré transformation includes a translation in spacetime. If "C" is a constant 4-vector (again, here written as a column vector) then the Poincaré transformation is
The Poincaré transformation matrix can be compactly expressed as a 5×5 matrix to include the translations in spacetime in a fifth column, and the four-vectors have an extra entry equal to unity,
The set of Poincaré transformations satisfies the properties of a group and is called the Poincaré group. This is the full symmetry of special relativity. Under the Erlangen program, Minkowski space can be viewed as the geometry defined by the Poincaré group, which combines Lorentz transformations with translations. 
The additional generators are the components of the four-momentum: energy is the generator of time translation, and the 3d momentum components are the generators of spatial translations in those directions. The commutation relations are enlarged to include the momenta with the boost and rotation generators.
Tensor formulation.
Writing the general matrix transformation of coordinates as the matrix equation
allows the transformation of other physical quantities that cannot be expressed as four-vectors, e.g., tensors or spinors of any order in 4d spacetime, to be defined. In the corresponding tensor index notation notation, the above matrix expression is
where upper and lower indices label covariant and contravariant components respectively, and the summation convention is applied. It is a standard convention to use Greek indices that take the value 0 for time components, and 1, 2, 3 for space components, while Latin indices simply take the values 1, 2, 3, for spatial components. Note that the first index (reading left to right) corresponds in the matrix notation to a "row index". The second index corresponds to the column index.
The transformation matrix is universal for all four-vectors, not just 4-dimensional spacetime coordinates. If is any four-vector, then in tensor index notation
Alternatively, one writes
in which the primed indices denote the indices of A in the primed frame. This notation cuts risk of exhausting the Greek alphabet roughly in half.
For a general -component object one may write
where is the appropriate representation of the Lorentz group, an matrix for every . In this case, the indices should "not" be though of as a spacetime indices (sometimes called a Lorentz indices), and they run from to . E.g. if is a bispinor, then the indices are called "Dirac indices".
There are also vector quantities with covariant indices. They are generally obtained from their corresponding objects with contravariant indices by the operation of "lowering and index", e.g.
where is the metric tensor. (The linked article also provides more information about what the operation of raising and lowering indices really is mathematically.) The inverse of this transformation is given by
where, when viewed as matrices, is the inverse of . As it happens, . This is referred to as "raising an index". To transform a covariant vector , first raise its index, then transform it according to the same rule as for contravariant -vectors, then finally lower the index;
But 
i. e. it is the -component of the "inverse" Lorentz transformation. One defines (as a matter of notation),
and may in this notation write
Now for a subtlety. The implied summation on the right hand side of 
is running over "a row index" of the matrix representing . Thus, in terms of matrices, this transformation should be thought of as the "inverse transpose" of acting on the column vector . That is, in pure matrix notation,
This means exactly that covariant vectors (thought of as column matrices) transform according to the dual representation of the standard representation of the Lorentz group. This notion generalizes to general representations, simply replace with . 
The transformation of any tensor quantity is given by:
where is defined above. This form can generally be reduced to the form for general -component objects given above with a single matrix () operating on column vectors. This latter form is sometimes preferred, e. g. for the electromagnetic field tensor.

</doc>
<doc id="27640668" url="https://en.wikipedia.org/wiki?curid=27640668" title="Lorentz-violating neutrino oscillations">
Lorentz-violating neutrino oscillations

Lorentz-violating neutrino oscillation refers to the quantum phenomenon of neutrino oscillations described in a framework that allows the breakdown of Lorentz invariance. Today, neutrino oscillation or change of one type of neutrino into another is an experimentally verified fact; however, the details of the underlying theory responsible for these processes remain an open issue and an active field of study. The conventional model of neutrino oscillations assumes that neutrinos are massive, which provides a successful description of a wide variety of experiments; however, there are a few oscillation signals that cannot be accommodated within this model, which motivates the study of other descriptions. In a theory with Lorentz violation neutrinos can oscillate with and without masses and many other novel effects described below appear. The generalization of the theory by incorporating Lorentz violation has shown to provide alternative scenarios to explain all the established experimental data through the construction of global models.
Introduction.
Conventional Lorentz-preserving descriptions of neutrinos explain the phenomenon of oscillations by endowing these particles with mass. However, if Lorentz violation occurs, oscillations could be due to other mechanisms. The general framework for Lorentz violation is called the Standard-Model Extension (SME). The neutrino sector of the SME provides a description of how Lorentz and CPT violation would affect neutrino propagation, interactions, and oscillations. This neutrino framework first appeared in
1997 as part of the general SME for Lorentz violation in particle physics, which is built from the operators of the Standard Model. An isotropic limit of the SME,
including a discussion on Lorentz-violating neutrino oscillations, was presented in a 1999 publication. Full details of the general formalism for Lorentz and CPT symmetry in the neutrino sector appeared in a 2004 publication. This work presented the minimal SME (mSME) for the neutrino sector, which involves only renormalizable terms. The incorporation of operators of arbitrary dimension in the neutrino sector was presented in 2011.
The Lorentz-violating contributions to the Lagrangian are built as observer Lorentz scalars by contracting standard field operators with controlling quantities called coefficients for Lorentz violation. These coefficients, arising from the spontaneous breaking of Lorentz symmetry, lead to non-standard effects that could be observed in current experiments. Tests of Lorentz symmetry attempt to measure these coefficients. A nonzero result would indicate Lorentz violation.
The construction of the neutrino sector of the SME includes the Lorentz-invariant terms of the standard neutrino massive model, Lorentz-violating terms that are even under CPT, and ones that are odd under CPT.
Since in field theory the breaking of CPT symmetry is accompanied by the breaking of Lorentz symmetry, the CPT-breaking terms are necessarily Lorentz breaking. It is reasonable to expect that Lorentz and CPT violation are suppressed at the Planck scale, so the coefficients for Lorentz violation are likely to be small. The interferometric nature of neutrino oscillation experiments, and also of neutral-meson systems, gives them exceptional sensitivity to such tiny effects. This holds promise for oscillation-based experiments to probe new physics and access regions of the SME coefficient space that are still untested.
General predictions.
Current experimental results indicate that neutrinos do indeed oscillate. These oscillations have a variety of possible implications, including the existence of neutrino masses, and the presence of several types of Lorentz violation. In the following, each category of Lorentz breaking is outlined.
Spectral anomalies.
In the standard Lorentz-invariant description of massive-neutrinos, the oscillation phase is proportional to the baseline "L" and inversely proportional to the neutrino energy "E". The mSME introduces dimension-three operators that lead to oscillation phases with no energy dependence. It also introduces dimension-four operators generating oscillation phases proportional to the energy. Standard oscillation amplitudes are controlled by three mixing angles
and one phase, all of which are constant. In the SME framework, Lorentz violation can lead to energy-dependent mixing parameters.
When the whole SME is considered and nonrenormalizable terms in the theory are not neglected, the energy dependence of the effective hamiltonian takes the form of an infinite series in powers of neutrino energy. The fast growth of elements in the hamiltonian could produce oscillation signals in short-baseline experiment, as in the puma model.
The unconventional energy dependence in the theory leads to other novel effects, including corrections to the dispersion relations that would make neutrinos move at velocities other than the speed of light. By this mechanism neutrinos could become faster-than-light particles. The most general form of the neutrino sector of the SME has been constructed by including operators of arbitrary dimension. In this formalism, the speed of propagation of neutrinos is obtained. Some of the interesting new features introduced by the violation of Lorentz invariance include dependence of this velocity on neutrino energy and direction of propagation. Moreover, different neutrino flavors could also have different speeds.
"L" − "E" conflicts.
The "L" − "E" conflicts refer to null or positive oscillation signals for values of "L" and "E" that are not consistent with the Lorentz-invariant explanation. For example, KamLAND and SNO observations require a mass-squared difference formula_1 to be consistent with the Lorentz-invariant phase proportional to "L"/"E". Similarly, Super-Kamiokande, K2K, and MINOS observations of atmospheric-neutrino oscillations require a mass-squared difference formula_2. Any neutrino-oscillation experiment must be consistent with either of
these two mass-squared differences for Lorentz invariance to hold. To date, this is the only class of signal for which there is positive evidence. The LSND experiment observed oscillations leading to a mass-squared difference that is inconsistent with results from solar- and atmospheric-neutrino observations. The oscillation phase requires formula_3. This anomaly can be understood in the presence of Lorentz violation.
Periodic variations.
Laboratory experiments follow complicated trajectories as the Earth rotates on its axis and revolves around the Sun. Since the fixed SME background fields are coupled with the particle fields, periodic variations associated with these motions would be one of the signatures of Lorentz violation.
There are two categories of periodic variations:
Compass asymmetries.
The breaking of rotation invariance can also lead to time-independent signals arising in the form of directional asymmetries at the location of the detector. This type of signal can cause differences in observed neutrino properties for neutrinos originating from different directions.
Neutrino-antineutrino mixing.
Some of the mSME coefficients lead to mixing between neutrinos and antineutrinos. These processes violate lepton-number conservation, but can readily be accommodated in the Lorentz-breaking SME framework. The breaking of invariance under rotations leads to the non-conservation of angular momentum, which allows a spin flip of the propagating neutrino that can oscillate into an antineutrino. Because of the lost of rotational symmetry coefficients responsible for this type of mixing always introduce direction dependence.
Classic CPT tests.
Since CPT violation implies Lorentz violation, traditional tests of CPT symmetry can also be used to search for deviations from Lorentz invariance. This test seeks evidence of formula_5. Some subtle features arise. For example, although CPT invariance implies formula_6, this relation can be satisfied even in the presence of CPT violation.
Global models of neutrino oscillations with Lorentz violation.
Global models are descriptions of neutrino oscillations that are consistent with all the established experimental data: solar, reactor, accelerator, and atmospheric neutrinos. The general SME theory of Lorentz-violating neutrinos has shown to be very successful as an alternative description of all observed neutrino data. These global models are based on the SME and exhibit some of the key signals of Lorentz violation described in the previous section.
Bicycle model.
The first phenomenological model using Lorentz-violating neutrinos was proposed by Kostelecky and Mewes in a 2004 paper. This so-called "bicycle" model exhibits direction dependence and only two parameters (two non-zero SME coefficients), instead of the six of the conventional massive model. One of the main characteristics of this model is that neutrinos are assumed to be massless. This simple model is compatible with solar, atmospheric, and long-baseline neutrino oscillation data. A novel feature of the bicycle model occurs at high energies, where the two SME coefficients combine to create a direction-dependent pseudomass. This leads to maximal mixing and an oscillation phase proportional to "L"/"E", as in the massive case.
Generalized bicycle model.
The bicycle model is an example of a very simple and realistic model that can accommodate most of the observed data using massless neutrinos in the presence of Lorentz violation. In 2007, Barger, Marfatia, and Whisnant constructed a more general version of this model by including more parameters. In this paper, it is shown that a combined analysis of solar, reactor, and long-baseline experiments excluded the bicycle model and its generalization. Despite this, the bicycle served as starting point for more elaborate models.
Tandem model.
The tandem model is an extended version of the bicycle presented in 2006 by Katori, Kostelecky, and Tayloe. It is a hybrid model that includes Lorentz violation and also mass terms for a subset of neutrino flavors. It attempts to construct a realistic model by applying a number of desirable criteria. In particular, acceptable models for neutrino violation should:
All these criteria are satisfied by the tandem model, which looks like a simple extension of the bicycle. Nevertheless, it involves isotropic coefficients only, which means that there is no direction dependence. The extra term is a massive term that reproduces the "L"/"E" phase at low energies observed by KamLAND. It turns out that the tandem model is consistent with atmospheric, solar, reactor, and short-baseline data, including LSND. Besides the consistency with all experimental data, the most remarkable feature of this model is the prediction of a low-energy excess in MiniBooNE. When the tandem is applied to short-baseline accelerator experiments, it is consistent with the KARMEN null result, due to the very short baseline. For MiniBooNE, the tandem model predicted an oscillation signal at low energy that drops off very quickly. The MiniBooNE results, released a year after the tandem model was published, did indeed show an unexplained excess at low energies. This excess cannot be understood within the standard massive-neutrino model, and the tandem remains one of the best candidates for its explanation.
Puma model.
The puma model was proposed by Diaz and Kostelecky in 2010 as a three-parameter model that exhibits consistency with all the established neutrino data (accelerator, atmospheric, reactor, and solar) and naturally describes the anomalous low-energy excess observed in MiniBooNE that is inconsistent with the conventional massive model. This is a hybrid model that includes Lorentz violation and neutrino masses. One of the main differences between this model and the bicycle and tandem models described above is the incorporation of nonrenormalizable terms in the theory, which lead to powers of the energy greater than one. Nonetheless, all these models share the characteristic of having a mixed energy dependence that leads to energy-dependent mixing angles, a feature absent in the conventional massive model. At low energies, the mass term dominates and the mixing takes the tribimaximal form, a widely used matrix postulated to describe neutrino mixing. This mixing added to the 1/"E" dependence of the mass term guarantees agreement with solar and KamLAND data. At high energies, Lorentz-violating contributions take over making the contribution of neutrino masses negligible. A seesaw mechanism is triggered, similar to that in the bicycle model, making one of the eigenvalues proportional to 1/"E", which usually come with neutrino masses. This feature lets the model mimic the effects of a mass term at high energies despite the fact that there are only non-negative powers of the energy. The energy dependence of the Lorentz-violating terms produce maximal formula_9 mixing, which makes the model consistent with atmospheric and accelerator data. The oscillation signal in MiniBooNE appears because the oscillation phase responsible for the oscillation channel formula_10 grows rapidly with energy and the oscillation amplitude is large only for energies below 500 MeV. The combination of these two effects produces an oscillation signal in MiniBooNE at low energies, in agreement with the data. Additionally, since the model includes a term associated to a CPT-odd Lorentz-violating operator, different probabilities appear for neutrinos and antineutrinos. Moreover, since the amplitude for formula_10 decreases for energies above 500 MeV, long-baseline experiments searching for nonzero formula_12 should measure different values depending on the energy; more precisely, the MINOS experiment should measure a value smaller than the T2K experiment according to the puma model, which agrees with current measurements.
Isotropic bicycle model.
In 2011, Barger, Liao, Marfatia, and Whisnant studied general bicycle-type models (without neutrino masses) that can be constructed using the minimal SME that are isotropic (direction independent). Results show that long-baseline accelerator and atmospheric data can be described by these models in virtue of the Lorentz-violating seesaw mechanism; nevertheless, there is a tension between solar and KamLAND data. Given this incompatibility, the authors concluded that renormalizable models with massless neutrinos are excluded by the data.
Theory.
From a general model-independent point of view, neutrinos oscillate because the effective hamiltonian describing their propagation is not diagonal in flavor space and has a non-degenerate spectrum, in other words, the eigenstates of the hamiltonian are linear superpositions of the flavor eigenstates of the weak interaction and there are at least two different eigenvalues. If we find a transformation formula_13 that puts the effective hamiltonian in flavor basis ("h"eff)"ab" in the diagonal form
(where the indices "a", "b" = "e", μ, τ and "a′", "b′" =1, 2, 3 denote the flavor and diagonal basis, respectively), then we can write the oscillation probability from a flavor state formula_15 to formula_16 as
where formula_18 are the eigenvalues. For the conventional massive model formula_19.
In the SME formalism, the neutrino sector is described by a 6-component vector with three active left-handed neutrinos and three right-handed antineutrinos. The effective Lorentz-violating Hamiltonian is a 6 × 6 matrix that takes the explicit form
where flavor indices have been suppressed for simplicity. The widehat on the elements of the last term indicates that these effective coefficients for Lorentz violation are associated to operators of arbitrary dimension. These elements are in general functions of the energy, neutrino direction of propagation, and coefficients for Lorentz violation. Each block corresponds to a 3 × 3 matrix. The 3 × 3 diagonal blocks describe neutrino–neutrino and antineutrino–antineutrino mixing, respectively. The 3 × 3 off-diagonal blocks lead to neutrino–antineutrino oscillations. This hamiltonian contains the information of propagation and oscillations of neutrinos. In particular, the speed of propagation relevant for time-of-flight measurements can be written
that corresponds to oscillation-free approximation of the hamiltonian above. In this expression the neutrino speed has been spherically decomposed by using the standard spherical harmonics. This expression shows how neutrino speed can depend on energy and direction of propagation. In general, this speed can also depend on neutrino flavor. The index "d" denotes the dimension of the operator that breaks Lorentz symmetry. The form of neutrino speed shows that faster-than-light neutrinos can naturally be described by the SME.
During the last decade, studies have mainly focused on the minimal sector of the general theory, in which case the hamiltonian above takes the explicit form
The indices of this effective Hamiltonian take the six values "A", "B" = "e", μ, τ, , , , for neutrinos and antineutrinos. The lowercase indices indicate neutrinos ("a", "b" = "e", μ, τ), and the barred lowercase indices indicate antineutrinos (, = , , ). Notice that the ultrarelativistic approximation formula_23 has been used.
The first term is diagonal and can be removed because it does not contribute to oscillations; however, it can play an important role in the stability of the theory. The second term is the standard massive-neutrino Hamiltonian. The third term is the Lorentz-violating contribution. It involves four types of coefficients for Lorentz violation. The coefficients formula_24 and formula_25 are of dimension one and zero, respectively. These coefficients are responsible for the mixing of left-handed neutrinos, leading to Lorentz-violating neutrino–neutrino oscillations. Similarly, the coefficients formula_26 and formula_27 mix right-handed antineutrinos, leading to Lorentz-violating antineutrino–antineutrino oscillations. Notice that these coefficients are 3 × 3 matrices having both spacetime (Greek) and flavor indices (Roman). The off-diagonal block involves the dimension-zero coefficients, formula_28, and the dimension-one coefficients, formula_29. These lead to neutrino–antineutrino oscillations. All spacetime indices are properly contracted forming observer Lorentz scalars. The four-momentum shows explicitly that the direction of propagation couples to the mSME coefficients, generating the periodic variations and compass asymmetries described in the previous section. Finally, note that coefficients with an odd number of spacetime indices are contracted with operators that break CPT. It follows that the "a"- and "g"-type coefficients are CPT-odd. By similar reasoning, the "c"- and "H"-type coefficients are CPT-even.
Applying the theory to experiments.
Negligible-mass description.
For most short baseline neutrino experiments, the ratio of experimental baseline to neutrino energy, "L"/"E", is small, and neutrino masses can be neglected because they are not responsible for oscillations. In these cases, the possibility exists of attributing observed oscillations to Lorentz violation, even if the neutrinos are massive. This limit of the theory is sometimes called the short-baseline approximation. Caution is necessary in this point, because, in short-baseline experiments, masses can become relevant if the energies are sufficiently low.
An analysis of this limit, presenting experimentally accessible coefficients for Lorentz violation, first appeared in a 2004 publication. Neglecting neutrino masses, the neutrino Hamiltonian becomes
In appropriate cases, the oscillation amplitude can be expanded in the form
This approximation is valid if the baseline "L" is short compared to the oscillation length given by "h"eff. Since "h"eff varies with energy, the term "short baseline" really depends on both "L" and "E". At leading order, the oscillation probability becomes
Remarkably, this mSME framework for short-baseline neutrino experiments, when applied to the LSND anomaly, leads to values of order formula_33 for formula_24 and formula_35 for formula_25. These numbers are in the range of what one might expect from quantum-gravity effects. Data analysis has been performed using the LSND, MINOS,
MiniBooNE, and IceCube 
experiments to set limits on the coefficients formula_24 and formula_25. These results, along with experimental results in other sectors of the SME, are summarized in the Data Tables for Lorentz and CPT violation.
Perturbative Lorentz-violating description.
For experiments where "L"/"E" is not small, neutrino masses dominate the oscillation effects. In these cases, Lorentz violation can be introduced as a perturbative effect in the form
where "h"0 is the standard massive-neutrino Hamiltonian, and δ"h" contains the Lorentz-breaking mSME terms. This limit of the general theory was introduced in a 2009 publication, and includes both neutrinos and antineutrinos in the 6 × 6 Hamiltonian formalism (1). In this work, the oscillation probability takes the form
where formula_41 is the standard expression. One of the results is that, at leading order, neutrino and antineutrino oscillations are decoupled from one another. This means neutrino–antineutrino oscillations are a second-order effect.
In the two-flavor limit, the first-order correction introduced by Lorentz violation to atmospheric neutrinos takes the simple form
This expression shows how the baseline of the experiment can enhance the effects of the mSME coefficients in δ"h".
This perturbative framework can be applied to most of the long-baseline experiments. It is also applicable in some short-baseline experiments with low-energy neutrinos. An analysis has been done in the case of several long-baseline experiments (DUSEL, ICARUS, K2K, MINOS, NOvA, OPERA, T2K, and T2KK), showing high sensitivities to the coefficients for Lorentz violation. Data analysis has been performed using the far detector of the MINOS experiment 
to set limits on the coefficients formula_24 and formula_25. These results are summarized in the Data Tables for Lorentz and CPT violation.

</doc>
<doc id="1718317" url="https://en.wikipedia.org/wiki?curid=1718317" title="Lorenz gauge condition">
Lorenz gauge condition

In electromagnetism, the Lorenz gauge or Lorenz gauge condition is a partial gauge fixing of the electromagnetic vector potential. The condition is that formula_1. This does not completely fix the gauge: one can still make a gauge transformation formula_2 where formula_3 is a harmonic scalar function (that is, a scalar function satisfying formula_4, the equation of a massless scalar field).
The Lorenz condition is used to eliminate the redundant spin-0 component in the (1/2,1/2) representation of the Lorentz group. It is equally used for massive spin-1 fields where the concept of gauge transformations does not apply at all.
The Lorenz condition is named after Ludvig Lorenz. It is a Lorentz invariant condition, and is frequently called the "Lorentz condition" because of confusion with Hendrik Lorentz, after whom Lorentz covariance is named.
Description.
In electromagnetism, the Lorenz condition is generally used in calculations of time-dependent electromagnetic fields through retarded potentials. The condition is
where formula_6 is the four-potential, the comma denotes a partial differentiation and the repeated index indicates that the Einstein summation convention is being used. The condition has the advantage of being Lorentz invariant. It still leaves substantial gauge degrees of freedom.
In ordinary vector notation and SI units, the condition is:
where formula_8 is the magnetic vector potential and formula_9 is the electric potential; see also Gauge fixing.
In Gaussian units the condition is:
A quick justification of the Lorenz gauge can be found using Maxwell's equations and the relation between the magnetic vector potential and the magnetic field:
Therefore,
Since the curl is zero, that means there is a scalar function formula_9 such that formula_14. This gives the well known equation for the electric field, formula_15. This result can be plugged into another one of Maxwell's equations,
This leaves,
To have Lorentz invariance, the time derivatives and spatial derivatives must be treated equally (i.e. of the same order). Therefore it is convenient to choose the Lorenz gauge condition, which gives the result
A similar procedure with a focus on the electric scalar potential and making the same gauge choice will yield
These are simpler and more symmetric forms of the inhomogeneous Maxwell's equations. Note that the Coulomb gauge also fixes the problem of Lorentz invariance, but leaves a coupling term with first-order derivatives.
Here formula_20 is the vacuum velocity of light, and formula_21 is the d'Alembertian operator. Interestingly, and unexpectedly at a first glance, these equations are not only valid under vacuum conditions, but also in polarized media 
, if formula_22 and formula_23 are source density and circulation density, respectively, of the electromagnetic induction fields formula_24 and formula_25 calculated as usual from formula_9 and formula_27 by the equations formula_28 and formula_29 The explicit solutions for formula_9 and formula_27 – unique, if all quantities vanish sufficiently fast at infinity – are known as retarded potentials.
History.
When originally published, Lorenz's work was not received well by Maxwell. Maxwell had eliminated the Coulomb electrostatic force from his derivation of the electromagnetic wave equation since he was working in what would nowadays be termed the Coulomb gauge. The Lorenz gauge hence contradicted Maxwell's original derivation of the EM wave equation by introducing a retardation effect to the Coulomb force and bringing it inside the EM wave equation alongside the time varying electric field, which was introduced in Lorenz's paper "On the identity of the vibrations of light with electrical currents". Lorenz's work was the first symmetrizing shortening of Maxwell's equations after Maxwell himself published his 1865 paper. In 1888, retarded potentials came into general use after Heinrich Rudolf Hertz's experiments on electromagnetic waves. In 1895, a further boost to the theory of retarded potentials came after J. J. Thomson's interpretation of data for electrons (after which investigation into electrical phenomena changed from time-dependent electric charge and electric current distributions over to moving point charges).

</doc>
<doc id="164402" url="https://en.wikipedia.org/wiki?curid=164402" title="Magnetic dipole">
Magnetic dipole

A magnetic dipole is the limit of either a closed loop of electric current or a pair of poles as the dimensions of the source are reduced to zero while keeping the magnetic moment constant. It is a magnetic analogue of the electric dipole, but the analogy is not complete. In particular, a magnetic monopole, the magnetic analogue of an electric charge, has never been observed. Moreover, one form of magnetic dipole moment is associated with a fundamental quantum property—the spin of elementary particles.
The magnetic field around any magnetic source looks increasingly like the field of a magnetic dipole as the distance from the source increases.
External magnetic field produced by a magnetic dipole moment.
In classical physics, the magnetic field of a dipole is calculated as the limit of either a current loop or a pair of charges as the source shrinks to a point while keeping the magnetic moment constant. For the current loop, this limit is most easily derived for the vector potential. Outside of the source region, this potential is (in SI units) 
with 4"π r"2 being the surface of a sphere of radius "r";
and the magnetic flux density (strength of the B-field) in teslas is
Alternatively one can obtain the scalar potential first from the magnetic pole limit,
and hence the magnetic field strength (or strength of the H-field) in ampere-turns per meter is
The magnetic field is symmetric under rotations about the axis of the magnetic moment.
Internal magnetic field of a dipole.
The two models for a dipole (current loop and magnetic poles) give the same predictions for the magnetic field far from the source. However, inside the source region they give different predictions. The magnetic field between poles is in the opposite direction to the magnetic moment (which points from the negative charge to the positive charge), while inside a current loop it is in the same direction (see the figure to the right). Clearly, the limits of these fields must also be different as the sources shrink to zero size. This distinction only matters if the dipole limit is used to calculate fields inside a magnetic material.
If a magnetic dipole is formed by making a current loop smaller and smaller, but keeping the product of current and area constant, the limiting field is
where is a unit vector, and is the Dirac delta function in three dimensions. Unlike the expressions in the previous section, this limit is correct for the internal field of the dipole.
If a magnetic dipole is formed by taking a "north pole" and a "south pole", bringing them closer and closer together but keeping the product of magnetic pole-charge and distance constant, the limiting field is
These fields are related by , where
is the magnetization.
Forces between two magnetic dipoles.
The force exerted by one dipole moment on another separated in space by a vector can be calculated using:
or 
where || is the distance between dipoles. The force acting on is in the opposite direction.
The torque can be obtained from the formula
Dipolar fields from finite sources.
The magnetic scalar potential produced by a finite source, but external to it, can be represented by a multipole expansion. Each term in the expansion is associated with a characteristic moment and a potential having a characteristic rate of decrease with distance from the source. Monopole moments have a rate of decrease, dipole moments have a rate, quadrupole moments have a rate, and so on. The higher the order, the faster the potential drops off. Since the lowest-order term observed in magnetic sources is the dipolar term, it dominates at large distances. Therefore, at large distances any magnetic source looks like a dipole with the same magnetic moment.

</doc>
<doc id="36563" url="https://en.wikipedia.org/wiki?curid=36563" title="Magnetic field">
Magnetic field

A magnetic field is the magnetic effect of electric currents and magnetic materials. The magnetic field at any given point is specified by both a "direction" and a "magnitude" (or strength); as such it is a vector field. The term is used for two distinct but closely related fields denoted by the symbols and , where is measured in units of amperes per meter (symbol: A·m−1 or A/m) in the SI. is measured in teslas (symbol:T) and newtons per meter per ampere (symbol: N·m−1·A−1 or N/(m·A)) in the SI. is most commonly defined in terms of the Lorentz force it exerts on moving electric charges.
Magnetic fields can be produced by moving electric charges and the intrinsic magnetic moments of elementary particles associated with a fundamental quantum property, their spin. In special relativity, electric and magnetic fields are two interrelated aspects of a single object, called the electromagnetic tensor; the split of this tensor into electric and magnetic fields depends on the relative velocity of the observer and charge. In quantum physics, the electromagnetic field is quantized and electromagnetic interactions result from the exchange of photons.
In everyday life, magnetic fields are most often encountered as a force created by permanent magnets, which pull on ferromagnetic materials such as iron, cobalt, or nickel, and attract or repel other magnets. Magnetic fields are widely used throughout modern technology, particularly in electrical engineering and electromechanics. The Earth produces its own magnetic field, which is important in navigation, and it shields the Earth's atmosphere from solar wind. Rotating magnetic fields are used in both electric motors and generators. Magnetic forces give information about the charge carriers in a material through the Hall effect. The interaction of magnetic fields in electric devices such as transformers is studied in the discipline of magnetic circuits.
History.
Although magnets and magnetism were known much earlier, the study of magnetic fields began in 1269 when French scholar Petrus Peregrinus de Maricourt mapped out the magnetic field on the surface of a spherical magnet using iron needles. Noting that the resulting field lines crossed at two points he named those points 'poles' in analogy to Earth's poles. He also clearly articulated the principle that magnets always have both a north and south pole, no matter how finely one slices them.
Almost three centuries later, William Gilbert of Colchester replicated Petrus Peregrinus' work and was the first to state explicitly that Earth is a magnet. Published in 1600, Gilbert's work, "De Magnete", helped to establish magnetism as a science.
In 1750, John Michell stated that magnetic poles attract and repel in accordance with an inverse square law. Charles-Augustin de Coulomb experimentally verified this in 1785 and stated explicitly that the north and south poles cannot be separated. Building on this force between poles, Siméon Denis Poisson (1781–1840) created the first successful model of the magnetic field, which he presented in 1824. In this model, a magnetic -field is produced by 'magnetic poles' and magnetism is due to small pairs of north/south magnetic poles.
Three discoveries challenged this foundation of magnetism, though. First, in 1819, Hans Christian Oersted discovered that an electric current generates a magnetic field encircling it. Then in 1820, André-Marie Ampère showed that parallel wires having currents in the same direction attract one another. Finally, Jean-Baptiste Biot and Félix Savart discovered the Biot–Savart law in 1820, which correctly predicts the magnetic field around any current-carrying wire.
Extending these experiments, Ampère published his own successful model of magnetism in 1825. In it, he showed the equivalence of electrical currents to magnets and proposed that magnetism is due to perpetually flowing loops of current instead of the dipoles of magnetic charge in Poisson's model. This has the additional benefit of explaining why magnetic charge can not be isolated. Further, Ampère derived both Ampère's force law describing the force between two currents and Ampère's law, which, like the Biot–Savart law, correctly described the magnetic field generated by a steady current. Also in this work, Ampère introduced the term electrodynamics to describe the relationship between electricity and magnetism.
In 1831, Michael Faraday discovered electromagnetic induction when he found that a changing magnetic field generates an encircling electric field. He described this phenomenon in what is known as Faraday's law of induction. Later, Franz Ernst Neumann proved that, for a moving conductor in a magnetic field, induction is a consequence of Ampère's force law. In the process he introduced the magnetic vector potential, which was later shown to be equivalent to the underlying mechanism proposed by Faraday.
In 1850, Lord Kelvin, then known as William Thomson, distinguished between two magnetic fields now denoted and . The former applied to Poisson's model and the latter to Ampère's model and induction. Further, he derived how and relate to each other.
The reason and are used for the two magnetic fields has been a source of some debate among science historians. Most agree that Kelvin avoided to prevent confusion with the SI fundamental unit of length, the Metre, abbreviated "m." However, some believe that Kelvin chose to honor his mentor Humphry Davy and chose because he and his wife never had children, and Kelvin strongly desired that his wife have a baby, and the word "baby" begins with "B." Others believe the choices were purely random.
Between 1861 and 1865, James Clerk Maxwell developed and published Maxwell's equations, which explained and united all of classical electricity and magnetism. The first set of these equations was published in a paper entitled "" in 1861. These equations were valid although incomplete. Maxwell completed his set of equations in his later 1865 paper "A Dynamical Theory of the Electromagnetic Field" and demonstrated the fact that light is an electromagnetic wave. Heinrich Hertz experimentally confirmed this fact in 1887.
The twentieth century extended electrodynamics to include relativity and quantum mechanics. Albert Einstein, in his paper of 1905 that established relativity, showed that both the electric and magnetic fields are part of the same phenomena viewed from different reference frames. (See moving magnet and conductor problem for details about the thought experiment that eventually helped Albert Einstein to develop special relativity.) Finally, the emergent field of quantum mechanics was merged with electrodynamics to form quantum electrodynamics (QED).
Definitions, units, and measurement.
The B-field.
The magnetic field can be defined in several equivalent ways based on the effects it has on its environment.
Often the magnetic field is defined by the force it exerts on a moving charged particle. It is known from experiments in electrostatics that a particle of charge in an electric field experiences a force . However, in other situations, such as when a charged particle moves in the vicinity of a current-carrying wire, the force also depends on the velocity of that particle. Fortunately, the velocity dependent portion can be separated out such that the force on the particle satisfies the "Lorentz force law",
Here is the particle's velocity and × denotes the cross product. The vector is termed the magnetic field, and it is "defined" as the vector field necessary to make the Lorentz force law correctly describe the motion of a charged particle. This definition allows the determination of in the following way
Alternatively, the magnetic field can be defined in terms of the torque it produces on a magnetic dipole (see magnetic torque on permanent magnets below).
The H-field.
In addition to , there is a quantity , which is also sometimes called the "magnetic field". In a vacuum, and are proportional to each other, with the multiplicative constant depending on the physical units. Inside a material they are different (see H and B inside and outside of magnetic materials). The term "magnetic field" is historically reserved for while using other terms for . Informally, though, and formally for some recent textbooks mostly in physics, the term 'magnetic field' is used to describe as well as or in place of .
There are many alternative names for both (see sidebar).
Units.
In SI units, is measured in teslas (symbol: T) and correspondingly (magnetic flux) is measured in webers (symbol: Wb) so that a flux density of 1 Wb/m2 is 1 tesla. The SI unit of tesla is equivalent to (newton·second)/(coulomb·metre). In Gaussian-cgs units, is measured in gauss (symbol: G). (The conversion is 1 T = 10,000 G.) The -field is measured in amperes per metre (A/m) in SI units, and in oersteds (Oe) in cgs units.
Measurement.
The precision attained for a magnetic field measurement for Gravity Probe B experiment is 5 attoteslas (); the largest magnetic field produced in a laboratory is 2.8 kT (VNIIEF in Sarov, Russia, 1998). The magnetic field of some astronomical objects such as magnetars are much higher; magnetars range from 0.1 to 100 GT (108 to 1011 T). See orders of magnitude (magnetic field).
Devices used to measure the local magnetic field are called magnetometers. Important classes of magnetometers include using a rotating coil, Hall effect magnetometers, NMR magnetometers, SQUID magnetometers, and fluxgate magnetometers. The magnetic fields of distant astronomical objects are measured through their effects on local charged particles. For instance, electrons spiraling around a field line produce synchrotron radiation that is detectable in radio waves.
Magnetic field lines.
Mapping the magnetic field of an object is simple in principle. First, measure the strength and direction of the magnetic field at a large number of locations (or at every point in space). Then, mark each location with an arrow (called a vector) pointing in the direction of the local magnetic field with its magnitude proportional to the strength of the magnetic field.
An alternative method to map the magnetic field is to 'connect' the arrows to form magnetic "field lines". The direction of the magnetic field at any point is parallel to the direction of nearby field lines, and the local density of field lines can be made proportional to its strength.
Magnetic field lines are like the contour lines (constant altitude) on a topographic map in that they represent something continuous, and a different mapping scale would show more or fewer lines. An advantage of using magnetic field lines as a representation is that many laws of magnetism (and electromagnetism) can be stated completely and concisely using simple concepts such as the 'number' of field lines through a surface. These concepts can be quickly 'translated' to their mathematical form. For example, the number of field lines through a given surface is the surface integral of the magnetic field.
Various phenomena have the effect of "displaying" magnetic field lines as though the field lines are physical phenomena. For example, iron filings placed in a magnetic field line up to form lines that correspond to 'field lines'. Magnetic fields' "lines" are also visually displayed in polar auroras, in which plasma particle dipole interactions create visible streaks of light that line up with the local direction of Earth's magnetic field.
Field lines can be used as a qualitative tool to visualize magnetic forces. In ferromagnetic substances like iron and in plasmas, magnetic forces can be understood by imagining that the field lines exert a tension, (like a rubber band) along their length, and a pressure perpendicular to their length on neighboring field lines. 'Unlike' poles of magnets attract because they are linked by many field lines; 'like' poles repel because their field lines do not meet, but run parallel, pushing on each other. The rigorous form of this concept is the electromagnetic stress–energy tensor.
Magnetic field and permanent magnets.
"Permanent magnets" are objects that produce their own persistent magnetic fields. They are made of ferromagnetic materials, such as iron and nickel, that have been magnetized, and they have both a north and a south pole.
Magnetic field of permanent magnets.
The magnetic field of permanent magnets can be quite complicated, especially near the magnet. The magnetic field of a small straight magnet is proportional to the magnet's "strength" (called its magnetic dipole moment ). The equations are non-trivial and also depend on the distance from the magnet and the orientation of the magnet. For simple magnets, points in the direction of a line drawn from the south to the north pole of the magnet. Flipping a bar magnet is equivalent to rotating its by 180 degrees.
The magnetic field of larger magnets can be obtained by modelling them as a collection of a large number of small magnets called dipoles each having their own . The magnetic field produced by the magnet then is the net magnetic field of these dipoles. And, any net force on the magnet is a result of adding up the forces on the individual dipoles.
There are two competing models for the nature of these dipoles. These two models produce two different magnetic fields, and . Outside a material, though, the two are identical (to a multiplicative constant) so that in many cases the distinction can be ignored. This is particularly true for magnetic fields, such as those due to electric currents, that are not generated by magnetic materials.
Magnetic pole model and the H-field.
It is sometimes useful to model the force and torques between two magnets as due to magnetic poles repelling or attracting each other in the same manner as the Coulomb force between electric charges. This is called the Gilbert model of magnetism, after William Gilbert. In this model, a magnetic -field is produced by "magnetic charges" that are 'smeared' around each pole. 
These "magnetic charges" are in fact related to the magnetization field 
The -field, therefore, is analogous to the electric field , which starts at a positive electric charge and ends at a negative electric charge. Near the north pole, therefore, all -field lines point away from the north pole (whether inside the magnet or out) while near the south pole (whether inside the magnet or out) all -field lines point toward the south pole. A north pole, then, feels a force in the direction of the -field while the force on the south pole is opposite to the -field.
In the magnetic pole model, the elementary magnetic dipole is formed by two opposite magnetic poles of pole strength separated by a small distance vector , such that . The magnetic pole model predicts correctly the field both inside and outside 
magnetic materials, in particular the fact that is opposite to the 
magnetization field inside a permanent magnet.
Since it is based on the fictitious idea of a "magnetic charge density", the Gilbert model has limitations. Magnetic poles cannot exist apart from each other as electric charges can, but always come in north/south pairs. If a magnetized object is divided in half, a new pole appears on the surface of each piece, so each has a pair of complementary poles. The magnetic pole model does not account for magnetism that is produced by electric currents.
Amperian loop model and the B-field.
After Oersted discovered that electric currents produce a magnetic field and Ampere discovered that electric currents attracted and repelled each other similar to magnets, it was natural to hypothesize that all magnetic fields are due to electric current loops. In this model developed by Ampere, the elementary magnetic dipole that makes up all magnets is a sufficiently small Amperian loop of current I. The dipole moment of this loop is where is the area of the loop.
These magnetic dipoles produce a magnetic -field. One important property of the -field produced this way is that magnetic -field lines neither start nor end (mathematically, is a solenoidal vector field); a field line either extends to infinity or wraps around to form a closed curve. To date no exception to this rule has been found. (See magnetic monopole below.) Magnetic field lines exit a magnet near its north pole and enter near its south pole, but inside the magnet -field lines continue through the magnet from the south pole back to the north. If a -field line enters a magnet somewhere it has to leave somewhere else; it is not allowed to have an end point. Magnetic poles, therefore, always come in N and S pairs.
More formally, since all the magnetic field lines that enter any given region must also leave that region, subtracting the 'number' of field lines that enter the region from the number that exit gives identically zero. Mathematically this is equivalent to:
where the integral is a surface integral over the closed surface (a closed surface is one that completely surrounds a region with no holes to let any field lines escape). Since points outward, the dot product in the integral is positive for -field pointing out and negative for -field pointing in.
There is also a corresponding differential form of this equation covered in Maxwell's equations below.
Force between magnets.
The force between two small magnets is quite complicated and depends on the strength and orientation of both magnets and the distance and direction of the magnets relative to each other. The force is particularly sensitive to rotations of the magnets due to magnetic torque. The force on each magnet depends on its magnetic moment and the magnetic field of the other.
To understand the force between magnets, it is useful to examine the "magnetic pole model" given above. In this model, the "-field" of one magnet pushes and pulls on "both" poles of a second magnet. If this -field is the same at both poles of the second magnet then there is no net force on that magnet since the force is opposite for opposite poles. If, however, the magnetic field of the first magnet is "nonuniform" (such as the near one of its poles), each pole of the second magnet sees a different field and is subject to a different force. This difference in the two forces moves the magnet in the direction of increasing magnetic field and may also cause a net torque.
This is a specific example of a general rule that magnets are attracted (or repulsed depending on the orientation of the magnet) into regions of higher magnetic field. Any non-uniform magnetic field, whether caused by permanent magnets or electric currents, exerts a force on a small magnet in this way.
The details of the Amperian loop model are different and more complicated but yield the same result: that magnetic dipoles are attracted/repelled into regions of higher magnetic field.
Mathematically, the force on a small magnet having a magnetic moment due to a magnetic field is:
where the gradient is the change of the quantity per unit distance and the direction is that of maximum increase of . To understand this equation, note that the dot product , where and represent the magnitude of the and vectors and is the angle between them. If is in the same direction as then the dot product is positive and the gradient points 'uphill' pulling the magnet into regions of higher -field (more strictly larger ). This equation is strictly only valid for magnets of zero size, but is often a good approximation for not too large magnets. The magnetic force on larger magnets is determined by dividing them into smaller regions each having their own then summing up the forces on each of these very small regions.
Magnetic torque on permanent magnets.
If two like poles of two separate magnets are brought near each other, and one of the magnets is allowed to turn, it promptly rotates to align itself with the first. In this example, the magnetic field of the stationary magnet creates a "magnetic torque" on the magnet that is free to rotate. This magnetic torque tends to align a magnet's poles with the magnetic field lines. A compass, therefore, turns to align itself with Earth's magnetic field.
Magnetic torque is used to drive electric motors. In one simple motor design, a magnet is fixed to a freely rotating shaft and subjected to a magnetic field from an array of electromagnets. By continuously switching the electric current through each of the electromagnets, thereby flipping the polarity of their magnetic fields, like poles are kept next to the rotor; the resultant torque is transferred to the shaft. See Rotating magnetic fields below.
As is the case for the force between magnets, the magnetic pole model leads more readily to the correct equation. Here, two equal and opposite magnetic charges experiencing the same also experience equal and opposite forces. Since these equal and opposite forces are in different locations, this produces a torque proportional to the distance (perpendicular to the force) between them. With the definition of as the pole strength times the distance between the poles, this leads to , where is a constant called the vacuum permeability, measuring V·s/(A·m) and is the angle between and .
The Amperian loop model also predicts the same magnetic torque. Here, it is the field interacting with the Amperian current loop through a Lorentz force described below. Again, the results are the same although the models are completely different.
Mathematically, the torque on a small magnet is proportional both to the applied magnetic field and to the magnetic moment of the magnet:
where × represents the vector cross product. Note that this equation includes all of the qualitative information included above. There is no torque on a magnet if is in the same direction as the magnetic field. (The cross product is zero for two vectors that are in the same direction.) Further, all other orientations feel a torque that twists them toward the direction of magnetic field.
Magnetic field and electric currents.
Currents of electric charges both generate a magnetic field and feel a force due to magnetic B-fields.
Magnetic field due to moving charges and electric currents.
All moving charged particles produce magnetic fields. Moving point charges, such as electrons, produce complicated but well known magnetic fields that depend on the charge, velocity, and acceleration of the particles.
Magnetic field lines form in concentric circles around a cylindrical current-carrying conductor, such as a length of wire. The direction of such a magnetic field can be determined by using the "right hand grip rule" (see figure at right). The strength of the magnetic field decreases with distance from the wire. (For an infinite length wire the strength is inversely proportional to the distance.)
Bending a current-carrying wire into a loop concentrates the magnetic field inside the loop while weakening it outside. Bending a wire into multiple closely spaced loops to form a coil or "solenoid" enhances this effect. A device so formed around an iron core may act as an "electromagnet", generating a strong, well-controlled magnetic field. An infinitely long cylindrical electromagnet has a uniform magnetic field inside, and no magnetic field outside. A finite length electromagnet produces a magnetic field that looks similar to that produced by a uniform permanent magnet, with its strength and polarity determined by the current flowing through the coil.
The magnetic field generated by a steady current (a constant flow of electric charges, in which charge neither accumulates nor is depleted at any point) is described by the "Biot–Savart law":
where the integral sums over the wire length where vector is the vector line element with direction in the same sense as the current , is the magnetic constant, is the distance between the location of and the location where the magnetic field is calculated, and is a unit vector in the direction of .
A slightly more general way of relating the current formula_6 to the -field is through Ampère's law:
where the line integral is over any arbitrary loop and formula_6enc is the current enclosed by that loop. Ampère's law is always valid for steady currents and can be used to calculate the -field for certain highly symmetric situations such as an infinite wire or an infinite solenoid.
In a modified form that accounts for time varying electric fields, Ampère's law is one of four Maxwell's equations that describe electricity and magnetism.
Force on moving charges and current.
Force on a charged particle.
A charged particle moving in a -field experiences a "sideways" force that is proportional to the strength of the magnetic field, the component of the velocity that is perpendicular to the magnetic field and the charge of the particle. This force is known as the "Lorentz force", and is given by
where
 is the force, is the electric charge of the particle, is the instantaneous velocity of the particle, and is the magnetic field (in teslas).
The Lorentz force is always perpendicular to both the velocity of the particle and the magnetic field that created it. When a charged particle moves in a static magnetic field, it traces a helical path in which the helix axis is parallel to the magnetic field, and in which the speed of the particle remains constant. Because the magnetic force is always perpendicular to the motion, the magnetic field can do no work on an isolated charge. It can only do work indirectly, via the electric field generated by a changing magnetic field. It is often claimed that the magnetic force can do work to a non-elementary magnetic dipole, or to charged particles whose motion is constrained by other forces, but this is incorrect because the work in those cases is performed by the electric forces of the charges deflected by the magnetic field.
Force on current-carrying wire.
The force on a current carrying wire is similar to that of a moving charge as expected since a charge carrying wire is a collection of moving charges. A current-carrying wire feels a force in the presence of a magnetic field. The Lorentz force on a macroscopic current is often referred to as the "Laplace force".
Consider a conductor of length , cross section , and charge due to electric current . If this conductor is placed in a magnetic field of magnitude that makes an angle with the velocity of charges in the conductor, the force exerted on a single charge is
so, for charges where 
the force exerted on the conductor is
where .
Direction of force.
The direction of force on a charge or a current can be determined by a mnemonic known as the "right-hand rule" (see the figure). Using the right hand and pointing the thumb in the direction of the moving positive charge or positive current and the fingers in the direction of the magnetic field the resulting force on the charge points outwards from the palm. The force on a negatively charged particle is in the opposite direction. If both the speed and the charge are reversed then the direction of the force remains the same. For that reason a magnetic field measurement (by itself) cannot distinguish whether there is a positive charge moving to the right or a negative charge moving to the left. (Both of these cases produce the same current.) On the other hand, a magnetic field combined with an electric field "can" distinguish between these, see Hall effect below.
An alternative mnemonic to the right hand rule Flemings's left hand rule.
Relation between H and B.
The formulas derived for the magnetic field above are correct when dealing with the entire current. A magnetic material placed inside a magnetic field, though, generates its own bound current, which can be a challenge to calculate. (This bound current is due to the sum of atomic sized current loops and the spin of the subatomic particles such as electrons that make up the material.) The -field as defined above helps factor out this bound current; but to see how, it helps to introduce the concept of "magnetization" first.
Magnetization.
The "magnetization" vector field represents how strongly a region of material is magnetized. It is defined as the net magnetic dipole moment per unit volume of that region. The magnetization of a uniform magnet is therefore a material constant, equal to the magnetic moment of the magnet divided by its volume. Since the SI unit of magnetic moment is A·m2, the SI unit of magnetization is ampere per meter, identical to that of the -field.
The magnetization field of a region points in the direction of the average magnetic dipole moment in that region. Magnetization field lines, therefore, begin near the magnetic south pole and ends near the magnetic north pole. (Magnetization does not exist outside of the magnet.)
In the Amperian loop model, the magnetization is due to combining many tiny Amperian loops to form a resultant current called "bound current". This bound current, then, is the source of the magnetic field due to the magnet. (See Magnetic dipoles below and for more information.) Given the definition of the magnetic dipole, the magnetization field follows a similar law to that of Ampere's law:
where the integral is a line integral over any closed loop and is the 'bound current' enclosed by that closed loop.
In the magnetic pole model, magnetization begins at and ends at magnetic poles. If a given region, therefore, has a net positive 'magnetic pole strength' (corresponding to a north pole) then it has more magnetization field lines entering it than leaving it. Mathematically this is equivalent to:
where the integral is a closed surface integral over the closed surface and is the 'magnetic charge' (in units of magnetic flux) enclosed by . (A closed surface completely surrounds a region with no holes to let any field lines escape.) The negative sign occurs because the magnetization field moves from south to north.
H-field and magnetic materials.
In SI units, the H-field is related to the B-field by
In terms of the H-field, Ampere's law is
where represents the 'free current' enclosed by the loop so that the line integral of does not depend at all on the bound currents. For the differential equivalent of this equation see Maxwell's equations. Ampere's law leads to the boundary condition 
where is the surface free current density and the unit normal formula_18 points in the direction from medium 2 to medium 1.
Similarly, a surface integral of over any closed surface is independent of the free currents and picks out the 'magnetic charges' within that closed surface:
which does not depend on the free currents.
The -field, therefore, can be separated into two independent parts:
where is the applied magnetic field due only to the free currents and is the demagnetizing field due only to the bound currents.
The magnetic -field, therefore, re-factors the bound current in terms of 'magnetic charges'. The field lines loop only around 'free current' and, unlike the magnetic field, begins and ends near magnetic poles as well.
Magnetism.
Most materials respond to an applied -field by producing their own magnetization and therefore their own -field. Typically, the response is weak and exists only when the magnetic field is applied. The term "magnetism" describes how materials respond on the microscopic level to an applied magnetic field and is used to categorize the magnetic phase of a material. Materials are divided into groups based upon their magnetic behavior:
In the case of paramagnetism and diamagnetism, the magnetization is often proportional to the applied magnetic field such that:
where is a material dependent parameter called the permeability. In some cases the permeability may be a second rank tensor so that may not point in the same direction as . These relations between and are examples of constitutive equations. However, superconductors and ferromagnets have a more complex to relation; see magnetic hysteresis.
Energy stored in magnetic fields.
Energy is needed to generate a magnetic field both to work against the electric field that a changing magnetic field creates and to change the magnetization of any material within the magnetic field. For non-dispersive materials this same energy is released when the magnetic field is destroyed so that this energy can be modeled as being stored in the magnetic field.
For linear, non-dispersive, materials (such that where is frequency-independent), the energy density is:
If there are no magnetic materials around then can be replaced by . The above equation cannot be used for nonlinear materials, though; a more general expression given below must be used.
In general, the incremental amount of work per unit volume needed to cause a small change of magnetic field is:
Once the relationship between and is known this equation is used to determine the work needed to reach a given magnetic state. For hysteretic materials such as ferromagnets and superconductors, the work needed also depends on how the magnetic field is created. For linear non-dispersive materials, though, the general equation leads directly to the simpler energy density equation given above.
Electromagnetism: the relationship between magnetic and electric fields.
Faraday's Law: Electric force due to a changing B-field.
A changing magnetic field, such as a magnet moving through a conducting coil, generates an electric field (and therefore tends to drive a current in such a coil). This is known as "Faraday's law" and forms the basis of many electrical generators and electric motors.
Mathematically, Faraday's law is:
where formula_25 is the electromotive force (or "EMF", the voltage generated around a closed loop) and is the "magnetic flux"—the product of the area times the magnetic field normal to that area. (This definition of magnetic flux is why is often referred to as "magnetic flux density".)
The negative sign represents the fact that any current generated by a changing magnetic field in a coil produces a magnetic field that "opposes" the "change" in the magnetic field that induced it. This phenomenon is known as Lenz's law.
This integral formulation of Faraday's law can be converted into a differential form, which applies under slightly different conditions. This form is covered as one of Maxwell's equations below.
Maxwell's correction to Ampère's Law: The magnetic field due to a changing electric field.
Similar to the way that a changing magnetic field generates an electric field, a changing electric field generates a magnetic field. This fact is known as "Maxwell's correction to Ampère's law". Maxwell's correction to Ampère's Law bootstrap together with Faraday's law of induction to form electromagnetic waves, such as light. Thus, a changing electric field generates a changing magnetic field, which generates a changing electric field again.
Maxwell's correction to Ampère law is applied as an additive term to Ampere's law given above. This additive term is proportional to the time rate of change of the electric flux and is similar to Faraday's law above but with a different and positive constant out front. (The electric flux through an area is proportional to the area times the perpendicular part of the electric field.)
This full Ampère law including the correction term is known as the Maxwell–Ampère equation. It is not commonly given in integral form because the effect is so small that it can typically be ignored in most cases where the integral form is used. The Maxwell term "is" critically important in the creation and propagation of electromagnetic waves. These, though, are usually described using the differential form of this equation given below.
Maxwell's equations.
Like all vector fields, a magnetic field has two important mathematical properties that relates it to its "sources". (For the "sources" are currents and changing electric fields.) These two properties, along with the two corresponding properties of the electric field, make up "Maxwell's Equations". Maxwell's Equations together with the Lorentz force law form a complete description of classical electrodynamics including both electricity and magnetism.
The first property is the divergence of a vector field , , which represents how 'flows' outward from a given point. As discussed above, a -field line never starts or ends at a point but instead forms a complete loop. This is mathematically equivalent to saying that the divergence of is zero. (Such vector fields are called solenoidal vector fields.) This property is called Gauss's law for magnetism and is equivalent to the statement that there are no isolated magnetic poles or magnetic monopoles. The electric field on the other hand begins and ends at electric charges so that its divergence is non-zero and proportional to the charge density (See Gauss's law).
The second mathematical property is called the curl, such that represents how curls or 'circulates' around a given point. The result of the curl is called a 'circulation source'. The equations for the curl of and of are called the Ampère–Maxwell equation and Faraday's law respectively. They represent the differential forms of the integral equations given above.
The complete set of Maxwell's equations then are:
where = complete microscopic current density and is the charge density.
Technically, is a pseudovector (also called an "axial vector") due to being defined by a vector cross product. (See diagram.)
As discussed above, materials respond to an applied electric field and an applied magnetic field by producing their own internal 'bound' charge and current distributions that contribute to and but are difficult to calculate. To circumvent this problem, and fields are used to re-factor Maxwell's equations in terms of the "free current density" and "free charge density" :
These equations are not any more general than the original equations (if the 'bound' charges and currents in the material are known). They also must be supplemented by the relationship between and as well as that between and . On the other hand, for simple relationships between these quantities this form of Maxwell's equations can circumvent the need to calculate the bound charges and currents.
Electric and magnetic fields: different aspects of the same phenomenon.
According to the special theory of relativity, the partition of the electromagnetic force into separate electric and magnetic components is not fundamental, but varies with the observational frame of reference: An electric force perceived by one observer may be perceived by another (in a different frame of reference) as a magnetic force, or a mixture of electric and magnetic forces.
Formally, special relativity combines the electric and magnetic fields into a rank-2 tensor, called the "electromagnetic tensor". Changing reference frames "mixes" these components. This is analogous to the way that special relativity "mixes" space and time into spacetime, and mass, momentum and energy into four-momentum.
Magnetic vector potential.
In advanced topics such as quantum mechanics and relativity it is often easier to work with a potential formulation of electrodynamics rather than in terms of the electric and magnetic fields. In this representation, the "vector potential" , and the scalar potential , are defined such that:
The vector potential may be interpreted as a "generalized potential momentum per unit charge" just as is interpreted as a "generalized potential energy per unit charge".
Maxwell's equations when expressed in terms of the potentials can be cast into a form that agrees with special relativity with little effort. In relativity together with forms the four-potential, analogous to the four-momentum that combines the momentum and energy of a particle. Using the four potential instead of the electromagnetic tensor has the advantage of being much simpler—and it can be easily modified to work with quantum mechanics.
Quantum electrodynamics.
In modern physics, the electromagnetic field is understood to be not a "classical" field, but rather a quantum field; it is represented not as a vector of three numbers at each point, but as a vector of three quantum operators at each point. The most accurate modern description of the electromagnetic interaction (and much else) is "quantum electrodynamics" (QED), which is incorporated into a more complete theory known as the "Standard Model of particle physics".
In QED, the magnitude of the electromagnetic interactions between charged particles (and their antiparticles) is computed using perturbation theory. These rather complex formulas produce a remarkable pictorial representation as Feynman diagrams in which virtual photons are exchanged.
Predictions of QED agree with experiments to an extremely high degree of accuracy: currently about 10−12 (and limited by experimental errors); for details see precision tests of QED. This makes QED one of the most accurate physical theories constructed thus far.
All equations in this article are in the classical approximation, which is less accurate than the quantum description mentioned here. However, under most everyday circumstances, the difference between the two theories is negligible.
Important uses and examples of magnetic field.
Earth's magnetic field.
The Earth's magnetic field is thought to be produced by convection currents in the outer liquid of Earth's core. The Dynamo theory proposes that these movements produce electric currents that, in turn, produce the magnetic field.
The presence of this field causes a compass, placed anywhere within it, to rotate so that the "north pole" of the magnet in the compass points roughly north, toward Earth's North Magnetic Pole. This is the traditional definition of the "north pole" of a magnet, although other equivalent definitions are also possible.
One confusion that arises from this definition is that, if Earth itself is considered as a magnet, the "south" pole of that magnet would be the one nearer the north magnetic pole, and vice versa. The north magnetic pole is so-named not because of the polarity of the field there but because of its geographical location. The north and south poles of a permanent magnet are so-called because they are "north-seeking" and "south-seeking", respectively.
The figure is a sketch of Earth's magnetic field represented by field lines. For most locations, the magnetic field has a significant up/down component in addition to the north/south component. (There is also an east/west component, as Earth's magnetic and geographical poles do not coincide.) The magnetic field can be visualised as a bar magnet buried deep in Earth's interior.
Earth's magnetic field is not constant—the strength of the field and the location of its poles vary. Moreover, the poles periodically reverse their orientation in a process called geomagnetic reversal. The most recent reversal occurred 780,000 years ago.
Rotating magnetic fields.
The "rotating magnetic field" is a key principle in the operation of alternating-current motors. A permanent magnet in such a field rotates so as to maintain its alignment with the external field. This effect was conceptualized by Nikola Tesla, and later utilized in his, and others', early AC (alternating current) electric motors.
A rotating magnetic field can be constructed using two orthogonal coils with 90 degrees phase difference in their AC currents. However, in practice such a system would be supplied through a three-wire arrangement with unequal currents.
This inequality would cause serious problems in standardization of the conductor size and so, to overcome it, three-phase systems are used where the three currents are equal in magnitude and have 120 degrees phase difference. Three similar coils having mutual geometrical angles of 120 degrees create the rotating magnetic field in this case. The ability of the three-phase system to create a rotating field, utilized in electric motors, is one of the main reasons why three-phase systems dominate the world's electrical power supply systems.
Synchronous motors use DC-voltage-fed rotor windings, which lets the excitation of the machine be controlled—and induction motors use short-circuited rotors (instead of a magnet) following the rotating magnetic field of a multicoiled stator. The short-circuited turns of the rotor develop eddy currents in the rotating field of the stator, and these currents in turn move the rotor by the Lorentz force.
In 1882, Nikola Tesla identified the concept of the rotating magnetic field. In 1885, Galileo Ferraris independently researched the concept. In 1888, Tesla gained for his work. Also in 1888, Ferraris published his research in a paper to the "Royal Academy of Sciences" in Turin.
Hall effect.
The charge carriers of a current carrying conductor placed in a transverse magnetic field experience a sideways Lorentz force; this results in a charge separation in a direction perpendicular to the current and to the magnetic field. The resultant voltage in that direction is proportional to the applied magnetic field. This is known as the "Hall effect".
The "Hall effect" is often used to measure the magnitude of a magnetic field. It is used as well to find the sign of the dominant charge carriers in materials such as semiconductors (negative electrons or positive holes).
Magnetic circuits.
An important use of is in "magnetic circuits" where inside a linear material. Here, is the magnetic permeability of the material. This result is similar in form to Ohm's law , where is the current density, is the conductance and is the electric field. Extending this analogy, the counterpart to the macroscopic Ohm's law () is:
where formula_37 is the magnetic flux in the circuit, formula_38 is the magnetomotive force applied to the circuit, and is the reluctance of the circuit. Here the reluctance is a quantity similar in nature to resistance for the flux.
Using this analogy it is straightforward to calculate the magnetic flux of complicated magnetic field geometries, by using all the available techniques of circuit theory.
Magnetic dipoles.
The magnetic field of a magnetic dipole is depicted in the figure. From outside, the ideal magnetic dipole is identical to that of an ideal electric dipole of the same strength. Unlike the electric dipole, a magnetic dipole is properly modeled as a current loop having a current and an area . Such a current loop has a magnetic moment of:
where the direction of is perpendicular to the area of the loop and depends on the direction of the current using the right-hand rule. An ideal magnetic dipole is modeled as a real magnetic dipole whose area has been reduced to zero and its current increased to infinity such that the product is finite. This model clarifies the connection between angular momentum and magnetic moment, which is the basis of the Einstein–de Haas effect "rotation by magnetization" and its inverse, the Barnett effect or "magnetization by rotation". Rotating the loop faster (in the same direction) increases the current and therefore the magnetic moment, for example.
It is sometimes useful to model the magnetic dipole similar to the electric dipole with two equal but opposite magnetic charges (one south the other north) separated by distance . This model produces an -field not a -field. Such a model is deficient, though, both in that there are no magnetic charges and in that it obscures the link between electricity and magnetism. Further, as discussed above it fails to explain the inherent connection between angular momentum and magnetism.
Magnetic monopole (hypothetical).
A "magnetic monopole" is a hypothetical particle (or class of particles) that has, as its name suggests, only one magnetic pole (either a north pole or a south pole). In other words, it would possess a "magnetic charge" analogous to an electric charge. Magnetic field lines would start or end on magnetic monopoles, so if they exist, they would give exceptions to the rule that magnetic field lines neither start nor end.
Modern interest in this concept stems from particle theories, notably Grand Unified Theories and superstring theories, that predict either the existence, or the possibility, of magnetic monopoles. These theories and others have inspired extensive efforts to search for monopoles. Despite these efforts, no magnetic monopole has been observed to date.
In recent research, materials known as spin ices can simulate monopoles, but do not contain actual monopoles.

</doc>
<doc id="65890" url="https://en.wikipedia.org/wiki?curid=65890" title="Magnetic flux">
Magnetic flux

In physics, specifically electromagnetism, the magnetic flux (often denoted or ) through a surface is the surface integral of the normal component of the magnetic field B passing through that surface. The SI unit of magnetic flux is the weber (Wb) (in derived units: volt-seconds), and the CGS unit is the maxwell. Magnetic flux is usually measured with a fluxmeter, which contains measuring coils and electronics, that evaluates the change of voltage in the measuring coils to calculate the magnetic flux.
Description.
The magnetic interaction is described in terms of a vector field, where each point in space (and time) is associated with a vector that determines what force a moving charge would experience at that point (see Lorentz force). Since a vector field is quite difficult to visualize at first, in elementary physics one may instead visualize this field with field lines. The magnetic flux through some surface, in this simplified picture, is proportional to the number of field lines passing through that surface (in some contexts, the flux may be defined to be precisely the number of field lines passing through that surface; although technically misleading, this distinction is not important). Note that the magnetic flux is the "net" number of field lines passing through that surface; that is, the number passing through in one direction minus the number passing through in the other direction (see below for deciding in which direction the field lines carry a positive sign and in which they carry a negative sign).
In more advanced physics, the field line analogy is dropped and the magnetic flux is properly defined as the surface integral of the normal component of the magnetic field passing through a surface. If the magnetic field is constant, the magnetic flux passing through a surface of vector area S is
where B is the magnitude of the magnetic field (the magnetic flux density) having the unit of Wb/m2 (tesla), S is the area of the surface, and "θ" is the angle between the magnetic field lines and the normal (perpendicular) to S. For a varying magnetic field, we first consider the magnetic flux through an infinitesimal area element dS, where we may consider the field to be constant: 
A generic surface, S, can then be broken into infinitesimal elements and the total magnetic flux through the surface is then the surface integral
From the definition of the magnetic vector potential A and the fundamental theorem of the curl the magnetic flux may also be defined as:
where the line integral is taken over the boundary of the surface "S", which is denoted ∂"S".
Magnetic flux through a closed surface.
Gauss's law for magnetism, which is one of the four Maxwell's equations, states that the total magnetic flux through a closed surface is equal to zero. (A "closed surface" is a surface that completely encloses a volume(s) with no holes.) This law is a consequence of the empirical observation that magnetic monopoles have never been found.
In other words, Gauss's law for magnetism is the statement:
for any closed surface "S".
Magnetic flux through an open surface.
While the magnetic flux through a closed surface is always zero, the magnetic flux through an open surface need not be zero and is an important quantity in electromagnetism. For example, a change in the magnetic flux passing through a loop of conductive wire will cause an electromotive force, and therefore an electric current, in the loop. The relationship is given by Faraday's law:
where
The two equations for the EMF are, firstly, the work per unit charge done against the Lorentz force in moving a test charge around the (possibly moving) surface boundary ∂Σ and, secondly, as the change of magnetic flux through the open surface Σ. This equation is the principle behind an electrical generator.
Comparison with electric flux.
By way of contrast, Gauss's law for electric fields, another of Maxwell's equations, is
where
Note that the flux of E through a closed surface is "not" always zero; this indicates the presence of "electric monopoles", that is, free positive or negative charges.

</doc>
<doc id="279624" url="https://en.wikipedia.org/wiki?curid=279624" title="Magnetic flux quantum">
Magnetic flux quantum

The magnetic flux, represented by the symbol , threading some contour or loop is defined as the magnetic inductance multiplied by the loop area , i.e. . Obviously, both and can be arbitrary and so is . However, if one deals with the superconducting loop or a hole in a bulk superconductor, it turns out that the magnetic flux threading such a hole/loop is quantized.
The (superconducting) magnetic flux quantum 
 is a combination of fundamental physical constants: the Planck constant and the electron charge . Its value is, therefore, the same for any superconductor.
The phenomenon of flux quantization was discovered experimentally by B. S. Deaver and W. M. Fairbank and, independently, by R. Doll and M. Näbauer, in 1961. The quantization of magnetic flux is closely related to the Little–Parks effect, but was predicted earlier by Fritz London in 1948 using a phenomenological model.
The inverse of the flux quantum, , is called the Josephson constant, and is denoted "K"J. It is the constant of proportionality of the Josephson effect, relating the potential difference across a Josephson junction to the frequency of the irradiation. The Josephson effect is very widely used to provide a standard for high-precision measurements of potential difference, which (since 1990) have been related to a fixed, "conventional" value of the Josephson constant, denoted "K"J–90.
Introduction.
The superconducting properties in each point of the superconductor are described by the "complex" quantum mechanical wave function — the superconducting order parameter. As any complex function can be written as , where is the amplitude and is the phase. It is obvious that changing the phase by will not change and, correspondingly, will not change any physical properties. However, in the superconductor of non-trivial topology, e.g. superconductor with the hole or superconducting loop/cylinder, the phase may continuously change from some value to the value as one goes around the hole/loop and comes to the same starting point. If this is so, then one has magnetic flux quanta trapped in the hole/loop.
Due to Meissner effect the magnetic induction inside the superconductor is zero. More exactly, magnetic field penetrates into a superconductor over a small distance called London's magnetic field penetration depth (denoted and usually ). The screening currents also flow in this -layer near the surface, creating magnetization inside the superconductor, which perfectly compensates the applied field , thus resulting in inside the superconductor.
It is important to note that the magnetic flux frozen in a loop/hole (plus its -layer) will always be quantized. However, the value of the flux quantum is equal to only when the path/trajectory around the hole described above can be chosen so that it lays in the superconducting region without screening currents, i.e. several away from the surface. There are geometries where this condition cannot be satisfied, e.g. a loop made of very thin () superconducting wire or the cylinder with the similar wall thickness. In the latter case, the flux has a quantum different from .
The flux quantization is a key idea behind a SQUID, which is one of the most sensitive magnetometers available.
Flux quantization also play in important role in the physics of a type II superconductors. When such a superconductor (now without any holes) is placed in a magnetic field with the strength between the first critical field and the second critical field , the field partially penetrates into the superconductor in a form of Abrikosov vortices. The Abrikosov vortex consists of a normal core—a cylinder of the normal (non-superconducting) phase with a diameter on the order of the , the superconducting coherence length. The normal core plays a role of a hole in the superconducting phase. The magnetic field lines pass along this normal core through the whole sample. The screening currents circulate in the -vicinity of the core and screen the rest of the superconductor from the magnetic field in the core. In total, each such Abrikosov vortex carries one quantum of magnetic flux . Although theoretically, it is possible to have more than one flux quantum per hole, the Abrikosov vortices with are unstable and split into several vortices with . In a real hole the states with are stable as the real hole cannot split itself into several smaller holes.
Measuring the magnetic flux.
The magnetic flux quantum may be measured with great precision by exploiting the Josephson effect. When coupled with the measurement of the von Klitzing constant , this provides the most precise values of Planck's constant obtained to date. This is remarkable since is generally associated with the behavior of microscopically small systems, whereas the quantization of magnetic flux in a superconductor and the quantum Hall effect are both collective phenomena associated with thermodynamically large numbers of particles.

</doc>
<doc id="540979" url="https://en.wikipedia.org/wiki?curid=540979" title="Magnetic moment">
Magnetic moment

The magnetic moment of a magnet is a quantity that determines the torque it will experience in an external magnetic field. A loop of electric current, a bar magnet, an electron (revolving around a nucleus), a molecule, and a planet all have magnetic moments.
The magnetic moment may be considered to be a vector having a magnitude and direction. The direction of the magnetic moment points from the south to north pole of the magnet. The magnetic field produced by the magnet is proportional to its magnetic moment. More precisely, the term "magnetic moment" normally refers to a system's magnetic dipole moment, which produces the first term in the multipole expansion of a general magnetic field. The dipole component of an object's magnetic field is symmetric about the direction of its magnetic dipole moment, and decreases as the inverse cube of the distance from the object.
Definition.
The magnetic moment is defined as a vector relating the aligning torque on the object from an externally applied magnetic field to the field vector itself. The relationship is given by
where formula_2 is the torque acting on the dipole and formula_3 is the external magnetic field, and formula_4 is the magnetic moment.
This definition is based on how one would measure the magnetic moment, in principle, of an unknown sample.
Units.
The unit for magnetic moment is not a base unit in the International System of Units (SI). As the torque is measured in newton-meters (N·m) and the magnetic field in teslas (T), the magnetic moment is measured in newton-meters per tesla. This has equivalents in other base units:
where A is amperes and J is joules.
In the CGS system, there are several different sets of electromagnetism units, of which the main ones are ESU, Gaussian, and EMU. Among these, there are two alternative (non-equivalent) units of magnetic dipole moment:
where statA is statamperes, cm is centimeters, erg is ergs, G is gauss and abA is abamperes. The ratio of these two non-equivalent CGS units (EMU/ESU) is equal to the speed of light in free space, expressed in cm·s−1.
All formulae in this article are correct in SI units; they may need to be changed for use in other unit systems. For example, in SI units, a loop of current with current "I" and area "A" has magnetic moment "IA" (see below), but in Gaussian units the magnetic moment is "IA"/"c".
Two representations of the cause of the magnetic moment.
The preferred classical explanation of a magnetic moment has changed over time. Before the 1930s, textbooks explained the moment using hypothetical magnetic point charges. Since then, most have defined it in terms of Ampèrian currents. In magnetic materials, the cause of the magnetic moment are the spin and orbital angular momentum states of the electrons, and whether atoms in one region are aligned with atoms in another.
Magnetic pole representation.
The sources of magnetic moments in materials can be represented by poles in analogy to electrostatics. Consider a bar magnet which has magnetic poles of equal magnitude but opposite polarity. Each pole is the source of magnetic force which weakens with distance. Since always come in pairs, their forces partially cancel each other because while one pole pulls, the other repels. This cancellation is greatest when the poles are close to each other i.e. when the bar magnet is short. The magnetic force produced by a bar magnet, at a given point in space, therefore depends on two factors: the strength "p" of its poles (magnetic pole strength), and the vector ℓ separating them. The moment is related to the fictitious poles as
It points in the direction from South to North pole. The analogy with electric dipoles should not be taken too far because magnetic dipoles are associated with angular momentum (see Magnetic moment and angular momentum). Nevertheless, magnetic poles are very useful for magnetostatic calculations, particularly in applications to ferromagnets. Practitioners using the magnetic pole approach generally represent the magnetic field by the irrotational field H, in analogy to the electric field E.
Integral representation.
We start from the definition of the differential magnetic moment pseudovector :
where × is the vector cross product, r is the position vector, and j is the electric current density.
It is very similar to the differential angular momentum, defined as:
where ρ is the mass density and "v" is the velocity vector.
Like in every pseudovector, by convention the direction of the cross product is given by the right hand grip rule.
Practitioners using the current loop model generally represent the magnetic field by the solenoidal field B, analogous to the electrostatic field D.
The integral magnetic moment of a charge distribution is therefore:
Let us start with a point particle; in this simple situation the magnetic moment is:
where r is the position of the electric charge "q" relative to the center of the circle and v is the instantaneous velocity of the charge, giving an electric current density "j". 
On the other hand for a point particle the angular momentum is defined as:
and in the planar case:
by defining the electric current with a vector area S ("x", "y", and "z" coordinates of this vector are the areas of projections of the loop onto the "yz", "zx", and "xy" planes):
Then by Stokes' theorem, integral magnetic moment then becomes expressible as:
The factor 1/2 in our definition above is only due to historical reason: the old definition of the magnetic moment was this last integral equation. If one had started from a differential definition:
then the coherent integral expression would have been:
Magnetic moment of a solenoid.
A generalization of the above current loop is a coil, or solenoid. Its moment is the vector sum of the moments of individual turns. If the solenoid has "N" identical turns (single-layer winding) and vector area S,
Magnetic moment and angular momentum.
The magnetic moment has a close connection with angular momentum called the gyromagnetic effect. This effect is expressed on a macroscopic scale in the Einstein-de Haas effect, or "rotation by magnetization," and its inverse, the Barnett effect, or "magnetization by rotation." In particular, when a magnetic moment is subject to a torque in a magnetic field that tends to align it with the applied magnetic field, the moment precesses (rotates about the axis of the applied field). This is a consequence of the concomitance of magnetic moment and angular momentum, that in case of charged massive particles corresponds to the concomitance of charge and mass in a particle.
Viewing a magnetic dipole as a rotating charged particle brings out the close connection between magnetic moment and angular momentum. Both the magnetic moment and the angular momentum increase with the rate of rotation. The ratio of the two is called the gyromagnetic ratio and is simply the half of the charge-to-mass ratio.
For a spinning charged solid with a uniform charge density to mass density ratio, the gyromagnetic ratio is equal to half the charge-to-mass ratio. This implies that a more massive assembly of charges spinning with the same angular momentum will have a proportionately weaker magnetic moment, compared to its lighter counterpart. Even though atomic particles cannot be accurately described as spinning charge distributions of uniform charge-to-mass ratio, this general trend can be observed in the atomic world, where the intrinsic angular momentum (spin) of each type of particle is a constant: a small half-integer times the reduced Planck constant "ħ". This is the basis for defining the magnetic moment units of Bohr magneton (assuming charge-to-mass ratio of the electron) and nuclear magneton (assuming charge-to-mass ratio of the proton).
Effects of an external magnetic field on a magnetic moment.
Force on a moment.
A magnetic moment in an externally produced magnetic field has a potential energy "U":
In a case when the external magnetic field is non-uniform, there will be a
force, proportional to the magnetic field gradient, acting on the magnetic moment itself. There has been some discussion on how to calculate the force acting on a magnetic dipole. There are two expressions for the force acting on a magnetic dipole, depending on whether the is a current loop or two monopoles (analogous to the electric dipole). The force obtained in the case of a current loop model is
In the case of a pair of monopoles being used (i.e. electric dipole model)
and one can be put in terms of the other via the relation
In all these expressions μ is the dipole and B is the magnetic field at its position. Note that if there are no currents or time-varying electrical fields and the two expressions agree.
An electron, nucleus, or atom placed in a uniform magnetic field will precess with a frequency known as the Larmor frequency. See Resonance.
Magnetic dipoles.
A magnetic dipole is the limit of either a current loop or a pair of poles as the dimensions of the source are reduced to zero while keeping the moment constant. As long as these limits only apply to fields far from the sources, they are equivalent. However, the two models give different predictions for the internal field (see below).
External magnetic field produced by a magnetic dipole moment.
Any system possessing a net magnetic dipole moment m will produce a dipolar magnetic field (described below) in the space surrounding the system. While the net magnetic field produced by the system can also have higher-order multipole components, those will drop off with distance more rapidly, so that only the dipolar component will dominate the magnetic field of the system at distances far away from it.
The vector potential of magnetic field produced by magnetic moment m is
For relation between the notions of magnetic moment and magnetization see magnetization.

</doc>
<doc id="175470" url="https://en.wikipedia.org/wiki?curid=175470" title="Magnetic monopole">
Magnetic monopole

A magnetic monopole is a hypothetical elementary particle in particle physics that is an isolated magnet with only one magnetic pole (a north pole without a south pole or vice versa). In more technical terms, a magnetic monopole would have a net "magnetic charge". Modern interest in the concept stems from particle theories, notably the grand unified and superstring theories, which predict their existence.
Magnetism in bar magnets and electromagnets does not arise from magnetic monopoles. There is no conclusive experimental evidence that magnetic monopoles exist at all in our universe.
Some condensed matter systems contain effective (non-isolated) magnetic monopole "quasi"-particles, or contain phenomena that are mathematically analogous to magnetic monopoles.
Historical background.
Pre-twentieth century.
Many early scientists attributed the magnetism of lodestones to two different "magnetic fluids" ("effluvia"), a north-pole fluid at one end and a south-pole fluid at the other, which attracted and repelled each other in analogy to positive and negative electric charge. However, an improved understanding of electromagnetism in the nineteenth century showed that the magnetism of lodestones was properly explained by Ampère's circuital law, not magnetic monopole fluids. Gauss's law for magnetism, one of Maxwell's equations, is the mathematical statement that magnetic monopoles do not exist. Nevertheless, it was pointed out by Pierre Curie in 1894 that magnetic monopoles "could" conceivably exist, despite not having been seen so far.
Twentieth century.
The "quantum" theory of magnetic charge started with a paper by the physicist Paul A.M. Dirac in 1931. In this paper, Dirac showed that if "any" magnetic monopoles exist in the universe, then all electric charge in the universe must be quantized (Dirac quantization condition). The electric charge "is", in fact, quantized, which is consistent with (but does not prove) the existence of monopoles.
Since Dirac's paper, several systematic monopole searches have been performed. Experiments in 1975 and 1982 produced candidate events that were initially interpreted as monopoles, but are now regarded as inconclusive. Therefore, it remains an open question whether monopoles exist.
Further advances in theoretical particle physics, particularly developments in grand unified theories and quantum gravity, have led to more compelling arguments (detailed below) that monopoles do exist. Joseph Polchinski, a string-theorist, described the existence of monopoles as "one of the safest bets that one can make about physics not yet seen". These theories are not necessarily inconsistent with the experimental evidence. In some theoretical models, magnetic monopoles are unlikely to be observed, because they are too massive to be created in particle accelerators (see below), and also too rare in the Universe to enter a particle detector with much probability.
Some condensed matter systems propose a structure superficially similar to a magnetic monopole, known as a flux tube. The ends of a flux tube form a magnetic dipole, but since they move independently, they can be treated for many purposes as independent magnetic monopole quasiparticles. Since 2009, numerous news reports from the popular media have incorrectly described these systems as the long-awaited discovery of the magnetic monopoles, but the two phenomena are only superficially related to one another. These condensed-matter systems continue to be an area of active research. (See "Monopoles" in condensed-matter systems below.)
Poles and magnetism in ordinary matter.
All matter ever isolated to date—including every atom on the periodic table and every particle in the standard model—has zero magnetic monopole charge. Therefore, the ordinary phenomena of magnetism and magnets have nothing to do with magnetic monopoles.
Instead, magnetism in ordinary matter comes from two sources. First, electric currents create magnetic fields according to Ampère's law. Second, many elementary particles have an "intrinsic" magnetic moment, the most important of which is the electron magnetic dipole moment. (This magnetism is related to quantum-mechanical "spin".)
Mathematically, the magnetic field of an object is often described in terms of a multipole expansion. This is an expression of the field as the sum of component fields with specific mathematical forms. The first term in the expansion is called the "monopole" term, the second is called "dipole", then "quadrupole", then "octupole", and so on. Any of these terms can be present in the multipole expansion of an electric field, for example. However, in the multipole expansion of a "magnetic" field, the "monopole" term is always exactly zero (for ordinary matter). A magnetic monopole, if it exists, would have the defining property of producing a magnetic field whose "monopole" term is nonzero.
A magnetic dipole is something whose magnetic field is predominantly or exactly described by the magnetic dipole term of the multipole expansion. The term "dipole" means "two poles", corresponding to the fact that a dipole magnet typically contains a "north pole" on one side and a "south pole" on the other side. This is analogous to an electric dipole, which has positive charge on one side and negative charge on the other. However, an electric dipole and magnetic dipole are fundamentally quite different. In an electric dipole made of ordinary matter, the positive charge is made of protons and the negative charge is made of electrons, but a magnetic dipole does "not" have different types of matter creating the north pole and south pole. Instead, the two magnetic poles arise simultaneously from the aggregate effect of all the currents and intrinsic moments throughout the magnet. Because of this, the two poles of a magnetic dipole must always have equal and opposite strength, and the two poles cannot be separated from each other.
Maxwell's equations.
Maxwell's equations of electromagnetism relate the electric and magnetic fields to each other and to the motions of electric charges. The standard equations provide for electric charges, but they posit no magnetic charges. Except for this difference, the equations are symmetric under the interchange of the electric and magnetic fields. In fact, symmetric Maxwell's equations can be written when all charges (and hence electric currents) are zero, and this is how the electromagnetic wave equation is derived.
Fully symmetric Maxwell's equations can also be written if one allows for the possibility of "magnetic charges" analogous to electric charges. With the inclusion of a variable for the density of these magnetic charges, say , there will also be a "magnetic current density" variable in the equations, .
If magnetic charges do not exist – or if they do exist but are not present in a region of space – then the new terms in Maxwell's equations are all zero, and the extended equations reduce to the conventional equations of electromagnetism such as (where is divergence and is the magnetic field).
In Gaussian cgs units.
The extended Maxwell's equations are as follows, in Gaussian cgs units:
In these equations is the "magnetic charge density", is the "magnetic current density", and is the "magnetic charge" of a test particle, all defined analogously to the related quantities of electric charge and current; is the particle's velocity and is the speed of light. For all other definitions and details, see Maxwell's equations. For the equations in nondimensionalized form, remove the factors of .
In SI units.
In SI units, there are two conflicting units in use for magnetic charge : webers (Wb) and ampere·meters (A·m). The conversion between them is , since the units are by dimensional analysis (H is the henry – the SI unit of inductance).
Maxwell's equations then take the following forms (using the same notation above):
Tensor formulation.
Maxwell's equations in the language of tensors makes Lorentz covariance clear. The generalized equations are:
where
For a particle having only electric charge, one can express its field using a four-potential, according to the standard covariant formulation of classical electromagnetism:
However, this formula is inadequate for a particle that has both electric and magnetic charge, and we must add a term involving another potential .
This formula for the fields is often called the Cabibbo-Ferrari relation, though Shanmugadhasan proposed it earlier. The quantity is the Levi-Civita symbol, and the indices (as usual) behave according to the Einstein summation convention.
Duality transformation.
The generalized Maxwell's equations possess a certain symmetry, called a "duality transformation". One can choose any real angle , and simultaneously change the fields and charges everywhere in the universe as follows (in Gaussian units):
where the primed quantities are the charges and fields before the transformation, and the unprimed quantities are after the transformation. The fields and charges after this transformation still obey the same Maxwell's equations. The matrix is a two-dimensional rotation matrix.
Because of the duality transformation, one cannot uniquely decide whether a particle has an electric charge, a magnetic charge, or both, just by observing its behavior and comparing that to Maxwell's equations. For example, it is merely a convention, not a requirement of Maxwell's equations, that electrons have electric charge but not magnetic charge; after a transformation, it would be the other way around. The key empirical fact is that all particles ever observed have the same ratio of magnetic charge to electric charge. Duality transformations can change the ratio to any arbitrary numerical value, but cannot change the fact that all particles have the same ratio. Since this is the case, a duality transformation can be made that sets this ratio to be zero, so that all particles have no magnetic charge. This choice underlies the "conventional" definitions of electricity and magnetism.
Dirac's quantization.
One of the defining advances in quantum theory was Paul Dirac's work on developing a relativistic quantum electromagnetism. Before his formulation, the presence of electric charge was simply "inserted" into the equations of quantum mechanics (QM), but in 1931 Dirac showed that a discrete charge naturally "falls out" of QM. That is to say, we can maintain the form of Maxwell's equations and still have magnetic charges.
Consider a system consisting of a single stationary electric monopole (an electron, say) and a single stationary magnetic monopole. Classically, the electromagnetic field surrounding them has a momentum density given by the Poynting vector, and it also has a total angular momentum, which is proportional to the product , and independent of the distance between them.
Quantum mechanics dictates, however, that angular momentum is quantized in units of , so therefore the product must also be quantized. This means that if even a single magnetic monopole existed in the universe, and the form of Maxwell's equations is valid, all electric charges would then be quantized.
What are the units in which magnetic charge would be quantized? Although it would be possible simply to integrate over all space to find the total angular momentum in the above example, Dirac took a different approach. This led him to new ideas. He considered a point-like magnetic charge whose magnetic field behaves as and is directed in the radial direction, located at the origin. Because the divergence of is equal to zero almost everywhere, except for the locus of the magnetic monopole at , one can locally define the vector potential such that the curl of the vector potential equals the magnetic field .
However, the vector potential cannot be defined globally precisely because the divergence of the magnetic field is proportional to the Dirac delta function at the origin. We must define one set of functions for the vector potential on the "northern hemisphere" (the half-space above the particle), and another set of functions for the "southern hemisphere". These two vector potentials are matched at the "equator" (the plane through the particle), and they differ by a gauge transformation. The wave function of an electrically-charged particle (a "probe charge") that orbits the "equator" generally changes by a phase, much like in the Aharonov–Bohm effect. This phase is proportional to the electric charge of the probe, as well as to the magnetic charge of the source. Dirac was originally considering an electron whose wave function is described by the Dirac equation.
Because the electron returns to the same point after the full trip around the equator, the phase of its wave function must be unchanged, which implies that the phase added to the wave function must be a multiple of :
where is the vacuum permittivity, is the reduced Planck's constant, is the speed of light, and is the set of integers.
This is known as the Dirac quantization condition. The hypothetical existence of a magnetic monopole would imply that the electric charge must be quantized in certain units; also, the existence of the electric charges implies that the magnetic charges of the hypothetical magnetic monopoles, if they exist, must be quantized in units inversely proportional to the elementary electric charge.
At the time it was not clear if such a thing existed, or even had to. After all, another theory could come along that would explain charge quantization without need for the monopole. The concept remained something of a curiosity. However, in the time since the publication of this seminal work, no other widely accepted explanation of charge quantization has appeared. (The concept of local gauge invariance—see gauge theory below—provides a natural explanation of charge quantization, without invoking the need for magnetic monopoles; but only if the U(1) gauge group is compact, in which case we will have magnetic monopoles anyway.)
If we maximally extend the definition of the vector potential for the southern hemisphere, it will be defined everywhere except for a semi-infinite line stretched from the origin in the direction towards the northern pole. This semi-infinite line is called the Dirac string and its effect on the wave function is analogous to the effect of the solenoid in the Aharonov–Bohm effect. The quantization condition comes from the requirement that the phases around the Dirac string are trivial, which means that the Dirac string must be unphysical. The Dirac string is merely an artifact of the coordinate chart used and should not be taken seriously.
The Dirac monopole is a singular solution of Maxwell's equation (because it requires removing the worldline from spacetime); in more complicated theories, it is superseded by a smooth solution such as the 't Hooft–Polyakov monopole.
Topological interpretation.
Dirac string.
A gauge theory like electromagnetism is defined by a gauge field, which associates a group element to each path in space time. For infinitesimal paths, the group element is close to the identity, while for longer paths the group element is the successive product of the infinitesimal group elements along the way.
In electrodynamics, the group is U(1), unit complex numbers under multiplication. For infinitesimal paths, the group element is which implies that for finite paths parametrized by , the group element is:
The map from paths to group elements is called the Wilson loop or the holonomy, and for a U(1) gauge group it is the phase factor which the wavefunction of a charged particle acquires as it traverses the path. For a loop:
So that the phase a charged particle gets when going in a loop is the magnetic flux through the loop. When a small solenoid has a magnetic flux, there are interference fringes for charged particles which go around the solenoid, or around different sides of the solenoid, which reveal its presence.
But if all particle charges are integer multiples of , solenoids with a flux of have no interference fringes, because the phase factor for any charged particle is . Such a solenoid, if thin enough, is quantum-mechanically invisible. If such a solenoid were to carry a flux of , when the flux leaked out from one of its ends it would be indistinguishable from a monopole.
Dirac's monopole solution in fact describes an infinitesimal line solenoid ending at a point, and the location of the solenoid is the singular part of the solution, the Dirac string. Dirac strings link monopoles and antimonopoles of opposite magnetic charge, although in Dirac's version, the string just goes off to infinity. The string is unobservable, so you can put it anywhere, and by using two coordinate patches, the field in each patch can be made nonsingular by sliding the string to where it cannot be seen.
Grand unified theories.
In a U(1) gauge group with quantized charge, the group is a circle of radius . Such a U(1) gauge group is called compact. Any U(1) which comes from a Grand Unified Theory is compact – because only compact higher gauge groups make sense. The size of the gauge group is a measure of the inverse coupling constant, so that in the limit of a large-volume gauge group, the interaction of any fixed representation goes to zero.
The case of the U(1) gauge group is a special case because all its irreducible representations are of the same size – the charge is bigger by an integer amount, but the field is still just a complex number – so that in U(1) gauge field theory it is possible to take the decompactified limit with no contradiction. The quantum of charge becomes small, but each charged particle has a huge number of charge quanta so its charge stays finite. In a non-compact U(1) gauge group theory, the charges of particles are generically not integer multiples of a single unit. Since charge quantization is an experimental certainty, it is clear that the U(1) gauge group of electromagnetism is compact.
GUTs lead to compact U(1) gauge groups, so they explain charge quantization in a way that seems to be logically independent from magnetic monopoles. However, the explanation is essentially the same, because in any GUT which breaks down into a U(1) gauge group at long distances, there are magnetic monopoles.
The argument is topological:
Hence, the Dirac monopole is a topological defect in a compact U(1) gauge theory. When there is no GUT, the defect is a singularity – the core shrinks to a point. But when there is some sort of short-distance regulator on space time, the monopoles have a finite mass. Monopoles occur in lattice U(1), and there the core size is the lattice size. In general, they are expected to occur whenever there is a short-distance regulator.
String theory.
In the universe, quantum gravity provides the regulator. When gravity is included, the monopole singularity can be a black hole, and for large magnetic charge and mass, the black hole mass is equal to the black hole charge, so that the mass of the magnetic black hole is not infinite. If the black hole can decay completely by Hawking radiation, the lightest charged particles cannot be too heavy. The lightest monopole should have a mass less than or comparable to its charge in natural units.
So in a consistent holographic theory, of which string theory is the only known example, there are always finite-mass monopoles. For ordinary electromagnetism, the mass bound is not very useful because it is about same size as the Planck mass.
Mathematical formulation.
In mathematics, a (classical) gauge field is defined as a connection over a principal G-bundle over spacetime. is the gauge group, and it acts on each fiber of the bundle separately.
A "connection" on a bundle tells you how to glue fibers together at nearby points of . It starts with a continuous symmetry group which acts on the fiber , and then it associates a group element with each infinitesimal path. Group multiplication along any path tells you how to move from one point on the bundle to another, by having the element associated to a path act on the fiber .
In mathematics, the definition of bundle is designed to emphasize topology, so the notion of connection is added on as an afterthought. In physics, the connection is the fundamental physical object. One of the fundamental observations in the theory of characteristic classes in algebraic topology is that many homotopical structures of nontrivial principal bundles may be expressed as an integral of some polynomial over any connection over it. Note that a connection over a trivial bundle can never give us a nontrivial principal bundle.
If space time is the space of all possible connections of the -bundle is connected. But consider what happens when we remove a timelike worldline from spacetime. The resulting spacetime is homotopically equivalent to the topological sphere .
A principal -bundle over is defined by covering by two charts, each homeomorphic to the open 2-ball such that their intersection is homeomorphic to the strip . 2-balls are homotopically trivial and the strip is homotopically equivalent to the circle . So a topological classification of the possible connections is reduced to classifying the transition functions. The transition function maps the strip to , and the different ways of mapping a strip into are given by the first homotopy group of .
So in the -bundle formulation, a gauge theory admits Dirac monopoles provided is not simply connected, whenever there are paths that go around the group that cannot be deformed to a constant path (a path whose image consists of a single point). U(1), which has quantized charges, is not simply connected and can have Dirac monopoles while , its universal covering group, is simply connected, doesn't have quantized charges and does not admit Dirac monopoles. The mathematical definition is equivalent to the physics definition provided that, following Dirac, gauge fields are allowed which are defined only patch-wise and the gauge field on different patches are glued after a gauge transformation.
The total magnetic flux is none other than the first Chern number of the principal bundle, and depends only upon the choice of the principal bundle, and not the specific connection over it. In other words, it's a topological invariant.
This argument for monopoles is a restatement of the lasso argument for a pure U(1) theory. It generalizes to dimensions with in several ways. One way is to extend everything into the extra dimensions, so that U(1) monopoles become sheets of dimension . Another way is to examine the type of topological singularity at a point with the homotopy group .
Grand unified theories.
In more recent years, a new class of theories has also suggested the existence of magnetic monopoles.
During the early 1970s, the successes of quantum field theory and gauge theory in the development of electroweak theory and the mathematics of the strong nuclear force led many theorists to move on to attempt to combine them in a single theory known as a Grand Unified Theory (GUT). Several GUTs were proposed, most of which implied the presence of a real magnetic monopole particle. More accurately, GUTs predicted a range of particles known as dyons, of which the most basic state was a monopole. The charge on magnetic monopoles predicted by GUTs is either 1 or 2 "gD", depending on the theory.
The majority of particles appearing in any quantum field theory are unstable, and they decay into other particles in a variety of reactions that must satisfy various conservation laws. Stable particles are stable because there are no lighter particles into which they can decay and still satisfy the conservation laws. For instance, the electron has a lepton number of one and an electric charge of one, and there are no lighter particles that conserve these values. On the other hand, the muon, essentially a heavy electron, can decay into the electron plus two quanta of energy, and hence it is not stable.
The dyons in these GUTs are also stable, but for an entirely different reason. The dyons are expected to exist as a side effect of the "freezing out" of the conditions of the early universe, or a symmetry breaking. In this scenario, the dyons arise due to the configuration of the vacuum in a particular area of the universe, according to the original Dirac theory. They remain stable not because of a conservation condition, but because there is no simpler "topological" state into which they can decay.
The length scale over which this special vacuum configuration exists is called the "correlation length" of the system. A correlation length cannot be larger than causality would allow, therefore the correlation length for making magnetic monopoles must be at least as big as the horizon size determined by the metric of the expanding universe. According to that logic, there should be at least one magnetic monopole per horizon volume as it was when the symmetry breaking took place.
Cosmological models of the events following the big bang make predictions about what the horizon volume was, which lead to predictions about present-day monopole density. Early models predicted an enormous density of monopoles, in clear contradiction to the experimental evidence. This was called the "monopole problem". Its widely accepted resolution was not a change in the particle-physics prediction of monopoles, but rather in the cosmological models used to infer their present-day density. Specifically, more recent theories of cosmic inflation drastically reduce the predicted number of magnetic monopoles, to a density small enough to make it unsurprising that humans have never seen one. This resolution of the "monopole problem" was regarded as a success of cosmic inflation theory. (However, of course, it is only a noteworthy success if the particle-physics monopole prediction is correct.) For these reasons, monopoles became a major interest in the 1970s and 80s, along with the other "approachable" predictions of GUTs such as proton decay.
Many of the other particles predicted by these GUTs were beyond the abilities of current experiments to detect. For instance, a wide class of particles known as the X and Y bosons are predicted to mediate the coupling of the electroweak and strong forces, but these particles are extremely heavy and well beyond the capabilities of any reasonable particle accelerator to create.
Searches for magnetic monopoles.
A number of attempts have been made to detect magnetic monopoles. One of the simpler ones is to use a loop of superconducting wire to look for even tiny magnetic sources, a so-called "superconducting quantum interference device", or SQUID. Given the predicted density, loops small enough to fit on a lab bench would expect to see about one monopole event per year. Although there have been tantalizing events recorded, in particular the event recorded by Blas Cabrera on the night of February 14, 1982 (thus, sometimes referred to as the "Valentine's Day Monopole"), there has never been reproducible evidence for the existence of magnetic monopoles. The lack of such events places a limit on the number of monopoles of about one monopole per 1029 nucleons.
Another experiment in 1975 resulted in the announcement of the detection of a moving magnetic monopole in cosmic rays by the team led by P. Buford Price. Price later retracted his claim, and a possible alternative explanation was offered by Alvarez. In his paper it was demonstrated that the path of the cosmic ray event that was claimed to be due to a magnetic monopole could be reproduced by the path followed by a platinum nucleus decaying first to osmium, and then to tantalum.
Other experiments rely on the strong coupling of monopoles with photons, as is the case for any electrically-charged particle as well. In experiments involving photon exchange in particle accelerators, monopoles should be produced in reasonable numbers, and detected due to their effect on the scattering of the photons. The probability of a particle being created in such experiments is related to their mass – with heavier particles being less likely to be created – so by examining the results of such experiments, limits on the mass of a magnetic monopole can be calculated. The most recent such experiments suggest that monopoles with masses below do not exist, while upper limits on their mass due to the very existence of the universe – which would have collapsed by now if they were too heavy – are about .
The MoEDAL experiment, installed at the Large Hadron Collider, is currently searching for magnetic monopoles and large supersymmetric particles using layers of special plastic sheets attached to the walls around LHCb's VELO detector. The particles it is looking for will damage the sheets along their path, with various identifying features.
The Russian astrophysicist Igor Novikov claims the fields of macroscopic black holes to be potential magnetic monopoles, representing the entrance to an Einstein–Rosen bridge.
"Monopoles" in condensed-matter systems.
Since around 2003, various condensed-matter physics groups have used the term "magnetic monopole" to describe a different and largely unrelated phenomenon.
A true magnetic monopole would be a new elementary particle, and would violate the law. A monopole of this kind, which would help to explain the law of charge quantization as formulated by Paul Dirac in 1931, has never been observed in experiments.
The monopoles studied by condensed-matter groups have none of these properties. They are not a new elementary particle, but rather are an emergent phenomenon in systems of everyday particles (protons, neutrons, electrons, photons); in other words, they are quasi-particles. They are not sources for the -field (i.e., they do not violate ); instead, they are sources for other fields, for example the -field, or the "-field" (related to superfluid vorticity). They are not directly relevant to grand unified theories or other aspects of particle physics, and do not help explain charge quantization—except insofar as studies of analogous situations can help confirm that the mathematical analyses involved are sound.
There are a number of examples in condensed-matter physics where collective behavior leads to emergent phenomena that resemble magnetic monopoles in certain respects, including most prominently the spin ice materials. While these should not be confused with hypothetical elementary monopoles existing in the vacuum, they nonetheless have similar properties and can be probed using similar techniques.
Some researchers use the term magnetricity to describe the manipulation of magnetic monopole quasiparticles in spin ice, in analogy to the word "electricity".
One example of the work on magnetic monopole quasiparticles is a paper published in the journal "Science" in September 2009, in which researchers Jonathan Morris and Alan Tennant from the Helmholtz-Zentrum Berlin für Materialien und Energie (HZB) along with Santiago Grigera from Instituto de Física de Líquidos y Sistemas Biológicos (IFLYSIB, CONICET) and other colleagues from Dresden University of Technology, University of St. Andrews and Oxford University described the observation of quasiparticles resembling magnetic monopoles. A single crystal of the spin ice material dysprosium titanate was cooled to a temperature between 0.6 kelvin and 2.0 kelvin. Using observations of neutron scattering, the magnetic moments were shown to align into interwoven tubelike bundles resembling Dirac strings. At the defect formed by the end of each tube, the magnetic field looks like that of a monopole. Using an applied magnetic field to break the symmetry of the system, the researchers were able to control the density and orientation of these strings. A contribution to the heat capacity of the system from an effective gas of these quasiparticles was also described.
This research went on to win the 2012 Europhysics Prize for condensed matter physics.
Another example is a paper in the February 11, 2011 issue of "Nature Physics" which describes creation and measurement of long-lived magnetic monopole quasiparticle currents in spin ice. By applying a magnetic-field pulse to crystal of dysprosium titanate at 0.36 K, the authors created a relaxing magnetic current that lasted for several minutes. They measured the current by means of the electromotive force it induced in a solenoid coupled to a sensitive amplifier, and quantitatively described it using a chemical kinetic model of point-like charges obeying the Onsager–Wien mechanism of carrier dissociation and recombination. They thus derived the microscopic parameters of monopole motion in spin ice and identified the distinct roles of free and bound magnetic charges.
In superfluids, there is a field , related to superfluid vorticity, which is mathematically analogous to the magnetic -field. Because of the similarity, the field is called a "synthetic magnetic field". In January 2014, it was reported that monopole quasiparticles for the field were created and studied in a spinor Bose–Einstein condensate. This constitutes the first example of a quasi-magnetic monopole observed within a system governed by quantum field theory.
Further descriptions in particle physics.
In physics the phrase "magnetic monopole" usually denoted a Yang–Mills potential and Higgs field whose equations of motion are determined by the Yang–Mills action
In mathematics, the phrase customarily refers to a static solution to these equations in the Bogomolny–Parasad–Sommerfeld limit which realizes, within topological class, the absolutes minimum of the functional
This means that it in a connection on a principal -bundle over (c.f. also Connections on a manifold; principal -object) and a section of the associated adjoint bundle of Lie algebras such that the curvature and covariant derivative satisfy the Bogomolny equations
and the boundary conditions.
Pure mathematical advances in the theory of monopoles from the 1980s onwards have often proceeded on the basis of physically motived questions.
The equations themselves are invariant under gauge transformation and orientation-preserving symmetries. When is large, defines a mapping from a 2-sphere of radius in to an adjoint orbit and the homotopy class of this mapping is called the magnetic charge. Most work has been done in the case SU(2), where the charge is a positive integer . The absolute minimum value of the functional is then and the coefficient in the asymptotic expansion of is .
The first SU(2) solution was found by E. B. Bogomolny, J. K. Parasad and C. M. Sommerfield in 1975. It is spherically symmetric of charge 1 and has the form
In 1980, C.H.Taubes showed by a gluing construction that there exist solutions for all large and soon after explicit axially-symmetric solutions were found. The first exact solution in the general case was given in 1981 by R.S.Ward for in terms of elliptic functions.
There are two ways of solving the Bogomolny equations. The first is by twistor methods. In the formulation of N.J. Hitchin, an arbitrary solution corresponds to a holomorphic vector bundle over the complex surface , the tangent bundle of the projective line. This is naturally isomorphic to the space of oriented straight lines in .
The boundary condition show that the holomorphic bundle is an extension of line bundles determined by a compact algebraic curve of genus (the spectral curve) in , satisfying certain constraints.
The second method, due to W.Nahm, involves solving an eigen value problem for the coupled Dirac operator and transforming the equations with their boundary conditions into a system of ordinary differential equations, the Nahm equations.
where is a -matrix valued function on .
Both constructions are based on analogous procedures for instantons, the key observation due to N.S.Manton being of the self-dual Yang–Mills equations (c.f. also Yang–Mills field) in .
The equivalence of the two methods for SU(2) and their general applicability was established in (see also). Explicit formulas for and are difficult to obtain by either method, despite some exact solutions of Nahm's equations in symmetric situations.
Maximally imbedded spherically symmetric magnetic monopole solutions in the Bogolomony-Parasad-Sommerfield limit for the gauge group SU(n) were exhibited by Bais. Gannoulis, Goddard and Olive, and Farwell and Minami
 showed that maximally imbedded spherically symmetric magnetic monopole solutions in the Bogolomony-Parasad-Sommerfield limit for an arbitrary simple gauge group corresponding to a Lie Algebra with Cartan matrix and level vector , are solutions to the Toda molecule
equation:
Non-singular solutions have a magnetic field vanishes at the origin. Explicit finite energy solutions for the Lie Algebras , and have been obtained using this method.
The case of a more general Lie group , where the stabilizer of at infinity is a maximal torus, was treated by M.K.Murray from the twistor point of view, where the single spectral curve of an SU(2)-monopole is replaced by a collection of curves indexed by the vertices of the Dynkin diagram of . The corresponding Nahm construction was designed by J.Hustubise and Murray.
The moduli space (c.f. also Moduli theory) of all SU(2) monopoles of charge up to gauge equivalence was shown by Taubes to be a smooth non-compact manifold of dimension . Restricting to gauge transformations that preserve the connection at infinity gives a -dimensional manifold , which is a circle bundle over the true moduli space and carries a natural complete hyper-Kähler metric (c.f. also Kähler–Einstein manifold). With suspected to any of the complex structures of the hyper-Kähler family, this manifold is holomorphically equivalent to the space of based rational mapping of degree from to itself.
The metric is known in twistor terms, and its Kähler potential can be written using the Riemann theta functions of the spectral curve, but only the case is known in a more conventional and usable form (as of 2000). This Atiyah–Hitchin manifold, the Einstein Taub-NUT metric and are the only 4-dimensional complete hyper-Kähler manifolds with a non-triholomorphic SU(2) action. Its geodesics have been studied and a programme of Manton concerning monopole dynamics put into effect. Further dynamical features have been elucidated by numerical and analytical techniques.
A cyclic -fold conering of splits isometrically is a product , where is the space of strongly centred monopoles. This space features in an application of S-duality in theoretical physics, and in G.B.Segal and A.Selby studied its topology and the harmonic forms defined on it, partially confirming the physical prediction.
Magnetic monopole on hyperbolic three-space were investigated from the twistor point of view by M. F. Atiyah (replacing the complex surface by the complement of the anti-diagonal in ) and in terms of discrete Nahm equations by Murray and M. A. Singer.

</doc>
<doc id="1191067" url="https://en.wikipedia.org/wiki?curid=1191067" title="Magnetic potential">
Magnetic potential

The term magnetic potential can be used for either of two quantities in classical electromagnetism: the "magnetic vector potential", A, (often simply called the "vector potential") and the "magnetic scalar potential", ψ. Both quantities can be used in certain circumstances to calculate the magnetic field.
The more frequently used magnetic vector potential, A, is defined such that the curl of A is the magnetic field B. Together with the electric potential, the magnetic vector potential can be used to specify the electric field, E as well. Therefore, many equations of electromagnetism can be written either in terms of the E and B, "or" in terms of the magnetic vector potential and electric potential. In more advanced theories such as quantum mechanics, most equations use the potentials and not the E and B fields.
The magnetic scalar potential ψ is sometimes used to specify the magnetic H-field in cases when there are no free currents, in a manner analogous to using the electric potential to determine the electric field in electrostatics. One important use of ψ is to determine the magnetic field due to permanent magnets when their magnetization is known. With some care the scalar potential can be extended to include free currents as well.
Magnetic vector potential.
The magnetic vector potential A is a vector field defined along with the electric potential "ϕ" (a scalar field) by the equations:
where B is the magnetic field and E is the electric field. In magnetostatics where there is no time-varying charge distribution, only the first equation is needed. (In the context of electrodynamics, the terms "vector potential" and "scalar potential" are used for "magnetic vector potential" and "electric potential", respectively. In mathematics, vector potential and scalar potential have more general meanings.)
Defining the electric and magnetic fields from potentials automatically satisfies two of Maxwell's equations: Gauss's law for magnetism and Faraday's Law. For example, if A is continuous and well-defined everywhere, then it is guaranteed not to result in magnetic monopoles. (In the mathematical theory of magnetic monopoles, A is allowed to be either undefined or multiple-valued in some places; see magnetic monopole for details).
Starting with the above definitions:
Alternatively, the existence of A and "ϕ" is guaranteed from these two laws using the Helmholtz's theorem. For example, since the magnetic field is divergence-free (Gauss's law for magnetism), i.e. ∇ • B = 0, A always exists that satisfies the above definition.
The vector potential A is used when studying the Lagrangian in classical mechanics and in quantum mechanics (see Schrödinger equation for charged particles, Dirac equation, Aharonov–Bohm effect).
In the SI system, the units of A are V·s·m−1 and are the same as that of momentum per unit charge.
Although the magnetic field B is a pseudovector (also called axial vector), the vector potential A is a polar vector. This means that if the right-hand rule for cross products were replaced with a left-hand rule, but without changing any other equations or definitions, then B would switch signs, but A would not change. This is an example of a general theorem: The curl of a polar vector is a pseudovector, and vice versa.
Gauge choices.
The above definition does not define the magnetic vector potential uniquely because, by definition, we can arbitrarily add curl-free components to the magnetic potential without changing the observed magnetic field. Thus, there is a degree of freedom available when choosing A. This condition is known as gauge invariance.
Maxwell's equations in terms of vector potential.
Using the above definition of the potentials and applying it to the other two Maxwell's equations (the ones that are not automatically satisfied) results in a complicated differential equation that can be simplified using the Lorenz gauge where A is chosen to satisfy:
Using the Lorenz gauge, Maxwell's equations can be written compactly in terms of the magnetic vector potential A and the electric scalar potential "ϕ":
In other gauges, the equations are different. A different notation to write these same equations (using four-vectors) is shown below.
Calculation of potentials from source distributions.
The solutions of Maxwell's equations in the Lorenz gauge (see Feynman and Jackson) with the boundary condition that both potentials go to zero sufficiently fast as they approach infinity are called the retarded potentials, which are the magnetic vector potential A (r, "t") and the electric scalar potential "ϕ"(r, "t") due to a current distribution of current density J(r′, "t"′), charge density "ρ"(r′, "t"′), and volume Ω, within which "ρ" and J are non-zero at least sometimes and some places):
where the fields at position vector r and time "t" are calculated at distant position r′ at an earlier time "t"′. The location r′ is a source point in the charge or current distribution (also the integration variable, within volume Ω). The earlier time "t"′ is called the "retarded time", and calculated as
There are a few notable things about A and "ϕ" calculated in this way:
In other gauges the formula for A and "ϕ" is different — for example, see Coulomb gauge for another possibility.
Depiction of the A field.
See Feynman for the depiction of the A field around a long thin solenoid.
Since 
assuming quasi-static conditions, i.e. 
the lines and contours of A relate to B like the lines and contours of B relate to j. Thus, a depiction of the A field around a loop of B flux (as would be produced in a toroidal inductor) is qualitatively the same as the B field around a loop of current.
The figure to the right is an artist's depiction of the A field. The thicker lines indicate paths of higher average intensity (shorter paths have higher intensity so that the path integral is the same). The lines are drawn to (aesthetically) impart the general look of the A-field.
The drawing tacitly assumes ∇ • A = 0, true under the following assumptions:
Electromagnetic four-potential.
In the context of special relativity, it is natural to join the magnetic vector potential together with the (scalar) electric potential into the electromagnetic potential, also called "four-potential".
One motivation for doing so is that the four-potential is a mathematical four-vector. Thus, using standard four-vector transformation rules, if the electric and magnetic potentials are known in one inertial reference frame, they can be simply calculated in any other inertial reference frame.
Another, related motivation is that the content of classical electromagnetism can be written in a concise and convenient form using the electromagnetic four potential, especially when the Lorenz gauge is used. In particular, in abstract index notation, the set of Maxwell's equations (in the Lorenz gauge) may be written (in Gaussian units) as follows:
where □ is the d'Alembertian and "J" is the four-current. The first equation is the Lorenz gauge condition while the second contains Maxwell's equations. The four-potential also plays a very important role in quantum electrodynamics.
Magnetic scalar potential.
The scalar potential is another useful quantity in describing the magnetic field, especially for permanent magnets.
In a simply connected domain where there is no free current,
hence we can define a "magnetic scalar potential", "ψ", as
Using the definition of H:
it follows that
Here ∇ • M acts as the source for magnetic field, much like ∇ • P acts as the source for electric field. So analogously to bound electric charge, the quantity
is called the "bound magnetic charge".
If there is free current, one may subtract the contribution of free current per Biot–Savart law from total magnetic field and solve the remainder with the scalar potential method. To date there has not been any reproducible evidence for the existence of magnetic monopoles.

</doc>
<doc id="551359" url="https://en.wikipedia.org/wiki?curid=551359" title="Magnetic quantum number">
Magnetic quantum number

In atomic physics, the magnetic quantum number is the third of a set of quantum numbers (the principal quantum number, the azimuthal quantum number, the magnetic quantum number, and the spin quantum number) which describe the unique quantum state of an electron and is designated by the letter "m". The magnetic quantum number denotes the energy levels available within a subshell. This quantum number indicates the possible orientation of orbital in space. The value of 'm' for a particular value of 'l' varies from +l to -l including zero.
Derivation.
There is a set of quantum numbers associated with the energy states of the atom. The four quantum numbers "n", "ℓ", "m", and "s" specify the complete and unique quantum state of a single electron in an atom called its wavefunction or orbital. The wavefunction of the Schrödinger wave equation reduces to the three equations that when solved lead to the first three quantum numbers. Therefore, the equations for the first 3 quantum numbers are all interrelated. The magnetic quantum number arose in the solution of the azimuthal part of the wave equation as shown below.
The magnetic quantum number associated with the quantum state is designated as "m". The quantum number "m" refers, loosely, to the direction of the angular momentum vector. The magnetic quantum number "m" only affects the electron's energy if it is in a magnetic field because in the absence of one, all spherical harmonics corresponding to the different arbitrary values of "m" are equivalent. "M" also affects the probability cloud. Given a particular "ℓ", "m" is entitled to be any integer from -"ℓ" up to "ℓ". More precisely, for a given orbital momentum quantum number "ℓ" (representing the azimuthal quantum number associated with angular momentum), there are 2"ℓ"+1 integral magnetic quantum numbers "m" ranging from -"ℓ" to "ℓ", which restrict the fraction of the total angular momentum along the quantization axis so that they are limited to the values "m". This phenomenon is known as space quantization. It was first demonstrated by two German physicists, Otto Stern and Walther Gerlach.
Since each electronic orbit has a magnetic moment in a magnetic field the electronic orbit will be subject to a torque which tends to make the vector formula_1 parallel to the field. The precession of the electronic orbit in a magnetic field is called the Larmor precession.
To describe the magnetic quantum number "m" you begin with an atomic electron's angular momentum, L, which is related to its quantum number "ℓ" by the following equation:
where formula_3 is the reduced Planck constant. The energy of any wave is the frequency multiplied by Planck's constant. This causes the wave to display particle-like packets of energy called quanta. To show each of the quantum numbers in the quantum state, the formulae for each quantum number include Planck's reduced constant which only allows particular or discrete or quantized energy levels.
To show that only certain discrete amounts of angular momentum are allowed, "ℓ" has to be an integer. The quantum number "m" refers to the projection of the angular momentum for any given direction, conventionally called the z direction. Lz, the component of angular momentum in the z direction, is given by the formula:
Another way of stating the formula for the magnetic quantum number formula_5 is the eigenvalue, Jz=mℓh/2π.
Where the quantum number "ℓ" is the subshell, the magnetic number "m" represents the number of possible values for available energy levels of that subshell as shown in the table below.
The magnetic quantum number determines the energy shift of an atomic orbital due to an external magnetic field, hence the name "magnetic" quantum number (Zeeman effect).
However, the actual magnetic dipole moment of an electron in an atomic orbital arrives not only from the electron angular momentum, but also from the electron spin, expressed in the spin quantum number.

</doc>
<doc id="19446" url="https://en.wikipedia.org/wiki?curid=19446" title="Magnetic resonance imaging">
Magnetic resonance imaging

Magnetic resonance imaging (MRI), nuclear magnetic resonance imaging (NMRI), or magnetic resonance tomography (MRT) is a medical imaging technique used in radiology to image the anatomy and the physiological processes of the body in both health and disease. MRI scanners use magnetic fields and radio waves to form images of the body. The technique is widely used in hospitals for medical diagnosis, staging of disease and follow-up without exposure of the body to ionizing radiation.
Medical uses.
MRI has a wide range of applications in medical diagnosis and over 25,000 scanners are estimated to be in use worldwide. MRI has an impact on diagnosis and treatment in many specialties although the effect on improved health outcomes is uncertain. Since MRI does not use any ionizing radiation, its use is generally favored in preference to CT when either modality could yield the same information. (In certain cases MRI is not preferred as it can be more expensive, time-consuming, and claustrophobia-exacerbating).
MRI is in general a safe technique but the number of incidents causing patient harm has risen. Contraindications to MRI include most cochlear implants and cardiac pacemakers, shrapnel and metallic foreign bodies in the eyes. The safety of MRI during the first trimester of pregnancy is uncertain, but it may be preferable to other options. The sustained increase in demand for MRI within the healthcare industry has led to concerns about cost effectiveness and overdiagnosis.
Neuroimaging.
MRI is the investigative tool of choice for neurological cancers, as it has better resolution than CT and offers better visualization of the posterior fossa. The contrast provided between grey and white matter makes it the best choice for many conditions of the central nervous system including demyelinating diseases, dementia, cerebrovascular disease, infectious diseases and epilepsy. Since many images are taken milliseconds apart, it shows how the brain responds to different stimuli; researchers can then study both the functional and structural brain abnormalities in psychological disorders. MRI is also used in MRI-guided stereotactic surgery and radiosurgery for treatment of intracranial tumors, arteriovenous malformations and other surgically treatable conditions using a device known as the N-localizer.
Cardiovascular.
Cardiac MRI is complementary to other imaging techniques, such as echocardiography, cardiac CT and nuclear medicine. Its applications include assessment of myocardial ischemia and viability, cardiomyopathies, myocarditis, iron overload, vascular diseases and congenital heart disease.
Musculoskeletal.
Applications in the musculoskeletal system includes spinal imaging, assessment of joint disease and soft tissue tumors.
Liver and gastrointestinal MRI.
Hepatobiliary MR is used to detect and characterize lesions of the liver, pancreas and bile ducts. Focal or diffuse disorders of the liver may be evaluated using diffusion-weighted, opposed-phase imaging and dynamic contrast enhancement sequences. Extracellular contrast agents are widely used in liver MRI and newer hepatobiliary contrast agents also provide the opportunity to perform functional biliary imaging. Anatomical imaging of the bile ducts is achieved by using a heavily T2-weighted sequence in magnetic resonance cholangiopancreatography (MRCP). Functional imaging of the pancreas is performed following administration of secretin. MR enterography provides non-invasive assessment of inflammatory bowel disease and small bowel tumors. MR-colonography can play a role in the detection of large polyps in patients at increased risk of colorectal cancer.
Functional MRI.
Functional MRI (fMRI) is used to understand how different parts of the brain respond to external stimuli or passive activity in a resting state. Blood oxygenation level dependent (BOLD) fMRI measures the hemodynamic response to transient neural activity resulting from a change in the ratio of oxyhemoglobin and deoxyhemoglobin. Statistical methods are used to construct a 3D parametric map of the brain indicating those regions of the cortex which demonstrate a significant change in activity in response to the task. fMRI has applications in behavioral and cognitive research as well as in planning neurosurgery of eloquent brain areas.
Oncology.
MRI is the investigation of choice in the preoperative staging of rectal and prostate cancer, and has a role in the diagnosis, staging, and follow-up of other tumors.
Safety.
Implants.
All patients are reviewed for contraindications prior to MRI scanning. Medical devices and implants are categorized as MR Safe, MR Conditional or MR Unsafe:
The MRI environment may cause harm in patients with MR-Unsafe devices such as cochlear implants and most permanent pacemakers. Several deaths have been reported in patients with pacemakers who have undergone MRI scanning without appropriate precautions. Many implants can be safely scanned if the appropriate conditions are adhered to and these are available online (see www.MRIsafety.com). MR Conditional pacemakers are increasingly available for selected patients.
Ferromagnetic foreign bodies such as shell fragments, or metallic implants such as surgical prostheses and ferromagnetic aneurysm clips are also potential risks. Interaction of the magnetic and radio frequency fields with such objects can lead to heating or torque of the object during an MRI.
Titanium and its alloys are safe from attraction and torque forces produced by the magnetic field, though there may be some risks associated with Lenz effect forces acting on titanium implants in sensitive areas within the subject, such as stapes implants in the inner ear.
Projectile risk.
The very high strength of the magnetic field can cause projectile effect (or "missile-effect") accidents, where ferromagnetic objects are attracted to the center of the magnet. Pennsylvania reported 27 cases of objects becoming projectiles in the MRI environment between 2004 and 2008. There have been incidents of injury and death. In one case, a 6-year-old boy died during an MRI exam, after a metal oxygen tank was pulled across the room and crushed the child's head.
To reduce the risk of projectile accidents, ferromagnetic objects and devices are typically prohibited in the proximity of the MRI scanner and patients undergoing MRI examinations are required to remove all metallic objects, often by changing into a gown or scrubs, and ferromagnetic detection devices are used at some sites.
MRI-EEG.
In research settings, structural MRI or functional MRI (fMRI) can be combined with EEG (electroencephalography) under the condition that the EEG equipment is MR compatible. Although EEG equipment (electrodes, amplifiers and peripherals) are either approved for research or clinical use, the same MR Safe, MR Conditional and MR Unsafe terminology applies. With the growth of the use of MR technology, the U.S. Food & Drug Administration [FDA] recognized the need for a consensus on standards of practice, and the FDA sought out ASTM International [ASTM] to achieve them. Committee F04 of ASTM developed F2503, Standard Practice for Marking Medical Devices and Other Items for Safety in the Magnetic Resonance Environment.
Genotoxic effects.
There is no proven risk of biological harm from even very powerful static magnetic fields. However, genotoxic (i.e., potentially carcinogenic) effects of MRI scanning have been demonstrated in vivo and in vitro, leading a recent review to recommend "a need for further studies and prudent use in order to avoid unnecessary examinations, according to the precautionary principle". In a comparison of genotoxic effects of MRI compared with those of CT scans, Knuuti et al. reported that even though the DNA damage detected after MRI was at a level comparable to that produced by scans using ionizing radiation (low-dose coronary CT angiography, nuclear imaging, and X-ray angiography), differences in the mechanism by which this damage takes place suggests that the cancer risk of MRI, if any, is unknown.
Peripheral nerve stimulation (PNS).
The rapid switching on and off of the magnetic field gradients is capable of causing nerve stimulation. Volunteers report a twitching sensation when exposed to rapidly switched fields, particularly in their extremities. The reason the peripheral nerves are stimulated is that the changing field increases with distance from the center of the gradient coils (which more or less coincides with the center of the magnet). Although PNS was not a problem for the slow, weak gradients used in the early days of MRI, the strong, rapidly switched gradients used in techniques such as EPI, fMRI, diffusion MRI, etc. are capable of inducing PNS. American and European regulatory agencies insist that manufacturers stay below specified "dB/"dt" limits ("dB/"dt" is the change in magnetic field strength per unit time) or else prove that no PNS is induced for any imaging sequence. As a result of "d"B/"dt" limitation, commercial MRI systems cannot use the full rated power of their gradient amplifiers.
Heating caused by absorption of radio waves.
Every MRI scanner has a powerful radio transmitter to generate the electromagnetic field which excites the spins. If the body absorbs the energy, heating occurs. For this reason, the transmitter rate at which energy is absorbed by the body has to be limited (see Specific absorption rate).
It has been claimed that tattoos made with iron containing dyes can lead to burns on the subject's body.
Acoustic noise.
Switching of field gradients causes a change in the Lorentz force experienced by the gradient coils, producing minute expansions and contractions of the coil itself. As the switching is typically in the audible frequency range, the resulting vibration produces loud noises (clicking, banging, or beeping). This is most marked with high-field machines and rapid-imaging techniques in which sound pressure levels can reach 120 dB(A) (equivalent to a jet engine at take-off), and therefore appropriate ear protection is essential for anyone inside the MRI scanner room during the examination.
Cryogens.
As described in Physics of Magnetic Resonance Imaging, many MRI scanners rely on cryogenic liquids to enable the superconducting capabilities of the electromagnetic coils within. Though the cryogenic liquids used are non-toxic, their physical properties present specific hazards.
An unintentional shut-down of a superconducting electromagnet, an event known as "quench", involves the rapid boiling of liquid helium from the device. If the rapidly expanding helium cannot be dissipated through an external vent, sometimes referred to as a 'quench pipe', it may be released into the scanner room where it may cause displacement of the oxygen and present a risk of asphyxiation.
Oxygen deficiency monitors are usually used as a safety precaution. Liquid helium, the most commonly used cryogen in MRI, undergoes near explosive expansion as it changes from a liquid to gaseous state. The use of an oxygen monitor is important to ensure that oxygen levels are safe for patient/physicians. Rooms built for superconducting MRI equipment should be equipped with pressure relief mechanisms and an exhaust fan, in addition to the required quench pipe.
Because a quench results in rapid loss of cryogens from the magnet, recommissioning the magnet is expensive and time-consuming. Spontaneous quenches are uncommon, but a quench may also be triggered by an equipment malfunction, an improper cryogen fill technique, contaminants inside the cryostat, or extreme magnetic or vibrational disturbances.
Pregnancy.
No effects of MRI on the fetus have been demonstrated.
In particular, MRI avoids the use of ionizing radiation, to which the fetus is particularly sensitive. However, as a precaution, current guidelines recommend that pregnant women undergo MRI only when essential. This is particularly the case during the first trimester of pregnancy, as organogenesis takes place during this period. The concerns in pregnancy are the same as for MRI in general, but the fetus may be more sensitive to the effects—particularly to heating and to noise. The use of gadolinium-based contrast media in pregnancy is an off-label indication and may only be administered in the lowest dose required to provide essential diagnostic information.
Despite these concerns, MRI is rapidly growing in importance as a way of diagnosing and monitoring congenital defects of the fetus because it can provide more diagnostic information than ultrasound and it lacks the ionizing radiation of CT. MRI without contrast agents is the imaging mode of choice for pre-surgical, in-utero diagnosis and evaluation of fetal tumors, primarily teratomas, facilitating open fetal surgery, other fetal interventions, and planning for procedures (such as the EXIT procedure) to safely deliver and treat babies whose defects would otherwise be fatal.
Claustrophobia and discomfort.
Although painless, MRI scans can be unpleasant for those who are claustrophobic or otherwise uncomfortable with the imaging device surrounding them. Older closed bore MRI systems have a fairly long tube or tunnel. The part of the body being imaged must lie at the center of the magnet, which is at the absolute center of the tunnel. Because scan times on these older scanners may be long (occasionally up to 40 minutes for the entire procedure), people with even mild claustrophobia are sometimes unable to tolerate an MRI scan without management. Some modern scanners have larger bores (up to 70 cm) and scan times are shorter. A 1.5 T wide short bore scanner increases the examination success rate in patients with claustrophobia and substantially reduces the need for anesthesia-assisted MRI examinations even when claustrophobia is severe.
Alternative scanner designs, such as open or upright systems, can also be helpful where these are available. Though open scanners have increased in popularity, they produce inferior scan quality because they operate at lower magnetic fields than closed scanners. However, commercial 1.5 tesla open systems have recently become available, providing much better image quality than previous lower field strength open models.
Mirror glasses can be used to help create the illusion of openness. The mirrors are angled at 45 degrees, allowing the patient to look down their body and out the end of the imaging area. The appearance is of an open tube pointing upwards (as seen when lying in the imaging area). Even though one can see around the glasses and the proximity of the device is very evident, this illusion is quite persuasive and relieves the claustrophobic feeling.
For babies and other young children, chemical sedation or general anesthesia are the norm, as these subjects cannot be expected or instructed to hold still during the scanning session. Children are also frequently sedated because they are frightened by the unfamiliar procedure and the loud noises. To reduce anxiety, some hospitals have specially designed child-friendly approaches that pretend the MRI machine is a spaceship or other fun experience.
Obese patients and pregnant women may find the MRI machine to be a tight fit. Pregnant women in the third trimester may also have difficulty lying on their backs for an hour or more without moving.
MRI versus CT.
MRI and computed tomography (CT) are complementary imaging technologies and each has advantages and limitations for particular applications. CT is more widely used than MRI in OECD countries with a mean of 132 vs 46 exams per 1000 population performed respectively. A concern is the potential for CT to contribute to radiation-induced cancer and in 2007 it was estimated that 0.4% of current cancers in the United States were due to CTs performed in the past, and that in the future this figure may rise to 1.5–2% based on historical rates of CT usage. An Australian study found that one in every 1800 CT scans was associated with an excess cancer. An advantage of MRI is that no ionizing radiation is used and so it is recommended over CT when either approach could yield the same diagnostic information. However, although the cost of MRI has fallen, making it more competitive with CT, there are not many common imaging scenarios in which MRI can simply replace CT, although this substitution has been suggested for the imaging of liver disease. The effect of low doses of radiation on carcinogenesis are also disputed. Although MRI is associated with biological effects, these have not been proven to cause measurable harm. In a comparison of possible genotoxic effects of MRI compared with those of CT scans, Knuuti et al. noted that although previous studies have demonstrated DNA damage associated with MRI, "the long-term biological and clinical significance of DNA double-strand breaks induced by MRI remains unknown".
Iodinated contrast medium is routinely used in CT and the main adverse events are anaphylactoid reactions and nephrotoxicity. Commonly used MRI contrast agents have a good safety profile but linear non-ionic agents in particular have been implicated in nephrogenic systemic fibrosis in patients with severely impaired renal function.
MRI is contraindicated in the presence of MR-unsafe implants, and although these patients may be imaged with CT, beam hardening artefact from metallic devices, such as pacemakers and implantable cardioverter-defibrillators, may also affect image quality. MRI is a longer investigation than CT and an exam may take between 20 - 40 mins depending on complexity.
Guidance.
Safety issues, including the potential for biostimulation device interference, movement of ferromagnetic bodies, and incidental localized heating, have been addressed in the American College of Radiology's "White Paper on MR Safety," which was originally published in 2002 and expanded in 2004. The "ACR White Paper on MR Safety" has been rewritten and was released early in 2007 under the new title "ACR Guidance Document for Safe MR Practices".
In December 2007, the Medicines and Healthcare Products Regulatory Agency (MHRA), a UK healthcare regulatory body, issued their "Safety Guidelines for Magnetic Resonance Imaging Equipment in Clinical Use". In February 2008, the Joint Commission, a US healthcare accrediting organization, issued a Sentinel Event Alert #38, their highest patient safety advisory, on MRI safety issues. In July 2008, the United States Veterans Administration, a federal governmental agency serving the healthcare needs of former military personnel, issued a substantial revision to their "MRI Design Guide", which includes physical and facility safety considerations.
The European Directive on Electromagnetic Fields.
This Directive (2013/35/EU - electromagnetic fields)
covers all known direct biophysical effects and indirect effects caused by electromagnetic fields within the EU and repealed the 2004/40/EC directive. The deadline for implementation of the new directive is 1 July 2016. Article 10 of the directive sets out the scope of the derogation for MRI, stating that the exposure limits may be exceeded during "the installation, testing, use, development, maintenance of or research related to magnetic resonance imaging (MRI) equipment for patients in the health sector, provided that certain conditions are met." Uncertainties remain regarding the scope and conditions
of this derogation.
Procedure.
To perform a study, the person is positioned within an MRI scanner which forms a strong magnetic field around the area to be imaged. In most medical applications, protons (hydrogen atoms) in tissues containing water molecules are used to create a signal that is processed to form an image of the body. First, energy from an oscillating magnetic field is temporarily applied to the patient at the appropriate resonance frequency. The excited hydrogen atoms emit a radio frequency signal which is measured by a receiving coil. The radio signal can be made to encode position information by varying the main magnetic field using gradient coils. As these coils are rapidly switched on and off they create the characteristic repetitive noise of an MRI scan. The contrast between different tissues is determined by the rate at which excited atoms return to the equilibrium state. Exogenous contrast agents may be given intravenously, orally or intra-articularly.
MRI requires a magnetic field that is both strong and uniform. The field strength of the magnet is measured in teslas – and while the majority of systems operate at 1.5T, commercial systems are available between 0.2T–7T. Most clinical magnets are superconducting which requires liquid helium. Lower field strengths can be achieved with permanent magnets, which are often used in "open" MRI scanners for claustrophobic patients.
Contrast.
Image contrast may be weighted to demonstrate different anatomical structures or pathologies. Each tissue returns to its equilibrium state after excitation by the independent processes of T1 (spin-lattice) and T2 (spin-spin) relaxation.
To create a T1-weighted image magnetization is allowed to recover before measuring the MR signal by changing the repetition time (TR). This image weighting is useful for assessing the cerebral cortex, identifying fatty tissue, characterizing focal liver lesions and in general for obtaining morphological information, as well as for post-contrast imaging.
To create a T2-weighted image magnetization is allowed to decay before measuring the MR signal by changing the echo time (TE). This image weighting is useful for detecting edema and inflammation, revealing white matter lesions and assessing zonal anatomy in the prostate and uterus.
Contrast agents.
MRI for imaging anatomical structures or blood flow do not require contrast agents as the varying properties of the tissues or blood provide natural contrasts. However, for more specific types of imaging the most commonly used intravenous contrast agents are based on chelates of gadolinium. In general, these agents have proved safer than the iodinated contrast agents used in X-ray radiography or CT. Anaphylactoid reactions are rare, occurring in approx. 0.03–0.1%. Of particular interest is the lower incidence of nephrotoxicity, compared with iodinated agents, when given at usual doses—this has made contrast-enhanced MRI scanning an option for patients with renal impairment, who would otherwise not be able to undergo contrast-enhanced CT.
Although gadolinium agents have proved useful for patients with renal impairment, in patients with severe renal failure requiring dialysis there is a risk of a rare but serious illness, nephrogenic systemic fibrosis, which may be linked to the use of certain gadolinium-containing agents. The most frequently linked is gadodiamide, but other agents have been linked too. Although a causal link has not been definitively established, current guidelines in the United States are that dialysis patients should only receive gadolinium agents where essential, and that dialysis should be performed as soon as possible after the scan to remove the agent from the body promptly. In Europe, where more gadolinium-containing agents are available, a classification of agents according to potential risks has been released. Recently, a new contrast agent named gadoxetate, brand name Eovist (US) or Primovist (EU), was approved for diagnostic use: this has the theoretical benefit of a dual excretion path.
History.
Magnetic resonance imaging was invented by Paul C. Lauterbur in September 1971; he published the theory behind it in March 1973. The factors leading to image contrast (differences in tissue relaxation time values) had been described nearly 20 years earlier by Erik Odeblad (physician and scientist).
In 1950, spin echoes were first detected by Erwin Hahn and in 1952, Herman Carr produced a one-dimensional NMR spectrum as reported in his Harvard PhD thesis. In the Soviet Union, Vladislav Ivanov filed (in 1960) a document with the USSR State Committee for Inventions and Discovery at Leningrad for a Magnetic Resonance Imaging device, although this was not approved until the 1970s.
In a 1971 paper in the journal "Science", Raymond Damadian, an American physician and professor at the Downstate Medical Center State University of New York (SUNY), reported that tumors and normal tissue can be distinguished in vivo by nuclear magnetic resonance ("NMR"). He suggested that these differences could be used to diagnose cancer, though later research would find that these differences, while real, are too variable for diagnostic purposes. Damadian's initial methods were flawed for practical use, relying on a point-by-point scan of the entire body and using relaxation rates, which turned out not to be an effective indicator of cancerous tissue. While researching the analytical properties of magnetic resonance, Damadian created a hypothetical magnetic resonance cancer-detecting machine in 1972. He filed the first patent for such a machine, U.S. patent #3,789,832 on March 17, 1972, which was later issued to him on February 5, 1974.
The US National Science Foundation notes "The patent included the idea of using NMR to 'scan' the human body to locate cancerous tissue." However, it did not describe a method for generating pictures from such a scan or precisely how such a scan might be done. Meanwhile, Paul Lauterbur at Stony Brook University expanded on Carr's technique and developed a way to generate the first MRI images, in 2D and 3D, using gradients. In 1973, Lauterbur published the first nuclear magnetic resonance image and the first cross-sectional image of a living mouse in January 1974. In the late 1970s, Peter Mansfield, a physicist and professor at the University of Nottingham, England, developed the echo-planar imaging (EPI) technique that would lead to scans taking seconds rather than hours and produce clearer images than Lauterbur had. Damadian, along with Larry Minkoff and Michael Goldsmith, obtained an image of a tumor in the thorax of a mouse in 1976. They also performed the first MRI body scan of a human being on July 3, 1977, studies which they published in 1977. In 1979, Richard S. Likes filed a patent on k-space *4,307,343.
During the 1970s a team led by John Mallard built the first full body MRI scanner at the University of Aberdeen. On 28 August 1980 they used this machine to obtain the first clinically useful image of a patient's internal tissues using Magnetic Resonance Imaging (MRI), which identified a primary tumour in the patient's chest, an abnormal liver, and secondary cancer in his bones. This machine was later used at St Bartholomew's Hospital, in London, from 1983 to 1993. Mallard and his team are credited for technological advances that led to the widespread introduction of MRI.
In 1975, the University of California, San Francisco Radiology Department founded the Radiologic Imaging Laboratory (RIL). With the support of Pfizer, Diasonics, and later Toshiba America MRI, the lab developed new imaging technology and installed systems in the US and worldwide. In 1981 RIL researchers, including Leon Kaufman and Lawrence Crooks, published "Nuclear Magnetic Resonance Imaging in Medicine". In the 1980s the book was considered the definitive introductory textbook to the subject. 
In 1980 Paul Bottomley joined the GE Research Center in Schenectady, NY, and his team ordered the highest field-strength magnet then available — a 1.5T system — and built the first high-field and overcame problems of coil design, RF penetration and signal-to-noise ratio to build the first whole-body MRI/MRS scanner. The results translated into the highly successful 1.5T MRI product-line, with over 20,000 systems in use today. In 1982, Bottomley performed the first localized MRS in the human heart and brain. After starting a collaboration on heart applications with Robert Weiss at Johns Hopkins, Bottomley returned to the university in 1994 as Russell Morgan Professor and director of the MR Research Division. Although MRI is most commonly performed at 1.5 T, higher fields such as 3T are gaining more popularity because of their increased sensitivity and resolution. In research laboratories, human studies have been performed at up to 9.4 T and animal studies have been performed at up to 21.1T.
2003 Nobel Prize.
Reflecting the fundamental importance and applicability of MRI in medicine, Paul Lauterbur of the University of Illinois at Urbana-Champaign and Sir Peter Mansfield of the University of Nottingham were awarded the 2003 Nobel Prize in Physiology or Medicine for their "discoveries concerning magnetic resonance imaging". The Nobel citation acknowledged Lauterbur's insight of using magnetic field gradients to determine spatial localization, a discovery that allowed rapid acquisition of 2D images. Mansfield was credited with introducing the mathematical formalism and developing techniques for efficient gradient utilization and fast imaging. The actual research that won the prize was done almost 30 years before while Paul Lauterbur was a professor in the Department of Chemistry at Stony Brook University in New York.
Economics.
In the UK, the price of a clinical 1.5 tesla MRI scanner is around €1,04 million/ with the lifetime maintenance cost broadly similar to the purchase cost. In the Netherlands, the average MRI scanner costs around €1 million, with a 7T MRI having been taken in use by the UMC Utrecht in December 2007, costing €7 million. Construction of MRI suites could cost up to /€370.000 or more, depending on project scope. Pre-polarizing MRI (PMRI) systems using resistive electromagnets have shown promise as a low cost alternative and have specific advantages for joint imaging near metal implants, however they are unlikely to be suitable for routine whole-body or neuroimaging applications.
MRI scanners have become significant sources of revenue for healthcare providers in the US. This is because of favorable reimbursement rates from insurers and federal government programs. Insurance reimbursement is provided in two components, an equipment charge for the actual performance and operation of the MRI scan and a professional charge for the radiologist's review of the images and/or data. In the US Northeast, an equipment charge might be $3,500/€2.600 and a professional charge might be $350/€260, although the actual fees received by the equipment owner and interpreting physician are often significantly less and depend on the rates negotiated with insurance companies or determined by the Medicare fee schedule. For example, an orthopedic surgery group in Illinois billed a charge of $1,116/€825 for a knee MRI in 2007, but the Medicare reimbursement in 2007 was only $470.91/€350. Many insurance companies require advance approval of an MRI procedure as a condition for coverage.
In the US, the Deficit Reduction Act of 2005 significantly reduced reimbursement rates paid by federal insurance programs for the equipment component of many scans, shifting the economic landscape. Many private insurers have followed suit.
In the United States, an MRI of the brain with and without contrast billed to Medicare Part B entails, on average, a technical payment of /€300 and a separate payment to the radiologist of /€70. In France, the cost of an MRI exam is approximately €150/. This covers three basic scans including one with an intravenous contrast agent as well as a consultation with the technician and a written report to the patient's physician. In Japan, the cost of an MRI examination (excluding the cost of contrast material and films) ranges from /€115 to /€133, with an additional radiologist professional fee of /€12,50. In India, the cost of an MRI examination including the fee for the radiologist's opinion comes to around Rs 3000–4000 (€37-49/), excluding the cost of contrast material. In the UK the retail price for an MRI scan privately ranges between £350 and £500 (€440-630).
Overuse.
Medical societies issue guidelines for when physicians should use MRI on patients and recommend against overuse. MRI can detect health problems or confirm a diagnosis, but medical societies often recommend that MRI not be the first procedure for creating a plan to diagnose or manage a patient's complaint. A common case is to use MRI to seek a cause of low back pain; the American College of Physicians, for example, recommends against this procedure as unlikely to result in a positive outcome for the patient.
Specialized applications.
Diffusion MRI.
Diffusion MRI measures the diffusion of water molecules in biological tissues. Clinically, diffusion MRI is useful for the diagnoses of conditions (e.g., stroke) or neurological disorders (e.g., multiple sclerosis), and helps better understand the connectivity of white matter axons in the central nervous system. In an isotropic medium (inside a glass of water for example), water molecules naturally move randomly according to turbulence and Brownian motion. In biological tissues however, where the Reynolds number is low enough for flows to be laminar, the diffusion may be anisotropic. For example, a molecule inside the axon of a neuron has a low probability of crossing the myelin membrane. Therefore, the molecule moves principally along the axis of the neural fiber. If it is known that molecules in a particular voxel diffuse principally in one direction, the assumption can be made that the majority of the fibers in this area are parallel to that direction.
The recent development of diffusion tensor imaging (DTI) enables diffusion to be measured in multiple directions and the fractional anisotropy in each direction to be calculated for each voxel. This enables researchers to make brain maps of fiber directions to examine the connectivity of different regions in the brain (using tractography) or to examine areas of neural degeneration and demyelination in diseases like multiple sclerosis.
Another application of diffusion MRI is diffusion-weighted imaging (DWI). Following an ischemic stroke, DWI is highly sensitive to the changes occurring in the lesion. It is speculated that increases in restriction (barriers) to water diffusion, as a result of cytotoxic edema (cellular swelling), is responsible for the increase in signal on a DWI scan. The DWI enhancement appears within 5–10 minutes of the onset of stroke symptoms (as compared to computed tomography, which often does not detect changes of acute infarct for up to 4–6 hours) and remains for up to two weeks. Coupled with imaging of cerebral perfusion, researchers can highlight regions of "perfusion/diffusion mismatch" that may indicate regions capable of salvage by reperfusion therapy.
Like many other specialized applications, this technique is usually coupled with a fast image acquisition sequence, such as echo planar imaging sequence.
Magnetic resonance angiography.
Magnetic resonance angiography (MRA) generates pictures of the arteries to evaluate them for stenosis (abnormal narrowing) or aneurysms (vessel wall dilatations, at risk of rupture). MRA is often used to evaluate the arteries of the neck and brain, the thoracic and abdominal aorta, the renal arteries, and the legs (called a "run-off"). A variety of techniques can be used to generate the pictures, such as administration of a paramagnetic contrast agent (gadolinium) or using a technique known as "flow-related enhancement" (e.g., 2D and 3D time-of-flight sequences), where most of the signal on an image is due to blood that recently moved into that plane, see also FLASH MRI. Techniques involving phase accumulation (known as phase contrast angiography) can also be used to generate flow velocity maps easily and accurately. Magnetic resonance venography (MRV) is a similar procedure that is used to image veins. In this method, the tissue is now excited inferiorly, while the signal is gathered in the plane immediately superior to the excitation plane—thus imaging the venous blood that recently moved from the excited plane.
Magnetic resonance spectroscopy.
Magnetic resonance spectroscopy (MRS) is used to measure the levels of different metabolites in body tissues. The MR signal produces a spectrum of resonances that corresponds to different molecular arrangements of the isotope being "excited". This signature is used to diagnose certain metabolic disorders, especially those affecting the brain, and to provide information on tumor metabolism.
Magnetic resonance spectroscopic imaging (MRSI) combines both spectroscopic and imaging methods to produce spatially localized spectra from within the sample or patient. The spatial resolution is much lower (limited by the available SNR), but the spectra in each voxel contains information about many metabolites. Because the available signal is used to encode spatial and spectral information, MRSI requires high SNR achievable only at higher field strengths (3 T and above).
Functional MRI.
Functional MRI (fMRI) measures signal changes in the brain that are due to changing neural activity. Compared to anatomical T1W imaging, the brain is scanned at lower spatial resolution but at a higher temporal resolution (typically once every 2–3 seconds). Increases in neural activity cause changes in the MR signal via "T" changes; this mechanism is referred to as the BOLD (blood-oxygen-level dependent) effect. Increased neural activity causes an increased demand for oxygen, and the vascular system actually overcompensates for this, increasing the amount of oxygenated hemoglobin relative to deoxygenated hemoglobin. Because deoxygenated hemoglobin attenuates the MR signal, the vascular response leads to a signal increase that is related to the neural activity. The precise nature of the relationship between neural activity and the BOLD signal is a subject of current research. The BOLD effect also allows for the generation of high resolution 3D maps of the venous vasculature within neural tissue.
While BOLD signal analysis is the most common method employed for neuroscience studies in human subjects, the flexible nature of MR imaging provides means to sensitize the signal to other aspects of the blood supply. Alternative techniques employ arterial spin labeling (ASL) or weighting the MRI signal by cerebral blood flow (CBF) and cerebral blood volume (CBV). The CBV method requires injection of a class of MRI contrast agents that are now in human clinical trials. Because this method has been shown to be far more sensitive than the BOLD technique in preclinical studies, it may potentially expand the role of fMRI in clinical applications. The CBF method provides more quantitative information than the BOLD signal, albeit at a significant loss of detection sensitivity.
Real-time MRI.
Real-time MRI refers to the continuous monitoring ("filming") of moving objects in real time. While many different strategies have been developed over the past two decades, a recent development reported a real-time MRI technique based on radial FLASH and iterative reconstruction that yields a temporal resolution of 20 to 30 milliseconds for images with an in-plane resolution of 1.5 to 2.0 mm. The new method promises to add important information about diseases of the joints and the heart. In many cases MRI examinations may become easier and more comfortable for patients.
Interventional MRI.
The lack of harmful effects on the patient and the operator make MRI well-suited for "interventional radiology", where the images produced by an MRI scanner are used to guide minimally invasive procedures. Of course, such procedures must be done without "any" ferromagnetic instruments.
A specialized growing subset of interventional MRI is that of intraoperative MRI in which the MRI is used in the surgical process. Some specialized MRI systems have been developed that allow imaging concurrent with the surgical procedure. More typical, however, is that the surgical procedure is temporarily interrupted so that MR images can be acquired to verify the success of the procedure or guide subsequent surgical work.
Magnetic resonance guided focused ultrasound.
In MRgFUS therapy, ultrasound beams are focused on a tissue—guided and controlled using MR thermal imaging—and due to the significant energy deposition at the focus, temperature within the tissue rises to more than 65 °C (150 °F), completely destroying it. This technology can achieve precise ablation of diseased tissue. MR imaging provides a three-dimensional view of the target tissue, allowing for precise focusing of ultrasound energy. The MR imaging provides quantitative, real-time, thermal images of the treated area. This allows the physician to ensure that the temperature generated during each cycle of ultrasound energy is sufficient to cause thermal ablation within the desired tissue and if not, to adapt the parameters to ensure effective treatment.
Multinuclear imaging.
Hydrogen is the most frequently imaged nucleus in MRI because it is present in biological tissues in great abundance, and because its high gyromagnetic ratio gives a strong signal. However, any nucleus with a net nuclear spin could potentially be imaged with MRI. Such nuclei include helium-3, lithium-7, carbon-13, fluorine-19, oxygen-17, sodium-23, phosphorus-31 and xenon-129. 23Na and 31P are naturally abundant in the body, so can be imaged directly. Gaseous isotopes such as 3He or 129Xe must be hyperpolarized and then inhaled as their nuclear density is too low to yield a useful signal under normal conditions. 17O and 19F can be administered in sufficient quantities in liquid form (e.g. 17O-water) that hyperpolarization is not a necessity.
Moreover, the nucleus of any atom that has a net nuclear spin and that is bonded to a hydrogen atom could potentially be imaged via heteronuclear magnetization transfer MRI that would image the high-gyromagnetic-ratio hydrogen nucleus instead of the low-gyromagnetic-ratio nucleus that is bonded to the hydrogen atom. In principle, hetereonuclear magnetization transfer MRI could be used to detect the presence or absence of specific chemical bonds.
Multinuclear imaging is primarily a research technique at present. However, potential applications include functional imaging and imaging of organs poorly seen on 1H MRI (e.g., lungs and bones) or as alternative contrast agents. Inhaled hyperpolarized 3He can be used to image the distribution of air spaces within the lungs. Injectable solutions containing 13C or stabilized bubbles of hyperpolarized 129Xe have been studied as contrast agents for angiography and perfusion imaging. 31P can potentially provide information on bone density and structure, as well as functional imaging of the brain. Multinuclear imaging holds the potential to chart the distribution of lithium in the human brain, this element finding use as an important drug for those with conditions such as bipolar disorder.
Molecular imaging by MRI.
MRI has the advantages of having very high spatial resolution and is very adept at morphological imaging and functional imaging. MRI does have several disadvantages though. First, MRI has a sensitivity of around 10−3 mol/L to 10−5 mol/L which, compared to other types of imaging, can be very limiting. This problem stems from the fact that the population difference between the nuclear spin states is very small at room temperature. For example, at 1.5 teslas, a typical field strength for clinical MRI, the difference between high and low energy states is approximately 9 molecules per 2 million. Improvements to increase MR sensitivity include increasing magnetic field strength, and hyperpolarization via optical pumping or dynamic nuclear polarization. There are also a variety of signal amplification schemes based on chemical exchange that increase sensitivity.
To achieve molecular imaging of disease biomarkers using MRI, targeted MRI contrast agents with high specificity and high relaxivity (sensitivity) are required. To date, many studies have been devoted to developing targeted-MRI contrast agents to achieve molecular imaging by MRI. Commonly, peptides, antibodies, or small ligands, and small protein domains, such as HER-2 affibodies, have been applied to achieve targeting. To enhance the sensitivity of the contrast agents, these targeting moieties are usually linked to high payload MRI contrast agents or MRI contrast agents with high relaxivities. A new class of gene targeting MR contrast agents (CA) has been introduced to show gene action of unique mRNA and gene transcription factor proteins. This new CA can trace cells with unique mRNA, microRNA and virus; tissue response to inflammation in living brains. The MR reports change in gene expression with positive correlation to TaqMan analysis, optical and electron microscopy.
Other specialized sequences.
New methods and variants of existing methods are often published when they are able to produce better results in specific fields. Examples of these recent improvements are "T"-weighted turbo spin-echo ("T"2 TSE MRI), double inversion recovery MRI (DIR-MRI) or phase-sensitive inversion recovery MRI (PSIR-MRI), all of them able to improve imaging of brain lesions. Another example is MP-RAGE (magnetization-prepared rapid acquisition with gradient echo), which improves images of multiple sclerosis cortical lesions.
Magnetization transfer MRI.
Magnetization transfer (MT) is a technique to enhance image contrast in certain applications of MRI.
Bound protons are associated with proteins and as they have a very short T2 decay they do not normally contribute to image contrast. However, because these protons have a broad resonance peak they can be excited by a radiofrequency pulse that has no effect on free protons. Their excitation increases image contrast by transfer of saturated spins from the bound pool into the free pool, thereby reducing the signal of free water. This homonuclear magnetization transfer provides an indirect measurement of macromolecular content in tissue. Implementation of homonuclear magnetization transfer involves choosing suitable frequency offsets and pulse shapes to saturate the bound spins sufficiently strongly, within the safety limits of specific absorption rate for MRI.
The most common use of this technique is for suppression of background signal in time of flight MR angiography. There are also applications in neuroimaging particularly in the characterization of white matter lesions in multiple sclerosis.
T1rho MRI.
T1ρ (T1rho): Molecules have a kinetic energy that is a function of the temperature and is expressed as translational and rotational motions, and by collisions between molecules. The moving dipoles disturb the magnetic field but are often extremely rapid so that the average effect over a long time-scale may be zero. However, depending on the time-scale, the interactions between the dipoles do not always average away. At the slowest extreme the interaction time is effectively infinite and occurs where there are large, stationary field disturbances (e.g., a metallic implant). In this case the loss of coherence is described as a "static dephasing". T2* is a measure of the loss of coherence in an ensemble of spins that includes all interactions (including static dephasing). T2 is a measure of the loss of coherence that excludes static dephasing, using an RF pulse to reverse the slowest types of dipolar interaction. There is in fact a continuum of interaction time-scales in a given biological sample, and the properties of the refocusing RF pulse can be tuned to refocus more than just static dephasing. In general, the rate of decay of an ensemble of spins is a function of the interaction times and also the power of the RF pulse. This type of decay, occurring under the influence of RF, is known as T1ρ. It is similar to T2 decay but with some slower dipolar interactions refocused, as well as static interactions, hence T1ρ≥T2.
Fluid attenuated inversion recovery (FLAIR).
Fluid Attenuated Inversion Recovery (FLAIR) is an inversion-recovery pulse sequence used
to nullify the signal from fluids. For example, it can be used in brain imaging to suppress cerebrospinal fluid (CSF) so as to bring out periventricular hyperintense lesions, such as multiple sclerosis (MS) plaques. By carefully choosing the inversion time TI (the time between the inversion and excitation pulses), the signal from any particular tissue can be suppressed.
Susceptibility weighted imaging (SWI).
Susceptibility weighted imaging (SWI), is a new type of contrast in MRI different from spin density, "T"1, or "T"2 imaging. This method exploits the susceptibility differences between tissues and uses a fully velocity compensated, three dimensional, RF spoiled, high-resolution, 3D gradient echo scan. This special data acquisition and image processing produces an enhanced contrast magnitude image very sensitive to venous blood, hemorrhage and iron storage. It is used to enhance the detection and diagnosis of tumors, vascular and neurovascular diseases (stroke and hemorrhage), multiple sclerosis, Alzheimer's, and also detects traumatic brain injuries that may not be diagnosed using other methods.
Neuromelanin imaging.
This method exploits the paramagnetic properties of neuromelanin and can be used to visualize the substantia nigra and the locus coeruleus. It is used to detect the atrophy of these nuclei in Parkinson's disease and other parkinsonisms, and also detects signal intensity changes in major depressive disorder and schizophrenia.

</doc>
<doc id="19716" url="https://en.wikipedia.org/wiki?curid=19716" title="Magnetism">
Magnetism

Magnetism is a class of physical phenomena that are mediated by magnetic fields. Electric currents and the magnetic moments of elementary particles give rise to a magnetic field, which acts on other currents and magnetic moments. Every material is influenced to some extent by a magnetic field. The most familiar effect is on permanent magnets, which have persistent magnetic moments caused by ferromagnetism. Most materials do not have permanent moments. Some are attracted to a magnetic field (paramagnetism); others are repulsed by a magnetic field (diamagnetism); others have a more complex relationship with an applied magnetic field (spin glass behavior and antiferromagnetism). Substances that are negligibly affected by magnetic fields are known as "non-magnetic" substances. These include copper, aluminium, gases, and plastic. Pure oxygen exhibits magnetic properties when cooled to a liquid state.
The magnetic state (or magnetic phase) of a material depends on temperature and other variables such as pressure and the applied magnetic field. A material may exhibit more than one form of magnetism as these variables change.
History.
Aristotle attributed the first of what could be called a scientific discussion on magnetism to Thales of Miletus, who lived from about 625 BC to about 545 BC. Around the same time, in ancient India, the Indian surgeon, Sushruta, was the first to make use of the magnet for surgical purposes.
In ancient China, the earliest literary reference to magnetism lies in a 4th-century BC book named after its author, "The Master of Demon Valley" (鬼谷子): "The lodestone makes iron come or it attracts it." The earliest mention of the attraction of a needle appears in a work composed between AD 20 and 100 ("Louen-heng"): "A lodestone attracts a needle." The Chinese scientist Shen Kuo (1031–1095) was the first person to write of the magnetic needle compass and that it improved the accuracy of navigation by employing the astronomical concept of true north "(Dream Pool Essays", AD 1088), and by the 12th century the Chinese were known to use the lodestone compass for navigation. They sculpted a directional spoon from lodestone in such a way that the handle of the spoon always pointed south.
Alexander Neckam, by 1187, was the first in Europe to describe the compass and its use for navigation. In 1269, Peter Peregrinus de Maricourt wrote the "Epistola de magnete", the first extant treatise describing the properties of magnets. In 1282, the properties of magnets and the dry compass were discussed by Al-Ashraf, a Yemeni physicist, astronomer, and geographer.
In 1600, William Gilbert published his "De Magnete, Magneticisque Corporibus, et de Magno Magnete Tellure" ("On the Magnet and Magnetic Bodies, and on the Great Magnet the Earth"). In this work he describes many of his experiments with his model earth called the terrella. From his experiments, he concluded that the Earth was itself magnetic and that this was the reason compasses pointed north (previously, some believed that it was the pole star (Polaris) or a large magnetic island on the north pole that attracted the compass).
An understanding of the relationship between electricity and magnetism began in 1819 with work by Hans Christian Ørsted, a professor at the University of Copenhagen, who discovered more or less by accident that an electric current could influence a compass needle. This landmark experiment is known as Ørsted's Experiment. Several other experiments followed, with André-Marie Ampère, who in 1820 discovered that the magnetic field circulating in a closed-path was related to the current flowing through the perimeter of the path; Carl Friedrich Gauss; Jean-Baptiste Biot and Félix Savart, both of whom in 1820 came up with the Biot–Savart law giving an equation for the magnetic field from a current-carrying wire; Michael Faraday, who in 1831 found that a time-varying magnetic flux through a loop of wire induced a voltage, and others finding further links between magnetism and electricity. James Clerk Maxwell synthesized and expanded these insights into Maxwell's equations, unifying electricity, magnetism, and optics into the field of electromagnetism. In 1905, Einstein used these laws in motivating his theory of special relativity, requiring that the laws held true in all inertial reference frames.
Electromagnetism has continued to develop into the 21st century, being incorporated into the more fundamental theories of gauge theory, quantum electrodynamics, electroweak theory, and finally the standard model.
Sources of magnetism.
Magnetism, at its root, arises from two sources:
Ordinarily, the enormous number of electrons in a material are arranged such that their magnetic moments (both orbital and intrinsic) cancel out. This is due, to some extent, to electrons combining into pairs with opposite intrinsic magnetic moments as a result of the Pauli exclusion principle (see "electron configuration"), or combining into filled subshells with zero net orbital motion. In both cases, the electron arrangement is so as to exactly cancel the magnetic moments from each electron. Moreover, even when the electron configuration "is" such that there are unpaired electrons and/or non-filled subshells, it is often the case that the various electrons in the solid will contribute magnetic moments that point in different, random directions, so that the material will not be magnetic.
However, sometimes—either spontaneously, or owing to an applied external magnetic field—each of the electron magnetic moments will be, on average, lined up. Then the material can produce a net total magnetic field, which can potentially be quite strong.
The magnetic behavior of a material depends on its structure, particularly its electron configuration, for the reasons mentioned above, and also on the temperature. At high temperatures, random thermal motion makes it more difficult for the electrons to maintain alignment.
Materials.
Diamagnetism.
Diamagnetism appears in all materials, and is the tendency of a material to oppose an applied magnetic field, and therefore, to be repelled by a magnetic field. However, in a material with paramagnetic properties (that is, with a tendency to enhance an external magnetic field), the paramagnetic behavior dominates. Thus, despite its universal occurrence, diamagnetic behavior is observed only in a purely diamagnetic material. In a diamagnetic material, there are no unpaired electrons, so the intrinsic electron magnetic moments cannot produce any bulk effect. In these cases, the magnetization arises from the electrons' orbital motions, which can be understood classically as follows:
Note that this description is meant only as an heuristic; a proper understanding requires a quantum-mechanical description.
Note that all materials undergo this orbital response. However, in paramagnetic and ferromagnetic substances, the diamagnetic effect is overwhelmed by the much stronger effects caused by the unpaired electrons.
Paramagnetism.
In a paramagnetic material there are "unpaired electrons", i.e. atomic or molecular orbitals with exactly one electron in them. While paired electrons are required by the Pauli exclusion principle to have their intrinsic ('spin') magnetic moments pointing in opposite directions, causing their magnetic fields to cancel out, an unpaired electron is free to align its magnetic moment in any direction. When an external magnetic field is applied, these magnetic moments will tend to align themselves in the same direction as the applied field, thus reinforcing it.
Ferromagnetism.
A ferromagnet, like a paramagnetic substance, has unpaired electrons. However, in "addition" to the electrons' intrinsic magnetic moment's tendency to be parallel to an "applied field", there is also in these materials a tendency for these magnetic moments to orient parallel to "each other" to maintain a lowered-energy state. Thus, even in the absence of an applied field, the magnetic moments of the electrons in the material spontaneously line up parallel to one another.
Every ferromagnetic substance has its own individual temperature, called the Curie temperature, or Curie point, above which it loses its ferromagnetic properties. This is because the thermal tendency to disorder overwhelms the energy-lowering due to ferromagnetic order.
Ferromagnetism only occurs in a few substances; the common ones are iron, nickel, cobalt, their alloys, and some alloys of rare earth metals.
Magnetic domains.
The magnetic moments of atoms in a ferromagnetic material cause them to behave something like tiny permanent magnets. They stick together and align themselves into small regions of more or less uniform alignment called magnetic domains or Weiss domains. Magnetic domains can be observed with a magnetic force microscope to reveal magnetic domain boundaries that resemble white lines in the sketch. There are many scientific experiments that can physically show magnetic fields.
When a domain contains too many molecules, it becomes unstable and divides into two domains aligned in opposite directions so that they stick together more stably as shown at the right.
When exposed to a magnetic field, the domain boundaries move so that the domains aligned with the magnetic field grow and dominate the structure (dotted yellow area) as shown at the left. When the magnetizing field is removed, the domains may not return to an unmagnetized state. This results in the ferromagnetic material's being magnetized, forming a permanent magnet.
When magnetized strongly enough that the prevailing domain overruns all others to result in only one single domain, the material is magnetically saturated. When a magnetized ferromagnetic material is heated to the Curie point temperature, the molecules are agitated to the point that the magnetic domains lose the organization and the magnetic properties they cause cease. When the material is cooled, this domain alignment structure spontaneously returns, in a manner roughly analogous to how a liquid can freeze into a crystalline solid.
Antiferromagnetism.
In an antiferromagnet, unlike a ferromagnet, there is a tendency for the intrinsic magnetic moments of neighboring valence electrons to point in "opposite" directions. When all atoms are arranged in a substance so that each neighbor is 'anti-aligned', the substance is antiferromagnetic. Antiferromagnets have a zero net magnetic moment, meaning no field is produced by them. Antiferromagnets are less common compared to the other types of behaviors, and are mostly observed at low temperatures. In varying temperatures, antiferromagnets can be seen to exhibit diamagnetic and ferrimagnetic properties.
In some materials, neighboring electrons want to point in opposite directions, but there is no geometrical arrangement in which "each" pair of neighbors is anti-aligned. This is called a spin glass, and is an example of geometrical frustration.
Ferrimagnetism.
Like ferromagnetism, ferrimagnets retain their magnetization in the absence of a field. However, like antiferromagnets, neighboring pairs of electron spins like to point in opposite directions. These two properties are not contradictory, because in the optimal geometrical arrangement, there is more magnetic moment from the sublattice of electrons that point in one direction, than from the sublattice that points in the opposite direction.
Most ferrites are ferrimagnetic. The first discovered magnetic substance, magnetite, is a ferrite and was originally believed to be a ferromagnet; Louis Néel disproved this, however, after discovering ferrimagnetism.
Superparamagnetism.
When a ferromagnet or ferrimagnet is sufficiently small, it acts like a single magnetic spin that is subject to Brownian motion. Its response to a magnetic field is qualitatively similar to the response of a paramagnet, but much larger.
Electromagnet.
An "electromagnet" is a type of magnet whose magnetism is produced by the flow of electric current. The magnetic field disappears when the current ceases.
Magnetism, electricity, and special relativity.
As a consequence of Einstein's theory of special relativity, electricity and magnetism are fundamentally interlinked. Both magnetism lacking electricity, and electricity without magnetism, are inconsistent with special relativity, due to such effects as length contraction, time dilation, and the fact that the magnetic force is velocity-dependent. However, when both electricity and magnetism are taken into account, the resulting theory (electromagnetism) is fully consistent with special relativity. In particular, a phenomenon that appears purely electric or purely magnetic to one observer may be a mix of both to another, or more generally the relative contributions of electricity and magnetism are dependent on the frame of reference. Thus, special relativity "mixes" electricity and magnetism into a single, inseparable phenomenon called electromagnetism, analogous to how relativity "mixes" space and time into spacetime.
All observations on electromagnetism apply to what might be considered to be primarily magnetism, e.g. perturbations in the magnetic field are necessarily accompanied by a nonzero electric field, and propagate at the speed of light.
Magnetic fields in a material.
In a vacuum,
where is the vacuum permeability.
In a material, 
The quantity is called "magnetic polarization".
If the field is small, the response of the magnetization in a diamagnet or paramagnet is approximately linear:
the constant of proportionality being called the magnetic susceptibility. If so,
In a hard magnet such as a ferromagnet, is not proportional to the field and is generally nonzero even when is zero (see Remanence).
Magnetic force.
The phenomenon of magnetism is "mediated" by the magnetic field. An electric current or magnetic dipole creates a magnetic field, and that field, in turn, imparts magnetic forces on other particles that are in the fields.
Maxwell's equations, which simplify to the Biot–Savart law in the case of steady currents, describe the origin and behavior of the fields that govern these forces. Therefore, magnetism is seen whenever electrically charged particles are in motion—for example, from movement of electrons in an electric current, or in certain cases from the orbital motion of electrons around an atom's nucleus. They also arise from "intrinsic" magnetic dipoles arising from quantum-mechanical spin.
The same situations that create magnetic fields—charge moving in a current or in an atom, and intrinsic magnetic dipoles—are also the situations in which a magnetic field has an effect, creating a force. Following is the formula for moving charge; for the forces on an intrinsic dipole, see magnetic dipole.
When a charged particle moves through a magnetic field B, it feels a Lorentz force F given by the cross product:
where 
Because this is a cross product, the force is perpendicular to both the motion of the particle and the magnetic field. It follows that the magnetic force does no work on the particle; it may change the direction of the particle's movement, but it cannot cause it to speed up or slow down. The magnitude of the force is
where formula_8 is the angle between v and B.
One tool for determining the direction of the velocity vector of a moving charge, the magnetic field, and the force exerted is labeling the index finger "V", the middle finger "B", and the thumb "F" with your right hand. When making a gun-like configuration, with the middle finger crossing under the index finger, the fingers represent the velocity vector, magnetic field vector, and force vector, respectively. See also right hand rule.
Magnetic dipoles.
A very common source of magnetic field shown in nature is a dipole, with a "South pole" and a "North pole", terms dating back to the use of magnets as compasses, interacting with the Earth's magnetic field to indicate North and South on the globe. Since opposite ends of magnets are attracted, the north pole of a magnet is attracted to the south pole of another magnet. The Earth's North Magnetic Pole (currently in the Arctic Ocean, north of Canada) is physically a south pole, as it attracts the north pole of a compass.
A magnetic field contains energy, and physical systems move toward configurations with lower energy. When diamagnetic material is placed in a magnetic field, a "magnetic dipole" tends to align itself in opposed polarity to that field, thereby lowering the net field strength. When ferromagnetic material is placed within a magnetic field, the magnetic dipoles align to the applied field, thus expanding the domain walls of the magnetic domains.
Magnetic monopoles.
Since a bar magnet gets its ferromagnetism from electrons distributed evenly throughout the bar, when a bar magnet is cut in half, each of the resulting pieces is a smaller bar magnet. Even though a magnet is said to have a north pole and a south pole, these two poles cannot be separated from each other. A monopole—if such a thing exists—would be a new and fundamentally different kind of magnetic object. It would act as an isolated north pole, not attached to a south pole, or vice versa. Monopoles would carry "magnetic charge" analogous to electric charge. Despite systematic searches since 1931, , they have never been observed, and could very well not exist.
Nevertheless, some theoretical physics models predict the existence of these magnetic monopoles. Paul Dirac observed in 1931 that, because electricity and magnetism show a certain symmetry, just as quantum theory predicts that individual positive or negative electric charges can be observed without the opposing charge, isolated South or North magnetic poles should be observable. Using quantum theory Dirac showed that if magnetic monopoles exist, then one could explain the quantization of electric charge—that is, why the observed elementary particles carry charges that are multiples of the charge of the electron.
Certain grand unified theories predict the existence of monopoles which, unlike elementary particles, are solitons (localized energy packets). The initial results of using these models to estimate the number of monopoles created in the big bang contradicted cosmological observations—the monopoles would have been so plentiful and massive that they would have long since halted the expansion of the universe. However, the idea of inflation (for which this problem served as a partial motivation) was successful in solving this problem, creating models in which monopoles existed but were rare enough to be consistent with current observations.
Quantum-mechanical origin of magnetism.
In principle all kinds of magnetism originate (similar to superconductivity) from specific quantum-mechanical phenomena (e.g. Mathematical formulation of quantum mechanics, in particular the chapters on spin and on the Pauli principle).
A successful model was developed already in 1927, by Walter Heitler and Fritz London, who derived quantum-mechanically, how hydrogen molecules are formed from hydrogen atoms, i.e. from the atomic hydrogen orbitals formula_9 and formula_10 centered at the nuclei "A" and "B", see below. That this leads to magnetism is not at all obvious, but will be explained in the following.
According to the Heitler-London theory, so-called two-body molecular formula_11-orbitals are formed, namely the resulting orbital is:
Here the last product means that a first electron, r1, is in an atomic hydrogen-orbital centered at the second nucleus, whereas the second electron runs around the first nucleus. This "exchange" phenomenon is an expression for the quantum-mechanical property that particles with identical properties cannot be distinguished. It is specific not only for the formation of chemical bonds, but as we will see, also for magnetism, i.e. in this connection the term exchange interaction arises, a term which is essential for the origin of magnetism, and which is stronger, roughly by factors 100 and even by 1000, than the energies arising from the electrodynamic dipole-dipole interaction.
As for the "spin function" formula_13, which is responsible for the magnetism, we have the already mentioned Pauli's principle, namely that a symmetric orbital (i.e. with the + sign as above) must be multiplied with an antisymmetric spin function (i.e. with a − sign), and "vice versa". Thus:
I.e., not only formula_15 and formula_10 must be substituted by "α" and "β", respectively (the first entity means "spin up", the second one "spin down"), but also the sign + by the − sign, and finally ri by the discrete values "s"i (= ±½); thereby we have formula_17 and formula_18. The "singlet state", i.e. the − sign, means: the spins are "antiparallel", i.e. for the solid we have antiferromagnetism, and for two-atomic molecules one has diamagnetism. The tendency to form a (homoeopolar) chemical bond (this means: the formation of a "symmetric" molecular orbital, i.e. with the + sign) results through the Pauli principle automatically in an "antisymmetric" spin state (i.e. with the − sign). In contrast, the Coulomb repulsion of the electrons, i.e. the tendency that they try to avoid each other by this repulsion, would lead to an "antisymmetric" orbital function (i.e. with the − sign) of these two particles, and complementary to a "symmetric" spin function (i.e. with the + sign, one of the so-called "triplet functions"). Thus, now the spins would be "parallel" (ferromagnetism in a solid, paramagnetism in two-atomic gases).
The last-mentioned tendency dominates in the metals iron, cobalt and nickel, and in some rare earths, which are "ferromagnetic". Most of the other metals, where the first-mentioned tendency dominates, are "nonmagnetic" (e.g. sodium, aluminium, and magnesium) or "antiferromagnetic" (e.g. manganese). Diatomic gases are also almost exclusively diamagnetic, and not paramagnetic. However, the oxygen molecule, because of the involvement of π-orbitals, is an exception important for the life-sciences.
The Heitler-London considerations can be generalized to the Heisenberg model of magnetism (Heisenberg 1928).
The explanation of the phenomena is thus essentially based on all subtleties of quantum mechanics, whereas the electrodynamics covers mainly the phenomenology.
Living things.
Some organisms can detect magnetic fields, a phenomenon known as magnetoception. Magnetobiology studies magnetic fields as a medical treatment; fields naturally produced by an organism are known as biomagnetism.

</doc>
<doc id="1070395" url="https://en.wikipedia.org/wiki?curid=1070395" title="Magnon">
Magnon

A magnon is a quasiparticle, a collective excitation of the electrons' spin structure in a crystal lattice. In the equivalent wave picture of quantum mechanics, a magnon can be viewed as a quantized spin wave. Magnons carry a fixed amount of energy and lattice momentum, and are spin-1, indicating they obey boson behavior.
Brief history.
The concept of a magnon was introduced in 1930 by Felix Bloch in order to explain the reduction of the spontaneous magnetization in a ferromagnet. At absolute zero temperature, a Heisenberg ferromagnet reaches the state of lowest energy, in which all of the atomic spins (and hence magnetic moments) point in the same direction. As the temperature increases, more and more spins deviate randomly from the alignment, increasing the internal energy and reducing the net magnetization. If one views the perfectly magnetized state at zero temperature as the vacuum state of the ferromagnet, the low-temperature state with a few misaligned spins can be viewed as a gas of quasiparticles, in this case magnons. Each magnon reduces the total spin along the direction of magnetization by one unit of ħ and the magnetization by γħ, where γ is the gyromagnetic ratio. This leads to Bloch's law for the temperature dependence of spontaneous magnetization:
formula_1
The quantitative theory of magnons, quantized spin waves, was developed further by Theodore Holstein, Henry Primakoff (1940), and Freeman Dyson (1956). Using the second quantization formalism they showed that magnons behave as weakly interacting quasiparticles obeying Bose–Einstein statistics (bosons). A comprehensive treatment can be found in the solid state textbook by Charles Kittel or the early review article by Van Kranendonk and Van Vleck.
Direct experimental detection of magnons by inelastic neutron scattering in ferrite was achieved in 1957 by Bertram Brockhouse. Since then magnons have been detected in ferromagnets, ferrimagnets, and antiferromagnets.
The fact that magnons obey the Bose–Einstein statistics was confirmed by the light scattering experiments done during the 1960s through the 1980s. Classical theory predicts equal intensity of Stokes and anti-Stokes lines. However, the scattering showed that if the magnon energy is comparable to or smaller than the thermal energy, or formula_2, then the Stokes line becomes more intense, as follows from Bose–Einstein statistics. Bose–Einstein condensation of magnons was proven in an antiferromagnet at low temperatures by Nikuni "et al." and in a ferrimagnet by Demokritov "et al." at room temperature. Recently Uchida "et al." reported the generation of spin currents by surface plasmon resonance.
Properties.
Magnon behavior can be studied with a variety of scattering techniques. Magnons behave as a Bose gas with no chemical potential. Microwave pumping can be used to excite spin waves and create additional non-equilibrium magnons which thermalize into phonons. At a critical density, a condensate is formed, which appears as the emission of monochromatic microwaves. This microwave source can be tuned with an applied magnetic field.

</doc>
<doc id="194990" url="https://en.wikipedia.org/wiki?curid=194990" title="Main diagonal">
Main diagonal

In linear algebra, the main diagonal (sometimes principal diagonal, primary diagonal, leading diagonal, or major diagonal) of a matrix formula_1 is the collection of entries formula_2 where formula_3. The following three matrices have their main diagonals indicated by red 1's: 
The antidiagonal (sometimes counterdiagonal, secondary diagonal, trailing diagonal or minor diagonal) of a dimension formula_5 square matrix, formula_6, is the collection of entries formula_7 such that formula_8. That is, it runs from the top right corner to the bottom left corner:

</doc>
<doc id="4643400" url="https://en.wikipedia.org/wiki?curid=4643400" title="Majorana fermion">
Majorana fermion

A Majorana fermion (), also referred to as a Majorana particle, is a fermion that is its own antiparticle. They were hypothesized by Ettore Majorana in 1937. The term is sometimes used in opposition to a Dirac fermion, which describes fermions that are not their own antiparticles.
All of the Standard Model fermions except the neutrino behave as Dirac fermions at low energy (after electroweak symmetry breaking), but the (massive) nature of the neutrino is not settled and it may be either Dirac or Majorana. In condensed matter physics, Majorana fermions exist as quasiparticle excitations in superconductors and can be used to form Majorana bound states governed by non-abelian statistics.
Theory.
The concept goes back to Majorana's suggestion in 1937 that neutral spin-1/2 particles can be described by a real wave equation (the Majorana equation), and would therefore be identical to their antiparticle (because the wave functions of particle and antiparticle are related by complex conjugation).
The difference between Majorana fermions and Dirac fermions can be expressed mathematically in terms of the creation and annihilation operators of second quantization. The creation operator formula_1 creates a fermion in quantum state formula_2 (described by a "real" wave function), whereas the annihilation operator formula_3 annihilates it (or, equivalently, creates the corresponding antiparticle). For a Dirac fermion the operators formula_1 and formula_3 are distinct, whereas for a Majorana fermion they are identical. 
In supersymmetry models, neutralinos--superpartners of gauge bosons and Higgs bosons--are Majorana.
Elementary particle.
Because particles and antiparticles have opposite conserved charges, in order to be a Majorana fermion, namely, it is its own antiparticle, it is necessarily uncharged. All of the elementary fermions of the Standard Model have gauge charges, so they cannot have fundamental Majorana masses. However, the right-handed sterile neutrinos introduced to explain neutrino oscillation could have Majorana masses. If they do, then at low energy (after electroweak symmetry breaking), by the seesaw mechanism, the neutrino fields would naturally behave as six Majorana fields, with three expected to have very high masses (comparable to the GUT scale) and the other three expected to have very low masses (comparable to 1 eV). If right-handed neutrinos exist but do not have a Majorana mass, the neutrinos would instead behave as three Dirac fermions and their antiparticles with masses coming directly from the Higgs interaction, like the other Standard Model fermions.
The seesaw mechanism is appealing because it would naturally explain why the observed neutrino masses are so small. However, if the neutrinos are Majorana then they violate the conservation of lepton number and even B − L.
Neutrinoless double beta decay, which can be viewed as two beta decay events with the produced antineutrinos immediately annihilating with one another, is only possible if neutrinos are their own antiparticles. Experiments are underway to search for this type of decay.
The high-energy analog of the neutrinoless double beta decay process is the production of same sign charged lepton pairs at hadron colliders; it is being searched for by both the ATLAS and CMS experiments at the Large Hadron Collider. In theories based on left–right symmetry, there is a deep connection between these processes.
In the most accepted explanation of the smallness of neutrino mass, the seesaw mechanism, the neutrino is naturally a Majorana fermion.
Majorana fermions cannot possess intrinsic electric or magnetic moments, only toroidal moments. Such minimal interaction with electromagnetic fields makes them potential candidates for cold dark matter.
Majorana bound states.
In superconducting materials, Majorana fermions can emerge as (non-fundamental) quasiparticles (which are more commonly referred as Bogoliubov quasiparticles in condensed matter.). This becomes possible because a quasiparticle in a superconductor is its own antiparticle. Majorana fermions (i.e. the Bogoliubov quasiparticles) in superconductors were observed by many experiments many years ago.
Mathematically, the superconductor imposes electron hole "symmetry" on the quasiparticle excitations, relating the creation operator formula_6 at energy formula_7 to the annihilation operator formula_8 at energy formula_9. Majorana fermions can be bound to a defect at zero energy, and then the combined objects are called Majorana bound states or Majorana zero modes. This name is more appropriate than Majorana fermion (although the distinction is not always made in the literature), because the statistics of these objects is no longer fermionic. Instead, the Majorana bound states are an example of non-abelian anyons: interchanging them changes the state of the system in a way that depends only on the order in which the exchange was performed. The non-abelian statistics that Majorana bound states possess allows them to be used as a building block for a topological quantum computer.
A quantum vortex in certain superconductors or superfluids can trap midgap states, so this is one source of Majorana bound states. Shockley states at the end points of superconducting wires or line defects are an alternative, purely electrical, source. An altogether different source uses the fractional quantum Hall effect as a substitute for the superconductor.
Experiments in superconductivity.
In 2008, Fu and Kane provided a groundbreaking development by theoretically predicting that Majorana bound states can appear at the interface between topological insulators and superconductors. Many proposals of a similar spirit soon followed, where it was shown that Majorana bound states can appear even without any topological insulator. An intense search to provide experimental evidence of Majorana bound states in superconductors first produced some positive results in 2012. A team from the Kavli Institute of Nanoscience at Delft University of Technology in the Netherlands reported an experiment involving indium antimonide nanowires connected to a circuit with a gold contact at one end and a slice of superconductor at the other. When exposed to a moderately strong magnetic field the apparatus showed a peak electrical conductance at zero voltage that is consistent with the formation of a pair of Majorana bound states, one at either end of the region of the nanowire in contact with the superconductor. This type of bounded state with zero energy was soon detected by several other groups in similar hybrid devices.
This experiment from Delft marks a possible verification of independent 2010 theoretical proposals from two groups predicting the solid state manifestation of Majorana bound states in semiconducting wires. However, it was also pointed out that some other trivial non-topological bounded states could highly mimic the zero voltage conductance peak of Majorana bound state.
In 2014, evidence of Majorana bound states was observed using a low-temperature scanning tunneling microscope, by scientists at Princeton University. It was suggested that Majorana bound states appeared at the edges of a chain of iron atoms formed on the surface of superconducting lead. Physicist Jason Alicea of California Institute of Technology, not involved in the research, said the study offered "compelling evidence" for Majorana fermions but that "we should keep in mind possible alternative explanations—even if there are no immediately obvious candidates".

</doc>
<doc id="628198" url="https://en.wikipedia.org/wiki?curid=628198" title="Majoron">
Majoron

In particle physics, majorons (named after Ettore Majorana) are a hypothetical type of Goldstone boson that are theorized to mediate the neutrino mass violation of lepton number or "B" − "L" in certain high energy collisions such as
Where two electrons collide to form two W bosons and the majoron J. The U(1)B–L symmetry is assumed to be global so that the majoron isn't "eaten up" by the gauge boson and spontaneously broken. Majorons were originally formulated in four dimensions by Y. Chikashige, R. N. Mohapatra and R. D. Peccei to understand neutrino masses by the seesaw mechanism and are being searched for in the neutrino-less double beta decay process. There are theoretical extensions of this idea into supersymmetric theories and theories involving extra compactified dimensions. By propagating through the extra spatial dimensions the detectable number of majoron creation events vary accordingly. Mathematically, majorons may be modeled by allowing them to propagate through a material while all other Standard Model forces are fixed to an orbifold point.
Searches.
Experiments studying double beta decay have set limits on decay modes that emit majorons.
NEMO has observed a variety of elements . EXO and Kamland-Zen have set half-life limits for majoron decays in xenon.

</doc>
<doc id="2073470" url="https://en.wikipedia.org/wiki?curid=2073470" title="Manifold">
Manifold

In mathematics, a manifold is a topological space that resembles Euclidean space near each point. More precisely, each point of an "n"-dimensional manifold has a neighbourhood that is homeomorphic to the Euclidean space of dimension "n". Lines and circles, but not figure eights, are one-dimensional manifolds. Two-dimensional manifolds are also called surfaces. Examples include the plane, the sphere, and the torus, which can all be embedded in three dimensional real space, but also the Klein bottle and real projective plane which cannot.
Although a manifold resembles Euclidean space near each point, globally it may not. For example, the surface of the sphere is not a Euclidean space, but in a region it can be charted by means of map projections of the region into the Euclidean plane (in the context of manifolds they are called "charts"). When a region appears in two neighbouring charts, the two representations do not coincide exactly and a transformation is needed to pass from one to the other, called a "transition map".
The concept of a manifold is central to many parts of geometry and modern mathematical physics because it allows more complicated structures to be described and understood in terms of the relatively well-understood properties of Euclidean space. Manifolds naturally arise as solution sets of systems of equations and as graphs of functions. Manifolds may have additional features. One important class of manifolds is the class of differentiable manifolds.
This differentiable structure allows calculus to be done on manifolds. A Riemannian metric on a manifold allows distances and angles to be measured. Symplectic manifolds serve as the phase spaces in the Hamiltonian formalism of classical mechanics, while four-dimensional Lorentzian manifolds model spacetime in general relativity.
Motivational examples.
Circle.
After a line, the circle is the simplest example of a topological manifold. Topology ignores bending, so a small piece of a circle is treated exactly the same as a small piece of a line. Consider, for instance, the top part of the unit circle, "x"2 + "y"2 = 1, where the "y"-coordinate is positive (indicated by the yellow circular arc in "Figure 1"). Any point of this arc can be uniquely described by its "x"-coordinate. So, projection onto the first coordinate is a continuous, and invertible, mapping from the upper arc to the open interval (−1,1):
Such functions along with the open regions they map are called "charts". Similarly, there are charts for the bottom (red), left (blue), and right (green) parts of the circle:
Together, these parts cover the whole circle and the four charts form an atlas for the circle.
The top and right charts overlap: their intersection lies in the quarter of the circle where both the "x"- and the "y"-coordinates are positive. The two charts χtop and χright each map this part into the interval (0, 1). Thus a function "T" from (0, 1) to itself can be constructed, which first uses the inverse of the top chart to reach the circle and then follows the right chart back to the interval. Let "a" be any number in (0, 1), then:
Such a function is called a "transition map".
The top, bottom, left, and right charts show that the circle is a manifold, but they do not form the only possible atlas. Charts need not be geometric projections, and the number of charts is a matter of some choice. Consider the charts
and
Here "s" is the slope of the line through the point at coordinates ("x","y") and the fixed pivot point (−1, 0); "t" follows similarly, but with pivot point (+1, 0). The inverse mapping from "s" to ("x", "y") is given by
It can easily be confirmed that "x"2 + "y"2 = 1 for all values of the slope "s". These two charts provide a second atlas for the circle, with
Each chart omits a single point, either (−1, 0) for "s" or (+1, 0) for "t", so neither chart alone is sufficient to cover the whole circle. It can be proved that it is not possible to cover the full circle with a single chart. For example, although it is possible to construct a circle from a single line interval by overlapping and "gluing" the ends, this does not produce a chart; a portion of the circle will be mapped to both ends at once, losing invertibility.
Enriched circle.
Viewed using calculus, the circle transition function "T" is simply a function between open intervals, which gives a meaning to the statement that "T" is differentiable. The transition map "T", and all the others, are differentiable on (0, 1); therefore, with this atlas the circle is a "differentiable manifold". It is also "smooth" and "analytic" because the transition functions have these properties as well.
Other circle properties allow it to meet the requirements of more specialized types of manifold. For example, the circle has a notion of distance between two points, the arc-length between the points; hence it is a "Riemannian manifold".
Sphere.
The sphere is an example of a manifold of dimension 2. The unit sphere of implicit equation
may be covered by an atlas of six charts: the plane divides the sphere into two half spheres ( and ), which may both be mapped on the disc by the projection on the plane of coordinates. This provides two charts; the four other charts are provided by a similar construction with the two other coordinate planes.
As for the circle, one may define one chart that covers the whole sphere excluding one point. Thus two charts are sufficient, but the sphere cannot be covered by a single chart.
This example is historically significant, as it has motivated the terminology; it became apparent that the whole surface of the Earth cannot have a plane representation consisting of a single map (also called "chart", see nautical chart), and therefore one needs atlases for covering the whole Earth surface.
Other curves.
Manifolds need not be connected (all in "one piece"); an example is a pair of separate circles.
Manifolds need not be closed; thus a line segment without its end points is a manifold. And they are never countable, unless the dimension of the manifold is 0. Putting these freedoms together, other examples of manifolds are a parabola, a hyperbola (two open, infinite pieces), and the locus of points on a cubic curve "y"2 = "x"3−"x" (a closed loop piece and an open, infinite piece).
However, excluded are examples like two touching circles that share a point to form a figure-8; at the shared point a satisfactory chart cannot be created. Even with the bending allowed by topology, the vicinity of the shared point looks like a "+", not a line. A "+" is not homeomorphic to a closed interval (line segment), since deleting the center point from the "+" gives a space with four components (i.e. pieces), whereas deleting a point from a closed interval gives a space with at most two pieces; topological operations always preserve the number of pieces.
History.
The study of manifolds combines many important areas of mathematics: it generalizes concepts such as curves and surfaces as well as ideas from linear algebra and topology.
Early development.
Before the modern concept of a manifold there were several important results.
Non-Euclidean geometry considers spaces where Euclid's parallel postulate fails. Saccheri first studied them in 1733. Lobachevsky, Bolyai, and Riemann developed them 100 years later. Their research uncovered two types of spaces whose geometric structures differ from that of classical Euclidean space; these gave rise to hyperbolic geometry and elliptic geometry. In the modern theory of manifolds, these notions correspond to Riemannian manifolds with constant negative and positive curvature, respectively.
Carl Friedrich Gauss may have been the first to consider abstract spaces as mathematical objects in their own right. His theorema egregium gives a method for computing the curvature of a surface without considering the ambient space in which the surface lies. Such a surface would, in modern terminology, be called a manifold; and in modern terms, the theorem proved that the curvature of the surface is an intrinsic property. Manifold theory has come to focus exclusively on these intrinsic properties (or invariants), while largely ignoring the extrinsic properties of the ambient space.
Another, more topological example of an intrinsic property of a manifold is its Euler characteristic. Leonhard Euler showed that for a convex polytope in the three-dimensional Euclidean space with "V" vertices (or corners), "E" edges, and "F" faces,
The same formula will hold if we project the vertices and edges of the polytope onto a sphere, creating a topological map with "V" vertices, "E" edges, and "F" faces, and in fact, will remain true for any spherical map, even if it does not arise from any convex polytope. Thus 2 is a topological invariant of the sphere, called its Euler characteristic. On the other hand, a torus can be sliced open by its 'parallel' and 'meridian' circles, creating a map with "V" = 1 vertex, "E" = 2 edges, and "F" = 1 face. Thus the Euler characteristic of the torus is 1 − 2 + 1 = 0. The Euler characteristic of other surfaces is a useful topological invariant, which can be extended to higher dimensions using Betti numbers. In the mid nineteenth century, the Gauss–Bonnet theorem linked the Euler characteristic to the Gaussian curvature.
Synthesis.
Investigations of Niels Henrik Abel and Carl Gustav Jacobi on inversion of elliptic integrals in the first half of 19th century led them to consider special types of complex manifolds, now known as Jacobians. Bernhard Riemann further contributed to their theory, clarifying the geometric meaning of the process of analytic continuation of functions of complex variables.
Another important source of manifolds in 19th century mathematics was analytical mechanics, as developed by Siméon Poisson, Jacobi, and William Rowan Hamilton. The possible states of a mechanical system are thought to be points of an abstract space, phase space in Lagrangian and Hamiltonian formalisms of classical mechanics. This space is, in fact, a high-dimensional manifold, whose dimension corresponds to the degrees of freedom of the system and where the points are specified by their generalized coordinates. For an unconstrained movement of free particles the manifold is equivalent to the Euclidean space, but various conservation laws constrain it to more complicated formations, e.g. Liouville tori. The theory of a rotating solid body, developed in the 18th century by Leonhard Euler and Joseph-Louis Lagrange, gives another example where the manifold is nontrivial. Geometrical and topological aspects of classical mechanics were emphasized by Henri Poincaré, one of the founders of topology.
Riemann was the first one to do extensive work generalizing the idea of a surface to higher dimensions. The name "manifold" comes from Riemann's original German term, "Mannigfaltigkeit", which William Kingdon Clifford translated as "manifoldness". In his Göttingen inaugural lecture, Riemann described the set of all possible values of a variable with certain constraints as a "Mannigfaltigkeit", because the variable can have "many" values. He distinguishes between "stetige Mannigfaltigkeit" and "diskrete" "Mannigfaltigkeit" ("continuous manifoldness" and "discontinuous manifoldness"), depending on whether the value changes continuously or not. As continuous examples, Riemann refers to not only colors and the locations of objects in space, but also the possible shapes of a spatial figure. Using induction, Riemann constructs an "n-fach ausgedehnte Mannigfaltigkeit" ("n times extended manifoldness" or "n-dimensional manifoldness") as a continuous stack of (n−1) dimensional manifoldnesses. Riemann's intuitive notion of a "Mannigfaltigkeit" evolved into what is today formalized as a manifold. Riemannian manifolds and Riemann surfaces are named after Riemann.
Poincaré's definition.
In his very influential paper, Analysis Situs, Henri Poincaré gave a definition of a (differentiable) manifold ("variété") which served as a precursor to the modern concept of a manifold.
In the first section of Analysis Situs, Poincaré defines a manifold as the level set of a continuously differentiable function between Euclidean spaces that satisfies the nondegeneracy hypothesis of the implicit function theorem. In the third section, he begins by remarking that the graph of a continuously differentiable function is a manifold in the latter sense. He then proposes a new, more general, definition of manifold based on a 'chain of manifolds' ("une chaîne des variétés").
Poincaré's notion of a 'chain of manifolds' is a precursor to the modern notion of atlas. In particular, he considers two manifolds defined respectively as graphs of functions formula_11 and formula_12. If these manifolds overlap ("a une partie commune"), then he requires that the coordinates formula_13 depend continuously differentiably on the coordinates formula_14 and vice versa ('"...les formula_15 sont fonctions analytiques des formula_16 et inversement"'). In this way he introduces a precursor to the notion of a chart and of a transition map. Note that it is implicit in Analysis Situs that a manifold obtained as a 'chain' is a subset of Euclidean space.
For example, the unit circle in the plane can be thought of as the graph of the function formula_17 or else the function formula_18 in a neighborhood of every point except the points (1,0) and (−1,0); and in a neighborhood of those points, it can be thought of as the graph of, respectively, formula_19 and formula_20. The reason the circle can be represented by a graph in the neighborhood of every point is because the left hand side of its defining equation formula_21 has nonzero gradient at every point of the circle. By the implicit function theorem, every submanifold of Euclidean space is locally the graph of a function.
Hermann Weyl gave an intrinsic definition for differentiable manifolds in his lecture course on Riemann surfaces in 1911–1912, opening the road to the general concept of a topological space that followed shortly. During the 1930s Hassler Whitney and others clarified the foundational aspects of the subject, and thus intuitions dating back to the latter half of the 19th century became precise, and developed through differential geometry and Lie group theory. Notably, the Whitney embedding theorem showed that the intrinsic definition in terms of charts was equivalent to Poincaré's definition in terms of subsets of Euclidean space.
Topology of manifolds: highlights.
Two-dimensional manifolds, also known as a 2D "surfaces" embedded in our common 3D space, were considered by Riemann under the guise of Riemann surfaces, and rigorously classified in the beginning of the 20th century by Poul Heegaard and Max Dehn. Henri Poincaré pioneered the study of three-dimensional manifolds and raised a fundamental question about them, today known as the Poincaré conjecture. After nearly a century of effort by many mathematicians, starting with Poincaré himself, a consensus among experts (as of 2006) is that Grigori Perelman has proved the Poincaré conjecture (see the Solution of the Poincaré conjecture). William Thurston's geometrization program, formulated in the 1970s, provided a far-reaching extension of the Poincaré conjecture to the general three-dimensional manifolds. Four-dimensional manifolds were brought to the forefront of mathematical research in the 1980s by Michael Freedman and in a different setting, by Simon Donaldson, who was motivated by the then recent progress in theoretical physics (Yang–Mills theory), where they serve as a substitute for ordinary 'flat' spacetime. Andrey Markov Jr. showed in 1960 that no algorithm exists for classifying four-dimensional manifolds. Important work on higher-dimensional manifolds, including analogues of the Poincaré conjecture, had been done earlier by René Thom, John Milnor, Stephen Smale and Sergei Novikov. One of the most pervasive and flexible techniques underlying much work on the topology of manifolds is Morse theory.
Mathematical definition.
Informally, a manifold is a space that is "modeled on" Euclidean space.
There are many different kinds of manifolds and generalizations.
In geometry and topology, all manifolds are topological manifolds, possibly with additional structure, most often a differentiable structure. In terms of constructing manifolds via patching, a manifold has an additional structure if the transition maps between different patches satisfy axioms beyond just continuity. For instance, differentiable manifolds have homeomorphisms on overlapping neighborhoods diffeomorphic with each other, so that the manifold has a well-defined set of functions which are differentiable in each neighborhood, and so differentiable on the manifold as a whole.
Formally, a topological manifold is a second countable Hausdorff space that is locally homeomorphic to Euclidean space.
"Second countable" and "Hausdorff" are point-set conditions;
"second countable" excludes spaces which are in some sense 'too large' such as the long line, while "Hausdorff" excludes spaces such as "the line with two origins" (these generalizations of manifolds are discussed in non-Hausdorff manifolds).
"Locally homeomorphic" to Euclidean space means that every point has a neighborhood homeomorphic to an open Euclidean "n"-ball,
Generally manifolds are taken to have a fixed dimension (the space must be locally homeomorphic to a fixed "n"-ball), and such a space is called an "n"-manifold; however, some authors admit manifolds where different points can have different dimensions. If a manifold has a fixed dimension, it is called a pure manifold. For example, the sphere has a constant dimension of 2 and is therefore a pure manifold whereas the disjoint union of a sphere and a line in three-dimensional space is "not" a pure manifold. Since dimension is a local invariant (i.e. the map sending each point to the dimension of its neighbourhood over which a chart is defined, is locally constant), each connected component has a fixed dimension.
Scheme-theoretically, a manifold is a locally ringed space, whose structure sheaf is locally isomorphic to the sheaf of continuous (or differentiable, or complex-analytic, etc.) functions on Euclidean space. This definition is mostly used when discussing analytic manifolds in algebraic geometry.
Broad definition.
The broadest common definition of manifold is a topological space locally homeomorphic to a topological vector space over the reals. This omits the point-set axioms, allowing higher cardinalities and non-Hausdorff manifolds; and it omits finite dimension, allowing structures such as Hilbert manifolds to be modeled on Hilbert spaces, Banach manifolds to be modeled on Banach spaces, and Fréchet manifolds to be modeled on Fréchet spaces. Usually one relaxes one or the other condition: manifolds with the point-set axioms are studied in general topology, while infinite-dimensional manifolds are studied in functional analysis.
Charts, atlases, and transition maps.
The spherical Earth is navigated using flat maps or charts, collected in an atlas. Similarly, a differentiable manifold can be described using mathematical maps, called "coordinate charts", collected in a mathematical "atlas". It is not generally possible to describe a manifold with just one chart, because the global structure of the manifold is different from the simple structure of the charts. For example, no single flat map can represent the entire Earth without separation of adjacent features across the map's boundaries or duplication of coverage. When a manifold is constructed from multiple overlapping charts, the regions where they overlap carry information essential to understanding the global structure.
Charts.
A coordinate map, a coordinate chart, or simply a chart, of a manifold is an invertible map between a subset of the manifold and a simple space such that both the map and its inverse preserve the desired structure. For a topological manifold, the simple space is some Euclidean space R"n" and interest focuses on the topological structure. This structure is preserved by homeomorphisms, invertible maps that are continuous in both directions.
In the case of a differentiable manifold, a set of charts called an atlas allows us to do calculus on manifolds. Polar coordinates, for example, form a chart for the plane R2 minus the positive "x"-axis and the origin. Another example of a chart is the map χtop mentioned in the section above, a chart for the circle.
Atlases.
The description of most manifolds requires more than one chart (a single chart is adequate for only the simplest manifolds). A specific collection of charts which covers a manifold is called an atlas. An atlas is not unique as all manifolds can be covered multiple ways using different combinations of charts. Two atlases are said to be C"k"-equivalent if their union is also a C"k" atlas.
The atlas containing all possible charts consistent with a given atlas is called the maximal atlas (i.e. an equivalence class containing that given atlas (under the already defined equivalence relation given in the previous paragraph)). Unlike an ordinary atlas, the maximal atlas of a given manifold is unique. Though it is useful for definitions, it is an abstract object and not used directly (e.g. in calculations).
Transition maps.
Charts in an atlas may overlap and a single point of a manifold may be represented in several charts. If two charts overlap, parts of them represent the same region of the manifold, just as a map of Europe and a map of Asia may both contain Moscow. Given two overlapping charts, a transition function can be defined which goes from an open ball in R"n" to the manifold and then back to another (or perhaps the same) open ball in R"n". The resultant map, like the map "T" in the circle example above, is called a change of coordinates, a coordinate transformation, a transition function, or a transition map.
Additional structure.
An atlas can also be used to define additional structure on the manifold. The structure is first defined on each chart separately. If all the transition maps are compatible with this structure, the structure transfers to the manifold.
This is the standard way differentiable manifolds are defined. If the transition functions of an atlas for a topological manifold preserve the natural differential structure of R"n" (that is, if they are diffeomorphisms), the differential structure transfers to the manifold and turns it into a differentiable manifold. Complex manifolds are introduced in an analogous way by requiring that the transition functions of an atlas are holomorphic functions. For symplectic manifolds, the transition functions must be symplectomorphisms.
The structure on the manifold depends on the atlas, but sometimes different atlases can be said to give rise to the same structure. Such atlases are called compatible.
These notions are made precise in general through the use of pseudogroups.
Manifold with boundary.
A manifold with boundary is a manifold with an edge. For example, a sheet of paper is a 2-manifold with a 1-dimensional boundary. The boundary of an "n"-manifold with boundary is an ("n" − 1)-manifold. A disk (circle plus interior) is a 2-manifold with boundary. Its boundary is a circle, a 1-manifold. A square with interior is also a 2-manifold with boundary. A ball (sphere plus interior) is a 3-manifold with boundary. Its boundary is a sphere, a 2-manifold. (See also Boundary (topology)).
In technical language, a manifold with boundary is a space containing both interior points and boundary points. Every interior point has a neighborhood homeomorphic to the open "n"-ball {("x"1, "x"2, …, "x""n") | Σ "x""i"2 < 1}. Every boundary point has a neighborhood homeomorphic to the "half" "n"-ball {("x"1, "x"2, …, "x""n") | Σ "x""i"2 < 1 and "x"1 ≥ 0}. The homeomorphism must send each boundary point to a point with "x"1 = 0.
Boundary and interior.
Let "M" be a manifold with boundary. The interior of "M", denoted Int "M", is the set of points in "M" which have neighborhoods homeomorphic to an open subset of R"n". The boundary of "M", denoted ∂"M", is the complement of Int "M" in "M". The boundary points can be characterized as those points which land on the boundary hyperplane ("x""n" = 0) of R"n"+ under some coordinate chart.
If "M" is a manifold with boundary of dimension "n", then Int "M" is a manifold (without boundary) of dimension "n" and ∂"M" is a manifold (without boundary) of dimension "n" − 1.
Construction.
A single manifold can be constructed in different ways, each stressing a different aspect of the manifold, thereby leading to a slightly different viewpoint.
Charts.
Perhaps the simplest way to construct a manifold is the one used in the example above of the circle. First, a subset of R2 is identified, and then an atlas covering this subset is constructed. The concept of "manifold" grew historically from constructions like this. Here is another example, applying this method to the construction of a sphere:
Sphere with charts.
A sphere can be treated in almost the same way as the circle. In mathematics a sphere is just the surface (not the solid interior), which can be defined as a subset of R3:
The sphere is two-dimensional, so each chart will map part of the sphere to an open subset of R2. Consider the northern hemisphere, which is the part with positive "z" coordinate (coloured red in the picture on the right). The function χ defined by
maps the northern hemisphere to the open unit disc by projecting it on the ("x", "y") plane. A similar chart exists for the southern hemisphere. Together with two charts projecting on the ("x", "z") plane and two charts projecting on the ("y", "z") plane, an atlas of six charts is obtained which covers the entire sphere.
This can be easily generalized to higher-dimensional spheres.
Patchwork.
A manifold can be constructed by gluing together pieces in a consistent manner, making them into overlapping charts. This construction is possible for any manifold and hence it is often used as a characterisation, especially for differentiable and Riemannian manifolds. It focuses on an atlas, as the patches naturally provide charts, and since there is no exterior space involved it leads to an intrinsic view of the manifold.
The manifold is constructed by specifying an atlas, which is itself defined by transition maps. A point of the manifold is therefore an equivalence class of points which are mapped to each other by transition maps. Charts map equivalence classes to points of a single patch. There are usually strong demands on the consistency of the transition maps. For topological manifolds they are required to be homeomorphisms; if they are also diffeomorphisms, the resulting manifold is a differentiable manifold.
This can be illustrated with the transition map "t" = 1⁄"s" from the second half of the circle example. Start with two copies of the line. Use the coordinate "s" for the first copy, and "t" for the second copy. Now, glue both copies together by identifying the point "t" on the second copy with the point "s" = 1⁄"t" on the first copy (the points "t" = 0 and "s" = 0 are not identified with any point on the first and second copy, respectively). This gives a circle.
Intrinsic and extrinsic view.
The first construction and this construction are very similar, but they represent rather different points of view. In the first construction, the manifold is seen as embedded in some Euclidean space. This is the "extrinsic view". When a manifold is viewed in this way, it is easy to use intuition from Euclidean spaces to define additional structure. For example, in a Euclidean space it is always clear whether a vector at some point is tangential or normal to some surface through that point.
The patchwork construction does not use any embedding, but simply views the manifold as a topological space by itself. This abstract point of view is called the "intrinsic view". It can make it harder to imagine what a tangent vector might be, and there is no intrinsic notion of a normal bundle, but instead there is an intrinsic stable normal bundle.
"n"-Sphere as a patchwork.
The "n"-sphere S"n" is a generalisation of the idea of a circle (1-sphere) and sphere (2-sphere) to higher dimensions. An "n"-sphere S"n" can be constructed by gluing together two copies of R"n". The transition map between them is defined as
This function is its own inverse and thus can be used in both directions. As the transition map is a smooth function, this atlas defines a smooth manifold.
In the case "n" = 1, the example simplifies to the circle example given earlier.
Identifying points of a manifold.
It is possible to define different points of a manifold to be same. This can be visualized as gluing these points together in a single point, forming a quotient space. There is, however, no reason to expect such quotient spaces to be manifolds. Among the possible quotient spaces that are not necessarily manifolds, orbifolds and CW complexes are considered to be relatively well-behaved. An example of a quotient space of a manifold that is also a manifold is the real projective space identified as a quotient space of the corresponding sphere.
One method of identifying points (gluing them together) is through a right (or left) action of a group, which acts on the manifold. Two points are identified if one is moved onto the other by some group element. If "M" is the manifold and "G" is the group, the resulting quotient space is denoted by "M" / "G" (or "G" \ "M").
Manifolds which can be constructed by identifying points include tori and real projective spaces (starting with a plane and a sphere, respectively).
Gluing along boundaries.
Two manifolds with boundaries can be glued together along a boundary. If this is done the right way, the result is also a manifold. Similarly, two boundaries of a single manifold can be glued together.
Formally, the gluing is defined by a bijection between the two boundaries. Two points are identified when they are mapped onto each other. For a topological manifold this bijection should be a homeomorphism, otherwise the result will not be a topological manifold. Similarly for a differentiable manifold it has to be a diffeomorphism. For other manifolds other structures should be preserved.
A finite cylinder may be constructed as a manifold by starting with a strip [0, 1] × [0, 1] and gluing a pair of opposite edges on the boundary by a suitable diffeomorphism. A projective plane may be obtained by gluing a sphere with a hole in it to a Möbius strip along their respective circular boundaries.
Cartesian products.
The Cartesian product of manifolds is also a manifold.
The dimension of the product manifold is the sum of the dimensions of its factors. Its topology is the product topology, and a Cartesian product of charts is a chart for the product manifold. Thus, an atlas for the product manifold can be constructed using atlases for its factors. If these atlases define a differential structure on the factors, the corresponding atlas defines a differential structure on the product manifold. The same is true for any other structure defined on the factors. If one of the factors has a boundary, the product manifold also has a boundary. Cartesian products may be used to construct tori and finite cylinders, for example, as S1 × S1 and S1 × [0, 1], respectively.
Manifolds with additional structure.
Topological manifolds.
The simplest kind of manifold to define is the topological manifold, which looks locally like some "ordinary" Euclidean space R"n". Formally, a topological manifold is a topological space locally homeomorphic to a Euclidean space. This means that every point has a neighbourhood for which there exists a homeomorphism (a bijective continuous function whose inverse is also continuous) mapping that neighbourhood to R"n". These homeomorphisms are the charts of the manifold.
It is to be noted that a "topological" manifold looks locally like a Euclidean space in a rather weak manner: while for each individual chart it is possible to distinguish differentiable functions or measure distances and angles, merely by virtue of being a topological manifold a space does not have any "particular" and "consistent" choice of such concepts. In order to discuss such properties for a manifold, one needs to specify further structure and consider differentiable manifolds and Riemannian manifolds discussed below. In particular, the same underlying topological manifold can have several mutually incompatible classes of differentiable functions and an infinite number of ways to specify distances and angles.
Usually additional technical assumptions on the topological space are made to exclude pathological cases. It is customary to require that the space be Hausdorff and second countable.
The "dimension" of the manifold at a certain point is the dimension of the Euclidean space that the charts at that point map to (number "n" in the definition). All points in a connected manifold have the same dimension. Some authors require that all charts of a topological manifold map to Euclidean spaces of same dimension. In that case every topological manifold has a topological invariant, its dimension. Other authors allow disjoint unions of topological manifolds with differing dimensions to be called manifolds.
Differentiable manifolds.
For most applications a special kind of topological manifold, namely a differentiable manifold, is used. If the local charts on a manifold are compatible in a certain sense, one can define directions, tangent spaces, and differentiable functions on that manifold. In particular it is possible to use calculus on a differentiable manifold. Each point of an "n"-dimensional differentiable manifold has a tangent space. This is an "n"-dimensional Euclidean space consisting of the tangent vectors of the curves through the point.
Two important classes of differentiable manifolds are smooth and analytic manifolds. For smooth manifolds the transition maps are smooth, that is infinitely differentiable. Analytic manifolds are smooth manifolds with the additional condition that the transition maps are analytic (they can be expressed as power series). The sphere can be given analytic structure, as can most familiar curves and surfaces.
There are also topological manifolds, i.e., locally Euclidean spaces, which possess no differentiable structures at all.
A rectifiable set generalizes the idea of a piecewise smooth or rectifiable curve to higher dimensions; however, rectifiable sets are not in general manifolds.
Riemannian manifolds.
To measure distances and angles on manifolds, the manifold must be Riemannian. A 'Riemannian manifold' is a differentiable manifold in which each tangent space is equipped with an inner product ⟨⋅,⋅⟩ in a manner which varies smoothly from point to point. Given two tangent vectors u and v, the inner product ⟨u,v⟩ gives a real number. The dot (or scalar) product is a typical example of an inner product. This allows one to define various notions such as length, angles, areas (or volumes), curvature, gradients of functions and divergence of vector fields.
All differentiable manifolds (of constant dimension) can be given the structure of a Riemannian manifold. The Euclidean space itself carries a natural structure of Riemannian manifold (the tangent spaces are naturally identified with the Euclidean space itself and carry the standard scalar product of the space). Many familiar curves and surfaces, including for example all "n"-spheres, are specified as subspaces of a Euclidean space and inherit a metric from their embedding in it.
Finsler manifolds.
A Finsler manifold allows the definition of distance but does not require the concept of angle; it is an analytic manifold in which each tangent space is equipped with a norm, ||·||, in a manner which varies smoothly from point to point. This norm can be extended to a metric, defining the length of a curve; but it cannot in general be used to define an inner product.
Any Riemannian manifold is a Finsler manifold.
Lie groups.
Lie groups, named after Sophus Lie, are differentiable manifolds that carry also the structure of a group which is such that the group operations are defined by smooth maps.
A Euclidean vector space with the group operation of vector addition is an example of a non-compact Lie group. 
A simple example of a compact Lie group is the circle: the group operation is simply rotation. This group, known as U(1), can be also characterised as the group of complex numbers of modulus 1 with multiplication as the group operation.
Other examples of Lie groups include special groups of matrices, which are all subgroups of the general linear group, the group of "n" by "n" matrices with non-zero determinant. If the matrix entries are real numbers, this will be an "n"2-dimensional disconnected manifold. The orthogonal groups, the symmetry groups of the sphere and hyperspheres, are "n"("n"−1)/2 dimensional manifolds, where "n"−1 is the dimension of the sphere. Further examples can be found in the table of Lie groups.
Classification and invariants.
Different notions of manifolds have different notions of classification and invariant; in this section we focus on smooth closed manifolds.
The classification of smooth closed manifolds is well-understood "in principle", except in dimension 4: in low dimensions (2 and 3) it is geometric, via the uniformization theorem and the solution of the Poincaré conjecture, and in high dimension (5 and above) it is algebraic, via surgery theory. This is a classification in principle: the general question of whether two smooth manifolds are diffeomorphic is not computable in general. Further, specific computations remain difficult, and there are many open questions.
Orientable surfaces can be visualized, and their diffeomorphism classes enumerated, by genus. Given two orientable surfaces, one can determine if they are diffeomorphic by computing their respective genera and comparing: they are diffeomorphic if and only if the genera are equal, so the genus forms a complete set of invariants.
This is much harder in higher dimensions: higher-dimensional manifolds cannot be directly visualized (though visual intuition is useful in understanding them), nor can their diffeomorphism classes be enumerated, nor can one in general determine if two different descriptions of a higher-dimensional manifold refer to the same object.
However, one can determine if two manifolds are "different" if there is some intrinsic characteristic that differentiates them. Such criteria are commonly referred to as invariants, because, while they may be defined in terms of some presentation (such as the genus in terms of a triangulation), they are the same relative to all possible descriptions of a particular manifold: they are "invariant" under different descriptions.
Naively, one could hope to develop an arsenal of invariant criteria that would definitively classify all manifolds up to isomorphism. Unfortunately, it is known that for manifolds of dimension 4 and higher, no program exists that can decide whether two manifolds are diffeomorphic.
Smooth manifolds have a rich set of invariants, coming from point-set topology, 
classic algebraic topology, and geometric topology. The most familiar invariants, which are visible for surfaces, are orientability (a normal invariant, also detected by homology) and genus (a homological invariant).
Smooth closed manifolds have no local invariants (other than dimension), though geometric manifolds have local invariants, notably the curvature of a Riemannian manifold and the torsion of a manifold equipped with an affine connection.
This distinction between local invariants and no local invariants is a common way to distinguish between geometry and topology. All invariants of a smooth closed manifold are thus global.
Algebraic topology is a source of a number of important global invariant properties. Some key criteria include the "simply connected" property and orientability (see below). Indeed, several branches of mathematics, such as homology and homotopy theory, and the theory of characteristic classes were founded in order to study invariant properties of manifolds.
Examples of surfaces.
Orientability.
In dimensions two and higher, a simple but important invariant criterion is the question of whether a manifold admits a meaningful orientation. 
Consider a topological manifold with charts mapping to R"n". Given an ordered basis for R"n", a chart causes its piece of the manifold to itself acquire a sense of ordering, which in 3-dimensions can be viewed as either right-handed or left-handed. Overlapping charts are not required to agree in their sense of ordering, which gives manifolds an important freedom. For some manifolds, like the sphere, charts can be chosen so that overlapping regions agree on their "handedness"; these are "orientable" manifolds. For others, this is impossible. The latter possibility is easy to overlook, because any closed surface embedded (without self-intersection) in three-dimensional space is orientable.
Some illustrative examples of non-orientable manifolds include: (1) the Möbius strip, which is a manifold with boundary, (2) the Klein bottle, which must intersect itself in its 3-space representation, and (3) the real projective plane, which arises naturally in geometry. 
Möbius strip.
Begin with an infinite circular cylinder standing vertically, a manifold without boundary. Slice across it high and low to produce two circular boundaries, and the cylindrical strip between them. This is an orientable manifold with boundary, upon which "surgery" will be performed. Slice the strip open, so that it could unroll to become a rectangle, but keep a grasp on the cut ends. Twist one end 180°, making the inner surface face out, and glue the ends back together seamlessly. This results in a strip with a permanent half-twist: the Möbius strip. Its boundary is no longer a pair of circles, but (topologically) a single circle; and what was once its "inside" has merged with its "outside", so that it now has only a "single" side.
Klein bottle.
Take two Möbius strips; each has a single loop as a boundary. Straighten out those loops into circles, and let the strips distort into cross-caps. Gluing the circles together will produce a new, closed manifold without boundary, the Klein bottle. Closing the surface does nothing to improve the lack of orientability, it merely removes the boundary. Thus, the Klein bottle is a closed surface with no distinction between inside and outside. Note that in three-dimensional space, a Klein bottle's surface must pass through itself. Building a Klein bottle which is not self-intersecting requires four or more dimensions of space.
Real projective plane.
Begin with a sphere centered on the origin. Every line through the origin pierces the sphere in two opposite points called "antipodes". Although there is no way to do so physically, it is possible (by considering a quotient space) to mathematically merge each antipode pair into a single point. The closed surface so produced is the real projective plane, yet another non-orientable surface. It has a number of equivalent descriptions and constructions, but this route explains its name: all the points on any given line through the origin project to the same "point" on this "plane".
Genus and the Euler characteristic.
For two dimensional manifolds a key invariant property is the genus, or the "number of handles" present in a surface. A torus is a sphere with one handle, a double torus is a sphere with two handles, and so on. Indeed, it is possible to fully characterize compact, two-dimensional manifolds on the basis of genus and orientability. In higher-dimensional manifolds genus is replaced by the notion of Euler characteristic, and more generally Betti numbers and homology and cohomology.
Maps of manifolds.
Just as there are various types of manifolds, there are various types of maps of manifolds. In addition to continuous functions and smooth functions generally, there are maps with special properties. In geometric topology a basic type are embeddings, of which knot theory is a central example, and generalizations such as immersions, submersions, covering spaces, and ramified covering spaces.
Basic results include the Whitney embedding theorem and Whitney immersion theorem.
In Riemannian geometry, one may ask for maps to preserve the Riemannian metric, leading to notions of isometric embeddings, isometric immersions, and Riemannian submersions; a basic result is the Nash embedding theorem.
Scalar-valued functions.
A basic example of maps between manifolds are scalar-valued functions on a manifold,
sometimes called regular functions or functionals, by analogy with algebraic geometry or linear algebra. These are of interest both in their own right, and to study the underlying manifold.
In geometric topology, most commonly studied are Morse functions, which yield handlebody decompositions, while in mathematical analysis, one often studies solution to partial differential equations, an important example of which is harmonic analysis, where one studies harmonic functions: the kernel of the Laplace operator. This leads to such functions as the spherical harmonics, and to heat kernel methods of studying manifolds, such as hearing the shape of a drum and some proofs of the Atiyah–Singer index theorem.

</doc>
<doc id="19048" url="https://en.wikipedia.org/wiki?curid=19048" title="Mass">
Mass

In physics, mass is a property of a physical body which determines the strength of its mutual gravitational attraction to other bodies, its resistance to being accelerated by a force, and in the theory of relativity gives the mass–energy content of a system. The SI unit of mass is the kilogram (kg).
Mass is not the same as weight, even though we often calculate an object's mass by measuring its weight with a spring scale instead of comparing it to known masses. An object on the Moon would weigh less than it would on Earth because of the lower gravity, but it would still have the same mass.
For everyday objects and energies well-described by Newtonian physics, mass describes the amount of matter in an object. However, at very high speeds or for subatomic particles, special relativity shows that energy is an additional source of mass. Thus, any stationary body having mass has an equivalent amount of energy, and all forms of energy resist acceleration by a force and have gravitational attraction.
There are several distinct phenomena which can be used to measure mass. Although some theorists have speculated some of these phenomena could be independent of each other, current experiments have found no difference among any of the ways used to measure mass:
The mass of an object determines its acceleration in the presence of an applied force. This phenomenon is called inertia. According to Newton's second law of motion, if a body of fixed mass "m" is subjected to a single force "F", its acceleration "a" is given by "F"/"m". A body's mass also determines the degree to which it generates or is affected by a gravitational field. If a first body of mass "m"A is placed at a distance "r" (center of mass to center of mass) from a second body of mass "m"B, each body experiences an attractive force , where is the "universal gravitational constant". This is sometimes referred to as gravitational mass. Repeated experiments since the 17th century have demonstrated that inertial and gravitational mass are identical; since 1915, this observation has been entailed "a priori" in the equivalence principle of general relativity.
Units of mass.
The standard International System of Units (SI) unit of mass is the kilogram (kg). The kilogram is 1000 grams (g), first defined in 1795 as one cubic decimeter of water at the melting point of ice. Then in 1889, the kilogram was redefined as the mass of the international prototype kilogram, and as such is independent of the meter, or the properties of water. As of January 2013, there are several proposals for redefining the kilogram yet again, including a proposal for defining it in terms of the Planck constant.
Other units are accepted for use in SI:
Outside the SI system, other units include:
Definitions of mass.
In physical science, one may distinguish conceptually between at least seven different aspects of "mass", or seven physical notions that involve the concept of "mass": Every experiment to date has shown these seven values to be proportional, and in some cases equal, and this proportionality gives rise to the abstract concept of mass.
Weight vs. mass.
In everyday usage, mass and "weight" are often used interchangeably. For instance, a person's weight may be stated as 75 kg. In a constant gravitational field, the weight of an object is proportional to its mass, and it is unproblematic to use the same unit for both concepts. But because of slight differences in the strength of the Earth's gravitational field at different places, the distinction becomes important for measurements with a precision better than a few percent, and for places far from the surface of the Earth, such as in space or on other planets. Conceptually, "mass" (measured in kilograms) refers to an intrinsic property of an object, whereas "weight" (measured in newtons) measures an object's resistance to deviating from its natural course of free fall, which can be influenced by the nearby gravitational field. No matter how strong the gravitational field, objects in free fall are weightless, though they still have mass.
The force known as "weight" is proportional to mass and acceleration in all situations where the mass is accelerated away from free fall. For example, when a body is at rest in a gravitational field (rather than in free fall), it must be accelerated by a force from a scale or the surface of a planetary body such as the Earth or the Moon. This force keeps the object from going into free fall. Weight is the opposing force in such circumstances, and is thus determined by the acceleration of free fall. On the surface of the Earth, for example, an object with a mass of 50 kilograms weighs 491 newtons, which means that 491 newtons is being applied to keep the object from going into free fall. By contrast, on the surface of the Moon, the same object still has a mass of 50 kilograms but weighs only 81.5 newtons, because only 81.5 newtons is required to keep this object from going into a free fall on the moon. Restated in mathematical terms, on the surface of the Earth, the weight "W" of an object is related to its mass "m" by , where is the acceleration due to Earth's gravitational field, (expressed as the acceleration experienced by a free-falling object).
For other situations, such as when objects are subjected to mechanical accelerations from forces other than the resistance of a planetary surface, the weight force is proportional to the mass of an object multiplied by the total acceleration away from free fall, which is called the proper acceleration. Through such mechanisms, objects in elevators, vehicles, centrifuges, and the like, may experience weight forces many times those caused by resistance to the effects of gravity on objects, resulting from planetary surfaces. In such cases, the generalized equation for weight "W" of an object is related to its mass "m" by the equation , where "a" is the proper acceleration of the object caused by all influences other than gravity. (Again, if gravity is the only influence, such as occurs when an object falls freely, its weight will be zero).
Macroscopically, mass is associated with matter, although matter is not, ultimately, as clearly defined a concept as mass. On the subatomic scale, not only fermions, the particles often associated with matter, but also some bosons, the particles that act as force carriers, have rest mass. Another problem for easy definition is that much of the rest mass of ordinary matter derives from the invariant mass contributed to matter by particles and kinetic energies which have no rest mass themselves (only 1% of the rest mass of matter is accounted for by the rest mass of its fermionic quarks and electrons). From a fundamental physics perspective, mass is the number describing under which the representation of the little group of the Poincaré group a particle transforms. In the Standard Model of particle physics, this symmetry is described as arising as a consequence of a coupling of particles with rest mass to a postulated additional field, known as the Higgs field.
The total mass of the observable universe is estimated at between 1052 kg and 1053 kg, corresponding to the rest mass of between 1079 and 1080 protons.
Inertial vs. gravitational mass.
Although inertial mass, passive gravitational mass and active gravitational mass are conceptually distinct, no experiment has ever unambiguously demonstrated any difference between them. In classical mechanics, Newton's third law implies that active and passive gravitational mass must always be identical (or at least proportional), but the classical theory offers no compelling reason why the gravitational mass has to equal the inertial mass. That it does is merely an empirical fact.
Albert Einstein developed his general theory of relativity starting from the assumption that this correspondence between inertial and (passive) gravitational mass is not accidental: that no experiment will ever detect a difference between them (the weak version of the equivalence principle). However, in the resulting theory, gravitation is not a force and thus not subject to Newton's third law, so "the equality of inertial and "active" gravitational mass [...] remains as puzzling as ever".
The equivalence of inertial and gravitational masses is sometimes referred to as the "Galilean equivalence principle" or the "weak equivalence principle". The most important consequence of this equivalence principle applies to freely falling objects. Suppose we have an object with inertial and gravitational masses "m" and "M", respectively. If the only force acting on the object comes from a gravitational field "g", combining Newton's second law and the gravitational law yields the acceleration
This says that the ratio of gravitational to inertial mass of any object is equal to some constant "K" if and only if all objects fall at the same rate in a given gravitational field. This phenomenon is referred to as the "universality of free-fall". (In addition, the constant "K" can be taken to be 1 by defining our units appropriately.)
The first experiments demonstrating the universality of free-fall were conducted by Galileo. It is commonly stated that Galileo obtained his results by dropping objects from the Leaning Tower of Pisa, but this is most likely apocryphal; actually, he performed his experiments with balls rolling down nearly frictionless inclined planes to slow the motion and increase the timing accuracy. Increasingly precise experiments have been performed, such as those performed by Loránd Eötvös, using the torsion balance pendulum, in 1889. , no deviation from universality, and thus from Galilean equivalence, has ever been found, at least to the precision 10−12. More precise experimental efforts are still being carried out.
The universality of free-fall only applies to systems in which gravity is the only acting force. All other forces, especially friction and air resistance, must be absent or at least negligible. For example, if a hammer and a feather are dropped from the same height through the air on Earth, the feather will take much longer to reach the ground; the feather is not really in "free"-fall because the force of air resistance upwards against the feather is comparable to the downward force of gravity. On the other hand, if the experiment is performed in a vacuum, in which there is no air resistance, the hammer and the feather should hit the ground at exactly the same time (assuming the acceleration of both objects towards each other, and of the ground towards both objects, for its own part, is negligible). This can easily be done in a high school laboratory by dropping the objects in transparent tubes that have the air removed with a vacuum pump. It is even more dramatic when done in an environment that naturally has a vacuum, as David Scott did on the surface of the Moon during Apollo 15.
A stronger version of the equivalence principle, known as the "Einstein equivalence principle" or the "strong equivalence principle", lies at the heart of the general theory of relativity. Einstein's equivalence principle states that within sufficiently small regions of space-time, it is impossible to distinguish between a uniform acceleration and a uniform gravitational field. Thus, the theory postulates that the force acting on a massive object caused by a gravitational field is a result of the object's tendency to move in a straight line (in other words its inertia) and should therefore be a function of its inertial mass and the strength of the gravitational field.
Origin of mass.
In theoretical physics, a mass generation mechanism is a theory which attempts to explain the origin of mass from the most fundamental laws of physics. To date, a number of different models have been proposed which advocate different views at the origin of mass. The problem is complicated by the fact that the notion of mass is strongly related to the gravitational interaction but a theory of the latter has not been yet reconciled with the currently popular model of particle physics, known as the Standard Model.
Pre-Newtonian concepts.
Weight as an amount.
The concept of amount is very old and predates recorded history. Humans, at some early era, realized that the weight of a collection of similar objects was directly proportional to the number of objects in the collection:
where "W" is the weight of the collection of similar objects and "n" is the number of objects in the collection. Proportionality, by definition, implies that two values have a constant ratio:
An early use of this relationship is a balance scale, which balances the force of one object's weight against the force of another object's weight. The two sides of a balance scale are close enough that the objects experience similar gravitational fields. Hence, if they have similar masses then their weights will also be similar. This allows the scale, by comparing weights, to also compare masses.
Consequently, historical weight standards were often defined in terms of amounts. The Romans, for example, used the carob seed (carat or siliqua) as a measurement standard. If an object's weight was equivalent to 1728 carob seeds, then the object was said to weigh one Roman pound. If, on the other hand, the object's weight was equivalent to 144 carob seeds then the object was said to weigh one Roman ounce (uncia). The Roman pound and ounce were both defined in terms of different sized collections of the same common mass standard, the carob seed. The ratio of a Roman ounce (144 carob seeds) to a Roman pound (1728 carob seeds) was:
Planetary motion.
In 1600 AD, Johannes Kepler sought employment with Tycho Brahe, who had some of the most precise astronomical data available. Using Brahe's precise observations of the planet Mars, Kepler spent the next five years developing his own method for characterizing planetary motion. In 1609, Johannes Kepler published his three laws of planetary motion, explaining how the planets orbit the Sun. In Kepler's final planetary model, he described planetary orbits as following elliptical paths with the Sun at a focal point of the ellipse. Kepler discovered that the square of the orbital period of each planet is directly proportional to the cube of the semi-major axis of its orbit, or equivalently, that the ratio of these two values is constant for all planets in the Solar System.
On 25 August 1609, Galileo Galilei demonstrated his first telescope to a group of Venetian merchants, and in early January of 1610, Galileo observed four dim objects near Jupiter, which he mistook for stars. However, after a few days of observation, Galileo realized that these "stars" were in fact orbiting Jupiter. These four objects (later named the Galilean moons in honor of their discoverer) were the first celestial bodies observed to orbit something other than the Earth or Sun. Galileo continued to observe these moons over the next eighteen months, and by the middle of 1611 he had obtained remarkably accurate estimates for their periods.
Galilean free fall.
Sometime prior to 1638, Galileo turned his attention to the phenomenon of objects in free fall, attempting to characterize these motions. Galileo was not the first to investigate Earth's gravitational field, nor was he the first to accurately describe its fundamental characteristics. However, Galileo's reliance on scientific experimentation to establish physical principles would have a profound effect on future generations of scientists. It is unclear if these were just hypothetical experiments used to illustrate a concept, or if they were real experiments performed by Galileo, but the results obtained from these experiments were both realistic and compelling. A biography by Galileo's pupil Vincenzo Viviani stated that Galileo had dropped balls of the same material, but different masses, from the Leaning Tower of Pisa to demonstrate that their time of descent was independent of their mass. In support of this conclusion, Galileo had advanced the following theoretical argument: He asked if two bodies of different masses and different rates of fall are tied by a string, does the combined system fall faster because it is now more massive, or does the lighter body in its slower fall hold back the heavier body? The only convincing resolution to this question is that all bodies must fall at the same rate.
A later experiment was described in Galileo's "Two New Sciences" published in 1638. One of Galileo's fictional characters, Salviati, describes an experiment using a bronze ball and a wooden ramp. The wooden ramp was "12 cubits long, half a cubit wide and three finger-breadths thick" with a straight, smooth, polished groove. The groove was lined with "parchment, also smooth and polished as possible". And into this groove was placed "a hard, smooth and very round bronze ball". The ramp was inclined at various angles to slow the acceleration enough so that the elapsed time could be measured. The ball was allowed to roll a known distance down the ramp, and the time taken for the ball to move the known distance was measured. The time was measured using a water clock described as follows:
Galileo found that for an object in free fall, the distance that the object has fallen is always proportional to the square of the elapsed time:
Galileo had shown that objects in free fall under the influence of the Earth’s gravitational field have a constant acceleration, and Galileo’s contemporary, Johannes Kepler, had shown that the planets follow elliptical paths under the influence of the Sun’s gravitational mass. However, Galileo’s free fall motions and Kepler’s planetary motions remained distinct during Galileo’s lifetime.
Newtonian mass.
Robert Hooke had published his concept of gravitational forces in 1674, stating that all celestial bodies have an attraction or gravitating power towards their own centers, and also attract all the other celestial bodies that are within the sphere of their activity. He further stated that gravitational attraction increases by how much nearer the body wrought upon is to their own center. In correspondence with Isaac Newton from 1679 and 1680, Hooke conjectures that gravitational forces might decrease according to the double of the distance between the two bodies. Hooke urged Newton, who was a pioneer in the development of calculus, to work through the mathematical details of Keplerian orbits to determine if Hooke's hypothesis was correct. Newton's own investigations verified that Hooke was correct, but due to personal differences between the two men, Newton chose not to reveal this to Hooke. Isaac Newton kept quiet about his discoveries until 1684, at which time he told a friend, Edmond Halley, that he had solved the problem of gravitational orbits, but had misplaced the solution in his office. After being encouraged by Halley, Newton decided to develop his ideas about gravity and publish all of his findings. In November 1684, Isaac Newton sent a document to Edmund Halley, now lost but presumed to have been titled "De motu corporum in gyrum" (Latin for "On the motion of bodies in an orbit"). Halley presented Newton's findings to the Royal Society of London, with a promise that a fuller presentation would follow. Newton later recorded his ideas in a three book set, entitled Philosophiæ Naturalis Principia Mathematica (Latin: "Mathematical Principles of Natural Philosophy"). The first was received by the Royal Society on 28 April 1685–6; the second on 2 March 1686–7; and the third on 6 April 1686–7. The Royal Society published Newton’s entire collection at their own expense in May 1686–7.
Isaac Newton had bridged the gap between Kepler’s gravitational mass and Galileo’s gravitational acceleration, resulting in the discovery of the following relationship which governed both of these:
If |a1| is non-zero, the fraction is well-defined, which allows us to measure the inertial mass of "m"1. In this case, "m"2 is our "reference" object, and we can define its mass "m" as (say) 1 kilogram. Then we can measure the mass of any other object in the universe by colliding it with the reference object and measuring the accelerations.
Additionally, mass relates a body's momentum p to its linear velocity v:
and the body's kinetic energy "K" to its velocity:
Atomic mass.
Typically, the mass of objects is measured in relation to that of the kilogram, which is defined as the mass of the "international prototype kilogram" (IPK), a platinum alloy cylinder stored in an environmentally-monitored safe secured in a vault at the International Bureau of Weights and Measures in France. However, the IPK is not convenient for measuring the masses of atoms and particles of similar scale, as it contains trillions of trillions of atoms, and has most certainly lost or gained a little mass over time despite the best efforts to prevent this. It is much easier to precisely compare an atom's mass to that of another atom, thus scientists developed the atomic mass unit. By definition, 1 u is exactly one twelfth of the mass of a carbon-12 atom, and by extension a carbon-12 atom has a mass of exactly 12 u.
Mass in relativity.
Special relativity.
In special relativity, there are two kinds of mass: rest mass (invariant mass), and relativistic mass (which increases with velocity). Rest mass is the Newtonian mass as measured by an observer moving along with the object. "Relativistic mass" is the total quantity of energy in a body or system divided by "c"2. The two are related by the following equation:
where formula_11 is the Lorentz factor:
The invariant mass of systems is the same for observers in all inertial frames, while the relativistic mass depends on the observer's frame of reference. In order to formulate the equations of physics such that mass values do not change between observers, it is convenient to use rest mass. The rest mass of a body is also related to its energy "E" and the magnitude of its momentum p by the relativistic energy-momentum equation:
So long as the system is closed with respect to mass and energy, both kinds of mass are conserved in any given frame of reference. The conservation of mass holds even as some types of particles are converted to others. Particles of matter may be converted to types of energy (e.g. light, kinetic energy, the potential energy in magnetic, electric and other fields) but this does not affect the amount of mass. Although things like heat may not be matter, all types of energy still continue to exhibit mass. Thus, mass and energy do not change into one another in relativity; rather, both are names for the same thing, and neither mass nor energy "appear" without the other.
Both rest and relativistic mass can be expressed as an energy by applying the well-known relationship "E" = "mc"2, yielding rest energy and "relativistic energy" (total system energy) respectively:
The "relativistic" mass and energy concepts are related to their "rest" counterparts, but they do not have the same value as their rest counterparts in systems where there is a net momentum. Because the relativistic mass is proportional to the energy, it has gradually fallen into disuse among physicists. There is disagreement over whether the concept remains useful pedagogically.
In bound systems, the binding energy must often be subtracted from the mass of the unbound system, because binding energy commonly leaves the system at the time it is bound. Mass is not conserved in this process because the system is not closed during the binding process. For example, the binding energy of atomic nuclei is often lost in the form of gamma rays when the nuclei are formed, leaving nuclides which have less mass than the free particles (nucleons) of which they are composed.
General relativity.
In general relativity, the equivalence principle is any of several related concepts dealing with the equivalence of gravitational and inertial mass. At the core of this assertion is Albert Einstein's idea that the gravitational force as experienced locally while standing on a massive body (such as the Earth) is the same as the "pseudo-force" experienced by an observer in a non-inertial (i.e. accelerated) frame of reference.
However, it turns out that it is impossible to find an objective general definition for the concept of invariant mass in general relativity. At the core of the problem is the non-linearity of the Einstein field equations, making it impossible to write the gravitational field energy as part of the stress–energy tensor in a way that is invariant for all observers. For a given observer, this can be achieved by the stress–energy–momentum pseudotensor.
Mass in quantum physics.
In classical mechanics, the inert mass of a particle appears in the Euler–Lagrange equation as a parameter "m":
After quantization, replacing the position vector "x" with a wave function, the parameter "m" appears in the kinetic energy operator:
In the ostensibly covariant (relativistically invariant) Dirac equation, and in natural units, this becomes:
...where the "mass" parameter "m" is now simply a constant associated with the quantum described by the wave function ψ.
In the Standard Model of particle physics as developed in the 1960s, there is the proposal that this term arises from the coupling of the field ψ to an additional field Φ, the so-called Higgs field. In the case of fermions, the Higgs mechanism results in the replacement of the term "m"ψ in the Lagrangian with formula_19. This shifts the explanandum of the value for the mass of each elementary particle to the value of the unknown couplings "G"ψ.
The tentatively confirmed discovery of a massive Higgs boson is regarded as a strong confirmation of this theory. But there is indirect evidence for the reality of the Electroweak symmetry breaking as described by the Higgs mechanism, and the non-existence of Higgs bosons would indicate a "Higgsless" description of this mechanism.
Tachyonic particles and imaginary (complex) mass.
A tachyonic field, or simply tachyon, is a quantum field with an imaginary mass. Although tachyons (particles that move faster than light) are a purely hypothetical concept not generally believed to exist,
 fields with imaginary mass have come to play an important role in modern physics and are discussed in popular books on physics. Under no circumstances do any excitations ever propagate faster than light in such theories – the presence or absence of a tachyonic mass has no effect whatsoever on the maximum velocity of signals (there is no violation of causality). While the "field" may have imaginary mass, any physical particles do not; the "imaginary mass" shows that the system becomes unstable, and sheds the instability by undergoing a type of phase transition called tachyon condensation (closely related to second order phase transitions) that results in symmetry breaking in current models of particle physics.
The term "tachyon" was coined by Gerald Feinberg in a 1967 paper, but it was soon realized that Feinberg's model in fact did not allow for superluminal speeds. Instead, the imaginary mass creates an instability in the configuration:- any configuration in which one or more field excitations are tachyonic will spontaneously decay, and the resulting configuration contains no physical tachyons. This process is known as tachyon condensation. Well known examples include the condensation of the Higgs boson in particle physics, and ferromagnetism in condensed matter physics.
Although the notion of a tachyonic imaginary mass might seem troubling because there is no classical interpretation of an imaginary mass, the mass is not quantized. Rather, the scalar field is; even for tachyonic quantum fields, the field operators at spacelike separated points still commute (or anticommute), thus preserving causality. Therefore, information still does not propagate faster than light, and solutions grow exponentially, but not superluminally (there is no violation of causality). Tachyon condensation drives a physical system that has reached a local limit and might naively be expected to produce physical tachyons, to an alternate stable state where no physical tachyons exist. Once the tachyonic field reaches the minimum of the potential, its quanta are not tachyons any more but rather are ordinary particles with a positive mass-squared.
This is a special case of the general rule, where unstable massive particles are formally described as having a complex mass, with the real part being their mass in usual sense, and the imaginary part being the decay rate in natural units. However, in quantum field theory, a particle (a "one-particle state") is roughly defined as a state which is constant over time; i.e., an eigenvalue of the Hamiltonian. An unstable particle is a state which is only approximately constant over time; If it exists long enough to be measured, it can be formally described as having a complex mass, with the real part of the mass greater than its imaginary part. If both parts are of the same magnitude, this is interpreted as a resonance appearing in a scattering process rather than particle, as it is considered not to exist long enough to be measured independently of the scattering process. In the case of a tachyon the real part of the mass is zero, and hence no concept of a particle can be attributed to it.
In a Lorentz invariant theory, the same formulas that apply to ordinary slower-than-light particles (sometimes called "bradyons" in discussions of tachyons) must also apply to tachyons. In particular the energy–momentum relation:
(where p is the relativistic momentum of the bradyon and m is its rest mass) should still apply, along with the formula for the total energy of a particle:
This equation shows that the total energy of a particle (bradyon or tachyon) contains a contribution from its rest mass (the "rest mass–energy") and a contribution from its motion, the kinetic energy.
When "v" is larger than "c", the denominator in the equation for the energy is "imaginary", as the value under the radical is negative. Because the total energy must be real, the numerator must "also" be imaginary: i.e. the rest mass m must be imaginary, as a pure imaginary number divided by another pure imaginary number is a real number.

</doc>
<doc id="491022" url="https://en.wikipedia.org/wiki?curid=491022" title="Mass in special relativity">
Mass in special relativity

Mass in special relativity incorporates the general understandings from the concept of mass–energy equivalence. Added to this concept is an additional complication resulting from the fact that mass is defined in two different ways in special relativity: one way defines mass ("rest mass" or "invariant mass") as an invariant quantity which is the same for all observers in all reference frames; in the other definition, the measure of mass ("relativistic mass") is dependent on the velocity of the observer.
The term "mass" in special relativity usually refers to the rest mass of the object, which is the Newtonian mass as measured by an observer moving along with the object. The "invariant mass" is another name for the "rest mass" of single particles. The more general invariant mass (calculated with a more complicated formula) loosely corresponds to the "rest mass" of a "system". Thus, invariant mass is a natural unit of mass used for systems which are being viewed from their center of momentum frame (COM frame), as when any closed system (for example a bottle of hot gas) is weighed, which requires that the measurement be taken in the center of momentum frame where the system has no net momentum. Under such circumstances the invariant mass is equal to the relativistic mass (discussed below), which is the total energy of the system divided by "c" (the speed of light) squared.
The concept of invariant mass does not require bound systems of particles, however. As such, it may also be applied to systems of unbound particles in high-speed relative motion. Because of this, it is often employed in particle physics for systems which consist of widely separated high-energy particles. If such systems were derived from a single particle, then the calculation of the invariant mass of such systems, which is a never-changing quantity, will provide the rest mass of the parent particle (because it is conserved over time).
It is often convenient in calculation that the invariant mass of a system is the total energy of the system (divided by "c"2) in the COM frame (where, by definition, the momentum of the system is zero). However, since the invariant mass of any system is also the same quantity in all inertial frames, it is a quantity often calculated from the total energy in the COM frame, then used to calculate system energies and momenta in other frames where the momenta are not zero, and the system total energy will necessarily be a different quantity than in the COM frame. As with energy and momentum, the invariant mass of a system cannot be destroyed or changed, and it is thus conserved, so long as the system is closed to all influences (The technical term is isolated system meaning that an idealized boundary is drawn around the system, and no mass/energy is allowed across it).
The term "relativistic mass" is also sometimes used. This is the sum total quantity of energy in a body or system (divided by "c"2). As seen from the center of momentum frame, the relativistic mass is also the invariant mass, as discussed above (just as the relativistic energy of a single particle is the same as its rest energy, when seen from its rest frame). For other frames, the relativistic mass (of a body or system of bodies) includes a contribution from the "net" kinetic energy of the body (the kinetic energy of the center of mass of the body), and is larger the faster the body moves. Thus, unlike the invariant mass, the "relativistic mass" depends on the observer's frame of reference. However, for given single frames of reference and for isolated systems, the relativistic mass is also a conserved quantity.
Although some authors present relativistic mass as a "fundamental" concept of the theory, it has been argued that this is wrong as the fundamentals of the theory relate to space–time. There is disagreement over whether the concept is pedagogically useful. The notion of mass as a property of an object from Newtonian mechanics does not bear a precise relationship to the concept in relativity.
For a discussion of mass in general relativity, see mass in general relativity. For a general discussion including mass in Newtonian mechanics, see the article on mass.
Terminology.
If a stationary box contains many particles, it weighs more in its rest frame, the faster the particles are moving. Any energy in the box (including the kinetic energy of the particles) adds to the mass, so that the relative motion of the particles contributes to the mass of the box. But if the box itself is moving (its center of mass is moving), there remains the question of whether the kinetic energy of the overall motion should be included in the mass of the system. The invariant mass is calculated excluding the kinetic energy of the system as a whole (calculated using the single velocity of the box, which is to say the velocity of the box's center of mass), while the relativistic mass is calculated including invariant mass PLUS the kinetic energy of the system which is calculated from the velocity of the center of mass.
Relativistic mass and rest mass are both traditional concepts in physics, but the relativistic mass corresponds to the total energy. The relativistic mass is the mass of the system as it would be measured on a scale, but in some cases (such as the box above) this fact remains true only because the system on average must be at rest to be weighed (it must have zero net momentum, which is to say, the measurement is in its center of momentum frame). For example, if an electron in a cyclotron is moving in circles with a relativistic velocity, the weight of the cyclotron+electron system is increased by the relativistic mass of the electron, not by the electron's rest mass. But the same is also true of any closed system, such as an electron-and-box, if the electron bounces at high speed inside the box. It is only the lack of total momentum in the system (the system momenta sum to zero) which allows the kinetic energy of the electron to be "weighed." If the electron is "stopped" and weighed, or the scale were somehow sent after it, it would not be moving with respect to the scale, and again the relativistic and rest masses would be the same for the single electron (and would be smaller). In general, relativistic and rest masses are equal only in systems which have no net momentum and the system center of mass is at rest; otherwise they may be different.
The invariant mass is proportional to the value of the total energy in one reference frame, the frame where the object as a whole is at rest (as defined below in terms of center of mass). This is why the invariant mass is the same as the rest mass for single particles. However, the invariant mass also represents the measured mass when the center of mass is at rest for systems of many particles. This special frame where this occurs is also called the center of momentum frame, and is defined as the inertial frame in which the center of mass of the object is at rest (another way of stating this is that it is the frame in which the momenta of the system's parts add to zero). For compound objects (made of many smaller objects, some of which may be moving) and sets of unbound objects (some of which may also be moving), only the center of mass of the system is required to be at rest, for the object's relativistic mass to be equal to its rest mass.
A so-called "massless" particle (such as a photon, or a theoretical graviton) moves at the speed of light in every frame of reference. In this case there is no transformation that will bring the particle to rest. The total energy of such particles becomes smaller and smaller in frames which move faster and faster in the same direction. As such, they have no rest mass, because they can never be measured in a frame where they are at rest. This property of having no rest mass is what causes these particles to be termed "massless." However, even massless particles have a relativistic mass, which varies with their observed energy in various frames of reference,
Invariant mass.
The invariant mass is the ratio of four-momentum to four-velocity:
and is also the ratio of four-acceleration to four-force when the rest mass is constant. The four-dimensional form of Newton's second law is:
The relativistic energy-momentum equation.
The relativistic expressions for "E" and "p" obey the "relativistic energy–momentum relation":
where the m is the rest mass, or the invariant mass for systems, and "E" is the total energy.
The equation is also valid for photons, which have m = 0:
and therefore
A photon's momentum is a function of its energy, but it is not proportional to the velocity, which is always c.
For an object at rest, the momentum p is zero, therefore
The rest mass is only proportional to the total energy in the rest frame of the object.
When the object is moving, the total energy is given by
To find the form of the momentum and energy as a function of velocity, it can be noted that the four-velocity, which is proportional to formula_8, is the only four-vector associated with the particle's motion, so that if there is a conserved four-momentum formula_9, it must be proportional to this vector. This allows expressing the ratio of energy to momentum as
resulting in a relation between E and v:
This results in 
and
these expressions can be written as
and
When working in units where "c" = 1, known as the natural unit system, all relativistic equations simplify. In particular, all three quantities E, p, m have the same dimension:
The equation is often written this way because the difference formula_18 is the relativistic length of the energy momentum four-vector, a length which is associated with rest mass or invariant mass in systems. If m > 0, then there is the rest frame, where p = 0, this equation states that E = m, revealing once more that invariant mass is the same as the energy in the rest frame.
The mass of composite systems.
The rest mass of a composite system is not the sum of the rest masses of the parts, unless all the parts are at rest. The total mass of a composite system includes the kinetic energy and field energy in the system.
The total energy "E" of a composite system can be determined by adding together the sum of the energies of its components. The total momentum formula_19 of the system, a vector quantity, can also be computed by adding together the momenta of all its components. Given the total energy "E" and the length (magnitude) "p" of the total momentum vector formula_19, the invariant mass is given by:
In a mathematical system where c = 1, for systems of particles (whether bound or unbound) the total system invariant mass is given equivalently by the following:
Where, again, the particle momenta formula_19 are first summed as vectors, and then the square of their resulting total magnitude (Euclidean norm) is used. This results in a scalar number, which is subtracted from the scalar value of the square of the total energy.
For such a system, in the special center of momentum frame where momenta sum to zero, again the system mass (called the invariant mass) corresponds to the total system energy or, in units where c=1, is identical to it. This invariant mass for a system remains the same quantity in any inertial frame, although the system total energy and total momenta are functions of the particular inertial frame which is chosen, and will vary in such a way between inertial frames as to keep the invariant mass the same for all observers. Invariant mass thus functions for systems of particles in the same capacity as "rest mass" does for single particles.
Note that the invariant mass of an isolated system (i.e., one closed to both mass and energy) is also independent of observer or inertial frame, and is a constant, conserved quantity for isolated systems and single observers, even during chemical and nuclear reactions. The concept of invariant mass is widely used in particle physics, because the invariant mass of a particle's decay products is equal to its rest mass. This is used to make measurements of the mass of particles like the Z boson or the top quark.
Conservation versus invariance of mass in special relativity.
Total energy is an additive conserved quantity (for single observers) in systems and in reactions between particles, but rest mass (in the sense of being a sum of particle rest masses) may not be conserved through an event in which rest masses of particles are converted to other types of energy, such as kinetic energy. Finding the sum of individual particle rest masses would require multiple observers, one for each particle rest inertial frame, and these observers ignore individual particle kinetic energy. Conservation laws require a single observer and a single inertial frame.
In general, for isolated systems and single observers, relativistic mass is conserved (each observer sees it constant over time), but is not invariant (that is, different observers see different values). Invariant mass, however, is both conserved "and" invariant (all single observers see the same value, which does not change over time).
The relativistic mass corresponds to the energy, so conservation of energy automatically means that relativistic mass is conserved for any given observer and inertial frame. However, this quantity, like the total energy of a particle, is not invariant. This means that, even though it is conserved for any observer during a reaction, its absolute "value" will change with the frame of the observer, and for different observers in different frames.
By contrast, the rest mass and invariant masses of systems and particles are "both" conserved "and" also invariant. For example: A closed container of gas (closed to energy as well) has a system "rest mass" in the sense that it can be weighed on a resting scale, even while it contains moving components. This mass is the invariant mass, which is equal to the total relativistic energy of the container (including the kinetic energy of the gas) only when it is measured in the center of momentum frame. Just as is the case for single particles, the calculated "rest mass" of such a container of gas does not change when it is in motion, although its "relativistic mass" does change.
The container may even be subjected to a force which gives it an overall velocity, or else (equivalently) it may be viewed from an inertial frame in which it has an overall velocity (that is, technically, a frame in which its center of mass has a velocity). In this case, its total relativistic mass and energy increase. However, in such a situation, although the container's total relativistic energy and total momenta increase, these energy and momentum increases subtract out in the "invariant mass" definition, so that the moving container's invariant mass will be calculated as the same value as if it were measured at rest, on a scale.
Closed (meaning totally isolated) systems.
All conservation laws in special relativity (for energy, mass, and momentum) require isolated systems, meaning systems that are totally isolated, with no mass-energy allowed in or out, over time. If a system is isolated, then both total energy and total momentum in the system are conserved over time for any observer in any single inertial frame, though their "absolute values" will vary, according to different observers in different inertial frames. The invariant mass of the system is also conserved, but does "not" change with different observers. This is also the familiar situation with single particles: all observers calculate "the same" particle rest mass (a special case of the invariant mass) no matter how they move (what inertial frame they choose), but different observers see different total energies and momenta for the same particle.
Conservation of invariant mass also requires the system to be enclosed so that no heat and radiation (and thus invariant mass) can escape. As in the example above, a physically enclosed or bound system does not need to be completely isolated from external forces for its mass to remain constant, because for bound systems these merely act to change the inertial frame of the system or the observer. Though such actions may change the total energy or momentum of the bound system, these two changes cancel, so that there is no change in the system's invariant mass. This is just the same result as with single particles: their calculated rest mass also remains constant no matter how fast they move, or how fast an observer sees them move.
On the other hand, for systems which are unbound, the "closure" of the system may be enforced by an idealized surface, inasmuch as no mass-energy can be allowed into or out of the test-volume over time, if conservation of system invariant mass is to hold during that time. If a force is allowed to act on (do work on) only one part of such an unbound system, this is equivalent to allowing energy into or out of the system, and the condition of "closure" to mass-energy (total isolation) is violated. In this case, conservation of invariant mass of the system also will no longer hold. Such a loss of rest mass in systems when energy is removed, according to "E=mc2" where "E" is the energy removed, and "m" is the change in rest mass, reflect changes of mass associated with movement of energy, not "conversion" of mass to energy.
The system invariant mass vs. the individual rest masses of parts of the system.
Again, in special relativity, the rest mass of a system is not required to be equal to the sum of the rest masses of the parts (a situation which would be analogous to gross mass-conservation in chemistry). For example, a massive particle can decay into photons which individually have no mass, but which (as a system) preserve the invariant mass of the particle which produced them. Also a box of moving non-interacting particles (e.g., photons, or an ideal gas) will have a larger invariant mass than the sum of the rest masses of the particles which compose it. This is because the total energy of all particles and fields in a system must be summed, and this quantity, as seen in the center of momentum frame, and divided by c2, is the system's invariant mass.
In special relativity, mass is not "converted" to energy, for all types of energy still retain their associated mass. Neither energy nor invariant mass can be destroyed in special relativity, and each is separately conserved over time in closed systems. Thus, a system's invariant mass may change "only" because invariant mass is allowed to escape, perhaps as light or heat. Thus, when reactions (whether chemical or nuclear) release energy in the form of heat and light, if the heat and light is "not" allowed to escape (the system is closed and isolated), the energy will continue to contribute to the system rest mass, and the system mass will not change. Only if the energy is released to the environment will the mass be lost; this is because the associated mass has been allowed out of the system, where it contributes to the mass of the surroundings.
The relativistic mass concept.
Transverse and longitudinal mass.
Concepts that were similar to what nowadays is called "relativistic mass", were already developed before the advent of special relativity. For example, it was recognized by J. J. Thomson in 1881 that a charged body is harder to set in motion than an uncharged body, which was worked out in more detail by Oliver Heaviside (1889) and George Frederick Charles Searle (1897). So the electrostatic energy behaves as having some sort of electromagnetic mass formula_24, which can increase the normal mechanical mass of the bodies.
Then, it was pointed out by Thomson and Searle that this electromagnetic mass also increases with velocity. This was further elaborated by Hendrik Lorentz (1899, 1904) in the framework of Lorentz ether theory. He defined mass as the ratio of force to acceleration, not as the ratio of momentum to velocity, so he needed to distinguish between the mass formula_25 parallel to the direction of motion and the mass formula_26 perpendicular to the direction of motion (where formula_27 is the Lorentz factor, "v" is the relative velocity between the aether and the object, and "c" is the speed of light). Only when the force is perpendicular to the velocity, Lorentz's mass is equal to what is now called "relativistic mass". Max Abraham (1902) called formula_28 "longitudinal mass" and formula_29 "transverse mass" (although Abraham used more complicated expressions than Lorentz's relativistic ones). So, according to Lorentz's theory no body can reach the speed of light because the mass becomes infinitely large at this velocity.
Also Albert Einstein initially used the concepts of longitudinal and transverse mass in his 1905 electrodynamics paper (equivalent to those of Lorentz, but with a different formula_29 by an unfortunate force definition, which was later corrected), and in another paper in 1906. However, he later abandoned velocity dependent mass concepts (see quote at the end of next section).
The precise relativistic expression (which is equivalent to Lorentz's) relating force and acceleration for a particle with non-zero rest mass formula_31 moving in the "x" direction with velocity "v" and associated Lorentz factor formula_32 is
Relativistic mass.
In special relativity, an object that has nonzero rest mass cannot travel at the speed of light. As the object approaches the speed of light, the object's energy and momentum increase without bound.
In the first years after 1905, following Lorentz and Einstein, the terms longitudinal and transverse mass were still in use. However, those expressions were replaced by the concept of "relativistic mass", an expression which was first defined by Gilbert N. Lewis and Richard C. Tolman in 1909. They defined the total energy and mass of a body as
and of a body at rest
with the ratio
Tolman in 1912 further elaborated on this concept, and stated: “the expression m0(1 - v2/c2)−1/2 is best suited for THE mass of a moving body.”
In 1934, Tolman argued that the relativistic mass formula formula_39 holds for all particles, including those moving at the speed of light, while the formula formula_40 only applies to a slower than light particle (a particle with a nonzero rest mass). Tolman remarked on this relation that "We have, moreover, of course the experimental verification of the expression in the case of moving electrons to which we shall call attention in §29. We shall hence have no hesitation in accepting the expression as correct in general for the mass of a moving particle."
When the relative velocity is zero, formula_32 is simply equal to 1, and the relativistic mass is reduced to the rest mass as one can see in the next two equations below. As the velocity increases toward the speed of light "c", the denominator of the right side approaches zero, and consequently formula_32 approaches infinity.
In the formula for momentum
the mass that occurs is the relativistic mass. In other words, the relativistic mass is the proportionality constant between the velocity and the momentum.
While Newton's second law remains valid in the form
the derived form formula_45 is not valid because formula_46 in formula_47 is generally not a constant (see the section above on transverse and longitudinal mass).
Even though Einstein initially used the expressions "longitudinal" and "transverse" mass in two papers (see previous section), in his first paper on formula_48 (1905) he treated "m" as what would now be called the "rest mass".
In later years Einstein expressed his dislike of the idea of "relativistic mass":
Controversy.
Okun and followers reject the concept of relativistic mass. Also Arnold B. Arons has argued against teaching the concept of relativistic mass:
For many years it was conventional to enter the discussion of dynamics through derivation of the relativistic mass, that is the mass–velocity relation, and this is probably still the dominant mode in textbooks. More recently, however, it has been increasingly recognized that relativistic mass is a troublesome and dubious concept. [See, for example, Okun (1989).]... The sound and rigorous approach to relativistic dynamics is through direct development of that expression for "momentum" that ensures conservation of momentum in all frames:
rather than through relativistic mass. 
Many contemporary authors such as Taylor and Wheeler avoid using the concept of relativistic mass altogether:
While space-time has the unbounded geometry of Minkowski-space, the velocity-space is bounded by "c" and has the geometry of hyperbolic geometry where relativistic-mass plays an analogous role to that of Newtonian-mass in the barycentric-coordinates of Euclidean geometry. The connection of velocity to hyperbolic-geometry enables the 3-velocity-dependent relativistic-mass to be related to the 4-velocity Minkowski-formalism.

</doc>
<doc id="659068" url="https://en.wikipedia.org/wiki?curid=659068" title="Mass number">
Mass number

The mass number (A), also called atomic mass number or nucleon number, is the total number of protons and neutrons (together known as nucleons) in an atomic nucleus. It determines the atomic mass of atoms. Because protons and neutrons both are baryons, the mass number A is identical with the baryon number B as of the nucleus as of the whole atom or ion. The mass number is different for each different isotope of a chemical element. This is not the same as the atomic number (Z) which denotes the number of protons in a nucleus, and thus uniquely identifies an element. Hence, the difference between the mass number and the atomic number gives the number of neutrons ("N") in a given nucleus: "N=A−Z".
The mass number is written either after the element name or as a superscript to the left of an element's symbol. For example, the most common isotope of carbon is carbon-12, or , which has 6 protons and 6 neutrons. The full isotope symbol would also have the atomic number (Z) as a subscript to the left of the element symbol directly below the mass number: . This is technically redundant, as each element is defined by its atomic number, so it is often omitted.
Mass number changes in radioactive decay.
Different types of radioactive decay are characterized by their changes in mass number as well as atomic number, according to the radioactive displacement law of Fajans and Soddy. 
For example, uranium-238 usually decays by alpha decay, where the nucleus loses two neutrons and two protons in the form of an alpha particle. Thus the atomic number and the number of neutrons each decrease by 2 ("Z": 92→90, "N": 146→144), so that the mass number decreases by 4 ("A" = 238→234); the result is an atom of thorium-234 and an alpha particle ():
On the other hand, carbon-14 naturally decays by radioactive beta decay, whereby one neutron is transmuted into a proton with the emission of an electron and an anti-neutrino. Thus the atomic number increases by 1 ("Z": 6→7) and the mass number remains the same ("A" = 14), while the number of neutrons decreases by 1 ("N": 8→7). The resulting atom is nitrogen-14, with seven protons and seven neutrons:
Another type of radioactive decay without change in mass number is emission of a gamma ray from a nuclear isomer or metastable excited state of an atomic nucleus. Since all the protons and neutrons remain in the nucleus unchanged in this process, the mass number is also unchanged.
Mass number and isotopic mass.
The mass number gives an estimate of the isotopic mass measured in atomic mass units (u). For 12C the isotopic mass is exactly 12, since the atomic mass unit is defined as 1/12 of the mass of 12C. For other isotopes, the isotopic mass is usually within 0.1 u of the mass number. For example, 35Cl has a mass number of 35 and an isotopic mass of 34.96885.
There are two reasons for the difference between mass number and isotopic mass, known as the mass defect:
Relative Atomic Mass of an element.
The mass number should also not be confused with the relative atomic mass (also called atomic weight) of an element, which is the ratio of the average atomic mass of the different isotopes of that element (weighted by abundance) to the unified atomic mass unit. This weighted average can be quite different from the near-integer values for individual isotopic masses.
For instance, there are two main isotopes of chlorine: chlorine-35 and chlorine-37. In any given sample of chlorine that has not been subjected to mass separation there will be roughly 75% of chlorine atoms which are chlorine-35 and only 25% of chlorine atoms which are chlorine-37. This gives chlorine a relative atomic mass of 35.5 (actually 35.4527 g/mol).
Moreover, the weighted average mass can be near-integer, but at the same time not corresponding to the mass of any natural isotope. For example, bromine has only two stable isotopes, 79Br and 81Br, naturally present in approximately equal fractions, which leads to the standard atomic mass of bromine close to 80 (79.904 g/mol), even though the isotope 80Br with such mass is unstable.

</doc>
<doc id="422481" url="https://en.wikipedia.org/wiki?curid=422481" title="Mass–energy equivalence">
Mass–energy equivalence

In physics, mass–energy equivalence is a concept formulated by Albert Einstein that explains the relationship between mass and energy. It states every mass has an energy equivalent and vice versa—expressed using the formula
where "E" is the energy of a physical system, "m" is the mass of the system, and "c" is the speed of light in a vacuum (about m/s). In words, energy equals mass multiplied by the speed of light squared. Because the speed of light is a very large number in everyday units, the formula implies that any small amount of matter contains a very large amount of energy. Some of this energy may be released as heat and light by nuclear transformations. This also serves to convert to , no matter what system of measurement units used.
Mass–energy equivalence arose originally from special relativity as a paradox described by Henri Poincaré. Einstein proposed it in 1905, in the paper "Does the inertia of a body depend upon its energy-content?", one of his Annus Mirabilis ("Miraculous Year") Papers. Einstein was the first to propose that the equivalence of mass and energy is a general principle and a consequence of the symmetries of space and time.
A consequence of the mass–energy equivalence is that if a body is stationary, it still has some internal or intrinsic energy, called its rest energy. Rest mass and rest energy are equivalent and remain proportional to one another. When the body is in motion (relative to an observer), its total energy is greater than its rest energy. The rest mass (or rest energy) remains an important quantity in this case because it remains the same regardless of this motion, even for the extreme speeds or gravity considered in special and general relativity; thus it is also called the invariant mass.
Nomenclature.
The formula was initially written in many different notations, and its interpretation and justification was further developed in several steps.
Conservation of mass and energy.
Mass and energy can be seen as two names (and two measurement units) for the same underlying, conserved physical quantity. Thus, the laws of conservation of energy and conservation of (total) mass are equivalent and both hold true. Einstein elaborated in a 1946 essay that "the principle of the conservation of mass [...] proved inadequate in the face of the special theory of relativity. It was therefore merged with the energy [conservation] principle—just as, about 60 years before, the principle of the conservation of mechanical energy had been combined with the principle of the conservation of heat [thermal energy]. We might say that the principle of the conservation of energy, having previously swallowed up that of the conservation of heat, now proceeded to swallow that of the conservation of mass—and holds the field alone."
If the conservation of mass law is interpreted as conservation of "rest" mass, it does not hold true in special relativity. The "rest" energy (equivalently, rest mass) of a particle can be converted, not "to energy" (it already "is" energy (mass)), but rather to "other" forms of energy (mass) that require motion, such as kinetic energy, thermal energy, or radiant energy. Similarly, kinetic or radiant energy can be converted to other kinds of particles that have rest energy (rest mass). In the transformation process, neither the total amount of mass nor the total amount of energy changes, since both properties are connected via a simple constant. This view requires that if either energy or (total) mass disappears from a system, it is always found that both have simply moved to another place, where they are both measurable as an increase of both energy and mass that corresponds to the loss in the first system.
Fast-moving objects and systems of objects.
When an object is pushed in the direction of motion, it gains momentum and energy, but when the object is already traveling near the speed of light, it cannot move much faster, no matter how much energy it absorbs. Its momentum and energy continue to increase without bounds, whereas its speed approaches a constant value—the speed of light. This implies that in relativity the momentum of an object cannot be a constant times the velocity, nor can the kinetic energy be a constant times the square of the velocity.
A property called the relativistic mass is defined as the ratio of the momentum of an object to its velocity. Relativistic mass depends on the motion of the object, so that different observers in relative motion see different values for it. If the object is moving slowly, the relativistic mass is nearly equal to the rest mass and both are nearly equal to the usual Newtonian mass. If the object is moving quickly, the relativistic mass is greater than the rest mass by an amount equal to the mass associated with the kinetic energy of the object. As the object approaches the speed of light, the relativistic mass grows infinitely, because the kinetic energy grows infinitely and this energy is associated with mass.
The relativistic mass is always equal to the total energy (rest energy plus kinetic energy) divided by . Because the relativistic mass is exactly proportional to the energy, relativistic mass and relativistic energy are nearly synonyms; the only difference between them is the units. If length and time are measured in natural units, the speed of light is equal to 1, and even this difference disappears. Then mass and energy have the same units and are always equal, so it is redundant to speak about relativistic mass, because it is just another name for the energy. This is why physicists usually reserve the useful short word "mass" to mean rest mass, or invariant mass, and not relativistic mass.
The relativistic mass of a moving object is larger than the relativistic mass of an object that is not moving, because a moving object has extra kinetic energy. The "rest mass" of an object is defined as the mass of an object when it is at rest, so that the rest mass is always the same, independent of the motion of the observer: it is the same in all inertial frames.
For things and systems made up of many parts, like an atomic nucleus, planet, or star, the relativistic mass is the sum of the relativistic masses (or energies) of the parts, because energies are additive in isolated systems. This is not true in open systems, however, if energy is subtracted. For example, if a system is "bound" by attractive forces, and the energy gained due to the forces of attraction in excess of the work done is removed from the system, then mass is lost with this removed energy. For example, the mass of an atomic nucleus is less than the total mass of the protons and neutrons that make it up, but this is only true after this energy from binding has been removed in the form of a gamma ray (which in this system, carries away the mass of the energy of binding). This mass decrease is also equivalent to the energy required to break up the nucleus into individual protons and neutrons (in this case, work and mass would need to be supplied). Similarly, the mass of the solar system is slightly less than the sum of the individual masses of the sun and planets.
For a system of particles going off in different directions, the invariant mass of the system is the analog of the rest mass, and is the same for all observers, even those in relative motion. It is defined as the total energy (divided by "c"2) in the center of mass frame (where by definition, the system total momentum is zero). A simple example of an object with moving parts but zero total momentum is a container of gas. In this case, the mass of the container is given by its total energy (including the kinetic energy of the gas molecules), since the system total energy and invariant mass are the same in any reference frame where the momentum is zero, and such a reference frame is also the only frame in which the object can be weighed. In a similar way, the theory of special relativity posits that the thermal energy in all objects (including solids) contributes to their total masses and weights, even though this energy is present as the kinetic and potential energies of the atoms in the object, and it (in a similar way to the gas) is not seen in the rest masses of the atoms that make up the object.
In a similar manner, even photons (light quanta), if trapped in a container space (as a photon gas or thermal radiation), would contribute a mass associated with their energy to the container. Such an extra mass, in theory, could be weighed in the same way as any other type of rest mass. This is true in special relativity theory, even though individually photons have no rest mass. The property that trapped energy "in any form" adds weighable mass to systems that have no net momentum is one of the characteristic and notable consequences of relativity. It has no counterpart in classical Newtonian physics, in which radiation, light, heat, and kinetic energy never exhibit weighable mass under any circumstances.
Just as the relativistic mass of an isolated system is conserved through time, so also is its invariant mass.This property allows the conservation of all types of mass in systems, and also conservation of all types of mass in reactions where matter is destroyed (annihilated), leaving behind the energy that was associated with it (which is now in non-material form, rather than material form). Matter may appear and disappear in various reactions, but mass and energy are both unchanged in this process.
Applicability of the strict mass–energy equivalence formula,.
As is noted above, two different definitions of mass have been used in special relativity, and also two different definitions of energy. The simple equation is not generally applicable to all these types of mass and energy, except in the special case that the total additive momentum is zero for the system under consideration. In such a case, which is always guaranteed when observing the system from either its center of mass frame or its center of momentum frame, is always true for any type of mass and energy that are chosen. Thus, for example, in the center of mass frame, the total energy of an object or system is equal to its rest mass times , a useful equality. This is the relationship used for the container of gas in the previous example. It is "not" true in other reference frames where the center of mass is in motion. In these systems or for such an object, its total energy depends on both its rest (or invariant) mass, and its (total) momentum.
In inertial reference frames other than the rest frame or center of mass frame, the equation remains true if the energy is the relativistic energy "and" the mass is the relativistic mass. It is also correct if the energy is the rest or invariant energy (also the minimum energy), "and" the mass is the rest mass, or the invariant mass. However, connection of the total or relativistic energy () with the rest or invariant mass () requires consideration of the system total momentum, in systems and reference frames where the total momentum has a non-zero value. The formula then required to connect the two different kinds of mass and energy, is the extended version of Einstein's equation, called the relativistic energy–momentum relation:
or
Here the term represents the square of the Euclidean norm (total vector length) of the various momentum vectors in the system, which reduces to the square of the simple momentum magnitude, if only a single particle is considered. This equation reduces to when the momentum term is zero. For photons where , the equation reduces to .
Meanings of the strict mass–energy equivalence formula,.
Mass–energy equivalence states that any object has a certain energy, even when it is stationary. In Newtonian mechanics, a motionless body has no kinetic energy, and it may or may not have other amounts of internal stored energy, like chemical energy or thermal energy, in addition to any potential energy it may have from its position in a field of force. In Newtonian mechanics, all of these energies are much smaller than the mass of the object times the speed of light squared.
In relativity, all the energy that moves with an object (that is, all the energy present in the object's rest frame) contributes to the total mass of the body, which measures how much it resists acceleration. Each potential and kinetic energy makes a proportional contribution to the mass. As noted above, even if a box of ideal mirrors "contains" light, then the individually massless photons still contribute to the total mass of the box, by the amount of their energy divided by c2.
In relativity, removing energy is removing mass, and for an observer in the center of mass frame, the formula indicates how much mass is lost when energy is removed. In a nuclear reaction, the mass of the atoms that come out is less than the mass of the atoms that go in, and the difference in mass shows up as heat and light with the same relativistic mass as the difference (and also the same invariant mass in the center of mass frame of the system). In this case, the in the formula is the energy released and removed, and the mass is how much the mass decreases. In the same way, when any sort of energy is added to an isolated system, the increase in the mass is equal to the added energy divided by . For example, when water is heated it gains about of mass for every joule of heat added to the water.
An object moves with different speed in different frames, depending on the motion of the observer, so the kinetic energy in both Newtonian mechanics and relativity is "frame dependent". This means that the amount of relativistic energy, and therefore the amount of relativistic mass, that an object is measured to have depends on the observer. The "rest mass" is defined as the mass that an object has when it is not moving (or when an inertial frame is chosen such that it is not moving). The term also applies to the invariant mass of systems when the system as a whole is not "moving" (has no net momentum). The rest and invariant masses are the smallest possible value of the mass of the object or system. They also are conserved quantities, so long as the system is isolated. Because of the way they are calculated, the effects of moving observers are subtracted, so these quantities do not change with the motion of the observer.
The rest mass is almost never additive: the rest mass of an object is not the sum of the rest masses of its parts. The rest mass of an object is the total energy of all the parts, including kinetic energy, as measured by an observer that sees the center of the mass of the object to be standing still. The rest mass adds up only if the parts are standing still and do not attract or repel, so that they do not have any extra kinetic or potential energy. The other possibility is that they have a positive kinetic energy and a negative potential energy that exactly cancels.
Binding energy and the "mass defect".
Whenever any type of energy is removed from a system, the mass associated with the energy is also removed, and the system therefore loses mass. This mass defect in the system may be simply calculated as , and this was the form of the equation historically first presented by Einstein in 1905. However, use of this formula in such circumstances has led to the false idea that mass has been "converted" to energy. This may be particularly the case when the energy (and mass) removed from the system is associated with the "binding energy" of the system. In such cases, the binding energy is observed as a "mass defect" or deficit in the new system.
The fact that the released energy is not easily weighed in many such cases, may cause its mass to be neglected as though it no longer existed. This circumstance has encouraged the false idea of conversion of "mass" to energy, rather than the correct idea that the binding energy of such systems is relatively large, and exhibits a measurable mass, which is removed when the binding energy is removed. This energy is often released in the form of light and heat, which is too quickly and widely dispersed to easily weigh, though it does carry mass.
The difference between the rest mass of a bound system and of the unbound parts is the binding energy of the system, if this energy has been removed after binding. For example, a water molecule weighs a little less than two free hydrogen atoms and an oxygen atom. The minuscule mass difference is the energy needed to split the molecule into three individual atoms (divided by ), which was given off as heat when the molecule formed (this heat had mass). Likewise, a stick of dynamite in theory weighs a little bit more than the fragments after the explosion, but this is true only so long as the fragments are cooled and the heat removed. In this case the mass difference is the energy/heat that is released when the dynamite explodes, and when this heat escapes, the mass associated with it escapes, only to be deposited in the surroundings, which absorb the heat (so that total mass is conserved).
Such a change in mass may only happen when the system is open, and the energy and mass escapes. Thus, if a stick of dynamite is blown up in a hermetically sealed chamber, the mass of the chamber and fragments, the heat, sound, and light would still be equal to the original mass of the chamber and dynamite. If sitting on a scale, the weight and mass would not change. This would in theory also happen even with a nuclear bomb, if it could be kept in an ideal box of infinite strength, which did not rupture or pass radiation. Thus, a 21.5 kiloton () nuclear bomb produces about one gram of heat and electromagnetic radiation, but the mass of this energy would not be detectable in an exploded bomb in an ideal box sitting on a scale; instead, the contents of the box would be heated to millions of degrees without changing total mass and weight. If then, however, a transparent window (passing only electromagnetic radiation) were opened in such an ideal box after the explosion, and a beam of X-rays and other lower-energy light allowed to escape the box, it would eventually be found to weigh one gram less than it had before the explosion. This weight loss and mass loss would happen as the box was cooled by this process, to room temperature. However, any surrounding mass that absorbed the X-rays (and other "heat") would "gain" this gram of mass from the resulting heating, so the mass "loss" would represent merely its relocation. Thus, no mass (or, in the case of a nuclear bomb, no matter) would be "converted" to energy in such a process. Mass and energy, as always, would both be separately conserved.
Massless particles.
Massless particles have zero rest mass. Their relativistic mass is simply their relativistic energy, divided by , or . The energy for photons is , where is Planck's constant and is the photon frequency. This frequency and thus the relativistic energy are frame-dependent.
If an observer runs away from a photon in the direction the photon travels from a source, and it catch up with the observer—when the photon catches up, the observer sees it as having less energy than it had at the source. The faster the observer is traveling with regard to the source when the photon catches up, the less energy the photon has. As an observer approaches the speed of light with regard to the source, the photon looks redder and redder, by relativistic Doppler effect (the Doppler shift is the relativistic formula), and the energy of a very long-wavelength photon approaches zero. This is why a photon is "massless"—this means that the rest mass of a photon is zero.
Massless particles contribute rest mass and invariant mass to systems.
Two photons moving in different directions cannot both be made to have arbitrarily small total energy by changing frames, or by moving toward or away from them. The reason is that in a two-photon system, the energy of one photon is decreased by chasing after it, but the energy of the other increases with the same shift in observer motion. Two photons not moving in the same direction exhibits an inertial frame where the combined energy is smallest, but not zero. This is called the center of mass frame or the center of momentum frame; these terms are almost synonyms (the center of mass frame is the special case of a center of momentum frame where the center of mass is put at the origin). The most that chasing a pair of photons can accomplish to decrease their energy is to put the observer in a frame where the photons have equal energy and are moving directly away from each other. In this frame, the observer is now moving in the same direction and speed as the center of mass of the two photons. The total momentum of the photons is now zero, since their momenta are equal and opposite. In this frame the two photons, as a system, have a mass equal to their total energy divided by . This mass is called the invariant mass of the pair of photons together. It is the smallest mass and energy the system may be seen to have, by any observer. It is only the invariant mass of a two-photon system that can be used to make a single particle with the same rest mass.
If the photons are formed by the collision of a particle and an antiparticle, the invariant mass is the same as the total energy of the particle and antiparticle (their rest energy plus the kinetic energy), in the center of mass frame, where they automatically move in equal and opposite directions (since they have equal momentum in this frame). If the photons are formed by the disintegration of a "single" particle with a well-defined rest mass, like the neutral pion, the invariant mass of the photons is equal to rest mass of the pion. In this case, the center of mass frame for the pion is just the frame where the pion is at rest, and the center of mass does not change after it disintegrates into two photons. After the two photons are formed, their center of mass is still moving the same way the pion did, and their total energy in this frame adds up to the mass energy of the pion. Thus, by calculating the invariant mass of pairs of photons in a particle detector, pairs can be identified that were probably produced by pion disintegration.
A similar calculation illustrates that the invariant mass of systems is conserved, even when massive particles (particles with rest mass) within the system are converted to massless particles (such as photons). In such cases, the photons contribute invariant mass to the system, even though they individually have no invariant mass or rest mass. Thus, an electron and positron (each of which has rest mass) may undergo annihilation with each other to produce two photons, each of which is massless (has no rest mass). However, in such circumstances, no system mass is lost. Instead, the system of both photons moving away from each other has an invariant mass, which acts like a rest mass for any system in which the photons are trapped, or that can be weighed. Thus, not only the quantity of relativistic mass, but also the quantity of invariant mass does not change in transformations between "matter" (electrons and positrons) and energy (photons).
Relation to gravity.
In physics, there are two distinct concepts of mass: the gravitational mass and the inertial mass. The gravitational mass is the quantity that determines the strength of the gravitational field generated by an object, as well as the gravitational force acting on the object when it is immersed in a gravitational field produced by other bodies. The inertial mass, on the other hand, quantifies how much an object accelerates if a given force is applied to it. The mass–energy equivalence in special relativity refers to the inertial mass. However, already in the context of Newton gravity, the Weak Equivalence Principle is postulated: the gravitational and the inertial mass of every object are the same. Thus, the mass–energy equivalence, combined with the Weak Equivalence Principle, results in the prediction that all forms of energy contribute to the gravitational field generated by an object. This observation is one of the pillars of the general theory of relativity.
The above prediction, that all forms of energy interact gravitationally, has been subject to experimental tests. The first observation testing this prediction was made in 1919. During a solar eclipse, Arthur Eddington observed that the light from stars passing close to the Sun was bent. The effect is due to the gravitational attraction of light by the Sun. The observation confirmed that the energy carried by light indeed is equivalent to a gravitational mass. Another seminal experiment, the Pound–Rebka experiment, was performed in 1960. In this test a beam of light was emitted from the top of a tower and detected at the bottom. The frequency of the light detected was higher than the light emitted. This result confirms that the energy of photons increases when they fall in the gravitational field of the Earth. The energy, and therefore the gravitational mass, of photons is proportional to their frequency as stated by the Planck's relation.
Application to nuclear physics.
Max Planck pointed out that the mass–energy equivalence formula implied that bound systems would have a mass less than the sum of their constituents, once the binding energy had been allowed to escape. However, Planck was thinking about chemical reactions, where the binding energy is too small to measure. Einstein suggested that radioactive materials such as radium would provide a test of the theory, but even though a large amount of energy is released per atom in radium, due to the half-life of the substance (1602 years), only a small fraction of radium atoms decay over an experimentally measurable period of time.
Once the nucleus was discovered, experimenters realized that the very high binding energies of the atomic nuclei should allow calculation of their binding energies, simply from mass differences. But it was not until the discovery of the neutron in 1932, and the measurement of the neutron mass, that this calculation could actually be performed (see nuclear binding energy for example calculation). A little while later, the first transmutation reactions (such as the Cockcroft–Walton experiment: ) verified Einstein's formula to an accuracy of ±0.5%.
In 2005, Rainville et al. published a direct test of the energy-equivalence of mass lost in the binding energy of a neutron to atoms of particular isotopes of silicon and sulfur, by comparing the mass lost to the energy of the emitted gamma ray associated with the neutron capture. The binding mass-loss agreed with the gamma ray energy to a precision of ±0.00004%, the most accurate test of to date.
The mass–energy equivalence formula was used in the understanding of nuclear fission reactions, and implies the great amount of energy that can be released by a nuclear fission chain reaction, used in both nuclear weapons and nuclear power. By measuring the mass of different atomic nuclei and subtracting from that number the total mass of the protons and neutrons as they would weigh separately, one gets the exact binding energy available in an atomic nucleus. This is used to calculate the energy released in any nuclear reaction, as the difference in the total mass of the nuclei that enter and exit the reaction.
Practical examples.
Einstein used the CGS system of units (centimeters, grams, seconds, dynes, and ergs), but the formula is independent of the system of units. In natural units, the numerical value of the speed of light is set to equal 1, and the formula expresses an equality of numerical values: . In the SI system (expressing the ratio in joules per kilogram using the value of "c" in meters per second):
So the energy equivalent of one gram (1/1000 of a kilogram) of mass is equivalent to:
or to the energy released by combustion of the following:
Any time energy is generated, the process can be evaluated from an perspective. For instance, the "Gadget"-style bomb used in the Trinity test and the bombing of Nagasaki had an explosive yield equivalent to 21 kt of TNT. About 1 kg of the approximately 6.15 kg of plutonium in each of these bombs fissioned into lighter elements totaling almost exactly one gram less, after cooling. The electromagnetic radiation and kinetic energy (thermal and blast energy) released in this explosion carried the missing one gram of mass. This occurs because nuclear binding energy is released whenever elements with more than 62 nucleons fission.
Another example is hydroelectric generation. The electrical energy produced by Grand Coulee Dam's turbines every 3.7 hours represents one gram of mass. This mass passes to electrical devices (such as lights in cities) powered by the generators, where it appears as a gram of heat and light. Turbine designers look at their equations in terms of pressure, torque, and RPM. However, Einstein's equations show that all energy has mass, and thus the electrical energy produced by a dam's generators, and the resulting heat and light, all retain their mass—which is equivalent to the energy. The potential energy—and equivalent mass—represented by the waters of the Columbia River as it descends to the Pacific Ocean would be converted to heat due to viscous friction and the turbulence of white water rapids and waterfalls were it not for the dam and its generators. This heat would remain as mass on site at the water, were it not for the equipment that converted some of this potential and kinetic energy into electrical energy, which can move from place to place (taking mass with it).
Whenever energy is added to a system, the system gains mass:
Note that no net mass or energy is really created or lost in any of these examples and scenarios. Mass/energy simply moves from one place to another. These are some examples of the "transfer" of energy and mass in accordance with the "principle of mass–energy conservation".
Efficiency.
Although mass cannot be converted to energy, in some reactions matter particles (which contain a form of rest energy) can be destroyed and converted to other types of energy that are more usable and obvious as forms of energy—such as light and energy of motion (heat, etc.). However, the total amount of energy and mass does not change in such a transformation. Even when particles are not destroyed, a certain fraction of the ill-defined "matter" in ordinary objects can be destroyed, and its associated energy liberated and made available as the more dramatic energies of light and heat, even though no identifiable real particles are destroyed, and even though (again) the total energy is unchanged (as also the total mass). Such conversions between types of energy (resting to active energy) happen in nuclear weapons, in which the protons and neutrons in atomic nuclei lose a small fraction of their average mass, but this mass loss is not due to the destruction of any protons or neutrons (or even, in general, lighter particles like electrons). Also the mass is not destroyed, but simply removed from the system. in the form of heat and light from the reaction.
In nuclear reactions, typically only a small fraction of the total mass–energy of the bomb converts into the mass–energy of heat, light, radiation, and motion—which are "active" forms that can be used. When an atom fissions, it loses only about 0.1% of its mass (which escapes from the system and does not disappear), and additionally, in a bomb or reactor not all the atoms can fission. In a modern fission-based atomic bomb, the efficiency is only about 40%, so only 40% of the fissionable atoms actually fission, and only about 0.03% of the fissile core mass appears as energy in the end. In nuclear fusion, more of the mass is released as usable energy, roughly 0.3%. But in a fusion bomb, the bomb mass is partly casing and non-reacting components, so that in practicality, again (coincidentally) no more than about 0.03% of the total mass of the entire weapon is released as usable energy (which, again, retains the "missing" mass). See nuclear weapon yield for practical details of this ratio in modern nuclear weapons.
In theory, it should be possible to destroy matter and convert all of the rest-energy associated with matter into heat and light (which would of course have the same mass), but none of the theoretically known methods are practical. One way to convert all the energy within matter into usable energy is to annihilate matter with antimatter. But antimatter is rare in our universe, and must be made first. Due to inefficient mechanisms of production, making antimatter always requires far more usable energy than would be released when it was annihilated.
Since most of the mass of ordinary objects resides in protons and neutrons, converting all the energy of ordinary matter into more useful energy requires that the protons and neutrons be converted to lighter particles, or particles with no rest-mass at all. In the Standard Model of particle physics, the number of protons plus neutrons is nearly exactly conserved. Still, Gerard 't Hooft showed that there is a process that converts protons and neutrons to antielectrons and neutrinos. This is the weak SU(2) instanton proposed by Belavin Polyakov Schwarz and Tyupkin. This process, can in principle destroy matter and convert all the energy of matter into neutrinos and usable energy, but it is normally extraordinarily slow. Later it became clear that this process happens at a fast rate at very high temperatures, since then, instanton-like configurations are copiously produced from thermal fluctuations. The temperature required is so high that it would only have been reached shortly after the big bang.
Many extensions of the standard model contain magnetic monopoles, and in some models of grand unification, these monopoles catalyze proton decay, a process known as the Callan–Rubakov effect. This process would be an efficient mass–energy conversion at ordinary temperatures, but it requires making monopoles and anti-monopoles first. The energy required to produce monopoles is believed to be enormous, but magnetic charge is conserved, so that the lightest monopole is stable. All these properties are deduced in theoretical models—magnetic monopoles have never been observed, nor have they been produced in any experiment so far.
A third known method of total matter–energy "conversion" (which again in practice only means conversion of one type of energy into a different type of energy), is using gravity, specifically black holes. Stephen Hawking theorized that black holes radiate thermally with no regard to how they are formed. So, it is theoretically possible to throw matter into a black hole and use the emitted heat to generate power. According to the theory of Hawking radiation, however, the black hole used radiates at a higher rate the smaller it is, producing usable powers at only small black hole masses, where usable may for example be something greater than the local background radiation. It is also worth noting that the ambient irradiated power would change with the mass of the black hole, increasing as the mass of the black hole decreases, or decreasing as the mass increases, at a rate where power is proportional to the inverse square of the mass. In a "practical" scenario, mass and energy could be dumped into the black hole to regulate this growth, or keep its size, and thus power output, near constant. This could result from the fact that mass and energy are lost from the hole with its thermal radiation.
Background.
Mass–velocity relationship.
In developing special relativity, Einstein found that the kinetic energy of a moving body is
with formula_5 the velocity, formula_6 the rest mass, and formula_7 the Lorentz factor.
He included the second term on the right to make sure that for small velocities the energy would be the same as in classical mechanics, thus satisfying the correspondence principle:
Without this second term, there would be an additional contribution in the energy when the particle is not moving.
Einstein found that the total momentum of a moving particle is:
It is this quantity that is conserved in collisions. The ratio of the momentum to the velocity is the relativistic mass, .
And the relativistic mass and the relativistic kinetic energy are related by the formula:
Einstein wanted to omit the unnatural second term on the right-hand side, whose only purpose is to make the energy at rest zero, and to declare that the particle has a total energy, which obeys:
which is a sum of the rest energy and the kinetic energy. This total energy is mathematically more elegant, and fits better with the momentum in relativity. But to come to this conclusion, Einstein needed to think carefully about collisions. This expression for the energy implied that matter at rest has a huge amount of energy, and it is not clear whether this energy is physically real, or just a mathematical artifact with no physical meaning.
In a collision process where all the rest-masses are the same at the beginning as at the end, either expression for the energy is conserved. The two expressions only differ by a constant that is the same at the beginning and at the end of the collision. Still, by analyzing the situation where particles are thrown off a heavy central particle, it is easy to see that the inertia of the central particle is reduced by the total energy emitted. This allowed Einstein to conclude that the inertia of a heavy particle is increased or diminished according to the energy it absorbs or emits.
Relativistic mass.
After Einstein first made his proposal, it became clear that the word mass can have two different meanings. Some denote the "relativistic mass" with an explicit index:
This mass is the ratio of momentum to velocity, and it is also the relativistic energy divided by (it is not Lorentz-invariant, in contrast to formula_6). The equation holds for moving objects. When the velocity is small, the relativistic mass and the rest mass are almost exactly the same.
Also Einstein (following Hendrik Lorentz and Max Abraham) used velocity- and direction-dependent mass concepts () in his 1905 electrodynamics paper and in another paper in 1906.
However, in his first paper on (1905), he treated as what would now be called the "rest mass". Some claim that (in later years) he did not like the idea of "relativistic mass".  When modern physicists say "mass", they are usually talking about rest mass, since if they meant "relativistic mass", they would just say "energy".
Considerable debate has ensued over the use of the concept "relativistic mass" and the connection of "mass" in relativity to "mass" in Newtonian dynamics. For example, one view is that only rest mass is a viable concept and is a property of the particle; while relativistic mass is a conglomeration of particle properties and properties of spacetime. A perspective that avoids this debate, due to Kjell Vøyenli, is that the Newtonian concept of mass as a particle property and the relativistic concept of mass have to be viewed as embedded in their own theories and as having no precise connection.
Low speed expansion.
We can rewrite the expression as a Taylor series:
For speeds much smaller than the speed of light, higher-order terms in this expression get smaller and smaller because is small. For low speeds we can ignore all but the first two terms:
The total energy is a sum of the rest energy and the Newtonian kinetic energy.
The classical energy equation ignores both the part, and the high-speed corrections. This is appropriate, because all the high-order corrections are small. Since only "changes" in energy affect the behavior of objects, whether we include the part makes no difference, since it is constant. For the same reason, it is possible to subtract the rest energy from the total energy in relativity. By considering the emission of energy in different frames, Einstein could show that the rest energy has a real physical meaning.
The higher-order terms are extra correction to Newtonian mechanics, and become important at higher speeds. The Newtonian equation is only a low-speed approximation, but an extraordinarily good one. All of the calculations used in putting astronauts on the moon, for example, could have been done using Newton's equations without any of the higher-order corrections.
The total mass energy equivalence should also include the rotational and vibrational kinetic energies as well as the linear kinetic energy at low speeds.
History.
While Einstein was the first to have correctly deduced the mass–energy equivalence formula, he was not the first to have related energy with mass. But nearly all previous authors thought that the energy that contributes to mass comes only from electromagnetic fields.
Newton: matter and light.
In 1717 Isaac Newton speculated that light particles and matter particles were inter-convertible in "Query 30" of the "Opticks", where he asks:
Swedenborg: matter composed of "pure and total motion".
In 1734 the Swedish scientist and theologian Emanuel Swedenborg in his "Principia" theorized that all matter is ultimately composed of dimensionless points of "pure and total motion." He described this motion as being without force, direction or speed, but having the potential for force, direction and speed everywhere within it.
Electromagnetic mass.
There were many attempts in the 19th and the beginning of the 20th century—like those of J. J. Thomson (1881), Oliver Heaviside (1888), and George Frederick Charles Searle (1897), Wilhelm Wien (1900), Max Abraham (1902), Hendrik Antoon Lorentz (1904) — to understand how the mass of a charged object depends on the electrostatic field. This concept was called electromagnetic mass, and was considered as being dependent on velocity and direction as well. Lorentz (1904) gave the following expressions for longitudinal and transverse electromagnetic mass:
where
Radiation pressure and inertia.
Another way of deriving some sort of electromagnetic mass was based on the concept of radiation pressure. In 1900, Henri Poincaré associated electromagnetic radiation energy with a "fictitious fluid" having momentum and mass 
By that, Poincaré tried to save the center of mass theorem in Lorentz's theory, though his treatment led to radiation paradoxes.
Friedrich Hasenöhrl showed in 1904, that electromagnetic cavity radiation contributes the "apparent mass"
to the cavity's mass. He argued that this implies mass dependence on temperature as well.
Einstein: mass–energy equivalence.
Albert Einstein did not formulate exactly the formula in his 1905 "Annus Mirabilis" paper "Does the Inertia of an object Depend Upon Its Energy Content?"; rather, the paper states that if a body gives off the energy in the form of radiation, its mass diminishes by . (Here, "radiation" means electromagnetic radiation, or light, and mass means the ordinary Newtonian mass of a slow-moving object.) This formulation relates only a change in mass to a change in energy without requiring the absolute relationship.
Objects with zero mass presumably have zero energy, so the extension that all mass is proportional to energy is obvious from this result. In 1905, even the hypothesis that changes in energy are accompanied by changes in mass was untested. Not until the discovery of the first type of antimatter (the positron in 1932) was it found that all of the mass of pairs of resting particles could be converted to radiation.
The first derivation by Einstein (1905).
Already in his relativity paper "On the electrodynamics of moving bodies", Einstein derived the correct expression for the kinetic energy of particles:
Now the question remained open as to which formulation applies to bodies at rest. This was tackled by Einstein in his paper "Does the inertia of a body depend upon its energy content?", where he used a body emitting two light pulses in opposite directions, having energies of before and after the emission as seen in its rest frame. As seen from a moving frame, this becomes and . Einstein obtained:
then he argued that can only differ from the kinetic energy "K" by an additive constant, which gives
Neglecting effects higher than third order in after a Taylor series expansion of the right side of this gives:
Einstein concluded that the emission reduces the body's mass by , and that the mass of a body is a measure of its energy content.
The correctness of Einstein's 1905 derivation of was criticized by Max Planck (1907), who argued that it is only valid to first approximation. Another criticism was formulated by Herbert Ives (1952) and Max Jammer (1961), asserting that Einstein's derivation is based on begging the question.
On the other hand, John Stachel and Roberto Torretti (1982) argued that Ives' criticism was wrong, and that Einstein's derivation was correct.
Hans Ohanian (2008) agreed with Stachel/Torretti's criticism of Ives, though he argued that Einstein's derivation was wrong for other reasons. For a recent review, see Hecht (2011).
Alternative version.
An alternative version of Einstein's thought experiment was proposed by Fritz Rohrlich (1990), who based his reasoning on the Doppler effect.
Like Einstein, he considered a body at rest with mass . If the body is examined in a frame moving with nonrelativistic velocity , it is no longer at rest and in the moving frame it has momentum . Then he supposed the body emits two pulses of light to the left and to the right, each carrying an equal amount of energy . In its rest frame, the object remains at rest after the emission since the two beams are equal in strength and carry opposite momentum.
However, if the same process is considered in a frame that moves with velocity "v" to the left, the pulse moving to the left is redshifted, while the pulse moving to the right is blue shifted. The blue light carries more momentum than the red light, so that the momentum of the light in the moving frame is not balanced: the light is carrying some net momentum to the right.
The object has not changed its velocity before or after the emission. Yet in this frame it has lost some right-momentum to the light. The only way it could have lost momentum is by losing mass. This also solves Poincaré's radiation paradox, discussed above.
The velocity is small, so the right-moving light is blueshifted by an amount equal to the nonrelativistic Doppler shift factor . The momentum of the light is its energy divided by , and it is increased by a factor of . So the right-moving light is carrying an extra momentum given by:
The left-moving light carries a little less momentum, by the same amount . So the total right-momentum in the light is twice . This is the right-momentum that the object lost.
The momentum of the object in the moving frame after the emission is reduced to this amount:
So the change in the object's mass is equal to the total energy lost divided by . Since any emission of energy can be carried out by a two step process, where first the energy is emitted as light and then the light is converted to some other form of energy, any emission of energy is accompanied by a loss of mass. Similarly, by considering absorption, a gain in energy is accompanied by a gain in mass.
Relativistic center-of-mass theorem (1906).
Like Poincaré, Einstein concluded in 1906 that the inertia of electromagnetic energy is a necessary condition for the center-of-mass theorem to hold. On this occasion, Einstein referred to Poincaré's 1900 paper and wrote:
In Einstein's more physical, as opposed to formal or mathematical, point of view, there was no need for fictitious masses. He could avoid the "perpetuum mobile" problem because, on the basis of the mass–energy equivalence, he could show that the transport of inertia that accompanies the emission and absorption of radiation solves the problem. Poincaré's rejection of the principle of action–reaction can be avoided through Einstein's , because mass conservation appears as a special case of the energy conservation law.
Others.
During the nineteenth century there were several speculative attempts to show that mass and energy were proportional in various ether theories. In 1873 Nikolay Umov pointed out a relation between mass and energy for ether in the form of , where . The writings of Samuel Tolver Preston, and a 1903 paper by Olinto De Pretto, presented a mass–energy relation. De Pretto's paper received recent press coverage when Umberto Bartocci discovered that there were only three degrees of separation linking De Pretto to Einstein, leading Bartocci to conclude that Einstein was probably aware of De Pretto's work.
Preston and De Pretto, following Le Sage, imagined that the universe was filled with an ether of tiny particles that always move at speed . Each of these particles has a kinetic energy of up to a small numerical factor. The nonrelativistic kinetic energy formula did not always include the traditional factor of 1/2, since Leibniz introduced kinetic energy without it, and the 1/2 is largely conventional in prerelativistic physics. By assuming that every particle has a mass that is the sum of the masses of the ether particles, the authors concluded that all matter contains an amount of kinetic energy either given by or depending on the convention. A particle ether was usually considered unacceptably speculative science at the time, and since these authors did not formulate relativity, their reasoning is completely different from that of Einstein, who used relativity to change frames.
Independently, Gustave Le Bon in 1905 speculated that atoms could release large amounts of latent energy, reasoning from an all-encompassing qualitative philosophy of physics.
Radioactivity and nuclear energy.
It was quickly noted after the discovery of radioactivity in 1897, that the total energy due to radioactive processes is about one "million times" greater than that involved in any known molecular change. However, it raised the question where this energy is coming from. After eliminating the idea of absorption and emission of some sort of Lesagian ether particles, the existence of a huge amount of latent energy, stored within matter, was proposed by Ernest Rutherford and Frederick Soddy in 1903. Rutherford also suggested that this internal energy is stored within normal matter as well. He went on to speculate in 1904:
Einstein's equation is in no way an explanation of the large energies released in radioactive decay (this comes from the powerful nuclear forces involved; forces that were still unknown in 1905). In any case, the enormous energy released from radioactive decay (which had been measured by Rutherford) was much more easily measured than the (still small) change in the gross mass of materials, as a result. Einstein's equation, by theory, can give these energies by measuring mass differences before and after reactions, but in practice, these mass differences in 1905 were still too small to be measured in bulk. Prior to this, the ease of measuring radioactive decay energies with a calorimeter was thought possibly likely to allow measurement of changes in mass difference, as a check on Einstein's equation itself. Einstein mentions in his 1905 paper that mass–energy equivalence might perhaps be tested with radioactive decay, which releases enough energy (the quantitative amount known roughly by 1905) to possibly be "weighed," when missing from the system (having been given off as heat). However, radioactivity seemed to proceed at its own unalterable (and quite slow, for radioactives known then) pace, and even when simple nuclear reactions became possible using proton bombardment, the idea that these great amounts of usable energy could be liberated at will with any practicality, proved difficult to substantiate. Rutherford was reported in 1933 to have declared that this energy could not be exploited efficiently: "Anyone who expects a source of power from the transformation of the atom is talking moonshine."
This situation changed dramatically in 1932 with the discovery of the neutron and its mass, allowing mass differences for single nuclides and their reactions to be calculated directly, and compared with the sum of masses for the particles that made up their composition. In 1933, the energy released from the reaction of lithium-7 plus protons giving rise to 2 alpha particles (as noted above by Rutherford), allowed Einstein's equation to be tested to an error of ±0.5%. However, scientists still did not see such reactions as a source of power.
After the very public demonstration of huge energies released from nuclear fission after the atomic bombings of Hiroshima and Nagasaki in 1945, the equation became directly linked in the public eye with the power and peril of nuclear weapons. The equation was featured as early as page 2 of the Smyth Report, the official 1945 release by the US government on the development of the atomic bomb, and by 1946 the equation was linked closely enough with Einstein's work that the cover of "Time" magazine prominently featured a picture of Einstein next to an image of a mushroom cloud emblazoned with the equation. Einstein himself had only a minor role in the Manhattan Project: he had cosigned a letter to the U.S. President in 1939 urging funding for research into atomic energy, warning that an atomic bomb was theoretically possible. The letter persuaded Roosevelt to devote a significant portion of the wartime budget to atomic research. Without a security clearance, Einstein's only scientific contribution was an analysis of an isotope separation method in theoretical terms. It was inconsequential, on account of Einstein not being given sufficient information (for security reasons) to fully work on the problem.
While is useful for understanding the amount of energy potentially released in a fission reaction, it was not strictly necessary to develop the weapon, once the fission process was known, and its energy measured at 200 MeV (which was directly possible, using a quantitative Geiger counter, at that time). As the physicist and Manhattan Project participant Robert Serber put it: "Somehow the popular notion took hold long ago that Einstein's theory of relativity, in particular his famous equation , plays some essential role in the theory of fission. Albert Einstein had a part in alerting the United States government to the possibility of building an atomic bomb, but his theory of relativity is not required in discussing fission. The theory of fission is what physicists call a non-relativistic theory, meaning that relativistic effects are too small to affect the dynamics of the fission process significantly." However the association between and nuclear energy has since stuck, and because of this association, and its simple expression of the ideas of Albert Einstein himself, it has become "the world's most famous equation".
While Serber's view of the strict lack of need to use mass–energy equivalence in designing the atomic bomb is correct, it does not take into account the pivotal role this relationship played in making the fundamental leap to the initial hypothesis that large atoms were energetically "allowed" to split into approximately equal parts (before this energy was in fact measured). In late 1938, Lise Meitner and Otto Robert Frisch—while on a winter walk during which they solved the meaning of Hahn's experimental results and introduced the idea that would be called atomic fission—directly used Einstein's equation to help them understand the quantitative energetics of the reaction that overcame the "surface tension-like" forces that hold the nucleus together, and allowed the fission fragments to separate to a configuration from which their charges could force them into an energetic "fission". To do this, they used "packing fraction", or nuclear binding energy values for elements, which Meitner had memorized. These, together with use of allowed them to realize on the spot that the basic fission process was energetically possible:
 ...We walked up and down in the snow, I on skis and she on foot. ...and gradually the idea took shape... explained by Bohr's idea that the nucleus is like a liquid drop; such a drop might elongate and divide itself... We knew there were strong forces that would resist, ..just as surface tension. But nuclei differed from ordinary drops. At this point we both sat down on a tree trunk and started to calculate on scraps of paper. ...the Uranium nucleus might indeed be a very wobbly, unstable drop, ready to divide itself... But, ...when the two drops separated they would be driven apart by electrical repulsion, about 200 MeV in all. Fortunately Lise Meitner remembered how to compute the masses of nuclei... and worked out that the two nuclei formed... would be lighter by about one-fifth the mass of a proton. Now whenever mass disappears energy is created, according to Einstein's formula E = mc2, and... the mass was just equivalent to 200 MeV; it all fitted!

</doc>
<doc id="20728" url="https://en.wikipedia.org/wiki?curid=20728" title="Mathematical formulation of quantum mechanics">
Mathematical formulation of quantum mechanics

The mathematical formulations of quantum mechanics are those mathematical formalisms that permit a rigorous description of quantum mechanics. Such are distinguished from mathematical formalisms for theories developed prior to the early 1900s by the use of abstract mathematical structures, such as infinite-dimensional Hilbert spaces and operators on these spaces. Many of these structures are drawn from functional analysis, a research area within pure mathematics that was influenced in part by the needs of quantum mechanics. In brief, values of physical observables such as energy and momentum were no longer considered as values of functions on phase space, but as eigenvalues; more precisely: as spectral values (point spectrum plus absolute continuous plus singular continuous spectrum) of linear operators in Hilbert space.
These formulations of quantum mechanics continue to be used today. At the heart of the description are ideas of "quantum state" and "quantum observable" which are radically different from those used in previous models of physical reality. While the mathematics permits calculation of many quantities that can be measured experimentally, there is a definite theoretical limit to values that can be simultaneously measured. This limitation was first elucidated by Heisenberg through a thought experiment, and is represented mathematically in the new formalism by the non-commutativity of operators representing quantum observables.
Prior to the emergence of quantum mechanics as a separate theory, the mathematics used in physics consisted mainly of formal mathematical analysis, beginning with calculus, and increasing in complexity up to differential geometry and partial differential equations. Probability theory was used in statistical mechanics. Geometric intuition played a strong role in the first two and, accordingly, theories of relativity were formulated entirely in terms of geometric concepts. The phenomenology of quantum physics arose roughly between 1895 and 1915, and for the 10 to 15 years before the emergence of quantum theory (around 1925) physicists continued to think of quantum theory within the confines of what is now called classical physics, and in particular within the same mathematical structures. The most sophisticated example of this is the Sommerfeld–Wilson–Ishiwara quantization rule, which was formulated entirely on the classical phase space.
History of the formalism.
The "old quantum theory" and the need for new mathematics.
In the 1890s, Planck was able to derive the blackbody spectrum which was later used to avoid the classical ultraviolet catastrophe by making the unorthodox assumption that, in the interaction of electromagnetic radiation with matter, energy could only be exchanged in discrete units which he called quanta. Planck postulated a direct proportionality between the frequency of radiation and the quantum of energy at that frequency. The proportionality constant, , is now called Planck's constant in his honor.
In 1905, Einstein explained certain features of the photoelectric effect by assuming that Planck's energy quanta were actual particles, which were later dubbed photons. 
All of these developments were phenomenological and challenged the theoretical physics of the time. Bohr and Sommerfeld went on to modify classical mechanics in an attempt to deduce the Bohr model from first principles. They proposed that, of all closed classical orbits traced by a mechanical system in its phase space, only the ones that enclosed an area which was a multiple of Planck's constant were actually allowed. The most sophisticated version of this formalism was the so-called Sommerfeld–Wilson–Ishiwara quantization. Although the Bohr model of the hydrogen atom could be explained in this way, the spectrum of the helium atom (classically an unsolvable 3-body problem) could not be predicted. The mathematical status of quantum theory remained uncertain for some time.
In 1923 de Broglie proposed that wave–particle duality applied not only to photons but to electrons and every other physical system.
The situation changed rapidly in the years 1925–1930, when working mathematical foundations were found through the groundbreaking work of Erwin Schrödinger, Werner Heisenberg, Max Born, Pascual Jordan, and the foundational work of John von Neumann, Hermann Weyl and Paul Dirac, and it became possible to unify several different approaches in terms of a fresh set of ideas. The physical interpretation of the theory was also clarified in these years after Werner Heisenberg discovered the uncertainty relations and Niels Bohr introduced the idea of complementarity.
The "new quantum theory".
Werner Heisenberg's matrix mechanics was the first successful attempt at replicating the observed quantization of atomic spectra. Later in the same year, Schrödinger created his wave mechanics. Schrödinger's formalism was considered easier to understand, visualize and calculate as it led to differential equations, which physicists were already familiar with solving. Within a year, it was shown that the two theories were equivalent.
Schrödinger himself initially did not understand the fundamental probabilistic nature of quantum mechanics, as he thought that the absolute square of the wave function of an electron should be interpreted as the charge density of an object smeared out over an extended, possibly infinite, volume of space. It was Max Born who introduced the interpretation of the absolute square of the wave function as the probability distribution of the position of a "pointlike" object. Born's idea was soon taken over by Niels Bohr in Copenhagen who then became the "father" of the Copenhagen interpretation of quantum mechanics. Schrödinger's wave function can be seen to be closely related to the classical Hamilton–Jacobi equation. The correspondence to classical mechanics was even more explicit, although somewhat more formal, in Heisenberg's matrix mechanics. In his PhD thesis project, Paul Dirac discovered that the equation for the operators in the Heisenberg representation, as it is now called, closely translates to classical equations for the dynamics of certain quantities in the Hamiltonian formalism of classical mechanics, when one expresses them through Poisson brackets, a procedure now known as canonical quantization.
To be more precise, already before Schrödinger, the young postdoctoral fellow Werner Heisenberg invented his matrix mechanics, which was the first correct quantum mechanics–– the essential breakthrough. Heisenberg's matrix mechanics formulation was based on algebras of infinite matrices, a very radical formulation in light of the mathematics of classical physics, although he started from the index-terminology of the experimentalists of that time, not even aware that his "index-schemes" were matrices, as Born soon pointed out to him. In fact, in these early years, linear algebra was not generally popular with physicists in its present form.
Although Schrödinger himself after a year proved the equivalence of his wave-mechanics and Heisenberg's matrix mechanics, the reconciliation of the two approaches and their modern abstraction as motions in Hilbert space is generally attributed to Paul Dirac, who wrote a lucid account in his 1930 classic The Principles of Quantum Mechanics. He is the third, and possibly most important, pillar of that field (he soon was the only one to have discovered a relativistic generalization of the theory). In his above-mentioned account, he introduced the bra–ket notation, together with an abstract formulation in terms of the Hilbert space used in functional analysis; he showed that Schrödinger's and Heisenberg's approaches were two different representations of the same theory, and found a third, most general one, which represented the dynamics of the system. His work was particularly fruitful in all kinds of generalizations of the field.
The first complete mathematical formulation of this approach, known as the Dirac–von Neumann axioms, is generally credited to John von Neumann's 1932 book "Mathematical Foundations of Quantum Mechanics", although Hermann Weyl had already referred to Hilbert spaces (which he called "unitary spaces") in his 1927 classic paper and book. It was developed in parallel with a new approach to the mathematical spectral theory based on linear operators rather than the quadratic forms that were David Hilbert's approach a generation earlier. Though theories of quantum mechanics continue to evolve to this day, there is a basic framework for the mathematical formulation of quantum mechanics which underlies most approaches and can be traced back to the mathematical work of John von Neumann. In other words, discussions about "interpretation" of the theory, and extensions to it, are now mostly conducted on the basis of shared assumptions about the mathematical foundations.
Later developments.
The application of the new quantum theory to electromagnetism resulted in quantum field theory, which was developed starting around 1930. Quantum field theory has driven the development of more sophisticated formulations of quantum mechanics, of which the one presented here is a simple special case. 
On a different front, von Neumann originally dispatched quantum measurement with his infamous postulate on the collapse of the wavefunction, raising a host of philosophical problems. Over the intervening 70 years, the "problem of measurement" became an active research area and itself spawned some new formulations of quantum mechanics.
A related topic is the relationship to classical mechanics. Any new physical theory is supposed to reduce to successful old theories in some approximation. For quantum mechanics, this translates into the need to study the so-called classical limit of quantum mechanics. Also, as Bohr emphasized, human cognitive abilities and language are inextricably linked to the classical realm, and so classical descriptions are intuitively more accessible than quantum ones. In particular, quantization, namely the construction of a quantum theory whose classical limit is a given and known classical theory, becomes an important area of quantum physics in itself.
Finally, some of the originators of quantum theory (notably Einstein and Schrödinger) were unhappy with what they thought were the philosophical implications of quantum mechanics. In particular, Einstein took the position that quantum mechanics must be incomplete, which motivated research into so-called hidden-variable theories. The issue of hidden variables has become in part an experimental issue with the help of quantum optics.
Mathematical structure of quantum mechanics.
A physical system is generally described by three basic ingredients: states; observables; and dynamics (or law of time evolution) or, more generally, a group of physical symmetries. A classical description can be given in a fairly direct way by a phase space model of mechanics: states are points in a symplectic phase space, observables are real-valued functions on it, time evolution is given by a one-parameter group of symplectic transformations of the phase space, and physical symmetries are realized by symplectic transformations. A quantum description consists of a Hilbert space of states, observables are self adjoint operators on the space of states, time evolution is given by a one-parameter group of unitary transformations on the Hilbert space of states, and physical symmetries are realized by unitary transformations.
Postulates of quantum mechanics.
The following summary of the mathematical framework of quantum mechanics can be partly traced back to the Dirac–von Neumann axioms.
One can in this formalism state Heisenberg's uncertainty principle and prove it as a theorem, although the exact historical sequence of events, concerning who derived what and under which framework, is the subject of historical investigations outside the scope of this article.
Furthermore, to the postulates of quantum mechanics one should also add basic statements on the properties of spin and Pauli's exclusion principle, see below.
Pictures of dynamics.
The time evolution of the state is given by a differentiable function from the real numbers , representing instants of time, to the Hilbert space of system states. This map is characterized by a differential equation as follows:
If denotes the state of the system at any one time , the following Schrödinger equation holds:
where is a densely defined self-adjoint operator, called the system Hamiltonian, is the imaginary unit and is the reduced Planck constant. As an observable, corresponds to the total energy of the system.
Alternatively, by Stone's theorem one can state that there is a strongly continuous one-parameter unitary group : such that
for all times . The existence of a self-adjoint Hamiltonian such that
is a consequence of Stone's theorem on one-parameter unitary groups. It is assumed that does not depend on time and that the perturbation starts at ; otherwise one must use the Dyson series, formally written as 
where formula_7 is Dyson's time-ordering symbol.
(This symbol permutes a product of noncommuting operators of the form 
into the uniquely determined re-ordered expression 
The result is a causal chain, the primary "cause" in the past on the utmost r.h.s., and finally the present "effect" on the utmost l.h.s. .)
It is then easily checked that the expected values of all observables are the same in both pictures 
and that the time-dependent Heisenberg operators satisfy
which is true for time-dependent . Notice the commutator expression is purely formal when one of the operators is unbounded. One would specify a representation for the expression to make sense of it.
The interaction picture does not always exist, though. In interacting quantum field theories, Haag's theorem states that the interaction picture does not exist. This is because the Hamiltonian cannot be split into a free and an interacting part within a superselection sector. Moreover, even if in the Schrödinger picture the Hamiltonian does not depend on time, e.g. , in the interaction picture it does, at least, if does not commute with , since 
So the above-mentioned Dyson-series has to be used anyhow.
The Heisenberg picture is the closest to classical Hamiltonian mechanics (for example, the commutators appearing in the above equations directly translate into the classical Poisson brackets); but this is already rather "high-browed", and the Schrödinger picture is considered easiest to visualize and understand by most people, to judge from pedagogical accounts of quantum mechanics. The Dirac picture is the one used in perturbation theory, and is specially associated to quantum field theory and many-body physics.
Similar equations can be written for any one-parameter unitary group of symmetries of the physical system. Time would be replaced by a suitable coordinate parameterizing the unitary group (for instance, a rotation angle, or a translation distance) and the Hamiltonian would be replaced by the conserved quantity associated to the symmetry (for instance, angular or linear momentum).
Representations.
The original form of the Schrödinger equation depends on choosing a particular representation of Heisenberg's canonical commutation relations. The Stone–von Neumann theorem dictates that all irreducible representations of the finite-dimensional Heisenberg commutation relations are unitarily equivalent. A systematic understanding of its consequences has led to the phase space formulation of quantum mechanics, which works in full phase space instead of Hilbert space, so then with a more intuitive link to the classical limit thereof. This picture also simplifies considerations
of quantization, the deformation extension from classical to quantum mechanics.
The quantum harmonic oscillator is an exactly solvable system where the different representations are easily compared. There, apart from the Heisenberg, or Schrödinger (position or momentum), or phase-space representations, one also encounters the Fock (number) representation and the Segal–Bargmann (Fock-space or coherent state) representation (named after Irving Segal and Valentine Bargmann). All four are unitarily equivalent.
Time as an operator.
The framework presented so far singles out time as "the" parameter that everything depends on. It is possible to formulate mechanics in such a way that time becomes itself an observable associated to a self-adjoint operator. At the classical level, it is possible to arbitrarily parameterize the trajectories of particles in terms of an unphysical parameter {{math|"s"}}, and in that case the time "t" becomes an additional generalized coordinate of the physical system. At the quantum level, translations in {{math|"s"}} would be generated by a "Hamiltonian" {{math|"H" − "E"}}, where "E" is the energy operator and {{math|"H"}} is the "ordinary" Hamiltonian. However, since "s" is an unphysical parameter, "physical" states must be left invariant by ""s"-evolution", and so the physical state space is the kernel of {{math|"H" − "E"}} (this requires the use of a rigged Hilbert space and a renormalization of the norm).
This is related to the quantization of constrained systems and quantization of gauge theories. It
is also possible to formulate a quantum theory of "events" where time becomes an observable (see D. Edwards).
Spin.
In addition to their other properties, all particles possess a quantity called spin, an "intrinsic angular momentum". Despite the name, particles do not literally spin around an axis, and quantum mechanical spin has no correspondence in classical physics. In the position representation, a spinless wavefunction has position {{math|r}} and time {{math|"t"}} as continuous variables, {{math|"ψ" {{=}} "ψ"(r, "t")}}, for spin wavefunctions the spin is an additional discrete variable: {{math|"ψ" {{=}} "ψ"(r, "t", "σ")}}, where {{math|"σ"}} takes the values;
That is, the state of a single particle with spin {{math|"S"}} is represented by a {{math|(2"S" + 1)}}-component spinor of complex-valued wave functions.
Two classes of particles with "very different" behaviour are bosons which have integer spin ({{math|"S" {{=}} 0, 1, 2...}}), and fermions possessing half-integer spin ({{math|"S" {{=}} {{frac|1|2}}, {{frac|3|2}}, {{frac|5|2}}, ...}}).
Pauli's principle.
The property of spin relates to another basic property concerning systems of {{math|"N"}} identical particles: Pauli's exclusion principle, which is a consequence of the following permutation behaviour of an {{math|"N"}}-particle wave function; again in the position representation one must postulate that for the transposition of any two of the {{math|"N"}} particles one always should have
i.e., on transposition of the arguments of any two particles the wavefunction should "reproduce", apart from a prefactor {{math|(−1)2"S"}} which is {{math|+1}} for bosons, but ({{math|−1}}) for fermions.
Electrons are fermions with {{math|"S" {{=}} 1/2}}; quanta of light are bosons with {{math|"S" {{=}} 1}}. In nonrelativistic quantum mechanics all particles are either bosons or fermions; in relativistic quantum theories also "supersymmetric" theories exist, where a particle is a linear combination of a bosonic and a fermionic part. Only in dimension {{math|"d" {{=}} 2}} can one construct entities where {{math|(−1)2"S"}} is replaced by an arbitrary complex number with magnitude 1, called anyons.
Although "spin" and the "Pauli principle" can only be derived from relativistic generalizations of quantum mechanics the properties mentioned in the last two paragraphs belong to the basic postulates already in the non-relativistic limit. Especially, many important properties in natural science, e.g. the periodic system of chemistry, are consequences of the two properties.
The problem of measurement.
The picture given in the preceding paragraphs is sufficient for description of a completely isolated system. However, it fails to account for one of the main differences between quantum mechanics and classical mechanics, that is, the effects of measurement. The von Neumann description of quantum measurement of an observable {{math|"A"}}, when the system is prepared in a pure state "{{math|"ψ"}}" is the following (note, however, that von Neumann's description dates back to the 1930s and is based on experiments as performed during that time – more specifically the Compton–Simon experiment; it is not applicable to most present-day measurements within the quantum domain):
where {{math|E"A"}} is the resolution of the identity (also called projection-valued measure) associated to {{math|"A"}}. Then the probability of the measurement outcome lying in an interval {{math|"B"}} of {{math|R}} is {{math|{{!}}E"A"("B") "ψ"{{!}}2}}. In other words, the probability is obtained by integrating the characteristic function of {{math|"B"}} against the countably additive measure
For example, suppose the state space is the {{math|"n"}}-dimensional complex Hilbert space {{math|C"n"}} and {{math|"A"}} is a Hermitian matrix with eigenvalues {{math|"λ""i"}}, with corresponding eigenvectors {{math|"ψi"}}. The projection-valued measure associated with {{math|"A"}}, {{math|E"A"}}, is then
where {{math|"B"}} is a Borel set containing only the single eigenvalue {{math|"λi"}}. If the system is prepared in state
Then the probability of a measurement returning the value {{math|"λi"}} can be calculated by integrating the spectral measure
over {{math|"Bi"}}. This gives trivially
The characteristic property of the von Neumann measurement scheme is that repeating the same measurement will give the same results. This is also called the "projection postulate".
A more general formulation replaces the projection-valued measure with a positive-operator valued measure (POVM). To illustrate, take again the finite-dimensional case. Here we would replace the rank-1 projections
by a finite set of positive operators
whose sum is still the identity operator as before (the resolution of identity). Just as a set of possible outcomes {{math|{"λ"1 ... "λn"} }} is associated to a projection-valued measure, the same can be said for a POVM. Suppose the measurement outcome is {{math|"λi"}}. Instead of collapsing to the (unnormalized) state
after the measurement, the system now will be in the state
Since the {{math|"Fi Fi*"}} operators need not be mutually orthogonal projections, the projection postulate of von Neumann no longer holds.
The same formulation applies to general mixed states.
In von Neumann's approach, the state transformation due to measurement is distinct from that due to time evolution in several ways. For example, time evolution is deterministic and unitary whereas measurement is non-deterministic and non-unitary. However, since both types of state transformation take one quantum state to another, this difference was viewed by many as unsatisfactory. The POVM formalism views measurement as one among many other quantum operations, which are described by completely positive maps which do not increase the trace.
In any case it seems that the above-mentioned problems can only be resolved if the time evolution included not only the quantum system, but also, and essentially, the classical measurement apparatus (see above).
The "relative state" interpretation.
An alternative interpretation of measurement is Everett's relative state interpretation, which was later dubbed the "many-worlds interpretation" of quantum physics.
List of mathematical tools.
Part of the folklore of the subject concerns the mathematical physics textbook Methods of Mathematical Physics put together by Richard Courant from David Hilbert's Göttingen University courses. The story is told (by mathematicians) that physicists had dismissed the material as not interesting in the current research areas, until the advent of Schrödinger's equation. At that point it was realised that the mathematics of the new quantum mechanics was already laid out in it. It is also said that Heisenberg had consulted Hilbert about his matrix mechanics, and Hilbert observed that his own experience with infinite-dimensional matrices had derived from differential equations, advice which Heisenberg ignored, missing the opportunity to unify the theory as Weyl and Dirac did a few years later. Whatever the basis of the anecdotes, the mathematics of the theory was conventional at the time, whereas the physics was radically new.
The main tools include:

</doc>
<doc id="20556859" url="https://en.wikipedia.org/wiki?curid=20556859" title="Matrix (mathematics)">
Matrix (mathematics)

In mathematics, a matrix (plural matrices) is a rectangular "array"—of numbers, symbols, or expressions, arranged in "rows" and "columns"—that is interpreted and manipulated in certain prescribed ways. One such way is to state the dimensions of the matrix. For example, the dimensions of the matrix below are "2" × "3" (read "two by three"), because there are two rows and three columns. 
The individual items in a matrix are called its "elements" or "entries". Provided that they are the same size (have the same number of rows and the same number of columns), two matrices can be added or subtracted element by element. The rule for matrix multiplication, however, is that two matrices can be multiplied only when the number of columns in the first equals the number of rows in the second. A major application of matrices is to represent linear transformations, that is, generalizations of linear functions such as . For example, the rotation of vectors in three dimensional space is a linear transformation which can be represented by a rotation matrix R: if v is a column vector (a matrix with only one column) describing the position of a point in space, the product Rv is a column vector describing the position of that point after a rotation. The product of two transformation matrices is a matrix that represents the composition of two linear transformations. Another application of matrices is in the solution of systems of linear equations. If the matrix is square, it is possible to deduce some of its properties by computing its determinant. For example, a square matrix has an inverse if and only if its determinant is not zero. Insight into the geometry of a linear transformation is obtainable (along with other information) from the matrix's eigenvalues and eigenvectors.
Applications of matrices are found in most scientific fields. In every branch of physics, including classical mechanics, optics, electromagnetism, quantum mechanics, and quantum electrodynamics, they are used to study physical phenomena, such as the motion of rigid bodies. In computer graphics, they are used to project a 3-dimensional image onto a 2-dimensional screen. In probability theory and statistics, stochastic matrices are used to describe sets of probabilities; for instance, they are used within the PageRank algorithm that ranks the pages in a Google search. Matrix calculus generalizes classical analytical notions such as derivatives and exponentials to higher dimensions.
A major branch of numerical analysis is devoted to the development of efficient algorithms for matrix computations, a subject that is centuries old and is today an expanding area of research. Matrix decomposition methods simplify computations, both theoretically and practically. Algorithms that are tailored to particular matrix structures, such as sparse matrices and near-diagonal matrices, expedite computations in finite element method and other computations. Infinite matrices occur in planetary theory and in atomic theory. A simple example of an infinite matrix is the matrix representing the derivative operator, which acts on the Taylor series of a function.
Definition.
A "matrix" is a rectangular array of numbers or other mathematical objects for which operations such as addition and multiplication are defined. Most commonly, a matrix over a field "F" is a rectangular array of scalars each of which is a member of "F". Most of this article focuses on "real" and "complex matrices", i.e., matrices whose elements are real numbers or complex numbers, respectively. More general types of entries are discussed below. For instance, this is a real matrix:
The numbers, symbols or expressions in the matrix are called its "entries" or its "elements". The horizontal and vertical lines of entries in a matrix are called "rows" and "columns", respectively.
Size.
The size of a matrix is defined by the number of rows and columns that it contains. A matrix with "m" rows and "n" columns is called an "m" × "n" matrix or "m"-by-"n" matrix, while "m" and "n" are called its "dimensions". For example, the matrix A above is a 3 × 2 matrix.
Matrices which have a single row are called "row vectors", and those which have a single column are called "column vectors". A matrix which has the same number of rows and columns is called a "square matrix". A matrix with an infinite number of rows or columns (or both) is called an "infinite matrix". In some contexts, such as computer algebra programs, it is useful to consider a matrix with no rows or no columns, called an "empty matrix".
Notation.
Matrices are commonly written in box brackets or large parentheses:
The specifics of symbolic matrix notation vary widely, with some prevailing trends. Matrices are usually symbolized using upper-case letters (such as A in the examples above), while the corresponding lower-case letters, with two subscript indices (e.g., "a"11, or "a"1,1), represent the entries. In addition to using upper-case letters to symbolize matrices, many authors use a special typographical style, commonly boldface upright (non-italic), to further distinguish matrices from other mathematical objects. An alternative notation involves the use of a double-underline with the variable name, with or without boldface style, (e.g., formula_4).
The entry in the "i"-th row and "j"-th column of a matrix A is sometimes referred to as the "i","j", ("i","j"), or ("i","j")th entry of the matrix, and most commonly denoted as "a""i","j", or "a""ij". Alternative notations for that entry are "A"["i,j"] or "A""i,j". For example, the (1,3) entry of the following matrix A is 5 (also denoted "a"13, "a"1,3, "A"["1,3"] or "A""1,3"):
Sometimes, the entries of a matrix can be defined by a formula such as "a""i","j" = "f"("i", "j"). For example, each of the entries of the following matrix A is determined by "a""ij" = "i" − "j". 
In this case, the matrix itself is sometimes defined by that formula, within square brackets or double parenthesis. For example, the matrix above is defined as A = ["i"-"j"], or A = (("i"-"j")). If matrix size is "m" × "n", the above-mentioned formula "f"("i", "j") is valid for any "i" = 1, ..., "m" and any "j" = 1, ..., "n". This can be either specified separately, or using "m" × "n" as a subscript. For instance, the matrix A above is 3 × 4 and can be defined as A = ["i" − "j"] ("i" = 1, 2, 3; "j" = 1, ..., 4), or A = ["i" − "j"]"3"×"4".
Some programming languages utilize doubly subscripted arrays (or arrays of arrays) to represent an "m"-×-"n" matrix. Some programming languages start the numbering of array indexes at zero, in which case the entries of an "m"-by-"n" matrix are indexed by and . This article follows the more common convention in mathematical writing where enumeration starts from 1.
The set of all "m"-by-"n" matrices is denoted 𝕄("m", "n").
Basic operations.
There are a number of basic operations that can be applied to modify matrices, called "matrix addition", "scalar multiplication", "transposition", "matrix multiplication", "row operations", and "submatrix".
Addition, scalar multiplication and transposition.
Familiar properties of numbers extend to these operations of matrices: for example, addition is commutative, i.e., the matrix sum does not depend on the order of the summands: A + B = B + A.
The transpose is compatible with addition and scalar multiplication, as expressed by ("c"A)T = "c"(AT) and (A + B)T = AT + BT. Finally, (AT)T = A.
Matrix multiplication.
"Multiplication" of two matrices is defined if and only if the number of columns of the left matrix is the same as the number of rows of the right matrix. If A is an "m"-by-"n" matrix and B is an "n"-by-"p" matrix, then their "matrix product" AB is the "m"-by-"p" matrix whose entries are given by dot product of the corresponding row of A and the corresponding column of B:
where 1 ≤ "i" ≤ "m" and 1 ≤ "j" ≤ "p". For example, the underlined entry 2340 in the product is calculated as 
Matrix multiplication satisfies the rules (AB)C = A(BC) (associativity), and (A+B)C = AC+BC as well as C(A+B) = CA+CB (left and right distributivity), whenever the size of the matrices is such that the various products are defined. The product AB may be defined without BA being defined, namely if A and B are "m"-by-"n" and "n"-by-"k" matrices, respectively, and Even if both products are defined, they need not be equal, i.e., generally 
i.e., matrix multiplication is not commutative, in marked contrast to (rational, real, or complex) numbers whose product is independent of the order of the factors. An example of two matrices not commuting with each other is:
whereas
Besides the ordinary matrix multiplication just described, there exist other less frequently used operations on matrices that can be considered forms of multiplication, such as the Hadamard product and the Kronecker product. They arise in solving matrix equations such as the Sylvester equation.
Row operations.
There are three types of row operations:
These operations are used in a number of ways, including solving linear equations and finding matrix inverses.
Submatrix.
A submatrix of a matrix is obtained by deleting any collection of rows and/or columns. For example, from the following 3-by-4 matrix, we can construct a 2-by-3 submatrix by removing row 3 and column 2:
The minors and cofactors of a matrix are found by computing the determinant of certain submatrices.
A principal submatrix is a square submatrix obtained by removing certain rows and columns. The definition varies from author to author. According to some authors, a principal submatrix is a submatrix in which the set of row indices that remain is the same as the set of column indices that remain. Other authors define a principal submatrix to be one in which the first "k" rows and columns, for some number "k", are the ones that remain; this type of submatrix has also been called a leading principal submatrix.
Linear equations.
Matrices can be used to compactly write and work with multiple linear equations, i.e., systems of linear equations. For example, if A is an "m"-by-"n" matrix, x designates a column vector (i.e., "n"×1-matrix) of "n" variables "x"1, "x"2, ..., "x""n", and b is an "m"×1-column vector, then the matrix equation
is equivalent to the system of linear equations
Linear transformations.
Matrices and matrix multiplication reveal their essential features when related to "linear transformations", also known as "linear maps". A real "m"-by-"n" matrix A gives rise to a linear transformation R"n" → R"m" mapping each vector x in R"n" to the (matrix) product Ax, which is a vector in R"m". Conversely, each linear transformation "f": R"n" → R"m" arises from a unique "m"-by-"n" matrix A: explicitly, the of A is the "i"th coordinate of "f"(e"j"), where e"j" = (0...,0,1,0...,0) is the unit vector with 1 in the "j"th position and 0 elsewhere. The matrix A is said to represent the linear map "f", and A is called the "transformation matrix" of "f".
For example, the 2×2 matrix
can be viewed as the transform of the unit square into a parallelogram with vertices at , , , and . The parallelogram pictured at the right is obtained by multiplying A with each of the column vectors formula_13 and formula_14 in turn. These vectors define the vertices of the unit square.
The following table shows a number of 2-by-2 matrices with the associated linear maps of R2. The blue original is mapped to the green grid and shapes. The origin (0,0) is marked with a black point.
Under the 1-to-1 correspondence between matrices and linear maps, matrix multiplication corresponds to composition of maps: if a "k"-by-"m" matrix B represents another linear map "g" : R"m" → R"k", then the composition is represented by BA since
The last equality follows from the above-mentioned associativity of matrix multiplication.
The rank of a matrix A is the maximum number of linearly independent row vectors of the matrix, which is the same as the maximum number of linearly independent column vectors. Equivalently it is the dimension of the image of the linear map represented by A. The rank-nullity theorem states that the dimension of the kernel of a matrix plus the rank equals the number of columns of the matrix.
Square matrices.
A square matrix is a matrix with the same number of rows and columns. An "n"-by-"n" matrix is known as a square matrix of order "n." Any two square matrices of the same order can be added and multiplied. 
The entries "a""ii" form the main diagonal of a square matrix. They lie on the imaginary line which runs from the top left corner to the bottom right corner of the matrix.
Main types.
Diagonal and triangular matrices.
If all entries of A below the main diagonal are zero, A is called an "upper triangular matrix". Similarly if all entries of "A" above the main diagonal are zero, A is called a "lower triangular matrix". If all entries outside the main diagonal are zero, A is called a diagonal matrix.
Identity matrix.
The identity matrix I"n" of size "n" is the "n"-by-"n" matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0, e.g.
It is a square matrix of order "n", and also a special kind of diagonal matrix. It is called an identity matrix because multiplication with it leaves a matrix unchanged: 
Symmetric or skew-symmetric matrix.
A square matrix A that is equal to its transpose, i.e., A = AT, is a symmetric matrix. If instead, A was equal to the negative of its transpose, i.e., A = −AT, then A is a skew-symmetric matrix. In complex matrices, symmetry is often replaced by the concept of Hermitian matrices, which satisfy A∗ = A, where the star or asterisk denotes the conjugate transpose of the matrix, i.e., the transpose of the complex conjugate of A.
By the spectral theorem, real symmetric matrices and complex Hermitian matrices have an eigenbasis; i.e., every vector is expressible as a linear combination of eigenvectors. In both cases, all eigenvalues are real. This theorem can be generalized to infinite-dimensional situations related to matrices with infinitely many rows and columns, see below.
Invertible matrix and its inverse.
A square matrix A is called "invertible" or "non-singular" if there exists a matrix B such that
If B exists, it is unique and is called the "inverse matrix" of A, denoted A−1.
Definite matrix.
A symmetric "n"×"n"-matrix is called" positive-definite" (respectively negative-definite; indefinite), if for all nonzero vectors x ∈ R"n" the associated quadratic form given by
takes only positive values (respectively only negative values; both some negative and some positive values). If the quadratic form takes only non-negative (respectively only non-positive) values, the symmetric matrix is called positive-semidefinite (respectively negative-semidefinite); hence the matrix is indefinite precisely when it is neither positive-semidefinite nor negative-semidefinite.
A symmetric matrix is positive-definite if and only if all its eigenvalues are positive, i.e., the matrix is positive-semidefinite and it is invertible. The table at the right shows two possibilities for 2-by-2 matrices.
Allowing as input two different vectors instead yields the bilinear form associated to A:
Orthogonal matrix.
An "orthogonal matrix" is a square matrix with real entries whose columns and rows are orthogonal unit vectors (i.e., orthonormal vectors). Equivalently, a matrix "A" is orthogonal if its transpose is equal to its inverse:
which entails
where "I" is the identity matrix.
An orthogonal matrix "A" is necessarily invertible (with inverse ), unitary (), and normal (). The determinant of any orthogonal matrix is either +1 or −1. A "special orthogonal matrix" is an orthogonal matrix with determinant +1. As a linear transformation, every orthogonal matrix with determinant +1 is a pure rotation, while every orthogonal matrix with determinant -1 is either a pure reflection, or a composition of reflection and rotation.
The complex analogue of an orthogonal matrix is a unitary matrix.
Main operations.
Trace.
The trace, tr(A) of a square matrix A is the sum of its diagonal entries. While matrix multiplication is not commutative as mentioned above, the trace of the product of two matrices is independent of the order of the factors: 
This is immediate from the definition of matrix multiplication:
Also, the trace of a matrix is equal to that of its transpose, i.e.,
Determinant.
The "determinant" det(A) or |A| of a square matrix A is a number encoding certain properties of the matrix. A matrix is invertible if and only if its determinant is nonzero. Its absolute value equals the area (in R2) or volume (in R3) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.
The determinant of 2-by-2 matrices is given by
The determinant of 3-by-3 matrices involves 6 terms (rule of Sarrus). The more lengthy Leibniz formula generalises these two formulae to all dimensions.
The determinant of a product of square matrices equals the product of their determinants: 
Adding a multiple of any row to another row, or a multiple of any column to another column, does not change the determinant. Interchanging two rows or two columns affects the determinant by multiplying it by −1. Using these operations, any matrix can be transformed to a lower (or upper) triangular matrix, and for such matrices the determinant equals the product of the entries on the main diagonal; this provides a method to calculate the determinant of any matrix. Finally, the Laplace expansion expresses the determinant in terms of minors, i.e., determinants of smaller matrices. This expansion can be used for a recursive definition of determinants (taking as starting case the determinant of a 1-by-1 matrix, which is its unique entry, or even the determinant of a 0-by-0 matrix, which is 1), that can be seen to be equivalent to the Leibniz formula. Determinants can be used to solve linear systems using Cramer's rule, where the division of the determinants of two related square matrices equates to the value of each of the system's variables.
Eigenvalues and eigenvectors.
A number λ and a non-zero vector v satisfying
are called an "eigenvalue" and an "eigenvector" of A, respectively. The number λ is an eigenvalue of an "n"×"n"-matrix A if and only if A−λI"n" is not invertible, which is equivalent to
The polynomial "p"A in an indeterminate "X" given by evaluation the determinant det("X"I"n"−A) is called the characteristic polynomial of A. It is a monic polynomial of degree "n". Therefore the polynomial equation "p"A(λ) = 0 has at most "n" different solutions, i.e., eigenvalues of the matrix. They may be complex even if the entries of A are real. According to the Cayley–Hamilton theorem, "p"A(A) = 0, that is, the result of substituting the matrix itself into its own characteristic polynomial yields the zero matrix.
Computational aspects.
Matrix calculations can be often performed with different techniques. Many problems can be solved by both direct algorithms or iterative approaches. For example, the eigenvectors of a square matrix can be obtained by finding a sequence of vectors x"n" converging to an eigenvector when "n" tends to infinity.
To be able to choose the more appropriate algorithm for each specific problem, it is important to determine both the effectiveness and precision of all the available algorithms. The domain studying these matters is called numerical linear algebra. As with other numerical situations, two main aspects are the complexity of algorithms and their numerical stability.
Determining the complexity of an algorithm means finding upper bounds or estimates of how many elementary operations such as additions and multiplications of scalars are necessary to perform some algorithm, e.g., multiplication of matrices. For example, calculating the matrix product of two "n"-by-"n" matrix using the definition given above needs "n"3 multiplications, since for any of the "n"2 entries of the product, "n" multiplications are necessary. The Strassen algorithm outperforms this "naive" algorithm; it needs only "n"2.807 multiplications. A refined approach also incorporates specific features of the computing devices.
In many practical situations additional information about the matrices involved is known. An important case are sparse matrices, i.e., matrices most of whose entries are zero. There are specifically adapted algorithms for, say, solving linear systems Ax = b for sparse matrices A, such as the conjugate gradient method.
An algorithm is, roughly speaking, numerically stable, if little deviations in the input values do not lead to big deviations in the result. For example, calculating the inverse of a matrix via Laplace's formula (Adj (A) denotes the adjugate matrix of A)
may lead to significant rounding errors if the determinant of the matrix is very small. The norm of a matrix can be used to capture the conditioning of linear algebraic problems, such as computing a matrix's inverse.
Although most computer languages are not designed with commands or libraries for matrices, as early as the 1970s, some engineering desktop computers such as the HP 9830 had ROM cartridges to add BASIC commands for matrices. Some computer languages such as APL were designed to manipulate matrices, and various mathematical programs can be used to aid computing with matrices.
Decomposition.
There are several methods to render matrices into a more easily accessible form. They are generally referred to as "matrix decomposition" or "matrix factorization" techniques. The interest of all these techniques is that they preserve certain properties of the matrices in question, such as determinant, rank or inverse, so that these quantities can be calculated after applying the transformation, or that certain matrix operations are algorithmically easier to carry out for some types of matrices.
The LU decomposition factors matrices as a product of lower (L) and an upper triangular matrices (U). Once this decomposition is calculated, linear systems can be solved more efficiently, by a simple technique called forward and back substitution. Likewise, inverses of triangular matrices are algorithmically easier to calculate. The "Gaussian elimination" is a similar algorithm; it transforms any matrix to row echelon form. Both methods proceed by multiplying the matrix by suitable elementary matrices, which correspond to permuting rows or columns and adding multiples of one row to another row. Singular value decomposition expresses any matrix A as a product UDV∗, where U and V are unitary matrices and D is a diagonal matrix.
The eigendecomposition or "diagonalization" expresses A as a product VDV−1, where D is a diagonal matrix and V is a suitable invertible matrix. If A can be written in this form, it is called diagonalizable. More generally, and applicable to all matrices, the Jordan decomposition transforms a matrix into Jordan normal form, that is to say matrices whose only nonzero entries are the eigenvalues λ1 to λn of A, placed on the main diagonal and possibly entries equal to one directly above the main diagonal, as shown at the right. Given the eigendecomposition, the "n"th power of A (i.e., "n"-fold iterated matrix multiplication) can be calculated via
and the power of a diagonal matrix can be calculated by taking the corresponding powers of the diagonal entries, which is much easier than doing the exponentiation for A instead. This can be used to compute the matrix exponential "e"A, a need frequently arising in solving linear differential equations, matrix logarithms and square roots of matrices. To avoid numerically ill-conditioned situations, further algorithms such as the Schur decomposition can be employed.
Abstract algebraic aspects and generalizations.
Matrices can be generalized in different ways. Abstract algebra uses matrices with entries in more general fields or even rings, while linear algebra codifies properties of matrices in the notion of linear maps. It is possible to consider matrices with infinitely many columns and rows. Another extension are tensors, which can be seen as higher-dimensional arrays of numbers, as opposed to vectors, which can often be realised as sequences of numbers, while matrices are rectangular or two-dimensional arrays of numbers. Matrices, subject to certain requirements tend to form groups known as matrix groups.
Matrices with more general entries.
This article focuses on matrices whose entries are real or complex numbers. However, matrices can be considered with much more general types of entries than real or complex numbers. As a first step of generalization, any field, i.e., a set where addition, subtraction, multiplication and division operations are defined and well-behaved, may be used instead of R or C, for example rational numbers or finite fields. For example, coding theory makes use of matrices over finite fields. Wherever eigenvalues are considered, as these are roots of a polynomial they may exist only in a larger field than that of the entries of the matrix; for instance they may be complex in case of a matrix with real entries. The possibility to reinterpret the entries of a matrix as elements of a larger field (e.g., to view a real matrix as a complex matrix whose entries happen to be all real) then allows considering each square matrix to possess a full set of eigenvalues. Alternatively one can consider only matrices with entries in an algebraically closed field, such as C, from the outset.
More generally, abstract algebra makes great use of matrices with entries in a ring "R". Rings are a more general notion than fields in that a division operation need not exist. The very same addition and multiplication operations of matrices extend to this setting, too. The set M("n", "R") of all square "n"-by-"n" matrices over "R" is a ring called matrix ring, isomorphic to the endomorphism ring of the left "R"-module "R""n". If the ring "R" is commutative, i.e., its multiplication is commutative, then M("n", "R") is a unitary noncommutative (unless "n" = 1) associative algebra over "R". The determinant of square matrices over a commutative ring "R" can still be defined using the Leibniz formula; such a matrix is invertible if and only if its determinant is invertible in "R", generalising the situation over a field "F", where every nonzero element is invertible. Matrices over superrings are called supermatrices.
Matrices do not always have all their entries in the same ring – or even in any ring at all. One special but common case is block matrices, which may be considered as matrices whose entries themselves are matrices. The entries need not be quadratic matrices, and thus need not be members of any ordinary ring; but their sizes must fulfil certain compatibility conditions.
Relationship to linear maps.
Linear maps R"n" → R"m" are equivalent to "m"-by-"n" matrices, as described above. More generally, any linear map between finite-dimensional vector spaces can be described by a matrix A = ("a""ij"), after choosing bases v1, ..., v"n" of "V", and w1, ..., w"m" of "W" (so "n" is the dimension of "V" and "m" is the dimension of "W"), which is such that
In other words, column "j" of "A" expresses the image of v"j" in terms of the basis vectors w"i" of "W"; thus this relation uniquely determines the entries of the matrix A. Note that the matrix depends on the choice of the bases: different choices of bases give rise to different, but equivalent matrices. Many of the above concrete notions can be reinterpreted in this light, for example, the transpose matrix A"T" describes the transpose of the linear map given by A, with respect to the dual bases.
These properties can be restated in a more natural way: the category of all matrices with entries in a field formula_22 with multiplication as composition is equivalent to the category of finite dimensional vector spaces and linear maps over this field.
More generally, the set of "m"×"n" matrices can be used to represent the "R"-linear maps between the free modules "R""m" and "R""n" for an arbitrary ring "R" with unity. When "n" = "m" composition of these maps is possible, and this gives rise to the matrix ring of "n"×"n" matrices representing the endomorphism ring of "R""n".
Matrix groups.
A group is a mathematical structure consisting of a set of objects together with a binary operation, i.e., an operation combining any two objects to a third, subject to certain requirements. A group in which the objects are matrices and the group operation is matrix multiplication is called a "matrix group". Since in a group every element has to be invertible, the most general matrix groups are the groups of all invertible matrices of a given size, called the general linear groups.
Any property of matrices that is preserved under matrix products and inverses can be used to define further matrix groups. For example, matrices with a given size and with a determinant of 1 form a subgroup of (i.e., a smaller group contained in) their general linear group, called a special linear group. Orthogonal matrices, determined by the condition
form the orthogonal group. Every orthogonal matrix has determinant 1 or −1. Orthogonal matrices with determinant 1 form a subgroup called "special orthogonal group".
Every finite group is isomorphic to a matrix group, as one can see by considering the regular representation of the symmetric group. General groups can be studied using matrix groups, which are comparatively well-understood, by means of representation theory.
Infinite matrices.
It is also possible to consider matrices with infinitely many rows and/or columns even if, being infinite objects, one cannot write down such matrices explicitly. All that matters is that for every element in the set indexing rows, and every element in the set indexing columns, there is a well-defined entry (these index sets need not even be subsets of the natural numbers). The basic operations of addition, subtraction, scalar multiplication and transposition can still be defined without problem; however matrix multiplication may involve infinite summations to define the resulting entries, and these are not defined in general.
If "R" is any ring with unity, then the ring of endomorphisms of formula_23 as a right "R" module is isomorphic to the ring of column finite matrices formula_24 whose entries are indexed by formula_25, and whose columns each contain only finitely many nonzero entries. The endomorphisms of "M" considered as a left "R" module result in an analogous object, the row finite matrices formula_26 whose rows each only have finitely many nonzero entries.
If infinite matrices are used to describe linear maps, then only those matrices can be used all of whose columns have but a finite number of nonzero entries, for the following reason. For a matrix A to describe a linear map "f": "V"→"W", bases for both spaces must have been chosen; recall that by definition this means that every vector in the space can be written uniquely as a (finite) linear combination of basis vectors, so that written as a (column) vector "v" of coefficients, only finitely many entries "v""i" are nonzero. Now the columns of A describe the images by "f" of individual basis vectors of "V" in the basis of "W", which is only meaningful if these columns have only finitely many nonzero entries. There is no restriction on the rows of "A" however: in the product A·"v" there are only finitely many nonzero coefficients of "v" involved, so every one of its entries, even if it is given as an infinite sum of products, involves only finitely many nonzero terms and is therefore well defined. Moreover, this amounts to forming a linear combination of the columns of A that effectively involves only finitely many of them, whence the result has only finitely many nonzero entries, because each of those columns do. One also sees that products of two matrices of the given type is well defined (provided as usual that the column-index and row-index sets match), is again of the same type, and corresponds to the composition of linear maps.
If "R" is a normed ring, then the condition of row or column finiteness can be relaxed. With the norm in place, absolutely convergent series can be used instead of finite sums. For example, the matrices whose column sums are absolutely convergent sequences form a ring. Analogously of course, the matrices whose row sums are absolutely convergent series also form a ring.
In that vein, infinite matrices can also be used to describe operators on Hilbert spaces, where convergence and continuity questions arise, which again results in certain constraints that have to be imposed. However, the explicit point of view of matrices tends to obfuscate the matter, and the abstract and more powerful tools of functional analysis can be used instead.
Empty matrices.
An "empty matrix" is a matrix in which the number of rows or columns (or both) is zero. Empty matrices help dealing with maps involving the zero vector space. For example, if "A" is a 3-by-0 matrix and "B" is a 0-by-3 matrix, then "AB" is the 3-by-3 zero matrix corresponding to the null map from a 3-dimensional space "V" to itself, while "BA" is a 0-by-0 matrix. There is no common notation for empty matrices, but most computer algebra systems allow creating and computing with them. The determinant of the 0-by-0 matrix is 1 as follows from regarding the empty product occurring in the Leibniz formula for the determinant as 1. This value is also consistent with the fact that the identity map from any finite dimensional space to itself has determinant 1, a fact that is often used as a part of the characterization of determinants.
Applications.
There are numerous applications of matrices, both in mathematics and other sciences. Some of them merely take advantage of the compact representation of a set of numbers in a matrix. For example, in game theory and economics, the payoff matrix encodes the payoff for two players, depending on which out of a given (finite) set of alternatives the players choose. Text mining and automated thesaurus compilation makes use of document-term matrices such as tf-idf to track frequencies of certain words in several documents.
Complex numbers can be represented by particular real 2-by-2 matrices via
under which addition and multiplication of complex numbers and matrices correspond to each other. For example, 2-by-2 rotation matrices represent the multiplication with some complex number of absolute value 1, as above. A similar interpretation is possible for quaternions and Clifford algebras in general.
Early encryption techniques such as the Hill cipher also used matrices. However, due to the linear nature of matrices, these codes are comparatively easy to break. Computer graphics uses matrices both to represent objects and to calculate transformations of objects using affine rotation matrices to accomplish tasks such as projecting a three-dimensional object onto a two-dimensional screen, corresponding to a theoretical camera observation. Matrices over a polynomial ring are important in the study of control theory.
Chemistry makes use of matrices in various ways, particularly since the use of quantum theory to discuss molecular bonding and spectroscopy. Examples are the overlap matrix and the Fock matrix used in solving the Roothaan equations to obtain the molecular orbitals of the Hartree–Fock method.
Graph theory.
The adjacency matrix of a finite graph is a basic notion of graph theory. It records which vertices of the graph are connected by an edge. Matrices containing just two different values (1 and 0 meaning for example "yes" and "no", respectively) are called logical matrices. The distance (or cost) matrix contains information about distances of the edges. These concepts can be applied to websites connected by hyperlinks or cities connected by roads etc., in which case (unless the connection network is extremely dense) the matrices tend to be sparse, i.e., contain few nonzero entries. Therefore, specifically tailored matrix algorithms can be used in network theory.
Analysis and geometry.
The Hessian matrix of a differentiable function "ƒ": R"n" → R consists of the second derivatives of "ƒ" with respect to the several coordinate directions, i.e.
It encodes information about the local growth behaviour of the function: given a critical point x = ("x"1, ..., "x""n"), i.e., a point where the first partial derivatives formula_29 of "ƒ" vanish, the function has a local minimum if the Hessian matrix is positive definite. Quadratic programming can be used to find global minima or maxima of quadratic functions closely related to the ones attached to matrices (see above).
Another matrix frequently used in geometrical situations is the Jacobi matrix of a differentiable map "f": R"n" → R"m". If "f"1, ..., "f""m" denote the components of "f", then the Jacobi matrix is defined as 
If "n" > "m", and if the rank of the Jacobi matrix attains its maximal value "m", "f" is locally invertible at that point, by the implicit function theorem.
Partial differential equations can be classified by considering the matrix of coefficients of the highest-order differential operators of the equation. For elliptic partial differential equations this matrix is positive definite, which has decisive influence on the set of possible solutions of the equation in question.
The finite element method is an important numerical method to solve partial differential equations, widely applied in simulating complex physical systems. It attempts to approximate the solution to some equation by piecewise linear functions, where the pieces are chosen with respect to a sufficiently fine grid, which in turn can be recast as a matrix equation.
Probability theory and statistics.
Stochastic matrices are square matrices whose rows are probability vectors, i.e., whose entries are non-negative and sum up to one. Stochastic matrices are used to define Markov chains with finitely many states. A row of the stochastic matrix gives the probability distribution for the next position of some particle currently in the state that corresponds to the row. Properties of the Markov chain like absorbing states, i.e., states that any particle attains eventually, can be read off the eigenvectors of the transition matrices.
Statistics also makes use of matrices in many different forms. Descriptive statistics is concerned with describing data sets, which can often be represented as data matrices, which may then be subjected to dimensionality reduction techniques. The covariance matrix encodes the mutual variance of several random variables. Another technique using matrices are linear least squares, a method that approximates a finite set of pairs ("x"1, "y"1), ("x"2, "y"2), ..., ("x""N", "y""N"), by a linear function
which can be formulated in terms of matrices, related to the singular value decomposition of matrices.
Random matrices are matrices whose entries are random numbers, subject to suitable probability distributions, such as matrix normal distribution. Beyond probability theory, they are applied in domains ranging from number theory to physics.
Symmetries and transformations in physics.
Linear transformations and the associated symmetries play a key role in modern physics. For example, elementary particles in quantum field theory are classified as representations of the Lorentz group of special relativity and, more specifically, by their behavior under the spin group. Concrete representations involving the Pauli matrices and more general gamma matrices are an integral part of the physical description of fermions, which behave as spinors. For the three lightest quarks, there is a group-theoretical representation involving the special unitary group SU(3); for their calculations, physicists use a convenient matrix representation known as the Gell-Mann matrices, which are also used for the SU(3) gauge group that forms the basis of the modern description of strong nuclear interactions, quantum chromodynamics. The Cabibbo–Kobayashi–Maskawa matrix, in turn, expresses the fact that the basic quark states that are important for weak interactions are not the same as, but linearly related to the basic quark states that define particles with specific and distinct masses.
Linear combinations of quantum states.
The first model of quantum mechanics (Heisenberg, 1925) represented the theory's operators by infinite-dimensional matrices acting on quantum states. This is also referred to as matrix mechanics. One particular example is the density matrix that characterizes the "mixed" state of a quantum system as a linear combination of elementary, "pure" eigenstates.
Another matrix serves as a key tool for describing the scattering experiments that form the cornerstone of experimental particle physics: Collision reactions such as occur in particle accelerators, where non-interacting particles head towards each other and collide in a small interaction zone, with a new set of non-interacting particles as the result, can be described as the scalar product of outgoing particle states and a linear combination of ingoing particle states. The linear combination is given by a matrix known as the S-matrix, which encodes all information about the possible interactions between particles.
Normal modes.
A general application of matrices in physics is to the description of linearly coupled harmonic systems. The equations of motion of such systems can be described in matrix form, with a mass matrix multiplying a generalized velocity to give the kinetic term, and a force matrix multiplying a displacement vector to characterize the interactions. The best way to obtain solutions is to determine the system's eigenvectors, its normal modes, by diagonalizing the matrix equation. Techniques like this are crucial when it comes to the internal dynamics of molecules: the internal vibrations of systems consisting of mutually bound component atoms. They are also needed for describing mechanical vibrations, and oscillations in electrical circuits.
Geometrical optics.
Geometrical optics provides further matrix applications. In this approximative theory, the wave nature of light is neglected. The result is a model in which light rays are indeed geometrical rays. If the deflection of light rays by optical elements is small, the action of a lens or reflective element on a given light ray can be expressed as multiplication of a two-component vector with a two-by-two matrix called ray transfer matrix: the vector's components are the light ray's slope and its distance from the optical axis, while the matrix encodes the properties of the optical element. Actually, there are two kinds of matrices, viz. a "refraction matrix" describing the refraction at a lens surface, and a "translation matrix", describing the translation of the plane of reference to the next refracting surface, where another refraction matrix applies.
The optical system, consisting of a combination of lenses and/or reflective elements, is simply described by the matrix resulting from the product of the components' matrices.
Electronics.
Traditional mesh analysis in electronics leads to a system of linear equations that can be described with a matrix.
The behaviour of many electronic components can be described using matrices. Let "A" be a 2-dimensional vector with the component's input voltage "v"1 and input current "i"1 as its elements, and let "B" be a 2-dimensional vector with the component's output voltage "v"2 and output current "i"2 as its elements. Then the behaviour of the electronic component can be described by "B" = "H" · "A", where "H" is a 2 x 2 matrix containing one impedance element ("h"12), one admittance element ("h"21) and two dimensionless elements ("h"11 and "h"22). Calculating a circuit now reduces to multiplying matrices.
History.
Matrices have a long history of application in solving linear equations but they were known as arrays until the 1800s. The Chinese text "The Nine Chapters on the Mathematical Art" written in 10th–2nd century BCE is the first example of the use of array methods to solve simultaneous equations, including the concept of determinants. In 1545 Italian mathematician Girolamo Cardano brought the method to Europe when he published "Ars Magna". The Japanese mathematician Seki used the same array methods to solve simultaneous equations in 1683. The Dutch Mathematician" "Jan de Witt represented transformations using arrays in his 1659 book "Elements of Curves" (1659). Between 1700 and 1710 Gottfried Wilhelm Leibniz publicized the use of arrays for recording information or solutions and experimented with over 50 different systems of arrays. Cramer presented his rule in 1750.
The term "matrix" (Latin for "womb", derived from "mater"—mother) was coined by James Joseph Sylvester in 1850, who understood a matrix as an object giving rise to a number of determinants today called minors, that is to say, determinants of smaller matrices that derive from the original one by removing columns and rows. In an 1851 paper, Sylvester explains:
Arthur Cayley published a treatise on geometric transformations using matrices that were not rotated versions of the coefficients being investigated as had previously been done. Instead he defined operations such as addition, subtraction, multiplication, and division as transformations of those matrices and showed the associative and distributive properties held true. Cayley investigated and demonstrated the non-commutative property of matrix multiplication as well as the commutative property of matrix addition. Early matrix theory had limited the use of arrays almost exclusively to determinants and Arthur Cayley's abstract matrix operations were revolutionary. He was instrumental in proposing a matrix concept independent of equation systems. In 1858 Cayley published his "Memoir on the theory of matrices" in which he proposed and demonstrated the Cayley-Hamilton theorem.
An English mathematician named Cullis was the first to use modern bracket notation for matrices in 1913 and he simultaneously demonstrated the first significant use of the notation A = ["a""i","j"] to represent a matrix where "a""i","j" refers to the" i"th row and the "j"th column.
The study of determinants sprang from several sources. Number-theoretical problems led Gauss to relate coefficients of quadratic forms, i.e., expressions such as and linear maps in three dimensions to matrices. Eisenstein further developed these notions, including the remark that, in modern parlance, matrix products are non-commutative. Cauchy was the first to prove general statements about determinants, using as definition of the determinant of a matrix A = ["a""i","j"] the following: replace the powers "a""j""k" by "a""jk" in the polynomial
where Π denotes the product of the indicated terms. He also showed, in 1829, that the eigenvalues of symmetric matrices are real. Jacobi studied "functional determinants"—later called Jacobi determinants by Sylvester—which can be used to describe geometric transformations at a local (or infinitesimal) level, see above; Kronecker's "Vorlesungen über die Theorie der Determinanten" and Weierstrass' "Zur Determinantentheorie", both published in 1903, first treated determinants axiomatically, as opposed to previous more concrete approaches such as the mentioned formula of Cauchy. At that point, determinants were firmly established.
Many theorems were first established for small matrices only, for example the Cayley–Hamilton theorem was proved for 2×2 matrices by Cayley in the aforementioned memoir, and by Hamilton for 4×4 matrices. Frobenius, working on bilinear forms, generalized the theorem to all dimensions (1898). Also at the end of the 19th century the Gauss–Jordan elimination (generalizing a special case now known as Gauss elimination) was established by Jordan. In the early 20th century, matrices attained a central role in linear algebra. partially due to their use in classification of the hypercomplex number systems of the previous century.
The inception of matrix mechanics by Heisenberg, Born and Jordan led to studying matrices with infinitely many rows and columns. Later, von Neumann carried out the mathematical formulation of quantum mechanics, by further developing functional analytic notions such as linear operators on Hilbert spaces, which, very roughly speaking, correspond to Euclidean space, but with an infinity of independent directions.
Other historical usages of the word “matrix” in mathematics.
The word has been used in unusual ways by at least two authors of historical importance.
Bertrand Russell and Alfred North Whitehead in their "Principia Mathematica" (1910–1913) use the word “matrix” in the context of their Axiom of reducibility. They proposed this axiom as a means to reduce any function to one of lower type, successively, so that at the “bottom” (0 order) the function is identical to its extension:
For example, a function Φ("x, y") of two variables "x" and "y" can be reduced to a "collection" of functions of a single variable, e.g., "y", by “considering” the function for all possible values of “individuals” "ai" substituted in place of variable "x". And then the resulting collection of functions of the single variable "y", i.e., ∀ai: Φ("ai, y"), can be reduced to a “matrix” of values by “considering” the function for all possible values of “individuals” "bi" substituted in place of variable "y":
Alfred Tarski in his 1946 "Introduction to Logic" used the word “matrix” synonymously with the notion of truth table as used in mathematical logic.

</doc>
<doc id="125276" url="https://en.wikipedia.org/wiki?curid=125276" title="Matrix addition">
Matrix addition

In mathematics, matrix addition is the operation of adding two matrices by adding the corresponding entries together. However, there are other operations which could also be considered as a kind of addition for matrices, the direct sum and the Kronecker sum.
Entrywise sum.
Two matrices must have an equal number of rows and columns to be added. The sum of two matrices A and B will be a matrix which has the same number of rows and columns as do A and B. The sum of A and B, denoted A + B, is computed by adding corresponding elements of A and B:
For example: 
We can also subtract one matrix from another, as long as they have the same dimensions. A − B is computed by subtracting corresponding elements of A and B, and has the same dimensions as A and B. For example:
Direct sum.
Another operation, which is used less often, is the direct sum (denoted by ⊕). Note the Kronecker sum is also denoted ⊕; the context should make the usage clear. The direct sum of any pair of matrices A of size "m" × "n" and B of size "p" × "q" is a matrix of size ("m" + "p") × ("n" + "q") defined as 
For instance,
The direct sum of matrices is a special type of block matrix, in particular the direct sum of square matrices is a block diagonal matrix.
The adjacency matrix of the union of disjoint graphs or multigraphs is the direct sum of their adjacency matrices. Any element in the direct sum of two vector spaces of matrices can be represented as a direct sum of two matrices.
In general, the direct sum of "n" matrices is:
where the zeros are actually blocks of zeros, i.e. zero matricies.
Kronecker sum.
The Kronecker sum is different from the direct sum but is also denoted by ⊕. It is defined using the Kronecker product ⊗ and normal matrix addition. If A is "n"-by-"n", B is "m"-by-"m" and formula_7 denotes the "k"-by-"k" identity matrix then the Kronecker sum is defined by:

</doc>
<doc id="1765852" url="https://en.wikipedia.org/wiki?curid=1765852" title="Matrix calculus">
Matrix calculus

In mathematics, matrix calculus is a specialized notation for doing multivariable calculus, especially over spaces of matrices. It collects the various partial derivatives of a single function with respect to many variables, and/or of a multivariate function with respect to a single variable, into vectors and matrices that can be treated as single entities. This greatly simplifies operations such as finding the maximum or minimum of a multivariate function and solving systems of differential equations. The notation used here is commonly used in statistics and engineering, while the tensor index notation is preferred in physics.
Two competing notational conventions split the field of matrix calculus into two separate groups. The two groups can be distinguished by whether they write the derivative of a scalar with respect to a vector as a column vector or a row vector. Both of these conventions are possible even when the common assumption is made that vectors should be treated as column vectors when combined with matrices (rather than row vectors). A single convention can be somewhat standard throughout a single field that commonly use matrix calculus (e.g. econometrics, statistics, estimation theory and machine learning). However, even within a given field different authors can be found using competing conventions. Authors of both groups often write as though their specific convention is standard. Serious mistakes can result when combining results from different authors without carefully verifying that compatible notations are used. Therefore, great care should be taken to ensure notational consistency. Definitions of these two conventions and comparisons between them are collected in the layout conventions section.
Scope.
Matrix calculus refers to a number of different notations that use matrices and vectors to collect the derivative of each component of the dependent variable with respect to each component of the independent variable. In general, the independent variable can be a scalar, a vector, or a matrix while the dependent variable can be any of these as well. Each different situation will lead to a different set of rules, or a separate calculus, using the broader sense of the term. Matrix notation serves as a convenient way to collect the many derivatives in an organized way.
As a first example, consider the gradient from vector calculus. For a scalar function of three independent variables, formula_1, the gradient is given by the vector equation
where formula_3 represents a unit vector in the formula_4 direction for formula_5. This type of generalized derivative can be seen as the derivative of a scalar, "f", with respect to a vector, formula_6 and its result can be easily collected in vector form.
More complicated examples include the derivative of a scalar function with respect to a matrix, known as the gradient matrix, which collects the derivative with respect to each matrix element in the corresponding position in the resulting matrix. In that case the scalar must be a function of each of the independent variables in the matrix. As another example, if we have an "n"-vector of dependent variables, or functions, of "m" independent variables we might consider the derivative of the dependent vector with respect to the independent vector. The result could be collected in an "m×n" matrix consisting of all of the possible derivative combinations. There are, of course, a total of nine possibilities using scalars, vectors, and matrices. Notice that as we consider higher numbers of components in each of the independent and dependent variables we can be left with a very large number of possibilities.
The six kinds of derivatives that can be most neatly organized in matrix form are collected in the following table.
Here, we have used the term "matrix" in its most general sense, recognizing that vectors and scalars are simply matrices with one column and then one row respectively. Moreover, we have used bold letters to indicate vectors and bold capital letters for matrices. This notation is used throughout.
Notice that we could also talk about the derivative of a vector with respect to a matrix, or any of the other unfilled cells in our table. However, these derivatives are most naturally organized in a tensor of rank higher than 2, so that they do not fit neatly into a matrix. In the following three sections we will define each one of these derivatives and relate them to other branches of mathematics. See the layout conventions section for a more detailed table.
Relation to other derivatives.
The matrix derivative is a convenient notation for keeping track of partial derivatives for doing calculations. The Fréchet derivative is the standard way in the setting of functional analysis to take derivatives with respect to vectors. In the case that a matrix function of a matrix is Fréchet differentiable, the two derivatives will agree up to translation of notations. As is the case in general for partial derivatives, some formulae may extend under weaker analytic conditions than the existence of the derivative as approximating linear mapping.
Usages.
Matrix calculus is used for deriving optimal stochastic estimators, often involving the use of Lagrange multipliers. This includes the derivation of:
Notation.
The vector and matrix derivatives presented in the sections to follow take full advantage of matrix notation, using a single variable to represent a large number of variables. In what follows we will distinguish scalars, vectors and matrices by their typeface. We will let "M"("n","m") denote the space of real "n×m" matrices with "n" rows and "m" columns. Such matrices will be denoted using bold capital letters: A, X, Y, etc. An element of "M"("n",1), that is, a column vector, is denoted with a boldface lowercase letter: a, x, y, etc. An element of "M"(1,1) is a scalar, denoted with lowercase italic typeface: "a", "t", "x", etc. XT denotes matrix transpose, tr(X) is the trace, and det(X) is the determinant. All functions are assumed to be of differentiability class "C"1 unless otherwise noted. Generally letters from first half of the alphabet (a, b, c, …) will be used to denote constants, and from the second half (t, x, y, …) to denote variables.
NOTE: As mentioned above, there are competing notations for laying out systems of partial derivatives in vectors and matrices, and no standard appears to be emerging yet. The next two introductory sections use the numerator layout convention simply for the purposes of convenience, to avoid overly complicating the discussion. The section after them discusses layout conventions in more detail. It is important to realize the following:
Alternatives.
The tensor index notation with its Einstein summation convention is very similar to the matrix calculus, except one writes only a single component at a time. It has the advantage that one can easily manipulate arbitrarily high rank tensors, whereas tensors of rank higher than two are quite unwieldy with matrix notation. All of the work here can be done in this notation without use of the single-variable matrix notation. However, many problems in estimation theory and other areas of applied mathematics would result in too many indices to properly keep track of, pointing in favor of matrix calculus in those areas. Also, Einstein notation can be very useful in proving the identities presented here, as an alternative to typical element notation, which can become cumbersome when the explicit sums are carried around. Note that a matrix can be considered a tensor of rank two.
Derivatives with vectors.
Because vectors are matrices with only one column, the simplest matrix derivatives are vector derivatives.
The notations developed here can accommodate the usual operations of vector calculus by identifying the space "M"("n",1) of "n"-vectors with the Euclidean space R"n", and the scalar "M"(1,1) is identified with R. The corresponding concept from vector calculus is indicated at the end of each subsection.
NOTE: The discussion in this section assumes the numerator layout convention for pedagogical purposes. Some authors use different conventions. The section on layout conventions discusses this issue in greater detail. The identities given further down are presented in forms that can be used in conjunction with all common layout conventions.
Vector-by-scalar.
The derivative of a vector 
formula_8,
by a scalar "x" is written (in numerator layout notation) as
In vector calculus the derivative of a vector y with respect to a scalar "x" is known as the tangent vector of the vector y, formula_10. Notice here that y:R"m" formula_11 R"m".
Example Simple examples of this include the velocity vector in Euclidean space, which is the tangent vector of the position vector (considered as a function of time). Also, the acceleration is the tangent vector of the velocity.
Scalar-by-vector.
The derivative of a scalar "y" by a vector 
formula_12,
is written (in numerator layout notation) as
In vector calculus, the gradient of a scalar field "y" in the space R"n" (whose independent coordinates are the components of x) is the derivative of a scalar by a vector. In physics, the electric field is the vector gradient of the electric potential.
The directional derivative of a scalar function "f"(x) of the space vector x in the direction of the unit vector u is defined using the gradient as follows.
Using the notation just defined for the derivative of a scalar with respect to a vector we can re-write the directional derivative as
formula_15
This type of notation will be nice when proving product rules and chain rules that come out looking similar to what we are familiar with for the scalar derivative.
Vector-by-vector.
Each of the previous two cases can be considered as an application of the derivative of a vector with respect to a vector, using a vector of size one appropriately. Similarly we will find that the derivatives involving matrices will reduce to derivatives involving vectors in a corresponding way.
The derivative of a vector function (a vector whose components are functions)
formula_8, 
with respect to an input vector,
formula_12,
is written (in numerator layout notation) as
In vector calculus, the derivative of a vector function y with respect to a vector x whose components represent a space is known as the pushforward or differential, or the Jacobian matrix.
The pushforward along a vector function f with respect to vector v in R"m" is given by
formula_19
Derivatives with matrices.
There are two types of derivatives with matrices that can be organized into a matrix of the same size. These are the derivative of a matrix by a scalar and the derivative of a scalar by a matrix respectively. These can be useful in minimization problems found many areas of applied mathematics and have adopted the names tangent matrix and gradient matrix respectively after their analogs for vectors.
NOTE: The discussion in this section assumes the numerator layout convention for pedagogical purposes. Some authors use different conventions. The section on layout conventions discusses this issue in greater detail. The identities given further down are presented in forms that can be used in conjunction with all common layout conventions.
Matrix-by-scalar.
The derivative of a matrix function Y by a scalar "x" is known as the tangent matrix and is given (in numerator layout notation) by
Scalar-by-matrix.
The derivative of a scalar "y" function of a matrix X of independent variables, with respect to the matrix X, is given (in numerator layout notation) by
Notice that the indexing of the gradient with respect to X is transposed as compared with the indexing of X. Important examples of scalar functions of matrices include the trace of a matrix and the determinant.
In analog with vector calculus this derivative is often written as the following.
Also in analog with vector calculus, the directional derivative of a scalar "f"(X) of a matrix X in the direction of matrix Y is given by
It is the gradient matrix, in particular, that finds many uses in minimization problems in estimation theory, particularly in the derivation of the Kalman filter algorithm, which is of great importance in the field.
Other matrix derivatives.
The three types of derivatives that have not been considered are those involving vectors-by-matrices, matrices-by-vectors, and matrices-by-matrices. These are not as widely considered and a notation is not widely agreed upon. As for vectors, the other two types of higher matrix derivatives can be seen as applications of the derivative of a matrix by a matrix by using a matrix with one column in the correct place. For this reason, in this subsection we consider only how one can write the derivative of a matrix by another matrix.
The differential or the matrix derivative of a matrix function F(X) that maps from "n"×"m" matrices to "p"×"q" matrices, F : "M"("n","m") formula_24 "M"("p","q"), is an element of "M"("p","q") ? "M"("m","n"), a fourth-rank tensor (the reversal of "m" and "n" here indicates the dual space of "M"("n","m")). In short it is an "m"×"n" matrix each of whose entries is a "p"×"q" matrix.
and note that each formula_26 is a "p"×"q" matrix defined as above. Note also that this matrix has its indexing transposed; "m" rows and "n" columns. The pushforward along F of an "n"×"m" matrix Y in "M"("n","m") is then
Note that this definition encompasses all of the preceding definitions as special cases.
According to Jan R. Magnus and Heinz Neudecker, the following notations are both unsuitable, as the determinant of the second resulting matrix would have "no interpretation" and "a useful chain rule does not exist" if these notations are being used:
The Jacobian matrix, according to Magnus and Neudecker, is
Layout conventions.
This section discusses the similarities and differences between notational conventions that are used in the various fields that take advantage of matrix calculus. Although there are largely two consistent conventions, some authors find it convenient to mix the two conventions in forms that are discussed below. After this section equations will be listed in both competing forms separately.
The fundamental issue is that the derivative of a vector with respect to a vector, i.e. formula_38, is often written in two competing ways. If the numerator y is of size "m" and the denominator x of size "n", then the result can be laid out as either an "m×n" matrix or "n×m" matrix, i.e. the elements of y laid out in columns and the elements of x laid out in rows, or vice versa. This leads to the following possibilities:
When handling the gradient formula_41 and the opposite case formula_42 we have the same issues. To be consistent, we should do one of the following:
Not all math textbooks and papers are consistent in this respect throughout the entire paper. That is, sometimes different conventions are used in different contexts within the same paper. For example, some choose denominator layout for gradients (laying them out as column vectors), but numerator layout for the vector-by-vector derivative formula_51
Similarly, when it comes to scalar-by-matrix derivatives formula_52 and matrix-by-scalar derivatives formula_53 then consistent numerator layout lays out according to Y and XT, while consistent denominator layout lays out according to YT and X. In practice, however, following a denominator layout for formula_53 and laying the result out according to YT, is rarely seen because it makes for ugly formulas that do not correspond to the scalar formulas. As a result, the following layouts can often be found:
In the following formulas, we handle the five possible combinations formula_60 and formula_55 separately. We also handle cases of scalar-by-scalar derivatives that involve an intermediate vector or matrix. (This can arise, for example, if a multi-dimensional parametric curve is defined in terms of a scalar variable, and then a derivative of a scalar function of the curve is taken with respect to the scalar that parameterizes the curve.) For each of the various combinations, we give numerator-layout and denominator-layout results, except in the cases above where denominator layout rarely occurs. In cases involving matrices where it makes sense, we give numerator-layout and mixed-layout results. As noted above, cases where vector and matrix denominators are written in transpose notation are equivalent to numerator layout with the denominators written without the transpose.
Keep in mind that various authors use different combinations of numerator and denominator layouts for different types of derivatives, and there is no guarantee that an author will consistently use either numerator or denominator layout for all types. Match up the formulas below with those quoted in the source to determine the layout used for that particular type of derivative, but be careful not to assume that derivatives of other types necessarily follow the same kind of layout.
When taking derivatives with an aggregate (vector or matrix) denominator in order to find a maximum or minimum of the aggregate, it should be kept in mind that using numerator layout will produce results that are transposed with respect to the aggregate. For example, in attempting to find the maximum likelihood estimate of a multivariate normal distribution using matrix calculus, if the domain is a "k"x1 column vector, then the result using the numerator layout will be in the form of a 1x"k" row vector. Thus, either the results should be transposed at the end or the denominator layout (or mixed layout) should be used.
The results of operations will be transposed when switching between numerator-layout and denominator-layout notation.
Numerator-layout notation.
Using numerator-layout notation, we have:
The following definitions are only provided in numerator-layout notation:
Denominator-layout notation.
Using denominator-layout notation, we have:
Identities.
As noted above, in general, the results of operations will be transposed when switching between numerator-layout and denominator-layout notation.
To help make sense of all the identities below, keep in mind the most important rules: the chain rule, product rule and sum rule. The sum rule applies universally, and the product rule applies in most of the cases below, provided that the order of matrix products is maintained, since matrix products are not commutative. The chain rule applies in some of the cases, but unfortunately does "not" apply in matrix-by-scalar derivatives or scalar-by-matrix derivatives (in the latter case, mostly involving the trace operator applied to matrices). In the latter case, the product rule can't quite be applied directly, either, but the equivalent can be done with a bit more work using the differential identities.
Vector-by-vector identities.
This is presented first because all of the operations that apply to vector-by-vector differentiation apply directly to vector-by-scalar or scalar-by-vector differentiation simply by reducing the appropriate vector in the numerator or denominator to a scalar.
Scalar-by-vector identities.
The fundamental identities are placed above the thick black line.
Vector-by-scalar identities.
NOTE: The formulas involving the vector-by-vector derivatives formula_74 and formula_75 (whose outputs are matrices) assume the matrices are laid out consistent with the vector layout, i.e. numerator-layout matrix when numerator-layout vector and vice versa; otherwise, transpose the vector-by-vector derivatives.
Scalar-by-matrix identities.
Note that exact equivalents of the scalar product rule and chain rule do not exist when applied to matrix-valued functions of matrices. However, the product rule of this sort does apply to the differential form (see below), and this is the way to derive many of the identities below involving the trace function, combined with the fact that the trace function allows transposing and cyclic permutation, i.e.:
For example, to compute formula_78
Therefore,
 || formula_84 ||formula_85
Matrix-by-scalar identities.
Further see Derivative of the exponential map.
Identities in differential form.
It is often easier to work in differential form and then convert back to normal derivatives. This only works well using the numerator layout. In these rules, "a" is a scalar.
To convert to normal derivative form, first convert it to one of the following canonical forms, and then use these identities:

</doc>
<doc id="2383040" url="https://en.wikipedia.org/wiki?curid=2383040" title="Matrix congruence">
Matrix congruence

In mathematics, two matrices A and B over a field are called congruent if there exists an invertible matrix P over the same field such that 
where "T" denotes the matrix transpose. Matrix congruence is an equivalence relation.
Matrix congruence arises when considering the effect of change of basis on the Gram matrix attached to a bilinear form or quadratic form on a finite-dimensional vector space: two matrices are congruent if and only if they represent the same bilinear form with respect to different bases.
Note that Halmos defines congruence in terms of conjugate transpose (with respect to a complex inner product space) rather than transpose, but this definition has not been adopted by most other authors.
Congruence over the reals.
Sylvester's law of inertia states that two congruent symmetric matrices with real entries have the same numbers of positive, negative, and zero eigenvalues. That is, the number of eigenvalues of each sign is an invariant of the associated quadratic form.

</doc>
<doc id="253873" url="https://en.wikipedia.org/wiki?curid=253873" title="Matrix decomposition">
Matrix decomposition

In the mathematical discipline of linear algebra, a matrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices. There are many different matrix decompositions; each finds use among a particular class of problems.
Example.
In numerical analysis, different decompositions are used to implement efficient matrix algorithms.
For instance, when solving a system of linear equations formula_1, the matrix "A" can be decomposed via the LU decomposition. The LU decomposition factorizes a matrix into a lower triangular matrix "L" and an upper triangular matrix "U". The systems formula_2 and formula_3 require fewer additions and multiplications to solve, compared with the original system formula_1, though one might require significantly more digits in inexact arithmetic such as floating point.
Similarly, the QR decomposition expresses "A" as "QR" with "Q" an orthogonal matrix and "R" an upper triangular matrix. The system "Q"("Rx") = "b" is solved by "Rx" = "Q"T"b" = "c", and the system "Rx" = "c" is solved by 'back substitution'. The number of additions and multiplications required is about twice that of using the LU solver, but no more digits are required in inexact arithmetic because the QR decomposition is numerically stable.
Decompositions based on eigenvalues and related concepts.
Jordan decomposition.
The Jordan normal form and the Jordan–Chevalley decomposition
Generalizations.
There exist analogues of the SVD, QR, LU and Cholesky factorizations for quasimatrices and cmatrices or continuous matrices. A ‘quasimatrix’ is, like a matrix, a rectangular scheme whose elements are indexed, but one discrete index is replaced by a continuous index. Likewise, a ‘cmatrix’, is continuous in both indices. As an example of a cmatrix, one can think of the kernel of an integral operator.
These factorizations are based on early work by , and . For an account, and a translation to English of the seminal papers, see .

</doc>
<doc id="7838569" url="https://en.wikipedia.org/wiki?curid=7838569" title="Matrix equivalence">
Matrix equivalence

In linear algebra, two rectangular "m"-by-"n" matrices "A" and "B" are called equivalent if
for some invertible "n"-by-"n" matrix "P" and some invertible "m"-by-"m" matrix "Q". Equivalent matrices represent the same linear transformation "V" → "W" under two different choices of a pair of bases of "V" and "W", with "P" and "Q" being the change of basis matrices in "V" and "W" respectively.
The notion of equivalence should not be confused with that of similarity, which is only defined for square matrices, and is much more restrictive (similar matrices are certainly equivalent, but equivalent square matrices need not be similar). That notion corresponds to matrices representing the same endomorphism "V" → "V" under two different choices of a "single" basis of "V", used both for initial vectors and their images.
Properties.
Matrix equivalence is an equivalence relation on the space of rectangular matrices.
For two rectangular matrices of the same size, their equivalence can also be characterized by the following conditions
Because the two matrices can be transformed into each other by elementary row operations, if two matrices are equivalent, they share the same row space.

</doc>
<doc id="672731" url="https://en.wikipedia.org/wiki?curid=672731" title="Matrix exponential">
Matrix exponential

In mathematics, the matrix exponential is a matrix function on square matrices analogous to the ordinary exponential function. Abstractly, the matrix exponential gives the connection between a matrix Lie algebra and the corresponding Lie group.
Let be an real or complex matrix. The exponential of , denoted by or , is the matrix given by the power series
The above series always converges, so the exponential of is well-defined. If is a 1×1 matrix the matrix exponential of is a 1×1 matrix whose single element is the ordinary exponential of the single element of .
Properties.
Let and be complex matrices and let and be arbitrary complex numbers. We denote the identity matrix by and the zero matrix by 0. The matrix exponential satisfies the following properties:
Linear differential equation systems.
One of the reasons for the importance of the matrix exponential is that it can be used to solve systems of linear ordinary differential equations. The solution of
where is a constant matrix, is given by
The matrix exponential can also be used to solve the inhomogeneous equation
See the section on applications below for examples.
There is no closed-form solution for differential equations of the form
where is not constant, but the Magnus series gives the solution as an infinite sum.
The exponential of sums.
For any real numbers (scalars) and we know that the exponential function satisfies . The same is true for commuting matrices. If matrices and commute (meaning that ), then
However, for matrices that do not commute the above equality does not necessarily hold. In this case the Baker–Campbell–Hausdorff formula can be used to calculate .
The converse is not true in general. The equation does not imply that and commute.
For Hermitian matrices there are two notable theorems related to the trace of matrix exponentials.
Golden–Thompson inequality.
If and are Hermitian matrices, then
Note that there is no requirement of commutativity. There are counterexamples to show that the Golden–Thompson inequality cannot be extended to three matrices – and, in any event, is not guaranteed to be real for Hermitian , , . However, the next theorem accomplishes this in one sense.
Lieb's theorem.
The Lieb's theorem, named after Elliott H. Lieb, states that, for a fixed Hermitian matrix , the function
is concave on the cone of positive-definite matrices.
The exponential map.
Note that the exponential of a matrix is always an invertible matrix. The inverse matrix of is given by . This is analogous to the fact that the exponential of a complex number is always nonzero. The matrix exponential then gives us a map
from the space of all "n"×"n" matrices to the general linear group of degree , i.e. the group of all "n"×"n" invertible matrices. In fact, this map is surjective which means that every invertible matrix can be written as the exponential of some other matrix (for this, it is essential to consider the field C of complex numbers and not R).
For any two matrices and ,
where || · || denotes an arbitrary matrix norm. It follows that the exponential map is continuous and Lipschitz continuous on compact subsets of .
The map
defines a smooth curve in the general linear group which passes through the identity element at "t" = 0.
In fact, this gives a one-parameter subgroup of the general linear group since
The derivative of this curve (or tangent vector) at a point "t" is given by
The derivative at "t" = 0 is just the matrix "X", which is to say that "X" generates this one-parameter subgroup.
More generally, for a generic -dependent exponent, ,
Taking the above expression outside the integral sign and expanding the integrand with the help of the Hadamard lemma one can obtain the following useful expression for the derivative of the matrix exponent,
Note that the coefficients in the expression above are different from what appears in the exponential. For a closed form, see derivative of the exponential map.
The determinant of the matrix exponential.
By Jacobi's formula, for any complex square matrix the following trace identity holds:
In addition to providing a computational tool, this formula demonstrates that a matrix exponential is always an invertible matrix. This follows from the fact that the right hand side of the above equation is always non-zero, and so , which implies that must be invertible.
In the real-valued case, the formula also exhibits the map 
to not be surjective, in contrast to the complex case mentioned earlier. This follows from the fact that, for real-valued matrices, the right-hand side of the formula is always positive, while there exist invertible matrices with a negative determinant.
Computing the matrix exponential.
Finding reliable and accurate methods to compute the matrix exponential is difficult, and this is still a topic of considerable current research in mathematics and numerical analysis. Matlab, GNU Octave, and SciPy all use the Padé approximant.
Diagonalizable case.
If a matrix is diagonal:
then its exponential can be obtained by exponentiating each entry on the main diagonal:
This also allows one to exponentiate diagonalizable matrices. If and is diagonal, then . Application of Sylvester's formula yields the same result. (To see this, note that addition and multiplication, hence also exponentiation, of diagonal matrices is equivalent to element-wise addition and multiplication, and hence exponentiation; in particular, the "one-dimensional" exponentiation is felt element-wise for the diagonal case.)
Projection case.
If is a projection matrix (i.e. is idempotent), its matrix exponential is . This may be derived by expansion of the definition of the exponential function and by use of the idempotency of :
Rotation case.
For a simple rotation in which the perpendicular unit vectors and specify a plane, the rotation matrix can be expressed in terms of a similar exponential function involving a generator and angle .
The formula for the exponential results from reducing the powers of in the series expansion and identifying the respective series coefficients of and with and respectively. The second expression here for is the same as the expression for in the article containing the derivation of the generator, .
In two dimensions, if formula_22 and formula_23, then formula_24, formula_25, and
reduces to the standard matrix for a plane rotation.
The matrix projects a vector onto the -plane and the rotation only affects this part of the vector. An example illustrating this is a rotation of in the plane spanned by and ,
formula_27
Let , so and its products with and are zero. This will allow us to evaluate powers of .
formula_28
Nilpotent case.
A matrix "N" is nilpotent if "N""q" = 0 for some integer "q". In this case, the matrix exponential "e""N" can be computed directly from the series expansion, as the series terminates after a finite number of terms:
Generalization.
When the minimal polynomial of a matrix "X" can be factored into a product of first degree polynomials, it can be expressed as a sum
where
This is the Jordan–Chevalley decomposition.
This means that we can compute the exponential of "X" by reducing to the previous two cases:
Note that we need the commutativity of "A" and "N" for the last step to work.
Another (closely related) method if the field is algebraically closed is to work with the Jordan form of "X". Suppose that "X" = "PJP" −1 where "J" is the Jordan form of "X". Then
Also, since
Therefore, we need only know how to compute the matrix exponential of a Jordan block. But each Jordan block is of the form
where "N" is a special nilpotent matrix. The matrix exponential of this block is given by
Evaluation by Laurent series.
By virtue of the Cayley–Hamilton theorem the matrix exponential is expressible as a polynomial of order −1.
If and are nonzero polynomials in one variable, such that , and if the meromorphic function
is entire, then
To prove this, multiply the first of the two above equalities by and replace by .
Such a polynomial can be found as follows−−see Sylvester's formula. Letting be a root of , is solved from the product of by the principal part of the Laurent series of at : It is proportional to the relevant Frobenius covariant. Then the sum "St" of the "Qa,t", where runs over all the roots of , can be taken as a particular . All the other "Qt" will be obtained by adding a multiple of to . In particular, , the Lagrange-Sylvester polynomial, is the only whose degree is less than that of .
Example: Consider the case of an arbitrary 2-by-2 matrix,
The exponential matrix , by virtue of the Cayley–Hamilton theorem, must be of the form 
Let and be the roots of the characteristic polynomial of ,
Then we have
and hence
if ; while, if ,
so that
Defining
we have
where is 0 if = 0, and if = 0.
Thus, 
Thus, as indicated above, the matrix having decomposed into the sum of two mutually commuting pieces, the traceful piece and the traceless piece,
the matrix exponential reduces to a plain product of the exponentials of the two respective pieces. This is a formula often used in physics, as it amounts to the analog of Euler's formula for Pauli spin matrices, that is rotations of the doublet representation of the group SU(2).
The polynomial can also be given the following "interpolation" characterization. Define , and ≡ deg. Then is the unique degree polynomial which satisfies whenever is less than the multiplicity of as a root of . We assume, as we obviously can, that is the minimal polynomial of . We further assume that is a diagonalizable matrix. In particular, the roots of are simple, and the "interpolation" characterization indicates that is given by the Lagrange interpolation formula, so it is the Lagrange−Sylvester polynomial .
At the other extreme, if , then
The simplest case not covered by the above observations is when formula_50 with , which yields
Evaluation by implementation of Sylvester's formula.
A practical, expedited computation of the above reduces to the following rapid steps.
Recall from above that an "n"-by-"n" matrix amounts to a linear combination of the first −1 powers of by the Cayley-Hamilton theorem. For diagonalizable matrices, as illustrated above, e.g. in the 2 by 2 case, Sylvester's formula yields , where the s are the Frobenius covariants of .
It is easiest, however, to simply solve for these s directly, by evaluating this expression and its first derivative at =0, in terms of and , to find the same answer as above.
But this simple procedure also works for defective matrices, in a generalization due to Buchheim. This is illustrated here for a 4-by-4 example of a matrix which is not diagonalizable, and the s are not projection matrices.
Consider
with eigenvalues and , each with a 
multiplicity of two.
Consider the exponential of each eigenvalue multiplied by , . Multiply each such by the corresponding undetermined coefficient matrix . If the eigenvalues have an algebraic multiplicity greater than 1, then repeat the process, but now multiplying by an extra factor of for each repetition, to ensure linear independence. (If one eigenvalue had a multiplicity of three, then there would be the three terms: formula_53. By contrast, when all eigenvalues are distinct, the s are just the Frobenius covariants, and solving for them as below just amounts to the inversion of the Vandermonde matrix of these 4 eigenvalues.)
Sum all such terms, here four such:
To solve for all of the unknown matrices in terms of the first three powers of and the identity, we need four equations, the above one providing one such at =0. Further, differentiate it with respect to ,
and again,
and once more,
Setting =0 in these four equations, the four coefficient matrices s may be solved for,
to yield 
Substituting with the value for yields the coefficient matrices
so the final answer is 
The procedure is much shorter than Putzer's algorithm sometimes utilized in such cases.
Illustrations.
Suppose that we want to compute the exponential of
Its Jordan form is
where the matrix "P" is given by
Let us first calculate exp("J"). We have
The exponential of a 1×1 matrix is just the exponential of the one entry of the matrix, so exp("J"1(4)) = ["e"4]. The exponential of "J"2(16) can be calculated by the formula "e"(λ"I" + "N") = "e"λ "e"N mentioned above; this yields
Therefore, the exponential of the original matrix "B" is
Applications.
Linear differential equations.
The matrix exponential has applications to systems of linear differential equations. (See also matrix differential equation.) Recall from earlier in this article that a "homogeneous" differential equation of the form
has solution .
If we consider the vector
we can express a system of "inhomogeneous" coupled linear differential equations as
Making an ansatz to use an integrating factor of and multiplying throughout, yields
The second step is possible due to the fact that, if , then . So, calculating leads to the solution to the system, by simply integrating the third step in s.
Example (homogeneous).
Consider the system
The associated defective matrix is
The matrix exponential is
so that the general solution of the homogeneous system is
amounting to
Example (inhomogeneous).
Consider now the inhomogeneous system
We again have
and
From before, we already have the general solution to the homogeneous equation. Since the sum of the homogeneous and particular solutions give the general solution to the inhomogeneous problem, we now only need find the particular solution.
We have, by above,
which could be further simplified to get the requisite particular solution determined through variation of parameters.
Note c = y"p"(0). For more rigor, see the following generalization.
Inhomogeneous case generalization: variation of parameters.
For the inhomogeneous case, we can use integrating factors (a method akin to variation of parameters). We seek a particular solution of the form , 
For to be a solution,
Thus,
where is determined by the initial conditions of the problem.
More precisely, consider the equation
with the initial condition , where
 is an by complex matrix,
 is a continuous function from some open interval to ℂ"n",
formula_91 is a point of , and
formula_92 is a vector of ℂ"n".
Left-multiplying the above displayed equality by yields
We claim that the solution to the equation
with the initial conditions formula_95 for 0 ≤ is
where the notation is as follows:
formula_97 is a monic polynomial of degree ,
 is a continuous complex valued function defined on some open interval ,
formula_91 is a point of ,
formula_99 is a complex number, and
 is the coefficient of formula_100 in the polynomial denoted by formula_101 in Subsection Evaluation by Laurent series above.
To justify this claim, we transform our order scalar equation into an order one vector equation by the usual reduction to a first order system. Our vector equation takes the form
where is the transpose companion matrix of . We solve this equation as explained above, computing the matrix exponentials by the observation made in Subsection Alternative above.
In the case = 2 we get the following statement. The solution to
is
where the functions and are as in Subsection Evaluation by Laurent series above.
Matrix-matrix exponentials.
The matrix exponential of another matrix (matrix-matrix exponential), is defined as
for any normal and non-singular matrix, and any complex matrix.
For matrix-matrix exponentials, there is a distinction between the left exponential and the right exponential , because the multiplication operator for matrix-to-matrix is not commutative. Moreover,

</doc>
<doc id="3983656" url="https://en.wikipedia.org/wiki?curid=3983656" title="Matrix group">
Matrix group

In mathematics, a matrix group is a group "G" consisting of invertible matrices over some field "K", usually fixed in advance, with operations of matrix multiplication and inversion. More generally, one can consider "n" × "n" matrices over a commutative ring "R". (The size of the matrices is restricted to be finite, as any group can be represented as a group of infinite matrices over any field.) A linear group is an abstract group that is isomorphic to a matrix group over a field "K", in other words, admitting a faithful, finite-dimensional representation over "K".
Any finite group is linear, because it can be realized by permutation matrices using Cayley's theorem. Among infinite groups, linear groups form an interesting and tractable class. Examples of groups that are not linear include all "sufficiently large" groups; for example, the infinite symmetric group of permutations of an infinite set. 
Basic examples.
The set "M""R"("n","n") of "n" × "n" matrices over a commutative ring "R" is itself a ring under matrix addition and multiplication. The group of units of "M""R"("n","n") is called the general linear group of "n" × "n" matrices over the ring "R" and is denoted "GL""n"("R") or "GL"("n","R"). All matrix groups are subgroups of some general linear group.
Classical groups.
Some particularly interesting matrix groups are the so-called classical groups. When the ring of coefficients of the matrix group is the real numbers, these groups are the classical Lie groups. When the underlying ring is a finite field the classical groups are groups of Lie type. These groups play an important role in the classification of finite simple groups.
Finite groups as matrix groups.
Every finite group is isomorphic to some matrix group. This is similar to Cayley's theorem which states that every finite group is isomorphic to some permutation group. Since the isomorphism property is transitive one need only consider how to form a matrix group from a permutation group.
Let "G" be a permutation group on "n" points (Ω = {1,2,…,n}) and let {"g"1...,"g""k"} be a generating set for "G". The general linear group "GL""n"(C) of "n"×"n" matrices over the complex numbers acts naturally on the vector space C"n". Let "B"={"b"1,…,"b""n"} be the standard basis for C"n". For each "g""i" let "M""i" in "GL""n"(C) be the matrix which sends each "b""j" to "b""g""i"("j"). That is, if the permutation "g""i" sends the point "j" to "k" then "M""i" sends the basis vector "b""j" to "b""k". Let "M" be the subgroup of "GL""n"(C) generated by {"M"1,…,"M""k"}. The action of "G" on Ω is then precisely the same as the action of "M" on "B". It can be proved that the function taking each "g""i" to "M""i" extends to an isomorphism and thus every group is isomorphic to a matrix group.
Note that the field (C in the above case) is irrelevant since "M" contains only elements with entries 0 or 1. One can just as easily perform the construction for an arbitrary field since the elements 0 and 1 exist in every field.
As an example, let "G" = "S"3, the symmetric group on 3 points. Let "g"1 = (1,2,3) and "g"2 = (1,2). Then
"M"1"b"1 = "b"2, "M"1"b"2 = "b"3 and "M"1"b"3 = "b"1. Likewise, "M"2"b"1 = "b"2, "M"2"b"2 = "b"1 and "M"2"b"3 = "b"3.
Representation theory and character theory.
Linear transformations and matrices are (generally speaking) well-understood objects in mathematics and have been used extensively in the study of groups. In particular representation theory studies homomorphisms from a group into a matrix group and character theory studies homomorphisms from a group into a field given by the trace of a representation.

</doc>
<doc id="396320" url="https://en.wikipedia.org/wiki?curid=396320" title="Matrix mechanics">
Matrix mechanics

Matrix mechanics is a formulation of quantum mechanics created by Werner Heisenberg, Max Born, and Pascual Jordan in 1925.
Matrix mechanics was the first conceptually autonomous and logically consistent formulation of quantum mechanics. Its account of quantum jumps supplanted the Bohr Model's electron orbits. It did so by interpreting the physical properties of particles as matrices that evolve in time. It is equivalent to the Schrödinger wave formulation of quantum mechanics, as manifest in Dirac's bra–ket notation.
In some contrast to the wave formulation, it produces spectra of energy operators by purely algebraic, ladder operator, methods. Relying on these methods, Pauli derived the hydrogen atom spectrum in 1926, before the development of wave mechanics.
Development of matrix mechanics.
In 1925, Werner Heisenberg, Max Born, and Pascual Jordan formulated the matrix mechanics representation of quantum mechanics.
Epiphany at Helgoland.
In 1925 Werner Heisenberg was working in Göttingen on the problem of calculating the spectral lines of hydrogen. By May 1925 he began trying to describe atomic systems by observables only. On June 7, to escape the effects of a bad attack of hay fever, Heisenberg left for the pollen free North Sea island of Helgoland. While there, in between climbing and learning by heart poems from Goethe's West-östlicher Diwan, he continued to ponder the spectral issue and eventually realised that adopting "non-commuting" observables might solve the problem, and he later wrote
The Three Fundamental Papers.
After Heisenberg returned to Göttingen, he showed Wolfgang Pauli his calculations, commenting at one point: 
On July 9 Heisenberg gave the same paper of his calculations to Max Born, saying,
"...he had written a crazy paper and did not dare to send it in for publication, and that Born should read it and advise him on it..."
prior to publication. Heisenberg then departed for a while, leaving Born to analyse the paper.
In the paper, Heisenberg formulated quantum theory without sharp electron orbits. Hendrik Kramers had earlier calculated the relative intensities of spectral lines in the Sommerfeld model by interpreting the Fourier coefficients of the orbits as intensities. But his answer, like all other calculations in the old quantum theory, was only correct for large orbits.
Heisenberg, after a collaboration with Kramers, began to understand that the transition probabilities were not quite classical quantities, because the only frequencies that appear in the Fourier series should be the ones that are observed in quantum jumps, not the fictional ones that come from Fourier-analyzing sharp classical orbits. He replaced the classical Fourier series with a matrix of coefficients, a fuzzed-out quantum analog of the Fourier series. Classically, the Fourier coefficients give the intensity of the emitted radiation, so in quantum mechanics the magnitude of the matrix elements of the position operator were the intensity of radiation in the bright-line spectrum. The quantities in Heisenberg's formulation were the classical position and momentum, but now they were no longer sharply defined. Each quantity was represented by a collection of Fourier coefficients with two indices, corresponding to the initial and final states. 
When Born read the paper, he recognized the formulation as one which could be transcribed and extended to the systematic language of matrices, which he had learned from his study under Jakob Rosanes at Breslau University. Born, with the help of his assistant and former student Pascual Jordan, began immediately to make the transcription and extension, and they submitted their results for publication; the paper was received for publication just 60 days after Heisenberg's paper. 
A follow-on paper was submitted for publication before the end of the year by all three authors. (A brief review of Born's role in the development of the matrix mechanics formulation of quantum mechanics along with a discussion of the key formula involving the non-commutivity of the probability amplitudes can be found in an article by Jeremy Bernstein. A detailed historical and technical account can be found in Mehra and Rechenberg's book "The Historical Development of Quantum Theory. Volume 3. The Formulation of Matrix Mechanics and Its Modifications 1925–1926.")
Up until this time, matrices were seldom used by physicists; they were considered to belong to the realm of pure mathematics. Gustav Mie had used them in a paper on electrodynamics in 1912 and Born had used them in his work on the lattices theory of crystals in 1921. While matrices were used in these cases, the algebra of matrices with their multiplication did not enter the picture as they did in the matrix formulation of quantum mechanics. 
Born, however, had learned matrix algebra from Rosanes, as already noted, but Born had also learned Hilbert's theory of integral equations and quadratic forms for an infinite number of variables as was apparent from a citation by Born of Hilbert's work "Grundzüge einer allgemeinen Theorie der Linearen Integralgleichungen" published in 1912. 
Jordan, too was well equipped for the task. For a number of years, he had been an assistant to Richard Courant at Göttingen in the preparation of Courant and David Hilbert's book "Methoden der mathematischen Physik I", which was published in 1924. This book, fortuitously, contained a great many of the mathematical tools necessary for the continued development of quantum mechanics.
In 1926, John von Neumann became assistant to David Hilbert, and he would coin the term Hilbert space to describe the algebra and analysis which were used in the development of quantum mechanics.
Heisenberg's reasoning.
Before matrix mechanics, the old quantum theory described the motion of a particle by a classical orbit, with well defined position and momentum "X"("t"), "P"("t"), with the restriction that the time integral over one period "T" of the momentum times the velocity must be a positive integer multiple of Planck's constant
While this restriction correctly selects orbits with more or less the
right energy values "En", the old quantum mechanical formalism did not describe time dependent processes, such as the emission or absorption of radiation.
When a classical particle is weakly coupled to a radiation field, so that the radiative damping can be neglected, it will emit radiation in a pattern which repeats itself every orbital period. The frequencies which make up the outgoing wave are then integer multiples of the orbital frequency, and this is a reflection of the fact that "X"("t") is periodic, so that its Fourier representation has frequencies 2π"n/T" only.
The coefficients "Xn" are complex numbers. The ones with negative frequencies must be the complex conjugates of the ones with positive frequencies, so that "X"("t") will always be real,
A quantum mechanical particle, on the other hand, can't emit radiation continuously, it can only emit photons. Assuming that the quantum particle started in orbit number "n", emitted a photon, then ended up in orbit number "m", the energy of the photon is , which means that its frequency is .
For large "n" and "m", but with "n"−"m" relatively small, these are the classical frequencies by Bohr's
correspondence principle
In the formula above, "T" is the classical period of either orbit "n" or orbit "m", since the difference between them is higher order in "h". But for "n" and "m" small, or if "n" − "m" is large, the frequencies are not integer multiples of any single frequency.
Since the frequencies which the particle emits are the same as the frequencies in the fourier description of its motion, this suggests that "something" in the time-dependent description of the particle is oscillating with frequency . Heisenberg called this quantity "Xnm",
and demanded that it should reduce to the classical Fourier coefficients in the classical limit. For large values of "n", "m" but with "n" − "m" relatively small,
"Xnm" is the th Fourier coefficient of the classical motion at orbit "n". Since "Xnm" has opposite frequency to "Xmn", the condition that "X" is real becomes
By definition, "Xnm" only has the frequency , so its time evolution is simple:
This is the original form of Heisenberg's equation of motion.
Given two arrays "Xnm" and "Pnm" describing two physical quantities, Heisenberg could form a new array of the same type by
combining the terms "XnkPkm", which also oscillate with the right frequency. Since the Fourier coefficients of the product of two quantities is the convolution of the Fourier coefficients of each one separately, the correspondence with Fourier series allowed Heisenberg to deduce the rule by which
the arrays should be multiplied,
Born pointed out that "this is the law of matrix multiplication", so that the position, the momentum, the energy, all the observable
quantities in the theory, are interpreted as matrices. Under this multiplication rule, the product depends on the order: "XP" is different from "PX".
The "X" matrix is a complete description of the motion of a quantum mechanical particle. Because the frequencies in the quantum motion are not multiples of a common frequency, the matrix elements "cannot be interpreted as the Fourier coefficients of a sharp classical trajectory". Nevertheless, as matrices, "X"("t") and "P"("t") satisfy the classical equations of motion; also see Ehrenfest's theorem, below.
Matrix basics.
When it was introduced by Werner Heisenberg, Max Born and Pascual Jordan in 1925, matrix mechanics was not immediately accepted and was a source of controversy, at first.
Schrödinger's later introduction of wave mechanics was greatly favored.
Part of the reason was that Heisenberg's formulation was in an odd mathematical language, for the time, while Schrödinger's formulation was based on familiar wave equations. But there was also a deeper sociological reason. Quantum mechanics had been developing by two paths, one under the direction of Einstein and the other under the direction of Bohr. Einstein emphasized wave–particle duality, while Bohr emphasized the discrete energy states and quantum jumps. De Broglie had shown how to reproduce the discrete energy states in Einstein's framework--- the quantum condition is the standing wave condition, and this gave hope to those in the Einstein school that all the discrete aspects of quantum mechanics would be subsumed into a continuous wave mechanics.
Matrix mechanics, on the other hand, came from the Bohr school, which was concerned with discrete energy states and quantum jumps. Bohr's followers did not appreciate physical models which pictured electrons as waves, or as anything at all. They preferred to focus on the quantities which were directly connected to experiments.
In atomic physics, spectroscopy gave observational data on atomic transitions arising from the interactions of atoms with light quanta. The Bohr school required that only those quantities which were in principle measurable by spectroscopy should appear in the theory. These quantities include the energy levels and their intensities but they do not include the exact location of a particle in its Bohr orbit. It is very hard to imagine an experiment which could determine whether an electron in the ground state of a hydrogen atom is to the right or to the left of the nucleus. It was a deep conviction that such questions did not have an answer.
The matrix formulation was built on the premise that all physical observables are represented by matrices, whose elements are indexed by two different energy levels. The set of eigenvalues of the matrix were eventually understood to be the set of all possible values that the observable can have. Since Heisenberg's matrices are Hermitian, the eigenvalues are real.
If an observable is measured and the result is a certain eigenvalue, the corresponding eigenvector is the state of the system immediately after the measurement. The act of measurement in matrix mechanics 'collapses' the state of the system. If one measures two observables simultaneously, the state of the system collapses to a common eigenvector of the two observables. Since most matrices don't have any eigenvectors in common, most observables can never be measured precisely at the same time. This is the uncertainty principle.
If two matrices share their eigenvectors, they can be simultaneously diagonalized. In the basis where they are both diagonal, it is clear that their product does not depend on their order because multiplication of diagonal matrices is just multiplication of numbers. The uncertainty principle, by contrast, is an expression of the fact that often two matrices "A" and "B" do not always commute, i.e., that "AB − BA" does not necessarily equal 0. The fundamental commutation relation of matrix mechanics,
implies then that "there are no states which simultaneously have a definite position and momentum".
This principle of uncertainty holds for many other pairs of observables as well. For example, the energy does not commute with the position either, so it is impossible to precisely determine the position and energy of an electron in an atom.
Nobel Prize.
In 1928, Albert Einstein nominated Heisenberg, Born, and Jordan for the Nobel Prize in Physics. The announcement of the Nobel Prize in Physics for 1932 was delayed until November 1933. It was at that time that it was announced Heisenberg had won the Prize for 1932 "for the creation of quantum mechanics, the application of which has, inter alia, led to the discovery of the allotropic forms of hydrogen" and Erwin Schrödinger and Paul Adrien Maurice Dirac shared the 1933 Prize "for the discovery of new productive forms of atomic theory". 
One can rightly ask why Born was not awarded the Prize in 1932 along with Heisenberg, and Bernstein gives some speculations on this matter. One of them is related to Jordan joining the Nazi Party on May 1, 1933 and becoming a Storm Trooper. Hence, Jordan's Party affiliations and Jordan's links to Born may have affected Born's chance at the Prize at that time. Bernstein also notes that when Born won the Prize in 1954, Jordan was still alive, and the Prize was awarded for the statistical interpretation of quantum mechanics, attributable to Born alone.
Heisenberg's reactions to Born for Heisenberg receiving the Prize for 1932 and for Born receiving the Prize in 1954 are also instructive in evaluating whether Born should have shared the Prize with Heisenberg. On November 25, 1933 Born received a letter from Heisenberg in which he said he had been delayed in writing due to a "bad conscience" that he alone had received the Prize "for work done in Göttingen in collaboration – you, Jordan and I." Heisenberg went on to say that Born and Jordan's contribution to quantum mechanics cannot be changed by "a wrong decision from the outside." 
In 1954, Heisenberg wrote an article honoring Max Planck for his insight in 1900. In the article, Heisenberg credited Born and Jordan for the final mathematical formulation of matrix mechanics and Heisenberg went on to stress how great their contributions were to quantum mechanics, which were not "adequately acknowledged in the public eye."
Mathematical development.
Once Heisenberg introduced the matrices for X and P, he could find their
matrix elements in special cases by guesswork, guided by the
correspondence principle. Since the matrix elements are the quantum mechanical
analogs of Fourier coefficients of the classical orbits, the simplest case is the
harmonic oscillator, where the classical position and momentum, X(t) and P(t), are sinusoidal.
Harmonic oscillator.
In units where the mass and frequency of the oscillator are equal to one (see nondimensionalization), the energy of the oscillator is
The level sets of are the clockwise orbits, and they are nested circles in phase space. The classical orbit with energy is
The old quantum condition dictates that the integral of over an orbit, which is the area of the circle in phase space, must be an integer multiple of Planck's constant. The area of the circle of radius is . So
or, in natural units where , the energy is an but an integer.
The Fourier components of and are simple, and more so if they are combined into the quantities
Both and have only a single frequency, and "X" and "P" can be recovered from their sum and difference.
Since has a classical Fourier series with only the lowest frequency, and the matrix element is the th Fourier coefficient of the classical orbit, the matrix for is nonzero only on the line just above the diagonal, where it is equal to . The matrix for is likewise only nonzero on the line below the diagonal, with
the same elements. 
Thus, from and, reconstruction yields
and
which, up to the choice of units, are the Heisenberg matrices for the harmonic oscillator. 
Note that both matrices are hermitian, since they are constructed from the Fourier coefficients of real quantities. 
Finding and is direct, since they are quantum Fourier coefficients so they evolve simply with time,
The matrix product of and is not hermitian, but has a real and imaginary part. The real part is one half the symmetric expression , while the imaginary part is proportional to the commutator 
It is simple to verify explicitly that in the case of the harmonic oscillator, is , multiplied by the identity.
It is likewise simple to verify that the matrix
is a diagonal matrix, with eigenvalues .
Conservation of energy.
The harmonic oscillator is an important case. Finding the matrices is easier than determining the general conditions from these special forms. For this reason, Heisenberg investigated the anharmonic oscillator, with Hamiltonian
In this case, the and matrices are no longer simple off diagonal matrices, since the corresponding classical orbits are slightly squashed and displaced, so that they have Fourier coefficients at every classical frequency. To determine the matrix elements, Heisenberg required that the classical equations of motion be obeyed as matrix equations,
He noticed that if this could be done, then , considered as a matrix function of and , will have zero time derivative.
where is the anticommutator,
Given that all the off diagonal elements have a nonzero frequency; being constant implies that is diagonal.
It was clear to Heisenberg that in this system, the energy could be exactly conserved in an arbitrary quantum system, a very encouraging sign.
The process of emission and absorption of photons seemed to demand that the conservation of energy will hold at best on average. If a wave containing exactly one photon passes over some atoms, and one of them absorbs it, that atom needs to tell the others that they can't absorb the photon anymore. But if the atoms are far apart, any signal cannot reach the other atoms in time, and they might end up absorbing the same photon anyway and dissipating the energy to the environment. When the signal reached them, the other atoms would have to somehow recall that energy. This paradox led Bohr, Kramers and Slater to abandon exact conservation of energy. Heisenberg's formalism, when extended to include the electromagnetic field, was obviously going to sidestep this problem, a hint that the interpretation of the theory will involve wavefunction collapse.
Differentiation trick — canonical commutation relations.
Demanding that the classical equations of motion are preserved is not a strong enough condition to determine the matrix elements. Planck's constant does not appear in the classical equations, so that the matrices could be constructed for many different values of and still satisfy the equations of motion, but with different energy levels.
So, in order to implement his program, Heisenberg needed to use the old quantum condition to fix the energy levels, then fill in the matrices with Fourier coefficients of the classical equations, then alter the matrix coefficients and the energy levels slightly to make sure the classical equations are satisfied. This is clearly not
satisfactory. The old quantum conditions refer to the area enclosed by the sharp classical orbits, which do not exist in the new formalism.
The most important thing that Heisenberg discovered is how to translate the old quantum condition into a simple statement in matrix mechanics.
To do this, he investigated the action integral as a matrix quantity,
There are several problems with this integral, all stemming from the incompatibility of the matrix formalism with the old picture of orbits. Which period "T" should be used? "Semiclassically", it should be either "m" or "n", but the difference is order , and an answer to order is sought. The "quantum" condition tells us that
"Jmn" is 2π"n" on the diagonal, so the fact that "J" is classically constant tells us that the off-diagonal elements are zero.
His crucial insight was to differentiate the quantum condition with respect to "n". This idea only makes complete sense in the classical limit, where "n" is
not an integer but the continuous action variable "J", but Heisenberg performed analogous manipulations with matrices, where the intermediate
expressions are sometimes discrete differences and sometimes derivatives.
In the following discussion, for the sake of clarity, the differentiation will be performed on the classical variables, and the transition to matrix mechanics will be done afterwards, guided by the correspondence principle.
In the classical setting, the derivative is the derivative with respect to "J" of the integral which defines "J", so it is tautologically equal to 1.
where the derivatives "dP/dJ" and "dX/dJ" should be interpreted as differences with respect to "J" at corresponding times on nearby orbits, exactly what would be obtained if the Fourier coefficients of the orbital motion were differentiated. (These derivatives are symplectically orthogonal in phase space to the time derivatives "dP/dt" and "dX/dt").
The final expression is clarified by introducing the variable canonically conjugate to "J", which is called the angle variable "θ":
The derivative with respect to time is a derivative with respect to "θ", up to a factor of 2π"T",
So the quantum condition integral is the average value over one cycle of the Poisson bracket of "X" and "P".
An analogous differentiation of the Fourier series of "P dX" demonstrates that the off-diagonal elements of the Poisson bracket are all zero. The Poisson bracket of two canonically conjugate variables, such as "X" and "P", is the constant value 1, so this integral really is the average value of 1; so it is 1, as we knew all along, because it is "dJ/dJ" after all. But Heisenberg, Born and Jordan, unlike Dirac, were not familiar with the theory of Poisson brackets, so, for them, the differentiation effectively evaluated {"X, P"} in "J, θ" coordinates.
The Poisson Bracket, unlike the action integral, does have a simple translation to matrix mechanics−−it normally corresponds to the imaginary part of the product of two variables, the commutator.
To see this, examine the (antisymmetrized) product of two matrices "A" and "B" in the correspondence limit, where the matrix elements are slowly varying functions of the index, keeping in mind that the answer is zero classically.
In the correspondence limit, when indices "m", "n" are large and nearby, while "k","r" are small, the rate of change of the matrix elements in the diagonal direction is the matrix element of the "J" derivative of the corresponding classical quantity. So its possible to shift any matrix element diagonally through the correspondence,
where the right hand side is really only the ("m" − "n")'th Fourier component
of "dA/dJ" at the orbit near "m" to this semiclassical order, not a full well-defined matrix.
The semiclassical time derivative of a matrix element is obtained up to a factor of "i" by multiplying by the distance from the diagonal,
since the coefficient "Am(m+k)" is semiclassically the "k"'th Fourier coefficient of the "m"-th classical orbit.
The imaginary part of the product of "A" and "B" can be evaluated by shifting the matrix elements around so as to reproduce the classical answer, which is zero.
The leading nonzero residual is then given entirely by the shifting. Since all the matrix elements are at indices which have a small distance from the large index position ("m,m"), it helps to introduce two temporary notations:
 for the matrices, and for the r'th Fourier components of classical quantities,
Flipping the summation variable in the first sum from to "r"' = "k" − "r", the matrix element becomes,
and it is clear that the principal (classical) part cancels.
The leading quantum part, neglecting the higher order product of derivatives in the residual expression, is then
so that, finally, 
which can be identified with times the -th classical Fourier component of the Poisson bracket.
Heisenberg's original differentiation trick was eventually extended to a full semiclassical derivation of the quantum condition, in collaboration with Born and Jordan.
Once they were able to establish that
this condition replaced and extended the old quantization rule, allowing the matrix elements of "P" and "X" for an arbitrary system to be determined 
simply from the form of the Hamiltonian.
The new quantization rule was "assumed to be universally true", even though the derivation from the old quantum theory required semiclassical reasoning.
State vectors and the Heisenberg equation.
To make the transition to standard quantum mechanics, the most important further addition was the quantum state vector, now written |"ψ"⟩,
which is the vector that the matrices act on. Without the state vector, it is not clear which particular motion the Heisenberg matrices are describing, since they include all the motions somewhere.
The interpretation of the state vector, whose components are written , was furnished by Born. This interpretation is statistical: the result of a measurement of the physical quantity corresponding to the matrix is random, with an average value equal to
Alternatively, and equivalently, the state vector gives the probability amplitude for the quantum system to be in the energy state .
Once the state vector was introduced, matrix mechanics could be rotated to "any basis", where the matrix need no longer be diagonal. The Heisenberg equation of motion in its original form states that evolves in time like a Fourier component,
which can be recast in differential form
and it can be restated so that it is true in an arbitrary basis, by noting that the matrix is diagonal with diagonal values ,
This is now a matrix equation, so it holds in any basis. This is the modern form of the Heisenberg equation of motion.
Its formal solution is:
All these forms of the equation of motion above say the same thing, that is equivalent to , through a basis rotation by the unitary matrix , a systematic picture elucidated by Dirac in his bra–ket notation.
Conversely, by rotating the basis for the state vector at each time by , the time dependence in the matrices can be undone. The matrices are now time independent, but the state vector rotates,
This is the Schrödinger equation for the state vector, and this time-dependent change of basis amounts to transformation to the Schrödinger picture, with ⟨"x"|"ψ"⟩ = "ψ(x)".
In quantum mechanics in the Heisenberg picture the state vector, |"ψ"⟩ does not change with time, while an observable "A" satisfies the 
Heisenberg equation of motion,
The extra term is for operators such as 
which have an "explicit time dependence", in addition to the time dependence from the unitary evolution discussed.
The Heisenberg picture does not distinguish time from space, so it is better suited to relativistic theories than the Schrödinger equation. Moreover, the similarity to classical physics is more manifest: the Hamiltonian equations of motion for classical mechanics are recovered by replacing the commutator above by the Poisson bracket (see also below). By the Stone–von Neumann theorem, the Heisenberg picture and the Schrödinger picture must be unitarily equivalent, as detailed below.
Further results.
Matrix mechanics rapidly developed into modern quantum mechanics, and gave interesting physical results on the spectra of atoms.
Wave mechanics.
Jordan noted that the commutation relations ensure that "P acts as a differential operator".
The operator identity
allows the evaluation of the commutator of "P" with any power of "X", and it implies that
which, together with linearity, implies that a "P"-commutator effectively differentiates any analytic matrix function of "X".
Assuming limits are defined sensibly, this extends to arbitrary functions−−but the extension need not be made explicit until a certain degree of mathematical rigor is required,
Since "X" is a Hermitian matrix, it should be diagonalizable, and it will be clear from the eventual form of "P" that every real number can be an eigenvalue. This makes some of the mathematics subtle, since there is a separate eigenvector for every point in space.
In the basis where "X" is diagonal, an arbitrary state can be written as a superposition of states with eigenvalues "x",
so that "ψ"("x") = ⟨"x"|"ψ"⟩, and the operator "X" multiplies each eigenvector by "x",
Define a linear operator "D" which differentiates ,
and note that
so that the operator −"iD" obeys the same commutation relation as "P". Thus, the difference between "P" and −"iD" must commute with "X",
so it may be simultaneously diagonalized with "X": its value acting on any eigenstate of "X" is some function "f" of the eigenvalue "x".
This function must be real, because both "P" and −"iD" are Hermitian,
rotating each state formula_49 by a phase , that is, redefining the phase of the wavefunction:
The operator "iD" is redefined by an amount:
which means that, in the rotated basis, "P" is equal to −"iD".
Hence, there is always a basis for the eigenvalues of "X" where the action of "P" on any wavefunction is known:
and the Hamiltonian in this basis is a linear differential operator on the state-vector components,
Thus, the equation of motion for the state vector is but a celebrated differential equation,
Since "D" is a differential operator, in order for it to be sensibly defined, there must be eigenvalues of "X" which neighbors every given value. This suggests that the only possibility is that the space of all eigenvalues of "X" is all real numbers, and that "P is iD, up to a phase rotation".
To make this rigorous requires a sensible discussion of the limiting space of functions, and in this space this is the Stone–von Neumann theorem: any operators "X" and "P" which obey the commutation relations can be made to act on a space of wavefunctions, with "P" a derivative operator. This implies that a Schrödinger picture is always available.
Matrix mechanics easily extends to many degrees of freedom in a natural way. Each degree of freedom has a separate "X" operator and a separate effective differential operator "P", and the wavefunction is a function of all the possible eigenvalues of the independent commuting "X" variables.
In particular, this means that a system of "N" interacting particles in 3 dimensions is described by one vector whose components in a basis where all the "X" are diagonal is a mathematical function of 3"N"-dimensional space "describing all their possible positions", effectively a "much bigger collection of values" than the mere collection of "N" three-dimensional wavefunctions in one physical space. Schrödinger came to the same conclusion independently, and eventually proved the equivalence of his own formalism to Heisenberg's.
Since the wavefunction is a property of the whole system, not of any one part, the description in quantum mechanics is not entirely local. The description of several quantum particles has them correlated, or entangled. This entanglement leads to strange correlations between distant particles which violate the classical Bell's inequality.
Even if the particles can only be in just two positions, the wavefunction for "N" particles requires 2"N" complex numbers, one for each total configuration of positions. This is exponentially many numbers in "N", so simulating quantum mechanics on a computer requires exponential resources. Conversely, this suggests that it might be possible to find quantum systems of size "N" which physically compute the answers to problems which classically require 2"N" bits to solve. This is the aspiration behind quantum computing.
Ehrenfest theorem.
For the time-independent operators "X" and "P", so the Heisenberg equation above reduces to:
where the square brackets denote the commutator. For a Hamiltonian which is formula_58, the "X" and "P" operators satisfy:
where the first is classically the velocity, and second is classically the force, or potential gradient. These reproduce Hamilton's form of Newton's laws of motion. In the Heisenberg picture, the "X" and "P" operators satisfy the classical equations of motion. You can take the expectation value of both sides of the equation to see that, in any state |"ψ"⟩:
So Newton's laws are exactly obeyed by the expected values of the operators in any given state. This is Ehrenfest's theorem, which is an obvious corollary of the Heisenberg equations of motion, but is less trivial in the Schrödinger picture, where Ehrenfest discovered it.
Transformation theory.
In classical mechanics, a canonical transformation of phase space coordinates is one which preserves the structure of the Poisson brackets. The new variables have the same Poisson brackets with each other as the original variables . Time evolution is a canonical transformation, since the phase space at any time is just as good a choice of variables as the phase space at any other time.
The Hamiltonian flow is the canonical transformation:
Since the Hamiltonian can be an arbitrary function of "x" and "p", there are such infinitesimal canonical transformations corresponding to "every classical quantity" , where serves as the Hamiltonian to generate a flow of points in phase space for an increment of time "s",
For a general function on phase space, its infinitesimal change at every step "ds" under this map is
The quantity is called the "infinitesimal generator" of the canonical transformation.
In quantum mechanics, the quantum analog is now a Hermitian matrix, and the equations of motion are given by commutators,
The infinitesimal canonial motions can be formally integrated, just as the Heisenberg equation of motion were integrated,
where and is an arbitrary parameter.
The definition of a quantum canonical transformation is thus an arbitrary unitary change of basis on the space of all state vectors. is an arbitrary unitary matrix, a complex rotation in phase space,
These transformations leave the sum of the absolute square of the wavefunction components "invariant", while they take states which are multiples of each other (including states which are imaginary multiples of each other) to states which are the "same" multiple of each other.
The interpretation of the matrices is that they act as "generators of motions on the space of states".
For example, the motion generated by "P" can be found by solving the Heisenberg equation of motion using "P" as a Hamiltonian,
These are translations of the matrix "X" by a multiple of the identity matrix,
This is the interpretation of the derivative operator "D": , "the exponential of a derivative operator is a translation" (so Lagrange's shift operator).
The "X" operator likewise generates translations in "P". The Hamiltonian generates "translations in time", the angular momentum generates "rotations in physical space", and the operator generates "rotations in phase space".
When a transformation, like a rotation in physical space, commutes with the Hamiltonian, the transformation is called a symmetry (behind a degeneracy) of the Hamiltonian−−the Hamiltonian expressed in terms of rotated coordinates is the same as the original Hamiltonian. This means that the change in the Hamiltonian under the infinitesimal symmetry generator "L" vanishes,
It then follows that the change in the generator under time translation also vanishes,
so that the matrix "L" is constant in time: it is conserved.
The one-to-one association of infinitesimal symmetry generators and conservation laws was discovered by Emmy Noether for classical mechanics, where the commutators are Poisson brackets, but the quantum-mechanical reasoning is identical. In quantum mechanics, any unitary symmetry transformation yields a conservation law, since if the matrix U has the property that
so it follows that 
and that the time derivative of "U" is zero−−it is conserved.
The eigenvalues of unitary matrices are pure phases, so that the value of a unitary conserved quantity is a complex number of unit magnitude, not a real number. Another way of saying this is that a unitary matrix is the exponential of "i" times a Hermitian matrix, so that the additive conserved real quantity, the phase, is only well-defined up to an integer multiple of "2π". Only when the unitary symmetry matrix is part of a family that comes arbitrarily close to the identity are the conserved real quantities single-valued, and then the demand that they are conserved become a much more exacting constraint.
Symmetries which can be continuously connected to the identity are called "continuous", and translations, rotations, and boosts are examples. Symmetries which cannot be continuously connected to the identity are "discrete", and the operation of space-inversion, or parity, and charge conjugation are examples.
The interpretation of the matrices as generators of canonical transformations is due to Paul Dirac. The correspondence between symmetries and matrices was shown by Eugene Wigner to be complete, if antiunitary matrices which describe symmetries which include time-reversal are included.
Selection rules.
It was physically clear to Heisenberg that the absolute squares of the matrix elements of , which are the Fourier coefficients of the oscillation, would yield the rate of emission of electromagnetic radiation.
In the classical limit of large orbits, if a charge with position and charge is oscillating next to an equal and opposite charge at position 0, the instantaneous dipole moment is , and the time variation of this moment translates directly into the space-time variation of the vector potential, which yields nested outgoing spherical waves.
For atoms, the wavelength of the emitted light is about 10,000 times the atomic radius, and the dipole moment is the only contribution to the radiative field, while all other details of the atomic charge distribution can be ignored.
Ignoring back-reaction, the power radiated in each outgoing mode is a sum of separate contributions from the square of each independent time Fourier mode of ,
Now, in Heisenberg's representation, the Fourier coefficients of the dipole moment are the matrix elements of . This correspondence allowed Heisenberg to provide the rule for the transition intensities, the fraction of the time that, starting from an initial state , a photon is emitted and the atom jumps to a final state ,
This then allowed the magnitude of the matrix elements to be interpreted statistically: "they give the intensity of the spectral lines, the probability for quantum jumps from the emission of dipole radiation".
Since the transition rates are given by the matrix elements of , wherever is zero, the corresponding transition should be absent. These were called the selection rules, which were a puzzle until the advent of matrix mechanics.
An arbitrary state of the Hydrogen atom, ignoring spin, is labelled by |"n";"ℓ,m" ⟩, where the value of ℓ is a measure of the total orbital angular momentum and is its -component, which defines the orbit orientation. The components of the angular momentum pseudovector are
where the products in this expression are independent of order and real, because different components of X and P commute.
The commutation relations of L with all three coordinate matrices "X, Y, Z" (or with any vector) are easy to find,
which confirms that the operator L generates rotations between the three components of the vector of coordinate matrices X.
From this, the commutator of "Lz" and the coordinate matrices "X, Y, Z" can be read off,
This means that the quantities have a simple commutation rule,
Just like the matrix elements of "X + iP" and "X − iP" for the harmonic oscillator Hamiltonian, this commutation law implies that these operators only have certain off diagonal matrix elements in states of definite "m",
meaning that the matrix takes an eigenvector of with eigenvalue to an eigenvector with eigenvalue + 1. Similarly, decrease by one unit, while does not change the value of .
So, in a basis of |"ℓ,m"⟩ states where and have definite values, the matrix elements of any of the three components of the position are zero, except when is the same or changes by one unit.
This places a constraint on the change in total angular momentum. Any state can be rotated so that its angular momentum is in the -direction as much as possible, where "m" = ℓ. The matrix element of the position acting on |"ℓ,m"⟩ can only produce values of "m" which are bigger by one unit, so that if the coordinates are rotated so that the final state is |"ℓ',ℓ' "⟩, the value of ℓ’ can be at most one bigger than the biggest value of ℓ that occurs in the initial state. So ℓ’ is at most ℓ + 1.
The matrix elements vanish for ℓ’ > ℓ + 1, and the reverse matrix element is determined by Hermiticity, so these vanish also when ℓ’ < ℓ - 1: Dipole transitions are forbidden with a change in angular momentum of more than one unit.
Sum rules.
The Heisenberg equation of motion determines the matrix elements of "P" in the Heisenberg basis from the matrix elements of "X".
which turns the diagonal part of the commutation relation into a sum rule for the magnitude of the matrix elements:
This yields a relation for the sum of the spectroscopic intensities to and from any given state, although to be absolutely correct, contributions from the radiative capture probability for unbound scattering states must be included in the sum:

</doc>
