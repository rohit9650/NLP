<doc id="29320146" url="https://en.wikipedia.org/wiki?curid=29320146" title="Event horizon">
Event horizon

In general relativity, an event horizon is a boundary in spacetime beyond which events cannot affect an outside observer. In layman's terms, it is defined as "the point of no return", i.e., the point at which the gravitational pull becomes so great as to make escape impossible. An event horizon is most commonly associated with black holes. Light emitted from inside the event horizon can never reach the outside observer. Likewise, any object approaching the horizon from the observer's side appears to slow down and never quite pass through the horizon, with its image becoming more and more redshifted as time elapses. The traveling object, however, experiences no strange effects and does, in fact, pass through the horizon in a finite amount of proper time. From here to the central singularity will take 0.0001 seconds in proper time, in free fall, for a 30 solar mass black hole. This infall time is proportional to the mass of the black hole.
More specific types of horizon include the related but distinct absolute and apparent horizons found around a black hole. Still other distinct notions include the Cauchy and Killing horizon; the photon spheres and ergospheres of the Kerr solution; particle and cosmological horizons relevant to cosmology; and isolated and dynamical horizons important in current black hole research.
Event horizon of a black hole.
One of the best-known examples of an event horizon derives from general relativity's description of a black hole, a celestial object so massive that no nearby matter or radiation can escape its gravitational field. Often, this is described as the boundary within which the black hole's escape velocity is greater than the speed of light. However, a more accurate description is that within this horizon, all lightlike paths (paths that light could take) and hence all paths in the forward light cones of particles within the horizon, are warped so as to fall farther into the hole. Once a particle is inside the horizon, moving into the hole is as inevitable as moving forward in time, and can actually be thought of as equivalent to doing so, depending on the spacetime coordinate system used.
The surface at the Schwarzschild radius acts as an event horizon in a non-rotating body that fits inside this radius (although a rotating black hole operates slightly differently). The Schwarzschild radius of an object is proportional to its mass. Theoretically, any amount of matter will become a black hole if compressed into a space that fits within its corresponding Schwarzschild radius. For the mass of the Sun this radius is approximately 3 kilometers and for the Earth it is about 9 millimeters. In practice, however, neither the Earth nor the Sun has the necessary mass and therefore the necessary gravitational force, to overcome electron and neutron degeneracy pressure. The minimal mass required for a star to be able to collapse beyond these pressures is the Tolman-Oppenheimer-Volkoff limit, which is approximately three solar masses.
Black hole event horizons are widely misunderstood. Common, although erroneous, is the notion that black holes “vacuum up” material in their neighborhood, where in fact they are no more capable of “seeking out” material to consume than any other gravitational attractor. As with any mass in the Universe, matter must come within its gravitational scope for the possibility to exist of capture or consolidation with any other mass. Equally common is the idea that matter can be observed “falling into” a black hole. This is not possible. Astronomers can only detect accretion disks around black holes, where material moves with such speed that friction creates high-energy radiation which can be detected (similarly, some matter from these accretion disks is forced out along the axes of spin of the black hole, creating visible jets when these streams interact with matter such as interstellar gas or when they happen to be aimed directly at Earth). Furthermore, a distant observer will never actually see something cross the horizon. Instead, while approaching the hole, the object will seem to go ever more slowly, while any light it emits will be further and further redshifted.
Cosmic event horizon.
In cosmology, the event horizon of the observable universe is the largest comoving distance from which light emitted "now" can ever reach the observer in the future. This differs from the concept of particle horizon, which represents the largest comoving distance from which light emitted in the "past" could have reached the observer at a given time. For events beyond that distance, light has not had time to reach our location, even if it were emitted at the time the Universe began. How the particle horizon changes with time depends on the nature of the expansion of the Universe. If the expansion has certain characteristics, there are parts of the Universe that will never be observable, no matter how long the observer waits for light from those regions to arrive. The boundary past which events cannot ever be observed is an event horizon, and it represents the maximum extent of the particle horizon.
The criterion for determining whether a particle horizon for the Universe exists is as follows. Define a comoving distance formula_1 by
In this equation, "a" is the scale factor, "c" is the speed of light, and "t0" is the age of the Universe. If formula_3 (i.e., points arbitrarily as far away as can be observed), then no event horizon exists. If formula_4, a horizon is present.
Examples of cosmological models without an event horizon are universes dominated by matter or by radiation. An example of a cosmological model with an event horizon is a universe dominated by the cosmological constant (a de Sitter universe).
A calculation of the speeds of the cosmological event and particle horizons was given in a paper on the FLRW cosmological model, approximating the Universe as composed of non-interacting constituents, each one being a perfect fluid.
Apparent horizon of an accelerated particle.
If a particle is moving at a constant velocity in a non-expanding universe free of gravitational fields, any event that occurs in that Universe will eventually be observable by the particle, because the forward light cones from these events intersect the particle's world line. On the other hand, if the particle is accelerating, in some situations light cones from some events never intersect the particle's world line. Under these conditions, an apparent horizon is present in the particle's (accelerating) reference frame, representing a boundary beyond which events are unobservable.
For example, this occurs with a uniformly accelerated particle. A space-time diagram of this situation is shown in the figure to the right. As the particle accelerates, it approaches, but never reaches, the speed of light with respect to its original reference frame. On the space-time diagram, its path is a hyperbola, which asymptotically approaches a 45 degree line (the path of a light ray). An event whose light cone's edge is this asymptote or is farther away than this asymptote can never be observed by the accelerating particle. In the particle's reference frame, there appears to be a boundary behind it from which no signals can escape (an apparent horizon).
While approximations of this type of situation can occur in the real world (in particle accelerators, for example), a true event horizon is never present, as this requires the particle to be accelerated indefinitely (requiring arbitrarily large amounts of energy and an arbitrarily large apparatus).
Interacting with an event horizon.
A misconception concerning event horizons, especially black hole event horizons, is that they represent an immutable surface that destroys objects that approach them. In practice, all event horizons appear to be some distance away from any observer, and objects sent towards an event horizon never appear to cross it from the sending observer's point of view (as the horizon-crossing event's light cone never intersects the observer's world line). Attempting to make an object near the horizon remain stationary with respect to an observer requires applying a force whose magnitude increases unbounded (becoming infinite) the closer it gets.
For the case of a horizon perceived by a uniformly accelerating observer in empty space, the horizon seems to remain a fixed distance from the observer no matter how its surroundings move. Varying the observer's acceleration may cause the horizon to appear to move over time, or may prevent an event horizon from existing, depending on the acceleration function chosen. The observer never touches the horizon and never passes a location where it appeared to be.
For the case of a horizon perceived by an occupant of a de Sitter universe, the horizon always appears to be a fixed distance away for a non-accelerating observer. It is never contacted, even by an accelerating observer.
For the case of the horizon around a black hole, observers stationary with respect to a distant object will all agree on where the horizon is. While this seems to allow an observer lowered towards the hole on a rope (or rod) to contact the horizon, in practice this cannot be done. The proper distance to the horizon is finite, so the length of rope needed would be finite as well, but if the rope were lowered slowly (so that each point on the rope was approximately at rest in Schwarzschild coordinates), the proper acceleration (G-force) experienced by points on the rope closer and closer to the horizon would approach infinity, so the rope would be torn apart. If the rope is lowered quickly (perhaps even in freefall), then indeed the observer at the bottom of the rope can touch and even cross the event horizon. But once this happens it is impossible to pull the bottom of rope back out of the event horizon, since if the rope is pulled taut, the forces along the rope increase without bound as they approach the event horizon and at some point the rope must break. Furthermore, the break must occur not at the event horizon, but at a point where the second observer can observe it.
Observers crossing a black hole event horizon can calculate the moment they have crossed it, but will not actually see or feel anything special happen at that moment. In terms of visual appearance, observers who fall into the hole perceive the black region constituting the horizon as lying at some apparent distance below them, and never experience crossing this visual horizon. Other objects that had entered the horizon along the same radial path but at an earlier time would appear below the observer but still above the visual position of the horizon, and if they had fallen in recently enough the observer could exchange messages with them before either one was destroyed by the gravitational singularity. Increasing tidal forces (and eventual impact with the hole's singularity) are the only locally noticeable effects.
Beyond general relativity.
The description of event horizons given by general relativity is thought to be incomplete. When the conditions under which event horizons occur are modeled using a more comprehensive picture of the way the Universe works, that includes both relativity and quantum mechanics, event horizons are expected to have properties that are different from those predicted using general relativity alone.
At present, it is expected that the primary impact of quantum effects is for event horizons to possess a temperature and so emit radiation. For black holes, this manifests as Hawking radiation, and the larger question of how the black hole possesses a temperature is part of the topic of black hole thermodynamics. For accelerating particles, this manifests as the Unruh effect, which causes space around the particle to appear to be filled with matter and radiation.
According to the controversial black hole firewall hypothesis, matter falling into a black hole would be burned to a crisp by a high energy "firewall" at the event horizon.
An alternative is provided by the complementarity principle, according to which in the chart of the far observer infalling matter is thermalized at the horizon and reemitted as Hawking radiation, while in the chart of an infalling observer matter continues undisturbed through the inner region and is destroyed at the singularity. This hypothesis does not violate the no-cloning theorem as there is a single copy of the information according to any given observer. Black hole complementarity is actually suggested by the scaling laws of strings approaching the event horizon, suggesting that in the Schwarzschild chart they stretch to cover the horizon and thermalize into a Planck length-thick membrane.
A complete description of event horizons is expected to, at minimum, require a theory of quantum gravity. One such candidate theory is M-theory. Another such candidate theory is loop quantum gravity.

</doc>
<doc id="43597" url="https://en.wikipedia.org/wiki?curid=43597" title="Exciton">
Exciton

An exciton is a bound state of an electron and an electron hole which are attracted to each other by the electrostatic Coulomb force. It is an electrically neutral quasiparticle that exists in insulators, semiconductors and in some liquids. The exciton is regarded as an elementary excitation of condensed matter that can transport energy without transporting net electric charge.
An exciton can form when a photon is absorbed by a semiconductor. This excites an electron from the valence band into the conduction band. In turn, this leaves behind a positively charged electron hole (an abstraction for the location from which an electron was moved). The electron in the conduction band is then effectively attracted to this localized hole by the repulsive Coulomb forces from large numbers of electrons surrounding the hole and excited electron. This attraction provides a stabilizing energy balance. Consequently, the exciton has slightly less energy than the unbound electron and hole. The wavefunction of the bound state is said to be "hydrogenic", an exotic atom state akin to that of a hydrogen atom. However, the binding energy is much smaller and the particle's size much larger than a hydrogen atom. This is because of both the screening of the Coulomb force by other electrons in the semiconductor (i.e., its dielectric constant), and the small effective masses of the excited electron and hole. The recombination of the electron and hole, i.e. the decay of the exciton, is limited by resonance stabilization due to the overlap of the electron and hole wave functions, resulting in an extended lifetime for the exciton.
The electron and hole may have either parallel or anti-parallel spins. The spins are coupled by the exchange interaction, giving rise to exciton fine structure. In periodic lattices, the properties of an exciton show momentum (k-vector) dependence.
The concept of excitons was first proposed by Yakov Frenkel in 1931, when he described the excitation of atoms in a lattice of insulators. He proposed that this excited state would be able to travel in a particle-like fashion through the lattice without the net transfer of charge.
Classification.
Excitons may be treated in two limiting cases, depending on the properties of the material in question.
Frenkel excitons.
In materials with a small dielectric constant, the Coulomb interaction between an electron and a hole may be strong and the excitons thus tend to be small, of the same order as the size of the unit cell. Molecular excitons may even be entirely located on the same molecule, as in fullerenes. This "Frenkel exciton", named after Yakov Frenkel, has a typical binding energy on the order of 0.1 to 1 eV. Frenkel excitons are typically found in alkali halide crystals and in organic molecular crystals composed of aromatic molecules, such as anthracene and tetracene.
Wannier-Mott excitons.
In semiconductors, the dielectric constant is generally large. Consequently, electric field screening tends to reduce the Coulomb interaction between electrons and holes. The result is a "Wannier exciton", which has a radius larger than the lattice spacing. Small effective mass of electrons that is typical of semiconductors also favors large exciton radii. As a result, the effect of the lattice potential can be incorporated into the effective masses of the electron and hole. Likewise, because of the lower masses and the screened Coulomb interaction, the binding energy is usually much less than that of a hydrogen atom, typically on the order of . This type of exciton was named for Gregory Wannier and Nevill Francis Mott. Wannier-Mott excitons are typically found in semiconductor crystals with small energy gaps and high dielectric constants, but have also been identified in liquids, such as liquid xenon. They are also known as "large excitons".
In single-wall carbon nanotubes, excitons have both Wannier-Mott and Frenkel character. This is due to the nature of the Coulomb interaction between electrons and holes in one-dimension. The dielectric function of the nanotube itself is large enough to allow for the spatial extent of the wave function to extend over a few to several nanometers along the tube axis, while poor screening in the vacuum or dielectric environment outside of the nanotube allows for large (0.4 to ) binding energies.
Often there is more than one band to choose from for the electron and the hole leading to different types of excitons in the same material. Even high-lying bands can be effective as femtosecond two-photon experiments have shown. At cryogenic temperatures, many higher excitonic levels can be observed approaching the edge of the band, forming a series of spectral absorption lines that are in principle similar to hydrogen spectral series.
Charge-transfer excitons.
An intermediate case between Frenkel and Wannier excitons, "charge-transfer excitons" (sometimes called simply "CT excitons") occur when the electron and the hole occupy adjacent molecules. They occur primarily in ionic crystals. Unlike Frenkel and Wannier excitons they display a static electric dipole moment.
Surface excitons.
At surfaces it is possible for so called "image states" to occur, where the hole is inside the solid and the electron is in the vacuum. These electron-hole pairs can only move along the surface.
Atomic and molecular excitons.
Alternatively, an exciton may be an excited state of an atom, ion, or molecule, the excitation wandering from one cell of the lattice to another.
When a molecule absorbs a quantum of energy that corresponds to a transition from one molecular orbital to another molecular orbital, the resulting electronic excited state is also properly described as an exciton. An electron is said to be found in the lowest unoccupied orbital and an electron hole in the highest occupied molecular orbital, and since they are found within the same molecular orbital manifold, the electron-hole state is said to be bound. Molecular excitons typically have characteristic lifetimes on the order of nanoseconds, after which the ground electronic state is restored and the molecule undergoes photon or phonon emission. Molecular excitons have several interesting properties, one of which is energy transfer (see Förster resonance energy transfer) whereby if a molecular exciton has proper energetic matching to a second molecule's spectral absorbance, then an exciton may transfer ("hop") from one molecule to another. The process is strongly dependent on intermolecular distance between the species in solution, and so the process has found application in sensing and "molecular rulers".
The hallmark of molecular excitons in organic molecular crystals are doublets and/or triplets of exciton absorption bands strongly polarized along crystallographic axes. In these crystals an elementary cell includes several molecules sitting in symmetrically identical positions, which results in the level degeneracy that is lifted by intermolecular interaction. As a result, absorption bands are polarized along the symmetry axes of the crystal. Such multiplets were discovered by Antonina Prikhot'ko and their genesis was proposed by Alexander Davydov. It is known as 'Davydov splitting'.
Giant oscillator strength of bound excitons.
Excitons are lowest excited states of the electronic subsystem of pure crystals. Impurities can bind excitons, and when the bound state is shallow, the oscillator strength for producing bound excitons is so high that impurity absorption can compete with intrinsic exciton absorption even at rather low impurity concentrations. This phenomenon is generic and applicable both to the large radius (Wannier-Mott) excitons and molecular (Frenkel) excitons. Hence, excitons bound to impurities and defects possess giant oscillator strength.
Self-trapping of excitons.
In crystals excitons interact with phonons, the lattice vibrations. If this coupling is weak as in typical semiconductors such as GaAs or Si, excitons are scattered by phonons. However, when the coupling is strong, excitons can be self-trapped. Self-trapping results in dressing excitons with a dense cloud of virtual phonons which strongly suppresses the ability of excitons to move across the crystal. In simpler terms, this means a local deformation of the crystal lattice around the exciton. Self-trapping can be achieved only if the energy of this deformation can compete with the width of the exciton band. Hence, it should be of atomic scale, of about an electron-volt.
Self-trapping of excitons is similar to forming strong-coupling polarons but with three essential differences. First, self-trapped exciton states are always of a small radius, of the order of lattice constant, due to their electric neutrality. Second, there exists a self-trapping barrier separating free and self-trapped states, hence, free excitons are metastable. Third, this barrier enables coexistence of free and self-trapped states of excitons. This means that spectral lines of free excitons and wide bands of self-trapped excitons can be seen simultaneously in absorption and luminescence spectra. It is interesting that while the self-trapped states are of lattice-spacing scale, the barrier has typically much larger scale. Indeed, its spacial scale is about formula_1 where formula_2 is effective mass of the exciton, formula_3 is the exciton-phonon coupling constant, and formula_4 is the characteristic frequency of optical phonons. Excitons are self-trapped when formula_2 and formula_3 are large, and then the spacial size of the barrier is large compared with the lattice spacing. Transforming a free exciton state into a self-trapped one proceeds as a collective tunneling of coupled exciton-lattice system (an instanton). Because formula_7 is large, tunneling can be described by a continuum theory. The height of the barrier formula_8. Because both formula_2 and formula_3 appear in the denominator of formula_11, the barriers are basically low. Therefore, free excitons can be seen in crystals with strong exciton-phonon coupling only in pure samples and at low temperatures. Coexistence of free and self-trapped excitons was observed in rare-gas solids, alkali-halides, and in molecular crystal of pyrene.
Interaction.
Excitons are the main mechanism for light emission in semiconductors at low temperature (when the characteristic thermal energy "kT" is less than the exciton binding energy), replacing the free electron-hole recombination at higher temperatures.
The existence of exciton states may be inferred from the absorption of light associated with their excitation. Typically, excitons are observed just below the band gap.
When excitons interact with photons a so-called polariton (also exciton-polariton) is formed. These excitons are sometimes referred to as "dressed excitons".
Provided the interaction is attractive, an exciton can bind with other excitons to form a biexciton, analogous to a dihydrogen molecule. If a large density of excitons is created in a material, they can interact with one another to form an electron-hole liquid, a state observed in k-space indirect semiconductors.
Additionally, excitons are integer-spin particles obeying Bose statistics in the low-density limit. In some systems, where the interactions are repulsive, a Bose–Einstein condensed state is predicted to be the ground state. Exciton condensates have been seen in a double quantum well systems.
Spatially Direct and Indirect Excitons.
Normally, excitons in a semiconductor have a very short lifetime due to the close proximity of the electron and hole. However, by placing the electron and hole in spatially separated quantum wells with an insulating barrier layer in between so called 'spatially indirect' excitons can be created. In contrast to ordinary (spatially direct), these spatially indirect excitons can have large spatial separation between the electron and hole, and thus possess a much longer lifetime. This is often used to cool excitons to very low temperatures in order to study Bose Einstein condensation (or rather its 2 dimensional analog).

</doc>
<doc id="53217" url="https://en.wikipedia.org/wiki?curid=53217" title="Exotic atom">
Exotic atom

An exotic atom is an otherwise normal atom in which one or more sub-atomic particles have been replaced by other particles of the same charge. For example, electrons may be replaced by other negatively charged particles such as muons (muonic atoms) or pions (pionic atoms). Because these substitute particles are usually unstable, exotic atoms typically have very short lifetimes.
Muonic atoms.
In a "muonic atom" (also called a "mu-mesic" atom), an electron is replaced by a muon, which, like the electron, is a lepton. Since leptons are only sensitive to weak, electromagnetic and gravitational forces, muonic atoms are governed to very high precision by the electromagnetic interaction. The description of these atoms is not complicated by strong forces between the lepton and the nucleus.
Since a muon is more massive than an electron, the Bohr orbits are closer to the nucleus in a muonic atom than in an ordinary atom, and corrections due to quantum electrodynamics are more important. Study of muonic atoms' energy levels as well as transition rates from excited states to the ground state therefore provide experimental tests of quantum electrodynamics.
Muon-catalyzed fusion is a technical application of muonic atoms.
Hadronic atoms.
A "hadronic atom" is an atom in which one or more of the orbital electrons is replaced by a charged hadron. Possible hadrons include mesons such as the pion or kaon, yielding a "mesonic atom"; antiprotons, yielding an "antiprotonic atom"; and the particle, yielding a or "sigmaonic atom".
Unlike leptons, hadrons can interact via the strong force, so the orbitals of hadronic atoms are influenced by nuclear forces between the nucleus and the hadron. Since the strong force is a short-range interaction, these effects are strongest if the atomic orbital involved is close to the nucleus, when the energy levels involved may broaden or disappear because of the absorption of the hadron by the nucleus. Hadronic atoms, such as pionic hydrogen and kaonic hydrogen, thus provide experimental probes of the theory of strong interactions, quantum chromodynamics.
Onium.
An "onium" (plural: "onia" ) is the bound state of a particle and its antiparticle. The classic onium is positronium, which consists of an electron and a positron bound together as a long-lived metastable state. Positronium has been studied since the 1950s to understand bound states in quantum field theory. A recent development called non-relativistic quantum electrodynamics (NRQED) used this system as a proving ground.
Pionium, a bound state of two oppositely-charged pions, is useful for exploring the strong interaction. This should also be true of protonium. The true analogs of positronium in the theory of strong interactions, however, are not exotic atoms but certain mesons, the "quarkonium states", which are made of a heavy quark such as the charm or bottom quark and its antiquark. (Top quarks are so heavy that they decay through the weak force before they can form bound states.) Exploration of these states through non-relativistic quantum chromodynamics (NRQCD) and lattice QCD are increasingly important tests of quantum chromodynamics.
Muonium, despite its name, is "not" an onium containing a muon and an antimuon, because IUPAC assigned that name to the system of an antimuon bound with an electron. However, the production of a muon/antimuon bound state, which is an onium, has been theorized.
Understanding bound states of hadrons such as pionium and protonium is also important in order to clarify notions related to exotic hadrons such as mesonic molecules and pentaquark states.
Hypernuclear atoms.
Atoms may be composed of electrons orbiting a hypernucleus that includes strange particles called hyperons. Such hypernuclear atoms are generally studied for their nuclear behaviour, falling into the realm of nuclear physics rather than atomic physics.
Quasiparticle atoms.
In condensed matter systems, specifically in some semiconductors, there are states called excitons which are bound states of an electron and an electron hole.

</doc>
<doc id="737839" url="https://en.wikipedia.org/wiki?curid=737839" title="Exotic baryon">
Exotic baryon

Exotic baryons are composite particles that are bound states of four or more quarks and additional elementary particles, which may include antiquarks or gluons. An example would be pentaquarks, consisting of four quarks and one antiquark. This is to be contrasted with ordinary baryons, which are bound states of just three quarks.
So far, the only observed exotic baryons are the pentaquarks and , discovered in 2015 by the LHCb collaboration.
Several types of exotic baryons that require physics beyond the Standard Model have been conjectured in order to explain specific experimental anomalies. There is no independent experimental evidence for any of these particles. One example is supersymmetric R-baryons, which are bound states of 3 quarks and a gluino. The lightest R-baryon is denoted as formula_1 and consists of an up quark, a down quark, a strange quark and a gluino. This particle is expected to be long lived or stable and has been invoked to explain ultra-high-energy cosmic rays. Stable exotic baryons are also candidates for strongly interacting dark matter.
It has been speculated by futurologist Ray Kurzweil that by the end of the 21st century it might be possible by using femtotechnology to create new chemical elements composed of exotic baryons that would eventually constitute a new periodic table of elements in which the elements would have completely different properties than the regular chemical elements.

</doc>
<doc id="1982315" url="https://en.wikipedia.org/wiki?curid=1982315" title="Exotic hadron">
Exotic hadron

Exotic hadrons are subatomic particles composed of quarks and gluons, but which do not fit into the usual scheme of hadrons. While bound by the strong interaction they are not predicted by the simple quark model. That is, exotic hadrons do not have the same quark content as ordinary hadrons: exotic baryons have more than just the three quarks of ordinary baryons and exotic mesons do not have one quark and one antiquark like ordinary mesons. Exotic hadrons can be searched for by looking for S-matrix poles with quantum numbers forbidden to ordinary hadrons. Experimental signatures for such exotic hadrons have been seen recently but remain a topic of controversy in particle physics.
Jaffe and Low suggested that the exotic hadrons manifest themselves as poles of the P matrix, and not of the S matrix. Experimental P-matrix poles are determined reliably in both the meson-meson channels and nucleon-nucleon channels.
History.
When the quark model was first postulated by Murray Gell-Mann and others in the 1960s, it was to organize the states known then to be in existence in a meaningful way. As Quantum Chromodynamics (QCD) developed over the next decade, it became apparent that there was no reason why only 3-quark and quark-antiquark combinations could exist. In addition, it seemed that gluons, the mediator particles of the strong interaction, could also form bound states by themselves (glueballs) and with quarks (hybrid hadrons). Several decades have passed without conclusive evidence of an exotic hadron that could be associated with the S-matrix pole.
In April 2014, The LHCb collaboration confirmed the existence of the Z(4430)−. Examinations of the character of the particle suggest that it may be exotic.
Candidates.
There are several exotic hadron candidates:

</doc>
<doc id="71478" url="https://en.wikipedia.org/wiki?curid=71478" title="Exotic matter">
Exotic matter

In physics, exotic matter is matter that somehow deviates from normal matter and has "exotic" properties. A more broad definition of exotic matter is any kind of non-baryonic matter—that is not made of baryons, the subatomic particles, such as protons and neutrons, of which the ordinary matter is composed. Exotic mass has been considered a colloquial term for matters such as dark matter, negative mass, or imaginary mass.
Types of exotic matter.
There are several types of exotic matter:
Negative mass.
Negative mass would possess some strange properties, such as accelerating in the direction opposite of applied force. For example, an object with negative inertial mass and positive electric charge would accelerate away from objects with negative charge, and towards objects with positive charge, the opposite of the normal rule that like charges repel and opposite charges attract. This behaviour can produce bizarre results: for instance, a gas containing a mixture of positive and negative matter particles will have the positive matter portion increase in temperature without bound. However, the negative matter portion gains negative temperature at the same rate, again balancing out.
Despite being inconsistent with the expected behavior of "normal" matter, negative mass is mathematically consistent and introduces no violation of conservation of momentum or energy. It is used in certain speculative theories, such as on the construction of wormholes and the Alcubierre drive. The closest known real representative of such exotic matter is the region of pseudo-negative-pressure density produced by the Casimir effect.
Imaginary mass.
A hypothetical particle with imaginary rest mass would always travel faster than the speed of light. Such particles are called tachyons. There is no confirmed existence of tachyons.
If the rest mass formula_2 is imaginary this implies that the denominator is imaginary because the total energy is an observable and thus must be real. Therefore the quantity under the square root must be negative, which can only happen if "v" is greater than "c". As noted by Gregory Benford "et al.," special relativity implies that tachyons, if they existed, could be used to communicate backwards in time (see tachyonic antitelephone). Because time travel is considered to be non-physical, tachyons are believed by physicists either to not exist, or else to be incapable of interacting with normal matter.
In quantum field theory, imaginary mass would induce tachyon condensation.
Materials at high pressure.
At high pressure, materials such as NaCl in the presence of an excess of either chlorine or sodium were transformed into compounds "forbidden" by classical chemistry, such as and . Quantum mechanical calculations predict the possibility of other compounds, such as , , , and . The materials are thermodynamically stable at high pressures. Such compounds may exist in natural environments that exist at high pressure, such as the deep ocean or inside planetary cores. The materials have potentially useful properties. For instance, is a two-dimensional metal, made of layers of pure sodium and salt that can conduct electricity. The salt layers act as insulators while the sodium layers act as conductors.

</doc>
<doc id="844914" url="https://en.wikipedia.org/wiki?curid=844914" title="Exotic meson">
Exotic meson

Non-quark model mesons include
All of these can be classed as mesons, because they are hadrons and carry zero baryon number. Of these, glueballs must be flavor singlets; that is, have zero isospin, strangeness, charm, bottomness, and topness. Like all particle states, they are specified by the quantum numbers which label representations of the Poincaré symmetry, q.e., "JPC" (where "J" is the angular momentum, "P" is the intrinsic parity, and "C" is the charge conjugation parity) and by the mass. One also specifies the isospin "I" of the meson.
Typically, every quark model meson comes in SU(3) flavor nonet: an octet and a flavor singlet. A glueball shows up as an extra ("supernumerary") particle outside the nonet. In spite of such seemingly simple counting, the assignment of any given state as a glueball, tetraquark, or hybrid remains tentative even today. Even when there is agreement that one of several states is one of these non-quark model mesons, the degree of mixing, and the precise assignment is fraught with uncertainties. There is also the considerable experimental labor of assigning quantum numbers to each state and crosschecking them in other experiments. As a result, all assignments outside the quark model are tentative. The remainder of this article outlines the situation as it stood at the end of 2004.
Lattice predictions.
Lattice QCD predictions for glueballs are now fairly stable, at least when virtual quarks are neglected. The two lowest states are
The 0−+ and exotic glueballs such as 0−− are all expected to lie above . Glueballs are necessarily isoscalar, with isospin "I" = 0.
The ground state "hybrid mesons" 0−+, 1−+, 1−−, and 2−+ all lie a little below . The hybrid with exotic quantum numbers 1−+ is at . The best lattice computations to date are made in the quenched approximation, which neglects virtual quarks loops. As a result, these computations miss mixing with meson states.
The 0++ states.
The data show five isoscalar resonances: f0(500), f0(980), f0(1370), f0(1500), and f0(1710). Of these the f0(500) is usually identified with the "σ" of chiral models. The decays and production of f0(1710) give strong evidence that it is also a meson.
Glueball candidate.
The f0(1370) and f0(1500) cannot both be a quark model meson, because one is supernumerary. The production of the higher mass state in two photon reactions such as or reactions is highly suppressed. The decays also give some evidence that one of these could be a glueball.
Tetraquark candidate.
The f0(980) has been identified by some authors as a tetraquark meson, along with the "I" = 1 states a0(980) and κ0(800). Two long-lived ("narrow" in the jargon of particle spectroscopy) states: the scalar (0++) state (2317) and the vector (1+) meson (2460), observed at CLEO and BaBar, have also been tentatively identified as tetraquark states. However, for these, other explanations are possible.
The 2++ states.
Two isoscalar states are definitely identified—f2(1270) and the f′2(1525). No other states have been consistently identified by all experiments. Hence it is difficult to say more about these states.
The 1−+ exotics and other states.
The two isovector exotics π1(1400) and π1(1600) seem to be well established experimentally. They are clearly not glueballs, but could be either a tetraquark or a hybrid. The evidence for such assignments is weak.
The π(1800) (0−+), ρ(1900) (1−−) and the η2(1870) (2−+) are fairly well identified states, which have been tentatively identified as hybrids by some authors. If this identification is correct, then it is a remarkable agreement with lattice computations, which place several hybrids in this range of masses.

</doc>
<doc id="1579565" url="https://en.wikipedia.org/wiki?curid=1579565" title="Exotic particle">
Exotic particle

An exotic particle is a kind of theoretical particle said to exist by some areas of modern physics, and whose alleged properties are extremely unusual. 
The best-known example is probably the tachyon, a hypothetical particle that always travels faster than light.
Another example of this is supersymmetry, a theoretical set of significantly heavier versions of known particles (this is also one of the proposed candidates for dark matter). 
Exotic particles are frequently featured in science fiction, either those with real-world academic backing, or entirely fictional.

</doc>
<doc id="11371560" url="https://en.wikipedia.org/wiki?curid=11371560" title="Expectation value (quantum mechanics)">
Expectation value (quantum mechanics)

In quantum mechanics, the expectation value is the probabilistic expected value of the result (measurement) of an experiment. It is not the most probable value of a measurement; indeed the expectation value may have zero probability of occurring. It is a fundamental concept in all areas of quantum physics.
Formalism in quantum mechanics.
In quantum theory, an experimental setup is described by the observable formula_1 to be measured, and the state formula_2 of the system. The expectation value of formula_1 in the state formula_2 is denoted as formula_5.
Mathematically, formula_1 is a self-adjoint operator on a Hilbert space. In the most commonly used case in quantum mechanics, formula_2 is a pure state, described by a normalized vector formula_8 in the Hilbert space. The expectation value of formula_1 in the state formula_8 is defined as
(1)      formula_11.
If dynamics is considered, either the vector formula_8 or the operator formula_1 is taken to be time-dependent, depending on whether the Schrödinger picture or Heisenberg picture is used. The evolution of the expectation value does not depend on this choice, however.
If formula_1 has a complete set of eigenvectors formula_15, with eigenvalues formula_16, then (1) can be expressed as
(2)      formula_17.
This expression is similar to the arithmetic mean, and illustrates the physical meaning of the mathematical formalism: The eigenvalues formula_16 are the possible outcomes of the experiment, and their corresponding coefficient formula_19 is the probability that this outcome will occur; it is often called the "transition probability".
A particularly simple case arises when formula_1 is a projection, and thus has only the eigenvalues 0 and 1. This physically corresponds to a "yes-no" type of experiment. In this case, the expectation value is the probability that the experiment results in "1", and it can be computed as
(3)      formula_21.
In quantum theory, also operators with non-discrete spectrum are in use, such as the position operator formula_22 in quantum mechanics. This operator does not have eigenvalues, but has a completely continuous spectrum. In this case, the vector formula_8 can be written as a complex-valued function formula_24 on the spectrum of formula_22 (usually the real line). For the expectation value of the position operator, one then has the formula
(4)      formula_26.
A similar formula holds for the momentum operator formula_27, in systems where it has continuous spectrum.
All the above formulas are valid for pure states formula_2 only. Prominently in thermodynamics, also "mixed states" are of importance; these
are described by a positive trace-class operator formula_29, the "statistical operator" or "density matrix". The expectation value then can be obtained as
(5)      formula_30.
General formulation.
In general, quantum states formula_2 are described by positive normalized linear functionals on the set of observables, mathematically often taken to be a C* algebra. The expectation value of an observable formula_1 is then given by
(6)      formula_33.
If the algebra of observables acts irreducibly on a Hilbert space, and if formula_2 is a "normal functional", that is, it is continuous in the ultraweak topology, then it can be written as
with a positive trace-class operator formula_36 of trace 1. This gives formula (5) above. In the case of a pure state, formula_37 is a projection onto a unit vector formula_8. Then formula_39, which gives formula (1) above.
formula_1 is assumed to be a self-adjoint operator. In the general case, its spectrum will neither be entirely discrete nor entirely continuous. Still, one can write formula_1 in a spectral decomposition,
with a projector-valued measure formula_27. For the expectation value of formula_1 in a pure state formula_45, this means
which may be seen as a common generalization of formulas (2) and (4) above.
In non-relativistic theories of finitely many particles (quantum mechanics, in the strict sense), the states considered are generally normal. However, in other areas of quantum theory, also non-normal states are in use: They appear, for example. in the form of KMS states in quantum statistical mechanics of infinitely extended media, and as charged states in quantum field theory. In these cases, the expectation value is determined only by the more general formula (6).
Example in configuration space.
As an example, let us consider a quantum mechanical particle in one spatial dimension, in the configuration space representation. Here the Hilbert space is formula_47, the space of square-integrable functions on the real line. Vectors formula_48 are represented by functions formula_24, called wave functions. The scalar product is given by formula_50. The wave functions have a direct interpretation as a probability distribution:
gives the probability of finding the particle in an infinitesimal interval of length formula_52 about some point formula_53.
As an observable, consider the position operator formula_22, which acts on wavefunctions formula_8 by
The expectation value, or mean value of measurements, of formula_22 performed on a very large number of "identical" independent systems will be given by
The expectation value only exists if the integral converges, which is not the case for all vectors formula_8. This is because the position operator is unbounded, and formula_8 has to be chosen from its domain of definition.
In general, the expectation of any observable can be calculated by replacing formula_22 with the appropriate operator. For example, to calculate the average momentum, one uses the momentum operator "in configuration space", formula_62. Explicitly, its expectation value is
Not all operators in general provide a measureable value. An operator that has a pure real expectation value is called an observable and its value can be directly measured in experiment.
Further reading.
The expectation value, in particular as presented in the section "Formalism in quantum mechanics", is covered in most elementary textbooks on quantum mechanics.
For a discussion of conceptual aspects, see:

</doc>
<doc id="9653" url="https://en.wikipedia.org/wiki?curid=9653" title="Expected value">
Expected value

In probability theory, the expected value of a random variable is intuitively the long-run average value of repetitions of the experiment it represents. For example, the expected value of a six-sided dice roll is 3.5 because, roughly speaking, the average of an extremely large number of dice rolls is practically always nearly equal to 3.5. Less roughly, the law of large numbers guarantees that the arithmetic mean of the values almost surely converges to the expected value as the number of repetitions goes to infinity. The expected value is also known as the expectation, mathematical expectation, EV, mean, or first moment.
More practically, the expected value of a discrete random variable is the probability-weighted average of all possible values. In other words, each possible value the random variable can assume is multiplied by its probability of occurring, and the resulting products are summed to produce the expected value. The same works for continuous random variables, except the sum is replaced by an integral and the probabilities by probability densities. The formal definition subsumes both of these and also works for distributions which are neither discrete nor continuous: the expected value of a random variable is the integral of the random variable with respect to its probability measure.
The expected value does not exist for random variables having some distributions with large "tails", such as the Cauchy distribution. For random variables such as these, the long-tails of the distribution prevent the sum/integral from converging.
The expected value is a key aspect of how one characterizes a probability distribution; it is one type of location parameter. By contrast, the variance is a measure of dispersion of the possible values of the random variable around the expected value. The variance itself is defined in terms of two expectations: it is the expected value of the squared deviation of the variable's value from the variable's expected value.
The expected value plays important roles in a variety of contexts. In regression analysis, one desires a formula in terms of observed data that will give a "good" estimate of the parameter giving the effect of some explanatory variable upon a dependent variable. The formula will give different estimates using different samples of data, so the estimate it gives is itself a random variable. A formula is typically considered good in this context if it is an unbiased estimator—that is, if the expected value of the estimate (the average value it would give over an arbitrarily large number of separate samples) can be shown to equal the true value of the desired parameter.
In decision theory, and in particular in choice under uncertainty, an agent is described as making an optimal choice in the context of incomplete information. For risk neutral agents, the choice involves using the expected values of uncertain quantities, while for risk averse agents it involves maximizing the expected value of some objective function such as a von Neumann-Morgenstern utility function. One example of using expected value in reaching optimal decisions is the Gordon-Loeb Model of information security investment. According to the model, one can conclude that the amount a firm spends to protect information should generally be only a small fraction of the expected loss (i.e., the expected value of the loss resulting from a cyber/information security breach).
Definition.
Univariate discrete random variable, finite case.
Suppose random variable "X" can take value "x"1 with probability "p"1, value "x"2 with probability "p"2, and so on, up to value "xk" with probability "pk". Then the expectation of this random variable "X" is defined as
Since all probabilities "pi" add up to one ("p"1 + "p"2 + ... + "pk" = 1), the expected value can be viewed as the weighted average, with "pi"’s being the weights:
If all outcomes "xi" are equally likely (that is, "p"1 = "p"2 = ... = "pk"), then the weighted average turns into the simple average. This is intuitive: the expected value of a random variable is the average of all values it can take; thus the expected value is what one expects to happen "on average". If the outcomes "xi" are not equally probable, then the simple average must be replaced with the weighted average, which takes into account the fact that some outcomes are more likely than the others. The intuition however remains the same: the expected value of "X" is what one expects to happen "on average".
Example 1. Let "X" represent the outcome of a roll of a fair six-sided . More specifically, "X" will be the number of pips showing on the top face of the after the toss. The possible values for "X" are 1, 2, 3, 4, 5, and 6, all equally likely (each having the probability of ). The expectation of "X" is
If one rolls the "n" times and computes the average (arithmetic mean) of the results, then as "n" grows, the average will almost surely converge to the expected value, a fact known as the strong law of large numbers. One example sequence of ten rolls of the is 2, 3, 1, 2, 5, 6, 2, 2, 2, 6, which has the average of 3.1, with the distance of 0.4 from the expected value of 3.5. The convergence is relatively slow: the probability that the average falls within the range is 21.6% for ten rolls, 46.1% for a hundred rolls and 93.7% for a thousand rolls. See the figure for an illustration of the averages of longer sequences of rolls of the and how they converge to the expected value of 3.5. More generally, the rate of convergence can be roughly quantified by e.g. Chebyshev's inequality and the Berry-Esseen theorem.
Example 2. The roulette game consists of a small ball and a wheel with 38 numbered pockets around the edge. As the wheel is spun, the ball bounces around randomly until it settles down in one of the pockets. Suppose random variable "X" represents the (monetary) outcome of a $1 bet on a single number ("straight up" bet). If the bet wins (which happens with probability ), the payoff is $35; otherwise the player loses the bet. The expected profit from such a bet will be
Univariate discrete random variable, countable case.
Let "X" be a discrete random variable taking values "x", "x", ... with probabilities "p", "p", ... respectively. Then the expected value of this random variable is the infinite sum
provided that this series converges absolutely (that is, the sum must remain finite if we were to replace all "x"'s with their absolute values). If this series does not converge absolutely, we say that the expected value of "X" does not exist.
For example, suppose random variable "X" takes values 1, −2, 3, −4, ..., with respective probabilities , , , , ..., where is a normalizing constant that ensures the probabilities sum up to one. Then the infinite sum
converges and its sum is equal to . However it would be incorrect to claim that the expected value of "X" is equal to this number—in fact E["X"] does not exist, as this series does not converge absolutely (see harmonic series).
Univariate continuous random variable.
If the probability distribution of formula_7 admits a probability density function formula_8, then the expected value can be computed as
General definition.
In general, if "X" is a random variable defined on a probability space , then the expected value of "X", denoted by E["X"], 〈"X"〉, "X" or E["X"], is defined as the Lebesgue integral 
When this integral exists, it is defined as the expectation of "X". Note that not all random variables have a finite expected value, since the integral may not converge absolutely; furthermore, for some it is not defined at all (e.g., Cauchy distribution). Two variables with the same probability distribution will have the same expected value, if it is defined.
It follows directly from the discrete case definition that if "X" is a constant random variable, i.e. "X" = "b" for some fixed real number "b", then the expected value of "X" is also "b".
The expected value of a measurable function of "X", "g"("X"), given that "X" has a probability density function "f"("x"), is given by the inner product of "f" and "g":
This is sometimes called the law of the unconscious statistician. Using representations as Riemann–Stieltjes integral and integration by parts the formula can be restated as
As a special case let "α" denote a positive real number. Then
In particular, if α = 1 and , then this reduces to
where "F" is the cumulative distribution function of "X". This last identity is an instance of what, in a non-probabilistic setting, has been called the layer cake representation.
The law of the unconscious statistician applies also to a measurable function "g" of several random variables "X"1, ... "X"n having a joint density "f":
Properties.
Constants.
The expected value of a constant is equal to the constant itself; i.e., if "c" is a constant, then .
Monotonicity.
If "X" and "Y" are random variables such that almost surely, then .
Linearity.
The expected value operator (or expectation operator) E is linear in the sense that
Note that the second result is valid even if "X" is not statistically independent of "Y". Combining the results from previous three equations, we can see that
for any two random variables "X" and "Y" (which need to be defined on the same probability space) and any real numbers "a", "b" and "c".
Iterated expectation.
Iterated expectation for discrete random variables.
For any two discrete random variables "X", "Y" one may define the conditional expectation:
which means that E["X"|"Y" = "y"] is a function of "y". Let "g"("y") be that function of "y"; then the notation E["X"|"Y"] is then a random variable in its own right, equal to "g"("Y").
Lemma. Then the expectation of "X" satisfies:
Proof.
The left-hand side of this equation is referred to as the "iterated expectation". The equation is sometimes called the "tower rule" or the "tower property"; it is treated under law of total expectation.
Iterated expectation for continuous random variables.
In the continuous case, the results are completely analogous. The definition of conditional expectation would use inequalities, density functions, and integrals to replace equalities, mass functions, and summations, respectively. However, the main result still holds:
Inequality.
If a random variable "X" is always less than or equal to another random variable "Y", the expectation of "X" is less than or equal to that of "Y":
If , then .
In particular, if we set Y to |"X"| we know "X" ≤ "Y" and −"X" ≤ "Y". Therefore, we know E["X"] ≤ E["Y"] and E[−"X"] ≤ E["Y"]. From the linearity of expectation we know −E["X"] ≤ E["Y"]. Therefore, the absolute value of expectation of a random variable is less than or equal to the expectation of its absolute value:
This is a special case of Jensen's inequality.
Non-multiplicativity.
If one considers the joint probability density function of "X" and "Y", say "j(x,y)", then the expectation of "XY" is
In general, the expected value operator is not multiplicative, i.e. E["XY"] is not necessarily equal to E["X"]·E["Y"]. In fact, the amount by which multiplicativity fails is called the covariance:
Thus multiplicativity holds precisely when , in which case "X" and "Y" are said to be uncorrelated (independent variables are a notable case of uncorrelated variables).
Now if "X" and "Y" are independent, then by definition where "f" and "g" are the marginal PDFs for "X" and "Y". Then
and .
Observe that independence of "X" and "Y" is required only to write "j"("x", "y") = "f"("x")"g"("y"), and this is required to establish the second equality above. The third equality follows from a basic application of the Fubini-Tonelli theorem.
Functional non-invariance.
In general, the expectation operator and functions of random variables do not commute; that is
A notable inequality concerning this topic is Jensen's inequality, involving expected values of convex (or concave) functions.
Relation to characteristic function.
The probability density function of a scalar random variable "X" is related to its characteristic function "φX" by the inversion formula:
We can use this inversion formula in expected value of a function "g(X)" to obtain
Changing the order of integration, we get
where
is the Fourier transform of formula_32 This can also be seen as a direct consequence of Plancherel theorem.
Uses and applications.
It is possible to construct an expected value equal to the probability of an event by taking the expectation of an indicator function that is one if the event has occurred and zero otherwise. This relationship can be used to translate properties of expected values into properties of probabilities, e.g. using the law of large numbers to justify estimating probabilities by frequencies.
The expected values of the powers of "X" are called the moments of "X"; the moments about the mean of "X" are expected values of powers of "X" − E["X"]. The moments of some random variables can be used to specify their distributions, via their moment generating functions.
To empirically estimate the expected value of a random variable, one repeatedly measures observations of the variable and computes the arithmetic mean of the results. If the expected value exists, this procedure estimates the true expected value in an unbiased manner and has the property of minimizing the sum of the squares of the residuals (the sum of the squared differences between the observations and the estimate). The law of large numbers demonstrates (under fairly mild conditions) that, as the size of the sample gets larger, the variance of this estimate gets smaller.
This property is often exploited in a wide variety of applications, including general problems of statistical estimation and machine learning, to estimate (probabilistic) quantities of interest via Monte Carlo methods, since most quantities of interest can be written in terms of expectation, e.g. formula_33 where formula_34 is the indicator function for set formula_35, i.e. formula_36.
 In classical mechanics, the center of mass is an analogous concept to expectation. For example, suppose "X" is a discrete random variable with values "xi" and corresponding probabilities "pi". Now consider a weightless rod on which are placed weights, at locations "xi" along the rod and having masses "pi" (whose sum is one). The point at which the rod balances is E["X"].
Expected values can also be used to compute the variance, by means of the computational formula for the variance
A very important application of the expectation value is in the field of quantum mechanics. The expectation value of a quantum mechanical operator formula_38 operating on a quantum state vector formula_39 is written as formula_40. The uncertainty in formula_38 can be calculated using the formula formula_42.
Expectation of matrices.
If "X" is an "m" × "n" matrix, then the expected value of the matrix is defined as the matrix of expected values:
This is utilized in covariance matrices.
Formulas for special cases.
Discrete distribution taking only non-negative integer values.
When a random variable takes only values in {0, 1, 2, 3, ...} we can use the following formula for computing its expectation (even when the expectation is infinite):
Proof.
Interchanging the order of summation, we have
This result can be a useful computational shortcut. For example, suppose we toss a coin where the probability of heads is "p". How many tosses can we expect until the first heads (not including the heads itself)? Let "X" be this number. Note that we are counting only the tails and not the heads which ends the experiment; in particular, we can have "X" = 0. The expectation of "X" may be computed by formula_47. This is because, when the first "i" tosses yield tails, the number of tosses is at least "i". The last equality used the formula for a geometric progression, formula_48 where formula_49.
Continuous distribution taking non-negative values.
Analogously with the discrete case above, when a continuous random variable "X" takes only non-negative values, we can use the following formula for computing its expectation (even when the expectation is infinite):
Proof: It is first assumed that "X" has a density "fX"("x"). We present two techniques:
and the bracket vanishes because (see Cumulative distribution function#Derived functions)
formula_52 as formula_53
In case no density exists, it is seen that
History.
The idea of the expected value originated in the middle of the 17th century from the study of the so-called problem of points, which seeks to divide the stakes "in a fair way" between two players who have to end their game before it's properly finished. This problem had been debated for centuries, and many conflicting proposals and solutions had been suggested over the years, when it was posed in 1654 to Blaise Pascal by French writer and amateur mathematician Chevalier de Méré. de Méré claimed that this problem couldn't be solved and that it showed just how flawed mathematics was when it came to its application to the real world. Pascal, being a mathematician, was provoked and determined to solve the problem once and for all. He began to discuss the problem in a now famous series of letters to Pierre de Fermat. Soon enough they both independently came up with a solution. They solved the problem in different computational ways but their results were identical because their computations were based on the same fundamental principle. The principle is that the value of a future gain should be directly proportional to the chance of getting it. This principle seemed to have come naturally to both of them. They were very pleased by the fact that they had found essentially the same solution and this in turn made them absolutely convinced they had solved the problem conclusively. However, they did not publish their findings. They only informed a small circle of mutual scientific friends in Paris about it.
Three years later, in 1657, a Dutch mathematician Christiaan Huygens, who had just visited Paris, published a treatise (see ) "De ratiociniis in ludo aleæ" on probability theory. In this book he considered the problem of points and presented a solution based on the same principle as the solutions of Pascal and Fermat. Huygens also extended the concept of expectation by adding rules for how to calculate expectations in more complicated situations than the original problem (e.g., for three or more players). In this sense this book can be seen as the first successful attempt of laying down the foundations of the theory of probability.
In the foreword to his book, Huygens wrote: "It should be said, also, that for some time some of the best mathematicians of France have occupied themselves with this kind of calculus so that no one should attribute to me the honour of the first invention. This does not belong to me. But these savants, although they put each other to the test by proposing to each other many questions difficult to solve, have hidden their methods. I have had therefore to examine and go deeply for myself into this matter by beginning with the elements, and it is impossible for me for this reason to affirm that I have even started from the same principle. But finally I have found that my answers in many cases do not differ from theirs." (cited by ). Thus, Huygens learned about de Méré's Problem in 1655 during his visit to France; later on in 1656 from his correspondence with Carcavi he learned that his method was essentially the same as Pascal's; so that before his book went to press in 1657 he knew about Pascal's priority in this subject.
Neither Pascal nor Huygens used the term "expectation" in its modern sense. In particular, Huygens writes: "That my Chance or Expectation to win any thing is worth just such a Sum, as wou'd procure me in the same Chance and Expectation at a fair Lay. ... If I expect a or b, and have an equal Chance of gaining them, my Expectation is worth ." More than a hundred years later, in 1814, Pierre-Simon Laplace published his tract "Théorie analytique des probabilités", where the concept of expected value was defined explicitly:
The use of the letter "E" to denote expected value goes back to W.A. Whitworth in 1901, who used a script E. The symbol has become popular since for English writers it meant "Expectation", for Germans "Erwartungswert", for Spanish "Esperanza matemática" and for French "Espérance mathématique".

</doc>
<doc id="1934129" url="https://en.wikipedia.org/wiki?curid=1934129" title="Explicit symmetry breaking">
Explicit symmetry breaking

In theoretical physics, explicit symmetry breaking is the breaking of a symmetry of a theory by terms in its defining equations of motion (most typically, to the Lagrangian or the Hamiltonian) that do not respect the symmetry. Usually this term is used in situations where these symmetry-breaking terms are small, so that the symmetry is approximately respected by the theory. An example is the spectral line splitting in the Zeeman effect, due to a magnetic interaction perturbation in the Hamiltonian of the atoms involved.
Explicit symmetry breaking differs from spontaneous symmetry breaking. In the latter, the defining equations respect the symmetry but the ground state (vacuum) of the theory breaks it.

</doc>
<doc id="330320" url="https://en.wikipedia.org/wiki?curid=330320" title="Exponential decay">
Exponential decay

A quantity is subject to exponential decay if it decreases at a rate proportional to its current value. Symbolically, this process can be expressed by the following differential equation, where "N" is the quantity and λ (lambda) is a positive rate called the exponential decay constant:
The solution to this equation (see derivation below) is:
Here "N"("t") is the quantity at time "t", and "N"0 = "N"(0) is the initial quantity, i.e. the quantity at time "t" = 0.
Measuring rates of decay.
Mean lifetime.
If the decaying quantity, "N(t)", is the number of discrete elements in a certain set, it is possible to compute the average length of time that an element remains in the set. This is called the mean lifetime (or simply the lifetime or the exponential time constant), τ, and it can be shown that it relates to the decay rate, λ, in the following way:
The mean lifetime can be looked at as a "scaling time", because we can write the exponential decay equation in terms of the mean lifetime, τ, instead of the decay constant, λ:
We can see that τ is the time at which the population of the assembly is reduced to "1/e = 0.367879441" times its initial value.
For example, if the initial population of the assembly, "N(0)", is 1000, then the population at time τ, "N"(τ), is 368.
A very similar equation will be seen below, which arises when the base of the exponential is chosen to be 2, rather than e. In that case the scaling time is the "half-life".
Half-life.
A more intuitive characteristic of exponential decay for many people is the time required for the decaying quantity to fall to one half of its initial value. This time is called the "half-life", and often denoted by the symbol "t"1/2. The half-life can be written in terms of the decay constant, or the mean lifetime, as:
When this expression is inserted for formula_6 in the exponential equation above, and ln 2 is absorbed into the base, this equation becomes:
Thus, the amount of material left is 2−1 = 1/2 raised to the (whole or fractional) number of half-lives that have passed. Thus, after 3 half-lives there will be 1/23 = 1/8 of the original material left.
Therefore, the mean lifetime formula_6 is equal to the half-life divided by the natural log of 2, or:
E.g. polonium-210 has a half-life of 138 days, and a mean lifetime of 200 days.
Solution of the differential equation.
The equation that describes exponential decay is
or, by rearranging,
Integrating, we have
where C is the constant of integration, and hence
where the final substitution, "N"0 = "e""C", is obtained by evaluating the equation at "t" = 0, as "N"0 is defined as being the quantity at "t" = 0.
This is the form of the equation that is most commonly used to describe exponential decay. Any one of decay constant, mean lifetime, or half-life is sufficient to characterise the decay. The notation λ for the decay constant is a remnant of the usual notation for an eigenvalue. In this case, λ is the eigenvalue of the negative of the differential operator with "N"("t") as the corresponding eigenfunction. The units of the decay constant are s−1.
Derivation of the mean lifetime.
Given an assembly of elements, the number of which decreases ultimately to zero, the mean lifetime, formula_6, (also called simply the lifetime) is the expected value of the amount of time before an object is removed from the assembly. Specifically, if the "individual lifetime" of an element of the assembly is the time elapsed between some reference time and the removal of that element from the assembly, the mean lifetime is the arithmetic mean of the individual lifetimes.
Starting from the population formula
we firstly let "c" be the normalizing factor to convert to a probability density function:
or, on rearranging,
We see that exponential decay is a scalar multiple of the exponential distribution (i.e. the individual lifetime of each object is exponentially distributed), which has a well-known expected value. We can compute it here using integration by parts.
Decay by two or more processes.
A quantity may decay via two or more different processes simultaneously. In general, these processes (often called "decay modes", "decay channels", "decay routes" etc.) have different probabilities of occurring, and thus occur at different rates with different half-lives, in parallel. The total decay rate of the quantity "N" is given by the "sum" of the decay routes; thus, in the case of two processes:
The solution to this equation is given in the previous section, where the sum of formula_20 is treated as a new total decay constant formula_21.
Partial mean life associated with an individual processes is by definition the multiplicative inverse of corresponding partial decay constant: formula_23. A combined formula_24 can be given in terms of formula_25s:
Since half-lives differ from mean life formula_6 by a constant factor, the same equation holds in terms of the two corresponding half-lives:
where formula_30 is the combined or total half-life for the process, formula_31 and formula_32 are so-named partial half-lives of corresponding processes. Terms "partial half-life" and "partial mean life" denote quantities derived from a decay constant as if the given decay mode were the only decay mode for the quantity. The term "partial half-life" is misleading, because it cannot be measured as a time interval for which a certain quantity is halved.
In terms of separate decay constants, the total half-life formula_30 can be shown to be
For a decay by three simultaneous exponential processes the total half-life can be computed as above:
Applications and examples.
Exponential decay occurs in a wide variety of situations. Most of these fall into the domain of the natural sciences.
Many decay processes that are often treated as exponential, are really only exponential so long as the sample is large and the law of large numbers holds. For small samples, a more general analysis is necessary, accounting for a Poisson process.

</doc>
<doc id="45906" url="https://en.wikipedia.org/wiki?curid=45906" title="Exponential distribution">
Exponential distribution

In probability theory and statistics, the exponential distribution (a.k.a. negative exponential distribution) is the probability distribution that describes the time between events in a Poisson process, i.e. a process in which events occur continuously and independently at a constant average rate. It is a particular case of the gamma distribution. It is the continuous analogue of the geometric distribution, and it has the key property of being memoryless. In addition to being used for the analysis of Poisson processes, it is found in various other contexts.
The exponential distribution is not the same as the class of exponential families of distributions, which is a large class of probability distributions that includes the exponential distribution as one of its members, but also includes the normal distribution, binomial distribution, gamma distribution, Poisson, and many others.
Characterization.
Probability density function.
The probability density function (pdf) of an exponential distribution is
Alternatively, this can be defined using the Heaviside step function, "H"("x").
Here λ > 0 is the parameter of the distribution, often called the "rate parameter". The distribution is supported on the interval [0, ∞). If a random variable "X" has this distribution, we write "X" ~ Exp(λ).
The exponential distribution exhibits infinite divisibility.
Cumulative distribution function.
The cumulative distribution function is given by
Alternatively, this can be defined using the Heaviside step function, "H"("x").
Alternative parameterization.
A commonly used alternative parametrization is to define the probability density function (pdf) of an exponential distribution as
where β > 0 is mean, standard deviation, and scale parameter of the distribution, the reciprocal of the "rate parameter", λ, defined above. In this specification, β is a "survival parameter" in the sense that if a random variable "X" is the duration of time that a given biological or mechanical system manages to survive and "X" ~ Exp(β) then E["X"] = β. That is to say, the expected duration of survival of the system is β units of time. The parametrization involving the "rate" parameter arises in the context of events arriving at a rate λ, when the time between events (which might be modeled using an exponential distribution) has a mean of β = λ−1.
The alternative specification is sometimes more convenient than the one given above, and some authors will use it as a standard definition. This alternative specification is not used here. Unfortunately this gives rise to a notational ambiguity. In general, the reader must check which of these two specifications is being used if an author writes ""X" ~ Exp(λ)", since either the notation in the previous (using λ) or the notation in this section (here, using "β" to avoid confusion) could be intended. An example of this notational switch: reference uses λ for β.
Properties.
Mean, variance, moments and median.
The mean or expected value of an exponentially distributed random variable "X" with rate parameter λ is given by
In light of the examples given above, this makes sense: if you receive phone calls at an average rate of 2 per hour, then you can expect to wait half an hour for every call.
The variance of "X" is given by
so the standard deviation is equal to the mean.
The moments of "X", for "n" = 1, 2, ..., are given by
The median of "X" is given by
where ln refers to the natural logarithm. Thus the absolute difference between the mean and median is
in accordance with the .
Memorylessness.
An exponentially distributed random variable "T" obeys the relation
When "T" is interpreted as the waiting time for an event to occur relative to some initial time, this relation implies that, if "T" is conditioned on a failure to observe the event over some initial period of time "s", the distribution of the remaining waiting time is the same as the original unconditional distribution. For example, if an event has not occurred after 30 seconds, the conditional probability that occurrence will take at least 10 more seconds is equal to the unconditional probability of observing the event more than 10 seconds relative to the initial time.
The exponential distribution and the geometric distribution are the only memoryless probability distributions.
The exponential distribution is consequently also necessarily the only continuous probability distribution that has a constant Failure rate.
Quantiles.
The quantile function (inverse cumulative distribution function) for Exp(λ) is
The quartiles are therefore:
And as a consequence the interquartile range is ln(3)/λ.
Kullback–Leibler divergence.
The directed Kullback–Leibler divergence of formula_13 ('approximating' distribution) from formula_14 ('true' distribution) is given by
Maximum entropy distribution.
Among all continuous probability distributions with support formula_16 and mean formula_17, the exponential distribution with formula_18 has the largest differential entropy. In other words, it is the maximum entropy probability distribution for a random variate formula_19 which is greater than or equal to zero and for which formula_20 is fixed.
Distribution of the minimum of exponential random variables.
Let "X"1, ..., "X""n" be independent exponentially distributed random variables with rate parameters λ1, ..., λ"n". Then
is also exponentially distributed, with parameter
This can be seen by considering the complementary cumulative distribution function:
The index of the variable which achieves the minimum is distributed according to the law
Note that
is not exponentially distributed.
Parameter estimation.
Suppose random variable "X" is exponentially distributed with rate parameter λ, and formula_26 are "n" independent samples from "X", with sample mean formula_27. Among the estimators of λ, the maximum likelihood estimator (MLE) is formula_28 and the uniformly minimum variance unbiased estimator (UMVUE) is formula_29. Of the estimators of the form formula_30, the one that minimizes the expected mean squared error is formula_31.
Maximum likelihood.
The likelihood function for λ, given an independent and identically distributed sample "x" = ("x"1, ..., "x""n") drawn from the variable, is:
where:
is the sample mean.
The derivative of the likelihood function's logarithm is:
Consequently the maximum likelihood estimate for the rate parameter is:
Although this is "not" an unbiased estimator of formula_36, formula_37 "is" an unbiased MLE estimator of formula_38 where formula_39 is the scale parameter defined in the 'Alternative parameterization' section above and the distribution mean.
Confidence intervals.
The 100(1 − α)% confidence interval for the rate parameter of an exponential distribution is given by:
which is also equal to:
where is the percentile of the chi squared distribution with "v" degrees of freedom, n is the number of observations of inter-arrival times in the sample, and x-bar is the sample average. A simple approximation to the exact interval endpoints can be derived using a normal approximation to the distribution. This approximation gives the following values for a 95% confidence interval:
This approximation may be acceptable for samples containing at least 15 to 20 elements.
Bayesian inference.
The conjugate prior for the exponential distribution is the gamma distribution (of which the exponential distribution is a special case). The following parameterization of the gamma probability density function is useful:
The posterior distribution "p" can then be expressed in terms of the likelihood function defined above and a gamma prior:
Now the posterior density "p" has been specified up to a missing normalizing constant. Since it has the form of a gamma pdf, this can easily be filled in, and one obtains:
Here the hyperparameter α can be interpreted as the number of prior observations, and β as the sum of the prior observations.
The posterior mean here is:
Generating exponential variates.
A conceptually very simple method for generating exponential variates is based on inverse transform sampling: Given a random variate "U" drawn from the uniform distribution on the unit interval (0, 1), the variate
has an exponential distribution, where "F" −1 is the quantile function, defined by
Moreover, if "U" is uniform on (0, 1), then so is 1 − "U". This means one can generate exponential variates as follows:
Other methods for generating exponential variates are discussed by Knuth and Devroye.
The ziggurat algorithm is a fast method for generating exponential variates.
A fast method for generating a set of ready-ordered exponential variates without using a sorting routine is also available.
Related distributions.
Other related distributions:
Applications of exponential distribution.
Occurrence of events.
The exponential distribution occurs naturally when describing the lengths of the inter-arrival times in a homogeneous Poisson process.
The exponential distribution may be viewed as a continuous counterpart of the geometric distribution, which describes the number of Bernoulli trials necessary for a "discrete" process to change state. In contrast, the exponential distribution describes the time for a continuous process to change state.
In real-world scenarios, the assumption of a constant rate (or probability per unit time) is rarely satisfied. For example, the rate of incoming phone calls differs according to the time of day. But if we focus on a time interval during which the rate is roughly constant, such as from 2 to 4 p.m. during work days, the exponential distribution can be used as a good approximate model for the time until the next phone call arrives. Similar caveats apply to the following examples which yield approximately exponentially distributed variables:
Exponential variables can also be used to model situations where certain events occur with a constant probability per unit length, such as the distance between mutations on a DNA strand, or between roadkills on a given road.
In queuing theory, the service times of agents in a system (e.g. how long it takes for a bank teller etc. to serve a customer) are often modeled as exponentially distributed variables. (The arrival of customers for instance is also modeled by the Poisson distribution if the arrivals are independent and distributed identically.) The length of a process that can be thought of as a sequence of several independent tasks follows the Erlang distribution (which is the distribution of the sum of several independent exponentially distributed variables).
Reliability theory and reliability engineering also make extensive use of the exponential distribution. Because of the "memoryless" property of this distribution, it is well-suited to model the constant hazard rate portion of the bathtub curve used in reliability theory. It is also very convenient because it is so easy to add failure rates in a reliability model. The exponential distribution is however not appropriate to model the overall lifetime of organisms or technical devices, because the "failure rates" here are not constant: more failures occur for very young and for very old systems.
In physics, if you observe a gas at a fixed temperature and pressure in a uniform gravitational field, the heights of the various molecules also follow an approximate exponential distribution, known as the Barometric formula. This is a consequence of the entropy property mentioned below.
In hydrology, the exponential distribution is used to analyze extreme values of such variables as monthly and annual maximum values of daily rainfall and river discharge volumes.
Prediction.
Having observed a sample of "n" data points from an unknown exponential distribution a common task is to use these samples to make predictions about future data from the same source. A common predictive distribution over future samples is the so-called plug-in distribution, formed by plugging a suitable estimate for the rate parameter λ into the exponential density function. A common choice of estimate is the one provided by the principle of maximum likelihood, and using this yields the predictive density over a future sample "x""n"+1, conditioned on the observed samples "x" = ("x"1, ..., "xn") given by
The Bayesian approach provides a predictive distribution which takes into account the uncertainty of the estimated parameter, although this may depend crucially on the choice of prior.
A predictive distribution free of the issues of choosing priors that arise under the subjective Bayesian approach is
which can be considered as
The accuracy of a predictive distribution may be measured using the distance or divergence between the true exponential distribution with rate parameter, λ0, and the predictive distribution based on the sample "x". The Kullback–Leibler divergence is a commonly used, parameterisation free measure of the difference between two distributions. Letting Δ(λ0||"p") denote the Kullback–Leibler divergence between an exponential with rate parameter λ0 and a predictive distribution "p" it can be shown that
where the expectation is taken with respect to the exponential distribution with rate parameter , and is the digamma function. It is clear that the CNML predictive distribution is strictly superior to the maximum likelihood plug-in distribution in terms of average Kullback–Leibler divergence for all sample sizes .

</doc>
<doc id="9678" url="https://en.wikipedia.org/wiki?curid=9678" title="Exponential function">
Exponential function

In mathematics, an exponential function is a function
of the form
The input variable "x" occurs as an exponent – hence the name. A function of the form "ƒ"("x") = "b""x" ± c is also considered an exponential function, and a function of the form "ƒ"("x") = "a"·"b""x" can be re-written as "ƒ"("x") = "b""x" ± c by the use of logarithms and so is an exponential function.
In contexts where the base "b" is not specified, especially in more theoretical contexts, the term exponential function is almost always understood to mean the natural exponential function
also written as
where "e" is Euler's number, a transcendental number approximately 2.718281828. The reason this number "e" is considered the "natural" base of exponential functions is that this function is its own derivative. Every exponential function is directly proportional to its own derivative, but only when the base is "e" does the constant of proportionality equal 1.
The exponential function is used to model a relationship in which a constant change in the independent variable gives the same proportional change (i.e. percentage increase or decrease) in the dependent variable. The function is often written as exp("x"), especially when it is impractical to write the independent variable as a superscript. The exponential function is widely used in physics, chemistry, engineering, mathematical biology, economics and mathematics.
The graph of is upward-sloping, and increases faster as "x" increases. The graph always lies above the "x"-axis but can get arbitrarily close to it for negative "x"; thus, the "x"-axis is a horizontal asymptote. The slope of the tangent to the graph at each point is equal to its "y" coordinate at that point. The inverse function is the natural logarithm ln("x"); because of this, some old texts refer to the exponential function as the antilogarithm.
In general, the variable "x" can be any real or complex number or even an entirely different kind of mathematical object; see the formal definition below.
Formal definition.
The exponential function "e""x" can be characterized in a variety of equivalent ways. In particular it may be defined by the following power series:
Using an alternate definition for the exponential function leads to the same result when expanded as a Taylor series.
Less commonly, "e""x" is defined as the solution "y" to the equation
It is also the following limit:
Overview.
The exponential function arises whenever a quantity grows or decays at a rate proportional to its current value. One such situation is continuously compounded interest, and in fact it was this that led Jacob Bernoulli in 1683 to the number
now known as "e". Later, in 1697, Johann Bernoulli studied the calculus of the exponential function.
If a principal amount of 1 earns interest at an annual rate of "x" compounded monthly, then the interest earned each month is "x"/12 times the current value, so each month the total value is multiplied by (1+"x"/12), and the value at the end of the year is (1+"x"/12)12. If instead interest is compounded daily, this becomes (1+"x"/365)365. Letting the number of time intervals per year grow without bound leads to the limit definition of the exponential function,
first given by Euler.
This is one of a number of characterizations of the exponential function; others involve series or differential equations.
From any of these definitions it can be shown that the exponential function obeys the basic exponentiation identity,
which is why it can be written as "e""x".
The derivative (rate of change) of the exponential function is the exponential function itself. More generally, a function with a rate of change "proportional" to the function itself (rather than equal to it) is expressible in terms of the exponential function. This function property leads to exponential growth and exponential decay.
The exponential function extends to an entire function on the complex plane. Euler's formula relates its values at purely imaginary arguments to trigonometric functions. The exponential function also has analogues for which the argument is a matrix, or even an element of a Banach algebra or a Lie algebra.
Derivatives and differential equations.
The importance of the exponential function in mathematics and the sciences stems mainly from properties of its derivative. In particular,
Proof:
That is, "e""x" is its own derivative and hence is a simple example of a Pfaffian function. Functions of the form "ce""x" for constant "c" are the only functions with that property (by the Picard–Lindelöf theorem). Other ways of saying the same thing include:
If a variable's growth or decay rate is proportional to its size—as is the case in unlimited population growth (see Malthusian catastrophe), continuously compounded interest, or radioactive decay—then the variable can be written as a constant times an exponential function of time. Explicitly for any real constant "k", a function "f": R→R satisfies "f"′ = "kf" if and only if "f"("x") = "ce""kx" for some constant "c".
Furthermore for any differentiable function "f"("x"), we find, by the chain rule:
Continued fractions for "e""x".
A continued fraction for "e""x" can be obtained via an identity of Euler:
The following generalized continued fraction for "e""z" converges more quickly:
or, by applying the substitution "z" = :
with a special case for "z" = 2:
This formula also converges, though more slowly, for "z" > 2. For example:
Complex plane.
As in the real case, the exponential function can be defined on the complex plane in several equivalent forms. One such definition parallels the power series definition for real numbers, where the real variable is replaced by a complex one:
The exponential function is periodic with imaginary period formula_19 and can be written as
where "a" and "b" are real values and on the right the real functions must be used if used as a definition (see also Euler's formula). This formula connects the exponential function with the trigonometric functions and to the hyperbolic functions.
When considered as a function defined on the complex plane, the exponential function retains the properties
for all "z" and "w".
The exponential function is an entire function as it is holomorphic over the whole complex plane. It takes on every complex number excepting 0 as value; that is, 0 is a lacunary value of the exponential function. This is an example of Picard's little theorem that any non-constant entire function takes on every complex number as value with at most one value excepted.
Extending the natural logarithm to complex arguments yields the complex logarithm log "z", which is a multivalued function.
We can then define a more general exponentiation:
for all complex numbers "z" and "w". This is also a multivalued function, even when "z" is real. This distinction is problematic, as the multivalued functions log "z" and "z""w" are easily confused with their single-valued equivalents when substituting a real number for "z". The rule about multiplying exponents for the case of positive real numbers must be modified in a multivalued context:
See failure of power and logarithm identities for more about problems with combining powers.
The exponential function maps any line in the complex plane to a logarithmic spiral in the complex plane with the center at the origin. Two special cases might be noted: when the original line is parallel to the real axis, the resulting spiral never closes in on itself; when the original line is parallel to the imaginary axis, the resulting spiral is a circle of some radius.
Computation of "a""b" where both "a" and "b" are complex.
Complex exponentiation "a""b" can be defined by converting "a" to polar coordinates and using the identity ("e"ln("a"))"b" = "a""b":
However, when "b" is not an integer, this function is multivalued, because "θ" is not unique (see failure of power and logarithm identities).
Matrices and Banach algebras.
The power series definition of the exponential function makes sense for square matrices (for which the function is called the matrix exponential) and more generally in any Banach algebra "B". In this setting, "e"0 = 1, and "e""x" is invertible with inverse "e"−"x" for any "x" in "B". If "xy" ="yx", then "e""x"+"y" = "e""x""e""y", but this identity can fail for noncommuting "x" and "y".
Some alternative definitions lead to the same function. For instance, "e""x" can be defined as
formula_30
Or "e""x" can be defined as "f"(1), where "f": R→"B" is the solution to the differential equation "f"′("t") = "xf"("t") with initial condition "f"(0) = 1.
Lie algebras.
Given a Lie group "G" and its associated Lie algebra formula_31, the exponential map is a map formula_32 satisfying similar properties. In fact, since R is the Lie algebra of the Lie group of all positive real numbers under multiplication, the ordinary exponential function for real arguments is a special case of the Lie algebra situation. Similarly, since the Lie group GL("n",R) of invertible "n" × "n" matrices has as Lie algebra M("n",R), the space of all "n" × "n" matrices, the exponential function for square matrices is a special case of the Lie algebra exponential map.
The identity exp("x" + "y") = exp("x")exp("y") can fail for Lie algebra elements "x" and "y" that do not commute; the Baker–Campbell–Hausdorff formula supplies the necessary correction terms.
Double exponential function.
The term double exponential function can have two meanings:
Factorials grow faster than exponential functions, but slower than double-exponential functions. Fermat numbers, generated by formula_33 and double Mersenne numbers generated by formula_34 are examples of double exponential functions.
Similar properties of "e" and the function "e""z".
The function "e""z" is not in C("z") (i.e., is not the quotient of two polynomials with complex coefficients).
For "n" distinct complex numbers {"a"1, …, "a""n"}, the set {"e""a"1"z", …, "e""a""n""z"} is linearly independent over C("z").
The function "e""z" is transcendental over C("z").
exp and expm1.
Some calculators provide a dedicated exp(x) function designed to provide a higher precision than achievable by using "ex" directly.
Some scientific calculators, computer algebra systems and programming languages (for example C99) support a special exponential minus 1 function alternatively named expm1(x), expm(x), or exp1m(x) to provide more accurate results for values of "x" near zero compared to using "exp(x)-1" directly. This function is implemented using a different internal algorithm to avoid an intermediate result near "1", thereby allowing both the argument and the result to be near zero. Similar inverse functions named "lnp1(x)", "ln1p(x)" or "log1p(x)" exist as well.

</doc>
<doc id="671768" url="https://en.wikipedia.org/wiki?curid=671768" title="Faddeev–Popov ghost">
Faddeev–Popov ghost

In physics, Faddeev–Popov ghosts (also called gauge ghosts or ghost fields) are additional fields which are introduced into gauge quantum field theories to maintain the consistency of the path integral formulation. They are named after Ludvig Faddeev and Victor Popov.
There is also a more general meaning of the word "ghost" in theoretical physics, which is discussed below (see "general ghosts in theoretical physics").
Overcounting in Feynman path integrals.
The necessity for Faddeev–Popov ghosts follows from the requirement that in the path integral formulation, quantum field theories should yield unambiguous, non-singular solutions. This is not possible when a gauge symmetry is present since there is no procedure for selecting any one solution from a range of physically equivalent solutions, all related by a gauge transformation. The problem stems from the path integrals overcounting field configurations related by gauge symmetries, since those correspond to the same physical state; the measure of the path integrals contains a factor which does not allow obtaining various results directly from the original action using the regular methods (e.g., Feynman diagrams). It is possible, however, to modify the action, such that the regular methods will be applicable by adding some additional fields, which break the gauge symmetry, which are called the "ghost fields". This technique is called the "Faddeev–Popov procedure" (see also BRST quantization). The ghost fields are a computational tool in that they do not correspond to any real particles in external states: they "only" appear as virtual particles in Feynman diagrams – or as the "absence" of some gauge configurations. However they are necessary to preserve unitarity.
The exact form or formulation of ghosts is dependent on the particular gauge chosen, although the same physical results are obtained with all the gauges. The Feynman-'t Hooft gauge is usually the simplest gauge for this purpose, and is assumed for the rest of this article.
Spin-statistics relation violated.
The Faddeev–Popov ghosts violate the spin-statistics relation, which is another reason why they are often regarded as "non-physical" particles.
For example, in Yang–Mills theories (such as quantum chromodynamics) the ghosts are complex scalar fields (spin 0), but they anti-commute (like fermions).
In general, anti-commuting ghosts are associated with fermionic symmetries, while commuting ghosts are associated with bosonic symmetries.
Gauge fields and associated ghost fields.
Every gauge field has an associated ghost, and where the gauge field acquires a mass via the Higgs mechanism, the associated ghost field acquires the same mass (in the Feynman-'t Hooft gauge only, not true for other gauges).
Appearance in Feynman diagrams.
In Feynman diagrams the ghosts appear as closed loops wholly composed of 3-vertices, attached to the rest of the diagram via a gauge particle at each 3-vertex. Their contribution to the S-matrix is exactly cancelled (in the Feynman-'t Hooft gauge) by a contribution from a similar loop of gauge particles with only 3-vertex couplings or gauge attachments to the rest of the diagram. (A loop of gauge particles not wholly composed of 3-vertex couplings is not cancelled by ghosts.) The opposite sign of the contribution of the ghost and gauge loops is due to them having opposite fermionic/bosonic natures. (Closed fermion loops have an extra −1 associated with them; bosonic loops don't.)
Ghost field Lagrangian.
The Lagrangian for the ghost fields formula_1 in Yang–Mills theories (where formula_2 is an index in the adjoint representation of the gauge group) is given by
The first term is a kinetic term like for regular complex scalar fields, and the second term describes the interaction with the gauge fields. Note that in "abelian" gauge theories (such as quantum electrodynamics) the ghosts do not have any effect since formula_4 and, consequently, the ghost particles do not interact with the gauge fields.
General ghosts in theoretical physics.
The Faddeev–Popov ghosts are sometimes referred to as "good ghosts". The "bad ghosts" represent another, more general meaning of the word "ghost" in theoretical physics: states of negative norm—or fields with the wrong sign of the kinetic term, such as Pauli–Villars ghosts—whose existence allows the probabilities to be negative thus violating unitarity.
Changing the symmetry.
Ghost particles could obtain the symmetry or break it in gauge fields. The "good ghost" particles actually obtain the symmetry by unchanging the "gauge fixing lagrangian" in a gauge transformation, while bad ghost particles break the symmetry by bringing in the non-abelian G-matrix which does change the symmetry, and this was the main reason to introduce the Gauge covariant and contravariant derivatives.

</doc>
<doc id="151590" url="https://en.wikipedia.org/wiki?curid=151590" title="Faraday cage">
Faraday cage

 
A Faraday cage or Faraday shield is an enclosure formed by conductive material or by a mesh of such material, used to block electric fields. Faraday cages are named after the English scientist Michael Faraday, who invented them in 1836.
A Faraday cage operates because an external electrical field causes the electric charges within the cage's conducting material to be distributed such that they cancel the field's effect in the cage's interior. This phenomenon is used to protect sensitive electronic equipment from external radio frequency interference (RFI). Faraday cages are also used to enclose devices that produce RFI, such as radio transmitters, to prevent their radio waves from interfering with other nearby equipment. They are also used to protect people and equipment against actual electric currents such as lightning strikes and electrostatic discharges, since the enclosing cage conducts current around the outside of the enclosed space and none passes though the interior. 
Faraday cages cannot block static or slowly varying magnetic fields, such as the Earth's magnetic field (a compass will still work inside). To a large degree, though, they shield the interior from external electromagnetic radiation if the conductor is thick enough and any holes are significantly smaller than the wavelength of the radiation. For example, certain computer forensic test procedures of electronic systems that require an environment free of electromagnetic interference can be carried out within a screened room. These rooms are spaces that are completely enclosed by one or more layers of a fine metal mesh or perforated sheet metal. The metal layers are grounded to dissipate any electric currents generated from external or internal electromagnetic fields, and thus they block a large amount of the electromagnetic interference. See also electromagnetic shielding.
The reception or transmission of radio waves, a form of electromagnetic radiation, to or from an antenna within a Faraday cage is heavily attenuated or blocked by the cage.
History.
In 1836, Michael Faraday observed that the excess charge on a charged conductor resided only on its exterior and had no influence on anything enclosed within it. To demonstrate this fact, he built a room coated with metal foil and allowed high-voltage discharges from an electrostatic generator to strike the outside of the room. He used an electroscope to show that there was no electric charge present on the inside of the room's walls.
Although this cage effect has been attributed to Michael Faraday's famous ice pail experiments performed in 1843, it was Benjamin Franklin in 1755 who observed the effect by lowering an uncharged cork ball suspended on a silk thread through an opening in an electrically charged metal can. In his words, "the cork was not attracted to the inside of the can as it would have been to the outside, and though it touched the bottom, yet when drawn out it was not found to be electrified (charged) by that touch, as it would have been by touching the outside. The fact is singular." Franklin had discovered the behavior of what we now refer to as a Faraday cage or shield (based on Faraday's later experiments which duplicated Franklin's cork and can).
Operation.
A Faraday cage is best understood as an approximation to an ideal hollow conductor. Externally or internally applied electromagnetic fields produce forces on the charge carriers (usually electrons) within the conductor; the charges are redistributed accordingly (that is, electric currents are generated). Once the charges have rearranged so as to cancel the applied field inside, the currents stop.
If a charge is placed inside an ungrounded Faraday cage, the internal face of the cage becomes charged (in the same manner described for an external charge) to prevent the existence of a field inside the body of the cage. However, this charging of the inner face re-distributes the charges in the body of the cage. This charges the outer face of the cage with a charge equal in sign and magnitude to the one placed inside the cage. Since the internal charge and the inner face cancel each other out, the spread of charges on the outer face is not affected by the position of the internal charge inside the cage. So for all intents and purposes, the cage generates the same DC electric field that it would generate if it were simply affected by the charge placed inside. The same is not true for electromagnetic waves.
If the cage is grounded, the excess charges will go to the ground instead of the outer face, so the inner face and the inner charge will cancel each other out and the rest of the cage will retain a neutral charge.
Effectiveness of shielding of a static electric field depends upon the geometry of the conductive material. In the case of a nonlinear varying electric field, and hence an accompanying varying magnetic field, the faster the variations are (i.e., the higher the frequencies), the better the material resists penetration, but on the other hand, the better it passes through a mesh of given size. In this case the shielding also depends on the electrical conductivity of the conductive materials used in the cages, as well as their thicknesses.

</doc>
<doc id="742288" url="https://en.wikipedia.org/wiki?curid=742288" title="Faraday's law of induction">
Faraday's law of induction

Faraday's law of induction is a basic law of electromagnetism predicting how a magnetic field will interact with an electric circuit to produce an electromotive force (EMF)—a phenomenon called electromagnetic induction. It is the fundamental operating principle of transformers, inductors, and many types of electrical motors, generators and solenoids.
The Maxwell–Faraday equation is a generalization of Faraday's law, and forms one of Maxwell's equations.
History.
Electromagnetic induction was discovered independently by Michael Faraday in 1831 and Joseph Henry in 1832. Faraday was the first to publish the results of his experiments. In Faraday's first experimental demonstration of electromagnetic induction (August 29, 1831), he wrapped two wires around opposite sides of an iron ring or "torus" (an arrangement similar to a modern toroidal transformer). Based on his assessment of recently discovered properties of electromagnets, he expected that when current started to flow in one wire, a sort of wave would travel through the ring and cause some electrical effect on the opposite side. He plugged one wire into a galvanometer, and watched it as he connected the other wire to a battery. Indeed, he saw a transient current (which he called a "wave of electricity") when he connected the wire to the battery, and another when he disconnected it. This induction was due to the change in magnetic flux that occurred when the battery was connected and disconnected. Within two months, Faraday had found several other manifestations of electromagnetic induction. For example, he saw transient currents when he quickly slid a bar magnet in and out of a coil of wires, and he generated a steady (DC) current by rotating a copper disk near the bar magnet with a sliding electrical lead ("Faraday's disk").
Michael Faraday explained electromagnetic induction using a concept he called lines of force. However, scientists at the time widely rejected his theoretical ideas, mainly because they were not formulated mathematically. An exception was James Clerk Maxwell, who used Faraday's ideas as the basis of his quantitative electromagnetic theory. In Maxwell's papers, the time-varying aspect of electromagnetic induction is expressed as a differential equation which Oliver Heaviside referred to as Faraday's law even though it is different from the original version of Faraday's law, and does not describe motional EMF. Heaviside's version (see Maxwell–Faraday equation below) is the form recognized today in the group of equations known as Maxwell's equations.
Lenz's law, formulated by Heinrich Lenz in 1834, describes "flux through the circuit", and gives the direction of the induced EMF and current resulting from electromagnetic induction (elaborated upon in the examples below).
Faraday's law.
Qualitative statement.
The most widespread version of Faraday's law states:
This version of Faraday's law strictly holds only when the closed circuit is a loop of infinitely thin wire, and is invalid in other circumstances as discussed below. A different version, the Maxwell–Faraday equation (discussed below), is valid in all circumstances.
Quantitative.
Faraday's law of induction makes use of the magnetic flux Φ"B" through a hypothetical surface Σ whose boundary is a wire loop. Since the wire loop may be moving, we write Σ("t") for the surface. The magnetic flux is defined by a surface integral:
where "dA is an element of surface area of the moving surface Σ("t"), B is the magnetic field (also called "magnetic flux density"), and B·"dA is a vector dot product (the infinitesimal amount of magnetic flux through the infinitesimal area element "d"A). In more visual terms, the magnetic flux through the wire loop is proportional to the number of magnetic flux lines that pass through the loop.
When the flux changes—because B changes, or because the wire loop is moved or deformed, or both—Faraday's law of induction says that the wire loop acquires an EMF, formula_2, defined as the energy available from a unit charge that has travelled once around the wire loop. Equivalently, it is the voltage that would be measured by cutting the wire to create an open circuit, and attaching a voltmeter to the leads.
Faraday's law states that the EMF is also given by the rate of change of the magnetic flux:
The surface integral at the right-hand side is the explicit expression for the magnetic flux ΦB through Σ.
Proof of Faraday's law.
The four Maxwell's equations (including the Maxwell–Faraday equation), along with the Lorentz force law, are a sufficient foundation to derive "everything" in classical electromagnetism. Therefore, it is possible to "prove" Faraday's law starting with these equations. Click "show" in the box below for an outline of this proof. (In an alternative approach, not shown here but equally valid, Faraday's law could be taken as the starting point and used to "prove" the Maxwell–Faraday equation and/or other laws.)
"Counterexamples" to Faraday's law.
Although Faraday's law is always true for loops of thin wire, it can give the wrong result if naively extrapolated to other contexts. One example is the homopolar generator (above left): A spinning circular metal disc in a homogeneous magnetic field generates a DC (constant in time) EMF. In Faraday's law, EMF is the time-derivative of flux, so a DC EMF is only possible if the magnetic flux is getting uniformly larger and larger perpetually. But in the generator, the magnetic field is constant and the disc stays in the same position, so no magnetic fluxes are growing larger and larger. So this example cannot be analyzed directly with Faraday's law.
Another example, due to Feynman, has a dramatic change in flux through a circuit, even though the EMF is arbitrarily small. See figure and caption above right.
In both these examples, the changes in the current path are different from the motion of the material making up the circuit. The electrons in a material tend to follow the motion of the atoms that make up the material, due to scattering in the bulk and work function confinement at the edges. Therefore, motional EMF is generated when a material's atoms are moving through a magnetic field, dragging the electrons with them, thus subjecting the electrons to the Lorentz force. In the homopolar generator, the material's atoms are moving, even though the overall geometry of the circuit is staying the same. In the second example, the material's atoms are almost stationary, even though the overall geometry of the circuit is changing dramatically. On the other hand, Faraday's law always holds for thin wires, because there the geometry of the circuit always changes in a direct relationship to the motion of the material's atoms.
Although Faraday's law does not apply to all situations, the Maxwell–Faraday equation and Lorentz force law are always correct and can always be used directly.
Both of the above examples can be correctly worked by choosing the appropriate path of integration for Faraday's law. Outside of context of thin wires, the path must never be chosen to go through the conductor in the shortest direct path. This is explained in detail in "The Electromagnetodynamics of Fluid" by W. F. Hughes and F. J. Young, John Wiley Inc. (1965).
Faraday's law and relativity.
Two phenomena.
Some physicists have remarked that Faraday's law is a single equation describing two different phenomena: the "motional EMF" generated by a magnetic force on a moving wire (see Lorentz force), and the "transformer EMF" generated by an electric force due to a changing magnetic field (due to the Maxwell–Faraday equation).
James Clerk Maxwell drew attention to this fact in his 1861 paper "". In the latter half of Part II of that paper, Maxwell gives a separate physical explanation for each of the two phenomena.
A reference to these two aspects of electromagnetic induction is made in some modern textbooks. As Richard Feynman states:
<br><br>
Einstein's view.
Reflection on this apparent dichotomy was one of the principal paths that led Einstein to develop special relativity:

</doc>
<doc id="265730" url="https://en.wikipedia.org/wiki?curid=265730" title="Femtometre">
Femtometre

The femtometre (American spelling femtometer, symbol fm) (Danish: ', "fifteen"; Ancient Greek: , ', "unit of measurement") is an SI unit of length equal to 10−15 metres. This distance can also be called fermi and was so named in honour of physicist Enrico Fermi, as it is a typical length-scale of nuclear physics.
Definition and equivalents.
1000 attometres = 1 femtometre = 1 fermi = 0.001 picometre = 1.0 × 10−15 metres
1,000,000 femtometres = 10 Ångström = 1 nanometre.
For example, the charge radius of a proton is approximately 0.84–0.87 femtometres while 
the radius of a gold nucleus is approximately 8.45 femtometres.
1 barn = 100 fm2
History.
The femtometre was adopted by the 11th "Conférence Générale des Poids et Mesures", and added to SI in 1964.
The fermi is named after the Italian physicist Enrico Fermi (1901–1954), one of the founders of nuclear physics. The term was coined by Robert Hofstadter in a 1956 paper published in "Reviews of Modern Physics" entitled "Electron Scattering and Nuclear Structure". The term is widely used by nuclear and particle physicists. When Hofstadter was awarded the 1961 Nobel Prize in Physics, it subsequently appears in the text of his 1961 Nobel Lecture, "The electron-scattering method and its application to the structure of nuclei and nucleons" (December 11, 1961).

</doc>
<doc id="922567" url="https://en.wikipedia.org/wiki?curid=922567" title="Fermi's golden rule">
Fermi's golden rule

In quantum physics, Fermi's golden rule is a simple formula for the "constant" transition rate (probability of transition per unit time) from one energy eigenstate of a quantum system into other energy eigenstates in a continuum, effected by a perturbation. This rate is effectively constant.
General.
Although named after Enrico Fermi, most of the work leading to the Golden Rule is due to Paul Dirac who formulated 20 years earlier a virtually identical equation, including the three components of a constant, the matrix element of the perturbation and an energy difference. It was given this name because, on account of its importance, Fermi dubbed it "Golden Rule No. 2." 
The rate and its derivation.
Consider the system to begin in an eigenstate, formula_1, of a given Hamiltonian, 0 . Consider the effect of a (possibly time-dependent) perturbing Hamiltonian, . If is time-independent, the system goes only into those states in the continuum that have the same energy as the initial state. If is oscillating as a function of time with an angular frequency , the transition is into states with energies that differ by from the energy of the initial state.
In both cases, the one-to-many "transition probability per unit of time" from the state formula_2 to a set of final states formula_3 is essentially constant. It is given, to first order in the perturbation, by
where is the density of final states (number of continuum states per unit of energy) and formula_5 is the matrix element (in bra–ket notation) of the perturbation between the final and initial states.
This transition probability is also called "decay probability" and is related to the inverse of the mean lifetime.
Fermi's golden rule is valid when the initial state has not been significantly depleted by scattering into the final states.
The standard way to derive the equation is to start with time-dependent perturbation theory and to take the limit for absorption under the assumption that the time of the measurement is much larger than the time needed for the transition.
The golden rule is a straightforward consequence of the Schrödinger equation, solved to lowest order in the perturbation of the Hamiltonian,
where and are the stationary eigenvalues and eigenfunctions of 0 . 
To lowest order in a constant perturbation which turns on at =0, then,
which integrates to 
for formula_9, for a state with =1, =0, transitioning to a state with . 
The transition rate is then
a sinc function peaking sharply for small , where the variation of the rate with is "linear", for an isolated state formula_11!
By dramatic contrast, for states of energy embedded in a continuum, they must be all accounted for collectively. For a density of states per unit energy interval , they must be integrated over their energies, and whence the corresponding s, 
For large , the sinc function is sharply peaked at ≈ 0, and negligible outside ; the density and transition element can be taken out of the integral, so that the rate
is now merely proportional to a constant Dirichlet integral, . 
"The time dependence has vanished", and the "constant decay rate" of the golden rule follows. As a constant, it underlies the exponential particle decay laws of radioactivity. (For excessively long times, however, the secular growth of the s invalidates lowest-order perturbation theory, which requires .)
Only the magnitude of the matrix element formula_14 enters the Fermi's Golden Rule. The phase of this matrix element, however, contains separate information about the transition process.
It appears in expressions that complement the Golden Rule in the semiclassical Boltzmann equation approach to electron transport.

</doc>
<doc id="701934" url="https://en.wikipedia.org/wiki?curid=701934" title="Fermi's interaction">
Fermi's interaction

In particle physics, Fermi's interaction (also the fermi theory of beta decay) is an explanation of the beta decay, proposed by Enrico Fermi in 1933. The theory posits four fermions directly interacting with one another, at one vertex.<br>
For example, this interaction explains beta decay of a neutron by direct coupling of a neutron with:
Fermi first introduced this coupling in his description of beta decay in 1933. The Fermi interaction was the precursor to the theory for the weak interaction where the interaction between the proton-neutron and electron-antineutrino is mediated by a virtual W− boson.
History of initial rejection and later publication.
Fermi first submitted his "tentative" theory of beta decay to the famous science journal "Nature", which rejected it for being "too speculative." "Nature" later admitted the rejection to be one of the great editorial blunders in its history. Fermi then submitted the paper to Italian and German publications, which accepted and published it in 1933 in those languages, but it did not appear at the time in a primary publication in English ("Nature" finally belatedly republished Fermi's report on beta decay in English on January 16, 1939).
Fermi found the initial rejection of the paper so troubling that he decided to take some time off from theoretical physics, and do only experimental physics. This would lead shortly to his famous work with activation of nuclei with slow neutrons.
The nature of the interaction.
The interaction could also explain muon decay via a coupling of a muon, electron-antineutrino, muon-neutrino and electron, with the same fundamental strength of the interaction. This hypothesis was put forward by Gershtein and Zeldovich and is known as the Conserved Vector Current hypothesis.
Fermi's four-fermion theory describes the weak interaction remarkably well. Unfortunately, the calculated cross-section grows as the square of the energy formula_1, making it unlikely that the theory is valid at energies much higher than about 100 GeV. The solution is to replace the four-fermion contact interaction by a more complete theory (UV completion)—an exchange of a W or Z boson as explained in the electroweak theory.
In the original theory, Fermi assumed that the form of interaction is a contact coupling of two vector currents. Subsequently, it was pointed out by Lee and Yang that nothing prevented the appearance of an axial, parity violating current, and this was confirmed by experiments carried out by Chien-Shiung Wu.
The inclusion of parity violation in Fermi's interaction was done by George Gamow and Edward Teller in the so-called Gamow-Teller Transitions which described Fermi's interaction in terms of Parity violating "allowed" decays and Parity conserving "superallowed" decays in terms of anti-parallel and parallel electron and neutrino spin states respectively. Before the advent of the electroweak theory and the Standard Model, George Sudarshan and Robert Marshak, and also independently Richard Feynman and Murray Gell-Mann, were able to determine the correct tensor structure (vector minus axial vector, ) of the four-fermion interaction.
Fermi constant.
The strength of Fermi's interaction is given by the Fermi coupling constant . The most precise experimental determination of the Fermi constant comes from measurements of the muon lifetime, which is inversely proportional to the square of (when neglecting the muon mass against the mass of the W boson). In modern terms:
formula_2
Here is the coupling constant of the weak interaction, and is the mass of the W boson which mediates the decay in question.
In the Standard Model, Fermi's constant is related to the Higgs vacuum expectation value formula_3

</doc>
<doc id="11529" url="https://en.wikipedia.org/wiki?curid=11529" title="Fermion">
Fermion

In particle physics, a fermion (a name coined by Paul Dirac from the surname of Enrico Fermi) is any particle characterized by Fermi–Dirac statistics. These particles obey the Pauli exclusion principle. Fermions include all quarks and leptons, as well as any composite particle made of an odd number of these, such as all baryons and many atoms and nuclei. Fermions differ from bosons, which obey Bose–Einstein statistics.
A fermion can be an elementary particle, such as the electron, or it can be a composite particle, such as the proton. According to the spin-statistics theorem in any reasonable relativistic quantum field theory, particles with integer spin are bosons, while particles with half-integer spin are fermions.
Besides this spin characteristic, fermions have another specific property: they possess conserved baryon or lepton quantum numbers. Therefore what is usually referred as the spin statistics relation is in fact a spin statistics-quantum number relation.
As a consequence of the Pauli exclusion principle, only one fermion can occupy a particular quantum state at any given time. If multiple fermions have the same spatial probability distribution, then at least one property of each fermion, such as its spin, must be different. Fermions are usually associated with matter, whereas bosons are generally force carrier particles, although in the current state of particle physics the distinction between the two concepts is unclear.
At low temperature fermions show superfluidity for uncharged particles and superconductivity for charged particles.
Composite fermions, such as protons and neutrons, are the key building blocks of everyday matter. Weakly interacting fermions can also display bosonic behavior under extreme conditions, such as superconductivity.
Elementary fermions.
The Standard Model recognizes two types of elementary fermions, quarks and leptons. In all, the model distinguishes 24 different fermions. There are six quarks (up, down, strange, charm, bottom and top quarks), and six leptons (electron, electron neutrino, muon, muon neutrino, tau particle and tau neutrino), along with the corresponding antiparticle of each of these.
Mathematically, fermions come in three types - Weyl fermions (massless), Dirac fermions (massive), and Majorana fermions (each its own antiparticle). Most Standard Model fermions are believed to be Dirac fermions, although it is unknown at this time whether the neutrinos are Dirac or Majorana fermions. Dirac fermions can be treated as a combination of two Weyl fermions. In July 2015, Weyl fermions have been experimentally realized in Weyl semimetals.
Composite fermions.
Composite particles (such as hadrons, nuclei, and atoms) can be bosons or fermions depending on their constituents. More precisely, because of the relation between spin and statistics, a particle containing an odd number of fermions is itself a fermion. It will have half-integer spin.
Examples include the following:
The number of bosons within a composite particle made up of simple particles bound with a potential has no effect on whether it is a boson or a fermion.
Fermionic or bosonic behavior of a composite particle (or system) is only seen at large (compared to size of the system) distances. At proximity, where spatial structure begins to be important, a composite particle (or system) behaves according to its constituent makeup.
Fermions can exhibit bosonic behavior when they become loosely bound in pairs. This is the origin of superconductivity and the superfluidity of helium-3: in superconducting materials, electrons interact through the exchange of phonons, forming Cooper pairs, while in helium-3, Cooper pairs are formed via spin fluctuations.
The quasiparticles of the fractional quantum Hall effect are also known as composite fermions, which are electrons with an even number of quantized vortices attached to them.
Skyrmions.
In a quantum field theory, there can be field configurations of bosons which are topologically twisted. These are coherent states (or solitons) which behave like a particle, and they can be fermionic even if all the constituent particles are bosons. This was discovered by Tony Skyrme in the early 1960s, so fermions made of bosons are named skyrmions after him.
Skyrme's original example involved fields which take values on a three-dimensional sphere, the original nonlinear sigma model which describes the large distance behavior of pions. In Skyrme's model, reproduced in the large N or string approximation to quantum chromodynamics (QCD), the proton and neutron are fermionic topological solitons of the pion field.
Whereas Skyrme's example involved pion physics, there is a much more familiar example in quantum electrodynamics with a magnetic monopole. A bosonic monopole with the smallest possible magnetic charge and a bosonic version of the electron will form a fermionic dyon.
The analogy between the Skyrme field and the Higgs field of the electroweak sector has been used to postulate that all fermions are skyrmions. This could explain why all known fermions have baryon or lepton quantum numbers and provide a physical mechanism for the Pauli exclusion principle.

</doc>
<doc id="159225" url="https://en.wikipedia.org/wiki?curid=159225" title="Fermi–Dirac statistics">
Fermi–Dirac statistics

In quantum statistics, a branch of physics, Fermi–Dirac statistics describes a distribution of particles over energy states in systems consisting of many identical particles that obey the Pauli exclusion principle. It is named after Enrico Fermi and Paul Dirac, each of whom discovered it independently (although Fermi defined the statistics earlier than Dirac).
Fermi–Dirac (F–D) statistics applies to identical particles with half-integer spin in a system in thermodynamic equilibrium. Additionally, the particles in this system are assumed to have negligible mutual interaction. This allows the many-particle system to be described in terms of single-particle energy states. The result is the F–D distribution of particles over these states and includes the condition that no two particles can occupy the same state, which has a considerable effect on the properties of the system. Since F–D statistics applies to particles with half-integer spin, these particles have come to be called fermions. It is most commonly applied to electrons, which are fermions with spin 1/2. Fermi–Dirac statistics is a part of the more general field of statistical mechanics and uses the principles of quantum mechanics.
History.
Before the introduction of Fermi–Dirac statistics in 1926, understanding some aspects of electron behavior was difficult due to seemingly contradictory phenomena. For example, the electronic heat capacity of a metal at room temperature seemed to come from 100 times fewer electrons than were in the electric current. It was also difficult to understand why the emission currents, generated by applying high electric fields to metals at room temperature, were almost independent of temperature.
The difficulty encountered by the electronic theory of metals at that time was due to considering that electrons were (according to classical statistics theory) all equivalent. In other words it was believed that each electron contributed to the specific heat an amount on the order of the Boltzmann constant "k".
This statistical problem remained unsolved until the discovery of F–D statistics.
F–D statistics was first published in 1926 by Enrico Fermi and Paul Dirac. According to an account, Pascual Jordan developed in 1925 the same statistics which he called "Pauli statistics", but it was not published in a timely manner. According to Dirac, it was first studied by Fermi, and Dirac called it Fermi statistics and the corresponding particles fermions.
F–D statistics was applied in 1926 by Fowler to describe the collapse of a star to a white dwarf. In 1927 Sommerfeld applied it to electrons in metals and in 1928 Fowler and Nordheim applied it to field electron emission from metals. Fermi–Dirac statistics continues to be an important part of physics.
Fermi–Dirac distribution.
For a system of identical fermions, the average number of fermions in a single-particle state is given by a logistic function, the Fermi–Dirac (F–D) distribution,
<br>
where is Boltzmann's constant, is the absolute temperature, formula_2 is the energy of the single-particle state , and is the total chemical potential.
At zero temperature, is equal to the Fermi energy plus the potential energy per electron. For the case of electrons in a semiconductor, , the point of symmetry, is typically called the Fermi level or electrochemical potential.
The F–D distribution is only valid if the number of fermions in the system is large enough so that adding one more fermion to the system has negligible effect on . Since the F–D distribution was derived using the Pauli exclusion principle, which allows at most one electron to occupy each possible state, a result is that formula_3 .
Distribution of particles over energy.
The above Fermi–Dirac distribution gives the distribution of identical fermions over single-particle energy states, where no more than one fermion can occupy a state. Using the F–D distribution, one can find the distribution of identical fermions over energy, where more than one fermion can have the same energy.
The average number of fermions with energy formula_2 can be found by multiplying the F–D distribution formula_5 by the degeneracy formula_6 (i.e. the number of states with energy formula_2 ),
<br>
When formula_9, it is possible that formula_10 since there is more than one state that can be occupied by fermions with the same energy formula_11.
When a quasi-continuum of energies formula_12 has an associated density of states formula_13 (i.e. the number of states per unit energy range per unit volume) the average number of fermions per unit energy range per unit volume is,
<br>
where formula_15 is called the Fermi function and is the same function that is used for the F–D distribution formula_16,
so that,
Quantum and classical regimes.
The classical regime, where Maxwell–Boltzmann statistics can be used as an approximation to Fermi–Dirac statistics, is found by considering the situation that is far from the limit imposed by the Heisenberg uncertainty principle for a particle's position and momentum. Using this approach, it can be shown that the classical situation occurs if the concentration of particles corresponds to an average interparticle separation formula_19 that is much greater than the average de Broglie wavelength formula_20 of the particles,
where formula_22 is Planck's constant, and formula_23 is the mass of a particle.
For the case of conduction electrons in a typical metal at "T" = 300K (i.e. approximately room temperature), the system is far from the classical regime because formula_24 . This is due to the small mass of the electron and the high concentration (i.e. small formula_25) of conduction electrons in the metal. Thus Fermi–Dirac statistics is needed for conduction electrons in a typical metal.
Another example of a system that is not in the classical regime is the system that consists of the electrons of a star that has collapsed to a white dwarf. Although the white dwarf's temperature is high (typically "T" = 10,000K on its surface), its high electron concentration and the small mass of each electron precludes using a classical approximation, and again Fermi–Dirac statistics is required.
Derivations of the Fermi–Dirac distribution.
Grand canonical ensemble.
The Fermi–Dirac distribution, which applies only to a quantum system of non-interacting fermions, is easily derived from the grand canonical ensemble. In this ensemble, the system is able to exchange energy and exchange particles with a reservoir (temperature "T" and chemical potential "µ" fixed by the reservoir).
Due to the non-interacting quality, each available single-particle level (with energy level "ϵ") forms a separate thermodynamic system in contact with the reservoir.
In other words, each single-particle level is a separate, tiny grand canonical ensemble.
By the Pauli exclusion principle there are only two possible microstates for the single-particle level: no particle (energy "E"=0), or one particle (energy "E"="ϵ"). The resulting partition function for that single-particle level therefore has just two terms:
and the average particle number for that single-particle substate is given by
This result applies for each single-particle level, and thus gives the Fermi–Dirac distribution for the entire state of the system.
The variance in particle number (due to thermal fluctuations) may also be derived (the particle number has a simple Bernoulli distribution):
This quantity is important in transport phenomena such as the Mott relations for electrical conductivity and thermoelectric coefficient for an electron gas, where the ability of an energy level to contribute to transport phenomena is proportional to formula_29.
Canonical ensemble.
It is also possible to derive Fermi–Dirac statistics in the canonical ensemble. Consider a many-particle system composed of "N" identical fermions that have negligible mutual interaction and are in thermal equilibrium. Since there is negligible interaction between the fermions, the energy formula_30 of a state formula_31 of the many-particle system can be expressed as a sum of single-particle energies,
where formula_33 is called the occupancy number and is the number of particles in the single-particle state formula_34 with energy formula_35. The summation is over all possible single-particle states formula_34.
The probability that the many-particle system is in the state formula_31, is given by the normalized canonical distribution,
where formula_39formula_40,   formula_41 is Boltzmann's constant, formula_42 is the absolute temperature, "e"formula_43 is called the Boltzmann factor, and the summation is over all possible states formula_44 of the many-particle system.   The average value for an occupancy number formula_45 is
Note that the state formula_31 of the many-particle system can be specified by the particle occupancy of the single-particle states, i.e. by specifying formula_48 so that
where the  formula_50 on the summation sign indicates that the sum is not over formula_51 and is subject to the constraint that the total number of particles associated with the summation is formula_52 . Note that formula_53 still depends on formula_51 through the formula_55 constraint, since in one case formula_56 and formula_53 is evaluated with formula_58 while in the other case formula_59 and formula_53 is evaluated with formula_61  To simplify the notation and to clearly indicate that formula_53 still depends on formula_51 through formula_64 , define
so that the previous expression for formula_66 can be rewritten and evaluated in terms of the formula_67,
The following approximation will be used to find an expression to substitute for formula_69 .
where      formula_71
If the number of particles formula_72 is large enough so that the change in the chemical potential formula_73 is very small when a particle is added to the system, then formula_74  Taking the base "e" antilog of both sides, substituting for formula_75, and rearranging,
Substituting the above into the equation for formula_77, and using a previous definition of formula_78 to substitute formula_79 for formula_78, results in the Fermi–Dirac distribution.
formula_81
Microcanonical ensemble.
A result can be achieved by directly analyzing the multiplicities of the system and using Lagrange multipliers.
Suppose we have a number of energy levels, labeled by index "i", each level
having energy ε"i"  and containing a total of "ni"  particles. Suppose each level contains "gi"  distinct sublevels, all of which have the same energy, and which are distinguishable. For example, two particles may have different momenta (i.e. their momenta may be along different directions), in which case they are distinguishable from each other, yet they can still have the same energy. The value of "gi"  associated with level "i" is called the "degeneracy" of that energy level. The Pauli exclusion principle states that only one fermion can occupy any such sublevel.
The number of ways of distributing "ni" indistinguishable particles among the "gi " sublevels of an energy level, with a maximum of one particle per sublevel, is given by the binomial coefficient, using its combinatorial interpretation
For example, distributing two particles in three sublevels will give population numbers of 110, 101, or 011 for a total of three ways which equals 3!/(2!1!). The number of ways that a set of occupation numbers "n""i" can be realized is the product of the ways that each individual energy level can be populated:
Following the same procedure used in deriving the Maxwell–Boltzmann statistics,
we wish to find the set of "ni" for which "W" is maximized, subject to the constraint that there be a fixed number of particles, and a fixed energy. We constrain our solution using Lagrange multipliers forming the function:
Using Stirling's approximation for the factorials, taking the derivative with respect to "ni", setting the result to zero, and solving for "ni" yields the Fermi–Dirac population numbers:
By a process similar to that outlined in the Maxwell–Boltzmann statistics article, it can be shown thermodynamically that formula_86 and formula_87 where formula_88 is the chemical potential, "k" is Boltzmann's constant and "T" is the temperature, so that finally, the probability that a state will be occupied is:

</doc>
<doc id="11617" url="https://en.wikipedia.org/wiki?curid=11617" title="Feynman diagram">
Feynman diagram

In theoretical physics, Feynman diagrams are pictorial representations of the mathematical expressions describing the behavior of subatomic particles. The scheme is named for its inventor, American physicist Richard Feynman, and was first introduced in 1948. The interaction of sub-atomic particles can be complex and difficult to understand intuitively. Feynman diagrams give a simple visualization of what would otherwise be a rather arcane and abstract formula. As David Kaiser writes, "since the middle of the 20th century, theoretical physicists have increasingly turned to this tool to help them undertake critical calculations", and as such "Feynman diagrams have revolutionized nearly every aspect of theoretical physics". While the diagrams are applied primarily to quantum field theory, they can also be used in other fields, such as solid-state theory.
Feynman used Ernst Stueckelberg's interpretation of the positron as if it were an electron moving backward in time. Thus, antiparticles are represented as moving backward along the time axis in Feynman diagrams.
The calculation of probability amplitudes in theoretical particle physics requires the use of rather large and complicated integrals over a large number of variables. These integrals do, however, have a regular structure, and may be represented graphically as Feynman diagrams. A Feynman diagram is a contribution of a particular class of particle paths, which join and split as described by the diagram. More precisely, and technically, a Feynman diagram is a graphical representation of a perturbative contribution to the transition amplitude or correlation function of a quantum mechanical or statistical field theory. Within the canonical formulation of quantum field theory, a Feynman diagram represents a term in the Wick's expansion of the perturbative S-matrix. Alternatively, the path integral formulation of quantum field theory represents the transition amplitude as a weighted sum of all possible histories of the system from the initial to the final state, in terms of either particles or fields. The transition amplitude is then given as the matrix element of the S-matrix between the initial and the final states of the quantum system.
Motivation and history.
When calculating scattering cross-sections in particle physics, the interaction between particles can be described by starting from a free field that describes the incoming and outgoing particles, and including an interaction Hamiltonian to describe how the particles deflect one another. The amplitude for scattering is the sum of each possible interaction history over all possible intermediate particle states. The number of times the interaction Hamiltonian acts is the order of the perturbation expansion, and the time-dependent perturbation theory for fields is known as the Dyson series. When the intermediate states at intermediate times are energy eigenstates (collections of particles with a definite momentum) the series is called old-fashioned perturbation theory.
The Dyson series can be alternatively rewritten as a sum over Feynman diagrams, where at each interaction vertex both the energy and momentum are conserved, but where the length of the energy momentum four vector is not equal to the mass. The Feynman diagrams are much easier to keep track of than old-fashioned terms, because the old-fashioned way treats the particle and antiparticle contributions as separate. Each Feynman diagram is the sum of exponentially many old-fashioned terms, because each internal line can separately represent either a particle or an antiparticle. In a non-relativistic theory, there are no antiparticles and there is no doubling, so each Feynman diagram includes only one term.
Feynman gave a prescription for calculating the amplitude for any given diagram from a field theory Lagrangian—the Feynman rules. Each internal line corresponds to a factor of the corresponding virtual particle's propagator; each vertex where lines meet gives a factor derived from an interaction term in the Lagrangian, and incoming and outgoing lines carry an energy, momentum, and spin.
In addition to their value as a mathematical tool, Feynman diagrams provide deep physical insight into the nature of particle interactions. Particles interact in every way available; in fact, intermediate virtual particles are allowed to propagate faster than light. The probability of each final state is then obtained by summing over all such possibilities. This is closely tied to the functional integral formulation of quantum mechanics, also invented by Feynman–see path integral formulation.
The naïve application of such calculations often produces diagrams whose amplitudes are infinite, because the short-distance particle interactions require a careful limiting procedure, to include particle self-interactions. The technique of renormalization, suggested by Ernst Stueckelberg and Hans Bethe and implemented by Dyson, Feynman, Schwinger, and Tomonaga compensates for this effect and eliminates the troublesome infinities. After renormalization, calculations using Feynman diagrams match experimental results with very high accuracy.
Feynman diagram and path integral methods are also used in statistical mechanics and can even be applied to classical mechanics.
Alternative names.
Murray Gell-Mann always referred to Feynman diagrams as Stueckelberg diagrams, after a Swiss physicist, Ernst Stueckelberg, who devised a similar notation many years earlier. Stueckelberg was motivated by the need for a manifestly covariant formalism for quantum field theory, but did not provide as automated a way to handle symmetry factors and loops, although he was first to find the correct physical interpretation in terms of forward and backward in time particle paths, all without the path-integral. Historically they were sometimes called Feynman–Dyson diagrams or Dyson graphs, because when they were introduced the path integral was unfamiliar, and Freeman Dyson's derivation from old-fashioned perturbation theory was easier to follow for physicists trained in earlier methods. However, in 2006 Dyson himself stated that the diagrams should be called "Feynman diagrams" because "he taught us how to use them". This reflects historical fact: Feynman had to lobby hard for the diagrams which confused the establishment physicists trained in equations and graphs.
Representation of physical reality.
In their presentations of fundamental interactions, written from the particle physics perspective, Gerard 't Hooft and Martinus Veltman gave good arguments for taking the original, non-regularized Feynman diagrams as the most succinct representation of our present knowledge about the physics of quantum scattering of fundamental particles. Their motivations are consistent with the convictions of James Daniel Bjorken and Sidney Drell:
So far there are no opposing opinions. In quantum field theories the Feynman diagrams are obtained from Lagrangian by Feynman rules.
Particle-path interpretation.
A Feynman diagram is a representation of quantum field theory processes in terms of particle paths. The particle trajectories are represented by the lines of the diagram, which can be squiggly or straight, with an arrow or without, depending on the type of particle. A point where lines connect to other lines is an interaction vertex, and this is where the particles meet and interact: by emitting or absorbing new particles, deflecting one another, or changing type.
There are three different types of lines: "internal lines" connect two vertices, "incoming lines" extend from "the past" to a vertex and represent an initial state, and "outgoing lines" extend from a vertex to "the future" and represent the final state. Sometimes, the bottom of the diagram is the past and the top the future; other times, the past is to the left and the future to the right. When calculating correlation functions instead of scattering amplitudes, there is no past and future and all the lines are internal. The particles then begin and end on little x's, which represent the positions of the operators whose correlation is being calculated.
Feynman diagrams are a pictorial representation of a contribution to the total amplitude for a process that can happen in several different ways. When a group of incoming particles are to scatter off each other, the process can be thought of as one where the particles travel over all possible paths, including paths that go backward in time.
Feynman diagrams are often confused with spacetime diagrams and bubble chamber images because they all describe particle scattering. Feynman diagrams are graphs that represent the trajectories of particles in intermediate stages of a scattering process. Unlike a bubble chamber picture, only the sum of all the Feynman diagrams represent any given particle interaction; particles do not choose a particular diagram each time they interact. The law of summation is in accord with the principle of superposition—every diagram contributes to the total amplitude for the process.
Description.
A Feynman diagram represents a perturbative contribution to the amplitude of a quantum transition from some initial quantum state to some final quantum state.
For example, in the process of electron-positron annihilation the initial state is one electron and one positron, the final state: two photons.
The initial state is often assumed to be at the left of the diagram and the final state at the right (although other conventions are also used quite often).
A Feynman diagram consists of points, called vertices, and lines attached to the vertices.
The particles in the initial state are depicted by lines sticking out in the direction of the initial state (e.g., to the left), the particles in the final state are represented by lines sticking out in the direction of the final state (e.g., to the right).
In QED there are two types of particles: electrons/positrons (called fermions) and photons (called gauge bosons). They are represented in Feynman diagrams as follows:
In QED a vertex always has three lines attached to it: one bosonic line, one fermionic line with arrow toward the vertex, and one fermionic line with arrow away from the vertex.
The vertices might be connected by a bosonic or fermionic propagator. A bosonic propagator is represented by a wavy line connecting two vertices (•~•). A fermionic propagator is represented by a solid line (with an arrow in one or another direction) connecting two vertices, (•←•).
The number of vertices gives the order of the term in the perturbation series expansion of the transition amplitude.
Electron/positron annihilation example.
The electron/positron annihilation interaction:
formula_1
has a contribution from the second order Feynman diagram shown adjacent:
In the initial state (at the bottom; early time) there is one electron (e−) and one positron (e+) and in the final state (at the top; late time) there are two photons (γ).
Canonical quantization formulation.
The probability amplitude for a transition of a quantum system from the initial state formula_2 to the final state formula_3 is given by the matrix element
where formula_5 is the S-matrix.
In the canonical quantum field theory the S-matrix is represented within the interaction picture by the perturbation series in the powers of the interaction Lagrangian,
where formula_7 is the interaction Lagrangian and formula_8 signifies the time-ordered product of operators.
A Feynman diagram is a graphical representation of a term in the Wick's expansion of the time-ordered product in the formula_9-th order term formula_10 of the S-matrix,
where formula_12 signifies the normal-product of the operators and formula_13 takes care of the possible sign change when commuting the fermionic operators to bring them together for a contraction (a propagator).
Feynman rules.
The diagrams are drawn according to the Feynman rules, which depend upon the interaction Lagrangian. For the QED interaction Lagrangian, formula_14, describing the interaction of a fermionic field formula_15 with a bosonic gauge field formula_16, the Feynman rules can be formulated in coordinate space as follows:
Example: second order processes in QED.
The second order perturbation term in the S-matrix is
Scattering of fermions.
The Wick's expansion of the integrand gives (among others) the following term
formula_25
where
formula_26
is the electromagnetic contraction (propagator) in the Feynman gauge. This term is represented by the Feynman diagram at the right. This diagram gives contributions to the following processes:
Compton scattering and annihilation/generation of formula_29 pairs.
Another interesting term in the expansion is
where
is the fermionic contraction (propagator).
Path integral formulation.
In a path-integral, the field Lagrangian, integrated over all possible field histories, defines the probability amplitude to go from one field configuration to another. In order to make sense, the field theory should have a well-defined ground state, and the integral should be performed a little bit rotated into imaginary time, i.e. a Wick Rotation.
Scalar field Lagrangian.
A simple example is the free relativistic scalar field in d-dimensions, whose action integral is:
The probability amplitude for a process is:
where A and B are space-like hypersurfaces that define the boundary conditions. The collection of all the formula_35 on the starting hypersurface give the initial value of the field, analogous to the starting position for a point particle, and the field values formula_36 at each point of the final hypersurface defines the final field value, which is allowed to vary, giving a different amplitude to end up at different values. This is the field-to-field transition amplitude.
The path integral gives the expectation value of operators between the initial and final state:
and in the limit that A and B recede to the infinite past and the infinite future, the only contribution that matters is from the ground state (this is only rigorously true if the path-integral is defined slightly rotated into imaginary time). The path integral should be thought of as analogous to a probability distribution, and it is convenient to define it so that multiplying by a constant doesn't change anything:
The normalization factor on the bottom is called the "partition function" for the field, and it coincides with the statistical mechanical partition function at zero temperature when rotated into imaginary time.
The initial-to-final amplitudes are ill-defined if one thinks of the continuum limit right from the beginning, because the fluctuations in the field can become unbounded. So the path-integral should be thought of as on a discrete square lattice, with lattice spacing formula_39 and the limit formula_40 should be taken carefully. If the final results do not depend on the shape of the lattice or the value of a, then the continuum limit exists.
On a lattice.
On a lattice, (i), the field can be expanded in Fourier modes:
Here the integration domain is over k restricted to a cube of side length formula_42, so that large values of k are not allowed. It is important to note that the k-measure contains the factors of formula_43 from Fourier transforms, this is the best standard convention for k-integrals in QFT. The lattice means that fluctuations at large k are not allowed to contribute right away, they only start to contribute in the limit formula_40. Sometimes, instead of a lattice, the field modes are just cut off at high values of k instead.
It is also convenient from time to time to consider the space-time volume to be finite, so that the k modes are also a lattice. This is not strictly as necessary as the space-lattice limit, because interactions in k are not localized, but it is convenient for keeping track of the factors in front of the k-integrals and the momentum-conserving delta functions that will arise.
On a lattice, (ii), the action needs to be discretized:
where formula_46 is a pair of nearest lattice neighbors formula_47 and formula_48. The discretization should be thought of as defining what the derivative formula_49 means.
In terms of the lattice Fourier modes, the action can be written:
For k near zero this is:
Now we have the continuum Fourier transform of the original action. In finite volume, the quantity formula_52 is not infinitesimal, but becomes the volume of a box made by neighboring Fourier modes, or formula_53.
The field formula_54 is real-valued, so the Fourier transform obeys:
In terms of real and imaginary parts, the real part of formula_56 is an even function of k, while the imaginary part is odd. The Fourier transform avoids double-counting, so that it can be written:
over an integration domain that integrates over each pair (k,−k) exactly once.
For a complex scalar field with action
the Fourier transform is unconstrained:
and the integral is over all k.
Integrating over all different values of formula_60 is equivalent to integrating over all Fourier modes, because taking a Fourier transform is a unitary linear transformation of field coordinates. When you change coordinates in a multidimensional integral by a linear transformation, the value of the new integral is given by the determinant of the transformation matrix. If
then
If A is a rotation, then
so that formula_64, and the sign depends on whether the rotation includes a reflection or not.
The matrix that changes coordinates from formula_60 to formula_56 can be read off from the definition of a Fourier transform.
and the Fourier inversion theorem tells you the inverse:
which is the complex conjugate-transpose, up to factors of formula_43. On a finite volume lattice, the determinant is nonzero and independent of the field values.
and the path integral is a separate factor at each value of k.
Dividing by I,
If Wick's theorem were correct, the higher moments would be given by all possible pairings of a list of 2n x's:
where the x's are all the same variable, the index is just to keep track of the number of ways to pair them. The first x can be paired with 2n−1 others, leaving 2n−2. The next unpaired x can be paired with 2n-3 different x's leaving 2n−4, and so on. This means that Wick's theorem, uncorrected, says that the expectation value of formula_75 should be:
and this is in fact the correct answer. So Wick's theorem holds no matter how many of the momenta of the internal variables coincide.
Interaction.
Interactions are represented by higher order contributions, since quadratic contributions are always Gaussian. The simplest interaction is the quartic self-interaction, with an action:
The reason for the combinatorial factor 4! will be clear soon. Writing the action in terms of the lattice (or continuum) Fourier modes:
Where formula_79 is the free action, whose correlation functions are given by Wick's theorem. The exponential of S in the path integral can be expanded in powers of formula_80, giving a series of corrections to the free action.
The path integral for the interacting action is then a power series of corrections to the free action. The term represented by X should be thought of as four half-lines, one for each factor of formula_82. The half-lines meet at a vertex, which contributes a delta-function that ensures that the sum of the momenta are all equal.
To compute a correlation function in the interacting theory, there is a contribution from the X terms now. For example, the path-integral for the four-field correlator:
which in the free field was only nonzero when the momenta k were equal in pairs, is now nonzero for all values of the k. The momenta of the insertions formula_84 can now match up with the momenta of the X's in the expansion. The insertions should also be thought of as half-lines, four in this case, which carry a momentum k, but one that is not integrated.
The lowest order contribution comes from the first nontrivial term formula_85 in the Taylor expansion of the action. Wick's theorem requires that the momenta in the X half-lines, the formula_82 factors in X, should match up with the momenta of the external half-lines in pairs. The new contribution is equal to:
The 4! inside X is canceled because there are exactly 4! ways to match the half-lines in X to the external half-lines. Each of these different ways of matching the half-lines together in pairs contributes exactly once, regardless of the values of the k's, by Wick's theorem.
Feynman diagrams.
The expansion of the action in powers of X gives a series of terms with progressively higher number of X's. The contribution from the term with exactly n X's are called n-th order.
The n-th order terms has:
By Wick's theorem, each pair of half-lines must be paired together to make a "line", and this line gives a factor of
which multiplies the contribution. This means that the two half-lines that make a line are forced to have equal and opposite momentum. The line itself should be labelled by an arrow, drawn parallel to the line, and labeled by the momentum in the line k. The half-line at the tail end of the arrow carries momentum k, while the half-line at the head-end carries momentum −k. If one of the two half-lines is external, this kills the integral over the internal k, since it forces the internal k to be equal to the external k. If both are internal, the integral over k remains.
The diagrams that are formed by linking the half-lines in the X's with the external half-lines, representing insertions, are the Feynman diagrams of this theory. Each line carries a factor of formula_91, the propagator, and either goes from vertex to vertex, or ends at an insertion. If it is internal, it is integrated over. At each vertex, the total incoming k is equal to the total outgoing k.
The number of ways of making a diagram by joining half-lines into lines almost completely cancels the factorial factors coming from the Taylor series of the exponential and the 4! at each vertex.
Loop order.
A forest diagram is one where all the internal lines have momentum that is completely determined by the external lines and the condition that the incoming and outgoing momentum are equal at each vertex. The contribution of these diagrams is a product of propagators, without any integration. A tree diagram is a connected forest diagram.
An example of a tree diagram is the one where each of four external lines end on an X. Another is when three external lines end on an X, and the remaining half-line joins up with another X, and the remaining half-lines of this
X run off to external lines. These are all also forest diagrams (as every tree is a forest); an example of a forest that is not a tree is when eight external lines end on two X's.
It is easy to verify that in all these cases, the momenta on all the internal lines is determined by the external momenta and the condition of momentum conservation in each vertex.
A diagram that is not a forest diagram is called a "loop" diagram, and an example is one where two lines of an X are joined to external lines, while the remaining two lines are joined to each other. The two lines joined to each other can have any momentum at all, since they both enter and leave the same vertex. A more complicated example is one where two X's are joined to each other by matching the legs one to the other. This diagram has no external lines at all.
The reason loop diagrams are called loop diagrams is because the number of k-integrals that are left undetermined by momentum conservation is equal to the number of independent closed loops in the diagram, where independent loops are counted as in homology theory. The homology is real-valued (actually R^d valued), the value associated with each line is the momentum. The boundary operator takes each line to the sum of the end-vertices with a positive sign at the head and a negative sign at the tail. The condition that the momentum is conserved is exactly the condition that the boundary of the k-valued weighted graph is zero.
A set of k-values can be relabeled whenever there is a closed loop going from vertex to vertex, never revisiting the same vertex. Such a cycle can be thought of as the boundary of a 2-cell. The k-labelings of a graph that conserves momentum (which has zero boundary) up to redefinitions of k (up to boundaries of 2-cells) define the first homology of a graph. The number of independent momenta that are not determined is then equal to the number of independent homology loops. For many graphs, this is equal to the number of loops as counted in the most intuitive way.
Symmetry factors.
The number of ways to form a given Feynman diagram by joining together half-lines is large, and by Wick's theorem, each way of pairing up the half-lines contributes equally. Often, this completely cancels the factorials in the denominator of each term, but the cancellation is sometimes incomplete.
The uncancelled denominator is called the "symmetry factor" of the diagram. The contribution of each diagram to the correlation function must be divided by its symmetry factor.
For example, consider the Feynman diagram formed from two external lines joined to one X, and the remaining two half-lines in the X joined to each other. There are 4×3 ways to join the external half-lines to the X, and then there is only one way to join the two remaining lines to each other. The X comes divided by 4!=4×3×2, but the number of ways to link up the X half lines to make the diagram is only 4×3, so the contribution of this diagram is divided by two.
For another example, consider the diagram formed by joining all the half-lines of one X to all the half-lines of another X. This diagram is called a "vacuum bubble", because it does not link up to any external lines. There are 4! ways to form this diagram, but the denominator includes a 2! (from the expansion of the exponential, there are two X's) and two factors of 4!. The contribution is multiplied by 4!/(2×4!×4!) = 1/48.
Another example is the Feynman diagram formed from two X's where each X links up to two external lines, and the remaining two half-lines of each X are joined to each other. The number of ways to link an X to two external lines is 4×3, and either X could link up to either pair, giving an additional factor of 2. The remaining two half-lines in the two X's can be linked to each other in two ways, so that the total number of ways to form the diagram is 4×3×4×3×2×2, while the denominator is 4!×4!×2!. The total symmetry factor is 2, and the contribution of this diagram is divided by 2.
The symmetry factor theorem gives the symmetry factor for a general diagram: the contribution of each Feynman diagram must be divided by the order of its group of automorphisms, the number of symmetries that it has.
An automorphism of a Feynman graph is a permutation M of the lines and a permutation N of the vertices with the following properties:
This theorem has an interpretation in terms of particle-paths: when identical particles are present, the integral over all intermediate particles must not double-count states that differ only by interchanging identical particles.
Proof: To prove this theorem, label all the internal and external lines of a diagram with a unique name. Then form the diagram by linking the a half-line to a name and then to the other half line.
Now count the number of ways to form the named diagram. Each permutation of the X's gives a different pattern of linking names to half-lines, and this is a factor of n!. Each permutation of the half-lines in a single X gives a factor of 4!. So a named diagram can be formed in exactly as many ways as the denominator of the Feynman expansion.
But the number of unnamed diagrams is smaller than the number of named diagram by the order of the automorphism group of the graph.
Connected diagrams: "linked-cluster theorem".
Roughly speaking, a Feynman diagram is called "connected" if all vertices and propagator lines are linked by a sequence of vertices and propagators of the diagram itself. If one views it as a (undirected) graph it is connected. The remarkable relevance of such diagrams in QFTs is due to the fact that they are sufficient to determine the quantum partition function formula_92. More precisely, connected Feynman diagrams determine
To see this, one should recall that
with formula_95 constructed from some (arbitrary) Feynman diagram that can be thought to consist of several connected components formula_96. If one encounters formula_97 (identical) copies of a component formula_96 within the Feynman diagram formula_95 one has to include a "symmetry factor" formula_100. However, in the end each contribution of a Feynman diagram formula_95 to the partition function has the generic form
where formula_103 labels the (infinite) many connected Feynman diagrams possible.
A scheme to successively create such contributions from the formula_95 to formula_105 is obtained by
and therefore yields
To establish the "normalization" formula_108 one simply calculates all connected "vacuum diagrams", i.e., the diagrams without any "sources" formula_109 (sometimes referred to as "external legs" of a Feynman diagram).
Vacuum bubbles.
An immediate consequence of the linked-cluster theorem is that all vacuum bubbles, diagrams without external lines, cancel when calculating correlation functions. A correlation function is given by a ratio of path-integrals:
The top is the sum over all Feynman diagrams, including disconnected diagrams that do not link up to external lines at all. In terms of the connected diagrams, the numerator includes the same contributions of vacuum bubbles as the denominator:
Where the sum over E diagrams includes only those diagrams each of whose connected components end on at least one external line. The vacuum bubbles are the same whatever the external lines, and give an overall multiplicative factor. The denominator is the sum over all vacuum bubbles, and dividing gets rid of the second factor.
The vacuum bubbles then are only useful for determining Z itself, which from the definition of the path integral is equal to:
where formula_113 is the energy density in the vacuum. Each vacuum bubble contains a factor of formula_114 zeroing the total k at each vertex, and when there are no external lines, this contains a factor of formula_115, because the momentum conservation is over-enforced. In finite volume, this factor can be identified as the total volume of space time. Dividing by the volume, the remaining integral for the vacuum bubble has an interpretation: it is a contribution to the energy density of the vacuum.
Sources.
Correlation functions are the sum of the connected Feynman diagrams, but the formalism treats the connected and disconnected diagrams differently. Internal lines end on vertices, while external lines go off to insertions. Introducing "sources" unifies the formalism, by making new vertices where one line can end.
Sources are external fields, fields that contribute to the action, but are not dynamical variables. A scalar field source is another scalar field h that contributes a term to the (Lorentz) Lagrangian:
In the Feynman expansion, this contributes H terms with one half-line ending on a vertex. Lines in a Feynman diagram can now end either on an X vertex, or on an H-vertex, and only one line enters an H vertex. The Feynman rule for an H-vertex is that a line from an H with momentum k gets a factor of h(k).
The sum of the connected diagrams in the presence of sources includes a term for each connected diagram in the absence of sources, except now the diagrams can end on the source. Traditionally, a source is represented by a little "x" with one line extending out, exactly as an insertion.
where formula_118 is the connected diagram with n external lines carrying momentum as indicated. The sum is over all connected diagrams, as before.
The field h is not dynamical, which means that there is no path integral over h: h is just a parameter in the Lagrangian, which varies from point to point. The path integral for the field is:
and it is a function of the values of h at every point. One way to interpret this expression is that it is taking the Fourier transform in field space. If there is a probability density on R^n, the Fourier transform of the probability density is:
The Fourier transform is the expectation of an oscillatory exponential. The path integral in the presence of a source h(x) is:
which, on a lattice, is the product of an oscillatory exponential for each field value:
The fourier transform of a delta-function is a constant, which gives a formal expression for a delta function:
This tells you what a field delta function looks like in a path-integral. For two scalar fields formula_124 and formula_125,
Which integrates over the Fourier transform coordinate, over h. This expression is useful for formally changing field coordinates in the path integral, much as a delta function is used to change coordinates in an ordinary multi-dimensional integral.
The partition function is now a function of the field h, and the physical partition function is the value when h is the zero function:
The correlation functions are derivatives of the path integral with respect to the source:
In Euclidean space, source contributions to the action can still appear with a factor of "i", so that they still do a Fourier transform.
Spin 1/2; "photons" and "ghosts".
Spin 1/2: Grassmann integrals.
The field path-integral can be extended to the Fermi case, but only if the notion of integration is expanded. A Grassmann integral of a free Fermi field is a high-dimensional determinant or Pfaffian, which defines the new type of Gaussian integration appropriate for Fermi fields.
The two fundamental formulas of Grassmann integration are:
where M is an arbitrary matrix and formula_129 are independent Grassmann variables for each index i, and
where S is the sign of the permutation that reorders the sequence of psi-bars and psis to put the ones that are paired up to make the delta-functions next to each other, with the psi-bar coming right before the psi. Since a psi-psi-bar pair is a commuting element of the Grassmann algebra, it doesn't matter what order the pairs are in. If more than one psi/psi-bar pair have the same k, the integral is zero, and it is easy to check that the sum over pairings gives zero in this case (there are always an even number of them). This is the Grassmann analog of the higher Gaussian moments that completed the Bosonic Wick's theorem earlier.
The rules for spin-1/2 Dirac particles are as follows: The propagator is the inverse of the Dirac operator, the lines have arrows just as for a complex scalar field, and the diagram acquires an overall factor of −1 for each closed Fermi loop. If there are an odd number of Fermi loops, the diagram changes sign. Historically, the −1 rule was very difficult for Feynman to discover. He discovered it after a long process of trial and error, since he lacked a proper theory of Grassmann integration.
The rule follows from the observation that the number of Fermi lines at a vertex is always even. Each term in the Lagrangian must always be Bosonic. A Fermi loop is counted by following Fermionic lines until one comes back to the starting point, then removing those lines from the diagram. Repeating this process eventually erases all the Fermionic lines: this is the Euler algorithm to 2-color a graph, which works whenever each vertex has even degree. Note that the number of steps in the Euler algorithm is only equal to the number of independent Fermionic homology cycles in the common special case that all terms in the Lagrangian are exactly quadratic in the Fermi fields, so that each vertex has exactly two Fermionic lines. When there are four-Fermi interactions (like in the Fermi effective theory of the Weak interactions) there are more k-integrals than Fermi loops. In this case, the counting rule should apply the Euler algorithm by pairing up the Fermi lines at each vertex into pairs that together form a bosonic factor of the term in the Lagrangian, and when entering a vertex by one line, the algorithm should always leave with the partner line.
To clarify and prove the rule, consider a Feynman diagram formed from vertices, terms in the Lagrangian, with Fermion fields. The full term is Bosonic, it is a commuting element of the Grassmann algebra, so the order in which the vertices appear is not important. The Fermi lines are linked into loops, and when traversing the loop, one can reorder the vertex terms one after the other as one goes around without any sign cost. The exception is when you return to the starting point, and the final half-line must be joined with the unlinked first half-line. This requires one permutation to move the last psi-bar to go in front of the first psi, and this gives the sign.
This rule is the only visible effect of the exclusion principle in internal lines. When there are external lines, the amplitudes are antisymmetric when two Fermi insertions for identical particles are interchanged. This is automatic in the source formalism, because the sources for Fermi fields are themselves Grassmann valued.
Spin 1: photons.
The naive propagator for photons is infinite, since the Lagrangian for the A-field is:
The quadratic form defining the propagator is non-invertible. The reason is the gauge invariance of the field, adding a gradient to A does not change the physics.
To fix this problem, one needs to fix a gauge. The most convenient way is to demand that the divergence of A is some function f, whose value is random from point to point. It does no harm to integrate over the values of f, since it only determines the choice of gauge. This procedure inserts the following factor into the path integral for A:
The first factor, the delta function, fixes the gauge. The second factor sums over different values of f that are inequivalent gauge fixings. This is simply
The additional contribution from gauge-fixing cancels the second half of the free Lagrangian, giving the Feynman Lagrangian:
which is just like four independent free scalar fields, one for each component of A. The Feynman propagator is:
The one difference is that the sign of one propagator is wrong in the Lorentz case: the timelike component has an opposite sign propagator. This means that these particle states have negative norm—they are not physical states. In the case of photons, it is easy to show by diagram methods that these states are not physical—their contribution cancels with longitudinal photons to only leave two physical photon polarization contributions for any value of k.
If the averaging over f is done with a coefficient different from 1/2, the two terms don't cancel completely. This gives a covariant Lagrangian with a coefficient formula_80, which does not affect anything:
and the covariant propagator for QED is:
Spin 1: nonabelian ghosts.
To find the Feynman rules for nonabelian Gauge fields, the procedure that performs the Gauge fixing must be carefully corrected to account for a change of variables in the path-integral.
The gauge fixing factor has an extra determinant from popping the delta function:
To find the form of the determinant, consider first a simple two-dimensional integral of a function f that depends only on r, not on the angle formula_140. Inserting an integral over theta:
The derivative-factor ensures that popping the delta function in formula_140 removes the integral. Exchanging the order of integration,
but now the delta-function can be popped in y,
The integral over formula_145 just gives an overall factor of formula_146, while the rate of change of formula_147 with a change in formula_148 is just x, so this exercise reproduces the standard formula for polar integration of a radial function:
In the path-integral for a nonabelian gauge field, the analogous manipulation is:
The factor in front is the volume of the gauge group, and it contributes a constant, which can be discarded. The remaining integral is over the gauge fixed action.
To get a covariant gauge, the gauge fixing condition is the same as in the Abelian case:
Whose variation under an infinitesimal gauge transformation is given by:
where formula_154 is the adjoint valued element of the Lie algebra at every point that performs the infinitesimal gauge transformation. This adds the Faddeev Popov determinant to the action:
which can be rewritten as a Grassman integral by introducing ghost fields:
The determinant is independent of f, so the path-integral over f can give the Feynman propagator (or a covariant propagator) by choosing the measure for f as in the abelian case. The full gauge fixed action is then the Yang Mills action in Feynman gauge with an additional ghost action:
The diagrams are derived from this action. The propagator for the spin-1 fields has the usual Feynman form. There are vertices of degree 3 with momentum factors whose couplings are the structure constants, and vertices of degree 4 whose couplings are products of structure constants. There are additional ghost loops, which cancel out timelike and longitudinal states in A loops.
In the Abelian case, the determinant for covariant gauges does not depend on A, so the ghosts do not contribute to the connected diagrams.
Particle-path representation.
Feynman diagrams were originally discovered by Feynman, by trial and error, as a way to represent the contribution to the S-matrix from different classes of particle trajectories.
Schwinger representation.
The Euclidean scalar propagator has a suggestive representation:
The meaning of this identity (which is an elementary integration) is made clearer by Fourier transforming to real space.
The contribution at any one value of formula_160 to the propagator is a Gaussian of width formula_161. The total propagation function from 0 to x is a weighted sum over all proper times formula_160 of a normalized Gaussian, the probability of ending up at x after a random walk of time formula_160.
The path-integral representation for the propagator is then:
which is a path-integral rewrite of the Schwinger representation.
The Schwinger representation is both useful for making manifest the particle aspect of the propagator, and for symmetrizing denominators of loop diagrams.
Combining denominators.
The Schwinger representation has an immediate practical application to loop diagrams. For example, For the diagram in the phi-4 theory formed by joining two x's together in two half-lines, and making the remaining lines external, the integral over the internal propagators in the loop is:
Here one line carries momentum k and the other k+p. The asymmetry can be fixed by putting everything in the Schwinger representation.
Now the exponent mostly depends on t+t',
except for the asymmetrical little bit. Defining the variable u=(t+t') and formula_168= t'/u, the variable u goes from 0 to infinity, while formula_168 goes from 0 to 1. The variable u is the total proper time for the loop, while formula_170 parametrizes the fraction of the proper time on the top of the loop vs. the bottom.
The Jacobian for this transformation of variables is easy to work out from the identities:
and "wedging" gives
This allows the u integral to be evaluated explicitly:
leaving only the formula_174-integral. This method, invented by Schwinger but usually attributed to Feynman, is called "combining denominator". Abstractly, it is the elementary identity:
But this form does not provide the physical motivation for introducing formula_174—formula_174 is the proportion of proper time on one of the legs of the loop.
Once the denominators are combined, a shift in k to formula_178 symmetrizes everything:
This form shows that the moment that p2 is more negative than 4 times the mass of the particle in the loop, which happens in a physical region of Lorentz space, the integral has a cut. This is exactly when the external momentum can create physical particles.
When the loop has more vertices, there are more denominators to combine:
The general rule follows from the Schwinger prescription for n+1 denominators:
The integral over the Schwinger parameters formula_182 can be split up as before into an integral over the total proper time formula_183 and an integral over the fraction of the proper time in all but the first segment of the loop formula_184 for formula_185. The v's are positive and add up to less than 1, so that the v integral is over an n dimensional simplex.
The Jacobian for the coordinate transformation can be worked out as before:
"Wedging" all these equation together, one obtains
This gives the integral:
where the simplex is the region defined by the conditions formula_190 and formula_191 as well as formula_192. Performing the u integral gives the general prescription for combining denominators:
Since the numerator of the integrand is not involved, the same prescription works for any loop, no matter what the spins are carried by the legs. The interpretation of the parameters formula_194 is that they are the fraction of the total proper time spent on each leg.
Scattering.
The correlation functions of a quantum field theory describe the scattering of particles. The definition of "particle" in relativistic field theory is not self-evident, because if you try to determine the position so that the uncertainty is less than the compton wavelength, the uncertainty in energy is large enough to produce more particles and antiparticles of the same type from the vacuum. This means that the notion of a single-particle state is to some extent incompatible with the notion of an object localized in space.
In the 1930s, Wigner gave a mathematical definition for single-particle states: they are a collection of states that form an irreducible representation of the Poincaré group. Single particle states describe an object with a finite mass, a well defined momentum, and a spin. This definition is fine for protons and neutrons, electrons and photons, but it excludes quarks, which are permanently confined, so the modern point of view is more accommodating: a particle is anything whose interaction can be described in terms of Feynman diagrams, which have an interpretation as a sum over particle trajectories.
A field operator can act to produce a one-particle state from the vacuum, which means that the field operator formula_195 produces a superposition of Wigner particle states. In the free field theory, the field produces one particle states only. But when there are interactions, the field operator can also produce 3-particle, 5-particle (if there is no +/− symmetry also 2, 4, 6 particle) states too. To compute the scattering amplitude for single particle states only requires a careful limit, sending the fields to infinity and integrating over space to get rid of the higher-order corrections.
The relation between scattering and correlation functions is the LSZ-theorem: The scattering amplitude for n particles to go to m-particles in a scattering event is the given by the sum of the Feynman diagrams that go into the correlation function for n+m field insertions, leaving out the propagators for the external legs.
For example, for the formula_196 interaction of the previous section, the order formula_80 contribution to the (Lorentz) correlation function is:
Stripping off the external propagators, that is, removing the factors of formula_199, gives the invariant scattering amplitude M:
which is a constant, independent of the incoming and outgoing momentum. The interpretation of the scattering amplitude is that the sum of formula_201 over all possible final states is the probability for the scattering event. The normalization of the single-particle states must be chosen carefully, however, to ensure that M is a relativistic invariant.
Non-relativistic single particle states are labeled by the momentum k, and they are chosen to have the same norm at every value of k. This is because the nonrelativistic unit operator on single particle states is:
In relativity, the integral over the k-states for a particle of mass m integrates over a hyperbola in E,k space defined by the energy–momentum relation:
If the integral weighs each k point equally, the measure is not Lorentz invariant. The invariant measure integrates over all values of k and E, restricting to the hyperbola with a Lorentz invariant delta function:
So the normalized k-states are different from the relativistically normalized k-states by a factor of formula_205
The invariant amplitude M is then the probability amplitude for relativistically normalized incoming states to become relativistically normalized outgoing states.
For nonrelativistic values of k, the relativistic normalization is the same as the nonrelativistic normalization (up to a constant factor formula_206 ). In this limit, the formula_207 invariant scattering amplitude is still constant. The particles created by the field phi scatter in all directions with equal amplitude.
The nonrelativistic potential, which scatters in all directions with an equal amplitude (in the Born approximation), is one whose Fourier transform is constant—a delta-function potential. The lowest order scattering of the theory reveals the non-relativistic interpretation of this theory—it describes a collection of particles with a delta-function repulsion. Two such particles have an aversion to occupying the same point at the same time.
Nonperturbative effects.
Thinking of Feynman diagrams as a perturbation series, nonperturbative effects like tunneling do not show up, because any effect that goes to zero faster than any polynomial does not affect the Taylor series. Even bound states are absent, since at any finite order particles are only exchanged a finite number of times, and to make a bound state, the binding force must last forever.
But this point of view is misleading, because the diagrams not only describe scattering, but they also are a representation of the short-distance field theory correlations. They encode not only asymptotic processes like particle scattering, they also describe the multiplication rules for fields, the operator product expansion. Nonperturbative tunneling processes involve field configurations that on average get big when the coupling constant gets small, but each configuration is a coherent superposition of particles whose local interactions are described by Feynman diagrams. When the coupling is small, these become collective processes that involve large numbers of particles, but where the interactions between each of the particles is simple.
This means that nonperturbative effects show up asymptotically in resummations of infinite classes of diagrams, and these diagrams can be locally simple. The graphs determine the local equations of motion, while the allowed large-scale configurations describe non-perturbative physics. But because Feynman propagators are nonlocal in time, translating a field process to a coherent particle language is not completely intuitive, and has only been explicitly worked out in certain special cases. In the case of nonrelativistic bound states, the Bethe–Salpeter equation describes the class of diagrams to include to describe a relativistic atom. For quantum chromodynamics, the Shifman Vainshtein Zakharov sum rules describe non-perturbatively excited long-wavelength field modes in particle language, but only in a phenomenological way.
The number of Feynman diagrams at high orders of perturbation theory is very large, because there are as many diagrams as there are graphs with a given number of nodes. Nonperturbative effects leave a signature on the way in which the number of diagrams and resummations diverge at high order. It is only because non-perturbative effects appear in hidden form in diagrams that it was possible to analyze nonperturbative effects in string theory, where in many cases a Feynman description is the only one available.

</doc>
<doc id="276526" url="https://en.wikipedia.org/wiki?curid=276526" title="Fiber bundle">
Fiber bundle

In mathematics, and particularly topology, a fiber bundle (or, in British English, fibre bundle) is a space that is "locally" a product space, but "globally" may have a different topological structure. Specifically, the similarity between a space "E" and a product space "B" × "F" is defined using a continuous surjective map
that in small regions of "E" behaves just like a projection from corresponding regions of "B" × "F" to "B". The map π, called the projection or submersion of the bundle, is regarded as part of the structure of the bundle. The space "E" is known as the total space of the fiber bundle, "B" as the base space, and "F" the fiber.
In the "trivial" case, "E" is just "B" × "F", and the map π is just the projection from the product space to the first factor. This is called a trivial bundle. Examples of non-trivial fiber bundles include the Möbius strip and Klein bottle, as well as nontrivial covering spaces. Fiber bundles such as the tangent bundle of a manifold and more general vector bundles play an important role in differential geometry and differential topology, as do principal bundles.
Mappings between total spaces of fiber bundles that "commute" with the projection maps are known as bundle maps, and the class of fiber bundles forms a category with respect to such mappings. A bundle map from the base space itself (with the identity mapping as projection) to "E" is called a section of "E". Fiber bundles can be specialized in a number of ways, the most common of which is requiring that the transitions between the local trivial patches lie in a certain topological group, known as the structure group, acting on the fiber "F".
Formal definition.
A fiber bundle is a structure ("E", "B", π, "F"), where "E", "B", and "F" are topological spaces and π : "E" → "B" is a continuous surjection satisfying a "local triviality" condition outlined below. The space "B" is called the base space of the bundle, "E" the total space, and "F" the fiber. The map π is called the projection map (or bundle projection). We shall assume in what follows that the base space "B" is connected.
We require that for every "x" in "E", there is an open neighborhood "U" ⊂ "B" of π("x") (which will be called a trivializing neighborhood) such that there is a homeomorphism φ: π−1("U") → "U" × "F" (where "U" × "F" is the product space) in such a way that π agrees with the projection onto the first factor. That is, the following diagram should commute:
where proj1 : "U" × "F" → "U" is the natural projection and φ : π−1("U") → "U" × "F" is a homeomorphism. The set of all {("U""i", φ"i")} is called a local trivialization of the bundle.
Thus for any "p" in "B", the preimage π−1({"p"}) is homeomorphic to "F" (since proj1−1({"p"}) clearly is) and is called the fiber over "p". Every fiber bundle π : "E" → "B" is an open map, since projections of products are open maps. Therefore "B" carries the quotient topology determined by the map π.
A fiber bundle ("E", "B", π, "F") is often denoted
that, in analogy with a short exact sequence, indicates which space is the fiber, total space and base space, as well as the map from total to base space.
A smooth fiber bundle is a fiber bundle in the category of smooth manifolds. That is, "E", "B", and "F" are required to be smooth manifolds and all the functions above are required to be smooth maps.
Examples.
Trivial bundle.
Let "E" = "B" × "F" and let π : "E" → "B" be the projection onto the first factor. Then "E" is a fiber bundle (of "F") over "B". Here "E" is not just locally a product but "globally" one. Any such fiber bundle is called a trivial bundle. Any fiber bundle over a contractible CW-complex is trivial.
Möbius strip.
Perhaps the simplest example of a nontrivial bundle "E" is the Möbius strip. It has the circle that runs lengthwise along the center of the strip as a base "B" and a line segment for the fiber "F", so the Möbius strip is a bundle of the line segment over the circle. A neighborhood "U" of a point "x" ∈ "B" is an arc; in the picture, this is the length of one of the squares. The preimage formula_3 in the picture is a (somewhat twisted) slice of the strip four squares wide and one long. The homeomorphism φ maps the preimage of "U" to a slice of a cylinder: curved, but not twisted.
The corresponding trivial bundle "B" × "F" would be a cylinder, but the Möbius strip has an overall "twist". Note that this twist is visible only globally; locally the Möbius strip and the cylinder are identical (making a single vertical cut in either gives the same space).
Klein bottle.
A similar nontrivial bundle is the Klein bottle which can be viewed as a "twisted" circle bundle over another circle. The corresponding non-twisted (trivial) bundle is the 2-torus, "S"1 × "S"1.
Covering map.
A covering space is a fiber bundle such that the bundle projection is a local homeomorphism. It follows that the fiber is a discrete space.
Vector and principal bundles.
A special class of fiber bundles, called vector bundles, are those whose fibers are vector spaces (to qualify as a vector bundle the structure group of the bundle — see below — must be a linear group). Important examples of vector bundles include the tangent bundle and cotangent bundle of a smooth manifold. From any vector bundle, one can construct the frame bundle of bases which is a principal bundle (see below).
Another special class of fiber bundles, called principal bundles, are bundles on whose fibers a free and transitive action by a group "G" is given, so that each fiber is a principal homogeneous space. The bundle is often specified along with the group by referring to it as a principal "G"-bundle. The group "G" is also the structure group of the bundle. Given a representation ρ of "G" on a vector space "V", a vector bundle with ρ("G")⊆Aut("V") as a structure group may be constructed, known as the associated bundle.
Sphere bundles.
A sphere bundle is a fiber bundle whose fiber is an "n"-sphere. Given a vector bundle "E" with a metric (such as the tangent bundle to a Riemannian manifold) one can construct the associated unit sphere bundle, for which the fiber over a point "x" is the set of all unit vectors in "E""x". When the vector bundle in question is the tangent bundle T("M"), the unit sphere bundle is known as the unit tangent bundle, and is denoted UT("M"). 
A sphere bundle is partially characterized by its Euler class, which is a degree "n"+1 cohomology class in the total space of the bundle. In the case "n"=1 the sphere bundle is called a circle bundle and the Euler class is equal to the first Chern class, which characterizes the topology of the bundle completely. For any "n", given the Euler class of a bundle, one can calculate its cohomology using a long exact sequence called the Gysin sequence.
See also: Wang sequence
Mapping tori.
If "X" is a topological space and "f":"X" → "X" is a homeomorphism then the mapping torus "Mf" has a natural structure of a fiber bundle over the circle with fiber "X". Mapping tori of homeomorphisms of surfaces are of particular importance in 3-manifold topology.
Quotient spaces.
If "G" is a topological group and "H" is a closed subgroup, then under some circumstances, the quotient space "G"/"H" together with the quotient map π : "G" → "G"/"H" is a fiber bundle, whose fiber is the topological space "H". A necessary and sufficient condition for ("G","G"/"H",π,"H") to form a fiber bundle is that the mapping π admit local cross-sections .
The most general conditions under which the quotient map will admit local cross-sections are not known, although if "G" is a Lie group and "H" a closed subgroup (and thus a Lie subgroup by Cartan's theorem), then the quotient map is a fiber bundle. One example of this is the Hopf fibration, "S"3 → "S"2 which is a fiber bundle over the sphere "S"2 whose total space is "S"3. From the perspective of Lie groups, "S"3 can be identified with the special unitary group SU(2). The abelian subgroup of diagonal matrices is isomorphic to the circle group U(1), and the quotient SU(2)/U(1) is diffeomorphic to the sphere.
More generally, if "G" is any topological group and "H" a closed subgroup which also happens to be a Lie group, then "G" → "G"/"H" is a fiber bundle.
Sections.
A section (or cross section) of a fiber bundle π is a continuous map "f" : "B" → "E" such that π("f"("x"))="x" for all "x" in "B". Since bundles do not in general have globally defined sections, one of the purposes of the theory is to account for their existence. The obstruction to the existence of a section can often be measured by a cohomology class, which leads to the theory of characteristic classes in algebraic topology.
The most well-known example is the hairy ball theorem, where the Euler class is the obstruction to the tangent bundle of the 2-sphere having a nowhere vanishing section.
Often one would like to define sections only locally (especially when global sections do not exist). A local section of a fiber bundle is a continuous map "f" : "U" → "E" where "U" is an open set in "B" and π("f"("x"))="x" for all "x" in "U". If ("U", φ) is a local trivialization chart then local sections always exist over "U". Such sections are in 1-1 correspondence with continuous maps "U" → "F". Sections form a sheaf.
Structure groups and transition functions.
Fiber bundles often come with a group of symmetries which describe the matching conditions between overlapping local trivialization charts. Specifically, let "G" be a topological group which acts continuously on the fiber space "F" on the left. We lose nothing if we require "G" to act effectively on "F" so that it may be thought of as a group of homeomorphisms of "F". A "G"-atlas for the bundle ("E", "B", π, "F") is a local trivialization such that for any two overlapping charts ("U""i", φ"i") and ("U""j", φ"j") the function
is given by
where "t""ij" : "U""i" ∩ "U""j" → "G" is a continuous map called a transition function. Two "G"-atlases are equivalent if their union is also a "G"-atlas. A "G"-bundle is a fiber bundle with an equivalence class of "G"-atlases. The group "G" is called the structure group of the bundle; the analogous term in physics is gauge group.
In the smooth category, a "G"-bundle is a smooth fiber bundle where "G" is a Lie group and the corresponding action on "F" is smooth and the transition functions are all smooth maps.
The transition functions "t""ij" satisfy the following conditions
The third condition applies on triple overlaps "Ui" ∩ "Uj" ∩ "Uk" and is called the cocycle condition (see Čech cohomology). The importance of this is that the transition functions determine the fiber bundle (if one assumes the Čech cocycle condition).
A principal "G"-bundle is a "G"-bundle where the fiber "F" is a principal homogeneous space for the left action of "G" itself (equivalently, one can specify that the action of "G" on the fiber "F" is free and transitive). In this case, it is often a matter of convenience to identify "F" with "G" and so obtain a (right) action of "G" on the principal bundle.
Bundle maps.
It is useful to have notions of a mapping between two fiber bundles. Suppose that "M" and "N" are base spaces, and π"E" : "E" → "M" and π"F" : "F" → "N" are fiber bundles over "M" and "N", respectively. A bundle map (or bundle morphism) consists of a pair of continuous functions
such that formula_10. That is, the following diagram commutes:
For fiber bundles with structure group "G" and whose total spaces are (right) "G"-spaces (such as a principal bundle), bundle morphisms are also required to be "G"-equivariant on the fibers. This means that formula_11 is also "G"-morphism from one "G"-space to another, i.e., formula_12 for all formula_13 and formula_14.
In case the base spaces "M" and "N" coincide, then a bundle morphism over "M" from the fiber bundle π"E" : "E" → "M" to π"F" : "F" → "M" is a map φ : "E" → "F" such that formula_15. This means that the bundle map φ : "E" → "F" covers the identity of "M". That is, formula_16 and the diagram commutes
Assume that both π"E" : "E" → "M" and π"F" : "F" → "M" are defined over the same base space "M". A bundle isomorphism is a bundle map formula_17 between π"E" : "E" → "M" and π"F" : "F" → "M" such that formula_16 and such that φ is also a homeomorphism.
Differentiable fiber bundles.
In the category of differentiable manifolds, fiber bundles arise naturally as submersions of one manifold to another. Not every (differentiable) submersion ƒ : "M" → "N" from a differentiable manifold "M" to another differentiable manifold "N" gives rise to a differentiable fiber bundle. For one thing, the map must be surjective, and ("M","N",ƒ) is called a fibered manifold. However, this necessary condition is not quite sufficient, and there are a variety of sufficient conditions in common use.
If "M" and "N" are compact and connected, then any submersion "f" : "M" → "N" gives rise to a fiber bundle in the sense that there is a fiber space "F" diffeomorphic to each of the fibers such that ("E","B",π,"F") = ("M","N",ƒ,"F") is a fiber bundle. (Surjectivity of ƒ follows by the assumptions already given in this case.) More generally, the assumption of compactness can be relaxed if the submersion ƒ : "M" → "N" is assumed to be a surjective proper map, meaning that ƒ−1("K") is compact for every compact subset "K" of "N". Another sufficient condition, due to , is that if ƒ : "M" → "N" is a surjective submersion with "M" and "N" differentiable manifolds such that the preimage ƒ−1{"x"} is compact and connected for all "x" ∈ "N", then ƒ admits a compatible fiber bundle structure .

</doc>
<doc id="1082841" url="https://en.wikipedia.org/wiki?curid=1082841" title="Fictitious force">
Fictitious force

A fictitious force, also called a pseudo force, d'Alembert force or inertial force, is an apparent force that acts on all masses whose motion is described using a non-inertial frame of reference, such as a rotating reference frame.
The force F does not arise from any physical interaction between two objects, but rather from the acceleration a of the non-inertial reference frame itself. As stated by Iro:
Assuming Newton's second law in the form F = "m"a, fictitious forces are always proportional to the mass "m".
A fictitious force on an object arises when the frame of reference used to describe the object's motion is accelerating compared to a non-accelerating frame. As a frame can accelerate in any arbitrary way, so can fictitious forces be as arbitrary (but only in direct response to the acceleration of the frame). However, four fictitious forces are defined for frames accelerated in commonly occurring ways: one caused by any relative acceleration of the origin in a straight line (rectilinear acceleration); two involving rotation: centrifugal force and Coriolis force; and a fourth, called the Euler force, caused by a variable rate of rotation, should that occur. Gravitational force would also be a fictitious force based upon a field model in which particles distort spacetime due to their mass.
Background.
The role of fictitious forces in Newtonian mechanics is described by Tonnelat:
Fictitious forces on Earth.
The surface of the Earth is a rotating reference frame. To solve classical mechanics problems exactly in an Earth-bound reference frame, three fictitious forces must be introduced, the Coriolis force, the centrifugal force (described below) and the Euler force. The Euler force is typically ignored because the variations in the angular velocity of the rotating Earth surface are usually insignificant. Both of the other fictitious forces are weak compared to most typical forces in everyday life, but they can be detected under careful conditions. For example, Léon Foucault was able to show the Coriolis force that results from the Earth's rotation using the Foucault pendulum. If the Earth were to rotate a thousand times faster (making each day only ≈86 seconds long), people could easily get the impression that such fictitious forces are pulling on them, as on a spinning carousel.
Detection of non-inertial reference frame.
Observers inside a closed box that is moving with a constant velocity cannot detect their own motion; however, observers within an accelerating reference frame can detect that they are in a non-inertial reference frame from the fictitious forces that arise. For example, for straight-line acceleration Vladimir Arnold presents the following theorem:
Other accelerations also give rise to fictitious forces, as described mathematically below. The physical explanation of motions in an inertial frame is the simplest possible, requiring no fictitious forces: fictitious forces are zero, providing a means to distinguish inertial frames from others.
An example of the detection of a non-inertial, rotating reference frame is the precession of a Foucault pendulum. In the non-inertial frame of the Earth, the fictitious Coriolis force is necessary to explain observations. In an inertial frame outside the Earth, no such fictitious force is necessary.
Examples of fictitious forces.
Acceleration in a straight line.
Figure 1 (top) shows an accelerating car. When a car accelerates, a passenger feels like they're being pushed back into the seat. In an inertial frame of reference attached to the road, there is no physical force moving the rider backward. However, in the rider's non-inertial reference frame attached to the accelerating car, there "is" a backward fictitious force. We mention two possible reasons for the force to clarify its (the force's) existence:
How can the accelerating frame be discovered to be non-inertial? In the accelerating frame, everything appears to be subject to zero net force, and nothing moves. Nonetheless, compression of the seat is observed and is explained in the accelerating frame (and in an inertial frame) by the force of acceleration on the seat from the car on one side, and the opposing force of reaction to acceleration by the passenger on the other. Identification of the accelerating frame as non-inertial cannot be based simply on the compression of the seat, which all observers can explain; rather it is based on the "simplicity" of the physical explanation for this compression.
The explanation of the seat compression in the accelerating frame requires not only the thrust from the axle of the car, but additional (fictitious) forces. In an inertial frame, only the thrust from the axle is necessary. Therefore, the inertial frame has a "simpler" physical explanation (not necessarily a simpler mathematical formulation, however), indicating the accelerating frame is a non-inertial frame of reference. In other words, in the inertial frame, fictitious forces are zero. See inertial frame for more detail.
This example illustrates how fictitious forces arise from switching from an inertial to a non-inertial reference frame. Calculations of physical quantities (compression of the seat, required force from the axle) made in any frame give the same answers, but in some cases calculations are easier to make in a non-inertial frame. (In this simple example, the calculations are equally complex for the two frames described.)
Circular motion.
A similar effect occurs in circular motion, circular from the standpoint of an inertial frame of reference attached to the road. When seen from a non-inertial frame of reference attached to the car, the fictitious force called the centrifugal force appears. If the car is moving at constant speed around a circular section of road, the occupants will feel pushed outside by this centrifugal force, away from the center of the turn. Again the situation can be viewed from inertial or non-inertial frames (for free body diagrams, see ):
A classic example of fictitious force in circular motion is the experiment of rotating spheres tied by a cord and spinning around their center of mass. In this case, as with the linearly accelerating car example, the identification of a rotating, non-inertial frame of reference can be based upon the vanishing of fictitious forces. In an inertial frame, fictitious forces are not necessary to explain the tension in the string joining the spheres. In a rotating frame, Coriolis and centrifugal forces must be introduced to predict the observed tension.
To consider another example, where a rotating reference frame is very natural to us, namely the surface of the rotating Earth, centrifugal force reduces the apparent force of gravity by about one part in a thousand, depending on latitude. This reduction is zero at the poles, maximum at the equator.
The fictitious Coriolis force, which is observed in rotational frames, is ordinarily visible only in very large-scale motion like the projectile motion of long-range guns or the circulation of the Earth's atmosphere (see Rossby number). Neglecting air resistance, an object dropped from a 50-meter-high tower at the equator will fall 7.7 millimeters eastward of the spot below where it is dropped because of the Coriolis force.
In the case of distant objects and a rotating reference frame, what must be taken into account is the resultant force of centrifugal and Coriolis force. Consider a distant star observed from a rotating spacecraft. In the reference frame co-rotating with the spacecraft, the distant star appears to move along a circular trajectory around the spacecraft. The apparent motion of the star is an apparent centripetal acceleration. Just like in the example above of the car in circular motion, the centrifugal force has the same magnitude as the fictitious centripetal force, but is directed in the opposite, centrifugal direction. In this case the Coriolis force is twice the magnitude of the centrifugal force, and it points in centripetal direction. The vector sum of the centrifugal force and the Coriolis force is the total fictitious force, which in this case points in centripetal direction.
Fictitious forces and work.
Fictitious forces can be considered to do work, provided that they move an object on a trajectory that changes its energy from potential to kinetic. For example, consider a person in a rotating chair holding a weight in their outstretched hand. If they pull their hand inward toward their body, from the perspective of the rotating reference frame, they have done work against the centrifugal force. When the weight is let go, it spontaneously flies outward relative to the rotating reference frame, because the centrifugal force does work on the object, converting its potential energy into kinetic. From an inertial viewpoint, of course, the object flies away from them because it is suddenly allowed to move in a straight line. This illustrates that the work done, like the total potential and kinetic energy of an object, can be different in a non-inertial frame than an inertial one.
Gravity as a fictitious force.
The notion of "fictitious force" comes up in general relativity. All fictitious forces are proportional to the mass of the object upon which they act, which is also true for gravity. This led Albert Einstein to wonder whether gravity was a fictitious force as well. He noted that a freefalling observer in a closed box would not be able to detect the force of gravity; hence, freefalling reference frames are equivalent to an inertial reference frame (the equivalence principle). Following up on this insight, Einstein was able to formulate a theory with gravity as a fictitious force; attributing the apparent acceleration of gravity to the curvature of spacetime. This idea underlies Einstein's theory of general relativity. See Eötvös experiment.
Mathematical derivation of fictitious forces.
General derivation.
Many problems require use of noninertial reference frames, for example, those involving satellites and particle accelerators. Figure 2 shows a particle with mass "m" and position vector xA("t") in a particular inertial frame A. Consider a non-inertial frame B whose origin relative to the inertial one is given by XAB("t"). Let the position of the particle in frame B be xB("t"). What is the force on the particle as expressed in the coordinate system of frame B? 
To answer this question, let the coordinate axis in B be represented by unit vectors uj with "j" any of { 1, 2, 3 } for the three coordinate axes. Then
The interpretation of this equation is that xB is the vector displacement of the particle as expressed in terms of the coordinates in frame B at time "t". From frame A the particle is located at:
As an aside, the unit vectors { u"j" } cannot change magnitude, so derivatives of these vectors express only rotation of the coordinate system B. On the other hand, vector XAB simply locates the origin of frame B relative to frame A, and so cannot include rotation of frame B.
Taking a time derivative, the velocity of the particle is:
The second term summation is the velocity of the particle, say vB as measured in frame B. That is:
The interpretation of this equation is that the velocity of the particle seen by observers in frame A consists of what observers in frame B call the velocity, namely vB, plus two extra terms related to the rate of change of the frame-B coordinate axes. One of these is simply the velocity of the moving origin vAB. The other is a contribution to velocity due to the fact that different locations in the non-inertial frame have different apparent velocities due to rotation of the frame; a point seen from a rotating frame has a rotational component of velocity that is greater the further the point is from the origin.
To find the acceleration, another time differentiation provides:
Using the same formula already used for the time derivative of xB, the velocity derivative on the right is:
Consequently,
The interpretation of this equation is as follows: the acceleration of the particle in frame A consists of what observers in frame B call the particle acceleration aB, but in addition there are three acceleration terms related to the movement of the frame-B coordinate axes: one term related to the acceleration of the origin of frame B, namely aAB, and two terms related to rotation of frame B. Consequently, observers in B will see the particle motion as possessing "extra" acceleration, which they will attribute to "forces" acting on the particle, but which observers in A say are "fictitious" forces arising simply because observers in B do not recognize the non-inertial nature of frame B.
The factor of two in the Coriolis force arises from two equal contributions: (i) the apparent change of an inertially constant velocity with time because rotation makes the direction of the velocity seem to change (a "d"vB/d"t" term) and (ii) an apparent change in the velocity of an object when its position changes, putting it nearer to or further from the axis of rotation (the change in formula_7 due to change in "x j" ).
To put matters in terms of forces, the accelerations are multiplied by the particle mass:
The force observed in frame B, FB = "m"aB is related to the actual force on the particle, FA, by
where:
Thus, we can solve problems in frame B by assuming that Newton's second law holds (with respect to quantities in that frame) and treating Ffictitious as an additional force.
Below are a number of examples applying this result for fictitious forces. More examples can be found in the article on centrifugal force.
Rotating coordinate systems.
A common situation in which noninertial reference frames are useful is when the reference frame is rotating. Because such rotational motion is non-inertial, due to the acceleration present in any rotational motion, a fictitious force can always be invoked by using a rotational frame of reference. Despite this complication, the use of fictitious forces often simplifies the calculations involved.
To derive expressions for the fictitious forces, derivatives are needed for the apparent time rate of change of vectors that take into account time-variation of the coordinate axes. If the rotation of frame "B" is represented by a vector Ω pointed along the axis of rotation with orientation given by the right-hand rule, and with magnitude given by
then the time derivative of any of the three unit vectors describing frame "B" is
and
as is verified using the properties of the vector cross product. These derivative formulas now are applied to the relationship between acceleration in an inertial frame, and that in a coordinate frame rotating with time-varying angular velocity ω("t"). From the previous section, where subscript A refers to the inertial frame and B to the rotating frame, setting aAB = 0 to remove any translational acceleration, and focusing on only rotational properties (see Eq. 1):
Collecting terms, the result is the so-called "acceleration transformation formula":
The physical acceleration aA due to what observers in the inertial frame A call "real external forces" on the object is, therefore, not simply the acceleration aB seen by observers in the rotational frame B, but has several additional geometric acceleration terms associated with the rotation of B. As seen in the rotational frame, the acceleration aB of the particle is given by rearrangement of the above equation as:
The net force upon the object according to observers in the rotating frame is FB = "m"aB. If their observations are to result in the correct force on the object when using Newton's laws, they must consider that the additional force Ffict is present, so the end result is FB = FA + Ffict. Thus, the fictitious force used by observers in B to get the correct behavior of the object from Newton's laws equals:
Here, the first term is the "Coriolis force", the second term is the "centrifugal force", and the third term is the "Euler force".
Orbiting coordinate systems.
As a related example, suppose the moving coordinate system "B" rotates in a circle of radius "R" about the fixed origin of inertial frame "A", but maintains its coordinate axes fixed in orientation, as in Figure 3. The acceleration of an observed body is now (see Eq. 1):
where the summations are zero inasmuch as the unit vectors have no time dependence. The origin of system "B" is located according to frame "A" at:
leading to a velocity of the origin of frame "B" as:
leading to an acceleration of the origin of "B" given by:
Because the first term, which is
is of the same form as the normal centrifugal force expression:
it is a natural extension of standard terminology (although there is no standard terminology for this case) to call this term a "centrifugal force". Whatever terminology is adopted, the observers in frame "B" must introduce a fictitious force, this time due to the acceleration from the orbital motion of their entire coordinate frame, that is radially outward away from the center of rotation of the origin of their coordinate system:
and of magnitude:
Notice that this "centrifugal force" has differences from the case of a rotating frame. In the rotating frame the centrifugal force is related to the distance of the object from the origin of frame "B", while in the case of an orbiting frame, the centrifugal force is independent of the distance of the object from the origin of frame "B", but instead depends upon the distance of the origin of frame "B" from "its" center of rotation, resulting in the "same" centrifugal fictitious force for "all" objects observed in frame "B".
Orbiting and rotating.
As a combination example, Figure 4 shows a coordinate system "B" that orbits inertial frame "A" as in Figure 3, but the coordinate axes in frame "B" turn so unit vector u1 always points toward the center of rotation. This example might apply to a test tube in a centrifuge, where vector u1 points along the axis of the tube toward its opening at its top. It also resembles the Earth-Moon system, where the Moon always presents the same face to the Earth. In this example, unit vector u3 retains a fixed orientation, while vectors u1, u2 rotate at the same rate as the origin of coordinates. That is,
Hence, the acceleration of a moving object is expressed as (see Eq. 1):
where the angular acceleration term is zero for constant rate of rotation.
Because the first term, which is
is of the same form as the normal centrifugal force expression:
it is a natural extension of standard terminology (although there is no standard terminology for this case) to call this term the "centrifugal force". Applying this terminology to the example of a tube in a centrifuge, if the tube is far enough from the center of rotation, |XAB| = "R" » |xB|, all the matter in the test tube sees the same acceleration (the same centrifugal force). Thus, in this case, the fictitious force is primarily a uniform centrifugal force along the axis of the tube, away from the center of rotation, with a value |FFict| = ω2 "R", where "R" is the distance of the matter in the tube from the center of the centrifuge. It is standard specification of a centrifuge to use the "effective" radius of the centrifuge to estimate its ability to provided centrifugal force. Thus, a first estimate of centrifugal force in a centrifuge can be based upon the distance of the tubes from the center of rotation, and corrections applied if needed.
Also, the test tube confines motion to the direction down the length of the tube, so vB is opposite to u1 and the Coriolis force is opposite to u2, that is, against the wall of the tube. If the tube is spun for a long enough time, the velocity vB drops to zero as the matter comes to an equilibrium distribution. For more details, see the articles on sedimentation and the Lamm equation.
A related problem is that of centrifugal forces for the Earth-Moon-Sun system, where three rotations appear: the daily rotation of the Earth about its axis, the lunar-month rotation of the Earth-Moon system about their center of mass, and the annual revolution of the Earth-Moon system about the Sun. These three motions influence the tides.
Crossing a carousel.
Figure 5 shows another example comparing the observations of an inertial observer with those of an observer on a rotating carousel. The carousel rotates at a constant angular velocity represented by the vector Ω with magnitude ω, pointing upward according to the right-hand rule. A rider on the carousel walks radially across it at constant speed, in what appears to the walker to be the straight line path inclined at 45° in Figure 5 . To the stationary observer, however, the walker travels a spiral path. The points identified on both paths in Figure 5 correspond to the same times spaced at equal time intervals. We ask how two observers, one on the carousel and one in an inertial frame, formulate what they see using Newton's laws.
Inertial observer.
The observer at rest describes the path followed by the walker as a spiral. Adopting the coordinate system shown in Figure 5, the trajectory is described by r("t"):
where the added π/4 sets the path angle at 45° to start with (just an arbitrary choice of direction), u"R" is a unit vector in the radial direction pointing from the center of the carousel to the walker at time "t". The radial distance "R"("t") increases steadily with time according to:
with "s" the speed of walking. According to simple kinematics, the velocity is then the first derivative of the trajectory:
with uθ a unit vector perpendicular to uR at time "t" (as can be verified by noticing that the vector dot product with the radial vector is zero) and pointing in the direction of travel.
The acceleration is the first derivative of the velocity:
The last term in the acceleration is radially inward of magnitude ω2 "R", which is therefore the instantaneous centripetal acceleration of circular motion. The first term is perpendicular to the radial direction, and pointing in the direction of travel. Its magnitude is 2"s"ω, and it represents the acceleration of the walker as the edge of the carousel is neared, and the arc of circle traveled in a fixed time increases, as can be seen by the increased spacing between points for equal time steps on the spiral in Figure 5 as the outer edge of the carousel is approached.
Applying Newton's laws, multiplying the acceleration by the mass of the walker, the inertial observer concludes that the walker is subject to two forces: the inward, radially directed centripetal force, and another force perpendicular to the radial direction that is proportional to the speed of the walker.
Rotating observer.
The rotating observer sees the walker travel a straight line from the center of the carousel to the periphery, as shown in Figure 5. Moreover, the rotating observer sees that the walker moves at a constant speed in the same direction, so applying Newton's law of inertia, there is "zero" force upon the walker. These conclusions do not agree with the inertial observer. To obtain agreement, the rotating observer has to introduce fictitious forces that appear to exist in the rotating world, even though there is no apparent reason for them, no apparent gravitational mass, electric charge or what have you, that could account for these fictitious forces.
To agree with the inertial observer, the forces applied to the walker must be exactly those found above. They can be related to the general formulas already derived, namely:
In this example, the velocity seen in the rotating frame is:
with uR a unit vector in the radial direction. The position of the walker as seen on the carousel is:
and the time derivative of Ω is zero for uniform angular rotation. Noticing that
and
we find:
To obtain a straight-line motion in the rotating world, a force exactly opposite in sign to the fictitious force must be applied to reduce the net force on the walker to zero, so Newton's law of inertia will predict a straight line motion, in agreement with what the rotating observer sees. The fictitious forces that must be combated are the Coriolis force (first term) and the centrifugal force (second term). (These terms are approximate.) By applying forces to counter these two fictitious forces, the rotating observer ends up applying exactly the same forces upon the walker that the inertial observer predicted were needed.
Because they differ only by the constant walking velocity, the walker and the rotational observer see the same accelerations. From the walker's perspective, the fictitious force is experienced as real, and combating this force is necessary to stay on a straight line radial path holding constant speed. It's like battling a crosswind while being thrown to the edge of the carousel.
Observation.
Notice that this kinematical discussion does not delve into the mechanism by which the required forces are generated. That is the subject of kinetics. In the case of the carousel, the kinetic discussion would involve perhaps a study of the walker's shoes and the friction they need to generate against the floor of the carousel, or perhaps the dynamics of skateboarding, if the walker switched to travel by skateboard. Whatever the means of travel across the carousel, the forces calculated above must be realized. A very rough analogy is heating your house: you must have a certain temperature to be comfortable, but whether you heat by burning gas or by burning coal is another problem. Kinematics sets the thermostat, kinetics fires the furnace.

</doc>
<doc id="26998617" url="https://en.wikipedia.org/wiki?curid=26998617" title="Field (physics)">
Field (physics)

In physics, a field is a physical quantity that has a value for each point in space and time. For example, on a weather map, the surface wind velocity is described by assigning a vector to each point on a map. Each vector represents the speed and direction of the movement of air at that point. As another example, an electric field can be thought of as a "condition in space" emanating from an electric charge and extending throughout the whole of space. When a test electric charge is placed in this electric field, the particle accelerates due to a force. Physicists have found the notion of a field to be of such practical utility for the analysis of forces that they have come to think of a force as due to a field.
In the modern framework of the quantum theory of fields, even without referring to a test particle, a field occupies space, contains energy, and its presence eliminates a true vacuum. 
This led physicists to consider electromagnetic fields to be a physical entity, making the field concept a supporting paradigm of the edifice of modern physics. "The fact that the electromagnetic field can possess momentum and energy makes it very real... a particle makes a field, and a field acts on another particle, and the field has such familiar properties as energy content and momentum, just as particles can have". In practice, the strength of most fields has been found to diminish with distance to the point of being undetectable. For instance the strength of many relevant classical fields, such as the gravitational field in Newton's theory of gravity or the electrostatic field in classical electromagnetism, is inversely proportional to the square of the distance from the source (i.e. they follow the Gauss's law). One consequence is that the Earth's gravitational field quickly becomes undetectable on cosmic scales.
A field can be classified as a scalar field, a vector field, a spinor field or a tensor field according to whether the represented physical quantity is a scalar, a vector, a spinor or a tensor, respectively. A field has a unique tensorial character in every point where it is defined: i.e. a field cannot be a scalar field somewhere and a vector field somewhere else. For example, the Newtonian gravitational field is a vector field: specifying its value at a point in spacetime requires three numbers, the components of the gravitational field vector at that point. Moreover, within each category (scalar, vector, tensor), a field can be either a "classical field" or a "quantum field", depending on whether it is characterized by numbers or quantum operators respectively. In fact in this theory an equivalent representation of field is a field particle, namely a boson.
History.
To Isaac Newton his law of universal gravitation simply expressed the gravitational force that acted between any pair of massive objects. When looking at the motion of many bodies all interacting with each other, such as the planets in the Solar System, dealing with the force between each pair of bodies separately rapidly becomes computationally inconvenient. In the eighteenth century, a new quantity was devised to simplify the bookkeeping of all these gravitational forces. This quantity, the gravitational field, gave at each point in space the total gravitational force which would be felt by an object with unit mass at that point. This did not change the physics in any way: it did not matter if you calculated all the gravitational forces on an object individually and then added them together, or if you first added all the contributions together as a gravitational field and then applied it to an object.
The development of the independent concept of a field truly began in the nineteenth century with the development of the theory of electromagnetism. In the early stages, André-Marie Ampère and Charles-Augustin de Coulomb could manage with Newton-style laws that expressed the forces between pairs of electric charges or electric currents. However, it became much more natural to take the field approach and express these laws in terms of electric and magnetic fields; in 1849 Michael Faraday became the first to coin the term "field".
The independent nature of the field became more apparent with James Clerk Maxwell's discovery that waves in these fields propagated at a finite speed. Consequently, the forces on charges and currents no longer just depended on the positions and velocities of other charges and currents at the same time, but also on their positions and velocities in the past.
Maxwell, at first, did not adopt the modern concept of a field as fundamental quantity that could independently exist. Instead, he supposed that the electromagnetic field expressed the deformation of some underlying medium—the luminiferous aether—much like the tension in a rubber membrane. If that were the case, the observed velocity of the electromagnetic waves should depend upon the velocity of the observer with respect to the aether. Despite much effort, no experimental evidence of such an effect was ever found; the situation was resolved by the introduction of the special theory of relativity by Albert Einstein in 1905. This theory changed the way the viewpoints of moving observers should be related to each other in such a way that velocity of electromagnetic waves in Maxwell's theory would be the same for all observers. By doing away with the need for a background medium, this development opened the way for physicists to start thinking about fields as truly independent entities.
In the late 1920s, the new rules of quantum mechanics were first applied to the electromagnetic fields. In 1927, Paul Dirac used quantum fields to successfully explain how the decay of an atom to lower quantum state lead to the spontaneous emission of a photon, the quantum of the electromagnetic field. This was soon followed by the realization (following the work of Pascual Jordan, Eugene Wigner, Werner Heisenberg, and Wolfgang Pauli) that all particles, including electrons and protons, could be understood as the quanta of some quantum field, elevating fields to the status of the most fundamental objects in nature. That said, John Wheeler and Richard Feynman seriously considered Newton's pre-field concept of action at a distance (although they set it aside because of the ongoing utility of the field concept for research in general relativity and quantum electrodynamics).
Classical fields.
There are several examples of classical fields. Classical field theories remain useful wherever quantum properties do not arise, and can be active areas of research. Elasticity of materials, fluid dynamics and Maxwell's equations are cases in point.
Some of the simplest physical fields are vector force fields. Historically, the first time that fields were taken seriously was with Faraday's lines of force when describing the electric field. The gravitational field was then similarly described.
Newtonian gravitation.
A classical field theory describing gravity is Newtonian gravitation, which describes the gravitational force as a mutual interaction between two masses.
Any body with mass "M" is associated with a gravitational field g which describes its influence on other bodies with mass. The gravitational field of "M" at a point r in space corresponds to the ratio between force F that "M" exerts on a small or negligible test mass "m" located at r and the test mass itself:
Stipulating that "m" is much smaller than "M" ensures that the presence of "m" has a negligible influence on the behavior of "M".
According to Newton's law of universal gravitation, F(r) is given by
where formula_3 is a unit vector lying along the line joining "M" and "m" and pointing from "m" to "M". Therefore, the gravitational field of M is
The experimental observation that inertial mass and gravitational mass are equal to an unprecedented level of accuracy leads to the identity that gravitational field strength is identical to the acceleration experienced by a particle. This is the starting point of the equivalence principle, which leads to general relativity.
Because the gravitational force F is conservative, the gravitational field g can be rewritten in terms of the gradient of a scalar function, the gravitational potential Φ(r):
Electromagnetism.
Michael Faraday first realized the importance of a field as a physical quantity, during his investigations into magnetism. He realized that electric and magnetic fields are not only fields of force which dictate the motion of particles, but also have an independent physical reality because they carry energy.
These ideas eventually led to the creation, by James Clerk Maxwell, of the first unified field theory in physics with the introduction of equations for the electromagnetic field. The modern version of these equations is called Maxwell's equations.
Electrostatics.
A charged test particle with charge "q" experiences a force F based solely on its charge. We can similarly describe the electric field E so that . Using this and Coulomb's law tells us that the electric field due to a single charged particle as
The electric field is conservative, and hence can be described by a scalar potential, "V"(r):
Magnetostatics.
A steady current "I" flowing along a path "ℓ" will exert a force on nearby moving charged particles that is quantitatively different from the electric field force described above. The force exerted by "I" on a nearby charge "q" with velocity v is
where B(r) is the magnetic field, which is determined from "I" by the Biot–Savart law:
The magnetic field is not conservative in general, and hence cannot usually be written in terms of a scalar potential. However, it can be written in terms of a vector potential, A(r):
Electrodynamics.
In general, in the presence of both a charge density ρ(r, "t") and current density J(r, "t"), there will be both an electric and a magnetic field, and both will vary in time. They are determined by Maxwell's equations, a set of differential equations which directly relate E and B to ρ and J.
Alternatively, one can describe the system in terms of its scalar and vector potentials "V" and A. A set of integral equations known as "retarded potentials" allow one to calculate "V" and A from ρ and J, and from there the electric and magnetic fields are determined via the relations
At the end of the 19th century, the electromagnetic field was understood as a collection of two vector fields in space. Nowadays, one recognizes this as a single antisymmetric 2nd-rank tensor field in spacetime.
Gravitation in general relativity.
Einstein's theory of gravity, called general relativity, is another example of a field theory. Here the principal field is the metric tensor, a symmetric 2nd-rank tensor field in spacetime. This replaces Newton's law of universal gravitation.
Waves as fields.
Waves can be constructed as physical fields, due to their finite propagation speed and causal nature when a simplified physical model of an isolated closed system is set . They are also subject to the inverse-square law.
For electromagnetic waves, there are optical fields, and terms such as near- and far-field limits for diffraction. In practice, though the field theories of optics are superseded by the electromagnetic field theory of Maxwell.
Quantum fields.
It is now believed that quantum mechanics should underlie all physical phenomena, so that a classical field theory should, at least in principle, permit a recasting in quantum mechanical terms; success yields the corresponding quantum field theory. For example, quantizing classical electrodynamics gives quantum electrodynamics. Quantum electrodynamics is arguably the most successful scientific theory; experimental data confirm its predictions to a higher precision (to more significant digits) than any other theory. The two other fundamental quantum field theories are quantum chromodynamics and the electroweak theory.
In quantum chromodynamics, the color field lines are coupled at short distances by gluons, which are polarized by the field and line up with it. This effect increases within a short distance (around 1 fm from the vicinity of the quarks) making the color force increase within a short distance, confining the quarks within hadrons. As the field lines are pulled together tightly by gluons, they do not "bow" outwards as much as an electric field between electric charges.
These three quantum field theories can all be derived as special cases of the so-called standard model of particle physics. General relativity, the Einsteinian field theory of gravity, has yet to be successfully quantized. However an extension, thermal field theory, deals with quantum field theory at "finite temperatures", something seldom considered in quantum field theory.
In BRST theory one deals with odd fields, e.g. Faddeev–Popov ghosts. There are different descriptions of odd classical fields both on graded manifolds and supermanifolds.
As above with classical fields, it is possible to approach their quantum counterparts from a purely mathematical view using similar techniques as before. The equations governing the quantum fields are in fact PDEs (specifically, relativistic wave equations (RWEs)). Thus one can speak of Yang–Mills, Dirac, Klein–Gordon and Schrödinger fields as being solutions to their respective equations. A possible problem is that these RWEs can deal with complicated mathematical objects with exotic algebraic properties (e.g. spinors are not tensors, so may need calculus over spinor fields), but these in theory can still be subjected to analytical methods given appropriate mathematical generalization.
Field theory.
Field theory usually refers to a construction of the dynamics of a field, i.e. a specification of how a field changes with time or with respect to other independent physical variables on which the field depends. Usually this is done by writing a Lagrangian or a Hamiltonian of the field, and treating it as the classical mechanics (or quantum mechanics) of a system with an infinite number of degrees of freedom. The resulting field theories are referred to as classical or quantum field theories.
The dynamics of a classical field are usually specified by the Lagrangian density in terms of the field components; the dynamics can be obtained by using the action principle.
It is possible to construct simple fields without any a priori knowledge of physics using only mathematics from several variable calculus, potential theory and partial differential equations (PDEs). For example, scalar PDEs might consider quantities such as amplitude, density and pressure fields for the wave equation and fluid dynamics; temperature/concentration fields for the heat/diffusion equations. Outside of physics proper (e.g., radiometry and computer graphics), there are even light fields. All these previous examples are scalar fields. Similarly for vectors, there are vector PDEs for displacement, velocity and vorticity fields in (applied mathematical) fluid dynamics, but vector calculus may now be needed in addition, being calculus over vector fields (as are these three quantities, and those for vector PDEs in general). More generally problems in continuum mechanics may involve for example, directional elasticity (from which comes the term "tensor", derived from the Latin word for stretch), complex fluid flows or anisotropic diffusion, which are framed as matrix-tensor PDEs, and then require matrices or tensor fields, hence matrix or tensor calculus. It should be noted that the scalars (and hence the vectors, matrices and tensors) can be real or complex as both are fields in the abstract-algebraic/ring-theoretic sense.
In a general setting, classical fields are described by sections of fiber bundles and their dynamics is formulated in the terms of jet manifolds (covariant classical field theory).
In modern physics, the most often studied fields are those that model the four fundamental forces which one day may lead to the Unified Field Theory.
Symmetries of fields.
A convenient way of classifying a field (classical or quantum) is by the symmetries it possesses. Physical symmetries are usually of two types:
Spacetime symmetries.
Fields are often classified by their behaviour under transformations of spacetime. The terms used in this classification are:
Internal symmetries.
Fields may have internal symmetries in addition to spacetime symmetries. For example, in many situations one needs fields which are a list of space-time scalars: (φ1, φ2, ... φ"N"). For example, in weather prediction these may be temperature, pressure, humidity, etc. In particle physics, the color symmetry of the interaction of quarks is an example of an internal symmetry of the strong interaction, as is the isospin or flavour symmetry.
If there is a symmetry of the problem, not involving spacetime, under which these components transform into each other, then this set of symmetries is called an "internal symmetry". One may also make a classification of the charges of the fields under internal symmetries.
Statistical field theory.
Statistical field theory attempts to extend the field-theoretic paradigm toward many-body systems and statistical mechanics. As above, it can be approached by the usual infinite number of degrees of freedom argument.
Much like statistical mechanics has some overlap between quantum and classical mechanics, statistical field theory has links to both quantum and classical field theories, especially the former with which it shares many methods. One important example is mean field theory.
Continuous random fields.
Classical fields as above, such as the electromagnetic field, are usually infinitely differentiable functions, but they are in any case almost always twice differentiable. In contrast, generalized functions are not continuous. When dealing carefully with classical fields at finite temperature, the mathematical methods of continuous random fields are used, because thermally fluctuating classical fields are nowhere differentiable. Random fields are indexed sets of random variables; a continuous random field is a random field that has a set of functions as its index set. In particular, it is often mathematically convenient to take a continuous random field to have a Schwartz space of functions as its index set, in which case the continuous random field is a tempered distribution.
We can think about a continuous random field, in a (very) rough way, as an ordinary function that is formula_13 almost everywhere, but such that when we take a weighted average of all the infinities over any finite region, we get a finite result. The infinities are not well-defined; but the finite values can be associated with the functions used as the weight functions to get the finite values, and that can be well-defined. We can define a continuous random field well enough as a linear map from a space of functions into the real numbers.

</doc>
<doc id="610202" url="https://en.wikipedia.org/wiki?curid=610202" title="Fine structure">
Fine structure

In atomic physics, the fine structure describes the splitting of the spectral lines of atoms due to electron spin and relativistic corrections to the non-relativistic Schrödinger equation.
The gross structure of line spectra is the line spectra predicted by the quantum mechanics of non-relativistic electrons with no spin. For a hydrogenic atom, the gross structure energy levels only depend on the principal quantum number n. However, a more accurate model takes into account relativistic and spin effects, which break the degeneracy of the energy levels and split the spectral lines. The scale of the fine structure splitting relative to the gross structure energies is on the order of ("Zα")2, where "Z" is the atomic number and "α" is the fine-structure constant, a dimensionless number equal to approximately formula_1.
The fine structure can be separated into three corrective terms: the kinetic energy term, the spin-orbit term, and the Darwinian term. The full Hamiltonian is given by
This can be seen as a non-relativistic approximation of the Dirac equation.
Kinetic energy relativistic correction.
Classically, the kinetic energy term of the Hamiltonian is
where formula_4 is the momentum and formula_5 is the mass of the electron.
However, when considering a more accurate theory of nature viz. special relativity, we must use a relativistic form of the kinetic energy,
where the first term is the total relativistic energy, and the second term is the rest energy of the electron. (formula_7 is the speed of light) Expanding this in a Taylor series ( specifically a binomial series ), we find
Then, the first order correction to the Hamiltonian is
Using this as a perturbation, we can calculate the first order energy corrections due to relativistic effects.
where formula_11 is the unperturbed wave function. Recalling the unperturbed Hamiltonian, we see
We can use this result to further calculate the relativistic correction:
For the hydrogen atom, formula_14, formula_15, and formula_16 where formula_17 is the Bohr Radius, formula_18 is the principal quantum number and formula_19 is the azimuthal quantum number. Therefore the first order relativistic correction for the hydrogen atom is
where we have used:
On final calculation, the order of magnitude for the relativistic correction to the ground state is formula_22.
Spin-orbit coupling.
For a hydrogen-like atom with formula_23 protons, orbital momentum formula_24 and electron spin formula_25, the spin-orbit term is given by:
formula_27 is the electron mass, formula_28 is the vacuum permittivity and formula_29 is the spin g-factor. formula_30 is the distance of the electron from the nucleus.
The spin-orbit correction can be understood by shifting from the standard frame of reference (where the electron orbits the nucleus) into one where the electron is stationary and the nucleus instead orbits it. In this case the orbiting nucleus functions as an effective current loop, which in turn will generate a magnetic field. However, the electron itself has a magnetic moment due to its intrinsic angular momentum. The two magnetic vectors, formula_31 and formula_32 couple together so that there is a certain energy cost depending on their relative orientation. This gives rise to the energy correction of the form
Notice that there is a factor of 2, called the Thomas precession, which comes from the relativistic calculation that changes back to the electron's frame from the nucleus frame.
Since
the expectation value for the Hamiltonian is:
Thus the order of magnitude for the spin-orbital coupling is formula_36.
Remark: On the (n,l,s)=(n,0,1/2) and (n,l,s)=(n,1,-1/2) energy level, which the fine structure said their level are the same. If we take the g-factor to be 2.0031904622, then, the calculated energy level will be different by using 2 as g-factor. Only using 2 as the g-factor, we can match the energy level in the 1st order approximation of the relativistic correction. When using the higher order approximation for the relativistic term, the 2.0031904622 g-factor may agree with each other. However, if we use the g-factor as 2.0031904622, the result does not agree with the formula, which included every effect.
Darwin term.
One term in the non-relativistic expansion of the Dirac equation is given by:
Thus, the Darwin term affects only the s-orbit. For example it gives the 2s-orbit the same energy as the 2p-orbit by raising the 2s-state by .
The Darwin term changes the effective potential at the nucleus. It can be interpreted as a smearing out of the electrostatic interaction between the electron and nucleus due to zitterbewegung, or rapid quantum oscillations, of the electron. This can be motivated by a short calculation
Quantum fluctuations allow for the creation of virtual electron-positron pairs with a lifetime estimated by the uncertainty principle formula_38. The distance the particles can move during this time is formula_39, the Compton wavelength. The electrons of the atom interact with those pairs. This yields a fluctuating electron position formula_40. Using a Taylor expansion, the effect on the potential formula_41 can be estimated:
Averaging over the fluctuations formula_43
gives the average potential
Approximating formula_46, this yields the perturbation of the potential due to fluctuations:
To compare with the expression above, plug in the Coulomb potential:
This is only slightly different.
Another mechanism that affects only the s-state is the Lamb shift, a further, smaller correction that arises in quantum electrodynamics that should not be confused with the Darwin term. The Darwin term gives the s-state and p-state the same energy, but the Lamb shift makes the s-state higher in energy than the p-state.
Total effect.
The total effect, obtained by summing the three components up, is given by the following expression:
where formula_50 is the total angular momentum (formula_51 if formula_52 and formula_53 otherwise). It is worth noting that this expression was first obtained by A. Sommerfeld based on the old Bohr theory; i.e., before the modern quantum mechanics was formulated.

</doc>
<doc id="49295" url="https://en.wikipedia.org/wiki?curid=49295" title="Fine-structure constant">
Fine-structure constant

In physics, the fine-structure constant, also known as Sommerfeld's constant, commonly denoted "α" (the Greek letter "alpha"), is a fundamental physical constant characterizing the strength of the electromagnetic interaction between elementary charged particles. It is related to the elementary charge (the electromagnetic coupling constant) "e", which characterizes the strength of the coupling of an elementary charged particle with the electromagnetic field, by the formula . Being a dimensionless quantity, it has the same numerical value in all systems of units. Arnold Sommerfeld introduced the fine-structure constant in 1916.
Definition.
Some equivalent definitions of "α" in terms of other fundamental physical constants are:
where:
The definition reflects the relationship between "α" and the electromagnetic coupling constant "e", which equals .
In non-SI units.
In electrostatic cgs units, the unit of electric charge, the statcoulomb, is defined so that the Coulomb constant, "k"e, or the permittivity factor, 4"πε"0, is 1 and dimensionless. Then the expression of the fine-structure constant, as commonly found in older physics literature, becomes
In natural units, commonly used in high energy physics, where , the value of the fine-structure constant is
As such, the fine-structure constant is just another, albeit dimensionless, quantity determining (or determined by) the elementary charge: in terms of such a natural unit of charge.
Measurement.
The 2014 CODATA recommended value of "α" is
This has a relative standard uncertainty of 0.32 parts per billion.
For reasons of convenience, historically the value of the reciprocal of the fine-structure constant is often specified. The 2014 CODATA recommended value is given by
While the value of "α" can be "estimated" from the values of the constants appearing in any of its definitions, the theory of quantum electrodynamics (QED) provides a way to measure "α" directly using the quantum Hall effect or the anomalous magnetic moment of the electron. The theory of QED predicts a relationship between the dimensionless magnetic moment of the electron and the fine-structure constant "α" (the magnetic moment of the electron is also referred to as "Landé "g"-factor" and symbolized as "g"). The most precise value of "α" obtained experimentally (as of 2012) is based on a measurement of "g" using a one-electron so-called "quantum cyclotron" apparatus, together with a calculation via the theory of QED that involved 12,672 tenth-order Feynman diagrams:
This measurement of "α" has a precision of 0.25 parts per billion. This value and uncertainty are about the same as the latest experimental results.
Physical interpretations.
The fine-structure constant, "α", has several physical interpretations. "α" is:
When perturbation theory is applied to quantum electrodynamics, the resulting perturbative expansions for physical results are expressed as sets of power series in "α". Because "α" is much less than one, higher powers of "α" are soon unimportant, making the perturbation theory practical in this case. On the other hand, the large value of the corresponding factors in quantum chromodynamics makes calculations involving the strong nuclear force extremely difficult.
Variation with energy scale.
According to the theory of the renormalization group, the value of the fine-structure constant (the strength of the electromagnetic interaction) grows logarithmically as the energy scale is increased. The observed value of "α" is associated with the energy scale of the electron mass; the electron is a lower bound for this energy scale because it (and the positron) is the lightest charged object whose quantum loops can contribute to the running. Therefore, 1/137.036 is the value of the fine-structure constant at zero energy. Moreover, as the energy scale increases, the strength of the electromagnetic interaction approaches that of the other two fundamental interactions, a fact important for grand unification theories. If quantum electrodynamics were an exact theory, the fine-structure constant would actually diverge at an energy known as the Landau pole. This fact makes quantum electrodynamics inconsistent beyond the perturbative expansions.
History.
Arnold Sommerfeld introduced the fine-structure constant in 1916, as part of his theory of the relativistic deviations of atomic spectral lines from the predictions of the Bohr model. The first physical interpretation of the fine-structure constant "α" was as the ratio of the velocity of the electron in the first circular orbit of the relativistic Bohr atom to the speed of light in the vacuum. Equivalently, it was the quotient between the minimum angular momentum allowed by relativity for a closed orbit, and the minimum angular momentum allowed for it by quantum mechanics. It appears naturally in Sommerfeld's analysis, and determines the size of the splitting or fine-structure of the hydrogenic spectral lines.
Is the fine-structure constant actually constant?
Physicists have pondered whether the fine-structure constant is in fact constant, or whether its value differs by location and over time. A varying "α" has been proposed as a way of solving problems in cosmology and astrophysics. String theory and other proposals for going beyond the Standard Model of particle physics have led to theoretical interest in whether the accepted physical constants (not just "α") actually vary.
Past rate of change.
The first experimenters to test whether the fine-structure constant might actually vary examined the spectral lines of distant astronomical objects and the products of radioactive decay in the Oklo natural nuclear fission reactor. Their findings were consistent with no variation in the fine-structure constant between these two vastly separated locations and times.
More recently, improved technology has made it possible to probe the value of "α" at much larger distances and to a much greater accuracy. In 1999, a team led by John K. Webb of the University of New South Wales claimed the first detection of a variation in "α". Using the Keck telescopes and a data set of 128 quasars at redshifts 0.5 < "z" < 3, Webb "et al." found that their spectra were consistent with a slight increase in "α" over the last 10–12 billion years. Specifically, they found that
In 2004, a smaller study of 23 absorption systems by Chand "et al.", using the Very Large Telescope, found no measureable variation:
However, in 2007 simple flaws were identified in the analysis method of Chand "et al.", discrediting those results.
King "et al." have used Markov Chain Monte Carlo methods to investigate the algorithm used by the UNSW group to determine formula_17 from the quasar spectra, and have found that the algorithm appears to produce correct uncertainties and maximum likelihood estimates for formula_17 for particular models. This suggests that the statistical uncertainties and best estimate for formula_17 stated by Webb "et al." and Murphy "et al." are robust.
Lamoreaux and Torgerson analyzed data from the Oklo natural nuclear fission reactor in 2004, and concluded that "α" has changed in the past 2 billion years by 45 parts per billion. They claimed that this finding was "probably accurate to within 20%." Accuracy is dependent on estimates of impurities and temperature in the natural reactor. These conclusions have to be verified.
In 2007, Khatri and Wandelt of the University of Illinois at Urbana-Champaign realized that the 21 cm hyperfine transition in neutral hydrogen of the early Universe leaves a unique absorption line imprint in the cosmic microwave background radiation. They proposed using this effect to measure the value of "α" during the epoch before the formation of the first stars. In principle, this technique provides enough information to measure a variation of 1 part in (4 orders of magnitude better than the current quasar constraints). However, the constraint which can be placed on "α" is strongly dependent upon effective integration time, going as "t"−1/2. The European LOFAR radio telescope would only be able to constrain Δ"α"/"α" to about 0.3%. The collecting area required to constrain Δ"α"/"α" to the current level of quasar constraints is on the order of 100 square kilometers, which is economically impracticable at the present time.
Present rate of change.
In 2008, Rosenband "et al." used the frequency ratio of and in single-ion optical atomic clocks to place a very stringent constraint on the present time variation of "α", namely Δ"α̇"/"α" = per year. Note that any present day null constraint on the time variation of alpha does not necessarily rule out time variation in the past. Indeed, some theories that predict a variable fine-structure constant also predict that the value of the fine-structure constant should become practically fixed in its value once the universe enters its current dark energy-dominated epoch.
Spatial variation - Australian dipole.
In September 2010 researchers from Australia said they had identified a dipole-like structure in the variation of the fine-structure constant across the observable universe. They used data on quasars obtained by the Very Large Telescope, combined with the previous data obtained by Webb at the Keck telescopes. The fine-structure constant appears to have been larger by one part in 100,000 in the direction of the southern hemisphere constellation Ara, 10 billion years ago. Similarly, the constant appeared to have been smaller by a similar fraction in the northern direction, 10 billion years ago.
In September and October 2010, after Webb's released research, physicists Chad Orzel and Sean M. Carroll suggested various approaches of how Webb's observations may be wrong. Orzel argues that the study may contain wrong data due to subtle differences in the two telescopes, in which one of the telescopes the data set was slightly high and on the other slightly low, so that they cancel each other out when they overlapped. He finds it suspicious that the triangles in the plotted graph of the quasars are so well-aligned (triangles representing sources examined with both telescopes). Carroll suggested a totally different approach; he looks at the fine-structure constant as a scalar field and claims that if the telescopes are correct and the fine-structure constant varies smoothly over the universe, then the scalar field must have a very small mass. However, previous research has shown that the mass is not likely to be extremely small. Both of these scientists' early criticisms point to the fact that different techniques are needed to confirm or contradict the results, as Webb, et al., also concluded in their study.
In October 2011, Webb "et al." reported
a variation in α dependent on both redshift and spatial direction. They report "the combined data set fits a spatial dipole" with an increase in α with redshift in one direction and a decrease in the other. "[I]ndependent VLT and Keck samples give consistent dipole directions and amplitudes..."
Anthropic explanation.
The anthropic principle is a controversial argument of why the fine-structure constant has the value it does: stable matter, and therefore life and intelligent beings, could not exist if its value were much different. For instance, were "α" to change by 4%, stellar fusion would not produce carbon, so that carbon-based life would be impossible. If "α" were > 0.1, stellar fusion would be impossible and no place in the universe would be warm enough for life as we know it.
Numerological explanations.
As a dimensionless constant which does not seem to be directly related to any mathematical constant, the fine-structure constant has long fascinated physicists.
Arthur Eddington argued that the value could be "obtained by pure deduction" and he related it to the Eddington number, his estimate of the number of protons in the Universe. This led him in 1929 to conjecture that its reciprocal was precisely the integer 137. Other physicists neither adopted this conjecture nor accepted his arguments but by the 1940s experimental values for 1/α deviated sufficiently from 137 to refute Eddington's argument.
The fine-structure constant so intrigued physicist Wolfgang Pauli that he collaborated with psychiatrist Carl Jung in a quest to understand its significance. Similarly, Max Born believed if the value of alpha were any different, the universe would be degenerate, and thus that 1/137 was a law of nature.
Richard Feynman, one of the originators and early developers of the theory of quantum electrodynamics (QED), referred to the fine-structure constant in these terms:
Conversely, statistician I. J. Good argued that a numerological explanation would only be acceptable if it came from a more fundamental theory that also provided a Platonic explanation of the value.
Attempts to find a mathematical basis for this dimensionless constant have continued up to the present time. However, no numerological explanation has ever been accepted by the community.

</doc>
<doc id="18233581" url="https://en.wikipedia.org/wiki?curid=18233581" title="Finite element method">
Finite element method

In mathematics, the finite element method (FEM) is a numerical technique for finding approximate solutions to boundary value problems for partial differential equations. It uses subdivision of a whole problem domain into simpler parts, called finite elements, and variational methods from the calculus of variations to solve the problem by minimizing an associated error function. Analogous to the idea that connecting many tiny straight lines can approximate a larger circle, FEM encompasses methods for connecting many simple element equations over many small subdomains, named finite elements, to approximate a more complex equation over a larger domain.
Basic concepts.
The subdivision of a whole domain into simpler parts has several advantages:
A typical work out of the method involves (1) dividing the domain of the problem into a collection of subdomains, with each subdomain represented by a set of element equations to the original problem, followed by (2) systematically recombining all sets of element equations into a global system of equations for the final calculation. The global system of equations has known solution techniques, and can be calculated from the initial values of the original problem to obtain a numerical answer.
In the first step above, the element equations are simple equations that locally approximate the original complex equations to be studied, where the original equations are often partial differential equations (PDE). To explain the approximation in this process, FEM is commonly introduced as a special case of Galerkin method. The process, in mathematical language, is to construct an integral of the inner product of the residual and the weight functions and set the integral to zero. In simple terms, it is a procedure that minimizes the error of approximation by fitting trial functions into the PDE. The residual is the error caused by the trial functions, and the weight functions are polynomial approximation functions that project the residual. The process eliminates all the spatial derivatives from the PDE, thus approximating the PDE locally with
These equation sets are the element equations. They are linear if the underlying PDE is linear, and vice versa. Algebraic equation sets that arise in the steady state problems are solved using numerical linear algebra methods, while ordinary differential equation sets that arise in the transient problems are solved by numerical integration using standard techniques such as Euler's method or the Runge-Kutta method.
In step (2) above, a global system of equations is generated from the element equations through a transformation of coordinates from the subdomains' local nodes to the domain's global nodes. This spatial transformation includes appropriate orientation adjustments as applied in relation to the reference coordinate system. The process is often carried out by FEM software using coordinate data generated from the subdomains.
FEM is best understood from its practical application, known as finite element analysis (FEA). FEA as applied in engineering is a computational tool for performing engineering analysis. It includes the use of mesh generation techniques for dividing a complex problem into small elements, as well as the use of software program coded with FEM algorithm. In applying FEA, the complex problem is usually a physical system with the underlying physics such as the Euler-Bernoulli beam equation, the heat equation, or the Navier-Stokes equations expressed in either PDE or integral equations, while the divided small elements of the complex problem represent different areas in the physical system.
FEA is a good choice for analyzing problems over complicated domains (like cars and oil pipelines), when the domain changes (as during a solid state reaction with a moving boundary), when the desired precision varies over the entire domain, or when the solution lacks smoothness. For instance, in a frontal crash simulation it is possible to increase prediction accuracy in "important" areas like the front of the car and reduce it in its rear (thus reducing cost of the simulation). Another example would be in numerical weather prediction, where it is more important to have accurate predictions over developing highly nonlinear phenomena (such as tropical cyclones in the atmosphere, or eddies in the ocean) rather than relatively calm areas.
History.
While it is difficult to quote a date of the invention of the finite element method, the method originated from the need to solve complex elasticity and structural analysis problems in civil and aeronautical engineering. Its development can be traced back to the work by A. Hrennikoff and R. Courant. In China, in the later 1950s and early 1960s, based on the computations of dam constructions, K. Feng proposed a systematic numerical method for solving partial differential equations. The method was called the finite difference method based on variation principle, which was another independent invention of finite element method. Although the approaches used by these pioneers are different, they share one essential characteristic: mesh discretization of a continuous domain into a set of discrete sub-domains, usually called elements.
Hrennikoff's work discretizes the domain by using a lattice analogy, while Courant's approach divides the domain into finite triangular subregions to solve second order elliptic partial differential equations (PDEs) that arise from the problem of torsion of a cylinder. Courant's contribution was evolutionary, drawing on a large body of earlier results for PDEs developed by Rayleigh, Ritz, and Galerkin.
The finite element method obtained its real impetus in the 1960s and 1970s by the developments of J. H. Argyris with co-workers at the University of Stuttgart, R. W. Clough with co-workers at UC Berkeley, O. C. Zienkiewicz with co-workers Ernest Hinton, Bruce Irons and others at the University of Swansea, Philippe G. Ciarlet at the University of Paris 6 and Richard Gallagher with co-workers at Cornell University. Further impetus was provided in these years by available open source finite element software programs. NASA sponsored the original version of NASTRAN, and UC Berkeley made the finite element program SAP IV widely available. In Norway the ship classification society Det Norske Veritas (now DNV GL) developed Sesam in 1969 for use in analysis of ships. A rigorous mathematical basis to the finite element method was provided in 1973 with the publication by Strang and Fix. The method has since been generalized for the numerical modeling of physical systems in a wide variety of engineering disciplines, e.g., electromagnetism, heat transfer, and fluid dynamics.
Technical discussion.
The structure of finite element methods.
Finite element methods are numerical methods for approximating the solutions of mathematical problems that are usually formulated so as to precisely state an idea of some aspect of physical reality.
A finite element method is characterized by a variational formulation, a discretization strategy, one or more solution algorithms and post-processing procedures.
Examples of variational formulation are the Galerkin method, the discontinuous Galerkin method, mixed methods, etc.
A discretization strategy is understood to mean a clearly defined set of procedures that cover (a) the creation of finite element meshes, (b) the definition of basis function on reference elements (also called shape functions) and (c) the mapping of reference elements onto the elements of the mesh. Examples of discretization strategies are the h-version, p-version, hp-version, x-FEM, isogeometric analysis, etc. Each discretization strategy has certain advantages and disadvantages. A reasonable criterion in selecting a discretization strategy is to realize nearly optimal performance for the broadest set of mathematical models in a particular model class.
There are various numerical solution algorithms that can be classified into two broad categories; direct and iterative solvers. These algorithms are designed to exploit the sparsity of matrices that depend on the choices of variational formulation and discretization strategy.
Postprocessing procedures are designed for the extraction of the data of interest from a finite element solution. In order to meet the requirements of solution verification, postprocessors need to provide for a posteriori error estimation in terms of the quantities of interest. When the errors of approximation are larger than what is considered acceptable then the discretization has to be changed either by an automated adaptive process or by action of the analyst. There are some very efficient postprocessors that provide for the realization of superconvergence.
Illustrative problems P1 and P2.
We will illustrate the finite element method using two sample problems from which the general method can be extrapolated. It is assumed that the reader is familiar with calculus and linear algebra.
P1 is a one-dimensional problem
where formula_2 is given, formula_3 is an unknown function of formula_4, and formula_5 is the second derivative of formula_3 with respect to formula_4.
P2 is a two-dimensional problem (Dirichlet problem)
where formula_9 is a connected open region in the formula_10 plane whose boundary formula_11 is "nice" (e.g., a smooth manifold or a polygon), and formula_12 and formula_13 denote the second derivatives with respect to formula_4 and formula_15, respectively.
The problem P1 can be solved "directly" by computing antiderivatives. However, this method of solving the boundary value problem (BVP) works only when there is one spatial dimension and does not generalize to higher-dimensional problems or to problems like formula_16. For this reason, we will develop the finite element method for P1 and outline its generalization to P2.
Our explanation will proceed in two steps, which mirror two essential steps one must take to solve a boundary value problem (BVP) using the FEM.
After this second step, we have concrete formulae for a large but finite-dimensional linear problem whose solution will approximately solve the original BVP. This finite-dimensional problem is then implemented on a computer.
Weak formulation.
The first step is to convert P1 and P2 into their equivalent weak formulations.
The weak form of P1.
If formula_3 solves P1, then for any smooth function formula_18 that satisfies the displacement boundary conditions, i.e. formula_19 at formula_20 and formula_21, we have
(1) formula_22
Conversely, if formula_3 with formula_24 satisfies (1) for every smooth function formula_25 then one may show that this formula_3 will solve P1. The proof is easier for twice continuously differentiable formula_3 (mean value theorem), but may be proved in a distributional sense as well.
By using integration by parts on the right-hand-side of (1), we obtain
(2)formula_28
where we have used the assumption that formula_29.
The weak form of P2.
If we integrate by parts using a form of Green's identities, we see that if formula_3 solves P2, then for any formula_18,
where formula_33 denotes the gradient and formula_34 denotes the dot product in the two-dimensional plane. Once more formula_35 can be turned into an inner product on a suitable space formula_36 of "once differentiable" functions of formula_9 that are zero on formula_11. We have also assumed that formula_39 (see Sobolev spaces). Existence and uniqueness of the solution can also be shown.
A proof outline of existence and uniqueness of the solution.
We can loosely think of formula_40 to be the absolutely continuous functions of formula_41 that are formula_42 at formula_20 and formula_21 (see Sobolev spaces). Such functions are (weakly) "once differentiable" and it turns out that the symmetric bilinear map formula_45 then defines an inner product which turns formula_40 into a Hilbert space (a detailed proof is nontrivial). On the other hand, the left-hand-side formula_47 is also an inner product, this time on the Lp space formula_48. An application of the Riesz representation theorem for Hilbert spaces shows that there is a unique formula_3 solving (2) and therefore P1. This solution is a-priori only a member of formula_40, but using elliptic regularity, will be smooth if formula_2 is.
Discretization.
P1 and P2 are ready to be discretized which leads to a common sub-problem (3). The basic idea is to replace the infinite-dimensional linear problem:
with a finite-dimensional version:
where formula_56 is a finite-dimensional subspace of formula_57. There are many possible choices for formula_56 (one possibility leads to the spectral method). However, for the finite element method we take formula_56 to be a space of piecewise polynomial functions.
For problem P1.
We take the interval formula_41, choose formula_61 values of formula_4 with formula_63 and we define formula_56 by:
where we define formula_66 and formula_67. Observe that functions in formula_56 are not differentiable according to the elementary definition of calculus. Indeed, if formula_69 then the derivative is typically not defined at any formula_70, formula_71. However, the derivative exists at every other value of formula_4 and one can use this derivative for the purpose of integration by parts.
For problem P2.
We need formula_56 to be a set of functions of formula_9. In the figure on the right, we have illustrated a triangulation of a 15 sided polygonal region formula_9 in the plane (below), and a piecewise linear function (above, in color) of this polygon which is linear on each triangle of the triangulation; the space formula_56 would consist of functions that are linear on each triangle of the chosen triangulation.
One often reads formula_77 instead of formula_56 in the literature. The reason is that one hopes that as the underlying triangular grid becomes finer and finer, the solution of the discrete problem (3) will in some sense converge to the solution of the original boundary value problem P2. The triangulation is then indexed by a real valued parameter formula_79 which one takes to be very small. This parameter will be related to the size of the largest or average triangle in the triangulation. As we refine the triangulation, the space of piecewise linear functions formula_56 must also change with formula_81, hence the notation formula_77. Since we do not perform such an analysis, we will not use this notation.
Choosing a basis.
To complete the discretization, we must select a basis of formula_56. In the one-dimensional case, for each control point formula_84 we will choose the piecewise linear function formula_85 in formula_56 whose value is formula_87 at formula_84 and zero at every formula_89, i.e.,
for formula_91; this basis is a shifted and scaled tent function. For the two-dimensional case, we choose again one basis function formula_85 per vertex formula_84 of the triangulation of the planar region formula_9. The function formula_85 is the unique function of formula_56 whose value is formula_87 at formula_84 and zero at every formula_89.
Depending on the author, the word "element" in "finite element method" refers either to the triangles in the domain, the piecewise linear basis function, or both. So for instance, an author interested in curved domains might replace the triangles with curved primitives, and so might describe the elements as being curvilinear. On the other hand, some authors replace "piecewise linear" by "piecewise quadratic" or even "piecewise polynomial". The author might then say "higher order element" instead of "higher degree polynomial". Finite element method is not restricted to triangles (or tetrahedra in 3-d, or higher order simplexes in multidimensional spaces), but can be defined on quadrilateral subdomains (hexahedra, prisms, or pyramids in 3-d, and so on). Higher order shapes (curvilinear elements) can be defined with polynomial and even non-polynomial shapes (e.g. ellipse or circle).
Examples of methods that use higher degree piecewise polynomial basis functions are the
hp-FEM and spectral FEM.
More advanced implementations (adaptive finite element methods) utilize a method to assess the quality of the results (based on error estimation theory) and modify the mesh during the solution aiming to achieve approximate solution within some bounds from the 'exact' solution of the continuum problem. Mesh adaptivity may utilize various techniques, the most popular are:
Small support of the basis.
The primary advantage of this choice of basis is that the inner products
and
will be zero for almost all formula_102.
In the one dimensional case, the support of formula_85 is the interval formula_106. Hence, the integrands of formula_103 and "formula_108" are identically zero whenever formula_109.
Similarly, in the planar case, if formula_110 and formula_84 do not share an edge of the triangulation, then the integrals
and
are both zero.
Matrix form of the problem.
If we write formula_114 and formula_115 then problem (3), taking formula_116 for formula_117, becomes
If we denote by formula_120 and formula_121 the column vectors formula_122 and formula_123, and if we let
and
be matrices whose entries are
and
then we may rephrase (4) as
It is not necessary to assume formula_115. For a general function formula_130, problem (3) with formula_116 for formula_117 becomes actually simpler, since no matrix formula_133 is used,
where formula_135 and formula_136 for formula_117.
As we have discussed before, most of the entries of formula_138 and formula_133 are zero because the basis functions formula_85 have small support. So we now have to solve a linear system in the unknown formula_120 where most of the entries of the matrix formula_138, which we need to invert, are zero.
Such matrices are known as sparse matrices, and there are efficient solvers for such problems (much more efficient than actually inverting the matrix.) In addition, formula_138 is symmetric and positive definite, so a technique such as the conjugate gradient method is favored. For problems that are not too large, sparse LU decompositions and Cholesky decompositions still work well. For instance, MATLAB's backslash operator (which uses sparse LU, sparse Cholesky, and other factorization methods) can be sufficient for meshes with a hundred thousand vertices.
The matrix formula_138 is usually referred to as the stiffness matrix, while the matrix formula_133 is dubbed the mass matrix.
General form of the finite element method.
In general, the finite element method is characterized by the following process.
A separate consideration is the smoothness of the basis functions. For second order elliptic boundary value problems, piecewise polynomial basis function that are merely continuous suffice (i.e., the derivatives are discontinuous.) For higher order partial differential equations, one must use smoother basis functions. For instance, for a fourth order problem such as formula_147, one may use piecewise quadratic basis functions that are formula_148.
Another consideration is the relation of the finite-dimensional space formula_56 to its infinite-dimensional counterpart, in the examples above formula_57. A conforming element method is one in which the space formula_56 is a subspace of the element space for the continuous problem. The example above is such a method. If this condition is not satisfied, we obtain a nonconforming element method, an example of which is the space of piecewise linear functions over the mesh which are continuous at each edge midpoint. Since these functions are in general discontinuous along the edges, this finite-dimensional space is not a subspace of the original formula_57.
Typically, one has an algorithm for taking a given mesh and subdividing it. If the main method for increasing precision is to subdivide the mesh, one has an "h"-method ("h" is customarily the diameter of the largest element in the mesh.) In this manner, if one shows that the error with a grid formula_81 is bounded above by formula_154, for some formula_155 and formula_156, then one has an order "p" method. Under certain hypotheses (for instance, if the domain is convex), a piecewise polynomial of order formula_157 method will have an error of order formula_158.
If instead of making "h" smaller, one increases the degree of the polynomials used in the basis function, one has a "p"-method. If one combines these two refinement types, one obtains an "hp"-method (hp-FEM). In the hp-FEM, the polynomial degrees can vary from element to element. High order methods with large uniform "p" are called spectral finite element methods (SFEM). These are not to be confused with spectral methods.
For vector partial differential equations, the basis functions may take values in formula_159.
Various types of finite element methods.
AEM.
The Applied Element Method, or AEM combines features of both FEM and Discrete element method, or (DEM).
Generalized finite element method.
The generalized finite element method (GFEM) uses local spaces consisting of functions, not necessarily polynomials, that reflect the available information on the unknown solution and thus ensure good local approximation. Then a partition of unity is used to “bond” these spaces together to form the approximating subspace. The effectiveness of GFEM has been shown when applied to problems with domains having complicated boundaries, problems with micro-scales, and problems with boundary layers.
hp-FEM.
The hp-FEM combines adaptively, elements with variable size "h" and polynomial degree "p" in order to achieve exceptionally fast, exponential convergence rates.
hpk-FEM.
The hpk-FEM combines adaptively, elements with variable size "h", polynomial degree of the local approximations "p" and global differentiability of the local approximations "(k-1)" in order to achieve best convergence rates.
XFEM.
The extended finite element method (XFEM), is a numerical technique based on the generalized finite element method (GFEM) and the partition of unity method (PUM). It extends the classical finite element method (FEM) approach by enriching the solution space for solutions to differential equations with discontinuous functions. Enriched finite element methods extend, or enrich, the approximation space so that it is able to naturally reproduce the challenging feature associated with the problem of interest: the discontinuity, singularity, boundary layer, etc. It was shown that for some problems, such an embedding of the problem's feature into the approximation space can significantly improve convergence rates and accuracy. Moreover, treating problems with discontinuities with extended Finite Element Methods suppresses the need to mesh and re-mesh the discontinuity surfaces, thus alleviating the computational costs and projection errors associated with conventional finite element methods, at the cost of restricting the discontinuities to mesh edges.
There exists several research codes implementing this technique to various degrees.
1. GetFEM++
2. xfem++
3. openxfem++
XFEM has also been implemented in code like Altair Radioss, ASTER, Morfeo, and Abaqus. It is increasingly being adopted by other commercial finite element software, with a few plugins and actual core implementations available (ANSYS, SAMCEF, OOFELIE, etc.).
Comparison to the finite difference method.
The finite difference method (FDM) is an alternative way of approximating solutions of PDEs. The differences between FEM and FDM are:
Generally, FEM is the method of choice in all types of analysis in structural mechanics (i.e. solving for deformation and stresses in solid bodies or dynamics of structures) while computational fluid dynamics (CFD) tends to use FDM or other methods like finite volume method (FVM). CFD problems usually require discretization of the problem into a large number of cells/gridpoints (millions and more), therefore cost of the solution favors simpler, lower order approximation within each cell. This is especially true for 'external flow' problems, like air flow around the car or airplane, or weather simulation.
Application.
A variety of specializations under the umbrella of the mechanical engineering discipline (such as aeronautical, biomechanical, and automotive industries) commonly use integrated FEM in design and development of their products. Several modern FEM packages include specific components such as thermal, electromagnetic, fluid, and structural working environments. In a structural simulation, FEM helps tremendously in producing stiffness and strength visualizations and also in minimizing weight, materials, and costs.
FEM allows detailed visualization of where structures bend or twist, and indicates the distribution of stresses and displacements. FEM software provides a wide range of simulation options for controlling the complexity of both modeling and analysis of a system. Similarly, the desired level of accuracy required and associated computational time requirements can be managed simultaneously to address most engineering applications. FEM allows entire designs to be constructed, refined, and optimized before the design is manufactured.
This powerful design tool has significantly improved both the standard of engineering designs and the methodology of the design process in many industrial applications. The introduction of FEM has substantially decreased the time to take products from concept to the production line. It is primarily through improved initial prototype designs using FEM that testing and development have been accelerated. In summary, benefits of FEM include increased accuracy, enhanced design and better insight into critical design parameters, virtual prototyping, fewer hardware prototypes, a faster and less expensive design cycle, increased productivity, and increased revenue.
FEA has also been proposed to use in stochastic modelling for numerically solving probability models.

</doc>
<doc id="323707" url="https://en.wikipedia.org/wiki?curid=323707" title="Finite group">
Finite group

In abstract algebra, a finite group is a mathematical group with a finite number of elements. A group is a set of elements together with an operation which associates, to each ordered pair of elements, an element of the set. With a finite group, the set is finite. 
History.
During the twentieth century, mathematicians investigated some aspects of the theory of finite groups in great depth, especially the local theory of finite groups and the theory of solvable and nilpotent groups. As a consequence, the complete classification of finite simple groups was achieved, meaning that all those simple groups from which all finite groups can be built are now known.
During the second half of the twentieth century, mathematicians such as Chevalley and Steinberg also increased our understanding of finite analogs of classical groups, and other related groups. One such family of groups is the family of general linear groups over finite fields. 
Finite groups often occur when considering symmetry of mathematical or
physical objects, when those objects admit just a finite number of structure-preserving transformations. The theory of Lie groups,
which may be viewed as dealing with "continuous symmetry", is strongly influenced by the associated Weyl groups. These are finite groups generated by reflections which act on a finite-dimensional Euclidean space. The properties of finite groups can thus play a role in subjects such as theoretical physics and chemistry.
Examples.
Permutation groups.
The symmetric group S"n" on a finite set of "n" symbols is the group whose elements are all the permutations of the "n" symbols, and whose group operation is the composition of such permutations, which are treated as bijective functions from the set of symbols to itself. Since there are "n"! ("n" factorial) possible permutations of a set of "n" symbols, it follows that the order (the number of elements) of the symmetric group S"n" is "n"!.
Cyclic groups.
A cyclic group Z"n" is a group all of whose elements are powers of a particular element a where , the identity. A typical realization of this group is as the complex "n"th roots of unity. Sending a to a primitive root of unity gives an isomorphism between the two. This can be done with any finite cyclic group.
Finite abelian groups.
An abelian group, also called a commutative group, is a group in which the result of applying the group operation to two group elements does not depend on their order (the axiom of commutativity). They are named after Niels Henrik Abel.
An arbitrary finite abelian group is isomorphic to a direct sum of finite cyclic groups of prime power order, and these orders are uniquely determined, forming a complete system of invariants. The automorphism group of a finite abelian group can be described directly in terms of these invariants. The theory had been first developed in the 1879 paper of Georg Frobenius and Ludwig Stickelberger and later was both simplified and generalized to finitely generated modules over a principal ideal domain, forming an important chapter of linear algebra.
Groups of Lie type.
A group of Lie type is a group closely related to the group "G"("k") of rational points of a reductive linear algebraic group "G" with values in the field "k". Finite groups of Lie type give the bulk of nonabelian finite simple groups. Special cases include the classical groups, the Chevalley groups, the Steinberg groups, and the Suzuki–Ree groups.
Finite groups of Lie type were among the first groups to be considered in mathematics, after cyclic, symmetric and alternating groups, with the projective special linear groups over prime finite fields, PSL(2, "p") being constructed by Évariste Galois in the 1830s. The systematic exploration of finite groups of Lie type started with Camille Jordan's theorem that the projective special linear group PSL(2, "q") is simple for "q" ≠ 2, 3. This theorem generalizes to projective groups of higher dimensions and gives an important infinite family PSL("n", "q") of finite simple groups. Other classical groups were studied by Leonard Dickson in the beginning of 20th century. In the 1950s Claude Chevalley realized that after an appropriate reformulation, many theorems about semisimple Lie groups admit analogues for algebraic groups over an arbitrary field "k", leading to construction of what are now called "Chevalley groups". Moreover, as in the case of compact simple Lie groups, the corresponding groups turned out to be almost simple as abstract groups ("Tits simplicity theorem"). Although it was known since 19th century that other finite simple groups exist (for example, Mathieu groups), gradually a belief formed that nearly all finite simple groups can be accounted for by appropriate extensions of Chevalley's construction, together with cyclic and alternating groups. Moreover, the exceptions, the sporadic groups, share many properties with the finite groups of Lie type, and in particular, can be constructed and characterized based on their "geometry" in the sense of Tits.
The belief has now become a theorem – the classification of finite simple groups. Inspection of the list of finite simple groups shows that groups of Lie type over a finite field include all the finite simple groups other than the cyclic groups, the alternating groups, the Tits group, and the 26 sporadic simple groups.
Main theorems.
Lagrange's theorem.
For any finite group "G", the order (number of elements) of every subgroup "H" of "G" divides the order of "G". The theorem is named after Joseph-Louis Lagrange.
Sylow theorems.
This provides a partial converse to Lagrange's theorem giving information about how many subgroups of a given order are contained in "G".
Cayley's theorem.
Cayley's theorem, named in honour of Arthur Cayley, states that every group "G" is isomorphic to a subgroup of the symmetric group acting on "G". This can be understood as an example of the group action of "G" on the elements of "G".
Burnside theorem.
Burnside's theorem in group theory states that if "G" is a finite group of order 
where "p" and "q" are prime numbers, and "a" and "b" are non-negative integers, then "G" is solvable. Hence each
non-Abelian finite simple group has order divisible by at least three distinct primes.
Feit-Thompson theorem.
The Feit–Thompson theorem, or odd order theorem, states that every finite group of odd order is solvable. It was proved by 
Classification of finite simple groups.
The classification of the finite simple groups is a theorem stating that every finite simple group belongs to one of four categories described below. These groups can be seen as the basic building blocks of all finite groups, in a way reminiscent of the way the prime numbers are the basic building blocks of the natural numbers. The Jordan–Hölder theorem is a more precise way of stating this fact about finite groups. However, a significant difference with respect to the case of integer factorization is that such "building blocks" do not necessarily determine uniquely a group, since there might be many non-isomorphic groups with the same composition series or, put in another way, the extension problem does not have a unique solution.
The proof of the theorem consists of tens of thousands of pages in several hundred journal articles written by about 100 authors, published mostly between 1955 and 2004. Gorenstein (d.1992), Lyons, and Solomon are gradually publishing a simplified and revised version of the proof.
Number of groups of a given order.
Given a positive integer "n", it is not at all a routine matter to determine how many isomorphism types of groups of order "n" there are. Every group of prime order is cyclic, because Lagrange's theorem implies that the cyclic subgroup generated by 
any of its non-identity elements is the whole group.
If "n" is the square of a prime, then there are exactly two possible isomorphism types of group of order "n", both of which are abelian. If "n" is a higher power of a prime, then results of Graham Higman and Charles Sims give asymptotically correct estimates for the number of isomorphism types of groups of order "n", and the number grows very rapidly as the power increases.
Depending on the prime factorization of "n," some restrictions may be placed on the structure of groups of order "n," as a consequence, for example, of results such as the Sylow theorems. For example, every group of order "pq" is cyclic when "q" < "p" are primes with "p"-1 not divisible by "q". For a necessary and sufficient condition, see cyclic number.
If "n" is squarefree, then any group of order "n" is solvable. Burnside's theorem, proved using group characters, states that every group of order "n" is solvable when "n" is divisible by fewer than three distinct primes, i.e. if "n" = "p""a""q""b", where "p" and "q" are prime numbers, and "a" and "b" are non-negative integers. By the Feit–Thompson theorem, which has a long and complicated proof, every group of order "n" is solvable when "n" is odd.
For every positive integer "n", most groups of order "n" are solvable. To see this for any particular order is usually not difficult (for example, there is, up to isomorphism, one non-solvable group and 12 solvable groups of order 60) but the proof of this for all orders uses the classification of finite simple groups. For any positive integer "n" there are at most two simple groups of order "n", and there are infinitely many positive integers "n" for which there are two non-isomorphic simple groups of order "n".

</doc>
<doc id="13005617" url="https://en.wikipedia.org/wiki?curid=13005617" title="Flat (geometry)">
Flat (geometry)

In geometry, a flat is a subset of -dimensional space that is congruent to a Euclidean space of lower dimension. The flats in two-dimensional space are points and lines, and the flats in three-dimensional space are points, lines, and planes.
In -dimensional space, there are flats of every dimension from 0 to . Flats of dimension are called hyperplanes.
Flats are similar to linear subspaces, except that they need not pass through the origin. If Euclidean space is considered as an affine space, the flats are precisely the affine subspaces. Flats are important in linear algebra, where they provide a geometric realization of the solution set for a system of linear equations.
A flat is also called a "linear manifold" or "linear variety".
Descriptions.
By equations.
A flat can be described by a system of linear equations. For example, a line in two-dimensional space can be described by a single linear equation involving and :
In three-dimensional space, a single linear equation involving , , and defines a plane, while a pair of linear equations can be used to describe a line. In general, a linear equation in variables describes a hyperplane, and a system of linear equations describes the intersection of those hyperplanes. Assuming the equations are consistent and linearly independent, a system of equations describes a flat of dimension .
Parametric.
A flat can also be described by a system of linear parametric equations. A line can be described by equations involving one parameter:
while the description of a plane would require two parameters:
In general, a parameterization of a flat of dimension would require parameters .
Operations and relations on flats.
Intersecting, parallel, and skew flats.
An intersection of flats is either a flat or the empty set.
If every line from the first flat is parallel to some line from the second flat, then these flats are parallel. Two parallel flats of the same dimension either coincide or do not intersect; they can be described by two systems of linear equations which differ only in their right-hand sides.
If flats do not intersect, and no line from the first flat is parallel to a line from the second flat, then these are skew flats. It is possible only if sum of their dimensions is less than dimension of the ambient space.
Join.
For two flats of dimensions and there exists the minimal flat which contains them, of dimension at most . If two flats intersect, then the dimension of the containing flat equals to dimension of the intersection.
Properties of operations.
These two operations (referred to as "meet" and "join") make the set of all flats in the Euclidean -space a lattice and can build systematic coordinates for flats in any dimension, leading to Grassmann coordinates or dual Grassmann coordinates. For example, a line in three-dimensional space is determined by two distinct points or by two distinct planes.
Though, the lattice of all flats is not a distributive lattice.
If two lines and intersect, then is a point. If is a point not lying on the same plane, then , both representing a line. But when and are parallel, this distributivity fails, giving on the left-hand side and a third parallel line on the right-hand side. The ambient space would be a projective space to accommodate intersections of parallel flats, which lead to objects "at infinity".
Euclidean geometry.
The aforementioned facts do not depend on the structure being that of Euclidean space (namely, involving Euclidean distance) and are correct in any affine space. In a Euclidean space:

</doc>
<doc id="1928465" url="https://en.wikipedia.org/wiki?curid=1928465" title="Flavour (particle physics)">
Flavour (particle physics)

In particle physics, flavour or flavor refers to a species of an elementary particle. The Standard Model counts six flavours of quarks and six flavours of leptons. They are conventionally parameterized with "flavour quantum numbers" that are assigned to all subatomic particles, including composite ones. For hadrons, these quantum numbers depend on the numbers of constituent quarks of each particular flavour.
Intuitive description.
Elementary particles are not eternal and indestructible. Unlike in classical mechanics, where forces only change a particle's momentum, the weak force can alter the essence of a particle, even an elementary particle. This means that it can convert one quark to another quark with different mass and electric charge, and the same for leptons. From the point of view of quantum mechanics, changing the flavour of a particle by the weak force is no different in principle from changing its spin by electromagnetic interaction, and should be described with quantum numbers as well. In particular, flavour states may undergo quantum superposition.
In atomic physics the principal quantum number of an electron specifies the electron shell in which it resides, which determines the energy level of the whole atom. In an analogous way, the five flavour quantum numbers of a quark specify which of six flavours (u, d, s, c, b, t) it has, and when these quarks are combined this results in different types of baryons and mesons with different masses, electric charges, and decay modes.
Flavour symmetry.
If there are two or more particles which have identical interactions, then they may be interchanged without affecting the physics. Any (complex) linear combination of these two particles give the same physics, as long as the combinations are orthogonal or perpendicular to each other. 
In other words, the theory possesses symmetry transformations such as formula_1, where and are the two fields (representing the various generations of leptons and quarks, see below), and is any unitary matrix with a unit determinant. Such matrices form a Lie group called SU(2) (see special unitary group). This is an example of flavour symmetry.
In quantum chromodynamics, flavour is a conserved global symmetry. In the electroweak theory, on the other hand, this symmetry is broken, and flavour changing processes exist, such as quark decay or neutrino oscillations.
Flavour quantum numbers.
Leptons.
All leptons carry a lepton number . In addition, leptons carry weak isospin, , which is − for the three charged leptons (i.e. electron, muon and tau) and + for the three associated neutrinos. Each doublet of a charged lepton and a neutrino consisting of opposite are said to constitute one generation of leptons. In addition, one defines a quantum number called weak hypercharge, , which is −1 for all left-handed leptons. Weak isospin and weak hypercharge are gauged in the Standard Model.
Leptons may be assigned the six flavour quantum numbers: electron number, muon number, tau number, and corresponding numbers for the neutrinos. These are conserved in strong and electromagnetic interactions, but violated by weak interactions. Therefore, such flavour quantum numbers are not of great use. A separate quantum number for each generation is more useful: electronic lepton number (+1 for electrons and electron neutrinos), muonic lepton number (+1 for muons and muon neutrinos), and tauonic lepton number (+1 for tau leptons and tau neutrinos). However, even these numbers are not absolutely conserved, as neutrinos of different generations can mix; that is, a neutrino of one flavour can transform into another flavour. The strength of such mixings is specified by a matrix called the Pontecorvo–Maki–Nakagawa–Sakata matrix (PMNS matrix).
Quarks.
All quarks carry a baryon number . They also all carry weak isospin, . The positive- quarks (up, charm, and top quarks) are called "up-type quarks" and negative- quarks (down, strange, and bottom quarks) are called "down-type quarks". Each doublet of up and down type quarks constitutes one generation of quarks.
For all the quark flavour quantum numbers (strangeness, charm, topness and bottomness) the convention is that the flavour charge and the electric charge of a quark have the same sign. Thus any flavour carried by a charged meson has the same sign as its charge. Quarks have the following flavour quantum numbers:
These five quantum numbers, together with baryon number (which is not a flavour quantum number) completely specify numbers of all 6 quark flavours separately (as , i.e. an antiquark is counted with the minus sign). They are conserved by both the electromagnetic and strong interactions (but not the weak interaction). From them can be built the derived quantum numbers:
The terms "strange" and "strangeness" predate the discovery of the quark, but continued to be used after its discovery for the sake of continuity (i.e. the strangeness of each type of hadron remained the same); strangeness of anti-particles being referred to as +1, and particles as −1 as per the original definition. Strangeness was introduced to explain the rate of decay of newly discovered particles, such as the kaon, and was used in the Eightfold Way classification of hadrons and in subsequent quark models. These quantum numbers are preserved under strong and electromagnetic interactions, but not under weak interactions.
For first-order weak decays, that is processes involving only one quark decay, these quantum numbers (e.g. charm) can only vary by 1 (); . Since first-order processes are more common than second-order processes (involving two quark decays), this can be used as an approximate "selection rule" for weak decays.
A quark of a given flavour is an eigenstate of the weak interaction part of the Hamiltonian: it will interact in a definite way with the W and Z bosons. On the other hand, a fermion of a fixed mass (an eigenstate of the kinetic and strong interaction parts of the Hamiltonian) is normally a superposition of various flavours. As a result, the flavour content of a quantum state may change as it propagates freely. The transformation from flavour to mass basis for quarks is given by the Cabibbo–Kobayashi–Maskawa matrix (CKM matrix). This matrix is analogous to the PMNS matrix for neutrinos, and defines the strength of flavour changes under weak interactions of quarks.
The CKM matrix allows for CP violation if there are at least three generations.
Antiparticles and hadrons.
Flavour quantum numbers are additive. Hence antiparticles have flavour equal in magnitude to the particle but opposite in sign. Hadrons inherit their flavour quantum number from their valence quarks: this is the basis of the classification in the quark model. The relations between the hypercharge, electric charge and other flavour quantum numbers hold for hadrons as well as quarks.
Quantum chromodynamics.
Quantum chromodynamics (QCD) contains six flavours of quarks. However, their masses differ and as a result they are not strictly interchangeable with each other. The up and down flavours are close to having equal masses, and the theory of these two quarks possesses an approximate SU(2) symmetry (isospin symmetry).
Under some circumstances, the masses of the quarks can be neglected entirely. One can then make flavour transformations independently on the left- and right-handed parts of each quark field. The flavour group is then a chiral group .
If all quarks had non-zero but equal masses, then this chiral symmetry is broken to the "vector symmetry" of the "diagonal flavour group" , which applies the same transformation to both helicities of the quarks. Such a reduction of the symmetry is called "explicit symmetry breaking". The amount of explicit symmetry breaking is controlled by the current quark masses in QCD.
Even if quarks are massless, chiral flavour symmetry can be spontaneously broken if the vacuum of the theory contains a chiral condensate (as it does in low-energy QCD). This gives rise to an effective mass for the quarks, often identified with the valence quark mass in QCD.
Symmetries of QCD.
Analysis of experiments indicate that the current quark masses of the lighter flavours of quarks are much smaller than the QCD scale, ΛQCD, hence chiral flavour symmetry is a good approximation to QCD for the up, down and strange quarks. The success of chiral perturbation theory and the even more naive chiral models spring from this fact. The valence quark masses extracted from the quark model are much larger than the current quark mass. This indicates that QCD has spontaneous chiral symmetry breaking with the formation of a chiral condensate. Other phases of QCD may break the chiral flavour symmetries in other ways.
Conservation laws.
All of the various charges discussed above are conserved by the fact that the charge operator is best understood as the generator of a symmetry that commutes with the Hamiltonian. Thus, the eigenvalues of the various charge operators are conserved.
Absolutely conserved flavour quantum numbers are: (including the baryon number for completeness)
In some theories, the individual baryon and lepton number conservation can be violated, if the difference between them () is conserved (see chiral anomaly). All other flavour quantum numbers are violated by the electroweak interactions. Strong interactions conserve all flavours.
History.
Some of the historical events that lead to the development of flavour symmetry are discussed in the article on isospin.

</doc>
<doc id="43590" url="https://en.wikipedia.org/wiki?curid=43590" title="Flux">
Flux

Flux is a simple and ubiquitous concept throughout physics and applied mathematics describing the flow of a physical property in space, frequently also with time variation. It is the basis of the field concept in physics and mathematics. There exist two common usages of the term flux, each with rigorous mathematical frameworks: For transport phenomena, flux is defined as a vector, whereas in electromagnetism, flux is a scalar quantity, as defined by the surface integral. The terms "flux", "current", "flux density", "current density", can sometimes be used interchangeably and ambiguously, though the terms used below match those of the contexts in the literature.
Origin of the term.
The word "flux" comes from Latin: "fluxus" means "flow", and "fluere" is "to flow". As "fluxion", this term was introduced into differential calculus by Isaac Newton.
Flux as flow rate per unit area.
In transport phenomena (heat transfer, mass transfer and fluid dynamics), flux is defined as the "rate of flow of a property per unit area," which has the dimensions [quantity]·[time]−1·[area]−1. For example, the magnitude of a river's current, i.e. the amount of water that flows through a cross-section of the river each second, or the amount of sunlight that lands on a patch of ground each second is also a kind of flux.
General mathematical definition (transport).
In this definition, flux is generally a vector due to the widespread and useful definition of vector area, although there are some cases where only the magnitude is important (like in number fluxes, see below). The frequent symbol is "j" (or "J"), and a definition for scalar flux of physical quantity "q" is the limit:
where:
is the flow of quantity "q" per unit time "t", and "A" is the area through which the quantity flows.
For vector flux, the surface integral of j over a surface "S", followed by an integral over the time duration "t"1 to "t"2, gives the total amount of the property flowing through the surface in that time ("t"2 − "t"1):
The area required to calculate the flux is real or imaginary, flat or curved, either as a cross-sectional area or a surface. The vector area is a combination of the magnitude of the area through which the mass passes through, "A", and a unit vector normal to the area, formula_4. The relation is formula_5.
If the flux j passes through the area at an angle θ to the area normal formula_4, then
where · is the dot product of the unit vectors. This is, the component of flux passing through the surface (i.e. normal to it) is "j" cos θ, while the component of flux passing tangential to the area is "j" sin θ, but there is "no" flux actually passing "through" the area in the tangential direction. The "only" component of flux passing normal to the area is the cosine component.
One could argue, based on the work of James Clerk Maxwell, that the transport definition precedes the more recent way the term is used in electromagnetism. The specific quote from Maxwell is:
Transport fluxes.
Eight of the most common forms of flux from the transport phenomena literature are defined as follows:
These fluxes are vectors at each point in space, and have a definite magnitude and direction. Also, one can take the divergence of any of these fluxes to determine the accumulation rate of the quantity in a control volume around a given point in space. For incompressible flow, the divergence of the volume flux is zero.
Chemical diffusion.
As mentioned above, chemical molar flux of a component A in an isothermal, isobaric system is defined in Fick's law of diffusion as:
where the nabla symbol ∇ denotes the gradient operator, "DAB" is the diffusion coefficient (m2·s−1) of component A diffusing through component B, "cA" is the concentration (mol/m3) of component A.
This flux has units of mol·m−2·s−1, and fits Maxwell's original definition of flux.
For dilute gases, kinetic molecular theory relates the diffusion coefficient "D" to the particle density "n" = "N"/"V", the molecular mass "m", the collision cross section formula_9, and the absolute temperature "T" by
where the second factor is the mean free path and the square root (with Boltzmann's constant "k") is the mean velocity of the particles.
In turbulent flows, the transport by eddy motion can be expressed as a grossly increased diffusion coefficient.
Quantum mechanics.
In quantum mechanics, particles of mass "m" in the quantum state ψ(r, t) have a probability density defined as
So the probability of finding a particle in a differential volume element d3r is
Then the number of particles passing perpendicularly through unit area of a cross-section per unit time is the probability flux;
This is sometimes referred to as the probability current or current density, or probability flux density.
Flux as a surface integral.
General mathematical definition (surface integral).
As a mathematical concept, flux is represented by the surface integral of a vector field,
where F is a vector field, and d"A" is the vector area of the surface "A", directed as the surface normal.
The surface has to be orientable, i.e. two sides can be distinguished: the surface does not fold back onto itself. Also, the surface has to be actually oriented, i.e. we use a convention as to flowing which way is counted positive; flowing backward is then counted negative.
The surface normal is directed usually by the right-hand rule.
Conversely, one can consider the flux the more fundamental quantity and call the vector field the flux density.
Often a vector field is drawn by curves (field lines) following the "flow"; the magnitude of the vector field is then the line density, and the flux through a surface is the number of lines. Lines originate from areas of positive divergence (sources) and end at areas of negative divergence (sinks).
See also the image at right: the number of red arrows passing through a unit area is the flux density, the curve encircling the red arrows denotes the boundary of the surface, and the orientation of the arrows with respect to the surface denotes the sign of the inner product of the vector field with the surface normals.
If the surface encloses a 3D region, usually the surface is oriented such that the influx is counted positive; the opposite is the outflux.
The divergence theorem states that the net outflux through a closed surface, in other words the net outflux from a 3D region, is found by adding the local net outflow from each point in the region (which is expressed by the divergence).
If the surface is not closed, it has an oriented curve as boundary. Stokes' theorem states that the flux of the curl of a vector field is the line integral of the vector field over this boundary. This path integral is also called circulation, especially in fluid dynamics. Thus the curl is the circulation density.
We can apply the flux and these theorems to many disciplines in which we see currents, forces, etc., applied through areas.
Electromagnetism.
One way to better understand the concept of flux in electromagnetism is by comparing it to a butterfly net. The amount of air moving through the net at any given instant in time is the flux. If the wind speed is high, then the flux through the net is large. If the net is made bigger, then the flux is larger even though the wind speed is the same. For the most air to move through the net, the opening of the net must be facing the direction the wind is blowing. If the net is parallel to the wind, then no wind will be moving through the net. The simplest way to think of flux is "how much air goes through the net", where the air is a velocity field and the net is the boundary of an imaginary surface.
Electric flux.
Two forms of electric flux are used, one for the E-field:
and one for the D-field (called the electric displacement):
This quantity arises in Gauss's law – which states that the flux of the electric field E out of a closed surface is proportional to the electric charge "QA" enclosed in the surface (independent of how that charge is distributed), the integral form is:
where ε0 is the permittivity of free space.
If one considers the flux of the electric field vector, E, for a tube near a point charge in the field the charge but not containing it with sides formed by lines tangent to the field, the flux for the sides is zero and there is an equal and opposite flux at both ends of the tube. This is a consequence of Gauss's Law applied to an inverse square field. The flux for any cross-sectional surface of the tube will be the same. The total flux for any surface surrounding a charge "q" is "q"/ε0.
In free space the electric displacement is given by the constitutive relation D = ε0 E, so for any bounding surface the D-field flux equals the charge "QA" within it. Here the expression "flux of" indicates a mathematical operation and, as can be seen, the result is not necessarily a "flow", since nothing actually flows along electric field lines.
Magnetic flux.
The magnetic flux density (magnetic field) having the unit Wb/m2 (Tesla) is denoted by B, and magnetic flux is defined analogously:
with the same notation above. The quantity arises in Faraday's law of induction, in integral form:
where "d"{{ell}} is an infinitesimal vector line element of the closed curve "C", with magnitude equal to the length of the infinitesimal line element, and direction given by the tangent to the curve "C", with the sign determined by the integration direction.
The time-rate of change of the magnetic flux through a loop of wire is minus the electromotive force created in that wire. The direction is such that if current is allowed to pass through the wire, the electromotive force will cause a current which "opposes" the change in magnetic field by itself producing a magnetic field opposite to the change. This is the basis for inductors and many electric generators.
Poynting flux.
Using this definition, the flux of the Poynting vector S over a specified surface is the rate at which electromagnetic energy flows through that surface, defined like before:
The flux of the Poynting vector through a surface is the electromagnetic power, or energy per unit time, passing through that surface. This is commonly used in analysis of electromagnetic radiation, but has application to other electromagnetic systems as well.
Confusingly, the Poynting vector is sometimes called the "power flux", which is an example of the first usage of flux, above. It has units of watts per square metre (W/m2).

</doc>
<doc id="10902" url="https://en.wikipedia.org/wiki?curid=10902" title="Force">
Force

In physics, a force is any interaction that, when unopposed, will change the motion of an object. In other words, a force can cause an object with mass to change its velocity (which includes to begin moving from a state of rest), i.e., to accelerate. Force can also be described by intuitive concepts such as a push or a pull. A force has both magnitude and direction, making it a vector quantity. It is measured in the SI unit of newtons and represented by the symbol F.
The original form of Newton's second law states that the net force acting upon an object is equal to the rate at which its momentum changes with time. If the mass of the object is constant, this law implies that the acceleration of an object is directly proportional to the net force acting on the object, is in the direction of the net force, and is inversely proportional to the mass of the object
Related concepts to force include: thrust, which increases the velocity of an object; drag, which decreases the velocity of an object; and torque, which produces changes in rotational speed of an object. In an extended body, each part usually applies forces on the adjacent parts; the distribution of such forces through the body is the so-called mechanical stress. Pressure is a simple type of stress. Stress usually causes deformation of solid materials, or flow in fluids.
Development of the concept.
Philosophers in antiquity used the concept of force in the study of stationary and moving objects and simple machines, but thinkers such as Aristotle and Archimedes retained fundamental errors in understanding force. In part this was due to an incomplete understanding of the sometimes non-obvious force of friction, and a consequently inadequate view of the nature of natural motion. A fundamental error was the belief that a force is required to maintain motion, even at a constant velocity. Most of the previous misunderstandings about motion and force were eventually corrected by Sir Isaac Newton; with his mathematical insight, he formulated laws of motion that were not improved-on for nearly three hundred years. By the early 20th century, Einstein developed a theory of relativity that correctly predicted the action of forces on objects with increasing momenta near the speed of light, and also provided insight into the forces produced by gravitation and inertia.
With modern insights into quantum mechanics and technology that can accelerate particles close to the speed of light, particle physics has devised a Standard Model to describe forces between particles smaller than atoms. The Standard Model predicts that exchanged particles called gauge bosons are the fundamental means by which forces are emitted and absorbed. Only four main interactions are known: in order of decreasing strength, they are: strong, electromagnetic, weak, and gravitational. High-energy particle physics observations made during the 1970s and 1980s confirmed that the weak and electromagnetic forces are expressions of a more fundamental electroweak interaction.
Pre-Newtonian concepts.
Since antiquity the concept of force has been recognized as integral to the functioning of each of the simple machines. The mechanical advantage given by a simple machine allowed for less force to be used in exchange for that force acting over a greater distance for the same amount of work. Analysis of the characteristics of forces ultimately culminated in the work of Archimedes who was especially famous for formulating a treatment of buoyant forces inherent in fluids.
Aristotle provided a philosophical discussion of the concept of a force as an integral part of Aristotelian cosmology. In Aristotle's view, the terrestrial sphere contained four elements that come to rest at different "natural places" therein. Aristotle believed that motionless objects on Earth, those composed mostly of the elements earth and water, to be in their natural place on the ground and that they will stay that way if left alone. He distinguished between the innate tendency of objects to find their "natural place" (e.g., for heavy bodies to fall), which led to "natural motion", and unnatural or forced motion, which required continued application of a force. This theory, based on the everyday experience of how objects move, such as the constant application of a force needed to keep a cart moving, had conceptual trouble accounting for the behavior of projectiles, such as the flight of arrows. The place where the archer moves the projectile was at the start of the flight, and while the projectile sailed through the air, no discernible efficient cause acts on it. Aristotle was aware of this problem and proposed that the air displaced through the projectile's path carries the projectile to its target. This explanation demands a continuum like air for change of place in general.
Aristotelian physics began facing criticism in Medieval science, first by John Philoponus in the 6th century.
The shortcomings of Aristotelian physics would not be fully corrected until the 17th century work of Galileo Galilei, who was influenced by the late Medieval idea that objects in forced motion carried an innate force of impetus. Galileo constructed an experiment in which stones and cannonballs were both rolled down an incline to disprove the Aristotelian theory of motion early in the 17th century. He showed that the bodies were accelerated by gravity to an extent that was independent of their mass and argued that objects retain their velocity unless acted on by a force, for example friction.
Newtonian mechanics.
Sir Isaac Newton sought to describe the motion of all objects using the concepts of inertia and force, and in doing so he found that they obey certain conservation laws. In 1687, Newton went on to publish his thesis "Philosophiæ Naturalis Principia Mathematica". In this work Newton set out three laws of motion that to this day are the way forces are described in physics.
First law.
Newton's First Law of Motion states that objects continue to move in a state of constant velocity unless acted upon by an external net force or "resultant force". This law is an extension of Galileo's insight that constant velocity was associated with a lack of net force (see a more detailed description of this below). Newton proposed that every object with mass has an innate inertia that functions as the fundamental equilibrium "natural state" in place of the Aristotelian idea of the "natural state of rest". That is, the first law contradicts the intuitive Aristotelian belief that a net force is required to keep an object moving with constant velocity. By making "rest" physically indistinguishable from "non-zero constant velocity", Newton's First Law directly connects inertia with the concept of relative velocities. Specifically, in systems where objects are moving with different velocities, it is impossible to determine which object is "in motion" and which object is "at rest". In other words, to phrase matters more technically, the laws of physics are the same in every inertial frame of reference, that is, in all frames related by a Galilean transformation.
For instance, while traveling in a moving vehicle at a constant velocity, the laws of physics do not change from being at rest. A person can throw a ball straight up in the air and catch it as it falls down without worrying about applying a force in the direction the vehicle is moving. This is true even though another person who is observing the moving vehicle pass by also observes the ball follow a curving parabolic path in the same direction as the motion of the vehicle. It is the inertia of the ball associated with its constant velocity in the direction of the vehicle's motion that ensures the ball continues to move forward even as it is thrown up and falls back down. From the perspective of the person in the car, the vehicle and everything inside of it is at rest: It is the outside world that is moving with a constant speed in the opposite direction. Since there is no experiment that can distinguish whether it is the vehicle that is at rest or the outside world that is at rest, the two situations are considered to be physically indistinguishable. Inertia therefore applies equally well to constant velocity motion as it does to rest.
The concept of inertia can be further generalized to explain the tendency of objects to continue in many different forms of constant motion, even those that are not strictly constant velocity. The rotational inertia of planet Earth is what fixes the constancy of the length of a day and the length of a year. Albert Einstein extended the principle of inertia further when he explained that reference frames subject to constant acceleration, such as those free-falling toward a gravitating object, were physically equivalent to inertial reference frames. This is why, for example, astronauts experience weightlessness when in free-fall orbit around the Earth, and why Newton's Laws of Motion are more easily discernible in such environments. If an astronaut places an object with mass in mid-air next to himself, it will remain stationary with respect to the astronaut due to its inertia. This is the same thing that would occur if the astronaut and the object were in intergalactic space with no net force of gravity acting on their shared reference frame. This principle of equivalence was one of the foundational underpinnings for the development of the general theory of relativity.
Second law.
A modern statement of Newton's Second Law is a vector equation:
where formula_2 is the momentum of the system, and formula_3 is the net (vector sum) force. In equilibrium, there is zero "net" force by definition, but (balanced) forces may be present nevertheless. In contrast, the second law states an "unbalanced" force acting on an object will result in the object's momentum changing over time.
By the definition of momentum,
where "m" is the mass and formula_5 is the velocity.
Newton's second law applies only to a system of constant mass, and hence "m" may be moved outside the derivative operator. The equation then becomes
By substituting the definition of acceleration, the algebraic version of Newton's Second Law is derived:
Newton never explicitly stated the formula in the reduced form above.
Newton's Second Law asserts the direct proportionality of acceleration to force and the inverse proportionality of acceleration to mass. Accelerations can be defined through kinematic measurements. However, while kinematics are well-described through reference frame analysis in advanced physics, there are still deep questions that remain as to what is the proper definition of mass. General relativity offers an equivalence between space-time and mass, but lacking a coherent theory of quantum gravity, it is unclear as to how or whether this connection is relevant on microscales. With some justification, Newton's second law can be taken as a quantitative definition of "mass" by writing the law as an equality; the relative units of force and mass then are fixed.
The use of Newton's Second Law as a "definition" of force has been disparaged in some of the more rigorous textbooks, because it is essentially a mathematical truism. Notable physicists, philosophers and mathematicians who have sought a more explicit definition of the concept of force include Ernst Mach, Clifford Truesdell and Walter Noll.
Newton's Second Law can be used to measure the strength of forces. For instance, knowledge of the masses of planets along with the accelerations of their orbits allows scientists to calculate the gravitational forces on planets.
Third law.
Newton's Third Law is a result of applying symmetry to situations where forces can be attributed to the presence of different objects. The third law means that all forces are "interactions" between different bodies, and thus that there is no such thing as a unidirectional force or a force that acts on only one body. Whenever a first body exerts a force F on a second body, the second body exerts a force −F on the first body. F and −F are equal in magnitude and opposite in direction. This law is sometimes referred to as the "action-reaction law", with F called the "action" and −F the "reaction". The action and the reaction are simultaneous:
If object 1 and object 2 are considered to be in the same system, then the net force on the system due to the interactions between objects 1 and 2 is zero since
This means that in a closed system of particles, there are no internal forces that are unbalanced. That is, the action-reaction force shared between any two objects in a closed system will not cause the center of mass of the system to accelerate. The constituent objects only accelerate with respect to each other, the system itself remains unaccelerated. Alternatively, if an external force acts on the system, then the center of mass will experience an acceleration proportional to the magnitude of the external force divided by the mass of the system.
Combining Newton's Second and Third Laws, it is possible to show that the linear momentum of a system is conserved. Using
and integrating with respect to time, the equation:
is obtained. For a system that includes objects 1 and 2,
which is the conservation of linear momentum. Using the similar arguments, it is possible to generalize this to a system of an arbitrary number of particles. This shows that exchanging momentum between constituent objects will not affect the net momentum of a system. In general, as long as all forces are due to the interaction of objects with mass, it is possible to define a system such that net momentum is never lost nor gained.
Special theory of relativity.
In the special theory of relativity, mass and energy are equivalent (as can be seen by calculating the work required to accelerate an object). When an object's velocity increases, so does its energy and hence its mass equivalent (inertia). It thus requires more force to accelerate it the same amount than it did at a lower velocity. Newton's Second Law
remains valid because it is a mathematical definition. But in order to be conserved, relativistic momentum must be redefined as:
where
The relativistic expression relating force and acceleration for a particle with constant non-zero rest mass formula_19 moving in the formula_20 direction is:
where the Lorentz factor
In the early history of relativity, the expressions formula_25 and formula_26 were called longitudinal and transverse mass. Relativistic force does not produce a constant acceleration, but an ever decreasing acceleration as the object approaches the speed of light. Note that formula_27 is undefined for an object with a non-zero rest mass at the speed of light, and the theory yields no prediction at that speed.
If formula_16 is very small compared to formula_17, then formula_30 is very close to 1 and 
is a close approximation. Even for use in relativity, however, one can restore the form of
through the use of four-vectors. This relation is correct in relativity when formula_33 is the four-force, formula_19 is the invariant mass, and formula_35 is the four-acceleration.
Descriptions.
Since forces are perceived as pushes or pulls, this can provide an intuitive understanding for describing forces. As with other physical concepts (e.g. temperature), the intuitive understanding of forces is quantified using precise operational definitions that are consistent with direct observations and compared to a standard measurement scale. Through experimentation, it is determined that laboratory measurements of forces are fully consistent with the conceptual definition of force offered by Newtonian mechanics.
Forces act in a particular direction and have sizes dependent upon how strong the push or pull is. Because of these characteristics, forces are classified as "vector quantities". This means that forces follow a different set of mathematical rules than physical quantities that do not have direction (denoted scalar quantities). For example, when determining what happens when two forces act on the same object, it is necessary to know both the magnitude and the direction of both forces to calculate the result. If both of these pieces of information are not known for each force, the situation is ambiguous. For example, if you know that two people are pulling on the same rope with known magnitudes of force but you do not know which direction either person is pulling, it is impossible to determine what the acceleration of the rope will be. The two people could be pulling against each other as in tug of war or the two people could be pulling in the same direction. In this simple one-dimensional example, without knowing the direction of the forces it is impossible to decide whether the net force is the result of adding the two force magnitudes or subtracting one from the other. Associating forces with vectors avoids such problems.
Historically, forces were first quantitatively investigated in conditions of static equilibrium where several forces canceled each other out. Such experiments demonstrate the crucial properties that forces are additive vector quantities: they have magnitude and direction. When two forces act on a point particle, the resulting force, the "resultant" (also called the "net force"), can be determined by following the parallelogram rule of vector addition: the addition of two vectors represented by sides of a parallelogram, gives an equivalent resultant vector that is equal in magnitude and direction to the transversal of the parallelogram. The magnitude of the resultant varies from the difference of the magnitudes of the two forces to their sum, depending on the angle between their lines of action. However, if the forces are acting on an extended body, their respective lines of application must also be specified in order to account for their effects on the motion of the body.
Free-body diagrams can be used as a convenient way to keep track of forces acting on a system. Ideally, these diagrams are drawn with the angles and relative magnitudes of the force vectors preserved so that graphical vector addition can be done to determine the net force.
As well as being added, forces can also be resolved into independent components at right angles to each other. A horizontal force pointing northeast can therefore be split into two forces, one pointing north, and one pointing east. Summing these component forces using vector addition yields the original force. Resolving force vectors into components of a set of basis vectors is often a more mathematically clean way to describe forces than using magnitudes and directions. This is because, for orthogonal components, the components of the vector sum are uniquely determined by the scalar addition of the components of the individual vectors. Orthogonal components are independent of each other because forces acting at ninety degrees to each other have no effect on the magnitude or direction of the other. Choosing a set of orthogonal basis vectors is often done by considering what set of basis vectors will make the mathematics most convenient. Choosing a basis vector that is in the same direction as one of the forces is desirable, since that force would then have only one non-zero component. Orthogonal force vectors can be three-dimensional with the third component being at right-angles to the other two.
Equilibrium.
Equilibrium occurs when the resultant force acting on a point particle is zero (that is, the vector sum of all forces is zero). When dealing with an extended body, it is also necessary that the net torque in it is 0.
There are two kinds of equilibrium: static equilibrium and dynamic equilibrium.
Static.
Static equilibrium was understood well before the invention of classical mechanics. Objects that are at rest have zero net force acting on them.
The simplest case of static equilibrium occurs when two forces are equal in magnitude but opposite in direction. For example, an object on a level surface is pulled (attracted) downward toward the center of the Earth by the force of gravity. At the same time, surface forces resist the downward force with equal upward force (called the normal force). The situation is one of zero net force and no acceleration.
Pushing against an object on a frictional surface can result in a situation where the object does not move because the applied force is opposed by static friction, generated between the object and the table surface. For a situation with no movement, the static friction force "exactly" balances the applied force resulting in no acceleration. The static friction increases or decreases in response to the applied force up to an upper limit determined by the characteristics of the contact between the surface and the object.
A static equilibrium between two forces is the most usual way of measuring forces, using simple devices such as weighing scales and spring balances. For example, an object suspended on a vertical spring scale experiences the force of gravity acting on the object balanced by a force applied by the "spring reaction force", which equals the object's weight. Using such tools, some quantitative force laws were discovered: that the force of gravity is proportional to volume for objects of constant density (widely exploited for millennia to define standard weights); Archimedes' principle for buoyancy; Archimedes' analysis of the lever; Boyle's law for gas pressure; and Hooke's law for springs. These were all formulated and experimentally verified before Isaac Newton expounded his Three Laws of Motion.
Dynamic.
Dynamic equilibrium was first described by Galileo who noticed that certain assumptions of Aristotelian physics were contradicted by observations and logic. Galileo realized that simple velocity addition demands that the concept of an "absolute rest frame" did not exist. Galileo concluded that motion in a constant velocity was completely equivalent to rest. This was contrary to Aristotle's notion of a "natural state" of rest that objects with mass naturally approached. Simple experiments showed that Galileo's understanding of the equivalence of constant velocity and rest were correct. For example, if a mariner dropped a cannonball from the crow's nest of a ship moving at a constant velocity, Aristotelian physics would have the cannonball fall straight down while the ship moved beneath it. Thus, in an Aristotelian universe, the falling cannonball would land behind the foot of the mast of a moving ship. However, when this experiment is actually conducted, the cannonball always falls at the foot of the mast, as if the cannonball knows to travel with the ship despite being separated from it. Since there is no forward horizontal force being applied on the cannonball as it falls, the only conclusion left is that the cannonball continues to move with the same velocity as the boat as it falls. Thus, no force is required to keep the cannonball moving at the constant forward velocity.
Moreover, any object traveling at a constant velocity must be subject to zero net force (resultant force). This is the definition of dynamic equilibrium: when all the forces on an object balance but it still moves at a constant velocity.
A simple case of dynamic equilibrium occurs in constant velocity motion across a surface with kinetic friction. In such a situation, a force is applied in the direction of motion while the kinetic friction force exactly opposes the applied force. This results in zero net force, but since the object started with a non-zero velocity, it continues to move with a non-zero velocity. Aristotle misinterpreted this motion as being caused by the applied force. However, when kinetic friction is taken into consideration it is clear that there is no net force causing constant velocity motion.
Forces in Quantum Mechanics.
The notion "force" keeps its meaning in quantum mechanics, though one is now dealing with operators instead of classical variables and though the physics is now described by the Schrödinger equation instead of Newtonian equations. This has the consequence that the results of a measurement are now sometimes "quantized", i.e. they appear in discrete portions. This is, of course, difficult to imagine in the context of "forces". However, the potentials V(x,y,z) or fields, from which the forces generally can be derived, are treated similar to classical position variables, i.e., formula_36.
This becomes different only in the framework of quantum field theory, where these fields are also quantized.
However, already in quantum mechanics there is one "caveat", namely the particles acting onto each other do not only possess the spatial variable, but also a discrete intrinsic angular momentum-like variable called the "spin", and there is the Pauli principle relating the space and the spin variables. Depending on the value of the spin, identical particles split into two different classes, fermions and bosons. If two identical fermions (e.g. electrons) have a "symmetric" spin function (e.g. parallel spins) the spatial variables must be "antisymmetric" (i.e. they exclude each other from their places much as if there was a repulsive force), and vice versa, i.e. for antiparallel "spins" the "position variables" must be symmetric (i.e. the appearant force must be attractive). Thus in the case of two fermions there is a strictly negative correlation between spatial and spin variables, whereas for two bosons (e.g. quanta of electromagnetic waves, photons) the correlation is strictly positive.
Thus the notion "force" loses already part of its meaning.
Feynman diagrams.
In modern particle physics, forces and the acceleration of particles are explained as a mathematical by-product of exchange of momentum-carrying gauge bosons. With the development of quantum field theory and general relativity, it was realized that force is a redundant concept arising from conservation of momentum (4-momentum in relativity and momentum of virtual particles in quantum electrodynamics). The conservation of momentum can be directly derived from the homogeneity or symmetry of space and so is usually considered more fundamental than the concept of a force. Thus the currently known fundamental forces are considered more accurately to be "fundamental interactions". When particle A emits (creates) or absorbs (annihilates) virtual particle B, a momentum conservation results in recoil of particle A making impression of repulsion or attraction between particles A A' exchanging by B. This description applies to all forces arising from fundamental interactions. While sophisticated mathematical descriptions are needed to predict, in full detail, the accurate result of such interactions, there is a conceptually simple way to describe such interactions through the use of Feynman diagrams. In a Feynman diagram, each matter particle is represented as a straight line (see world line) traveling through time, which normally increases up or to the right in the diagram. Matter and anti-matter particles are identical except for their direction of propagation through the Feynman diagram. World lines of particles intersect at interaction vertices, and the Feynman diagram represents any force arising from an interaction as occurring at the vertex with an associated instantaneous change in the direction of the particle world lines. Gauge bosons are emitted away from the vertex as wavy lines and, in the case of virtual particle exchange, are absorbed at an adjacent vertex.
The utility of Feynman diagrams is that other types of physical phenomena that are part of the general picture of fundamental interactions but are conceptually separate from forces can also be described using the same rules. For example, a Feynman diagram can describe in succinct detail how a neutron decays into an electron, proton, and neutrino, an interaction mediated by the same gauge boson that is responsible for the weak nuclear force.
Fundamental forces.
All of the forces in the universe are based on four fundamental interactions. The strong and weak forces are nuclear forces that act only at very short distances, and are responsible for the interactions between subatomic particles, including nucleons and compound nuclei. The electromagnetic force acts between electric charges, and the gravitational force acts between masses. All other forces in nature derive from these four fundamental interactions. For example, friction is a manifestation of the electromagnetic force acting between the atoms of two surfaces, and the Pauli exclusion principle, which does not permit atoms to pass through each other. Similarly, the forces in springs, modeled by Hooke's law, are the result of electromagnetic forces and the Exclusion Principle acting together to return an object to its equilibrium position. Centrifugal forces are acceleration forces that arise simply from the acceleration of rotating frames of reference.
The development of fundamental theories for forces proceeded along the lines of unification of disparate ideas. For example, Isaac Newton unified the force responsible for objects falling at the surface of the Earth with the force responsible for the orbits of celestial mechanics in his universal theory of gravitation. Michael Faraday and James Clerk Maxwell demonstrated that electric and magnetic forces were unified through one consistent theory of electromagnetism. In the 20th century, the development of quantum mechanics led to a modern understanding that the first three fundamental forces (all except gravity) are manifestations of matter (fermions) interacting by exchanging virtual particles called gauge bosons. This standard model of particle physics posits a similarity between the forces and led scientists to predict the unification of the weak and electromagnetic forces in electroweak theory subsequently confirmed by observation. The complete formulation of the standard model predicts an as yet unobserved Higgs mechanism, but observations such as neutrino oscillations indicate that the standard model is incomplete. A Grand Unified Theory allowing for the combination of the electroweak interaction with the strong force is held out as a possibility with candidate theories such as supersymmetry proposed to accommodate some of the outstanding unsolved problems in physics. Physicists are still attempting to develop self-consistent unification models that would combine all four fundamental interactions into a theory of everything. Einstein tried and failed at this endeavor, but currently the most popular approach to answering this question is string theory.
Gravitational.
What we now call gravity was not identified as a universal force until the work of Isaac Newton. Before Newton, the tendency for objects to fall towards the Earth was not understood to be related to the motions of celestial objects. Galileo was instrumental in describing the characteristics of falling objects by determining that the acceleration of every object in free-fall was constant and independent of the mass of the object. Today, this acceleration due to gravity towards the surface of the Earth is usually designated as formula_37 and has a magnitude of about 9.81 meters per second squared (this measurement is taken from sea level and may vary depending on location), and points toward the center of the Earth. This observation means that the force of gravity on an object at the Earth's surface is directly proportional to the object's mass. Thus an object that has a mass of formula_19 will experience a force:
In free-fall, this force is unopposed and therefore the net force on the object is its weight. For objects not in free-fall, the force of gravity is opposed by the reactions of their supports. For example, a person standing on the ground experiences zero net force, since his weight is balanced by a normal force exerted by the ground.
Newton's contribution to gravitational theory was to unify the motions of heavenly bodies, which Aristotle had assumed were in a natural state of constant motion, with falling motion observed on the Earth. He proposed a law of gravity that could account for the celestial motions that had been described earlier using Kepler's laws of planetary motion.
Newton came to realize that the effects of gravity might be observed in different ways at larger distances. In particular, Newton determined that the acceleration of the Moon around the Earth could be ascribed to the same force of gravity if the acceleration due to gravity decreased as an inverse square law. Further, Newton realized that the acceleration due to gravity is proportional to the mass of the attracting body. Combining these ideas gives a formula that relates the mass (formula_40) and the radius (formula_41) of the Earth to the gravitational acceleration:
where formula_43 is the distance between the two objects' centers of mass and formula_44 is the unit vector pointed in the direction away from the center of the first object toward the center of the second object.
This formula was powerful enough to stand as the basis for all subsequent descriptions of motion within the solar system until the 20th century. During that time, sophisticated methods of perturbation analysis were invented to calculate the deviations of orbits due to the influence of multiple bodies on a planet, moon, comet, or asteroid. The formalism was exact enough to allow mathematicians to predict the existence of the planet Neptune before it was observed.
It was only the orbit of the planet Mercury that Newton's Law of Gravitation seemed not to fully explain. Some astrophysicists predicted the existence of another planet (Vulcan) that would explain the discrepancies; however, despite some early indications, no such planet could be found. When Albert Einstein formulated his theory of general relativity (GR) he turned his attention to the problem of Mercury's orbit and found that his theory added a correction, which could account for the discrepancy. This was the first time that Newton's Theory of Gravity had been shown to be less correct than an alternative.
Since then, and so far, general relativity has been acknowledged as the theory that best explains gravity. In GR, gravitation is not viewed as a force, but rather, objects moving freely in gravitational fields travel under their own inertia in straight lines through curved space-time – defined as the shortest space-time path between two space-time events. From the perspective of the object, all motion occurs as if there were no gravitation whatsoever. It is only when observing the motion in a global sense that the curvature of space-time can be observed and the force is inferred from the object's curved path. Thus, the straight line path in space-time is seen as a curved line in space, and it is called the "ballistic trajectory" of the object. For example, a basketball thrown from the ground moves in a parabola, as it is in a uniform gravitational field. Its space-time trajectory (when the extra ct dimension is added) is almost a straight line, slightly curved (with the radius of curvature of the order of few light-years). The time derivative of the changing momentum of the object is what we label as "gravitational force".
Electromagnetic.
The electrostatic force was first described in 1784 by Coulomb as a force that existed intrinsically between two charges. The properties of the electrostatic force were that it varied as an inverse square law directed in the radial direction, was both attractive and repulsive (there was intrinsic polarity), was independent of the mass of the charged objects, and followed the superposition principle. Coulomb's law unifies all these observations into one succinct statement.
Subsequent mathematicians and physicists found the construct of the "electric field" to be useful for determining the electrostatic force on an electric charge at any point in space. The electric field was based on using a hypothetical "test charge" anywhere in space and then using Coulomb's Law to determine the electrostatic force. Thus the electric field anywhere in space is defined as
where formula_46 is the magnitude of the hypothetical test charge.
Meanwhile, the Lorentz force of magnetism was discovered to exist between two electric currents. It has the same mathematical character as Coulomb's Law with the proviso that like currents attract and unlike currents repel. Similar to the electric field, the magnetic field can be used to determine the magnetic force on an electric current at any point in space. In this case, the magnitude of the magnetic field was determined to be
where formula_48 is the magnitude of the hypothetical test current and formula_49 is the length of hypothetical wire through which the test current flows. The magnetic field exerts a force on all magnets including, for example, those used in compasses. The fact that the Earth's magnetic field is aligned closely with the orientation of the Earth's axis causes compass magnets to become oriented because of the magnetic force pulling on the needle.
Through combining the definition of electric current as the time rate of change of electric charge, a rule of vector multiplication called Lorentz's Law describes the force on a charge moving in a magnetic field. The connection between electricity and magnetism allows for the description of a unified "electromagnetic force" that acts on a charge. This force can be written as a sum of the electrostatic force (due to the electric field) and the magnetic force (due to the magnetic field). Fully stated, this is the law:
where formula_3 is the electromagnetic force, formula_46 is the magnitude of the charge of the particle, formula_53 is the electric field, formula_5 is the velocity of the particle that is crossed with the magnetic field (formula_55).
The origin of electric and magnetic fields would not be fully explained until 1864 when James Clerk Maxwell unified a number of earlier theories into a set of 20 scalar equations, which were later reformulated into 4 vector equations by Oliver Heaviside and Josiah Willard Gibbs. These "Maxwell Equations" fully described the sources of the fields as being stationary and moving charges, and the interactions of the fields themselves. This led Maxwell to discover that electric and magnetic fields could be "self-generating" through a wave that traveled at a speed that he calculated to be the speed of light. This insight united the nascent fields of electromagnetic theory with optics and led directly to a complete description of the electromagnetic spectrum.
However, attempting to reconcile electromagnetic theory with two observations, the photoelectric effect, and the nonexistence of the ultraviolet catastrophe, proved troublesome. Through the work of leading theoretical physicists, a new theory of electromagnetism was developed using quantum mechanics. This final modification to electromagnetic theory ultimately led to quantum electrodynamics (or QED), which fully describes all electromagnetic phenomena as being mediated by wave–particles known as photons. In QED, photons are the fundamental exchange particle, which described all interactions relating to electromagnetism including the electromagnetic force.
It is a common misconception to ascribe the stiffness and rigidity of solid matter to the repulsion of like charges under the influence of the electromagnetic force. However, these characteristics actually result from the Pauli exclusion principle. Since electrons are fermions, they cannot occupy the same quantum mechanical state as other electrons. When the electrons in a material are densely packed together, there are not enough lower energy quantum mechanical states for them all, so some of them must be in higher energy states. This means that it takes energy to pack them together. While this effect is manifested macroscopically as a structural force, it is technically only the result of the existence of a finite set of electron states.
Nuclear.
There are two "nuclear forces", which today are usually described as interactions that take place in quantum theories of particle physics. The strong nuclear force is the force responsible for the structural integrity of atomic nuclei while the weak nuclear force is responsible for the decay of certain nucleons into leptons and other types of hadrons.
The strong force is today understood to represent the interactions between quarks and gluons as detailed by the theory of quantum chromodynamics (QCD). The strong force is the fundamental force mediated by gluons, acting upon quarks, antiquarks, and the gluons themselves. The (aptly named) strong interaction is the "strongest" of the four fundamental forces.
The strong force only acts "directly" upon elementary particles. However, a residual of the force is observed between hadrons (the best known example being the force that acts between nucleons in atomic nuclei) as the nuclear force. Here the strong force acts indirectly, transmitted as gluons, which form part of the virtual pi and rho mesons, which classically transmit the nuclear force (see this topic for more). The failure of many searches for free quarks has shown that the elementary particles affected are not directly observable. This phenomenon is called color confinement.
The weak force is due to the exchange of the heavy W and Z bosons. Its most familiar effect is beta decay (of neutrons in atomic nuclei) and the associated radioactivity. The word "weak" derives from the fact that the field strength is some 1013 times less than that of the strong force. Still, it is stronger than gravity over short distances. A consistent electroweak theory has also been developed, which shows that electromagnetic forces and the weak force are indistinguishable at a temperatures in excess of approximately 1015 kelvins. Such temperatures have been probed in modern particle accelerators and show the conditions of the universe in the early moments of the Big Bang.
Non-fundamental forces.
Some forces are consequences of the fundamental ones. In such situations, idealized models can be utilized to gain physical insight.
Normal force.
The normal force is due to repulsive forces of interaction between atoms at close contact. When their electron clouds overlap, Pauli repulsion (due to fermionic nature of electrons) follows resulting in the force that acts in a direction normal to the surface interface between two objects. The normal force, for example, is responsible for the structural integrity of tables and floors as well as being the force that responds whenever an external force pushes on a solid object. An example of the normal force in action is the impact force on an object crashing into an immobile surface.
Friction.
Friction is a surface force that opposes relative motion. The frictional force is directly related to the normal force that acts to keep two solid objects separated at the point of contact. There are two broad classifications of frictional forces: static friction and kinetic friction.
The static friction force (formula_56) will exactly oppose forces applied to an object parallel to a surface contact up to the limit specified by the coefficient of static friction (formula_57) multiplied by the normal force (formula_58). In other words, the magnitude of the static friction force satisfies the inequality:
The kinetic friction force (formula_60) is independent of both the forces applied and the movement of the object. Thus, the magnitude of the force equals:
where formula_62 is the coefficient of kinetic friction. For most surface interfaces, the coefficient of kinetic friction is less than the coefficient of static friction.
Tension.
Tension forces can be modeled using ideal strings that are massless, frictionless, unbreakable, and unstretchable. They can be combined with ideal pulleys, which allow ideal strings to switch physical direction. Ideal strings transmit tension forces instantaneously in action-reaction pairs so that if two objects are connected by an ideal string, any force directed along the string by the first object is accompanied by a force directed along the string in the opposite direction by the second object. By connecting the same string multiple times to the same object through the use of a set-up that uses movable pulleys, the tension force on a load can be multiplied. For every string that acts on a load, another factor of the tension force in the string acts on the load. However, even though such machines allow for an increase in force, there is a corresponding increase in the length of string that must be displaced in order to move the load. These tandem effects result ultimately in the conservation of mechanical energy since the work done on the load is the same no matter how complicated the machine.
Elastic force.
An elastic force acts to return a spring to its natural length. An ideal spring is taken to be massless, frictionless, unbreakable, and infinitely stretchable. Such springs exert forces that push when contracted, or pull when extended, in proportion to the displacement of the spring from its equilibrium position. This linear relationship was described by Robert Hooke in 1676, for whom Hooke's law is named. If formula_63 is the displacement, the force exerted by an ideal spring equals:
where formula_65 is the spring constant (or force constant), which is particular to the spring. The minus sign accounts for the tendency of the force to act in opposition to the applied load.
Continuum mechanics.
Newton's laws and Newtonian mechanics in general were first developed to describe how forces affect idealized point particles rather than three-dimensional objects. However, in real life, matter has extended structure and forces that act on one part of an object might affect other parts of an object. For situations where lattice holding together the atoms in an object is able to flow, contract, expand, or otherwise change shape, the theories of continuum mechanics describe the way forces affect the material. For example, in extended fluids, differences in pressure result in forces being directed along the pressure gradients as follows:
where formula_67 is the volume of the object in the fluid and formula_68 is the scalar function that describes the pressure at all locations in space. Pressure gradients and differentials result in the buoyant force for fluids suspended in gravitational fields, winds in atmospheric science, and the lift associated with aerodynamics and flight.
A specific instance of such a force that is associated with dynamic pressure is fluid resistance: a body force that resists the motion of an object through a fluid due to viscosity. For so-called "Stokes' drag" the force is approximately proportional to the velocity, but opposite in direction:
where:
More formally, forces in continuum mechanics are fully described by a stress–tensor with terms that are roughly defined as
where formula_73 is the relevant cross-sectional area for the volume for which the stress-tensor is being calculated. This formalism includes pressure terms associated with forces that act normal to the cross-sectional area (the matrix diagonals of the tensor) as well as shear terms associated with forces that act parallel to the cross-sectional area (the off-diagonal elements). The stress tensor accounts for forces that cause all strains (deformations) including also tensile stresses and compressions.
Fictitious forces.
There are forces that are frame dependent, meaning that they appear due to the adoption of non-Newtonian (that is, non-inertial) reference frames. Such forces include the centrifugal force and the Coriolis force. These forces are considered fictitious because they do not exist in frames of reference that are not accelerating. Because these forces are not genuine they are also referred to as "pseudo forces".
In general relativity, gravity becomes a fictitious force that arises in situations where spacetime deviates from a flat geometry. As an extension, Kaluza–Klein theory and string theory ascribe electromagnetism and the other fundamental forces respectively to the curvature of differently scaled dimensions, which would ultimately imply that all forces are fictitious.
Rotations and torque.
Forces that cause extended objects to rotate are associated with torques. Mathematically, the torque of a force formula_3 is defined relative to an arbitrary reference point as the cross-product:
where
Torque is the rotation equivalent of force in the same way that angle is the rotational equivalent for position, angular velocity for velocity, and angular momentum for momentum. As a consequence of Newton's First Law of Motion, there exists rotational inertia that ensures that all bodies maintain their angular momentum unless acted upon by an unbalanced torque. Likewise, Newton's Second Law of Motion can be used to derive an analogous equation for the instantaneous angular acceleration of the rigid body:
where
This provides a definition for the moment of inertia, which is the rotational equivalent for mass. In more advanced treatments of mechanics, where the rotation over a time interval is described, the moment of inertia must be substituted by the tensor that, when properly analyzed, fully determines the characteristics of rotations including precession and nutation.
Equivalently, the differential form of Newton's Second Law provides an alternative definition of torque:
Newton's Third Law of Motion requires that all objects exerting torques themselves experience equal and opposite torques, and therefore also directly implies the conservation of angular momentum for closed systems that experience rotations and revolutions through the action of internal torques.
Centripetal force.
For an object accelerating in circular motion, the unbalanced force acting on the object equals:
where formula_19 is the mass of the object, formula_16 is the velocity of the object and formula_43 is the distance to the center of the circular path and formula_44 is the unit vector pointing in the radial direction outwards from the center. This means that the unbalanced centripetal force felt by any object is always directed toward the center of the curving path. Such forces act perpendicular to the velocity vector associated with the motion of an object, and therefore do not change the speed of the object (magnitude of the velocity), but only the direction of the velocity vector. The unbalanced force that accelerates an object can be resolved into a component that is perpendicular to the path, and one that is tangential to the path. This yields both the tangential force, which accelerates the object by either slowing it down or speeding it up, and the radial (centripetal) force, which changes its direction.
Kinematic integrals.
Forces can be used to define a number of physical concepts by integrating with respect to kinematic variables. For example, integrating with respect to time gives the definition of impulse:
which by Newton's Second Law must be equivalent to the change in momentum (yielding the Impulse momentum theorem).
Similarly, integrating with respect to position gives a definition for the work done by a force:
which is equivalent to changes in kinetic energy (yielding the work energy theorem).
Power "P" is the rate of change d"W"/d"t" of the work "W", as the trajectory is extended by a position change formula_89 in a time interval d"t":
with formula_91 the velocity.
Potential energy.
Instead of a force, often the mathematically related concept of a potential energy field can be used for convenience. For instance, the gravitational force acting upon an object can be seen as the action of the gravitational field that is present at the object's location. Restating mathematically the definition of energy (via the definition of work), a potential scalar field formula_92 is defined as that field whose gradient is equal and opposite to the force produced at every point:
Forces can be classified as conservative or nonconservative. Conservative forces are equivalent to the gradient of a potential while nonconservative forces are not.
Conservative forces.
A conservative force that acts on a closed system has an associated mechanical work that allows energy to convert only between kinetic or potential forms. This means that for a closed system, the net mechanical energy is conserved whenever a conservative force acts on the system. The force, therefore, is related directly to the difference in potential energy between two different locations in space, and can be considered to be an artifact of the potential field in the same way that the direction and amount of a flow of water can be considered to be an artifact of the contour map of the elevation of an area.
Conservative forces include gravity, the electromagnetic force, and the spring force. Each of these forces has models that are dependent on a position often given as a radial vector formula_76 emanating from spherically symmetric potentials. Examples of this follow:
For gravity:
where formula_96 is the gravitational constant, and formula_97 is the mass of object "n".
For electrostatic forces:
where formula_99 is electric permittivity of free space, and formula_100 is the electric charge of object "n".
For spring forces:
where formula_65 is the spring constant.
Nonconservative forces.
For certain physical scenarios, it is impossible to model forces as being due to gradient of potentials. This is often due to macrophysical considerations that yield forces as arising from a macroscopic statistical average of microstates. For example, friction is caused by the gradients of numerous electrostatic potentials between the atoms, but manifests as a force model that is independent of any macroscale position vector. Nonconservative forces other than friction include other contact forces, tension, compression, and drag. However, for any sufficiently detailed description, all these forces are the results of conservative ones since each of these macroscopic forces are the net results of the gradients of microscopic potentials.
The connection between macroscopic nonconservative forces and microscopic conservative forces is described by detailed treatment with statistical mechanics. In macroscopic closed systems, nonconservative forces act to change the internal energies of the system, and are often associated with the transfer of heat. According to the Second law of thermodynamics, nonconservative forces necessarily result in energy transformations within closed systems from ordered to more random conditions as entropy increases.
Units of measurement.
The SI unit of force is the newton (symbol N), which is the force required to accelerate a one kilogram mass at a rate of one meter per second squared, or . The corresponding CGS unit is the dyne, the force required to accelerate a one gram mass by one centimeter per second squared, or . A newton is thus equal to 100,000 dynes.
The gravitational foot-pound-second English unit of force is the pound-force (lbf), defined as the force exerted by gravity on a pound-mass in the standard gravitational field of . The pound-force provides an alternative unit of mass: one slug is the mass that will accelerate by one foot per second squared when acted on by one pound-force.
An alternative unit of force in a different foot-pound-second system, the absolute fps system, is the poundal, defined as the force required to accelerate a one-pound mass at a rate of one foot per second squared. The units of slug and poundal are designed to avoid a constant of proportionality in Newton's Second Law.
The pound-force has a metric counterpart, less commonly used than the newton: the kilogram-force (kgf) (sometimes kilopond), is the force exerted by standard gravity on one kilogram of mass. The kilogram-force leads to an alternate, but rarely used unit of mass: the metric slug (sometimes mug or hyl) is that mass that accelerates at when subjected to a force of 1 kgf. The kilogram-force is not a part of the modern SI system, and is generally deprecated; however it still sees use for some purposes as expressing aircraft weight, jet thrust, bicycle spoke tension, torque wrench settings and engine output torque. Other arcane units of force include the sthène, which is equivalent to 1000 N, and the kip, which is equivalent to 1000 lbf.
See also Ton-force.
Force measurement.
See force gauge, spring scale, load cell

</doc>
<doc id="1364622" url="https://en.wikipedia.org/wiki?curid=1364622" title="Four-dimensional space">
Four-dimensional space

In mathematics, four-dimensional space ("4D") is a geometric space with four dimensions. It typically is more specifically four-dimensional Euclidean space, generalizing the rules of three-dimensional Euclidean space. It has been studied by mathematicians and philosophers for over two centuries, both for its own interest and for the insights it offered into mathematics and related fields.
Algebraically, it is generated by applying the rules of vectors and coordinate geometry to a space with four dimensions. In particular a vector with four elements (a 4-tuple) can be used to represent a position in four-dimensional space. The space is a Euclidean space, so has a metric and norm, and so all directions are treated as the same: the additional dimension is indistinguishable from the other three.
In modern physics, space and time are unified in a four-dimensional Minkowski continuum called spacetime, whose metric treats the time dimension differently from the three spatial dimensions (see below for the definition of the Minkowski metric/pairing). Spacetime is "not" a Euclidean space.
History.
Lagrange wrote in his "Mécanique analytique" (published 1788, based on work done around 1755) that mechanics can be viewed as operating in a four-dimensional space — three of dimensions of space, and one of time. In 1827 Möbius realized that a fourth dimension would allow a three-dimensional form to be rotated onto its mirror-image, and by 1853 Ludwig Schläfli had discovered many polytopes in higher dimensions, although his work was not published until after his death. Higher dimensions were soon put on firm footing by Bernhard Riemann's 1854 Habilitationsschrift, "Über die Hypothesen welche der Geometrie zu Grunde liegen", in which he considered a "point" to be any sequence of coordinates ("x"1, ..., "x""n"). The possibility of geometry in higher dimensions, including four dimensions in particular, was thus established.
An arithmetic of four dimensions called quaternions was defined by William Rowan Hamilton in 1843. This associative algebra was the source of the science of vector analysis in three dimensions as recounted in "A History of Vector Analysis". Soon after tessarines and coquaternions were introduced as other four-dimensional algebras over R.
One of the first major expositors of the fourth dimension was Charles Howard Hinton, starting in 1880 with his essay "What is the Fourth Dimension?"; published in the Dublin University magazine. He coined the terms "tesseract", "ana" and "kata" in his book "A New Era of Thought", and introduced a method for visualising the fourth dimension using cubes in the book "Fourth Dimension".
In 1886 Victor Schlegel described his method of visualizing four-dimensional objects with Schlegel diagrams.
In 1908, Hermann Minkowski presented a paper consolidating the role of time as the fourth dimension of spacetime, the basis for Einstein's theories of special and general relativity. But the geometry of spacetime, being non-Euclidean, is profoundly different from that popularised by Hinton. The study of Minkowski space required new mathematics quite different from that of four-dimensional Euclidean space, and so developed along quite different lines. This separation was less clear in the popular imagination, with works of fiction and philosophy blurring the distinction, so in 1973 H. S. M. Coxeter felt compelled to write:
Vectors.
Mathematically four-dimensional space is simply a space with four spatial dimensions, that is a space that needs four parameters to specify a point in it. For example, a general point might have position vector a, equal to
This can be written in terms of the four standard basis vectors (e1, e2, e3, e4), given by
so the general vector a is
Vectors add, subtract and scale as in three dimensions.
The dot product of Euclidean three-dimensional space generalizes to four dimensions as
It can be used to calculate the norm or length of a vector,
Minkowski spacetime is four-dimensional space with geometry defined by a nondegenerate pairing different from the dot product:
As an example, the distance squared between the points (0,0,0,0) and (1,1,1,0) is 3 in both the Euclidean and Minkowskian 4-spaces, while the distance squared between (0,0,0,0) and (1,1,1,1) is 4 in Euclidean space and 2 in Minkowski space; increasing formula_7 actually decreases the metric distance. This leads to many of the well known apparent "paradoxes" of relativity.
The cross product is not defined in four dimensions. Instead the exterior product is used for some applications, and is defined as follows:
This is bivector valued, with bivectors in four dimensions forming a six-dimensional linear space with basis (e12, e13, e14, e23, e24, e34). They can be used to generate rotations in four dimensions.
Orthogonality and vocabulary.
In the familiar 3-dimensional space in which we live there are three coordinate axes — usually labeled "x", "y", and "z" — with each axis orthogonal (i.e. perpendicular) to the other two. The six cardinal directions in this space can be called "up", "down", "east", "west", "north", and "south". Positions along these axes can be called "altitude", "longitude", and "latitude". Lengths measured along these axes can be called "height", "width", and "depth".
Comparatively, 4-dimensional space has an extra coordinate axis, orthogonal to the other three, which is usually labeled "w". To describe the two additional cardinal directions, Charles Howard Hinton coined the terms "ana" and "kata", from the Greek words meaning "up toward" and "down from", respectively. A position along the "w" axis can be called "spissitude", as coined by Henry More.
Geometry.
The geometry of 4-dimensional space is much more complex than that of 3-dimensional space, due to the extra degree of freedom.
Just as in 3 dimensions there are polyhedra made of two dimensional polygons, in 4 dimensions there are 4-polytopes made of polyhedra. In 3 dimensions there are 5 regular polyhedra known as the Platonic solids. In 4 dimensions there are 6 convex regular 4-polytopes, the analogues of the Platonic solids. Relaxing the conditions for regularity generates a further 58 convex uniform 4-polytopes, analogous to the 13 semi-regular Archimedean solids in three dimensions. Relaxing the conditions for convexity generates a further 10 nonconvex regular 4-polytopes.
In 3 dimensions, a circle may be extruded to form a cylinder. In 4 dimensions, there are several different cylinder-like objects. A sphere may be extruded to obtain a spherical cylinder (a cylinder with spherical "caps", known as a spherinder), and a cylinder may be extruded to obtain a cylindrical prism (a cubinder). The Cartesian product of two circles may be taken to obtain a duocylinder. All three can "roll" in 4-dimensional space, each with its own properties.
In 3 dimensions, curves can form knots but surfaces cannot (unless they are self-intersecting). In 4 dimensions, however, knots made using curves can be trivially untied by displacing them in the fourth direction, but 2-dimensional surfaces can form non-trivial, non-self-intersecting knots in 4-dimensional space. Because these surfaces are 2-dimensional, they can form much more complex knots than strings in 3-dimensional space can. The Klein bottle is an example of such a knotted surface . Another such surface is the real projective plane. 
Hypersphere.
The set of points in Euclidean 4-space having the same distance R from a fixed point P0 forms a hypersurface known as a 3-sphere. The hyper-volume of the enclosed space is:
This is part of the Friedmann–Lemaître–Robertson–Walker metric in General relativity where "R" is substituted by function "R(t)" with "t" meaning the cosmological age of the universe. Growing or shrinking "R" with time means expanding or collapsing universe, depending on the mass density inside.
Cognition.
Research using virtual reality finds that humans in spite of living in a three-dimensional world can without special practice make spatial judgments based on the length of, and angle between, line segments embedded in four-dimensional space. The researchers noted that "the participants in our study had minimal practice in these tasks, and it remains an open question whether it is possible to obtain more sustainable, definitive, and richer 4D representations with increased perceptual experience in 4D virtual environments." In another study, the ability of humans to orient themselves in 2D, 3D and 4D mazes has been tested. Each maze consisted of four path segments of random length and connected with orthogonal random bends, but without branches or loops (i.e. actually labyrinths). The graphical interface was based on John McIntosh's free 4D Maze game. The participating persons had to navigate through the path and finally estimate the linear direction back to the starting point. The researchers found that some of the participants were able to mentally integrate their path after some practice in 4D (the lower-dimensional cases were for comparison and for the participants to learn the method).
Dimensional analogy.
To understand the nature of four-dimensional space, a device called "dimensional analogy" is commonly employed. Dimensional analogy is the study of how ("n" − 1) dimensions relate to "n" dimensions, and then inferring how "n" dimensions would relate to ("n" + 1) dimensions.
Dimensional analogy was used by Edwin Abbott Abbott in the book "Flatland", which narrates a story about a square that lives in a two-dimensional world, like the surface of a piece of paper. From the perspective of this square, a three-dimensional being has seemingly god-like powers, such as ability to remove objects from a safe without breaking it open (by moving them across the third dimension), to see everything that from the two-dimensional perspective is enclosed behind walls, and to remain completely invisible by standing a few inches away in the third dimension.
By applying dimensional analogy, one can infer that a four-dimensional being would be capable of similar feats from our three-dimensional perspective. Rudy Rucker illustrates this in his novel "Spaceland", in which the protagonist encounters four-dimensional beings who demonstrate such powers.
Cross-sections.
As a three-dimensional object passes through a two-dimensional plane, a two-dimensional being would only see a cross-section of the three-dimensional object. For example, if a spherical balloon passed through a sheet of paper, a being on the paper would see first a single point, then a circle gradually growing larger, then smaller again until it shrank to a point and then disappeared. Similarly, if a four-dimensional object passed through three dimensions, we would see a three-dimensional cross-section of the four-dimensional object—for example, a hypersphere would appear first as a point, then as a growing sphere, with the sphere then shrinking to a single point and then disappearing. This means of visualizing aspects of the fourth dimension was used in the novel Flatland and also in several works of Charles Howard Hinton.
Projections.
A useful application of dimensional analogy in visualizing the fourth dimension is in projection. A projection is a way for representing an "n"-dimensional object in "n" − 1 dimensions. For instance, computer screens are two-dimensional, and all the photographs of three-dimensional people, places and things are represented in two dimensions by projecting the objects onto a flat surface. When this is done, depth is removed and replaced with indirect information. The retina of the eye is also a two-dimensional array of receptors but the brain is able to perceive the nature of three-dimensional objects by inference from indirect information (such as shading, foreshortening, binocular vision, etc.). Artists often use perspective to give an illusion of three-dimensional depth to two-dimensional pictures.
Similarly, objects in the fourth dimension can be mathematically projected to the familiar 3 dimensions, where they can be more conveniently examined. In this case, the 'retina' of the four-dimensional eye is a three-dimensional array of receptors. A hypothetical being with such an eye would perceive the nature of four-dimensional objects by inferring four-dimensional depth from indirect information in the three-dimensional images in its retina.
The perspective projection of three-dimensional objects into the retina of the eye introduces artifacts such as foreshortening, which the brain interprets as depth in the third dimension. In the same way, perspective projection from four dimensions produces similar foreshortening effects. By applying dimensional analogy, one may infer four-dimensional "depth" from these effects.
As an illustration of this principle, the following sequence of images compares various views of the 3-dimensional cube with analogous projections of the 4-dimensional tesseract into three-dimensional space.
Shadows.
A concept closely related to projection is the casting of shadows.
If a light is shone on a three dimensional object, a two-dimensional shadow is cast. By dimensional analogy, light shone on a two-dimensional object in a two-dimensional world would cast a one-dimensional shadow, and light on a one-dimensional object in a one-dimensional world would cast a zero-dimensional shadow, that is, a point of non-light. Going the other way, one may infer that light shone on a four-dimensional object in a four-dimensional world would cast a three-dimensional shadow.
If the wireframe of a cube is lit from above, the resulting shadow is a square within a square with the corresponding corners connected. Similarly, if the wireframe of a tesseract were lit from “above” (in the fourth dimension), its shadow would be that of a three-dimensional cube within another three-dimensional cube. (Note that, technically, the visual representation shown here is actually a two-dimensional image of the three-dimensional shadow of the four-dimensional wireframe figure.)
Bounding volumes.
Dimensional analogy also helps in inferring basic properties of objects in higher dimensions. For example, two-dimensional objects are bounded by one-dimensional boundaries: a square is bounded by four edges. Three-dimensional objects are bounded by two-dimensional surfaces: a cube is bounded by 6 square faces. By applying dimensional analogy, one may infer that a four-dimensional cube, known as a tesseract, is bounded by three-dimensional volumes. And indeed, this is the case: mathematics shows that the tesseract is bounded by 8 cubes. Knowing this is key to understanding how to interpret a three-dimensional projection of the tesseract. The boundaries of the tesseract project to "volumes" in the image, not merely two-dimensional surfaces.
Visual scope.
Being three-dimensional, we are only able to see the world with our eyes in two dimensions. A four-dimensional being would be able to see the world in three dimensions. For example, it would be able to see all six sides of an opaque box simultaneously, and in fact, what is inside the box at the same time, just as we can see the interior of a square on a piece of paper. It would be able to see all points in 3-dimensional space simultaneously, including the inner structure of solid objects and things obscured from our three-dimensional viewpoint. Our brains receive images in the second dimension and use reasoning to help us "picture" three-dimensional objects.
Limitations.
Reasoning by analogy from familiar lower dimensions can be an excellent intuitive guide, but care must be exercised not to accept results that are not more rigorously tested. For example, consider the formulas for the circumference of a circle
formula_10
and the surface area of a sphere:
formula_11.
One might be tempted to suppose that the surface volume of a hypersphere is formula_12, or perhaps formula_13, but either of these would be wrong. The correct formula is formula_14.

</doc>
<doc id="226424" url="https://en.wikipedia.org/wiki?curid=226424" title="Four-vector">
Four-vector

In special relativity, a four-vector is an object with four in general complex components that transform in a specific way under Lorentz transformations. Specifically, a four-vector is an element of a 4-dimensional vector space considered as a representation space of the standard representation of the Lorentz group, the (½,½) representation. It differs from a Euclidean vector in how its magnitude is determined. The transformations that preserve this magnitude are the Lorentz transformations. They include spatial rotations, boosts (a change by a constant velocity to another inertial reference frame), and temporal and spatial inversions. 
Four-vectors describe, for instance, position in spacetime modeled as Minkowski space, a particles 4-momentum , the amplitude of the electromagnetic four-potential at a point in spacetime, and the elements of the subspace spanned by the gamma matrices inside the Dirac algebra. 
The Lorentz group may be represented by 4×4 matrices . The action of a Lorentz transformation on a general contravariant four-vector (like the examples above), regarded as a column vector with Cartesian coordinates with respect to an inertial frame in the entries, is given by
(matrix multiplication) where the components of the primed object refer to the new frame. Related to the examples above that are given as contravariant vectors, there are also the corresponding covariant vectors , and . These transform according to the rule
where denotes the matrix transpose. This rule is different from the above rule. It corresponds to the dual representation of the standard representation. However, for the Lorentz group the dual of any representation is equivalent to the original representation. Thus the objects with covariant indices are four-vectors as well.
For an example of a well-behaved four-component object in special relativity that is "not" a four-vector, see bispinor. It is similarly defined, the difference being that the transformation rule under Lorentz transformations is given by a representation other than the standard representation. In this case, the rule reads , where is a 4×4 matrix other than . Similar remarks apply to objects with fewer or more components that are well-behaved under Lorentz transformations. These include scalars, spinors, tensors and spinor-tensors.
The article considers four-vectors in the context of special relativity. Although the concept of four-vectors also extends to general relativity, some of the results stated in this article require modification in general relativity. 
Notation.
The notations in this article are: lowercase bold for three-dimensional vectors, hats for three-dimensional unit vectors, capital bold for four dimensional vectors (except for the four-gradient), and tensor index notation.
Four-vector algebra.
Four-vectors in a real-valued basis.
A four-vector "A" is a vector with a "timelike" component and three "spacelike" components, and can be written in various equivalent notations:
The upper indices indicate contravariant components. Here the standard convention that Latin indices take values for spatial components, so that "i" = 1, 2, 3, and Greek indices take values for space "and time" components, so "α" = 0, 1, 2, 3, used with the summation convention. The split between the time component and the spatial components is a useful one to make when determining contractions of one four vector with other tensor quantities, such as for calculating Lorentz invariants in inner products (examples are given below), or raising and lowering indices. 
In special relativity, the spacelike basis E1, E2, E3 and components "A"1, "A"2, "A"3 are often Cartesian basis and components:
although, of course, any other basis and components may be used, such as spherical polar coordinates
or cylindrical polar coordinates, 
or any other orthogonal coordinates, or even general curvilinear coordinates. Note the coordinate labels are always subscripted as labels and are not indices taking numerical values. In general relativity, local curvilinear coordinates in a local basis must be used. Geometrically, a four-vector can still be interpreted as an arrow, but in spacetime - not just space. In relativity, the arrows are drawn as part of Minkowski diagram (also called "spacetime diagram"). In this article, four-vectors will be referred to simply as vectors.
It is also customary to represent the bases by column vectors:
so that:
The relation between the covariant and contravariant coordinates is through the Minkowski metric tensor, "η" which raises and lowers indices as follows:
and in various equivalent notations the covariant components are:
where the lowered index indicates it to be covariant. Often the metric is diagonal, as is the case for orthogonal coordinates (see line element), but not in general curvilinear coordinates.
The bases can be represented by row vectors:
so that:
The motivation for the above conventions are that the inner product is a scalar, see below for details.
Lorentz transformation.
Given two inertial or rotated frames of reference, a four-vector is defined as a quantity which transforms according to the Lorentz transformation matrix Λ:
In index notation, the contravariant and covariant components transform according to, respectively:
in which the matrix Λ has components Λ"μν" in row "μ" and column "ν", and the inverse matrix Λ−1 has components Λ"μν" in row "μ" and column "ν". 
For background on the nature of this transformation definition, see tensor. All four-vectors transform in the same way, and this can be generalized to four-dimensional relativistic tensors; see special relativity.
Pure rotations about an arbitrary axis.
For two frames rotated by a fixed angle "θ" about an axis defined by the unit vector:
without any boosts, the matrix Λ has components given by:
where "δij" is the Kronecker delta, and "εijk" is the three-dimensional Levi-Civita symbol. The spacelike components of 4-vectors are rotated, while the timelike components remain unchanged. 
For the case of rotations about the "z"-axis only, the spacelike part of the Lorentz matrix reduces to the rotation matrix about the "z"-axis:
Pure boosts in an arbitrary direction.
For two frames moving at constant relative 3-velocity v (not 4-velocity, see below), it is convenient to denote and define the relative velocity in units of "c" by:
Then without rotations, the matrix Λ has components given by:
where the Lorentz factor is defined by:
and "δij" is the Kronecker delta. Contrary to the case for pure rotations, the spacelike and timelike components are mixed together under boosts.
For the case of a boost in the "x"-direction only, the matrix reduces to;
Where the rapidity "ϕ" expression has been used, written in terms of the hyperbolic functions:
This Lorentz matrix illustrates the boost to be a "hyperbolic rotation" in four dimensional spacetime, analogous to the circular rotation above in three-dimensional space.
Properties.
Linearity.
Four-vectors have the same linearity properties as Euclidean vectors in three dimensions. They can be added in the usual entrywise way:
and similarly scalar multiplication by a scalar "λ" is defined entrywise by:
Then subtraction is the inverse operation of addition, defined entrywise by:
Minkowski tensor.
Applying the Minkowski tensor "η" to two four-vectors A and B, writing the result in dot product notation, we have, using Einstein notation:
It is convenient to rewrite the definition in matrix form:
in which case "ημν" above is the entry in row "μ" and column "ν" of the Minkowski metric as a square matrix. The Minkowski metric is not a Euclidean metric, because it is indefinite (see metric signature). A number of other expressions can be used because the metric tensor raises and lowers the components of A and B. For contra/co-variant components of A and co/contra-variant components of B, we have:
so in the matrix notation:
while for A and B each in covariant components: 
with a similar matrix expression to the above.
Applying the Minkowski tensor to a four-vector A with itself we get:
which, depending on the case, may be considered the square, or its negative, of the length of the vector.
Following are two common choices for the metric tensor in the standard basis (essentially Cartesian coordinates). If orthogonal coordinates are used, there would be scale factors along the diagonal part of the spacelike part of the metric, while for general curvilinear coordinates the entire spacelike part of the metric would have components dependent on the curvilinear basis used.
Standard basis, (+−−−) signature.
In the (+−−−) metric signature, evaluating the summation over indices gives:
while in matrix form:
It is a recurring theme in special relativity to take the expression
in one reference frame, where "C" is the value of the inner product in this frame, and:
in another frame, in which "C"′ is the value of the inner product in this frame. Then since the inner product is an invariant, these must be equal:
that is:
Considering that physical quantities in relativity are four-vectors, this equation has the appearance of a "conservation law", but there is no "conservation" involved. The primary significance of the Minkowski inner product is that for any two four-vectors, its value is invariant for all observers; a change of coordinates does not result in a change in value of the inner product. The components of the four-vectors change from one frame to another; A and A′ are connected by a Lorentz transformation, and similarly for B and B′, although the inner products are the same in all frames. Nevertheless, this type of expression is exploited in relativistic calculations on a par with conservation laws, since the magnitudes of components can be determined without explicitly performing any Lorentz transformations. A particular example is with energy and momentum in the energy-momentum relation derived from the four-momentum vector (see also below).
In this signature we have:
With the signature (+−−−), four-vectors may be classified as either spacelike if formula_41, timelike if formula_42, and null vectors if formula_43.
Standard basis, (−+++) signature.
Some authors define "η" with the opposite sign, in which case we have the (−+++) metric signature. Evaluating the summation with this signature:
while the matrix form is:
Note that in this case, in one frame:
while in another:
so that:
which is equivalent to the above expression for "C" in terms of A and B. Either convention will work. With the Minkowski metric defined in the two ways above, the only difference between covariant and contravariant four-vector components are signs, therefore the signs depend on which sign convention is used.
We have:
With the signature (−+++), four-vectors may be classified as either spacelike if formula_42, timelike if formula_41, and null vectors if formula_43..
Dual vectors.
Applying the Minkowski tensor is often expressed as the effect of the dual vector of one vector on the other:
Here the "Aν"s are the components of the dual vector A* of A in the dual basis and called the covariant coordinates of A, while the original "Aν" components are called the contravariant coordinates.
Four-vector calculus.
Derivatives and differentials.
In special relativity (but not general relativity), the derivative of a four-vector with respect to a scalar "λ" (invariant) is itself a four-vector. It is also useful to take the differential of the four-vector, "d"A and divide it by the differential of the scalar, "dλ":
where the contravariant components are:
while the covariant components are:
In relativistic mechanics, one often takes the differential of a four-vector and divides by the differential in proper time (see below).
Fundamental four-vectors.
Four-position.
A point in Minkowski space is a time and spatial position, called an "event", or sometimes the position 4-vector or 4-position, described in some reference frame by a set of four coordinates:
where r is the three-dimensional space position vector. If r is a function of coordinate time "t" in the same frame, i.e. r = r("t"), this corresponds to a sequence of events as "t" varies. The definition "R"0 = "ct" ensures that all the coordinates have the same units (of distance). These coordinates are the components of the "position four-vector" for the event.
The "displacement four-vector" is defined to be an "arrow" linking two events:
For the differential 4-position on a world line we have, using a norm notation:
defining the differential line element d"s" and differential proper time increment d"τ", but this "norm" is also:
so that:
When considering physical phenomena, differential equations arise naturally; however, when considering space and time derivatives of functions, it is unclear which reference frame these derivatives are taken with respect to. It is agreed that time derivatives are taken with respect to the proper time τ. As proper time is an invariant, this guarantees that the proper-time-derivative of any four-vector is itself a four-vector. It is then important to find a relation between this proper-time-derivative and another time derivative (using the coordinate time "t" of an inertial reference frame). This relation is provided by taking the above differential invariant spacetime interval, then dividing by ("cdt")2 to obtain:
where u = "d"r/"dt" is the coordinate 3-velocity of an object measured in the same frame as the coordinates "x", "y", "z", and coordinate time "t", and 
is the Lorentz factor. This provides a useful relation between the differentials in coordinate time and proper time:
This relation can also be found from the time transformation in the Lorentz transformations. Important four-vectors in relativity theory can be defined by dividing by this differential.
Four-gradient.
Considering that partial derivatives are linear operators, one can form a four-gradient from the partial time derivative /"t" and the spatial gradient ∇. Using the standard basis, in index and abbreviated notations, the contravariant components are:
Note the basis vectors are placed in front of the components, to prevent confusion between taking the derivative of the basis vector, or simply indicating the partial derivative is a component of this four-vector. The covariant components are:
Since this is an operator, it doesn't have a "length", but evaluating the inner product of the operator with itself gives another operator:
called the D'Alembert operator.
Kinematics.
Four-velocity.
The four-velocity of a particle is defined by:
Geometrically, U is a normalized vector tangent to the world line of the particle. Using the differential of the 4-position, the magnitude of the 4-velocity can be obtained:
in short, the magnitude of the 4-velocity for any object is always a fixed constant:
The norm is also:
so that:
which reduces to the definition the Lorentz factor.
Four-acceleration.
The four-acceleration is given by:
where a = "d"u/"dt" is the coordinate 3-acceleration. Since the magnitude of U is a constant, the four acceleration is orthogonal to the four velocity, i.e. the Minkowski inner product of the four-acceleration and the four-velocity is zero:
which is true for all world lines. The geometric meaning of 4-acceleration is the curvature vector of the world line in Minkowski space.
Dynamics.
Four-momentum.
For a massive particle of rest mass (or invariant mass) "m"0, the four-momentum is given by:
where the total energy of the moving particle is:
and the total relativistic momentum is:
Taking the inner product of the four-momentum with itself:
and also:
which leads to the energy–momentum relation:
This last relation is useful relativistic mechanics, essential in relativistic quantum mechanics and relativistic quantum field theory, all with applications to particle physics.
Four-force.
The four-force acting on a particle is defined analogously to the 3-force as the time derivative of 3-momentum in Newton's second law:
where "P" is the power transferred to move the particle, and f is the 3-force acting on the particle. For a particle of constant invariant mass "m"0, this is equivalent to
An invariant derived from the 4-force is:
from the above result.
Thermodynamics.
Four-heat flux.
The 4-heat flux vector field, is essentially similar to the 3d heat flux vector field q, in the local frame of the fluid:
where "T" is absolute temperature and "k" is thermal conductivity.
Four-baryon number flux.
The flux of baryons is: 
where "n" is the number density of baryons in the local rest frame of the baryon fluid (positive values for baryons, negative for antibaryons), and U the 4-velocity field (of the fluid) as above.
Four-entropy.
The 4-entropy vector is defined by:
where "s" is the entropy per baryon, and "T" the absolute temperature, in the local rest frame of the fluid.
Electromagnetism.
Examples of four-vectors in electromagnetism include the following.
Four-current.
The electromagnetic four-current is defined by
formed from the current density j and charge density "ρ".
Four-potential.
The electromagnetic four-potential defined by
formed from the vector potential a and the scalar potential "ϕ". The four-potential is not uniquely determined, because it depends on a choice of gauge.
Waves.
Four-frequency.
A photonic plane wave can be described by the four-frequency defined as
where "ν" is the frequency of the wave and formula_90 is a unit vector in the travel direction of the wave. Now:
so the 4-frequency of a photon is always a null vector.
Four-wavevector.
The quantities reciprocal to time "t" and space r are the angular frequency "ω" and wave vector k, respectively. They form the components of the 4-WaveVector or wave 4-vector:
A wave packet of nearly monochromatic light can be described by:
The de Broglie relations then showed that 4-WaveVector applied to matter waves as well as to light waves. :
yielding formula_95 and formula_96, where "ħ" is the Planck constant divided by 2"π". 
The square of the norm is:
and by the de Broglie relation:
we have the matter wave analogue of the energy–momentum relation:
Note that for massless particles, in which case , we have:
or ||k|| = "ω"/"c". Note this is consistent with the above case; for photons with a 3-wavevector of modulus "ω"/"c", in the direction of wave propagation defined by the unit vector formula_90.
Quantum theory.
In quantum mechanics, the 4-probability current or probability 4-current is analogous to the electromagnetic 4-current:
where "ρ" is the probability density function corresponding to the time component, and j is the probability current vector. In non-relativistic quantum mechanics, this current is always well defined because the expressions for density and current are positive definite and can admit a probability interpretation. In relativistic quantum mechanics and quantum field theory, it is not always possible to find a current, particularly when interactions are involved.
Replacing the energy by the energy operator and the momentum by the momentum operator in the four-momentum, one obtains the four-momentum operator, used in relativistic wave equations.
Other formulations.
Four-vectors in the algebra of physical space.
A four-vector "A" can also be defined in using the Pauli matrices as a basis, again in various equivalent notations:
or explicitly:
and in this formulation, the four-vector is represented as a Hermitian matrix (the matrix transpose and complex conjugate of the matrix leaves it unchanged), rather than a real-valued column or row vector. The determinant of the matrix is the modulus of the four-vector, so the determinant is an invariant:
This idea of using the Pauli matrices as basis vectors is employed in the algebra of physical space, an example of a Clifford algebra.
Four-vectors in spacetime algebra.
In spacetime algebra, another example of Clifford algebra, the gamma matrices can also form a basis. (They are also called the Dirac matrices, owing to their appearance in the Dirac equation). There is more than one way to express the gamma matrices, detailed in that main article.
The Feynman slash notation is a shorthand for a four-vector A contracted with the gamma matrices:
The four-momentum contracted with the gamma matrices is an important case in relativistic quantum mechanics and relativistic quantum field theory. In the Dirac equation and other relativistic wave equations, terms of the form:
appear, in which the energy "E" and momentum components are replaced by their respective operators.

</doc>
<doc id="52247" url="https://en.wikipedia.org/wiki?curid=52247" title="Fourier transform">
Fourier transform

The Fourier transform decomposes a function of time (a "signal") into the frequencies that make it up, similarly to how a musical chord can be expressed as the amplitude (or loudness) of its constituent notes. The Fourier transform of a function of time itself is a complex-valued function of frequency, whose absolute value represents the amount of that frequency present in the original function, and whose complex argument is the phase offset of the basic sinusoid in that frequency. The Fourier transform is called the "frequency domain representation" of the original signal. The term "Fourier transform" refers to both the frequency domain representation and the mathematical operation that associates the frequency domain representation to a function of time. The Fourier transform is not limited to functions of time, but in order to have a unified language, the domain of the original function is commonly referred to as the "time domain". For many functions of practical interest one can define an operation that reverses this: the "inverse Fourier transformation", also called "Fourier synthesis", of a frequency domain representation combines the contributions of all the different frequencies to recover the original function of time.
Linear operations performed in one domain (time or frequency) have corresponding operations in the other domain, which are sometimes easier to perform. The operation of differentiation in the time domain corresponds to multiplication by the frequency, so some differential equations are easier to analyze in the frequency domain. Also, convolution in the time domain corresponds to ordinary multiplication in the frequency domain. Concretely, this means that any linear time-invariant system, such as a filter applied to a signal, can be expressed relatively simply as an operation on frequencies. After performing the desired operations, transformation of the result can be made back to the time domain. Harmonic analysis is the systematic study of the relationship between the frequency and time domains, including the kinds of functions or operations that are "simpler" in one or the other, and has deep connections to almost all areas of modern mathematics.
Functions that are localized in the time domain have Fourier transforms that are spread out across the frequency domain and vice versa, a phenomenon known as the uncertainty principle. The critical case for this principle is the Gaussian function, of substantial importance in probability theory and statistics as well as in the study of physical phenomena exhibiting normal distribution (e.g., diffusion). The Fourier transform of a Gaussian function is another Gaussian function. Joseph Fourier introduced the transform in his study of heat transfer, where Gaussian functions appear as solutions of the heat equation.
The Fourier transform can be formally defined as an improper Riemann integral, making it an integral transform, although this definition is not suitable for many applications requiring a more sophisticated integration theory. For example, many relatively simple applications use the Dirac delta function, which can be treated formally as if it were a function, but the justification requires a mathematically more sophisticated viewpoint. The Fourier transform can also be generalized to functions of several variables on Euclidean space, sending a function of space to a function of momentum (or a function of space and time to a function of 4-momentum). This idea makes the spatial Fourier transform very natural in the study of waves, as well as in quantum mechanics, where it is important to be able to represent wave solutions as functions of either space or momentum and sometimes both. In general, functions to which Fourier methods are applicable are complex-valued, and possibly vector-valued. Still further generalization is possible to functions on groups, which, besides the original Fourier transform on or (viewed as groups under addition), notably includes the discrete-time Fourier transform (DTFT, group = ), the discrete Fourier transform (DFT, group = ) and the Fourier series or circular Fourier transform (group = , the unit circle ≈ closed finite interval with endpoints identified). The latter is routinely employed to handle periodic functions. The fast Fourier transform (FFT) is an algorithm for computing the DFT.
Definition.
There are several common conventions for defining the Fourier transform formula_1 of an integrable function formula_2 , . This article will use the following definition:
When the independent variable "x" represents "time" (with SI unit of seconds), the transform variable "ξ" represents frequency (in hertz). Under suitable conditions, formula_4 is determined by formula_5 via the inverse transform:
The statement that formula_4 can be reconstructed from formula_5 is known as the Fourier inversion theorem, and was first introduced in Fourier's "Analytical Theory of Heat" , , although what would be considered a proof by modern standards was not given until much later . The functions formula_4 and formula_1 often are referred to as a "Fourier integral pair" or "Fourier transform pair" .
For other common conventions and notations, including using the angular frequency "ω" instead of the frequency "ξ", see Other conventions and Other notations below. The Fourier transform on Euclidean space is treated separately, in which the variable "x" often represents position and "ξ" momentum.
History.
In 1822, Joseph Fourier showed that some functions could be written as an infinite sum of harmonics.
Introduction.
One motivation for the Fourier transform comes from the study of Fourier series. In the study of Fourier series, complicated but periodic functions are written as the sum of simple waves mathematically represented by sines and cosines. The Fourier transform is an extension of the Fourier series that results when the period of the represented function is lengthened and allowed to approach infinity .
Due to the properties of sine and cosine, it is possible to recover the amplitude of each wave in a Fourier series using an integral. In many cases it is desirable to use Euler's formula, which states that , to write Fourier series in terms of the basic waves "e"2"iθ". This has the advantage of simplifying many of the formulas involved, and provides a formulation for Fourier series that more closely resembles the definition followed in this article. Re-writing sines and cosines as complex exponentials makes it necessary for the Fourier coefficients to be complex valued. The usual interpretation of this complex number is that it gives both the amplitude (or size) of the wave present in the function and the phase (or the initial angle) of the wave. These complex exponentials sometimes contain negative "frequencies". If "θ" is measured in seconds, then the waves and both complete one cycle per second, but they represent different frequencies in the Fourier transform. Hence, frequency no longer measures the number of cycles per unit time, but is still closely related.
There is a close connection between the definition of Fourier series and the Fourier transform for functions "f" that are zero outside an interval. For such a function, we can calculate its Fourier series on any interval that includes the points where "f" is not identically zero. The Fourier transform is also defined for such a function. As we increase the length of the interval on which we calculate the Fourier series, then the Fourier series coefficients begin to look like the Fourier transform and the sum of the Fourier series of "f" begins to look like the inverse Fourier transform. To explain this more precisely, suppose that "T" is large enough so that the interval [−"T"/2, "T"/2] contains the interval on which "f" is not identically zero. Then the "n"-th series coefficient "cn" is given by:
Comparing this to the definition of the Fourier transform, it follows that formula_12 since "f"("x") is zero outside [−"T"/2,"T"/2]. Thus the Fourier coefficients are just the values of the Fourier transform sampled on a grid of width 1/"T", multiplied by the grid width 1/"T".
Under appropriate conditions, the Fourier series of "f" will equal the function "f". In other words, "f" can be written:
where the last sum is simply the first sum rewritten using the definitions ξ"n" = "n"/"T", and Δξ = ("n" + 1)/"T" − "n"/"T" = 1/"T".
This second sum is a Riemann sum, and so by letting "T" → ∞ it will converge to the integral for the inverse Fourier transform given in the definition section. Under suitable conditions this argument may be made precise .
In the study of Fourier series the numbers "cn" could be thought of as the "amount" of the wave present in the Fourier series of "f". Similarly, as seen above, the Fourier transform can be thought of as a function that measures how much of each individual frequency is present in our function "f", and we can recombine these waves by using an integral (or "continuous sum") to reproduce the original function.
Example.
The following figures provide a visual illustration of how the Fourier transform measures whether a frequency is present in a particular function. The function depicted "f"("t") = cos(6"t") e−"t"2 oscillates at 3 Hz (if "t" measures seconds) and tends quickly to 0. (The second factor in this equation is an envelope function that shapes the continuous sinusoid into a short pulse. Its general form is a Gaussian function). This function was specially chosen to have a real Fourier transform that can easily be plotted. The first image contains its graph. In order to calculate formula_14 we must integrate e−2"i"(3"t")"f"("t"). The second image shows the plot of the real and imaginary parts of this function. The real part of the integrand is almost always positive, because when "f"("t") is negative, the real part of e−2"i"(3"t") is negative as well. Because they oscillate at the same rate, when "f"("t") is positive, so is the real part of e−2"i"(3"t"). The result is that when you integrate the real part of the integrand you get a relatively large number (in this case 0.5). On the other hand, when you try to measure a frequency that is not present, as in the case when we look at formula_15, you see that both real and imaginary component of this function vary rapidly between positive and negative values, as plotted in the third image. Therefore, in this case, the integrand oscillates fast enough so that the integral is very small and the value for the fourier transform for that frequency is nearly zero. 
The general situation may be a bit more complicated than this, but this in spirit is how the Fourier transform measures how much of an individual frequency is present in a function "f"("t").
Properties of the Fourier transform.
Here we assume "f"("x"), "g"("x") and "h"("x") are "integrable functions": Lebesgue-measurable on the real line satisfying:
We denote the Fourier transforms of these functions by formula_17 , formula_18  and  formula_19 respectively.
Basic properties.
The Fourier transform has the following basic properties: .
That is, the evaluation of the Fourier transform in the origin (formula_34) equals the integral of "f" over all its domain.
Invertibility and periodicity.
Under suitable conditions on the function "f", it can be recovered from its Fourier transform formula_35 Indeed, denoting the Fourier transform operator by formula_36 so formula_37 then for suitable functions, applying the Fourier transform twice simply flips the function: formula_38, which can be interpreted as "reversing time". Since reversing time is two-periodic, applying this twice yields formula_39 so the Fourier transform operator is four-periodic, and similarly the inverse Fourier transform can be obtained by applying the Fourier transform three times: formula_40 In particular the Fourier transform is invertible (under suitable conditions).
More precisely, defining the parity operator formula_41 that inverts time, formula_42:
These equalities of operators require careful definition of the space of functions in question, defining equality of functions (equality at every point? equality almost everywhere?) and defining equality of operators – that is, defining the topology on the function space and operator space in question. These are not true for all functions, but are true under various conditions, which are the content of the various forms of the Fourier inversion theorem.
This four-fold periodicity of the Fourier transform is similar to a rotation of the plane by 90°, particularly as the two-fold iteration yields a reversal, and in fact this analogy can be made precise. While the Fourier transform can simply be interpreted as switching the time domain and the frequency domain, with the inverse Fourier transform switching them back, more geometrically it can be interpreted as a rotation by 90° in the time–frequency domain (considering time as the "x"-axis and frequency as the "y"-axis), and the Fourier transform can be generalized to the fractional Fourier transform, which involves rotations by other angles. This can be further generalized to linear canonical transformations, which can be visualized as the action of the special linear group SL2(R) on the time–frequency plane, with the preserved symplectic form corresponding to the uncertainty principle, below. This approach is particularly studied in signal processing, under time–frequency analysis.
Units and Duality.
In mathematics, one often does not think of any units as being attached to the two variables formula_45 and formula_46.
But in physical applications, formula_46 must have inverse units to the units of formula_45.
For example, if formula_45 is measured in seconds, formula_46
should be in cycles per second for the formulas here to be valid. If the scale of formula_45 is changed and formula_45 is measured in units of formula_53 seconds, then either formula_46 must be in the so-called "angular frequency", or one must insert some constant scale factor into some of the formulas.
If formula_45 is measured in units of length, then formula_46 must be in inverse length, e.g., wavenumbers. That is to say, there are two copies of the real line: one measured in one set of units, where formula_45 ranges, and the other in inverse units to the units of formula_45, and which is the range of formula_46.
So these are two distinct copies of the real line, and cannot be identified with each other. Therefore, the Fourier transform goes from one space of functions to a different space of functions: functions which have a different domain of definition.
In general, formula_46 must always be taken to be a linear form on the space of formula_45s, which is to say that the second real line is the dual space of the first real line. See the article on linear algebra for a more formal explanation and for more details. This point of view becomes essential in generalisations of the Fourier transform to general symmetry groups, including the case of Fourier series.
That there is no one preferred way (often, one says "no canonical way") to compare the two copies of the real line which are involved in the Fourier transform—fixing the units on one line does not force the scale of the units on the other line—is the reason for the plethora of rival conventions on the definition of the Fourier transform. The various definitions resulting from different choices of units differ by various constants. If the units of formula_45 are in seconds but the units of formula_46 are in angular frequency, then the angular frequency variable is often denoted by one or another Greek letter, for example, formula_64 is quite common. Thus (writing formula_65 for the alternative definition and formula_66 for the definition adopted in this article)
as before, but the corresponding alternative inversion formula would then have to be
To have something involving angular frequency but with greater symmetry between the Fourier transform and the inversion formula, one very often sees still another alternative definition of the Fourier transform, with a factor of formula_69, thus
and the corresponding inversion formula then has to be
Furthermore, there is no way to fix which square root of negative one will be meant by the symbol formula_72 (it makes no sense to speak of "the positive square root" since only real numbers can be positive, similarly it makes no sense to say "rotation counter-clockwise", because until formula_72 is chosen, there is no fixed way to draw the complex plane), and hence one occasionally sees the Fourier transform written with formula_72 in the exponent instead of formula_75, and vice versa for the inversion formula, a convention that is equally valid as the one chosen in this article, which is the more usual one.
For example, in probability theory, the characteristic function formula_76 of the probability density function formula_4 of a random variable formula_78 of continuous type is defined without a negative sign in the exponential, and since the units of formula_79 are ignored, there is no formula_53 either:
(In probability theory, and in mathematical statistics, the use of the Fourier—Stieltjes transform is preferred,
because so many random variables are not of continuous type, and do not possess a density function, and one must treat discontinuous distribution functions, i.e., measures which possess "atoms".)
From the higher point of view of group characters, which is much more abstract, all these arbitrary choices disappear, as will be explained in the later section of this article, on the notion of the Fourier transform of a function on an Abelian locally compact group.
Uniform continuity and the Riemann–Lebesgue lemma.
The Fourier transform may be defined in some cases for non-integrable functions, but the Fourier transforms of integrable functions have several strong properties.
The Fourier transform, formula_5, of any integrable function "f" is uniformly continuous and formula_83 . By the "Riemann–Lebesgue lemma" ,
However, formula_5 need not be integrable. For example, the Fourier transform of the rectangular function, which is integrable, is the sinc function, which is not Lebesgue integrable, because its improper integrals behave analogously to the alternating harmonic series, in converging to a sum without being absolutely convergent.
It is not generally possible to write the "inverse transform" as a Lebesgue integral. However, when both "f" and formula_5 are integrable, the inverse equality
holds almost everywhere. That is, the Fourier transform is injective on "L"1(R). (But if "f" is continuous, then equality holds for every "x".)
Plancherel theorem and Parseval's theorem.
Let "f"("x") and "g"("x") be integrable, and let formula_17 and formula_18 be their Fourier transforms. If "f"("x") and "g"("x") are also square-integrable, then we have Parseval's Formula :
where the bar denotes complex conjugation.
The Plancherel theorem, which follows from the above, states that 
Plancherel's theorem makes it possible to extend the Fourier transform, by a continuity argument, to a unitary operator on "L"2(R). On "L"1(R)∩"L"2(R), this extension agrees with original Fourier transform defined on "L"1(R), thus enlarging the domain of the Fourier transform to "L"1(R) + "L"2(R) (and consequently to "L"p(R) for 1 ≤ p ≤ 2). Plancherel's theorem has the interpretation in the sciences that the Fourier transform preserves the energy of the original quantity. The terminology of these formulas is not quite standardised. Parseval's theorem was proved only for Fourier series, and was first proved by Liapounoff. But Parseval's formula makes sense for the Fourier transform as well, and so even though in the context of the Fourier transform it was proved by Plancherel, it is still often referred to as Parseval's formula, or Parseval's relation, or even Parseval's theorem.
See Pontryagin duality for a general formulation of this concept in the context of locally compact abelian groups.
Poisson summation formula.
The Poisson summation formula (PSF) is an equation that relates the Fourier series coefficients of the periodic summation of a function to values of the function's continuous Fourier transform. The Poisson summation formula says that for sufficiently regular functions formula_92,
It has a variety of useful forms that are derived from the basic one by application of the Fourier transform's scaling and time-shifting properties. The formula has applications in engineering, physics, and number theory. The frequency-domain dual of the standard Poisson summation formula is also called the discrete-time Fourier transform.
Poisson summation is generally associated with the physics of periodic media, such as heat conduction on a circle. The fundamental solution of the heat equation on a circle is called a theta function. It is used in number theory to prove the transformation properties of theta functions, which turn out to be a type of modular form, and it is connected more generally to the theory of automorphic forms where it appears on one side of the Selberg trace formula.
Differentiation.
Suppose "f"("x") is a differentiable function, and both "f" and its derivative "f"' are integrable. Then the Fourier transform of the derivative is given by
More generally, the Fourier transformation of the "n"-th derivative "f(n)" is given by
By applying the Fourier transform and using these formulas, some ordinary differential equations can be transformed into algebraic equations, which are much easier to solve. These formulas also give rise to the rule of thumb "formula_96 is smooth if and only if formula_17 quickly falls down to 0 for formula_98." By using the analogous rules for the inverse Fourier transform, one can also say "formula_96 quickly falls down to 0 for formula_100 if and only if formula_17 is smooth."
Convolution theorem.
The Fourier transform translates between convolution and multiplication of functions. If "f"("x") and "g"("x") are integrable functions with Fourier transforms formula_17 and formula_18 respectively, then the Fourier transform of the convolution is given by the product of the Fourier transforms formula_17 and formula_18 (under other conventions for the definition of the Fourier transform a constant factor may appear).
This means that if:
where ∗ denotes the convolution operation, then:
In linear time invariant (LTI) system theory, it is common to interpret "g"("x") as the impulse response of an LTI system with input "f"("x") and output "h"("x"), since substituting the unit impulse for "f"("x") yields "h"("x") = "g"("x"). In this case, formula_18 represents the frequency response of the system.
Conversely, if "f"("x") can be decomposed as the product of two square integrable functions "p"("x") and "q"("x"), then the Fourier transform of "f"("x") is given by the convolution of the respective Fourier transforms formula_109 and formula_110.
Cross-correlation theorem.
In an analogous manner, it can be shown that if "h"("x") is the cross-correlation of "f"("x") and "g"("x"):
then the Fourier transform of "h"("x") is:
As a special case, the autocorrelation of function "f"("x") is:
for which
Eigenfunctions.
One important choice of an orthonormal basis for "L"2(R) is given by the Hermite functions
where He"n"("x") are the "probabilist's" Hermite polynomials, defined by
Under this convention for the Fourier transform, we have that
In other words, the Hermite functions form a complete orthonormal system of eigenfunctions for the Fourier transform on "L"2(R) . However, this choice of eigenfunctions is not unique. There are only four different eigenvalues of the Fourier transform (±1 and ±"i") and any linear combination of eigenfunctions with the same eigenvalue gives another eigenfunction. As a consequence of this, it is possible to decompose "L"2(R) as a direct sum of four spaces "H"0, "H"1, "H"2, and "H"3 where the Fourier transform acts on "He""k" simply by multiplication by "i""k".
Since the complete set of Hermite functions provides a resolution of the identity, the Fourier transform can be represented by such a sum of terms weighted by the above eigenvalues, and these sums can be explicitly summed. This approach to define the Fourier transform was first done by Norbert Wiener . Among other properties, Hermite functions decrease exponentially fast in both frequency and time domains, and they are thus used to define a generalization of the Fourier transform, namely the fractional Fourier transform used in time-frequency analysis . In physics, this transform was introduced by Edward Condon .
Complex domain.
The integral for the Fourier transform
can be studied for complex values of its argument formula_46. Depending on the properties of formula_4, this might not converge off the real axis at all, or it might converge to a complex analytic function for all values of formula_121, or something in between.
The Paley–Wiener theorem says that formula_4 is smooth (i.e., formula_123-times differentiable for all positive integers formula_123) and compactly supported if and only if formula_125 is a holomorphic function for which there exists a constant formula_126 such that for any integer formula_127,
for some constant formula_129. (In this case, formula_4 is supported on formula_131.)
This can be expressed by saying that formula_5 is an entire function which is rapidly decreasing in formula_133 (for fixed formula_134) and of exponential growth in formula_134 (uniformly in formula_133).
The space of such functions of a complex variable is called the Paley—Wiener space. This theorem has been generalised to semi-simple Lie groups.
If formula_4 is supported on the half-line formula_141, then formula_4 is said to be "causal" because the impulse response function of a physically realisable filter must have this property, as no effect can precede its cause. Paley and Wiener showed that then formula_5 extends to a holomorphic function on the complex lower half-plane formula_144 which tends to zero as formula_145 goes to infinity. The converse is false and it is not known how to characterise the Fourier transform of a causal function.
Laplace transform.
The Fourier transform formula_146 is intimately related with the Laplace transform formula_147, which is also used for the solution of differential equations and the analysis of filters.
Chatfield, indeed, has said that "... the Laplace and the Fourier transforms [of a causal function] are the same, provided that the real part of formula_148 is zero."
It may happen that a function formula_4 for which the Fourier integral does not converge on the real axis at all, nevertheless has a complex Fourier transform defined in some region of the complex plane.
For example, if formula_150 is of exponential growth, i.e.,
for some constants formula_152, then
convergent for all formula_154, is the two-sided Laplace transform of formula_4.
The more usual version ("one-sided") of the Laplace transform is
If formula_4 is also causal, then
Thus, extending the Fourier transform to the complex domain means it includes the Laplace transform as a special case—the case of causal functions—but with the change of variable formula_159.
Inversion.
If formula_5 has no poles for formula_161, then
by Cauchy's integral theorem. Therefore, the Fourier inversion formula
can use integration along different lines, parallel to the real
axis.
Theorem: If formula_163 for formula_164, and formula_165 for some constants formula_166, then
for any formula_168.
This theorem implies the Mellin inversion formula for the Laplace transformation,
for any formula_170, where formula_171 is the Laplace transform of formula_150.
The hypotheses can be weakened, as in the results of Carleman and Hunt, to formula_173 being formula_174, provided that formula_175 is in the interior of a closed interval on which formula_92 is continuous and of bounded variation, and provided that the integrals are taken in the sense of Cauchy principal values.
formula_177 versions of these inversion formulas are also available.
Fourier transform on Euclidean space.
The Fourier transform can be defined in any arbitrary number of dimensions "n". As with the one-dimensional case, there are many conventions. For an integrable function "f"(x), this article takes the definition:
where x and ξ are "n"-dimensional vectors, and is the dot product of the vectors. The dot product is sometimes written as formula_179.
All of the basic properties listed above hold for the "n"-dimensional Fourier transform, as do Plancherel's and Parseval's theorem. When the function is integrable, the Fourier transform is still uniformly continuous and the Riemann–Lebesgue lemma holds. 
Uncertainty principle.
Generally speaking, the more concentrated "f"("x") is, the more spread out its Fourier transform formula_180 must be. In particular, the scaling property of the Fourier transform may be seen as saying: if we "squeeze" a function in "x", its Fourier transform "stretches out" in "ξ". It is not possible to arbitrarily concentrate both a function and its Fourier transform.
The trade-off between the compaction of a function and its Fourier transform can be formalized in the form of an uncertainty principle by viewing a function and its Fourier transform as conjugate variables with respect to the symplectic form on the time–frequency domain: from the point of view of the linear canonical transformation, the Fourier transform is rotation by 90° in the time–frequency domain, and preserves the symplectic form.
Suppose "f"("x") is an integrable and square-integrable function. Without loss of generality, assume that "f"("x") is normalized:
It follows from the Plancherel theorem that formula_180 is also normalized.
The spread around "x" = 0 may be measured by the "dispersion about zero" defined by
In probability terms, this is the second moment of |"f"("x")|2 about zero.
The Uncertainty principle states that, if "f"("x") is absolutely continuous and the functions "x"·"f"("x") and "f"′("x") are square integrable, then
The equality is attained only in the case formula_185 (hence formula_186) where σ > 0 is arbitrary and formula_187 so that "f" is "L"2–normalized . In other words, where "f" is a (normalized) Gaussian function with variance σ2, centered at zero, and its Fourier transform is a Gaussian function with variance σ−2.
In fact, this inequality implies that:
for any "x"0, ξ0 ∈ R  .
In quantum mechanics, the momentum and position wave functions are Fourier transform pairs, to within a factor of Planck's constant. With this constant properly taken into account, the inequality above becomes the statement of the Heisenberg uncertainty principle .
A stronger uncertainty principle is the Hirschman uncertainty principle, which is expressed as:
where "H"("p") is the differential entropy of the probability density function "p"("x"):
where the logarithms may be in any base that is consistent. The equality is attained for a Gaussian, as in the previous case.
Sine and cosine transforms.
Fourier's original formulation of the transform did not use complex numbers, but rather sines and cosines. Statisticians and others still use this form. An absolutely integrable function formula_4 for which Fourier inversion holds good can be expanded in terms of genuine frequencies (avoiding negative frequencies, which are sometimes considered hard to interpret physically) formula_192 by
formula_193
This is called an expansion as a trigonometric integral, or a Fourier integral expansion. The coefficient functions formula_194 and formula_195 can be found by using variants of the Fourier cosine transform and the Fourier sine transform (the normalisations are, again, not standardised):
formula_196
and
formula_197
Older literature refers to the two transform functions, the Fourier cosine transform, formula_194, and the Fourier sine transform, formula_195.
The function "f" can be recovered from the sine and cosine transform using
formula_200
together with trigonometric identities. This is referred to as Fourier's integral formula.
Spherical harmonics.
Let the set of homogeneous harmonic polynomials of degree "k" on R"n" be denoted by A"k". The set A"k" consists of the solid spherical harmonics of degree "k". The solid spherical harmonics play a similar role in higher dimensions to the Hermite polynomials in dimension one. Specifically, if "f"("x") = "e"−|"x"|2"P"("x") for some "P"("x") in A"k", then formula_201. Let the set H"k" be the closure in "L"2(R"n") of linear combinations of functions of the form "f"(|"x"|)"P"("x") where "P"("x") is in A"k". The space "L"2(R"n") is then a direct sum of the spaces H"k" and the Fourier transform maps each space H"k" to itself and is possible to characterize the action of the Fourier transform on each space H"k" . Let "f"("x") = "f"0(|"x"|)"P"("x") (with "P"("x") in A"k"), then formula_202 where
Here "J"("n" + 2"k" − 2)/2 denotes the Bessel function of the first kind with order ("n" + 2"k" − 2)/2. When "k" = 0 this gives a useful formula for the Fourier transform of a radial function . Note that this is essentially the Hankel transform. Moreover, there is a simple recursion relating the cases "n" + 2 and "n" allowing to compute, e.g., the three-dimensional Fourier transform of a radial function from the one-dimensional one.
Restriction problems.
In higher dimensions it becomes interesting to study "restriction problems" for the Fourier transform. The Fourier transform of an integrable function is continuous and the restriction of this function to any set is defined. But for a square-integrable function the Fourier transform could be a general "class" of square integrable functions. As such, the restriction of the Fourier transform of an "L"2(R"n") function cannot be defined on sets of measure 0. It is still an active area of study to understand restriction problems in "L""p" for 1 < "p" < 2. Surprisingly, it is possible in some cases to define the restriction of a Fourier transform to a set "S", provided "S" has non-zero curvature. The case when "S" is the unit sphere in R"n" is of particular interest. In this case the Tomas–Stein restriction theorem states that the restriction of the Fourier transform to the unit sphere in R"n" is a bounded operator on "Lp" provided 1 ≤ "p" ≤ .
One notable difference between the Fourier transform in 1 dimension versus higher dimensions concerns the partial sum operator. Consider an increasing collection of measurable sets "E""R" indexed by "R" ∈ (0,∞): such as balls of radius "R" centered at the origin, or cubes of side 2"R". For a given integrable function "f", consider the function "fR" defined by:
Suppose in addition that "f" ∈ "Lp"(R"n"). For "n" = 1 and , if one takes "ER" = (−"R", "R"), then "fR" converges to "f" in "Lp" as "R" tends to infinity, by the boundedness of the Hilbert transform. Naively one may hope the same holds true for "n" > 1. In the case that "ER" is taken to be a cube with side length "R", then convergence still holds. Another natural candidate is the Euclidean ball "E""R" = {ξ : |ξ| < "R"}. In order for this partial sum operator to converge, it is necessary that the multiplier for the unit ball be bounded in "Lp"(R"n"). For "n" ≥ 2 it is a celebrated theorem of Charles Fefferman that the multiplier for the unit ball is never bounded unless "p" = 2 . In fact, when , this shows that not only may "fR" fail to converge to "f" in "Lp", but for some functions "f" ∈ "Lp"(R"n"), "fR" is not even an element of "Lp".
Fourier transform on function spaces.
On "L""p" spaces.
The definition of the Fourier transform by the integral formula
is valid for Lebesgue integrable functions "f"; that is, "f" ∈ "L"1(R"n").
The Fourier transform formula_206: "L"1(R"n") → "L"∞(R"n") is a bounded operator. This follows from the observation that
which shows that its operator norm is bounded by 1. Indeed, it equals 1, which can be seen, for example, from the transform of the rect function. The image of "L"1 is a subset of the space "C"0(R"n") of continuous functions that tend to zero at infinity (the Riemann–Lebesgue lemma), although it is not the entire space. Indeed, there is no simple characterization of the image.
Since compactly supported smooth functions are integrable and dense in "L"2(R"n"), the Plancherel theorem allows us to extend the definition of the Fourier transform to general functions in "L"2(R"n") by continuity arguments. The Fourier transform in "L"2(R"n") is no longer given by an ordinary Lebesgue integral, although it can be computed by an improper integral, here meaning that for an "L"2 function "f",
"To 400:" The variables "ξx", "ξy", "ωx", "ωy", "νx" and "νy" are real numbers.
The integrals are taken over the entire plane.
"To 401:" Both functions are Gaussians, which may not have unit volume.
"To 402:" The function is defined by circ("r") = 1 0 ≤ "r" ≤ 1, and is 0 otherwise. This is the Airy distribution, and is expressed using "J"1 (the order 1 Bessel function of the first kind). 
Formulas for general "n"-dimensional functions.
</math>
"To 501":
The function is the indicator function of the interval . The function Γ("x") is the gamma function. The function is a Bessel function of the first kind, with order . Taking and produces 402. 
"To 502":
See Riesz potential where the constant is given by formula_215. The formula also holds for all by analytic continuation, but then the function and its Fourier transforms need to be understood as suitably regularized tempered distributions. See homogeneous distribution.
"To 503":
This is the formula for a multivariate normal distribution normalized to 1 with a mean of 0. Bold variables are vectors or matrices. Following the notation of the aforementioned page, formula_216 and formula_217
"To 504":
Here formula_218. See .

</doc>
<doc id="10913" url="https://en.wikipedia.org/wiki?curid=10913" title="Fractal">
Fractal

A fractal is a natural phenomenon or a mathematical set that exhibits a repeating pattern that displays at every scale. It is also known as expanding symmetry or evolving symmetry. If the replication is exactly the same at every scale, it is called a self-similar pattern. An example of this is the Menger Sponge. Fractals can also be nearly the same at different levels. This latter pattern is illustrated in the magnifications of the Mandelbrot set. Fractals also include the idea of a detailed pattern that repeats itself.
Fractals are different from other geometric figures because of the way in which they scale. Doubling the edge lengths of a polygon multiplies its area by four, which is two (the ratio of the new to the old side length) raised to the power of two (the dimension of the space the polygon resides in). Likewise, if the radius of a sphere is doubled, its volume scales by eight, which is two (the ratio of the new to the old radius) to the power of three (the dimension that the sphere resides in). But if a fractal's one-dimensional lengths are all doubled, the spatial content of the fractal scales by a power that is not necessarily an integer. This power is called the fractal dimension of the fractal, and it usually exceeds the fractal's topological dimension.
As mathematical equations, fractals are usually nowhere differentiable. An infinite fractal curve can be conceived of as winding through space differently from an ordinary line, still being a 1-dimensional line yet having a fractal dimension indicating it also resembles a surface.
The mathematical roots of the idea of fractals have been traced throughout the years as a formal path of published works, starting in the 17th century with notions of recursion, then moving through increasingly rigorous mathematical treatment of the concept to the study of continuous but not differentiable functions in the 19th century, and on to the coining of the word "fractal" in the 20th century with a subsequent burgeoning of interest in fractals and computer-based modelling in the 21st century. The term "fractal" was first used by mathematician Benoît Mandelbrot in 1975. Mandelbrot based it on the Latin "frāctus" meaning "broken" or "fractured", and used it to extend the concept of theoretical fractional dimensions to geometric patterns in nature.
There is some disagreement amongst authorities about how the concept of a fractal should be formally defined. Mandelbrot himself summarized it as "beautiful, damn hard, increasingly useful. That's fractals." The general consensus is that theoretical fractals are infinitely self-similar, iterated, and detailed mathematical constructs having fractal dimensions, of which many examples have been formulated and studied in great depth. Fractals are not limited to geometric patterns, but can also describe processes in time. Fractal patterns with various degrees of self-similarity have been rendered or studied in images, structures and sounds and found in nature, technology, art, and law.
Introduction.
The word "fractal" often has different connotations for laypeople than for mathematicians, where the layperson is more likely to be familiar with fractal art than a mathematical conception. The mathematical concept is difficult to define formally even for mathematicians, but key features can be understood with little mathematical background.
The feature of "self-similarity", for instance, is easily understood by analogy to zooming in with a lens or other device that zooms in on digital images to uncover finer, previously invisible, new structure. If this is done on fractals, however, no new detail appears; nothing changes and the same pattern repeats over and over, or for some fractals, nearly the same pattern reappears over and over. Self-similarity itself is not necessarily counter-intuitive (e.g., people have pondered self-similarity informally such as in the infinite regress in parallel mirrors or the homunculus, the little man inside the head of the little man inside the head...). The difference for fractals is that the pattern reproduced must be detailed.
This idea of being detailed relates to another feature that can be understood without mathematical background: Having a fractional or fractal dimension greater than its topological dimension, for instance, refers to how a fractal scales compared to how geometric shapes are usually perceived. A regular line, for instance, is conventionally understood to be 1-dimensional; if such a curve is divided into pieces each 1/3 the length of the original, there are always 3 equal pieces. In contrast, consider the Koch snowflake. It is also 1-dimensional for the same reason as the ordinary line, but it has, in addition, a fractal dimension greater than 1 because of how its detail can be measured. The fractal curve divided into parts 1/3 the length of the original line becomes 4 pieces rearranged to repeat the original detail, and this unusual relationship is the basis of its fractal dimension.
This also leads to understanding a third feature, that fractals as mathematical equations are "nowhere differentiable". In a concrete sense, this means fractals cannot be measured in traditional ways. To elaborate, in trying to find the length of a wavy non-fractal curve, one could find straight segments of some measuring tool small enough to lay end to end over the waves, where the pieces could get small enough to be considered to conform to the curve in the normal manner of measuring with a tape measure. But in measuring a wavy fractal curve such as the Koch snowflake, one would never find a small enough straight segment to conform to the curve, because the wavy pattern would always re-appear, albeit at a smaller size, essentially pulling a little more of the tape measure into the total length measured each time one attempted to fit it tighter and tighter to the curve.
History.
The history of fractals traces a path from chiefly theoretical studies to modern applications in computer graphics, with several notable people contributing canonical fractal forms along the way. According to Pickover, the mathematics behind fractals began to take shape in the 17th century when the mathematician and philosopher Gottfried Leibniz pondered recursive self-similarity (although he made the mistake of thinking that only the straight line was self-similar in this sense). In his writings, Leibniz used the term "fractional exponents", but lamented that "Geometry" did not yet know of them. Indeed, according to various historical accounts, after that point few mathematicians tackled the issues and the work of those who did remained obscured largely because of resistance to such unfamiliar emerging concepts, which were sometimes referred to as mathematical "monsters". Thus, it was not until two centuries had passed that in 1872 Karl Weierstrass presented the first definition of a function with a graph that would today be considered fractal, having the non-intuitive property of being everywhere continuous but nowhere differentiable. Not long after that, in 1883, Georg Cantor, who attended lectures by Weierstrass, published examples of subsets of the real line known as Cantor sets, which had unusual properties and are now recognized as fractals. Also in the last part of that century, Felix Klein and Henri Poincaré introduced a category of fractal that has come to be called "self-inverse" fractals.
One of the next milestones came in 1904, when Helge von Koch, extending ideas of Poincaré and dissatisfied with Weierstrass's abstract and analytic definition, gave a more geometric definition including hand drawn images of a similar function, which is now called the Koch snowflake. Another milestone came a decade later in 1915, when Wacław Sierpiński constructed his famous triangle then, one year later, his carpet. By 1918, two French mathematicians, Pierre Fatou and Gaston Julia, though working independently, arrived essentially simultaneously at results describing what are now seen as fractal behaviour associated with mapping complex numbers and iterative functions and leading to further ideas about attractors and repellors (i.e., points that attract or repel other points), which have become very important in the study of fractals. Very shortly after that work was submitted, by March 1918, Felix Hausdorff expanded the definition of "dimension", significantly for the evolution of the definition of fractals, to allow for sets to have noninteger dimensions. The idea of self-similar curves was taken further by Paul Lévy, who, in his 1938 paper "Plane or Space Curves and Surfaces Consisting of Parts Similar to the Whole" described a new fractal curve, the Lévy C curve.
Different researchers have postulated that without the aid of modern computer graphics, early investigators were limited to what they could depict in manual drawings, so lacked the means to visualize the beauty and appreciate some of the implications of many of the patterns they had discovered (the Julia set, for instance, could only be visualized through a few iterations as very simple drawings]). That changed, however, in the 1960s, when Benoît Mandelbrot started writing about self-similarity in papers such as "How Long Is the Coast of Britain? Statistical Self-Similarity and Fractional Dimension", which built on earlier work by Lewis Fry Richardson. In 1975 Mandelbrot solidified hundreds of years of thought and mathematical development in coining the word "fractal" and illustrated his mathematical definition with striking computer-constructed visualizations. These images, such as of his canonical Mandelbrot set, captured the popular imagination; many of them were based on recursion, leading to the popular meaning of the term "fractal". Currently, fractal studies are essentially exclusively computer-based.
Characteristics.
One often cited description that Mandelbrot published to describe geometric fractals is "a rough or fragmented geometric shape that can be split into parts, each of which is (at least approximately) a reduced-size copy of the whole"; this is generally helpful but limited. Authors disagree on the exact definition of "fractal", but most usually elaborate on the basic ideas of self-similarity and an unusual relationship with the space a fractal is embedded in. One point agreed on is that fractal patterns are characterized by fractal dimensions, but whereas these numbers quantify complexity (i.e., changing detail with changing scale), they neither uniquely describe nor specify details of how to construct particular fractal patterns. In 1975 when Mandelbrot coined the word "fractal", he did so to denote an object whose Hausdorff–Besicovitch dimension is greater than its topological dimension. It has been noted that this dimensional requirement is not met by fractal space-filling curves such as the Hilbert curve.
According to Falconer, rather than being strictly defined, fractals should, in addition to being nowhere differentiable and able to have a fractal dimension, be generally characterized by a gestalt of the following features;
As a group, these criteria form guidelines for excluding certain cases, such as those that may be self-similar without having other typically fractal features. A straight line, for instance, is self-similar but not fractal because it lacks detail, is easily described in Euclidean language, has the same Hausdorff dimension as topological dimension, and is fully defined without a need for recursion.
Brownian motion.
A path generated by a one dimensional Wiener process is a fractal curve of dimension 1.5, and Brownian motion is a finite version of this.
Common techniques for generating fractals.
Images of fractals can be created by fractal generating programs.
Simulated fractals.
Fractal patterns have been modeled extensively, albeit within a range of scales rather than infinitely, owing to the practical limits of physical time and space. Models may simulate theoretical fractals or natural phenomena with fractal features. The outputs of the modelling process may be highly artistic renderings, outputs for investigation, or benchmarks for fractal analysis. Some specific applications of fractals to technology are listed elsewhere. Images and other outputs of modelling are normally referred to as being "fractals" even if they do not have strictly fractal characteristics, such as when it is possible to zoom into a region of the fractal image that does not exhibit any fractal properties. Also, these may include calculation or display artifacts which are not characteristics of true fractals.
Modeled fractals may be sounds, digital images, electrochemical patterns, circadian rhythms, etc.
Fractal patterns have been reconstructed in physical 3-dimensional space and virtually, often called "in silico" modeling. Models of fractals are generally created using fractal-generating software that implements techniques such as those outlined above. As one illustration, trees, ferns, cells of the nervous system, blood and lung vasculature, and other branching patterns in nature can be modeled on a computer by using recursive algorithms and L-systems techniques. The recursive nature of some patterns is obvious in certain examples—a branch from a tree or a frond from a fern is a miniature replica of the whole: not identical, but similar in nature. Similarly, random fractals have been used to describe/create many highly irregular real-world objects. A limitation of modeling fractals is that resemblance of a fractal model to a natural phenomenon does not prove that the phenomenon being modeled is formed by a process similar to the modeling algorithms.
Natural phenomena with fractal features.
Approximate fractals found in nature display self-similarity over extended, but finite, scale ranges. The connection between fractals and leaves, for instance, is currently being used to determine how much carbon is contained in trees. Phenomena known to have fractal features include: 
<br>
In creative works.
The paintings of American artist Jackson Pollock have a definite fractal dimension. While Pollock's paintings appear to be composed of chaotic dripping and splattering, computer analysis demonstrates a degree of self-similarity at different scales (levels of detail) in his work.
Decalcomania, a technique used by artists such as Max Ernst, can produce fractal-like patterns. It involves pressing paint between two surfaces and pulling them apart.
Cyberneticist Ron Eglash has suggested that fractal geometry and mathematics are prevalent in African art, games, divination, trade, and architecture. Circular houses appear in circles of circles, rectangular houses in rectangles of rectangles, and so on. Such scaling patterns can also be found in African textiles, sculpture, and even cornrow hairstyles.
In a 1996 interview with Michael Silverblatt, David Foster Wallace admitted that the structure of the first draft of "Infinite Jest" he gave to his editor Michael Pietsch was inspired by fractals, specifically the Sierpinski triangle (a.k.a. Sierpinski gasket) but that the edited novel is "more like a lopsided Sierpinsky Gasket".
See also.
Fractal-generating programs.
There are many fractal generating programs available, both free and commercial. Some of the fractal generating programs include:
Most of the above programs make two-dimensional fractals, with a few creating three-dimensional fractal objects, such as quaternions, mandelbulbs and mandelboxes.

</doc>
<doc id="74263" url="https://en.wikipedia.org/wiki?curid=74263" title="Frame of reference">
Frame of reference

In physics, a frame of reference (or reference frame) consists of an abstract coordinate system and the set of physical reference points that uniquely fix (locate and orient) the coordinate system and standardize measurements.
In n dimensions, n+1 reference points are sufficient to fully define a reference frame. Using rectangular (Cartesian) coordinates, a reference frame may be defined with a reference point at the origin and a reference point at one unit distance along each of the n coordinate axes.
In Einsteinian relativity, reference frames are used to specify the relationship between a moving observer and the phenomenon or phenomena under observation. In this context, the phrase often becomes "observational frame of reference" (or "observational reference frame"), which implies that the observer is at rest in the frame, although not necessarily located at its origin. A relativistic reference frame includes (or implies) the coordinate time, which does not correspond across different frames moving relatively to each other. The situation thus differs from Galilean relativity, where all possible coordinate times are essentially equivalent.
Different aspects of "frame of reference".
The need to distinguish between the various meanings of "frame of reference" has led to a variety of terms. For example, sometimes the type of coordinate system is attached as a modifier, as in "Cartesian frame of reference". Sometimes the state of motion is emphasized, as in "rotating frame of reference". Sometimes the way it transforms to frames considered as related is emphasized as in "Galilean frame of reference". Sometimes frames are distinguished by the scale of their observations, as in "macroscopic" and "microscopic frames of reference".
In this article, the term "observational frame of reference" is used when emphasis is upon the "state of motion" rather than upon the coordinate choice or the character of the observations or observational apparatus. In this sense, an observational frame of reference allows study of the effect of motion upon an entire family of coordinate systems that could be attached to this frame. On the other hand, a "coordinate system" may be employed for many purposes where the state of motion is not the primary concern. For example, a coordinate system may be adopted to take advantage of the symmetry of a system. In a still broader perspective, the formulation of many problems in physics employs "generalized coordinates", "normal modes" or "eigenvectors", which are only indirectly related to space and time. It seems useful to divorce the various aspects of a reference frame for the discussion below. We therefore take observational frames of reference, coordinate systems, and observational equipment as independent concepts, separated as below:
Here is a quotation applicable to moving observational frames formula_1 and various associated Euclidean three-space coordinate systems ["R", "R′", "etc."]:
and this on the utility of separating the notions of formula_1 and ["R", "R′", "etc."]:
and this, also on the distinction between formula_1 and ["R", "R′", "etc."]:
and from J. D. Norton:
The discussion is taken beyond simple space-time coordinate systems by Brading and Castellani. Extension to coordinate systems using generalized coordinates underlies the Hamiltonian and Lagrangian formulations of quantum field theory, classical relativistic mechanics, and quantum gravity.
Coordinate systems.
Although the term "coordinate system" is often used (particularly by physicists) in a nontechnical sense, the term "coordinate system" does have a precise meaning in mathematics, and sometimes that is what the physicist means as well.
A coordinate system in mathematics is a facet of geometry or of algebra, in particular, a property of manifolds (for example, in physics, configuration spaces or phase spaces). The coordinates of a point r in an "n"-dimensional space are simply an ordered set of "n" numbers: 
In a general Banach space, these numbers could be (for example) coefficients in a functional expansion like a Fourier series. In a physical problem, they could be spacetime coordinates or normal mode amplitudes. In a robot design, they could be angles of relative rotations, linear displacements, or deformations of joints. Here we will suppose these coordinates can be related to a Cartesian coordinate system by a set of functions:
where "x", "y", "z", "etc." are the "n" Cartesian coordinates of the point. Given these functions, coordinate surfaces are defined by the relations:
The intersection of these surfaces define coordinate lines. At any selected point, tangents to the intersecting coordinate lines at that point define a set of basis vectors {e1, e2, …, en} at that point. That is:
which can be normalized to be of unit length. For more detail see curvilinear coordinates.
Coordinate surfaces, coordinate lines, and basis vectors are components of a coordinate system. If the basis vectors are orthogonal at every point, the coordinate system is an orthogonal coordinate system.
An important aspect of a coordinate system is its metric tensor "gik", which determines the arc length "ds" in the coordinate system in terms of its coordinates:
where repeated indices are summed over.
As is apparent from these remarks, a coordinate system is a mathematical construct, part of an axiomatic system. There is no necessary connection between coordinate systems and physical motion (or any other aspect of reality). However, coordinate systems can include time as a coordinate, and can be used to describe motion. Thus, Lorentz transformations and Galilean transformations may be viewed as coordinate transformations.
General and specific topics of coordinate systems can be pursued following the See also links below.
Observational frames of reference.
An observational frame of reference, often referred to as a "physical frame of reference", a "frame of reference", or simply a "frame", is a physical concept related to an observer and the observer's state of motion. Here we adopt the view expressed by Kumar and Barve: an observational frame of reference is characterized "only by its state of motion". However, there is lack of unanimity on this point. In special relativity, the distinction is sometimes made between an "observer" and a "frame". According to this view, a "frame" is an "observer" plus a coordinate lattice constructed to be an orthonormal right-handed set of spacelike vectors perpendicular to a timelike vector. See Doran. This restricted view is not used here, and is not universally adopted even in discussions of relativity. In general relativity the use of general coordinate systems is common (see, for example, the Schwarzschild solution for the gravitational field outside an isolated sphere).
There are two types of observational reference frame: inertial and non-inertial. An inertial frame of reference is defined as one in which all laws of physics take on their simplest form. In special relativity these frames are related by Lorentz transformations, which are parametrized by rapidity. In Newtonian mechanics, a more restricted definition requires only that Newton's first law holds true; that is, a Newtonian inertial frame is one in which a free particle travels in a straight line at constant speed, or is at rest. These frames are related by Galilean transformations. These relativistic and Newtonian transformations are expressed in spaces of general dimension in terms of representations of the Poincaré group and of the Galilean group.
In contrast to the inertial frame, a non-inertial frame of reference is one in which fictitious forces must be invoked to explain observations. An example is an observational frame of reference centered at a point on the Earth's surface. This frame of reference orbits around the center of the Earth, which introduces the fictitious forces known as the Coriolis force, centrifugal force, and gravitational force. (All of these forces including gravity disappear in a truly inertial reference frame, which is one of free-fall.)
Measurement apparatus.
A further aspect of a frame of reference is the role of the measurement apparatus (for example, clocks and rods) attached to the frame (see Norton quote above). This question is not addressed in this article, and is of particular interest in quantum mechanics, where the relation between observer and measurement is still under discussion (see measurement problem).
In physics experiments, the frame of reference in which the laboratory measurement devices are at rest is usually referred to as the laboratory frame or simply "lab frame." An example would be the frame in which the detectors for a particle accelerator are at rest. The lab frame in some experiments is an inertial frame, but it is not required to be (for example the laboratory on the surface of the Earth in many physics experiments is not inertial). In particle physics experiments, it is often useful to transform energies and momenta of particles from the lab frame where they are measured, to the center of momentum frame "COM frame" in which calculations are sometimes simplified, since potentially all kinetic energy still present in the COM frame may be used for making new particles.
In this connection it may be noted that the clocks and rods often used to describe observers' measurement equipment in thought, in practice are replaced by a much more complicated and indirect metrology that is connected to the nature of the vacuum, and uses atomic clocks that operate according to the standard model and that must be corrected for gravitational time dilation. (See second, meter and kilogram).
In fact, Einstein felt that clocks and rods were merely expedient measuring devices and they should be replaced by more fundamental entities based upon, for example, atoms and molecules.
Examples of inertial frames of reference.
Simple example.
Consider a situation common in everyday life. Two cars travel along a road, both moving at constant velocities. See Figure 1. At some particular moment, they are separated by 200 metres. The car in front is travelling at 22 metres per second and the car behind is travelling at 30 metres per second. If we want to find out how long it will take the second car to catch up with the first, there are three obvious "frames of reference" that we could choose.
First, we could observe the two cars from the side of the road. We define our "frame of reference" "S" as follows. We stand on the side of the road and start a stop-clock at the exact moment that the second car passes us, which happens to be when they are a distance "d" = 200 m apart. Since neither of the cars is accelerating, we can determine their positions by the following formulas, where formula_12 is the position in meters of car one after time "t" in seconds and formula_13 is the position of car two after time "t".
Notice that these formulas predict at "t" = 0 s the first car is 200 m down the road and the second car is right beside us, as expected. We want to find the time at which formula_15. Therefore we set formula_15 and solve for formula_17, that is:
Alternatively, we could choose a frame of reference "S′" situated in the first car. In this case, the first car is stationary and the second car is approaching from behind at a speed of . In order to catch up to the first car, it will take a time of , that is, 25 seconds, as before. Note how much easier the problem becomes by choosing a suitable frame of reference. The third possible frame of reference would be attached to the second car. That example resembles the case just discussed, except the second car is stationary and the first car moves backward towards it at 8 m / s.
It would have been possible to choose a rotating, accelerating frame of reference, moving in a complicated manner, but this would have served to complicate the problem unnecessarily. It is also necessary to note that one is able to convert measurements made in one coordinate system to another. For example, suppose that your watch is running five minutes fast compared to the local standard time. If you know that this is the case, when somebody asks you what time it is, you are able to deduct five minutes from the time displayed on your watch in order to obtain the correct time. The measurements that an observer makes about a system depend therefore on the observer's frame of reference (you might say that the bus arrived at 5 past three, when in fact it arrived at three).
Additional example.
For a simple example involving only the orientation of two observers, consider two people standing, facing each other on either side of a north-south street. See Figure 2. A car drives past them heading south. For the person facing east, the car was moving toward the right. However, for the person facing west, the car was moving toward the left. This discrepancy is because the two people used two different frames of reference from which to investigate this system.
For a more complex example involving observers in relative motion, consider Alfred, who is standing on the side of a road watching a car drive past him from left to right. In his frame of reference, Alfred defines the spot where he is standing as the origin, the road as the x-axis and the direction in front of him as the positive y-axis. To him, the car moves along the "x" axis with some velocity "v" in the positive x-direction. Alfred's frame of reference is considered an inertial frame of reference because he is not accelerating (ignoring effects such as Earth's rotation and gravity).
Now consider Betsy, the person driving the car. Betsy, in choosing her frame of reference, defines her location as the origin, the direction to her right as the positive "x"-axis, and the direction in front of her as the positive "y"-axis. In this frame of reference, it is Betsy who is stationary and the world around her that is moving – for instance, as she drives past Alfred, she observes him moving with velocity "v" in the negative "y"-direction. If she is driving north, then north is the positive "y"-direction; if she turns east, east becomes the positive "y"-direction.
Finally, as an example of non-inertial observers, assume Candace is accelerating her car. As she passes by him, Alfred measures her acceleration and finds it to be "a" in the negative x-direction. Assuming Candace's acceleration is constant, what acceleration does Betsy measure? If Betsy's velocity "v" is constant, she is in an inertial frame of reference, and she will find the acceleration to be the same as Alfred in her frame of reference, "a" in the negative "y"-direction. However, if she is accelerating at rate "A" in the negative "y"-direction (in other words, slowing down), she will find Candace's acceleration to be "a′" = "a" − "A" in the negative "y"-direction - a smaller value than Alfred has measured. Similarly, if she is accelerating at rate "A" in the positive y-direction (speeding up), she will observe Candace's acceleration as "a′" = "a" + "A" in the negative "y"-direction – a larger value than Alfred's measurement.
Frames of reference are especially important in special relativity, because when a frame of reference is moving at some significant fraction of the speed of light, then the flow of time in that frame does not necessarily apply in another frame. The speed of light is considered to be the only true constant between moving frames of reference.
Remarks.
It is important to note some assumptions made above about the various inertial frames of reference. Newton, for instance, employed universal time, as explained by the following example. Suppose that you own two clocks, which both tick at exactly the same rate. You synchronize them so that they both display exactly the same time. The two clocks are now separated and one clock is on a fast moving train, traveling at constant velocity towards the other. According to Newton, these two clocks will still tick at the same rate and will both show the same time. Newton says that the rate of time as measured in one frame of reference should be the same as the rate of time in another. That is, there exists a "universal" time and all other times in all other frames of reference will run at the same rate as this universal time irrespective of their position and velocity. This concept of time and simultaneity was later generalized by Einstein in his special theory of relativity (1905) where he developed transformations between inertial frames of reference based upon the universal nature of physical laws and their economy of expression (Lorentz transformations).
It is also important to note that the definition of inertial reference frame can be extended beyond three-dimensional Euclidean space. Newton's assumed a Euclidean space, but general relativity uses a more general geometry. As an example of why this is important, let us consider the geometry of an ellipsoid. In this geometry, a "free" particle is defined as one at rest or traveling at constant speed on a geodesic path. Two free particles may begin at the same point on the surface, traveling with the same constant speed in different directions. After a length of time, the two particles collide at the opposite side of the ellipsoid. Both "free" particles traveled with a constant speed, satisfying the definition that no forces were acting. No acceleration occurred and so Newton's first law held true. This means that the particles were in inertial frames of reference. Since no forces were acting, it was the geometry of the situation which caused the two particles to meet each other again. In a similar way, it is now common to describe that we exist in a four-dimensional geometry known as spacetime. In this picture, the curvature of this 4D space is responsible for the way in which two bodies with mass are drawn together even if no forces are acting. This curvature of spacetime replaces the force known as gravity in Newtonian mechanics and special relativity.
Non-inertial frames.
Here the relation between inertial and non-inertial observational frames of reference is considered. The basic difference between these frames is the need in non-inertial frames for fictitious forces, as described below.
An accelerated frame of reference is often delineated as being the "primed" frame, and all variables that are dependent on that frame are notated with primes, e.g. "x′", "y′", "a′".
The vector from the origin of an inertial reference frame to the origin of an accelerated reference frame is commonly notated as R. Given a point of interest that exists in both frames, the vector from the inertial origin to the point is called r, and the vector from the accelerated origin to the point is called r′.
From the geometry of the situation, we get
Taking the first and second derivatives of this with respect to time, we obtain
where V and A are the velocity and acceleration of the accelerated system with respect to the inertial system and v and a are the velocity and acceleration of the point of interest with respect to the inertial frame.
These equations allow transformations between the two coordinate systems; for example, we can now write Newton's second law as
When there is accelerated motion due to a force being exerted there is manifestation of inertia. If an electric car designed to recharge its battery system when decelerating is switched to braking, the batteries are recharged, illustrating the physical strength of manifestation of inertia. However, the manifestation of inertia does not prevent acceleration (or deceleration), for manifestation of inertia occurs in response to change in velocity due to a force. Seen from the perspective of a rotating frame of reference the manifestation of inertia appears to exert a force (either in centrifugal direction, or in a direction orthogonal to an object's motion, the Coriolis effect).
A common sort of accelerated reference frame is a frame that is both rotating and translating (an example is a frame of reference attached to a CD which is playing while the player is carried). This arrangement leads to the equation (see Fictitious force for a derivation):
or, to solve for the acceleration in the accelerated frame,
Multiplying through by the mass "m" gives
where

</doc>
<doc id="10779" url="https://en.wikipedia.org/wiki?curid=10779" title="Frequency">
Frequency

Frequency is the number of occurrences of a repeating event per unit time. It is also referred to as temporal frequency, which emphasizes the contrast to spatial frequency and angular frequency. The period is the duration of time of one cycle in a repeating event, so the period is the reciprocal of the frequency. For example, if a newborn baby's heart beats at a frequency of 120 times a minute, its period – the interval between beats – is half a second (60 seconds (i.e., a minute) divided by 120 beats). Frequency is an important parameter used in science and engineering to specify the rate of oscillatory and vibratory phenomena, such as mechanical vibrations, audio (sound) signals, radio waves, and light.
Definitions.
For cyclical processes, such as rotation, oscillations, or waves, frequency is defined as a number of cycles per unit time. In physics and engineering disciplines, such as optics, acoustics, and radio, frequency is usually denoted by a Latin letter "f" or by the Greek letter "formula_1" or "ν" (nu) (see e.g. Planck's formula).
The period, usually denoted by "T", is the duration of one cycle, and is the reciprocal of the frequency "f":
Units.
The SI unit of frequency is the hertz (Hz), named after the German physicist Heinrich Hertz; one hertz means that an event repeats once per second. A previous name for this unit was cycles per second (cps). The SI unit for period is the second.
A traditional unit of measure used with rotating mechanical devices is revolutions per minute, abbreviated r/min or rpm. 60 rpm equals one hertz.
Period versus frequency.
As a matter of convenience, longer and slower waves, such as ocean surface waves, tend to be described by wave period rather than frequency. Short and fast waves, like audio and radio, are usually described by their frequency instead of period. These commonly used conversions are listed below:
In wave propagation.
For periodic waves in nondispersive media (that is, media in which the wave speed is independent of frequency), frequency has an inverse relationship to the wavelength, "λ" (lambda). Even in dispersive media, the frequency "f" of a sinusoidal wave is equal to the phase velocity "v" of the wave divided by the wavelength "λ" of the wave:
In the special case of electromagnetic waves moving through a vacuum, then "v = c", where "c" is the speed of light in a vacuum, and this expression becomes:
When waves from a monochrome source travel from one medium to another, their frequency remains the same—only their wavelength and speed change.
Measurement.
By counting.
Calculating the frequency of a repeating event is accomplished by counting the number of times that event occurs within a specific time period, then dividing the count by the length of the time period. For example, if 71 events occur within 15 seconds the frequency is:
If the number of counts is not very large, it is more accurate to measure the time interval for a predetermined number of occurrences, rather than the number of occurrences within a specified time. The latter method introduces a random error into the count of between zero and one count, so on average half a count. This is called "gating error" and causes an average error in the calculated frequency of "Δf" = 1/(2 "Tm"), or a fractional error of "Δf" / "f" = 1/(2 "f Tm") where Tm is the timing interval and f is the measured frequency. This error decreases with frequency, so it is a problem at low frequencies where the number of counts N is small.
By stroboscope.
An older method of measuring the frequency of rotating or vibrating objects is to use a stroboscope. This is an intense repetitively flashing light (strobe light) whose frequency can be adjusted with a calibrated timing circuit. The strobe light is pointed at the rotating object and the frequency adjusted up and down. When the frequency of the strobe equals the frequency of the rotating or vibrating object, the object completes one cycle of oscillation and returns to its original position between the flashes of light, so when illuminated by the strobe the object appears stationary. Then the frequency can be read from the calibrated readout on the stroboscope. A downside of this method is that an object rotating at an integer multiple of the strobing frequency will also appear stationary.
By frequency counter.
Higher frequencies are usually measured with a frequency counter. This is an electronic instrument which measures the frequency of an applied repetitive electronic signal and displays the result in hertz on a digital display. It uses digital logic to count the number of cycles during a time interval established by a precision quartz time base. Cyclic processes that are not electrical in nature, such as the rotation rate of a shaft, mechanical vibrations, or sound waves, can be converted to a repetitive electronic signal by transducers and the signal applied to a frequency counter. Frequency counters can currently cover the range up to about 100 GHz. This represents the limit of direct counting methods; frequencies above this must be measured by indirect methods.
Heterodyne methods.
Above the range of frequency counters, frequencies of electromagnetic signals are often measured indirectly by means of heterodyning (frequency conversion). A reference signal of a known frequency near the unknown frequency is mixed with the unknown frequency in a nonlinear mixing device such as a diode. This creates a heterodyne or "beat" signal at the difference between the two frequencies. If the two signals are close together in frequency the heterodyne is low enough to be measured by a frequency counter. This process only measures the difference between the unknown frequency and the reference frequency, which must be determined by some other method. To reach higher frequencies, several stages of heterodyning can be used. Current research is extending this method to infrared and light frequencies (optical heterodyne detection).
Examples.
Light.
Visible light is an electromagnetic wave, consisting of oscillating electric and magnetic fields traveling through space. The frequency of the wave determines its color: is red light, is violet light, and between these (in the range 4-) are all the other colors of the rainbow. An electromagnetic wave can have a frequency less than , but it will be invisible to the human eye; such waves are called infrared (IR) radiation. At even lower frequency, the wave is called a microwave, and at still lower frequencies it is called a radio wave. Likewise, an electromagnetic wave can have a frequency higher than , but it will be invisible to the human eye; such waves are called ultraviolet (UV) radiation. Even higher-frequency waves are called X-rays, and higher still are gamma rays.
All of these waves, from the lowest-frequency radio waves to the highest-frequency gamma rays, are fundamentally the same, and they are all called electromagnetic radiation. They all travel through a vacuum at the same speed (the speed of light), giving them wavelengths inversely proportional to their frequencies.
where "c" is the speed of light ("c" in a vacuum, or less in other media), "f" is the frequency and λ is the wavelength.
In dispersive media, such as glass, the speed depends somewhat on frequency, so the wavelength is not quite inversely proportional to frequency.
Sound.
Sound propagates as mechanical vibration waves of pressure and displacement, in air or other substances. Frequency is the property of sound that most determines pitch.
The frequencies an ear can hear are limited to a specific range of frequencies. The audible frequency range for humans is typically given as being between about 20 Hz and 20,000 Hz (20 kHz), though the high frequency limit usually reduces with age. Other species have different hearing ranges. For example, some dog breeds can perceive vibrations up to 60,000 Hz.
In many media, such as air, the speed of sound is approximately independent of frequency, so the wavelength of the sound waves (distance between repetitions) is approximately inversely proportional to frequency.
Line current.
In Europe, Africa, Australia, Southern South America, most of Asia, and Russia, the frequency of the alternating current in household electrical outlets is 50 Hz (close to the tone G), whereas in North America and Northern South America, the frequency of the alternating current in household electrical outlets is 60 Hz (between the tones B♭ and B; that is, a minor third above the European frequency). The frequency of the 'hum' in an audio recording can show where the recording was made, in countries using a European, or an American, grid frequency.

</doc>
<doc id="185427" url="https://en.wikipedia.org/wiki?curid=185427" title="Function (mathematics)">
Function (mathematics)

In mathematics, a function is a relation between a set of inputs and a set of permissible outputs with the property that each input is related to exactly one output. An example is the function that relates each real number "x" to its square "x"2. The output of a function "f" corresponding to an input "x" is denoted by "f"("x") (read ""f" of "x""). In this example, if the input is −3, then the output is 9, and we may write "f"(−3) = 9. Likewise, if the input is 3, then the output is also 9, and we may write "f"(3) = 9. (The same output may be produced by more than one input, but each input gives only one output.) The input variable(s) are sometimes referred to as the argument(s) of the function.
Functions of various kinds are "the central objects of investigation" in most fields of modern mathematics. There are many ways to describe or represent a function. Some functions may be defined by a formula or algorithm that tells how to compute the output for a given input. Others are given by a picture, called the graph of the function. In science, functions are sometimes defined by a table that gives the outputs for selected inputs. A function could be described implicitly, for example as the inverse to another function or as a solution of a differential equation.
The input and output of a function can be expressed as an ordered pair, ordered so that the first element is the input (or tuple of inputs, if the function takes more than one input), and the second is the output. In the example above, "f"("x") = "x"2, we have the ordered pair (−3, 9). If both input and output are real numbers, this ordered pair can be viewed as the Cartesian coordinates of a point on the graph of the function.
In modern mathematics, a function is defined by its set of inputs, called the "domain"; a set containing the set of outputs, and possibly additional elements, as members, called its "codomain"; and the set of all input-output pairs, called its "graph". Sometimes the codomain is called the function's "range", but more commonly the word "range" is used to mean, instead, specifically the set of outputs (this is also called the "image" of the function). For example, we could define a function using the rule "f"("x") = "x"2 by saying that the domain and codomain are the real numbers, and that the graph consists of all pairs of real numbers ("x", "x"2). The image of this function is the set of non-negative real numbers. Collections of functions with the same domain and the same codomain are called function spaces, the properties of which are studied in such mathematical disciplines as real analysis, complex analysis, and functional analysis.
In analogy with arithmetic, it is possible to define addition, subtraction, multiplication, and division of functions, in those cases where the output is a number. Another important operation defined on functions is function composition, where the output from one function becomes the input to another function.
Introduction and examples.
For an example of a function, let "X" be the set consisting of four shapes: a red triangle, a yellow rectangle, a green hexagon, and a red square; and let "Y" be the set consisting of five colors: red, blue, green, pink, and yellow. Linking each shape to its color is a function from "X" to "Y": each shape is linked to a color (i.e., an element in "Y"), and each shape is "linked", or "mapped", to exactly one color. There is no shape that lacks a color and no shape that has two or more colors. This function will be referred to as the "color-of-the-shape function".
The input to a function is called the argument and the output is called the value. The set of all permitted inputs to a given function is called the domain of the function, while the set of permissible outputs is called the codomain. Thus, the domain of the "color-of-the-shape function" is the set of the four shapes, and the codomain consists of the five colors. The concept of a function does "not" require that every possible output is the value of some argument, e.g. the color blue is not the color of any of the four shapes in "X".
A second example of a function is the following: the domain is chosen to be the set of natural numbers (1, 2, 3, 4, ...), and the codomain is the set of integers (..., −3, −2, −1, 0, 1, 2, 3, ...). The function associates to any natural number "n" the number 4−"n". For example, to 1 it associates 3 and to 10 it associates −6.
A third example of a function has the set of polygons as domain and the set of natural numbers as codomain. The function associates a polygon with its number of vertices. For example, a triangle is associated with the number 3, a square with the number 4, and so on.
The term range is sometimes used either for the codomain or for the set of all the actual values a function has.
Definition.
In order to avoid the use of the informally defined concepts of "rules" and "associates", the above intuitive explanation of functions is completed with a formal definition. This definition relies on the notion of the Cartesian product. The Cartesian product of two sets "X" and "Y" is the set of all ordered pairs, written ("x", "y"), where "x" is an element of "X" and "y" is an element of "Y". The "x" and the "y" are called the components of the ordered pair. The Cartesian product of "X" and "Y" is denoted by "X" × "Y".
A function "f" from "X" to "Y" is a subset of the Cartesian product "X" × "Y" subject to the following condition: every element of "X" is the first component of one and only one ordered pair in the subset. In other words, for every "x" in "X" there is exactly one element "y" such that the ordered pair ("x", "y") is contained in the subset defining the function "f". This formal definition is a precise rendition of the idea that to each "x" is associated an element "y" of "Y", namely the uniquely specified element "y" with the property just mentioned.
Considering the "color-of-the-shape" function above, the set "X" is the domain consisting of the four shapes, while "Y" is the codomain consisting of five colors. There are twenty possible ordered pairs (four shapes times five colors), one of which is
The "color-of-the-shape" function described above consists of the set of those ordered pairs,
where the color is the actual color of the given shape. Thus, the pair ("red triangle", "red") is in the function, but the pair ("yellow rectangle", "red") is not.
Notation.
A function "f" with domain "X" and codomain "Y" is commonly denoted by
or
In this context, the elements of "X" are called arguments of "f". For each argument "x", the corresponding unique "y" in the codomain is called the function value at "x" or the "image" of "x" under "f". It is written as "f"("x"). One says that "f" associates "y" with "x" or maps "x" to "y". This is abbreviated by
A general function is often denoted by "f". Special functions have names, for example, the signum function is denoted by sgn. Given a real number "x", its image under the signum function is then written as sgn("x"). Here, the argument is denoted by the symbol "x", but different symbols may be used in other contexts. For example, in physics, the velocity of some body, depending on the time, is denoted "v"("t"). The parentheses around the argument may be omitted when there is little chance of confusion, thus: ; this is known as prefix notation.
In order to denote a specific function, the notation formula_4 (an arrow with a bar at its tail) is used. For example, the above function reads
The first part can be read as:
The second part is read:
In other words, this function has the natural numbers as domain, the integers as codomain. Strictly speaking, a function is properly defined only when the domain and codomain are specified. For example, the formula "f"("x") = 4 − "x" alone (without specifying the codomain and domain) is not a properly defined function. Moreover, the function
(with different domain) is not considered the same function, even though the formulas defining "f" and "g" agree, and similarly with a different codomain. Despite that, many authors drop the specification of the domain and codomain, especially if these are clear from the context. So in this example many just write "f"("x") = 4 − "x". Sometimes, the maximal possible domain is also understood implicitly: a formula such as formula_7 may mean that the domain of "f" is the set of real numbers "x" where the square root is defined (in this case "x" ≤ 2 or "x" ≥ 3).
To define a function, sometimes a dot notation is used in order to emphasize the functional nature of an expression without assigning a special symbol to the variable. For instance, formula_8 stands for the function formula_9, formula_10 stands for the integral function formula_11, and so on.
Specifying a function.
A function can be defined by any mathematical condition relating each argument (input value) to the corresponding output value. If the domain is finite, a function "f" may be defined by simply tabulating all the arguments "x" and their corresponding function values "f"("x"). More commonly, a function is defined by a formula, or (more generally) an algorithm — a recipe that tells how to compute the value of "f"("x") given any "x" in the domain.
There are many other ways of defining functions. Examples include piecewise definitions, induction or recursion, algebraic or analytic closure, limits, analytic continuation, infinite series, and as solutions to integral and differential equations. The lambda calculus provides a powerful and flexible syntax for defining and combining functions of several variables. In advanced mathematics, some functions exist because of an axiom, such as the Axiom of Choice.
Graph.
The "graph" of a function is its set of ordered pairs "F". This is an abstraction of the idea of a graph as a picture showing the function plotted on a pair of coordinate axes; for example, , the point above 3 on the horizontal axis and to the right of 9 on the vertical axis, lies on the graph of
Formulas and algorithms.
Different formulas or algorithms may describe the same function. For instance is exactly the same function as .
Furthermore, a function need not be described by a formula, expression, or algorithm, nor need it deal with numbers at all: the domain and codomain of a function may be arbitrary sets. One example of a function that acts on non-numeric inputs takes English words as inputs and returns the first letter of the input word as output.
As an example, the factorial function is defined on the nonnegative integers and produces a nonnegative integer. It is defined by the following inductive algorithm: 0! is defined to be 1, and "n"! is defined to be "n"("n" − 1)! for all positive integers "n". The factorial function is denoted with the exclamation mark (serving as the symbol of the function) after the variable (postfix notation).
Computability.
Functions that send integers to integers, or finite strings to finite strings, can sometimes be defined by an algorithm, which gives a precise description of a set of steps for computing the output of the function from its input. Functions definable by an algorithm are called "computable functions". For example, the Euclidean algorithm gives a precise process to compute the greatest common divisor of two positive integers. Many of the functions studied in the context of number theory are computable.
Fundamental results of computability theory show that there are functions that can be precisely defined but are not computable. Moreover, in the sense of cardinality, almost all functions from the integers to integers are not computable. The number of computable functions from integers to integers is countable, because the number of possible algorithms is. The number of all functions from integers to integers is higher: the same as the cardinality of the real numbers. Thus most functions from integers to integers are not computable. Specific examples of uncomputable functions are known, including the busy beaver function and functions related to the halting problem and other undecidable problems.
Basic properties.
There are a number of general basic properties and notions. In this section, "f" is a function with domain "X" and codomain "Y".
Image and preimage.
If "A" is any subset of the domain "X", then "f"("A") is the subset of the codomain "Y" consisting of all images of elements of A. We say the "f"("A") is the "image" of A under f. The "image" of "f" is given by "f"("X"). On the other hand, the "inverse image" (or "preimage", "complete inverse image") of a subset "B" of the codomain "Y" under a function "f" is the subset of the domain "X" defined by
So, for example, the preimage of {4, 9} under the squaring function is the set {−3,−2,2,3}. The term range usually refers to the image, but sometimes it refers to the codomain.
By definition of a function, the image of an element "x" of the domain is always a single element "y" of the codomain. Conversely, though, the preimage of a singleton set (a set with exactly one element) may in general contain any number of elements. For example, if "f"("x") = 7 (the constant function taking value 7), then the preimage of {5} is the empty set but the preimage of {7} is the entire domain. It is customary to write "f"−1("b") instead of "f"−1({"b"}), i.e.
This set is sometimes called the fiber of "b" under "f".
Use of "f"("A") to denote the image of a subset "A" ⊆ "X" is consistent so long as no subset of the domain is also an element of the domain. In some fields (e.g., in set theory, where ordinals are also sets of ordinals) it is convenient or even necessary to distinguish the two concepts; the customary notation is "f"["A"] for the set { "f"("x"): x ∈ "A" }. Likewise, some authors use square brackets to avoid confusion between the inverse image and the inverse function. Thus they would write "f"−1["B"] and "f"−1["b"] for the preimage of a set and a singleton.
Injective and surjective functions.
A function is called "injective" (or "one-to-one", or an injection) if "f"("a") ≠ "f"("b") for any two "different" elements "a" and "b" of the domain. It is called surjective (or "onto") if "f"("X") = "Y". That is, it is surjective if for every element "y" in the codomain there is an "x" in the domain such that "f"("x") = "y". Finally "f" is called "bijective" if it is both injective and surjective. This nomenclature was introduced by the Bourbaki group.
The above "color-of-the-shape" function is not injective, since two distinct shapes (the red triangle and the red rectangle) are assigned the same value. Moreover, it is not surjective, since the image of the function contains only three, but not all five colors in the codomain.
Function composition.
The "function composition" of two functions takes the output of one function as the input of a second one. More specifically, the composition of "f" with a function "g": "Y" → "Z" is the function formula_14 defined by
That is, the value of "x" is obtained by first applying "f" to "x" to obtain "y" = "f"("x") and then applying "g" to "y" to obtain "z" = "g"("y"). In the notation formula_16, the function on the right, "f", acts first and the function on the left, "g" acts second, reversing English reading order. The notation can be memorized by reading the notation as ""g" of "f" or "g" after "f"". The composition formula_16 is only defined when the codomain of "f" is the domain of "g". Assuming that, the composition in the opposite order formula_18 need not be defined. Even if it is, i.e., if the codomain of "f" is the codomain of "g", it is "not" in general true that
That is, the order of the composition is important. For example, suppose "f"("x") = "x"2 and "g"("x") = "x"+1. Then "g"("f"("x")) = "x"2+1, while "f"("g"("x")) = ("x"+1)2, which is "x"2+2"x"+1, a different function.
Identity function.
The unique function over a set "X" that maps each element to itself is called the "identity function" for "X", and typically denoted by id"X". Each set has its own identity function, so the subscript cannot be omitted unless the set can be inferred from context. Under composition, an identity function is "neutral": if "f" is any function from "X" to "Y", then
Restrictions and extensions.
Informally, a "restriction" of a function "f" is the result of trimming its domain. More precisely, if "S" is any subset of "X", the restriction of "f" to "S" is the function "f"|"S" from "S" to "Y" such that "f"|"S"("s") = "f"("s") for all "s" in "S". If "g" is a restriction of "f", then it is said that "f" is an "extension" of "g".
The "overriding" of "f": "X" → "Y" by "g": "W" → "Y" (also called "overriding union") is an extension of "g" denoted as ("f" ⊕ "g"): ("X" ∪ "W") → Y. Its graph is the set-theoretical union of the graphs of "g" and "f"|"X" \ "W". Thus, it relates any element of the domain of "g" to its image under "g", and any other element of the domain of "f" to its image under "f". Overriding is an associative operation; it has the empty function as an identity element. If "f"|"X" ∩ "W" and "g"|"X" ∩ "W" are pointwise equal (e.g., the domains of "f" and "g" are disjoint), then the union of "f" and "g" is defined and is equal to their overriding union. This definition agrees with the definition of union for binary relations.
Inverse function.
An "inverse function" for "f", denoted by "f"−1, is a function in the opposite direction, from "Y" to "X", satisfying
That is, the two possible compositions of "f" and "f"−1 need to be the respective identity maps of "X" and "Y".
As a simple example, if "f" converts a temperature in degrees Celsius "C" to degrees Fahrenheit "F", the function converting degrees Fahrenheit to degrees Celsius would be a suitable "f"−1.
Such an inverse function exists if and only if "f" is bijective. In this case, "f" is called invertible. The notation formula_23 (or, in some texts, just formula_24) and "f"−1 are akin to multiplication and reciprocal notation. With this analogy, identity functions are like the multiplicative identity, 1, and inverse functions are like reciprocals (hence the notation).
Types of functions.
Real-valued functions.
A real-valued function "f" is one whose codomain is the set of real numbers or a subset thereof. If, in addition, the domain is also a subset of the reals, "f" is a real valued function of a real variable. The study of such functions is called real analysis.
Real-valued functions enjoy so-called pointwise operations. That is, given two functions
where "Y" is a subset of the reals (and "X" is an arbitrary set), their (pointwise) sum "f"+"g" and product "f" ⋅ "g" are functions with the same domain and codomain. They are defined by the formulas:
In a similar vein, complex analysis studies functions whose domain and codomain are both the set of complex numbers. In most situations, the domain and codomain are understood from context, and only the relationship between the input and output is given, but if formula_26, then in real variables the domain is limited to non-negative numbers.
The following table contains a few particularly important types of real-valued functions:
Further types of functions.
There are many other special classes of functions that are important to particular branches of mathematics, or particular applications.
Here is a partial list:
Function spaces.
The set of all functions from a set "X" to a set "Y" is denoted by "X" → "Y", by ["X" → "Y"], or by "Y""X". The latter notation is motivated by the fact that, when "X" and "Y" are finite and of size |"X"| and |"Y"|, then the number of functions "X" → "Y" is |"Y""X"| = |"Y"||"X"|. This is an example of the convention from enumerative combinatorics that provides notations for sets based on their cardinalities. If "X" is infinite and there is more than one element in "Y" then there are uncountably many functions from "X" to "Y", though only countably many of them can be expressed with a formula or algorithm.
Currying.
An alternative approach to handling functions with multiple arguments is to transform them into a chain of functions that each takes a single argument. For instance, one can interpret Add(3,5) to mean "first produce a function that adds 3 to its argument, and then apply the 'Add 3' function to 5". This transformation is called currying: Add 3 is curry(Add) applied to 3. There is a bijection between the function spaces "C""A"×"B" and ("C""B")"A".
When working with curried functions it is customary to use prefix notation with function application considered left-associative, since juxtaposition of multiple arguments—as in ("f" "x" "y")—naturally maps to evaluation of a curried function. Conversely, the → and ⟼ symbols are considered to be right-associative, so that curried functions may be defined by a notation such as "f": ℤ → ℤ → ℤ = "x" ⟼ "y" ⟼ "x"·"y".
Variants and generalizations.
Alternative definition of a function.
The above definition of "a function from "X" to "Y"" is generally agreed on, however there are two different ways a "function" is normally defined where the domain "X" and codomain "Y" are not explicitly or implicitly specified. Usually this is not a problem as the domain and codomain normally will be known. With one definition saying the function defined by on the reals does not completely specify a function as the codomain is not specified, and in the other it is a valid definition.
In the other definition a function is defined as a set of ordered pairs where each first element only occurs once. The domain is the set of all the first elements of a pair and there is no explicit codomain separate from the image. Concepts like surjective have to be refined for such functions, more specifically by saying that a (given) function is "surjective on a (given) set" if its image equals that set. For example, we might say a function "f" is surjective on the set of real numbers.
If a function is defined as a set of ordered pairs with no specific codomain, then indicates that "f" is a function whose domain is "X" and whose image is a subset of "Y". This is the case in the ISO standard. "Y" may be referred to as the codomain but then any set including the image of "f" is a valid codomain of "f". This is also referred to by saying that ""f" maps "X" into "Y"" In some usages "X" and "Y" may subset the ordered pairs, e.g. the function "f" on the real numbers such that "y"="x"2 when used as in means the function defined only on the interval [0,2]. With the definition of a function as an ordered triple this would always be considered a partial function.
An alternative definition of the composite function "g"("f"("x")) defines it for the set of all "x" in the domain of "f" such that "f(x)" is in the domain of "g". Thus the real square root of −"x"2 is a function only defined at 0 where it has the value 0.
Functions are commonly defined as a type of relation. A relation from "X" to "Y" is a set of ordered pairs with x ∈ X and y ∈ Y. A function from "X" to "Y" can be described as a relation from "X" to "Y" that is left-total and right-unique. However when "X" and "Y" are not specified there is a disagreement about the definition of a relation that parallels that for functions. Normally a relation is just defined as a set of ordered pairs and a correspondence is defined as a triple , however the distinction between the two is often blurred or a relation is never referred to without specifying the two sets. The definition of a function as a triple defines a function as a type of correspondence, whereas the definition of a function as a set of ordered pairs defines a function as a type of relation.
Many operations in set theory, such as the power set, have the class of all sets as their domain, and therefore, although they are informally described as functions, they do not fit the set-theoretical definition outlined above, because a class is not necessarily a set. However some definitions of relations and functions define them as classes of pairs rather than sets of pairs and therefore do include the power set as a function.
Partial and multi-valued functions.
In some parts of mathematics, including recursion theory and functional analysis, it is convenient to study "partial functions" in which some values of the domain have no association in the graph; i.e., single-valued relations. For example, the function "f" such that "f"("x") = 1/"x" does not define a value for "x" = 0, since division by zero is not defined. Hence "f" is only a partial function from the real line to the real line. The term total function can be used to stress the fact that every element of the domain does appear as the first element of an ordered pair in the graph.
In other parts of mathematics, non-single-valued relations are similarly conflated with functions: these are called "multivalued functions", with the corresponding term single-valued function for ordinary functions.
Functions with multiple inputs and outputs.
The concept of function can be extended to an object that takes a combination of two (or more) argument values to a single result. This intuitive concept is formalized by a function whose domain is the Cartesian product of two or more sets.
For example, consider the function that associates two integers to their product: "f"("x", "y") = "x"·"y". This function can be defined formally as having domain ℤ×ℤ, the set of all integer pairs; codomain ℤ; and, for graph, the set of all pairs (("x","y"), "x"·"y"). Note that the first component of any such pair is itself a pair (of integers), while the second component is a single integer.
The function value of the pair ("x","y") is "f"(("x","y")). However, it is customary to drop one set of parentheses and consider "f"("x","y") a function of two variables, "x" and "y". Functions of two variables may be plotted on the three-dimensional Cartesian as ordered triples of the form ("x","y","f"("x","y")).
The concept can still further be extended by considering a function that also produces output that is expressed as several variables. For example, consider the integer divide function, with domain ℤ×ℕ and codomain ℤ×ℕ. The resultant (quotient, remainder) pair is a single value in the codomain seen as a Cartesian product.
Binary operations.
The familiar binary operations of arithmetic, addition and multiplication, can be viewed as functions from ℝ×ℝ to ℝ. This view is generalized in abstract algebra, where "n"-ary functions are used to model the operations of arbitrary algebraic structures. For example, an abstract group is defined as a set "X" and a function "f" from "X"×"X" to "X" that satisfies certain properties.
Traditionally, addition and multiplication are written in the infix notation: "x"+"y" and "x"×"y" instead of +("x", "y") and ×("x", "y").
Functors.
The idea of structure-preserving functions, or homomorphisms, led to the abstract notion of morphism, the key concept of category theory. In fact, functions "f": "X" → "Y" are the morphisms in the category of sets, including the empty set: if the domain "X" is the empty set, then the subset of "X" × "Y" describing the function is necessarily empty, too. However, this is still a well-defined function. Such a function is called an empty function. In particular, the identity function of the empty set is defined, a requirement for sets to form a category.
The concept of categorification is an attempt to replace set-theoretic notions by category-theoretic ones. In particular, according to this idea, sets are replaced by categories, while functions between sets are replaced by functors.

</doc>
<doc id="11180" url="https://en.wikipedia.org/wiki?curid=11180" title="Functional analysis">
Functional analysis

Functional analysis is a branch of mathematical analysis, the core of which is formed by the study of vector spaces endowed with some kind of limit-related structure (e.g. inner product, norm, topology, etc.) and the linear operators acting upon these spaces and respecting these structures in a suitable sense. The historical roots of functional analysis lie in the study of spaces of functions and the formulation of properties of transformations of functions such as the Fourier transform as transformations defining continuous, unitary etc. operators between function spaces. This point of view turned out to be particularly useful for the study of differential and integral equations.
The usage of the word "functional" goes back to the calculus of variations, implying a function whose argument is a function and the name was first used in Hadamard's 1910 book on that subject. However, the general concept of a functional had previously been introduced in 1887 by the Italian mathematician and physicist Vito Volterra. The theory of nonlinear functionals was continued by students of Hadamard, in particular Fréchet and Lévy. Hadamard also founded the modern school of linear functional analysis further developed by Riesz and the group of Polish mathematicians around Stefan Banach.
In modern introductory texts to functional analysis, the subject is seen as the study of vector spaces endowed with a topology, in particular infinite-dimensional spaces. In contrast, linear algebra deals mostly with finite-dimensional spaces, and does not use topology. An important part of functional analysis is the extension of the theory of measure, integration, and probability to infinite dimensional spaces, also known as infinite dimensional analysis.
Normed vector spaces.
The basic and historically first class of spaces studied in functional analysis are complete normed vector spaces over the real or complex numbers. Such spaces are called Banach spaces. An important example is a Hilbert space, where the norm arises from an inner product. These spaces are of fundamental importance in many areas, including the mathematical formulation of quantum mechanics.
More generally, functional analysis includes the study of Fréchet spaces and other topological vector spaces not endowed with a norm.
An important object of study in functional analysis are the continuous linear operators defined on Banach and Hilbert spaces. These lead naturally to the definition of C*-algebras and other operator algebras.
Hilbert spaces.
Hilbert spaces can be completely classified: there is a unique Hilbert space up to isomorphism for every cardinality of the orthonormal basis. Finite-dimensional Hilbert spaces are fully understood in linear algebra, and infinite-dimensional separable Hilbert spaces are isomorphic to formula_1. Separability being important for applications, functional analysis of Hilbert spaces consequently mostly deals with this space. One of the open problems in functional analysis is to prove that every bounded linear operator on a Hilbert space has a proper invariant subspace. Many special cases of this invariant subspace problem have already been proven.
Banach spaces.
General Banach spaces are more complicated than Hilbert spaces, and cannot be classified in such a simple manner as those. In particular, many Banach spaces lack a notion analogous to an orthonormal basis.
Examples of Banach spaces are formula_2-spaces for any real number formula_3 . Given also a measure formula_4 on set formula_5, then formula_6, sometimes also denoted formula_7 or formula_8, has as its vectors equivalence classes formula_9 of measurable functions whose absolute value's formula_10-th power has finite integral, that is, functions formula_11 for which one has
If formula_4 is the counting measure, then the integral may be replaced by a sum. That is, we require
Then it is not necessary to deal with equivalence classes, and the space is denoted formula_15, written more simply formula_16 in the case when formula_5 is the set of non-negative integers.
In Banach spaces, a large part of the study involves the dual space: the space of all continuous linear maps from the space into its underlying field, so-called functionals. A Banach space can be canonically identified with a subspace of its bidual, which is the dual of its dual space. The corresponding map is an isometry but in general not onto. A general Banach space and its bidual need not even be isometrically isomorphic in any way, contrary to the finite-dimensional situation. This is explained in the dual space article.
Also, the notion of derivative can be extended to arbitrary functions between Banach spaces. See, for instance, the Fréchet derivative article.
Major and foundational results.
Important results of functional analysis include:
Uniform boundedness principle.
The uniform boundedness principle or Banach–Steinhaus theorem is one of the fundamental results in functional analysis. Together with the Hahn–Banach theorem and the open mapping theorem, it is considered one of the cornerstones of the field. In its basic form, it asserts that for a family of continuous linear operators (and thus bounded operators) whose domain is a Banach space, pointwise boundedness is equivalent to uniform boundedness in operator norm.
The theorem was first published in 1927 by Stefan Banach and Hugo Steinhaus but it was also proven independently by Hans Hahn.
Theorem (Uniform Boundedness Principle). Let "X" be a Banach space and "Y" be a normed vector space. Suppose that "F" is a collection of continuous linear operators from "X" to "Y". If for all "x" in "X" one has
then
Spectral theorem.
There are many theorems known as the spectral theorem, but one in particular has many applications in functional analysis. Let "A" be the operator of multiplication by "t" on "L"2[0, 1], that is
Theorem: Let "A" be a bounded self-adjoint operator on a Hilbert space "H". Then there is a measure space ("X", Σ, μ) and a real-valued essentially bounded measurable function "f" on "X" and a unitary operator "U":"H" → "L"2μ("X") such that
where "T" is the multiplication operator:
and formula_23
This is the beginning of the vast research area of functional analysis called operator theory; see also the spectral measure.
There is also an analogous spectral theorem for bounded normal operators on Hilbert spaces. The only difference in the conclusion is that now formula_24 may be complex-valued.
Hahn-Banach theorem.
The Hahn–Banach theorem is a central tool in functional analysis. It allows the extension of bounded linear functionals defined on a subspace of some vector space to the whole space, and it also shows that there are "enough" continuous linear functionals defined on every normed vector space to make the study of the dual space "interesting".
Hahn–Banach theorem: If is a sublinear function, and is a linear functional on a linear subspace which is dominated by on , i.e.
then there exists a linear extension of to the whole space , i.e., there exists a linear functional such that
Open mapping theorem.
The open mapping theorem, also known as the Banach–Schauder theorem (named after Stefan Banach and Juliusz Schauder), is a fundamental result which states that if a continuous linear operator between Banach spaces is surjective then it is an open map. More precisely,:
The proof uses the Baire category theorem, and completeness of both "X" and "Y" is essential to the theorem. The statement of the theorem is no longer true if either space is just assumed to be a normed space, but is true if "X" and "Y" are taken to be Fréchet spaces.
Closed graph theorem.
The closed graph theorem states the following:
If "X" is a topological space and "Y" is a compact Hausdorff space, then the graph of a linear map "T" from "X" to "Y" is closed if and only if "T" is continuous.
Other topics.
List of functional analysis topics.
Foundations of mathematics considerations.
Most spaces considered in functional analysis have infinite dimension. To show the existence of a vector space basis for such spaces may require Zorn's lemma. However, a somewhat different concept, Schauder basis, is usually more relevant in functional analysis. Many very important theorems require the Hahn–Banach theorem, usually proved using axiom of choice, although the strictly weaker Boolean prime ideal theorem suffices. The Baire category theorem, needed to prove many important theorems, also requires a form of axiom of choice.
Points of view.
Functional analysis in its includes the following tendencies:

</doc>
<doc id="301504" url="https://en.wikipedia.org/wiki?curid=301504" title="Functional derivative">
Functional derivative

In the calculus of variations, a field of mathematical analysis, the functional derivative (or variational derivative) relates a change in a functional to a change in a function that the functional depends on.
In the calculus of variations, functionals are usually expressed in terms of an integral of functions, their arguments, and their derivatives. In an integrand of a functional, if a function is varied by adding to it another function that is arbitrarily small, and the resulting integrand is expanded in powers of , the coefficient of in the first order term is called the functional derivative.
For example, consider the functional
where . If is varied by adding to it a function , and the resulting integrand is expanded in powers of , then the change in the value of to first order in can be expressed as follows:
The coefficient of , denoted as , is called the functional derivative of with respect to at the point . For this example functional, the functional derivative is the left hand side of the Euler-Lagrange equation,
Definition.
In this section, the functional derivative is defined. Then the functional differential is defined in terms of the functional derivative.
Functional derivative.
Given a manifold representing (continuous/smooth/with certain boundary conditions/etc.) functions and a functional defined as 
the functional derivative of "ρ"], denoted , is defined by
where is an arbitrary function. The quantity is called the variation of . In other words,
is a linear functional, so by the Riesz–Markov–Kakutani representation theorem, this functional is given by integration against some measure.
Then is defined to be the Radon–Nikodym derivative of this measure.
We think of the function as the gradient of at the point and 
as the directional derivative at point in the direction of . Then analogous to vector calculus, the inner product with the gradient gives the directional derivative.
Functional differential.
The differential (or variation or first variation) of the functional ["ρ"] is, 
Heuristically, is the change in , so we 'formally' have , and then
this is similar in form to the total differential of a function ,
where are independent variables. 
Comparing the last two equations, the functional derivative has a role similar to that of the partial derivative , where the variable of integration is like a continuous version of the summation index .
Formal description.
The definition of a functional derivative may be made more mathematically precise and formal by defining the space of functions more carefully. For example, when the space of functions is a Banach space, the functional derivative becomes known as the Fréchet derivative, while one uses the Gâteaux derivative on more general locally convex spaces. Note that the well-known Hilbert spaces are special cases of Banach spaces. The more formal treatment allows many theorems from ordinary calculus and analysis to be generalized to corresponding theorems in functional analysis, as well as numerous new theorems to be stated.
Properties.
Like the derivative of a function, the functional derivative satisfies the following properties, where ["ρ"] and ["ρ"] are functionals:
where are constants.
Determining functional derivatives.
We give a formula to determine functional derivatives for a common class of functionals that can be written as the integral of a function and its derivatives. This is a generalization of the Euler–Lagrange equation: indeed, the functional derivative was introduced in physics within the derivation of the Lagrange equation of the second kind from the principle of least action in Lagrangian mechanics (18th century). The first three examples below are taken from density functional theory (20th century), the fourth from statistical mechanics (19th century).
Formula.
Given a functional 
and a function () that vanishes on the boundary of the region of integration, from a previous section Definition,
The second line is obtained using the total derivative, where "ρ" is a derivative of a scalar with respect to a vector. The third line was obtained by use of a product rule for divergence. The fourth line was obtained using the divergence theorem and the condition that on the boundary of the region of integration. Since is also an arbitrary function, applying the fundamental lemma of calculus of variations to the last line, the functional derivative is
where "ρ" = "ρ"() and , "ρ", ∇"ρ"). This formula is for the case of the functional form given by ["ρ"] at the beginning of this section. For other functional forms, the definition of the functional derivative can be used as the starting point for its determination. (See the example Coulomb potential energy functional.)
The above equation for the functional derivative can be generalized to the case that includes higher dimensions and higher order derivatives. The functional would be,
where the vector , and is a tensor whose components are partial derivative operators of order , 
An analogous application of the definition of the functional derivative yields
In the last two equations, the components of the tensor formula_20 are partial derivatives of with respect to partial derivatives of "ρ",
and the tensor scalar product is,
Examples.
Thomas–Fermi kinetic energy functional.
The Thomas–Fermi model of 1927 used a kinetic energy functional for a noninteracting uniform electron gas in a first attempt of density-functional theory of electronic structure:
Since the integrand of ["ρ"] does not involve derivatives of "ρ", the functional derivative of ["ρ"] is,
Coulomb potential energy functional.
For the electron-nucleus potential, Thomas and Fermi employed the Coulomb potential energy functional
Applying the definition of functional derivative,
So,
For the classical part of the electron-electron interaction, Thomas and Fermi employed the Coulomb potential energy functional
From the definition of the functional derivative, 
The first and second terms on the right hand side of the last equation are equal, since and in the second term can be interchanged without changing the value of the integral. Therefore,
and the functional derivative of the electron-electron coulomb potential energy functional ["ρ"] is,
The second functional derivative is
Weizsäcker kinetic energy functional.
In 1935 von Weizsäcker proposed to add a gradient correction to the Thomas-Fermi kinetic energy functional to make it suit better a molecular electron cloud:
where
Using a previously derived formula for the functional derivative,
and the result is,
Entropy.
The entropy of a discrete random variable is a functional of the probability mass function.
Thus,
Thus,
Exponential.
Let
Using the delta function as a test function,
Thus,
This is particularly useful in calculating the correlation functions from the partition function in quantum field theory.
Functional derivative of a function.
A function can be written in the form of an integral like a functional. For example,
Since the integrand does not depend on derivatives of "ρ", the functional derivative of "ρ" is,
Using the delta function as a test function.
In physics, it's common to use the Dirac delta function formula_45 in place of a generic test function formula_46, for yielding the functional derivative at the point formula_47 (this is a point of the whole functional derivative as a partial derivative is a component of the gradient):
This works in cases when formula_49 formally can be expanded as a series (or at least up to first order) in formula_50. The formula is however not mathematically rigorous, since formula_51 is usually not even defined.
The definition given in a previous section is based on a relationship that holds for all test functions , so one might think that it should hold also when is chosen to be a specific function such as the delta function. However, the latter is not a valid test function.
In the definition, the functional derivative describes how the functional formula_52 changes as a result of a small change in the entire function formula_53. The particular form of the change in formula_53 is not specified, but it should stretch over the whole interval on which formula_55 is defined. Employing the particular form of the perturbation given by the delta function has the meaning that formula_53 is varied only in the point formula_47. Except for this point, there is no variation in formula_53.

</doc>
<doc id="10890" url="https://en.wikipedia.org/wiki?curid=10890" title="Fundamental interaction">
Fundamental interaction

Fundamental interactions, also known as fundamental forces, are the interactions in physical systems that do not appear to be reducible to more basic interactions. There are four conventionally accepted fundamental interactions—gravitational, electromagnetic, strong nuclear, and weak nuclear. Each one is understood as the dynamics of a "field". The gravitational force is modelled as a continuous classical field. The other three are each modelled as discrete quantum fields, and exhibit a measurable unit or "elementary particle".
The two nuclear interactions produce strong forces at minuscule, subatomic distances. The strong nuclear interaction is responsible for the binding of atomic nuclei. The weak nuclear interaction also acts on the nucleus, mediating radioactive decay. Electromagnetism and gravity produce significant forces at macroscopic scales where the effects can be seen directly in every day life. Electrical and magnetic fields tend to cancel each other out when large collections of objects are considered, so over the largest distances (on the scale of planets and galaxies), gravity tends to be the dominant force.
Theoretical physicists working beyond the Standard Model seek to quantize the gravitational field toward predictions that particle physicists can experimentally confirm, thus yielding acceptance to a theory of quantum gravity (QG) (Phenomena suitable to model as a fifth force—perhaps an added gravitational effect—remain widely disputed.) Other theorists seek to unite the electroweak and strong fields within a Grand Unified Theory (GUT). While all four fundamental interactions are widely thought to align on a highly minuscule scale, particle accelerators cannot produce the massive energy levels required to experimentally probe at that Planck scale (which would experimentally confirm such theories.) Yet some theories, such as the string theory, seek both QG and GUT within one framework, unifying all four fundamental interactions along with mass generation within a theory of everything (ToE).
General relativity.
In his 1687 theory, Isaac Newton postulated space as an infinite and unalterable physical structure existing before, within, and around all objects while their states and relations unfold at a constant pace everywhere, thus absolute space and time. Inferring that all objects bearing mass approach at a constant rate, but collide by impact proportional to their masses, Newton inferred that matter exhibits an attractive force. His law of universal gravitation mathematically stated it to span the entire universe instantly (despite absolute time), or, if not actually a force, to be instant interaction among all objects (despite absolute space.) As conventionally interpreted, Newton's theory of motion modelled a "central force" without a communicating medium. Thus Newton's theory violated the first principle of mechanical philosophy, as stated by Descartes, "No action at a distance". Conversely, during the 1820s, when explaining magnetism, Michael Faraday inferred a "field" filling space and transmitting that force. Faraday conjectured that ultimately, all forces unified into one.
In the early 1870s, James Clerk Maxwell unified electricity and magnetism as effects of an electromagnetic field whose third consequence was light, travelling at constant speed in a vacuum. The electromagnetic field theory contradicted predictions of Newton's theory of motion, unless physical states of the luminiferous aether—presumed to fill all space whether within matter or in a vacuum and to manifest the electromagnetic field—aligned all phenomena and thereby held valid the Newtonian principle relativity or invariance. Disfavouring hypotheses at unobservables, Albert Einstein discarded the aether, and aligned electrodynamics with relativity by denying absolute space and time, and stating relative space and time. The two phenomena altered in the vicinity of an object measured to be in motion—length contraction and time dilation for the object experienced to be in relative motion—Einstein's principle special relativity, published in 1905.
Special relativity was accepted as a theory too. It rendered Newton's theory of motion apparently untenable, especially since Newtonian physics postulated an object's mass to be constant. A consequence of special relativity is mass being a variant form of energy, condensed into an object. By the equivalence principle, published by Einstein in 1907, gravitation is indistinguishable from acceleration, perhaps two phenomena sharing a mechanism. That year, Hermann Minkowski modelled special relativity to a unification of space and time, 4D spacetime. Stretching the three spatial dimensions onto the single dimension of time's arrow, Einstein arrived at the general theory of relativity in 1915. Einstein interpreted space as a substance, "Einstein-aether", whose physical properties receive motion from an object and transmit it to other objects while modulating events unfolding. Equivalent to energy, mass contracts space, which dilates time—events unfold more slowly—establishing local tension. The object relieves it in the likeness of a free fall at light speed along the pathway of least resistance, a straight line's equivalent on the curved surface of 4D spacetime, a pathway termed "worldline".
Einstein abolished "action at a distance" by theorizing a gravitational field—4D spacetime—that waves while transmitting motion across the universe at light speed. All objects always travel at light speed in 4D spacetime. At zero relative speed, an object is observed to travel none through space, but age most rapidly. That is, an object at relative rest in 3D space exhibits its constant energy to an observer by exhibiting top speed along 1D time flow. Conversely, at highest relative speed, an object traverses 3D space at light speed, yet is ageless, none of its constant energy available to internal motion as flow along 1D time. Whereas Newtonian inertia is an idealized case of an object either keeping rest or holding constant velocity by its hypothetical existence in a universe otherwise devoid of matter, Einsteinian inertia is indistinguishable from an object experiencing no acceleration by existing in a gravitational field possibly full of matter distributed uniformly. Conversely, even massless energy manifests gravitation—which is acceleration—on local objects by "curving" the surface of 4D spacetime. Physicists renounced belief that motion must be mediated by a "force".
Standard Model.
The electromagnetic, strong, and weak interactions associate with elementary particles, whose behaviours are modelled in quantum mechanics (QM). For predictive success with QM's probabilistic outcomes, particle physics conventionally models QM events across a field set to special relativity, altogether relativistic quantum field theory (QFT). Force particles, called gauge bosons—"force carriers" or "messenger particles" of underlying fields—interact with matter particles, called fermions. Everyday matter is atoms, composed of three fermion types: up-quarks and down-quarks constituting, as well as electrons orbiting, the atom's nucleus. Atoms interact, form molecules, and manifest further properties through electromagnetic interactions among their electrons absorbing and emitting photons, the electromagnetic field's force carrier, which if unimpeded traverse potentially infinite distance. Electromagnetism's QFT is quantum electrodynamics (QED).
The electromagnetic interaction was modelled with the weak interaction, whose force carriers are W and Z bosons, traversing the minuscule distance, in electroweak theory (EWT). Electroweak interaction would operate at such high temperatures as soon after the presumed Big Bang, but, as the early universe cooled, split into electromagnetic and weak interactions. The strong interaction, whose force carrier is the gluon, traversing minuscule distance among quarks, is modeled in quantum chromodynamics (QCD). EWT, QCD, and the Higgs mechanism, whereby the Higgs field manifests Higgs bosons that interact with some quantum particles and thereby endow those particles with mass comprise particle physics' Standard Model (SM). Predictions are usually made using calculational approximation methods, although such perturbation theory is inadequate to model some experimental observations (for instance bound states and solitons.) Still, physicists widely accept the Standard Model as science's most experimentally confirmed theory.
Beyond the Standard Model, some theorists work to unite the electroweak and strong interactions within a Grand Unified Theory (GUT). Some attempts at GUTs hypothesize "shadow" particles, such that every known matter particle associates with an undiscovered force particle, and vice versa, altogether supersymmetry (SUSY). Other theorists seek to quantize the gravitational field by the modelling behaviour of its hypothetical force carrier, the graviton and achieve quantum gravity (QG). One approach to QG is loop quantum gravity (LQG). Still other theorists seek both QG and GUT within one framework, reducing all four fundamental interactions to a Theory of Everything (ToE). The most prevalent aim at a ToE is string theory, although to model matter particles, it added SUSY to force particles—and so, strictly speaking, became superstring theory. Multiple, seemingly disparate superstring theories were unified on a backbone, M theory. Theories beyond the Standard Model remain highly speculative, lacking great experimental support.
Overview of the fundamental interactions.
In the conceptual model of fundamental interactions, matter consists of fermions, which carry properties called charges and spin ± (intrinsic angular momentum ±, where ħ is the reduced Planck constant). They attract or repel each other by exchanging bosons.
The interaction of any pair of fermions in perturbation theory can then be modelled thus:
The exchange of bosons always carries energy and momentum between the fermions, thereby changing their speed and direction. The exchange may also transport a charge between the fermions, changing the charges of the fermions in the process (e.g., turn them from one type of fermion to another). Since bosons carry one unit of angular momentum, the fermion's spin direction will flip from + to − (or vice versa) during such an exchange (in units of the reduced Planck's constant).
Because an interaction results in fermions attracting and repelling each other, an older term for "interaction" is force.
According to the present understanding, there are four fundamental interactions or forces: gravitation, electromagnetism, the weak interaction, and the strong interaction. Their magnitude and behaviour vary greatly, as described in the table below. Modern physics attempts to explain every observed physical phenomenon by these fundamental interactions. Moreover, reducing the number of different interaction types is seen as desirable. Two cases in point are the unification of:
Both magnitude ("relative strength") and "range", as given in the table, are meaningful only within a rather complex theoretical framework. It should also be noted that the table below lists properties of a conceptual scheme that is still the subject of ongoing research.
The modern (perturbative) quantum mechanical view of the fundamental forces other than gravity is that particles of matter (fermions) do not directly interact with each other, but rather carry a charge, and exchange virtual particles (gauge bosons), which are the interaction carriers or force mediators. For example, photons mediate the interaction of electric charges, and gluons mediate the interaction of color charges.
The interactions.
Gravity.
"Gravitation" is by far the weakest of the four interactions. The weakness of gravity can easily be demonstrated by suspending a pin using a simple magnet (such as a refrigerator magnet). The magnet is able to hold the pin against the gravitational pull of the entire Earth.
Yet gravitation is very important for macroscopic objects and over macroscopic distances for the following reasons. Gravitation:
Even though electromagnetism is far stronger than gravitation, electrostatic attraction is not relevant for large celestial bodies, such as planets, stars, and galaxies, simply because such bodies contain equal numbers of protons and electrons and so have a net electric charge of zero. Nothing "cancels" gravity, since it is only attractive, unlike electric forces which can be attractive or repulsive. On the other hand, all objects having mass are subject to the gravitational force, which only attracts. Therefore, only gravitation matters on the large-scale structure of the universe.
The long range of gravitation makes it responsible for such large-scale phenomena as the structure of galaxies, black holes, and it retards the expansion of the universe. Gravitation also explains astronomical phenomena on more modest scales, such as planetary orbits, as well as everyday experience: objects fall; heavy objects act as if they were glued to the ground, and animals can only jump so high.
Gravitation was the first interaction to be described mathematically. In ancient times, Aristotle hypothesized that objects of different masses fall at different rates. During the Scientific Revolution, Galileo Galilei experimentally determined that this was not the case — neglecting the friction due to air resistance, and buoyancy forces if an atmosphere is present (e.g. the case of a dropped air-filled balloon vs a water-filled balloon) all objects accelerate toward the Earth at the same rate. Isaac Newton's law of Universal Gravitation (1687) was a good approximation of the behaviour of gravitation. Our present-day understanding of gravitation stems from Albert Einstein's General Theory of Relativity of 1915, a more accurate (especially for cosmological masses and distances) description of gravitation in terms of the geometry of space-time.
Merging general relativity and quantum mechanics (or quantum field theory) into a more general theory of quantum gravity is an area of active research. It is hypothesized that gravitation is mediated by a massless spin-2 particle called the graviton.
Although general relativity has been experimentally confirmed (at least for weak fields ) on all but the smallest scales, there are rival theories of gravitation. Those taken seriously by the physics community all reduce to general relativity in some limit, and the focus of observational work is to establish limitations on what deviations from general relativity are possible.
Proposed extra dimensions could explain why the gravity force is so weak.
Electroweak interaction.
Electromagnetism and weak interaction appear to be very different at everyday low energies. They can be modeled using two different theories. However, above unification energy, on the order of 100 GeV, they would merge into a single electroweak force.
Electroweak theory is very important for modern cosmology, particularly on how the universe evolved. This is because shortly after the Big Bang, the temperature was approximately above 1015 K. Electromagnetic force and weak force were merged into a combined electroweak force.
For contributions to the unification of the weak and electromagnetic interaction between elementary particles, Abdus Salam, Sheldon Glashow and Steven Weinberg were awarded the Nobel Prize in Physics in 1979.
Electromagnetism.
Electromagnetism is the force that acts between electrically charged particles. This phenomenon includes the electrostatic force acting between charged particles at rest, and the combined effect of electric and magnetic forces acting between charged particles moving relative to each other.
Electromagnetism is infinite-ranged like gravity, but vastly stronger, and therefore describes a number of macroscopic phenomena of everyday experience such as friction, rainbows, lightning, and all human-made devices using electric current, such as television, lasers, and computers. Electromagnetism fundamentally determines all macroscopic, and many atomic levels, properties of the chemical elements, including all chemical bonding.
In a four kilogram (~1 gallon) jug of water there are
formula_1
of total electron charge. Thus, if we place two such jugs a meter apart, the electrons in one of the jugs repel those in the other jug with a force of
formula_2
This is larger than the planet Earth would weigh if weighed on another Earth. The atomic nuclei in one jug also repel those in the other with the same force. However, these repulsive forces are canceled by the attraction of the electrons in jug A with the nuclei in jug B and the attraction of the nuclei in jug A with the electrons in jug B, resulting in no net force. Electromagnetic forces are tremendously stronger than gravity but cancel out so that for large bodies gravity dominates.
Electrical and magnetic phenomena have been observed since ancient times, but it was only in the 19th century that it was discovered that electricity and magnetism are two aspects of the same fundamental interaction. By 1864, Maxwell's equations had rigorously quantified this unified interaction. Maxwell's theory, restated using vector calculus, is the classical theory of electromagnetism, suitable for most technological purposes.
The constant speed of light in a vacuum (customarily described with the letter "c") can be derived from Maxwell's equations, which are consistent with the theory of special relativity. Einstein's 1905 theory of special relativity, however, which flows from the observation that the speed of light is constant no matter how fast the observer is moving, showed that the theoretical result implied by Maxwell's equations has profound implications far beyond electromagnetism on the very nature of time and space.
In another work that departed from classical electro-magnetism, Einstein also explained the photoelectric effect by hypothesizing that light was transmitted in quanta, which we now call photons. Starting around 1927, Paul Dirac combined quantum mechanics with the relativistic theory of electromagnetism. Further work in the 1940s, by Richard Feynman, Freeman Dyson, Julian Schwinger, and Sin-Itiro Tomonaga, completed this theory, which is now called quantum electrodynamics, the revised theory of electromagnetism. Quantum electrodynamics and quantum mechanics provide a theoretical basis for electromagnetic behavior such as quantum tunneling, in which a certain percentage of electrically charged particles move in ways that would be impossible under the classical electromagnetic theory, that is necessary for everyday electronic devices such as transistors to function.
Weak interaction.
The "weak interaction" or "weak nuclear force" is responsible for some nuclear phenomena such as beta decay. Electromagnetism and the weak force are now understood to be two aspects of a unified electroweak interaction — this discovery was the first step toward the unified theory known as the Standard Model. In the theory of the electroweak interaction, the carriers of the weak force are the massive gauge bosons called the W and Z bosons. The weak interaction is the only known interaction which does not conserve parity; it is left-right asymmetric. The weak interaction even violates CP symmetry but does conserve CPT.
Strong interaction.
The "strong interaction", or "strong nuclear force", is the most complicated interaction, mainly because of the way it varies with distance. At distances greater than 10 femtometers, the strong force is practically unobservable. Moreover, it holds only inside the atomic nucleus.
After the nucleus was discovered in 1908, it was clear that a new force was needed to overcome the electrostatic repulsion, a manifestation of electromagnetism, of the positively charged protons. Otherwise, the nucleus could not exist. Moreover, the force had to be strong enough to squeeze the protons into a volume that is 10−15 of that of the entire atom. From the short range of this force, Hideki Yukawa predicted that it was associated with a massive particle, whose mass is approximately 100 MeV.
The 1947 discovery of the pion ushered in the modern era of particle physics. Hundreds of hadrons were discovered from the 1940s to 1960s, and an extremely complicated theory of hadrons as strongly interacting particles was developed. Most notably:
While each of these approaches offered deep insights, no approach led directly to a fundamental theory.
Murray Gell-Mann along with George Zweig first proposed fractionally charged quarks in 1961. Throughout the 1960s, different authors considered theories similar to the modern fundamental theory of quantum chromodynamics (QCD) as simple models for the interactions of quarks. The first to hypothesize the gluons of QCD were Moo-Young Han and Yoichiro Nambu, who introduced the quark color charge and hypothesized that it might be associated with a force-carrying field. At that time, however, it was difficult to see how such a model could permanently confine quarks. Han and Nambu also assigned each quark color an integer electrical charge, so that the quarks were fractionally charged only on average, and they did not expect the quarks in their model to be permanently confined.
In 1971, Murray Gell-Mann and Harald Fritzsch proposed that the Han/Nambu color gauge field was the correct theory of the short-distance interactions of fractionally charged quarks. A little later, David Gross, Frank Wilczek, and David Politzer discovered that this theory had the property of asymptotic freedom, allowing them to make contact with experimental evidence. They concluded that QCD was the complete theory of the strong interactions, correct at all distance scales. The discovery of asymptotic freedom led most physicists to accept QCD since it became clear that even the long-distance properties of the strong interactions could be consistent with experiment if the quarks are permanently confined.
Assuming that quarks are confined, Mikhail Shifman, Arkady Vainshtein, and Valentine Zakharov were able to compute the properties of many low-lying hadrons directly from QCD, with only a few extra parameters to describe the vacuum. In 1980, Kenneth G. Wilson published computer calculations based on the first principles of QCD, establishing, to a level of confidence tantamount to certainty, that QCD will confine quarks. Since then, QCD has been the established theory of the strong interactions.
QCD is a theory of fractionally charged quarks interacting by means of 8 photon-like particles called gluons. The gluons interact with each other, not just with the quarks, and at long distances the lines of force collimate into strings. In this way, the mathematical theory of QCD not only explains how quarks interact over short distances but also the string-like behavior, discovered by Chew and Frautschi, which they manifest over longer distances.
Beyond the Standard Model.
Numerous theoretical efforts have been made to systematize the existing four fundamental interactions on the model of electroweak unification.
Grand Unified Theories (GUTs) are proposals to show that all of the fundamental interactions, other than gravity, arise from a single interaction with symmetries that break down at low energy levels. GUTs predict relationships among constants of nature that are unrelated in the SM. GUTs also predict gauge coupling unification for the relative strengths of the electromagnetic, weak, and strong forces, a prediction verified at the Large Electron–Positron Collider in 1991 for supersymmetric theories.
Theories of everything, which integrate GUTs with a quantum gravity theory face a greater barrier, because no quantum gravity theories, which include string theory, loop quantum gravity, and twistor theory, have secured wide acceptance. Some theories look for a graviton to complete the Standard Model list of force-carrying particles, while others, like loop quantum gravity, emphasize the possibility that time-space itself may have a quantum aspect to it.
Some theories beyond the Standard Model include a hypothetical fifth force, and the search for such a force is an ongoing line of experimental research in physics. In supersymmetric theories, there are particles that acquire their masses only through supersymmetry breaking effects and these particles, known as moduli can mediate new forces. Another reason to look for new forces is the recent discovery that the expansion of the universe is accelerating (also known as dark energy), giving rise to a need to explain a nonzero cosmological constant, and possibly to other modifications of general relativity. Fifth forces have also been suggested to explain phenomena such as CP violations, dark matter, and dark flow.

</doc>
<doc id="6095269" url="https://en.wikipedia.org/wiki?curid=6095269" title="G-factor (physics)">
G-factor (physics)

A g"-factor (also called g" value or dimensionless magnetic moment) is a dimensionless quantity that characterizes the magnetic moment and gyromagnetic ratio of a particle or nucleus. It is essentially a proportionality constant that relates the observed magnetic moment "μ" of a particle to its angular momentum quantum number and a unit of magnetic moment, usually the Bohr magneton or nuclear magneton.
Definition.
Dirac particle.
The spin magnetic moment of a charged, spin-1/2 particle that does not possess any internal structure (a Dirac particle) is given by
where μ is the spin magnetic moment of the particle, "g" is the "g"-factor of the particle, "e" is the elementary charge, "m" is the mass of the particle, and S is the spin angular momentum of the particle (with magnitude "ħ"/2 for Dirac particles).
Baryon or nucleus.
Protons, neutrons, nuclei and other composite baryonic particles have magnetic moments arising from their spin (both of which may be zero). Conventionally, the associated "g"-factors are defined using the nuclear magneton, and thus implicitly using the proton's mass rather than the particle's mass as for a Dirac particle. The formula used under this convention is
where μ is the magnetic moment of the nucleon or nucleus resulting from its spin, "g" is the effective "g"-factor, I is its spin angular momentum, "μ"N is the nuclear magneton, "e" is the elementary charge and "m"p is the proton rest mass.
Calculation.
Electron "g"-factors.
There are three magnetic moments associated with an electron: one from its spin angular momentum, one from its orbital angular momentum, and one from its total angular momentum (the quantum-mechanical sum of those two components). Corresponding to these three moments are three different "g"-factors:
Electron spin "g"-factor.
The most famous of these is the "electron spin g-factor" (more often called simply the "electron g-factor"), "g"e, defined by
where μs is the magnetic moment resulting from the spin of an electron, S is its spin angular momentum, and "μ"B is the Bohr magneton. In atomic physics, the electron spin "g"-factor is often defined as the "absolute value" or "negative" of "g"e:
The "z"-component of the magnetic moment then becomes
The value "g"s is roughly equal to 2.002319, and is known to extraordinary precision. The reason it is not "precisely" two is explained by quantum electrodynamics calculation of the anomalous magnetic dipole moment.
Electron orbital "g"-factor.
Secondly, the "electron orbital g-factor", "g"L, is defined by
where μL is the magnetic moment resulting from the orbital angular momentum of an electron, L is its orbital angular momentum, and "μ"B is the Bohr magneton. The value of "g"L is exactly equal to one, by a quantum-mechanical argument analogous to the derivation of the classical magnetogyric ratio. For an electron in an orbital with a magnetic quantum number ml, the "z"-component of the orbital angular momentum is
which, since "g"L = 1, is "μ"B"m"l
Total angular momentum (Landé) "g"-factor.
Thirdly, the "Landé g-factor", "gJ", is defined by
where μ is the total magnetic moment resulting from both spin and orbital angular momentum of an electron, is its total angular momentum, and "μ"B is the Bohr magneton. The value of "g"J is related to "g"L and "g"s by a quantum-mechanical argument; see the article Landé g-factor.
Muon "g"-factor.
The muon, like the electron, has a "g"-factor associated with its spin, given by the equation
where μ is the magnetic moment resulting from the muon’s spin, S is the spin angular momentum, and "m"μ is the muon mass.
That the muon "g"-factor is not quite the same as the electron "g"-factor is mostly explained by quantum electrodynamics and its calculation of the anomalous magnetic dipole moment. Almost all of the small difference between the two values (99.96% of it) is due to a well-understood lack of a heavy-particle diagrams contributing to the probability for emission of a photon representing the magnetic dipole field, which are present for muons, but not electrons, in QED theory. These are entirely a result of the mass difference between the particles.
However, not all of the difference between the g-factors for electrons and muons is exactly explained by the Standard Model. The muon "g"-factor can, in theory, be affected by physics beyond the Standard Model, so it has been measured very precisely, in particular at the Brookhaven National Laboratory. In the E821 collaboration final report in November 2006, the experimental measured value is , compared to the theoretical prediction of . This is a difference of 3.4 standard deviations, suggesting that beyond-the-Standard-Model physics may be having an effect. The Brookhaven muon storage ring has been transported to Fermilab where the g−2 experiment will use it to make more precise measurements of muon g-factor.
Measured "g"-factor values.
The electron "g"-factor is one of the most precisely measured values in physics, with a relative standard uncertainty of .

</doc>
<doc id="295183" url="https://en.wikipedia.org/wiki?curid=295183" title="Galilean transformation">
Galilean transformation

In physics, a Galilean transformation is used to transform between the coordinates of two reference frames which differ only by constant relative motion within the constructs of Newtonian physics, and forms the Galilean group. It is the group of motions of Galilean relativity action on the four dimensions of space and time, forming the Galilean geometry. This is the passive transformation point of view. The equations below, although apparently obvious, are valid only at speeds much less than the speed of light. In special relativity the Galilean transformations are replaced by Poincaré transformations; conversely, the group contraction in the classical limit of Poincaré transformations yields Galilean transformations.
Galileo formulated these concepts in his description of "uniform motion". 
The topic was motivated by Galileo's description of the motion of a ball rolling down a ramp, by which he measured the numerical value for the acceleration of gravity near the surface of the Earth.
Translation.
Though the transformations are named for Galileo, it is absolute time and space as conceived by Isaac Newton that provides their domain of definition. In essence, the Galilean transformations embody the intuitive notion of addition and subtraction of velocities as vectors.
This assumption is abandoned in the Poincaré transformations. These relativistic transformations are applicable to all velocities, whilst the Galilean transformation can be regarded as a low-velocity approximation to the Poincaré transformation.
The notation below describes the relationship under the Galilean transformation between the coordinates and of a single arbitrary event, as measured in two coordinate systems S and S', in uniform relative motion (velocity "v") in their common "x" and "x"′ directions, with their spatial origins coinciding at time : 
Note that the last equation expresses the assumption of a universal time independent of the relative motion of different observers.
In the language of linear algebra, this transformation is considered a shear mapping, and is described with a matrix acting on a vector. With motion parallel to the "x"-axis, the transformation acts on only two components:
Though matrix representations are not strictly necessary for Galilean transformation, they provide the means for direct comparison to transformation methods in special relativity.
Galilean transformations.
The Galilean symmetries can be uniquely written as the composition of a "rotation", a "translation" and a "uniform motion" of spacetime. Let x represent a point in three-dimensional space, and "t" a point in one-dimensional time. A general point in spacetime is given by an ordered pair . A uniform motion, with velocity v, is given by formula_6 where v is in R3. A translation is given by formula_7 where a in R3 and "b" in R. A rotation is given by formula_8 where is an orthogonal transformation. As a Lie group, the Galilean transformations have 10 dimensions.
Galilean group.
Two Galilean transformations compose to form a third Galilean transformation. The set of all Galilean transformations SGal(3) on space forms a group with composition as the group operation. The group is sometimes presented as a matrix group with spacetime events ("t", x, 1) as vectors where "t" is real and x in R3 is a position in space. A matrix version of SGal(3) has been suggested:
where "s" is real and "v", "x", "y" are in R3 and R is a rotation matrix. The composition of transformations is then accomplished through matrix multiplication. SGal(3) has named subgroups. Let "m" represent the transformation matrix with parameters "v", "R", "s", "y":
The parameters "s", "v", "R", "y" span ten dimensions. Since the transformations depend continuously on "s", "v", "R", "y", SGal(3) is a continuous group, also called a topological group. The structure of SGal(3) can be understood by reconstruction from subgroups. The semidirect product combination (formula_14) of groups is required. 
Origin in group contraction.
Here, we only look at the Lie algebra of the Galilean group; it is then easy to extend the results to the Lie group.
The relevant Lie algebra is spanned by and (an antisymmetric tensor), subject to commutation relations, where
 is the generator of time translations (Hamiltonian), "Pi" is the generator of translations (momentum operator), "Ci" is the generator of Galileian boosts, and "Lij" stands for a generator of rotations (angular momentum operator). 
This Lie Algebra is seen to be a special classical limit of the algebra of the Poincaré group, in the limit . Technically, the Galilean group is a celebrated group contraction of the Poincaré group: renaming the generators of the latter as , where is the speed of light, or any function thereof diverging as , the commutation relations (structure constants) of the latter limit to that of the former.
Note the group invariants , .
Central extension of the Galilean group.
One could, instead, augment the Galilean group by a central extension into the Lie algebra spanned by , such that commutes with everything (i.e. lies in the center), and

</doc>
<doc id="2907966" url="https://en.wikipedia.org/wiki?curid=2907966" title="Gamma matrices">
Gamma matrices

In mathematical physics, the gamma matrices, formula_1, also known as the Dirac matrices, are a set of conventional matrices with specific anticommutation relations that ensure they generate a matrix representation of the Clifford algebra "C"ℓ1,3(R). It is also possible to define higher-dimensional gamma matrices. When interpreted as the matrices of the action of a set of orthogonal basis vectors for contravariant vectors in Minkowski space, the column vectors on which the matrices act become a space of spinors, on which the Clifford algebra of spacetime acts. This in turn makes it possible to represent infinitesimal spatial rotations and Lorentz boosts. Spinors facilitate spacetime computations in general, and in particular are fundamental to the Dirac equation for relativistic spin-½ particles.
In Dirac representation, the four contravariant gamma matrices are
formula_4 is the time-like matrix and the other three are space-like matrices.
Analogous sets of gamma matrices can be defined in any dimension and signature of the metric. For example, the Pauli matrices are a set of "gamma" matrices in dimension 3 with metric of Euclidean signature (3,0). In five spacetime dimensions, the four gammas above together with the fifth gamma matrix to be presented below generate the Clifford algebra.
Mathematical structure.
The defining property for the gamma matrices to generate a Clifford algebra is the anticommutation relation
where formula_6 is the anticommutator, formula_7 is the Minkowski metric with signature and formula_8 is the identity matrix.
This defining property is more fundamental than the numerical values used in the specific representation of the gamma matrices. 
Covariant gamma matrices are defined by
and Einstein notation is assumed.
Note that the other sign convention for the metric, necessitates either a change in the defining equation:
or a multiplication of all gamma matrices by formula_11, which of course changes their hermiticity properties detailed below. Under the alternative sign convention for the metric the covariant gamma matrices are then defined by
Physical structure.
The Clifford Algebra over spacetime can be regarded as the set of real linear operators from to itself, , or more generally, when complexified to , as the set of linear operators from any 4-dimensional complex vector space to itself. More simply, given a basis for , is just the set of all complex matrices, but endowed with a Clifford algebra structure. Spacetime is assumed to be endowed with the Minkowski metric . A space of bispinors, , is also assumed at every point in spacetime, endowed with the bispinor representation of the Lorentz group. The bispinor fields of the Dirac equations, evaluated at any point in spacetime, are elements of , see below. The Clifford algebra is assumed to act on as well (by matrix multiplication with column vectors in for all ). This will be the primary view of elements of in this section.
For each linear transformation of , there is a transformation of given by for in . If belongs to a representation of the Lorentz group, then the induced action will also belong to a representation of the Lorentz group, see Representation theory of the Lorentz group.
If is the bispinor representation acting on of an arbitrary Lorentz transformation in the standard (4-vector) representation acting on , then there is a corresponding operator on given by
showing that the can be viewed as a "basis" of a representation space of the 4-vector representation of the Lorentz group sitting inside the Clifford algebra. This means that quantities of the form
should be treated as 4-vectors in manipulations. It also means that indices can be raised and lowered on the using the metric as with any 4-vector. The notation is called the Feynman slash notation. The slash operation maps the unit vectors of , or any 4-dimensional vector space, to basis vectors . The transformation rule for slashed quantities is simply
One should note that this is different from the transformation rule for the , which are now treated as (fixed) basis vectors. The designation of the 4-tuple as a 4-vector sometimes found in the literature is thus a slight misnomer. The latter transformation corresponds to an active transformation of the components of a slashed quantity in terms of the basis , and the former to a passive transformation of the basis itself.
The elements form a representation of the Lie algebra of the Lorentz group. This is a spin representation. When these matrices, and linear combinations of them, are exponentiated, they are bispinor representations of the Lorentz group, e.g., the of above are of this form. The 6-dimensional space the span is the representation space of a tensor representation of the Lorentz group. For the higher order elements of the Clifford algebra in general, and their transformation rules, see the article Dirac algebra. But it is noted here that the Clifford algebra has no subspace being the representation space of a spin representation of the Lorentz group in the context used here.
Expressing the Dirac equation.
In natural units, the Dirac equation may be written as
where formula_17 is a Dirac spinor.
Switching to Feynman notation, the Dirac equation is
The fifth gamma matrix, 5.
It is useful to define the product of the four gamma matrices as follows:
Although formula_20 uses the letter gamma, it is not one of "the" gamma matrices of "C"ℓ1,3(R). The number 5 is a relic of old notation in which formula_4 was called "formula_22".
formula_20 has also an alternative form:
Proof
This can be seen by exploiting the fact that all the four gamma matrices anticommute, so 
where formula_26 is the type (4,4) generalized Kronecker delta in 4 dimensions. If formula_27 denotes the Levi-Civita symbol in "n" dimensions, we can use the identity formula_28.
Then we get
This matrix is useful in discussions of quantum mechanical chirality. For example, a Dirac field can be projected onto its left-handed and right-handed components by:
Some properties are:
The set therefore, by the last two properties (keeping in mind that ) and those of the old gammas, forms the basis of the Clifford algebra in spacetime dimensions for the metric signature . In metric signature , the set is used, where the are the appropriate ones for the signature. This pattern is repeated for spacetime dimension even and the next odd dimension for all . For more detail, see Higher-dimensional gamma matrices.
Identities.
The following identities follow from the fundamental anticommutation relation, so they hold in any basis (although the last one depends on the sign choice for formula_20).
Miscellaneous identities.
Proofs of 1 & 2
To show 
one begins with the standard anticommutation relation
One can make this situation look similar by using the metric formula_37:
To show 
We again will use the standard commutation relation. So start:
Proof of 3
To show
Use the anticommutator to shift formula_40 to the right
Using the relation formula_41 we can contract the last two gammas, and get
Finally using the anticommutator identity, we get
Proof of 4
Proof of 5
If formula_43 then formula_44
and it is easy to verify the identity. That is the case also when
formula_45, formula_46 or formula_47. On the other hand, if all three indices are
different, formula_48, formula_49 and formula_50 and both sides are completely antisymmetric (the left hand side because of the anticommutativity of the formula_51 matrices, and on the right hand side because of the antisymmetry of formula_52. It thus suffices verifying the identities for the cases of formula_53, formula_54, formula_55 and formula_56.
formula_57
formula_58
formula_59
formula_60
Trace identities.
The gamma matrices obey the following trace identities:
Proving the above involves the use of three main properties of the Trace operator:
Proof of 0
From the definition of the gamma matrices, 
We get 
or equivalently,
where formula_64 is a number, and formula_65 is a matrix.
This implies formula_66
Proof of 1
To show
First note that
We'll also use two facts about the fifth gamma matrix formula_69 that says:
So lets use these two facts to prove this identity for the first non-trivial case: the trace of three gamma matrices. Step one is to put in one pair of formula_69's in front of the three original formula_72's, and step two is to swap the formula_69 matrix back to the original position, after making use of the cyclicity of the trace.
This can only be fulfilled if
The extension to 2n+1 (n integer) gamma matrices, is found by placing two gamma-5s after (say) the 2n-th gamma matrix in the trace, commuting one out to the right (giving a minus sign) and commuting the other gamma-5 2n steps out to the left [with sign change (-1)^2n =1 ]. Then we use cyclic identity to get the two gamma-5s together and hence they square to identity, leaving us with the trace equalling minus itself, i.e. 0.
Proof of 2
If an odd number of gamma matrices appear in a trace followed by formula_20, our goal is to move formula_20 from the right side to the left. This will leave the trace invariant by the cyclic property. In order to do this move, we must anticommute it with all of the other gamma matrices. This means that we anticommute it an odd number of times and pick up a minus sign. A trace equal to the negative of itself must be zero.
Proof of 3
To show
Begin with,
Proof of 4
For the term on the right, we'll continue the pattern of swapping formula_78 with its neighbor to the left,
Again, for the term on the right swap formula_78 with its neighbor to the left,
Eq (3) is the term on the right of eq (2), and eq (2) is the term on the right of eq (1). We'll also use identity number 3 to simplify terms like so:
So finally Eq (1), when you plug all this information in gives
The terms inside the trace can be cycled, so
So really (4) is
or
Proof of 5
To show
begin with
Add formula_87 to both sides of the above to see
Now, this pattern can also be used to show 
Simply add two factors of formula_90, with formula_91 different from formula_92 and formula_93. Anticommute three times instead of once, picking up three minus signs, and cycle using the cyclic property of the trace.
So,
Proof of 6
For a proof of identity 6, the same trick still works unless formula_95 is some permutation of (0123), so that all 4 gammas appear. The anticommutation rules imply that interchanging two of the indices changes the sign of the trace, so formula_96 must be proportional to formula_97 formula_98. The proportionality constant is formula_99, as can be checked by plugging in formula_100, writing out formula_69, and remembering that the trace of the identity is 4.
Proof of 7
Denote the product of formula_102 gamma matrices by formula_103 Consider the Hermitian conjugate of formula_104:
Conjugating with formula_4 one more time to get rid of the two formula_4s that are there, we see that formula_107 is the reverse of formula_104. Now,
Normalization.
The gamma matrices can be chosen with extra hermiticity conditions which are restricted
by the above anticommutation relations however. We can impose
and for the other gamma matrices (for )
One checks immediately that these hermiticity relations hold for the Dirac representation.
The above conditions can be combined in the relation
The hermiticity conditions are not invariant under the action formula_114 of a Lorentz transformation formula_115 because formula_116 is not necessarily a unitary transformation due to the noncompactness of the Lorentz group.
Feynman slash notation.
The Feynman slash notation is defined by
for any 4-vector .
Here are some similar identities to the ones above, but involving slash notation:
Other representations.
The matrices are also sometimes written using the 2×2 identity matrix, formula_128, and
where "k" runs from 1 to 3 and the σk are Pauli matrices.
Dirac basis.
The gamma matrices we have written so far are appropriate for acting on Dirac spinors written in the "Dirac basis"; in fact, the Dirac basis is defined by these matrices. To summarize, in the Dirac basis:
Weyl (chiral) basis.
Another common choice is the "Weyl" or "chiral basis", in which formula_131 remains the same but formula_4 is different, and so formula_20 is also different, and diagonal,
or in more compact notation:
The Weyl basis has the advantage that its chiral projections take a simple form,
The idempotence of the chiral projections is manifest.
By slightly abusing the notation and reusing the symbols formula_137 we can then identify
where now formula_139 and formula_140 are left-handed and right-handed
two-component Weyl spinors.
Another possible choice of the Weyl basis has
The chiral projections take a slightly different form from the other Weyl choice,
In other words,
where formula_139 and formula_140 are the left-handed and right-handed
two-component Weyl spinors, as before.
Majorana basis.
There is also the Majorana basis, in which all of the Dirac matrices are imaginary and spinors are real. In terms of the Pauli matrices, it can be written as
The reason for making the gamma matrices imaginary is solely to obtain the particle physics metric (+,−,−,−) in which squared masses are positive. The Majorana representation however is real. One can factor out the formula_11 to obtain a different representation with four component real spinors and real gamma matrices. The consequence of removing the formula_11 is that the only possible metric with real gamma matrices is (−,+,+,+).
"C"ℓ1,3(C) and "C"ℓ1,3(R).
The Dirac algebra can be regarded as a complexification of the real algebra "C"ℓ1,3(R), called the space time algebra:
"C"ℓ1,3(R) differs from "C"ℓ1,3(C): in "C"ℓ1,3(R) only "real" linear combinations of the gamma matrices and their products are allowed.
Two things deserve to be pointed out. As "Clifford algebras", "C"ℓ1,3(C) and "C"ℓ4(C) are isomorphic, see classification of Clifford algebras. The reason is that the underlying signature of the spacetime metric loses its signature (3,1) upon passing to the complexification. However, the transformation required to bring the bilinear form to the complex canonical form is not a Lorentz transformation and hence not "permissible" (at the very least impractical) since all physics is tightly knit to the Lorentz symmetry and it is preferable to keep it manifest.
Proponents of geometric algebra strive to work with real algebras wherever that is possible. They argue that it is generally possible (and usually enlightening) to identify the presence of an imaginary unit in a physical equation. Such units arise from one of the many quantities in a real Clifford algebra that square to −1, and these have geometric significance because of the properties of the algebra and the interaction of its various subspaces. Some of these proponents also question whether it is necessary or even useful to introduce an additional imaginary unit in the context of the Dirac equation.
However, in contemporary practice, the Dirac algebra rather than the space time algebra continues to be the standard environment the spinors of the Dirac equation "live" in.
Euclidean Dirac matrices.
In quantum field theory one can Wick rotate the time axis to transit from Minkowski space to Euclidean space. This is particularly useful in some renormalization procedures as well as lattice gauge theory. In Euclidean space, there are two commonly used representations of Dirac Matrices:
Chiral representation.
Notice that the factors of formula_11 have been inserted in the spatial gamma matrices so that the Euclidean Clifford algebra
will emerge. It is also worth noting that there are variants of this which insert instead formula_154 on one of the matrices, such as in lattice QCD codes which use the chiral basis.
Different from Minkowski space, in Euclidean space,
Using the anti-commutator and noting that in Euclidean space formula_156, one shows that
In Chiral basis in Euclidean space,
which differs from the Minkowski version by a minus sign because we have sent formula_159 and reordered the matrices with the anti-commutator.

</doc>
<doc id="18616290" url="https://en.wikipedia.org/wiki?curid=18616290" title="Gamma ray">
Gamma ray

Gamma radiation, also known as gamma rays, and denoted by the Greek letter γ, refers to electromagnetic radiation of an extremely high frequency and therefore consists of high-energy photons. Gamma rays are ionizing radiation, and are thus biologically hazardous. They are classically produced by the decay of atomic nuclei as they transition from a high energy state to a lower state known as gamma decay, but may also be produced by other processes. Paul Villard, a French chemist and physicist, discovered gamma radiation in 1900, while studying radiation emitted from radium. Villard's radiation was named "gamma rays" by Ernest Rutherford in 1903.
Natural sources of gamma rays on Earth include gamma decay from naturally occurring radioisotopes, and secondary radiation from atmospheric interactions with cosmic ray particles. Rare terrestrial natural sources produce gamma rays that are not of a nuclear origin, such as lightning strikes and terrestrial gamma-ray flashes. Additionally, gamma rays are produced by a number of astronomical processes in which very high-energy electrons are produced, that in turn cause secondary gamma rays via bremsstrahlung, inverse Compton scattering, and synchrotron radiation. However, a large fraction of such astronomical gamma rays are screened by Earth's atmosphere and can only be detected by spacecraft. Gamma rays are produced by nuclear fusion in the core of stars including the Sun (such as the CNO cycle), but are absorbed or inelastically scattered by the stellar material before escaping and are not observable from Earth.
Gamma rays typically have frequencies above 10 exahertz (or >1019 Hz), and therefore have energies above 100 keV and wavelengths less than 10 picometers (10−12 meter), which is less than the diameter of an atom. However, this is not a strict definition, but rather only a rule-of-thumb description for natural processes. Electromagnetic radiation from radioactive decay of atomic nuclei is referred to as "gamma rays" no matter its energy, so that there is no "lower" limit to gamma energy derived from radioactive decay. This radiation commonly has energy of a few hundred keV, and almost always less than 10 MeV. In astronomy, gamma rays are defined by their energy, and no production process needs to be specified. The energies of gamma rays from astronomical sources range to over 10 TeV, an energy far too large to result from radioactive decay. A notable example is extremely powerful bursts of high-energy radiation referred to as long duration gamma-ray bursts, of energies higher than can be produced by radioactive decay. These bursts of gamma rays, thought to be due to the collapse of stars called hypernovae, are the most powerful events so far discovered in the cosmos.
History of discovery.
The first gamma ray source to be discovered historically was the radioactive decay process called gamma decay. In this type of decay, an excited nucleus emits a gamma ray almost immediately upon formation (it is now understood that a nuclear isomeric transition, however, can produce inhibited gamma decay with a measurable and much longer half-life). Paul Villard, a French chemist and physicist, discovered gamma radiation in 1900, while studying radiation emitted from radium. Villard knew that his described radiation was more powerful than previously described types of rays from radium, which included beta rays, first noted as "radioactivity" by Henri Becquerel in 1896, and alpha rays, discovered as a less penetrating form of radiation by Rutherford, in 1899. However, Villard did not consider naming them as a different fundamental type. Villard's radiation was recognized as being of a type fundamentally different from previously named rays, by Ernest Rutherford, who in 1903 named Villard's rays "gamma rays" by analogy with the beta and alpha rays that Rutherford had differentiated in 1899. The "rays" emitted by radioactive elements were named in order of their power to penetrate various materials, using the first three letters of the Greek alphabet: alpha rays as the least penetrating, followed by beta rays, followed by gamma rays as the most penetrating. Rutherford also noted that gamma rays were not deflected (or at least, not "easily" deflected) by a magnetic field, another property making them unlike alpha and beta rays.
Gamma rays were first thought to be particles with mass, like alpha and beta rays. Rutherford initially believed that they might be extremely fast beta particles, but their failure to be deflected by a magnetic field indicated that they had no charge. In 1914, gamma rays were observed to be reflected from crystal surfaces, proving that they were electromagnetic radiation. Rutherford and his coworker Edward Andrade measured the wavelengths of gamma rays from radium, and found that they were similar to X-rays, but with shorter wavelengths and (thus) higher frequency. This was eventually recognized as giving them also more energy per photon, as soon as the latter term became generally accepted. A gamma decay was then understood to usually emit a single gamma photon.
Sources of gamma rays.
Natural sources of gamma rays on Earth include gamma decay from naturally occurring radioisotopes such as potassium-40, and also as a secondary radiation from various atmospheric interactions with cosmic ray particles. Some rare terrestrial natural sources that produce gamma rays that are not of a nuclear origin, are lightning strikes and terrestrial gamma-ray flashes, which produce high energy emissions from natural high-energy voltages. Gamma rays are produced by a number of astronomical processes in which very high-energy electrons are produced. Such electrons produce secondary gamma rays by the mechanisms of bremsstrahlung, inverse Compton scattering and synchrotron radiation. A large fraction of such astronomical gamma rays are screened by Earth's atmosphere and must be detected by spacecraft. Notable artificial sources of gamma rays include fission such as occurs in nuclear reactors, and high energy physics experiments, such as neutral pion decay and nuclear fusion.
General characteristics.
The distinction between X-rays and gamma rays has changed in recent decades. Originally, the electromagnetic radiation emitted by X-ray tubes almost invariably had a longer wavelength than the radiation (gamma rays) emitted by radioactive nuclei. Older literature distinguished between X- and gamma radiation on the basis of wavelength, with radiation shorter than some arbitrary wavelength, such as 10−11 m, defined as gamma rays.
However, with artificial sources now able to duplicate any electromagnetic radiation that originates in the nucleus, as well as far higher energies, the wavelengths characteristic of radioactive gamma ray sources vs. other types, now completely overlap. Thus, gamma rays are now usually distinguished by their origin: X-rays are emitted by definition by electrons outside the nucleus, while gamma rays are emitted by the nucleus. Exceptions to this convention occur in astronomy, where gamma decay is seen in the afterglow of certain supernovas, but other high energy processes known to involve other than radioactive decay are still classed as sources of gamma radiation.
Naming conventions and overlap in terminology.
In the past, the distinction between X-rays and gamma rays was based on energy, with gamma rays being considered a higher-energy version of electromagnetic radiation. However, modern high-energy X-rays produced by linear accelerators for megavoltage treatment in cancer often have higher energy (4 to 25 MeV) than do most classical gamma rays produced by nuclear gamma decay. One of the most common gamma ray emitting isotopes used in diagnostic nuclear medicine, technetium-99m, produces gamma radiation of the same energy (140 keV) as that produced by diagnostic X-ray machines, but of significantly lower energy than therapeutic photons from linear particle accelerators. In the medical community today, the convention that radiation produced by nuclear decay is the only type referred to as "gamma" radiation is still respected.
Because of this broad overlap in energy ranges, in physics the two types of electromagnetic radiation are now often defined by their origin: X-rays are emitted by electrons (either in orbitals outside of the nucleus, or while being accelerated to produce bremsstrahlung-type radiation), while gamma rays are emitted by the nucleus or by means of other particle decays or annihilation events. There is no lower limit to the energy of photons produced by nuclear reactions, and thus ultraviolet or lower energy photons produced by these processes would also be defined as "gamma rays". The only naming-convention that is still universally respected is the rule that electromagnetic radiation that is known to be of atomic nuclear origin is "always" referred to as "gamma rays," and never as X-rays. However, in physics and astronomy, the converse convention (that all gamma rays are considered to be of nuclear origin) is frequently violated.
In astronomy, higher energy gamma and X-rays are defined by energy, since the processes that produce them may be uncertain and photon energy, not origin, determines the required astronomical detectors needed. High energy photons occur in nature that are known to be produced by processes other than nuclear decay but are still referred to as gamma radiation. An example is "gamma rays" from lightning discharges at 10 to 20 MeV, and known to be produced by the bremsstrahlung mechanism.
Another example is gamma-ray bursts, now known to be produced from processes too powerful to involve simple collections of atoms undergoing radioactive decay. This has led to the realization that many gamma rays produced in astronomical processes result not from radioactive decay or particle annihilation, but rather in much the same manner as the production of X-rays. Although gamma rays in astronomy are discussed below as non-radioactive events, in fact a few gamma rays are known in astronomy to originate explicitly from gamma decay of nuclei (as demonstrated by their spectra and emission half life). A classic example is that of supernova SN 1987A, which emits an "afterglow" of gamma-ray photons from the decay of newly made radioactive nickel-56 and cobalt-56. Most gamma rays in astronomy, however, arise by other mechanisms. Astronomical literature tends to write "gamma-ray" with a hyphen, by analogy to X-rays, rather than in a way analogous to alpha rays and beta rays. This notation tends to subtly stress the non-nuclear source of most astronomical "gamma-rays".
Units of measure and exposure.
The measure of gamma rays' ionizing ability is called the exposure:
However, the effect of gamma and other ionizing radiation on living tissue is more closely related to the amount of energy deposited rather than the charge. This is called the absorbed dose:
The equivalent dose is the measure of the biological effect of radiation on human tissue. For gamma rays, it is equal to the absorbed dose.
Properties.
Shielding.
Shielding from gamma rays requires large amounts of mass, in contrast to alpha particles, which can be blocked by paper or skin, and beta particles, which can be shielded by foil. Gamma rays are better absorbed by materials with high atomic numbers and high density, although neither effect is important compared to the total mass per area in the path of the gamma ray. For this reason, a lead shield is only modestly better (20–30% better) as a gamma shield than an equal mass of another shielding material, such as aluminium, concrete, water or soil; lead's major advantage is not in lower weight, but rather its compactness due to its higher density. Protective clothing, goggles and respirators can protect from internal contact with or ingestion of alpha or beta emitting particles, but provide no protection from gamma radiation from external sources.
The higher the energy of the gamma rays, the thicker the shielding made from the same shielding material is required. Materials for shielding gamma rays are typically measured by the thickness required to reduce the intensity of the gamma rays by one half (the half value layer or HVL). For example, gamma rays that require (0.4″) of lead to reduce their intensity by 50% will also have their intensity reduced in half by of granite rock, 6 cm (2½″) of concrete, or 9 cm (3½″) of packed soil. However, the mass of this much concrete or soil is only 20–30% greater than that of lead with the same absorption capability. Depleted uranium is used for shielding in portable gamma ray sources, but here the savings in weight over lead are larger, as portable sources' shape resembles a sphere to some extent, and the volume of a sphere is dependent on the cube of the radius; so a source with its radius cut in half will have its volume reduced by a factor of eight, which will more than compensate uranium's greater density (as well as reducing bulk). In a nuclear power plant, shielding can be provided by steel and concrete in the pressure and particle containment vessel, while water provides a radiation shielding of fuel rods during storage or transport into the reactor core. The loss of water or removal of a "hot" fuel assembly into the air would result in much higher radiation levels than when kept under water.
Matter interaction.
When a gamma ray passes through matter, the probability for absorption is proportional to the thickness of the layer, the density of the material, and the absorption cross section of the material. The total absorption shows an exponential decrease of intensity with distance from the incident surface:
where x is the thickness of the material from the incident surface, μ = "n"σ is the absorption coefficient, measured in cm−1, "n" the number of atoms per cm3 of the material (atomic density) and σ the absorption cross section in cm2.
As it passes through matter, gamma radiation ionizes via three processes: the photoelectric effect, Compton scattering, and pair production.
The secondary electrons (and/or positrons) produced in any of these three processes frequently have enough energy to produce much ionization themselves.
Additionally, gamma rays, particularly high energy ones, can interact with atomic nuclei resulting in ejection of particles in photodisintegration, or in some cases, even nuclear fission (photofission).
Light interaction.
High-energy (from 80 GeV to ~10 TeV) gamma rays arriving from far-distant quasars are used to estimate the extragalactic background light in the universe: The highest-energy rays interact more readily with the background light photons and thus the density of the background light may be estimated by analyzing the incoming gamma ray spectra.
Gamma ray production.
Gamma rays can be produced by a wide range of phenomena, both nuclear and non-nuclear.
Radioactive decay (gamma decay).
Gamma rays are produced during gamma decay, which normally occurs after other forms of decay occur, such as alpha or beta decay. An excited nucleus can decay by the emission of an or particle. The daughter nucleus that results is usually left in an excited state. It can then decay to a lower energy state by emitting a gamma ray photon, in a process called gamma decay.
The emission of a gamma ray from an excited nucleus typically, requires only 10−12 seconds, and is thus nearly instantaneous. Gamma decay may also follow nuclear reactions such as neutron capture, nuclear fission, or nuclear fusion. Gamma decay is also a mode of relaxation of many excited states of atomic nuclei following other types of radioactive decay, such as beta decay, so long as these states possess the necessary component of nuclear spin. When high-energy gamma rays, electrons, or protons bombard materials, the excited atoms emit characteristic "secondary" gamma rays, which are products of the creation of excited nuclear states in the bombarded atoms. Such transitions, a form of nuclear gamma fluorescence, form a topic in nuclear physics called gamma spectroscopy). Formation of fluorescent gamma rays are a rapid subtype of radioactive gamma decay.
In certain cases, the excited nuclear state that follows the emission of a beta particle or other type of excitation, may be more stable than average, and is termed a metastable excited state, if its decay takes (at least) 100 to 1000 times longer than the average 10−12 seconds. Such relatively long-lived excited nuclei are termed nuclear isomers, and their decays are termed isomeric transitions. Such nuclei have half-lifes that are more easily measurable, and rare nuclear isomers are able to stay in their excited state for minutes, hours, days, or occasionally far longer, before emitting a gamma ray. The process of isomeric transition is therefore similar to any gamma emission, but differs in that it involves the intermediate metastable excited state(s) of the nuclei. Metastable states are often characterized by high nuclear spin, requiring a change in spin of several units or more with gamma decay, instead of a single unit transition that occurs in only 10−12 seconds. The rate of gamma decay is also slowed when the energy of excitation of the nucleus is small.
An emitted gamma ray from any type of excited state may transfer its energy directly to any electrons, but most probably to one of the K shell electrons of the atom, causing it to be ejected from that atom, in a process generally termed the photoelectric effect (external gamma rays and ultraviolet rays may also cause this effect). The photoelectric effect should not be confused with the internal conversion process, in which a gamma ray photon is not produced as an intermediate particle (rather, a "virtual gamma ray" may be thought to mediate the process).
Gamma rays, X-rays, visible light, and radio waves are all forms of electromagnetic radiation. The only difference is the frequency and hence the energy of those photons. Gamma rays are generally the most energetic of these, although a broad overlap with X-ray energies occurs. An example of gamma ray production follows:
First decays to excited by beta decay emission of an electron of 0.31 MeV. Then the excited decays to the ground state (see nuclear shell model) by emitting gamma rays in succession of 1.17 MeV followed by 1.33 MeV. This path is followed 99.88% of the time:
Another example is the alpha decay of to form ; which is followed by gamma emission. In some cases, the gamma emission spectrum of the daughter nucleus is quite simple, (e.g. /) while in other cases, such as with (/ and /), the gamma emission spectrum is complex, revealing that a series of nuclear energy levels exist.
Because a beta decay is accompanied by the emission of a neutrino that also carries a varying amount of energy away, the beta emission spectrum does not have sharp lines, but instead is broad. Hence, it is not possible to describe the different energy levels found in the nucleus using beta decay energies alone.
Gamma spectroscopy is the study of the energetic transitions in atomic nuclei, which are generally associated with the absorption or emission of gamma rays. As in optical spectroscopy (see Franck Condon effect) the absorption of gamma rays by a nucleus is especially likely (i.e., peaks in a "resonance") when the energy of the gamma ray is the same as that of an energy transition in the nucleus. In the case of gamma rays, such a resonance is seen in the technique of Mössbauer spectroscopy. In the Mössbauer effect the narrow resonance absorption for nuclear gamma absorption can be successfully attained by physically immobilizing atomic nuclei in a crystal. The immobilization of nuclei at both ends of a gamma resonance interaction is required so that no gamma energy is lost to the kinetic energy of recoiling nuclei at either the emitting or absorbing end of a gamma transition. Such loss of energy causes gamma ray resonance absorption to fail. However, when emitted gamma rays carry essentially all of the energy of the atomic nuclear de-excitation that produces them, this energy is also sufficient to excite the same energy state in a second immobilized nucleus of the same type.
Gamma rays from sources other than radioactive decay.
A few gamma rays in astronomy are known to arise from gamma decay (see discussion of SN1987A) but most do not.
Photons from astrophysical sources that carry energy in the gamma radiation range are often explicitly called gamma-radiation. In addition to nuclear emissions, they are often produced by sub-atomic particle and particle-photon interactions. Those include electron-positron annihilation, neutral pion decay, bremsstrahlung, inverse Compton scattering, and synchrotron radiation.
Extraterrestrial, high energy gamma rays include the gamma ray background produced when cosmic rays (either high speed electrons or protons) collide with ordinary matter, producing pair-production gamma rays at 511 keV. Alternatively, bremsstrahlung are produced at energies of tens of MeV or more when cosmic ray electrons interact with nuclei of sufficiently high atomic number (see gamma ray image of the Moon at the beginning of this article, for illustration).
The so-called "long-duration" gamma-ray bursts produce a total energy output of about 1044 joules (as much energy as our Sun will produce in its entire life-time) but in a period of only 20 to 40 seconds. Gamma rays are approximately 50% of the total energy output. The leading hypotheses for the mechanism of production of these highest-known intensity beams of radiation, are inverse Compton scattering and synchrotron radiation from high-energy charged particles. These processes occur as relativistic charged particles leave the region of the event horizon of a newly formed black hole created during supernova explosion. The beam of particles moving at relativistic speeds are focused for a few tens of seconds by the magnetic field of the exploding hypernova. The fusion explosion of the hypernova drives the energetics of the process. If the narrowly directed beam happens to be pointed toward the Earth, it shines at gamma ray frequencies with such intensity, that it can be detected even at distances of up to 10 billion light years, which is close to the edge of the visible universe.
Health effects.
Gamma rays cause damage at a cellular level and are penetrating, causing diffuse damage throughout the body. However, they are less ionising than alpha or beta particles, which are less penetrating.
Low levels of gamma rays cause a stochastic health risk, which for radiation dose assessment is defined as the "probability" of cancer induction and genetic damage. High doses produce deterministic effects, which is the "severity" of acute tissue damage that is certain to happen. These effects are compared to the physical quantity absorbed dose measured by the unit gray (Gy).
Uses.
Gamma rays provide information about some of the most energetic phenomena in the universe; however, they are largely absorbed by the Earth's atmosphere. Instruments aboard high-altitude balloons and satellites missions, such as the Fermi Gamma-ray Space Telescope, provide our only view of the universe in gamma rays.
Gamma-induced molecular changes can also be used to alter the properties of semi-precious stones, and is often used to change white topaz into blue topaz.
Non-contact industrial sensors commonly use sources of gamma radiation in the refining, mining, chemical, food, soaps and detergents, and pulp and paper industries, for the measurement of levels, density, and thicknesses. Typically, these use Co-60 or Cs-137 isotopes as the radiation source.
In the US, gamma ray detectors are beginning to be used as part of the Container Security Initiative (CSI). These machines are advertised to be able to scan 30 containers per hour.
Gamma radiation is often used to kill living organisms, in a process called irradiation. Applications of this include the sterilization of medical equipment (as an alternative to autoclaves or chemical means), the removal of decay-causing bacteria from many foods and the prevention of the sprouting of fruit and vegetables to maintain freshness and flavor.
Despite their cancer-causing properties, gamma rays are also used to treat some types of cancer, since the rays kill cancer cells also. In the procedure called gamma-knife surgery, multiple concentrated beams of gamma rays are directed to the growth in order to kill the cancerous cells. The beams are aimed from different angles to concentrate the radiation on the growth while minimizing damage to surrounding tissues.
Gamma rays are also used for diagnostic purposes in nuclear medicine in imaging techniques. A number of different gamma-emitting radioisotopes are used. For example, in a PET scan a radiolabeled sugar called fludeoxyglucose emits positrons that are annihilated by electrons, producing pairs of gamma rays that highlight cancer as the cancer often has a higher metabolic rate than the surrounding tissues. The most common gamma emitter used in medical applications is the nuclear isomer technetium-99m which emits gamma rays in the same energy range as diagnostic X-rays. When this radionuclide tracer is administered to a patient, a gamma camera can be used to form an image of the radioisotope's distribution by detecting the gamma radiation emitted (see also SPECT). Depending on which molecule has been labeled with the tracer, such techniques can be employed to diagnose a wide range of conditions (for example, the spread of cancer to the bones via bone scan).
Body response.
When gamma radiation breaks DNA molecules, a cell may be able to repair the damaged genetic material, within limits. However, a study of Rothkamm and Lobrich has shown that this repair process works well after high-dose exposure but is much slower than in the case of a low-dose exposure.
Risk assessment.
The natural outdoor exposure in Great Britain ranges from 0.1 to 0.5 µSv/h with significant increase around known nuclear and contaminated sites. Natural exposure to gamma rays is about 1 to 2 mSv per year, and the average total amount of radiation received in one year per inhabitant in the USA is 3.6 mSv. There is a small increase in the dose, due to naturally occurring gamma radiation, around small particles of high atomic number materials in the human body caused by the photoelectric effect.
By comparison, the radiation dose from chest radiography (about 0.06 mSv) is a fraction of the annual naturally occurring background radiation dose. A chest CT delivers 5 to 8 mSv. A whole-body PET/CT scan can deliver 14 to 32 mSv depending on the protocol. The dose from fluoroscopy of the stomach is much higher, approximately 50 mSv (14 times the annual yearly background).
An acute full-body equivalent single exposure dose of 1 Sv (1000 mSv) causes slight blood changes, but 2.0–3.5 Sv (2.0–3.5 Gy) causes very severe syndrome of nausea, hair loss, and hemorrhaging, and will cause death in a sizable number of cases—-about 10% to 35% without medical treatment. A dose of 5 Sv (5 Gy) is considered approximately the LD50 (lethal dose for 50% of exposed population) for an acute exposure to radiation even with standard medical treatment. A dose higher than 5 Sv (5 Gy) brings an increasing chance of death above 50%. Above 7.5–10 Sv (7.5–10 Gy) to the entire body, even extraordinary treatment, such as bone-marrow transplants, will not prevent the death of the individual exposed (see "Radiation poisoning"). (Doses much larger than this may, however, be delivered to selected parts of the body in the course of radiation therapy.)
For low dose exposure, for example among nuclear workers, who receive an average yearly radiation dose of 19 mSv, the risk of dying from cancer (excluding leukemia) increases by 2 percent. For a dose of 100 mSv, the risk increase is 10 percent. By comparison, risk of dying from cancer was increased by 32 percent for the survivors of the atomic bombing of Hiroshima and Nagasaki.

</doc>
<doc id="1372450" url="https://en.wikipedia.org/wiki?curid=1372450" title="Gauge anomaly">
Gauge anomaly

In theoretical physics, a gauge anomaly is an example of an anomaly: it is an effect of quantum mechanics—usually a one-loop diagram—that invalidates the gauge symmetry of a quantum field theory; i.e. of a gauge theory.
Anomalies in gauge symmetries lead to an inconsistency, since a gauge symmetry is required in order to cancel unphysical degrees of freedom with a negative norm (such as a photon polarized in the time direction). Therefore all gauge anomalies must cancel out. This indeed happens in the Standard Model.
The term gauge anomaly is usually used for vector gauge anomalies. Another type of gauge anomaly is the gravitational anomaly, because reparametrization is a gauge symmetry in gravitation.
Calculation of the anomaly.
In vector gauge anomalies (in gauge symmetries whose gauge boson is a vector), the anomaly is a chiral anomaly, and can be calculated exactly at one loop level, via a Feynman diagram with a chiral fermion running in the loop (a polygon) with "n" external gauge bosons attached to the loop where formula_1 where formula_2 is the spacetime dimension. Anomalies occur only in even spacetime dimensions. For example, the anomalies in the usual 4 spacetime dimensions arise from triangle Feynman diagrams.
Let us look at the (semi)effective action we get after integrating over the chiral fermions. If there is a gauge anomaly, the resulting action will not be gauge invariant. If we denote by formula_3 the operator corresponding to an infinitesimal gauge transformation by ε, then the Frobenius consistency condition requires that
for any functional formula_5, including the (semi)effective action S where [,] is the Lie bracket. As formula_6 is linear in ε, we can write
where Ω(d) is d-form as a functional of the nonintegrated fields and is linear in ε. Let us make the further assumption (which turns out to be valid in all the cases of interest) that this functional is local (i.e. Ω(d)(x) only depends upon the values of the fields and their derivatives at x) and that it can be expressed as the exterior product of p-forms. If the spacetime Md is closed (i.e. without boundary) and oriented, then it is the boundary of some d+1 dimensional oriented manifold Md+1. If we then arbitrarily extend the fields (including ε) as defined on Md to Md+1 with the only condition being they match on the boundaries and the expression Ω(d), being the exterior product of p-forms, can be extended and defined in the interior, then
The Frobenius consistency condition now becomes
As the previous equation is valid for "any" arbitrary extension of the fields into the interior,
Because of the Frobenius consistency condition, this means that there exists a d+1-form Ω(d+1) (not depending upon ε) defined over Md+1 satisfying
Ω(d+1) is often called a Chern-Simons form.
Once again, if we assume Ω(d+1) can be expressed as an exterior product and that it can be extended into a d+1 -form in a d+2 dimensional oriented manifold, we can define
in d+2 dimensions. Ω(d+2) is gauge invariant:
as d and δε commute.

</doc>
<doc id="430790" url="https://en.wikipedia.org/wiki?curid=430790" title="Gauge boson">
Gauge boson

In particle physics, a gauge boson is a force carrier, a bosonic particle that carries any of the fundamental interactions of nature. Elementary particles, whose interactions are described by a gauge theory, interact with each other by the exchange of gauge bosons—usually as virtual particles.
Gauge bosons in the Standard Model.
The Standard Model of particle physics recognizes four kinds of gauge bosons: photons, which carry the electromagnetic interaction; W and Z bosons, which carry the weak interaction; and gluons, which carry the strong interaction.
Isolated gluons do not occur at low energies because they are color-charged, and subject to color confinement.
Multiplicity of gauge bosons.
In a quantized gauge theory, gauge bosons are quanta of the gauge fields. Consequently, there are as many gauge bosons as there are generators of the gauge field. In quantum electrodynamics, the gauge group is "U"(1); in this simple case, there is only one gauge boson. In quantum chromodynamics, the more complicated group "SU"(3) has eight generators, corresponding to the eight gluons. The three W and Z bosons correspond (roughly) to the three generators of "SU"(2) in GWS theory.
Massive gauge bosons.
For technical reasons involving gauge invariance, gauge bosons are described mathematically by field equations for massless particles. Therefore, at a naïve theoretical level all gauge bosons are required to be massless, and the forces that they describe are required to be long-ranged. The conflict between this idea and experimental evidence that the weak and strong interactions have a very short range requires further theoretical insight.
According to the Standard Model, the W and Z bosons gain mass via the Higgs mechanism. In the Higgs mechanism, the four gauge bosons (of "SU"(2)×"U"(1) symmetry) of the unified electroweak interaction couple to a Higgs field. This field undergoes spontaneous symmetry breaking due to the shape of its interaction potential. As a result, the universe is permeated by a nonzero Higgs vacuum expectation value (VEV). This VEV couples to three of the electroweak gauge bosons (the Ws and Z), giving them mass; the remaining gauge boson remains massless (the photon). This theory also predicts the existence of a scalar Higgs boson, which has been observed in experiments that were reported on 4 July 2012.
Beyond the Standard Model.
Grand unification theories.
A grand unified theory predicts additional gauge bosons named X and Y bosons. The hypothetical X and Y bosons direct interactions between quarks and leptons, hence violating conservation of baryon number and causing proton decay. Such bosons would be even more massive than W and Z bosons due to symmetry breaking. Analysis of data collected from such sources as the Super-Kamiokande neutrino detector has yielded no evidence of X and Y bosons.
Gravitons.
The fourth fundamental interaction, gravity, may also be carried by a boson, called the graviton. In the absence of experimental evidence and a mathematically coherent theory of quantum gravity, it is unknown whether this would be a gauge boson or not. The role of gauge invariance in general relativity is played by a similar symmetry: diffeomorphism invariance.
W' and Z' bosons.
W' and Z' bosons refer to hypothetical new gauge bosons (named in analogy with the Standard Model W and Z bosons).

</doc>
<doc id="2949102" url="https://en.wikipedia.org/wiki?curid=2949102" title="Gauge covariant derivative">
Gauge covariant derivative

The gauge covariant derivative is a generalization of the covariant derivative used in general relativity. If a theory has gauge transformations, it means that some physical properties of certain equations are preserved under those transformations. Likewise, the gauge covariant derivative is the ordinary derivative modified in such a way as to make it behave like a true vector operator, so that equations written using the covariant derivative preserve their physical properties under gauge transformations.
Fluid dynamics.
In fluid dynamics, the gauge covariant derivative of a fluid may be defined as
where formula_2 is a velocity vector field of a fluid.
Gauge theory.
In gauge theory, which studies a particular class of fields which are of importance in quantum field theory, the minimally-coupled gauge covariant derivative is defined as
where formula_4 is the electromagnetic vector potential.
Construction of the covariant derivative through Gauge covariance requirement.
Consider a generic (possibly non-Abelian) Gauge transformation given by
where formula_9 is an element of the Lie algebra associated with the Lie group of transformations, and can be expressed in terms of the generators as formula_10.
The partial derivative formula_11 transforms accordingly as
and a kinetic term of the form formula_13 is thus not invariant under this transformation.
We can introduce the covariant derivative formula_14 in this context as a generalization of the partial derivative formula_11 which transforms covariantly under the Gauge transformation, i.e. an object satisfying
which in operatorial form takes the form
We thus compute (omitting the explicit formula_18 dependencies for brevity)
where
The requirement for formula_14 to transform covariantly is now translated in the condition
To obtain an explicit expression we make the Ansatz
from which it follows that
and
which, using formula_27, takes the form
We have thus found an object formula_14 such that
Quantum electrodynamics.
If a gauge transformation is given by
and for the gauge potential 
then formula_33 transforms as
and formula_35 transforms as
and formula_37 transforms as
so that
and formula_40 in the QED Lagrangian is therefore gauge invariant, and the gauge covariant derivative is thus named aptly.
On the other hand, the non-covariant derivative formula_41 would not preserve the Lagrangian's gauge symmetry, since
Quantum chromodynamics.
In quantum chromodynamics, the gauge covariant derivative is
where formula_44 is the coupling constant, formula_45 is the gluon gauge field, for eight different gluons formula_46, formula_47 is a four-component Dirac spinor, and where formula_48 is one of the eight Gell-Mann matrices, formula_46.
Standard Model.
The covariant derivative in the Standard Model can be expressed in the following form:

</doc>
<doc id="2000736" url="https://en.wikipedia.org/wiki?curid=2000736" title="Gauge fixing">
Gauge fixing

In the physics of gauge theories, gauge fixing (also called choosing a gauge) denotes a mathematical procedure for coping with redundant degrees of freedom in field variables. By definition, a gauge theory represents each physically distinct configuration of the system as an equivalence class of detailed local field configurations. Any two detailed configurations in the same equivalence class are related by a gauge transformation, equivalent to a shear along unphysical axes in configuration space. Most of the quantitative physical predictions of a gauge theory can only be obtained under a coherent prescription for suppressing or ignoring these unphysical degrees of freedom.
Although the unphysical axes in the space of detailed configurations are a fundamental property of the physical model, there is no special set of directions "perpendicular" to them. Hence there is an enormous amount of freedom involved in taking a "cross section" representing each physical configuration by a "particular" detailed configuration (or even a weighted distribution of them). Judicious gauge fixing can simplify calculations immensely, but becomes progressively harder as the physical model becomes more realistic; its application to quantum field theory is fraught with complications related to renormalization, especially when the computation is continued to higher orders. Historically, the search for logically consistent and computationally tractable gauge fixing procedures, and efforts to demonstrate their equivalence in the face of a bewildering variety of technical difficulties, has been a major driver of mathematical physics from the late nineteenth century to the present.
Gauge freedom.
The archetypical gauge theory is the Heaviside–Gibbs formulation of continuum electrodynamics in terms of an electromagnetic four-potential, which is presented here in space/time asymmetric Heaviside notation. The electric field E and magnetic field B of Maxwell's equations contain only "physical" degrees of freedom, in the sense that every "mathematical" degree of freedom in an electromagnetic field configuration has a separately measurable effect on the motions of test charges in the vicinity. These "field strength" variables can be expressed in terms of the electric scalar potential formula_1 and the magnetic vector potential A through the relations:
If the transformation
is made, then B remains unchanged, since
However, this transformation changes E according to
If another change
is made then E also remains the same. Hence, the E and B fields are unchanged if one takes any function and simultaneously transforms A and φ via the transformations () and ().
A particular choice of the scalar and vector potentials is a gauge (more precisely, gauge potential) and a scalar function ψ used to change the gauge is called a gauge function. The existence of arbitrary numbers of gauge functions corresponds to the U(1) gauge freedom of this theory. Gauge fixing can be done in many ways, some of which we exhibit below.
Although classical electromagnetism is now often spoken of as a gauge theory, it was not originally conceived in these terms. The motion of a classical point charge is affected only by the electric and magnetic field strengths at that point, and the potentials can be treated as a mere mathematical device for simplifying some proofs and calculations. Not until the advent of quantum field theory could it be said that the potentials themselves are part of the physical configuration of a system. The earliest consequence to be accurately predicted and experimentally verified was the Aharonov–Bohm effect, which has no classical counterpart. Nevertheless, gauge freedom is still true in these theories. For example, the Aharonov–Bohm effect depends on a line integral of A around a closed loop, and this integral is not changed by 
Gauge fixing in non-abelian gauge theories, such as Yang–Mills theory and general relativity, is a rather more complicated topic; for details see Gribov ambiguity, Faddeev–Popov ghost, and frame bundle.
An illustration.
By looking at a cylindrical rod can one tell whether it is twisted? If the rod is perfectly cylindrical, then the circular symmetry of the cross section makes it impossible to tell whether or not it is twisted. However, if there were a straight line drawn along the length of the rod, then one could easily say whether or not there is a twist by looking at the state of the line. Drawing a line is gauge fixing. Drawing the line spoils the gauge symmetry, i.e., the circular symmetry U(1) of the cross section at each point of the rod. The line is the equivalent of a gauge function; it need not be straight. Almost any line is a valid gauge fixing, i.e., there is a large gauge freedom. To tell whether the rod is twisted, you need to first know the gauge. Physical quantities, such as the energy of the torsion, do not depend on the gauge, i.e., are gauge invariant.
Coulomb gauge.
The Coulomb gauge (also known as the transverse gauge) is much used in quantum chemistry and condensed matter physics and is defined by the gauge condition (more precisely, gauge fixing condition)
It is particularly useful for "semi-classical" calculations in quantum mechanics, in which the vector potential is quantized but the Coulomb interaction is not.
The Coulomb gauge has a number of properties:
/R^3 </math>.
which can be confirmed by calculating the div and curl of A. The divergence of A at infinity is a consequence of the unphysical assumption that the magnetic field is uniform throughout the whole of space. Although this vector potential is unrealistic in general it can provide a good approximation to the potential in a finite volume of space in which the magnetic field is uniform.
where is an arbitrary scalar field called the gauge function. The fields that are the derivatives of the gauge function are known as pure gauge fields and the arbitrariness associated with the gauge function is known as gauge freedom. In a calculation that is carried out correctly the pure gauge terms have no effect on any physical observable. A quantity or expression that does not depend on the gauge function is said to be gauge invariant: all physical observables are required to be gauge invariant. A gauge transformation from the Coulomb gauge to another gauge is made by taking the gauge function to be the sum of a specific function which will give the desired gauge transformation and the arbitrary function. If the arbitrary function is then set to zero, the gauge is said to be fixed. Calculations may be carried out in a fixed gauge but must be done in a way that is gauge invariant.
Lorenz gauge.
The "Lorenz gauge" is given, in SI units, by:
and in Gaussian units by:
This may be rewritten as:
where is the electromagnetic four-potential, ∂μ the 4-gradient [using the metric signature (+−−−)].
It is unique among the constraint gauges in retaining manifest Lorentz invariance. Note, however, that this gauge was originally named after the Danish physicist Ludvig Lorenz and not after Hendrik Lorentz; it is often misspelled "Lorentz gauge". (Neither was the first to use it in calculations; it was introduced in 1888 by George F. FitzGerald.)
The Lorenz gauge leads to the following inhomogeneous wave equations for the potentials:
It can be seen from these equations that, in the absence of current and charge, the solutions are potentials which propagate at the speed of light.
The Lorenz gauge is "incomplete" in the sense that there remains a subspace of gauge transformations which preserve the constraint. These remaining degrees of freedom correspond to gauge functions which satisfy the wave equation
These remaining gauge degrees of freedom propagate at the speed of light. To obtain a fully fixed gauge, one must add boundary conditions along the light cone of the experimental region.
Maxwell's equations in the Lorenz gauge simplify to 
where is the four-current.
Two solutions of these equations for the same current configuration differ by a solution of the vacuum wave equation 
In this form it is clear that the components of the potential separately satisfy the Klein–Gordon equation, and hence that the Lorenz gauge condition allows transversely, longitudinally, and "time-like" polarized waves in the four-potential. The transverse polarizations correspond to classical radiation, i. e., transversely polarized waves in the field strength. To suppress the "unphysical" longitudinal and time-like polarization states, which are not observed in experiments at classical distance scales, one must also employ auxiliary constraints known as Ward identities. Classically, these identities are equivalent to the continuity equation 
Many of the differences between classical and quantum electrodynamics can be accounted for by the role that the longitudinal and time-like polarizations play in interactions between charged particles at microscopic distances.
"Rξ" gauges.
The "Rξ" gauges are a generalization of the Lorenz gauge applicable to theories expressed in terms of an action principle with Lagrangian density formula_20. Instead of "fixing" the gauge by constraining the gauge field "a priori" via an auxiliary equation, one adds to the "physical" (gauge invariant) Lagrangian a gauge "breaking" term
The choice of the parameter ξ determines the choice of gauge. The Landau gauge, obtained as the limit ξ → 0, is classically equivalent to Lorenz gauge, but postponing taking the limit until after the theory is quantized improves the rigor of certain existence and equivalence proofs. Most quantum field theory computations are simplest in the Feynman–'t Hooft gauge, in which ; a few are more tractable in other "Rξ" gauges, such as the Yennie gauge .
An equivalent formulation of "Rξ" gauge uses an auxiliary field, a scalar field "B" with no independent dynamics:
The auxiliary field can be eliminated by "completing the square" to obtain the previous form. From a mathematical perspective the auxiliary field is a variety of Goldstone boson, and its use has advantages when identifying the asymptotic states of the theory, and especially when generalizing beyond QED.
Historically, the use of "Rξ" gauges was a significant technical advance in extending quantum electrodynamics computations beyond one-loop order. In addition to retaining manifest Lorentz invariance, the "Rξ" prescription breaks the symmetry under local gauge "transformations" while preserving the ratio of functional measures of any two physically distinct gauge "configurations". This permits a change of variables in which infinitesimal perturbations along "physical" directions in configuration space are entirely uncoupled from those along "unphysical" directions, allowing the latter to be absorbed into the physically meaningless normalization of the functional integral. When ξ is finite, each physical configuration (orbit of the group of gauge transformations) is represented not by a single solution of a constraint equation but by a Gaussian distribution centered on the extremum of the gauge breaking term. In terms of the Feynman rules of the gauge-fixed theory, this appears as a contribution to the photon propagator for internal lines from virtual photons of unphysical polarization.
The photon propagator, which is the multiplicative factor corresponding to an internal photon in the Feynman diagram expansion of a QED calculation, contains a factor "g"μν corresponding to the Minkowski metric. An expansion of this factor as a sum over photon polarizations involves terms containing all four possible polarizations. Transversely polarized radiation can be expressed mathematically as a sum over either a linearly or circularly polarized basis. Similarly, one can combine the longitudinal and time-like gauge polarizations to obtain "forward" and "backward" polarizations; these are a form of light cone coordinates in which the metric is off-diagonal. An expansion of the "g"μν factor in terms of circularly polarized (spin ±1) and light cone coordinates is called a spin sum. Spin sums can be very helpful both in simplifying expressions and in obtaining a physical understanding of the experimental effects associated with different terms in a theoretical calculation.
Richard Feynman used arguments along approximately these lines largely to justify calculation procedures that produced consistent, finite, high precision results for important observable parameters such as the anomalous magnetic moment of the electron. Although his arguments sometimes lacked mathematical rigor even by physicists' standards and glossed over details such as the derivation of Ward–Takahashi identities of the quantum theory, his calculations worked, and Freeman Dyson soon demonstrated that his method was substantially equivalent to those of Julian Schwinger and Sin-Itiro Tomonaga, with whom Feynman shared the 1965 Nobel Prize in Physics.
Forward and backward polarized radiation can be omitted in the asymptotic states of a quantum field theory (see Ward–Takahashi identity). For this reason, and because their appearance in spin sums can be seen as a mere mathematical device in QED (much like the electromagnetic four-potential in classical electrodynamics), they are often spoken of as "unphysical". But unlike the constraint-based gauge fixing procedures above, the "Rξ" gauge generalizes well to non-abelian gauge groups such as the SU(3) of QCD. The couplings between physical and unphysical perturbation axes do not entirely disappear under the corresponding change of variables; to obtain correct results, one must account for the non-trivial Jacobian of the embedding of gauge freedom axes within the space of detailed configurations. This leads to the explicit appearance of forward and backward polarized gauge bosons in Feynman diagrams, along with Faddeev–Popov ghosts, which are even more "unphysical" in that they violate the spin-statistics theorem. The relationship between these entities, and the reasons why they do not appear as particles in the quantum mechanical sense, becomes more evident in the BRST formalism of quantization.
Maximal Abelian gauge.
In any non-Abelian gauge theory, any maximal Abelian gauge is an "incomplete" gauge which fixes the gauge freedom outside of the maximal Abelian subgroup. Examples are
This applies regularly in higher algebras (of groups in the algebras), for example the Clifford Algebra and as it is regularly.
Less commonly used gauges.
Weyl gauge.
The Weyl gauge (also known as the Hamiltonian or temporal gauge) is an "incomplete" gauge obtained by the choice
It is named after Hermann Weyl.
Multipolar gauge.
The gauge condition of the Multipolar gauge (also known as the Line gauge, point gauge or Poincaré gauge) is:
This is another gauge in which the potentials can be expressed in a simple way in terms of the instantaneous fields
Fock–Schwinger gauge.
The gauge condition of the Fock–Schwinger gauge (sometimes called the relativistic Poincaré gauge) is:
where "x"μ is the position four-vector.

</doc>
<doc id="23122856" url="https://en.wikipedia.org/wiki?curid=23122856" title="Gauge principle">
Gauge principle

In physics, a gauge principle specifies a procedure for obtaining an interaction term from a free Lagrangian which is symmetric with respect to a continuous symmetry—the results of localizing (or gauging) the global symmetry group must be accompanied by the inclusion of additional fields (such as the electromagnetic field), with appropriate kinetic and interaction terms in the action, in such a way that the extended Lagrangian is covariant with respect to a new extended group of local transformations.

</doc>
<doc id="23912155" url="https://en.wikipedia.org/wiki?curid=23912155" title="Gauge theory">
Gauge theory

In physics, a gauge theory is a type of field theory in which the Lagrangian is invariant under a continuous group of local transformations.
The term "gauge" refers to redundant degrees of freedom in the Lagrangian. The transformations between possible gauges, called "gauge transformations", form a Lie group—referred to as the "symmetry group" or the "gauge group" of the theory. Associated with any Lie group is the Lie algebra of group generators. For each group generator there necessarily arises a corresponding vector field called the "gauge field". Gauge fields are included in the Lagrangian to ensure its invariance under the local group transformations (called "gauge invariance"). When such a theory is quantized, the quanta of the gauge fields are called "gauge bosons". If the symmetry group is non-commutative, the gauge theory is referred to as "non-abelian", the usual example being the Yang–Mills theory.
Many powerful theories in physics are described by Lagrangians that are invariant under some symmetry transformation groups. When they are invariant under a transformation identically performed at "every" point in the space in which the physical processes occur, they are said to have a global symmetry. The requirement of local symmetry, the cornerstone of gauge theories, is a stricter constraint. In fact, a global symmetry is just a local symmetry whose group's parameters are fixed in space-time.
Gauge theories are important as the successful field theories explaining the dynamics of elementary particles. Quantum electrodynamics is an abelian gauge theory with the symmetry group U(1) and has one gauge field, the electromagnetic four-potential, with the photon being the gauge boson. The Standard Model is a non-abelian gauge theory with the symmetry group U(1)×SU(2)×SU(3) and has a total of twelve gauge bosons: the photon, three weak bosons and eight gluons.
Gauge theories are also important in explaining gravitation in the theory of general relativity. Its case is somewhat unique in that the gauge field is a tensor, the Lanczos tensor. Theories of quantum gravity, beginning with gauge gravitation theory, also postulate the existence of a gauge boson known as the graviton. Gauge symmetries can be viewed as analogues of the principle of general covariance of general relativity in which the coordinate system can be chosen freely under arbitrary diffeomorphisms of spacetime. Both gauge invariance and diffeomorphism invariance reflect a redundancy in the description of the system. An alternative theory of gravitation, gauge theory gravity, replaces the principle of general covariance with a true gauge principle with new gauge fields.
Historically, these ideas were first stated in the context of classical electromagnetism and later in general relativity. However, the modern importance of gauge symmetries appeared first in the relativistic quantum mechanics of electronsquantum electrodynamics, elaborated on below. Today, gauge theories are useful in condensed matter, nuclear and high energy physics among other subfields.
History and importance.
The earliest field theory having a gauge symmetry was Maxwell's formulation of electrodynamics in 1864. The importance of this symmetry remained unnoticed in the earliest formulations. Similarly unnoticed, Hilbert had derived the Einstein field equations by postulating the invariance of the action under a general coordinate transformation. Later Hermann Weyl, in an attempt to unify general relativity and electromagnetism, conjectured that "Eichinvarianz" or invariance under the change of scale (or "gauge") might also be a local symmetry of general relativity. After the development of quantum mechanics, Weyl, Vladimir Fock and Fritz London modified gauge by replacing the scale factor with a complex quantity and turned the scale transformation into a change of phase, which is a U(1) gauge symmetry. This explained the electromagnetic field effect on the wave function of a charged quantum mechanical particle. This was the first widely recognised gauge theory, popularised by Pauli in the 1940s.
In 1954, attempting to resolve some of the great confusion in elementary particle physics, Chen Ning Yang and Robert Mills introduced non-abelian gauge theories as models to understand the strong interaction holding together nucleons in atomic nuclei. (Ronald Shaw, working under Abdus Salam, independently introduced the same notion in his doctoral thesis.) Generalizing the gauge invariance of electromagnetism, they attempted to construct a theory based on the action of the (non-abelian) SU(2) symmetry group on the isospin doublet of protons and neutrons. This is similar to the action of the U(1) group on the spinor fields of quantum electrodynamics. In particle physics the emphasis was on using quantized gauge theories.
This idea later found application in the quantum field theory of the weak force, and its unification with electromagnetism in the electroweak theory. Gauge theories became even more attractive when it was realized that non-abelian gauge theories reproduced a feature called asymptotic freedom. Asymptotic freedom was believed to be an important characteristic of strong interactions. This motivated searching for a strong force gauge theory. This theory, now known as quantum chromodynamics, is a gauge theory with the action of the SU(3) group on the color triplet of quarks. The Standard Model unifies the description of electromagnetism, weak interactions and strong interactions in the language of gauge theory.
In the 1970s, Sir Michael Atiyah began studying the mathematics of solutions to the classical Yang–Mills equations. In 1983, Atiyah's student Simon Donaldson built on this work to show that the differentiable classification of smooth 4-manifolds is very different from their classification up to homeomorphism. Michael Freedman used Donaldson's work to exhibit exotic R4s, that is, exotic differentiable structures on Euclidean 4-dimensional space. This led to an increasing interest in gauge theory for its own sake, independent of its successes in fundamental physics. In 1994, Edward Witten and Nathan Seiberg invented gauge-theoretic techniques based on supersymmetry that enabled the calculation of certain topological invariants (the Seiberg–Witten invariants). These contributions to mathematics from gauge theory have led to a renewed interest in this area.
The importance of gauge theories in physics is exemplified in the tremendous success of the mathematical formalism in providing a unified framework to describe the quantum field theories of electromagnetism, the weak force and the strong force. This theory, known as the Standard Model, accurately describes experimental predictions regarding three of the four fundamental forces of nature, and is a gauge theory with the gauge group SU(3) × SU(2) × U(1). Modern theories like string theory, as well as general relativity, are, in one way or another, gauge theories.
Description.
Global and local symmetries.
In physics, the mathematical description of any physical situation usually contains excess degrees of freedom; the same physical situation is equally well described by many equivalent mathematical configurations. For instance, in Newtonian dynamics, if two configurations are related by a Galilean transformation (an inertial change of reference frame) they represent the same physical situation. These transformations form a group of "symmetries" of the theory, and a physical situation corresponds not to an individual mathematical configuration but to a class of configurations related to one another by this symmetry group.
This idea can be generalized to include local as well as global symmetries, analogous to much more abstract "changes of coordinates" in a situation where there is no preferred "inertial" coordinate system that covers the entire physical system. A gauge theory is a mathematical model that has symmetries of this kind, together with a set of techniques for making physical predictions consistent with the symmetries of the model.
Example of global symmetry.
When a quantity occurring in the mathematical configuration is not just a number but has some geometrical significance, such as a velocity or an axis of rotation, its representation as numbers arranged in a vector or matrix is also changed by a coordinate transformation. For instance, if one description of a pattern of fluid flow states that the fluid velocity in the neighborhood of ("x"=1, "y"=0) is 1 m/s in the positive "x" direction, then a description of the same situation in which the coordinate system has been rotated clockwise by 90 degrees states that the fluid velocity in the neighborhood of ("x"=0, "y"=1) is 1 m/s in the positive "y" direction. The coordinate transformation has affected both the coordinate system used to identify the "location" of the measurement and the basis in which its "value" is expressed. As long as this transformation is performed globally (affecting the coordinate basis in the same way at every point), the effect on values that represent the "rate of change" of some quantity along some path in space and time as it passes through point "P" is the same as the effect on values that are truly local to "P".
Use of fiber bundles to describe local symmetries.
In order to adequately describe physical situations in more complex theories, it is often necessary to introduce a "coordinate basis" for some of the objects of the theory that do not have this simple relationship to the coordinates used to label points in space and time. (In mathematical terms, the theory involves a fiber bundle in which the fiber at each point of the base space consists of possible coordinate bases for use when describing the values of objects at that point.) In order to spell out a mathematical configuration, one must choose a particular coordinate basis at each point (a "local section" of the fiber bundle) and express the values of the objects of the theory (usually "fields" in the physicist's sense) using this basis. Two such mathematical configurations are equivalent (describe the same physical situation) if they are related by a transformation of this abstract coordinate basis (a change of local section, or "gauge transformation").
In most gauge theories, the set of possible transformations of the abstract gauge basis at an individual point in space and time is a finite-dimensional Lie group. The simplest such group is U(1), which appears in the modern formulation of quantum electrodynamics (QED) via its use of complex numbers. QED is generally regarded as the first, and simplest, physical gauge theory. The set of possible gauge transformations of the entire configuration of a given gauge theory also forms a group, the "gauge group" of the theory. An element of the gauge group can be parameterized by a smoothly varying function from the points of spacetime to the (finite-dimensional) Lie group, such that the value of the function and its derivatives at each point represents the action of the gauge transformation on the fiber over that point.
A gauge transformation with constant parameter at every point in space and time is analogous to a rigid rotation of the geometric coordinate system; it represents a global symmetry of the gauge representation. As in the case of a rigid rotation, this gauge transformation affects expressions that represent the rate of change along a path of some gauge-dependent quantity in the same way as those that represent a truly local quantity. A gauge transformation whose parameter is "not" a constant function is referred to as a local symmetry; its effect on expressions that involve a derivative is qualitatively different from that on expressions that don't. (This is analogous to a non-inertial change of reference frame, which can produce a Coriolis effect.)
Gauge fields.
The "gauge covariant" version of a gauge theory accounts for this effect by introducing a gauge field (in mathematical language, an Ehresmann connection) and formulating all rates of change in terms of the covariant derivative with respect to this connection. The gauge field becomes an essential part of the description of a mathematical configuration. A configuration in which the gauge field can be eliminated by a gauge transformation has the property that its field strength (in mathematical language, its curvature) is zero everywhere; a gauge theory is "not" limited to these configurations. In other words, the distinguishing characteristic of a gauge theory is that the gauge field does not merely compensate for a poor choice of coordinate system; there is generally no gauge transformation that makes the gauge field vanish.
When analyzing the dynamics of a gauge theory, the gauge field must be treated as a dynamical variable, similarly to other objects in the description of a physical situation. In addition to its interaction with other objects via the covariant derivative, the gauge field typically contributes energy in the form of a "self-energy" term. One can obtain the equations for the gauge theory by:
This is the sense in which a gauge theory "extends" a global symmetry to a local symmetry, and closely resembles the historical development of the gauge theory of gravity known as general relativity.
Physical experiments.
Gauge theories are used to model the results of physical experiments, essentially by:
The mathematical descriptions of the "setup information" and the "possible measurement outcomes" (loosely speaking, the "boundary conditions" of the experiment) are generally not expressible without reference to a particular coordinate system, including a choice of gauge. (If nothing else, one assumes that the experiment has been adequately isolated from "external" influence, which is itself a gauge-dependent statement.) Mishandling gauge dependence in boundary conditions is a frequent source of anomalies in gauge theory calculations, and gauge theories can be broadly classified by their approaches to anomaly avoidance.
Continuum theories.
The two gauge theories mentioned above (continuum electrodynamics and general relativity) are examples of continuum field theories. The techniques of calculation in a continuum theory implicitly assume that:
These assumptions are close enough to be valid across a wide range of energy scales and experimental conditions, to allow these theories to make accurate predictions about almost all of the phenomena encountered in daily life, from light, heat, and electricity to eclipses and spaceflight. They fail only at the smallest and largest scales (due to omissions in the theories themselves) and when the mathematical techniques themselves break down (most notably in the case of turbulence and other chaotic phenomena).
Quantum field theories.
Other than these classical continuum field theories, the most widely known gauge theories are quantum field theories, including quantum electrodynamics and the Standard Model of elementary particle physics. The starting point of a quantum field theory is much like that of its continuum analog: a gauge-covariant action integral that characterizes "allowable" physical situations according to the principle of least action. However, continuum and quantum theories differ significantly in how they handle the excess degrees of freedom represented by gauge transformations. Continuum theories, and most pedagogical treatments of the simplest quantum field theories, use a gauge fixing prescription to reduce the orbit of mathematical configurations that represent a given physical situation to a smaller orbit related by a smaller gauge group (the global symmetry group, or perhaps even the trivial group).
More sophisticated quantum field theories, in particular those that involve a non-abelian gauge group, break the gauge symmetry within the techniques of perturbation theory by introducing additional fields (the Faddeev–Popov ghosts) and counterterms motivated by anomaly cancellation, in an approach known as BRST quantization. While these concerns are in one sense highly technical, they are also closely related to the nature of measurement, the limits on knowledge of a physical situation, and the interactions between incompletely specified experimental conditions and incompletely understood physical theory . The mathematical techniques that have been developed in order to make gauge theories tractable have found many other applications, from solid-state physics and crystallography to low-dimensional topology.
Classical gauge theory.
Classical electromagnetism.
Historically, the first example of gauge symmetry discovered was classical electromagnetism. In electrostatics, one can either discuss the electric field, E, or its corresponding electric potential, "V". Knowledge of one makes it possible to find the other, except that potentials differing by a constant, formula_1, correspond to the same electric field. This is because the electric field relates to "changes" in the potential from one point in space to another, and the constant "C" would cancel out when subtracting to find the change in potential. In terms of vector calculus, the electric field is the gradient of the potential, formula_2. Generalizing from static electricity to electromagnetism, we have a second potential, the vector potential A, with
The general gauge transformations now become not just formula_1 but
where "f" is any function that depends on position and time. The fields remain the same under the gauge transformation, and therefore Maxwell's equations are still satisfied. That is, Maxwell's equations have a gauge symmetry.
An example: Scalar O("n") gauge theory.
The following illustrates how local gauge invariance can be "motivated" heuristically starting from global symmetry properties, and how it leads to an interaction between originally non-interacting fields.
Consider a set of "n" non-interacting real scalar fields, with equal masses "m". This system is described by an action that is the sum of the (usual) action for each scalar field formula_6
The Lagrangian (density) can be compactly written as
by introducing a vector of fields 
The term formula_10 is Einstein notation for the partial derivative of formula_11 in each of the four dimensions. 
It is now transparent that the Lagrangian is invariant under the transformation
whenever "G" is a "constant" matrix belonging to the "n"-by-"n" orthogonal group O("n"). This is seen to preserve the Lagrangian, since the derivative of formula_11 transforms identically to formula_11 and both quantities appear inside dot products in the Lagrangian (orthogonal transformations preserve the dot product).
This characterizes the "global" symmetry of this particular Lagrangian, and the symmetry group is often called the gauge group; the mathematical term is structure group, especially in the theory of G-structures. Incidentally, Noether's theorem implies that invariance under this group of transformations leads to the conservation of the "currents"
where the "Ta" matrices are generators of the SO("n") group. There is one conserved current for every generator.
Now, demanding that this Lagrangian should have "local" O("n")-invariance requires that the "G" matrices (which were earlier constant) should be allowed to become functions of the space-time coordinates "x".
In this case, the "G" matrices do not "pass through" the derivatives, when "G" = "G"("x"),
The failure of the derivative to commute with "G" introduces an additional term (in keeping with the product rule), which spoils the invariance of the Lagrangian. In order to rectify this we define a new derivative operator such that the derivative of formula_11 again transforms identically with formula_11
This new "derivative" is called a (gauge) covariant derivative and takes the form
Where "g" is called the coupling constant; a quantity defining the strength of an interaction.
After a simple calculation we can see that the gauge field "A"("x") must transform as follows
The gauge field is an element of the Lie algebra, and can therefore be expanded as
There are therefore as many gauge fields as there are generators of the Lie algebra.
Finally, we now have a "locally gauge invariant" Lagrangian
Pauli uses the term "gauge transformation of the first type" to mean the transformation of formula_11, while the compensating transformation in formula_26 is called a "gauge transformation of the second type".
The difference between this Lagrangian and the original "globally gauge-invariant" Lagrangian is seen to be the interaction Lagrangian
This term introduces interactions between the "n" scalar fields just as a consequence of the demand for local gauge invariance. However, to make this interaction physical and not completely arbitrary, the mediator "A"("x") needs to propagate in space. That is dealt with in the next section by adding yet another term, formula_28, to the Lagrangian. In the quantized version of the obtained classical field theory, the quanta of the gauge field "A"("x") are called gauge bosons. The interpretation of the interaction Lagrangian in quantum field theory is of scalar bosons interacting by the exchange of these gauge bosons.
The Yang–Mills Lagrangian for the gauge field.
The picture of a classical gauge theory developed in the previous section is almost complete, except for the fact that to define the covariant derivatives "D", one needs to know the value of the gauge field formula_29 at all space-time points. Instead of manually specifying the values of this field, it can be given as the solution to a field equation. Further requiring that the Lagrangian that generates this field equation is locally gauge invariant as well, one possible form for the gauge field Lagrangian is (conventionally) written as
with
and the trace being taken over the vector space of the fields. This is called the Yang–Mills action. Other gauge invariant actions also exist (e.g., nonlinear electrodynamics, Born–Infeld action, Chern–Simons model, theta term, etc.).
Note that in this Lagrangian term there is no field whose transformation counterweighs the one of formula_26. Invariance of this term under gauge transformations is a particular case of "a priori" classical (geometrical) symmetry. This symmetry must be restricted in order to perform quantization, the procedure being denominated gauge fixing, but even after restriction, gauge transformations may be possible.
The complete Lagrangian for the gauge theory is now
An example: Electrodynamics.
As a simple application of the formalism developed in the previous sections, consider the case of electrodynamics, with only the electron field. The bare-bones action that generates the electron field's Dirac equation is
The global symmetry for this system is
The gauge group here is U(1), just rotations of the phase angle of the field, with the particular rotation determined by the constant "θ".
"Localising" this symmetry implies the replacement of θ by θ("x"). An appropriate covariant derivative is then
Identifying the "charge" "e" (not to be confused with the mathematical constant e in the symmetry description) with the usual electric charge (this is the origin of the usage of the term in gauge theories), and the gauge field "A"("x") with the four-vector potential of electromagnetic field results in an interaction Lagrangian
where formula_38 is the usual four vector electric current density. The gauge principle is therefore seen to naturally introduce the so-called minimal coupling of the electromagnetic field to the electron field.
Adding a Lagrangian for the gauge field formula_39 in terms of the field strength tensor exactly as in electrodynamics, one obtains the Lagrangian used as the starting point in quantum electrodynamics.
Mathematical formalism.
Gauge theories are usually discussed in the language of differential geometry. Mathematically, a "gauge" is just a choice of a (local) section of some principal bundle. A gauge transformation is just a transformation between two such sections.
Although gauge theory is dominated by the study of connections (primarily because it's mainly studied by high-energy physicists), the idea of a connection is not central to gauge theory in general. In fact, a result in general gauge theory shows that affine representations (i.e., affine modules) of the gauge transformations can be classified as sections of a jet bundle satisfying certain properties. There are representations that transform covariantly pointwise (called by physicists gauge transformations of the first kind), representations that transform as a connection form (called by physicists gauge transformations of the second kind, an affine representation)—and other more general representations, such as the B field in BF theory. There are more general nonlinear representations (realizations), but these are extremely complicated. Still, nonlinear sigma models transform nonlinearly, so there are applications.
If there is a principal bundle "P" whose base space is space or spacetime and structure group is a Lie group, then the sections of "P" form a principal homogeneous space of the group of gauge transformations.
Connections (gauge connection) define this principal bundle, yielding a covariant derivative ∇ in each associated vector bundle. If a local frame is chosen (a local basis of sections), then this covariant derivative is represented by the connection form "A", a Lie algebra-valued 1-form, which is called the gauge potential in physics. This is evidently not an intrinsic but a frame-dependent quantity. The curvature form "F", a Lie algebra-valued 2-form that is an intrinsic quantity, is constructed from a connection form by
where d stands for the exterior derivative and formula_42 stands for the wedge product. (formula_43 is an element of the vector space spanned by the generators formula_44, and so the components of formula_43 do not commute with one another. Hence the wedge product formula_46 does not vanish.)
Infinitesimal gauge transformations form a Lie algebra, which is characterized by a smooth Lie-algebra-valued scalar, ε. Under such an infinitesimal gauge transformation,
where formula_48 is the Lie bracket.
One nice thing is that if formula_49, then formula_50 where D is the covariant derivative
Also, formula_52, which means formula_53 transforms covariantly.
Not all gauge transformations can be generated by infinitesimal gauge transformations in general. An example is when the base manifold is a compact manifold without boundary such that the homotopy class of mappings from that manifold to the Lie group is nontrivial. See instanton for an example.
The "Yang–Mills action" is now given by
where * stands for the Hodge dual and the integral is defined as in differential geometry.
A quantity which is gauge-invariant (i.e., invariant under gauge transformations) is the Wilson loop, which is defined over any closed path, γ, as follows:
where χ is the character of a complex representation ρ and formula_56 represents the path-ordered operator.
Quantization of gauge theories.
Gauge theories may be quantized by specialization of methods which are applicable to any quantum field theory. However, because of the subtleties imposed by the gauge constraints (see section on Mathematical formalism, above) there are many technical problems to be solved which do not arise in other field theories. At the same time, the richer structure of gauge theories allows simplification of some computations: for example Ward identities connect different renormalization constants.
Methods and aims.
The first gauge theory quantized was quantum electrodynamics (QED). The first methods developed for this involved gauge fixing and then applying canonical quantization. The Gupta–Bleuler method was also developed to handle this problem. Non-abelian gauge theories are now handled by a variety of means. Methods for quantization are covered in the article on quantization.
The main point to quantization is to be able to compute quantum amplitudes for various processes allowed by the theory. Technically, they reduce to the computations of certain correlation functions in the vacuum state. This involves a renormalization of the theory.
When the running coupling of the theory is small enough, then all required quantities may be computed in perturbation theory. Quantization schemes intended to simplify such computations (such as canonical quantization) may be called perturbative quantization schemes. At present some of these methods lead to the most precise experimental tests of gauge theories.
However, in most gauge theories, there are many interesting questions which are non-perturbative. Quantization schemes suited to these problems (such as lattice gauge theory) may be called non-perturbative quantization schemes. Precise computations in such schemes often require supercomputing, and are therefore less well-developed currently than other schemes.
Anomalies.
Some of the symmetries of the classical theory are then seen not to hold in the quantum theory; a phenomenon called an anomaly. Among the most well known are:
Pure gauge.
A pure gauge is the set of field configurations obtained by a gauge transformation on the null-field configuration, i.e., a gauge-transform of zero. So it is a particular "gauge orbit" in the field configuration's space.
Thus, in the abelian case, where formula_57, the pure gauge is just the set of field configurations formula_58 for all .

</doc>
<doc id="715679" url="https://en.wikipedia.org/wiki?curid=715679" title="Gaugino">
Gaugino

In particle physics, a gaugino is the hypothetical superpartner of a gauge field, as predicted by gauge theory combined with supersymmetry. They are fermions.
In the minimal supersymmetric extension of the standard model the following gauginos exist:
Gauginos mix with higgsinos, the superpartners of the Higgs field's degrees of freedom, to form linear combinations ("mass eigenstates") called neutralinos (electrically neutral) and charginos (electrically charged). In many models the lightest supersymmetric particle (LSP), often a neutralino such as the photino, is stable. In that case it is a WIMP and a candidate for dark matter.

</doc>
<doc id="74964" url="https://en.wikipedia.org/wiki?curid=74964" title="Gauss's law">
Gauss's law

In physics, Gauss's law, also known as Gauss's flux theorem, is a law relating the distribution of electric charge to the resulting electric field.
The law was formulated by Carl Friedrich Gauss in 1835, but was not published until 1867. It is one of Maxwell's four equations, which form the basis of classical electrodynamics, the other three being Gauss's law for magnetism, Faraday's law of induction, and Ampère's law with Maxwell's correction. Gauss's law can be used to derive Coulomb's law, and vice versa.
Qualitative description.
In words, Gauss's law states that:
Gauss's law has a close mathematical similarity with a number of laws in other areas of physics, such as Gauss's law for magnetism and Gauss's law for gravity. In fact, any "inverse-square law" can be formulated in a way similar to Gauss's law: For example, Gauss's law itself is essentially equivalent to the inverse-square Coulomb's law, and Gauss's law for gravity is essentially equivalent to the inverse-square Newton's law of gravity.
Gauss's law is something of an electrical analogue of Ampère's law, which deals with magnetism.
The law can be expressed mathematically using vector calculus in integral form and differential form, both are equivalent since they are related by the divergence theorem, also called Gauss's theorem. Each of these forms in turn can also be expressed two ways: In terms of a relation between the electric field E and the total electric charge, or in terms of the electric displacement field D and the "free" electric charge.
Equation involving E field.
Gauss's law can be stated using either the electric field E or the electric displacement field D. This section shows some of the forms with E; the form with D is below, as are other forms with E.
Integral form.
Gauss's law may be expressed as:
where Φ"E" is the electric flux through a closed surface "S" enclosing any volume "V", "Q" is the total charge enclosed within "S", and "ε"0 is the electric constant. The electric flux Φ"E" is defined as a surface integral of the electric field:
where E is the electric field, dA is a vector representing an infinitesimal element of area, and · represents the dot product of two vectors.
Since the flux is defined as an "integral" of the electric field, this expression of Gauss's law is called the "integral form".
Applying the integral form.
If the electric field is known everywhere, Gauss's law makes it quite easy, in principle, to find the distribution of electric charge: The charge in any given region can be deduced by integrating the electric field to find the flux.
However, much more often, it is the reverse problem that needs to be solved: The electric charge distribution is known, and the electric field needs to be computed. This is much more difficult, since if you know the total flux through a given surface, that gives almost no information about the electric field, which (for all you know) could go in and out of the surface in arbitrarily complicated patterns.
An exception is if there is some symmetry in the situation, which mandates that the electric field passes through the surface in a uniform way. Then, if the total flux is known, the field itself can be deduced at every point. Common examples of symmetries which lend themselves to Gauss's law include cylindrical symmetry, planar symmetry, and spherical symmetry. See the article Gaussian surface for examples where these symmetries are exploited to compute electric fields.
Differential form.
By the divergence theorem, Gauss's law can alternatively be written in the "differential form":
where is the divergence of the electric field, ε0 is the electric constant, and "ρ" is the total electric charge density (charge per unit volume).
Equivalence of integral and differential forms.
The integral and differential forms are mathematically equivalent, by the divergence theorem. Here is the argument more specifically.
Equation involving D field.
Free, bound, and total charge.
The electric charge that arises in the simplest textbook situations would be classified as "free charge"—for example, the charge which is transferred in static electricity, or the charge on a capacitor plate. In contrast, "bound charge" arises only in the context of dielectric (polarizable) materials. (All materials are polarizable to some extent.) When such materials are placed in an external electric field, the electrons remain bound to their respective atoms, but shift a microscopic distance in response to the field, so that they're more on one side of the atom than the other. All these microscopic displacements add up to give a macroscopic net charge distribution, and this constitutes the "bound charge".
Although microscopically, all charge is fundamentally the same, there are often practical reasons for wanting to treat bound charge differently from free charge. The result is that the more "fundamental" Gauss's law, in terms of E (above), is sometimes put into the equivalent form below, which is in terms of D and the free charge only.
Integral form.
This formulation of Gauss's law states the total charge form:
where Φ"D" is the D-field flux through a surface "S" which encloses a volume "V", and "Q"free is the free charge contained in "V". The flux Φ"D" is defined analogously to the flux Φ"E" of the electric field E through "S":
Differential form.
The differential form of Gauss's law, involving free charge only, states:
where is the divergence of the electric displacement field, and "ρ"free is the free electric charge density.
Equation for linear materials.
In homogeneous, isotropic, nondispersive, linear materials, there is a simple relationship between E and D:
where "ε" is the permittivity of the material. For the case of vacuum (aka free space), "ε" = "ε"0. Under these circumstances, Gauss's law modifies to
for the integral form, and
for the differential form.
Relation to Coulomb's law.
Deriving Gauss's law from Coulomb's law.
Gauss's law can be derived from Coulomb's law.
References.
Jackson, John David (1998). Classical Electrodynamics, 3rd ed., New York: Wiley. ISBN 0-471-30932-X.

</doc>
<doc id="16704344" url="https://en.wikipedia.org/wiki?curid=16704344" title="Gauss's law for gravity">
Gauss's law for gravity

In physics, Gauss's law for gravity, also known as Gauss's flux theorem for gravity, is a law of physics that is essentially equivalent to Newton's law of universal gravitation. It is named after Carl Friedrich Gauss. Although Gauss's law for gravity is equivalent to Newton's law, there are many situations where Gauss's law for gravity offers a more convenient and simple way to do a calculation than Newton's law.
The form of Gauss's law for gravity is mathematically similar to Gauss's law for electrostatics, one of Maxwell's equations. Gauss's law for gravity has the same mathematical relation to Newton's law that Gauss's law for electricity bears to Coulomb's law. This is because both Newton's law and Coulomb's law describe inverse-square interaction in a 3-dimensional space.
Qualitative statement of the law.
The gravitational field g (also called gravitational acceleration) is a vector field – a vector at each point of space (and time). It is defined so that the gravitational force experienced by a particle is equal to the mass of the particle multiplied by the gravitational field at that point.
"Gravitational flux" is a surface integral of the gravitational field over a closed surface, analogous to how magnetic flux is a surface integral of the magnetic field.
Gauss's law for gravity states:
Integral form.
The integral form of Gauss's law for gravity states:
where
The left-hand side of this equation is called the flux of the gravitational field. Note that according to the law it is always negative (or zero), and never positive. This can be contrasted with Gauss's law for electricity, where the flux can be either positive or negative. The difference is because "charge" can be either positive or negative, while "mass" can only be positive.
Differential form.
The differential form of Gauss's law for gravity states
where formula_2 denotes divergence, "G" is the universal gravitational constant, and "ρ" is the mass density at each point.
Relation to the integral form.
The two forms of Gauss's law for gravity are mathematically equivalent. The divergence theorem states:
where "V" is a closed region bounded by a simple closed oriented surface ∂"V" and "dV" is an infinitesimal piece of the volume "V" (see volume integral for more details). The gravitational field g must be a continuously differentiable vector field defined on a neighborhood of "V".
Given also that
we can apply the divergence theorem to the integral form of Gauss's law for gravity, which becomes:
which can be rewritten:
This has to hold simultaneously for every possible volume "V"; the only way this can happen is if the integrands are equal. Hence we arrive at
which is the differential form of Gauss's law for gravity.
It is possible to derive the integral form from the differential form using the reverse of this method.
Although the two forms are equivalent, one or the other might be more convenient to use in a particular computation.
Relation to Newton's law.
Deriving Gauss's law from Newton's law.
Gauss's law for gravity can be derived from Newton's law of universal gravitation, which states that the gravitational field due to a point mass is:
where
A proof using vector calculus is shown in the box below. It is mathematically identical to the proof of Gauss's law (in electrostatics) starting from Coulomb's law.
Poisson's equation and gravitational potential.
Since the gravitational field has zero curl (equivalently, gravity is a conservative force) as mentioned above, it can be written as the gradient of a scalar potential, called the gravitational potential:
Then the differential form of Gauss's law for gravity becomes Poisson's equation:
This provides an alternate means of calculating the gravitational potential and gravitational field. Although computing g via Poisson's equation is mathematically equivalent to computing g directly from Gauss's law, one or the other approach may be an easier computation in a given situation.
In radially symmetric systems, the gravitational potential is a function of only one variable (namely, formula_11), and Poisson's equation becomes (see Del in cylindrical and spherical coordinates):
while the gravitational field is:
When solving the equation it should be taken into account that in the case of finite densities ∂ϕ/∂"r" has to be continuous at boundaries (discontinuities of the density), and zero for "r" = 0.
Applications.
Gauss's law can be used to easily derive the gravitational field in certain cases where a direct application of Newton's law would be more difficult (but not impossible). See the article Gaussian surface for more details on how these derivations are done. Three such applications are as follows:
Bouguer plate.
We can conclude (by using a "Gaussian pillbox") that for an infinite, flat plate (Bouguer plate) of any finite thickness, the gravitational field outside the plate is perpendicular to the plate, towards it, with magnitude 2"πG" times the mass per unit area, independent of the distance to the plate (see also gravity anomalies).
More generally, for a mass distribution with the density depending on one Cartesian coordinate "z" only, gravity for any "z" is 2"πG" times (the mass per unit area above "z", minus the mass per unit area below "z").
In particular, a combination of two equal parallel infinite plates does not produce any gravity inside.
Cylindrically symmetric mass distribution.
In the case of an infinite cylindrically symmetric mass distribution we can conclude (by using a cylindrical Gaussian surface) that the field strength at a distance "r" from the center is inward with a magnitude of 2"G"/"r" times the total mass per unit length at a smaller distance (from the axis), regardless of any masses at a larger distance.
For example, inside an infinite hollow cylinder, the field is zero.
Spherically symmetric mass distribution.
In the case of a spherically symmetric mass distribution we can conclude (by using a spherical Gaussian surface) that the field strength at a distance r from the center is inward with a magnitude of "G"/"r"2 times only the total mass within a smaller distance than r. All the mass at a greater distance than r from the center can be ignored.
For example, a hollow sphere does not produce any net gravity inside. The gravitational field inside is the same as if the hollow sphere were not there (i.e. the resultant field is that of any masses inside and outside the sphere only).
Although this follows in one or two lines of algebra from Gauss's law for gravity, it took Isaac Newton several pages of cumbersome calculus to derive it directly using his law of gravity; see the article shell theorem for this direct derivation.
Derivation from Lagrangian.
The Lagrangian density for Newtonian gravity is
Applying Hamilton's principle to this Lagrangian, the result is Gauss's law for gravity:
See Lagrangian (field theory) for details.
In fiction.
In Arthur C. Clarke's science fiction novel, "2010: Odyssey Two", while investigating the alien Monolith orbiting Jupiter, the "Leonov" 's chief scientist, Vasili Orlov, has engineer Curnow park one of the revived "Discovery" 's space pods a short distance from the Monolith's two-kilometer-long surface, recalling Bauguer's Anomaly, derived from Gauss's law. He remarks, "I've just remembered an exercise from one of my college astronomy courses - the gravitational attraction of an infinite flat plate. I never thought I'd have a chance of using it in real life." 

</doc>
<doc id="16094518" url="https://en.wikipedia.org/wiki?curid=16094518" title="Gauss's law for magnetism">
Gauss's law for magnetism

In physics, Gauss's law for magnetism is one of the four Maxwell's equations that underlie classical electrodynamics. It states that the magnetic field B has divergence equal to zero, in other words, that it is a solenoidal vector field. It is equivalent to the statement that magnetic monopoles do not exist. Rather than "magnetic charges", the basic entity for magnetism is the magnetic dipole. (Of course, if monopoles were ever found, the law would have to be modified, as elaborated below.)
Gauss's law for magnetism can be written in two forms, a "differential form" and an "integral form". These forms are equivalent due to the divergence theorem.
The name "Gauss's law for magnetism" is not universally used. The law is also called "Absence of free magnetic poles". (or some variant); one reference even explicitly says the law has "no name". It is also referred to as the "transversality requirement" because for plane waves it requires that the polarization be transverse to the direction of propagation.
Differential form.
The differential form for Gauss's law for magnetism is:
where ∇• denotes divergence, and B is the magnetic field.
Integral form.
The integral form of Gauss's law for magnetism states:
where "S" is any closed surface (see image right), and dA is a vector, whose magnitude is the area of an infinitesimal piece of the surface "S", and whose direction is the outward-pointing surface normal (see surface integral for more details).
The left-hand side of this equation is called the net flux of the magnetic field out of the surface, and Gauss's law for magnetism states that it is always zero.
The integral and differential forms of Gauss's law for magnetism are mathematically equivalent, due to the divergence theorem. That said, one or the other might be more convenient to use in a particular computation.
The law in this form states that for each volume element in space, there are exactly the same number of "magnetic field lines" entering and exiting the volume. No total "magnetic charge" can build up in any point in space. For example, the south pole of the magnet is exactly as strong as the north pole, and free-floating south poles without accompanying north poles (magnetic monopoles) are not allowed. In contrast, this is not true for other fields such as electric fields or gravitational fields, where total electric charge or mass can build up in a volume of space.
Vector potential.
Due to the Helmholtz decomposition theorem, Gauss's law for magnetism is equivalent to the following statement:
The vector field A is called the magnetic vector potential.
Note that there is more than one possible A which satisfies this equation for a given B field. In fact, there are infinitely many: any field of the form ∇φ can be added onto A to get an alternative choice for A, by the identity (see Vector calculus identities):
since the curl of a gradient is the zero vector field:
This arbitrariness in A is called gauge freedom.
Field lines.
The magnetic field B, like any vector field, can be depicted via field lines (also called "flux lines")-- that is, a set of curves whose direction corresponds to the direction of B, and whose areal density is proportional to the magnitude of B. Gauss's law for magnetism is equivalent to the statement that the field lines have neither a beginning nor an end: Each one either forms a closed loop, winds around forever without ever quite joining back up to itself exactly, or extends to infinity.
Modification if magnetic monopoles exist.
If magnetic monopoles were discovered, then Gauss's law for magnetism would state the divergence of B would be proportional to the "magnetic charge density" ρm, analogous to Gauss's law for electric field. For zero net magnetic charge density (ρm = 0), the original form of Gauss's magnetism law is the result.
The modified formula in SI units is not standard; in one variation, magnetic charge has units of webers, in another it has units of ampere-meters.
where "μ"0 is the vacuum permeability.
So far no magnetic monopoles have been found, despite extensive search.
History.
The equation formula_1 was one of Maxwell's original eight equations. However, the interpretation was somewhat different: Maxwell's A field directly corresponded to an important physical quantity which he believed corresponded to Faraday's "electrotonic state", while the modern interpretation emphasizes gauge freedom, the idea that there are many possible A fields, all equally valid.

</doc>
<doc id="285623" url="https://en.wikipedia.org/wiki?curid=285623" title="Gaussian curvature">
Gaussian curvature

In differential geometry, the Gaussian curvature or Gauss curvature "Κ" of a surface at a point is the product of the principal curvatures, "κ"1 and "κ"2, at the given point:
For example, a sphere of radius "r" has Gaussian curvature "1/r2" everywhere, and a flat plane and a cylinder have Gaussian curvature 0 everywhere. The Gaussian curvature can also be negative, as in the case of a hyperboloid or the inside of a torus.
Gaussian curvature is an "intrinsic" measure of curvature, depending only on distances that are measured on the surface, not on the way it is isometrically embedded in any space. This is the content of the Theorema egregium.
Gaussian curvature is named after Carl Friedrich Gauss who published the Theorema egregium in 1827.
Informal definition.
At any point on a surface we can find a normal vector which is at right angles to the surface; planes containing the normal are called "normal planes". The intersection of a normal plane and the surface will form a curve called a "normal section" and the curvature of this curve is the "normal curvature". For most points on most surfaces, different sections will have different curvatures; the maximum and minimum values of these are called the principal curvatures, call these κ1, κ2. The Gaussian curvature is the product of the two principal curvatures Κ = κ1 κ2.
The sign of the Gaussian curvature can be used to characterise the surface.
Most surfaces will contain regions of positive Gaussian curvature (elliptical points) and regions of negative Gaussian curvature separated by a curve of points with zero Gaussian curvature called a parabolic line.
Relation to geometries.
When a surface has a constant zero Gaussian curvature then it is a developable surface and the geometry of the surface is Euclidean geometry.
When a surface has a constant positive Gaussian curvature then it is a sphere and the geometry of the surface is spherical geometry.
When a surface has a constant negative Gaussian curvature then if this curvature then it is a pseudospherical surface and the geometry of the surface is hyperbolic geometry.
Further informal discussion.
In differential geometry, the two principal curvatures at a given point of a surface are the eigenvalues of the shape operator at the point. They measure how the surface bends by different amounts in different directions at that point. We represent the surface by the implicit function theorem as the graph of a function, "f", of two variables, in such a way that the point "p" is a critical point, i.e., the gradient of "f" vanishes (this can always be attained by a suitable rigid motion). Then the Gaussian curvature of the surface at "p" is the determinant of the Hessian matrix of "f" (being the product of the eigenvalues of the Hessian). (Recall that the Hessian is the 2-by-2 matrix of second derivatives.) This definition allows one immediately to grasp the distinction between cup/cap "versus" saddle point.
Alternative definitions.
It is also given by
where formula_3

</doc>
<doc id="245552" url="https://en.wikipedia.org/wiki?curid=245552" title="Gaussian function">
Gaussian function

In mathematics, a Gaussian function, often simply referred to as a Gaussian, is a function of the form:
for arbitrary real constants , and . It is named after the mathematician Carl Friedrich Gauss.
The graph of a Gaussian is a characteristic symmetric "bell curve" shape. The parameter is the height of the curve's peak, is the position of the center of the peak and (the standard deviation, sometimes called the Gaussian RMS width) controls the width of the "bell". 
Gaussian functions are widely used in statistics where they describe the normal distributions, in signal processing where they serve to define Gaussian filters, in image processing where two-dimensional Gaussians are used for Gaussian blurs, and in mathematics where they are used to solve heat equations and diffusion equations and to define the Weierstrass transform.
Properties.
Gaussian functions arise by composing the exponential function with a concave quadratic function. The Gaussian functions are thus those functions whose logarithm is a concave quadratic function.
The parameter is related to the full width at half maximum (FWHM) of the peak according to
Alternatively, the parameter can be interpreted by saying that the two inflection points of the function occur at and .
The "full width at tenth of maximum" (FWTM) for a Gaussian could be of interest and is 
Gaussian functions are analytic, and their limit as is 0 (for the above case of ).
Gaussian functions are among those functions that are elementary but lack elementary antiderivatives; the integral of the Gaussian function is the error function. Nonetheless their improper integrals over the whole real line can be evaluated exactly, using the Gaussian integral
and one obtains
This integral is 1 if and only if formula_6, and in this case the Gaussian is the probability density function of a normally distributed random variable with expected value and variance :
These Gaussians are plotted in the accompanying figure.
Gaussian functions centered at zero minimize the Fourier uncertainty principle.
The product of two Gaussian functions is a Gaussian, and the convolution of two Gaussian functions is also a Gaussian, with variance being the sum of the original variances: formula_8. The product of two Gaussian probability density functions, though, is not in general a Gaussian PDF.
Taking the Fourier transform (unitary, angular frequency convention) of a Gaussian function with parameters , and yields another Gaussian function, with parameters formula_9, and formula_10. So in particular the Gaussian functions with and formula_11 are kept fixed by the Fourier transform (they are eigenfunctions of the Fourier transform with eigenvalue 1).
A physical realization is that of the diffraction pattern: for example, a photographic slide whose transmissivity has a Gaussian variation is also a Gaussian function.
The fact that the Gaussian function is an eigenfunction of the continuous Fourier transform
allows us to derive the following interesting identity from the Poisson summation formula:
Integral of a Gaussian function.
The integral of an arbitrary Gaussian function is
An alternative form is
where "f" must be strictly positive for the integral to converge.
Proof.
The integral
for some real constants a, b, c > 0 can be calculated by putting it into the form of a Gaussian integral. First, the constant "a" can simply be factored out of the integral. Next, the variable of integration is changed from "x" to "y" = "x" - "b".
and then to formula_17
Then, using the Gaussian integral identity
we have
Two-dimensional Gaussian function.
In two dimensions, the power to which "e" is raised in the Gaussian function is any negative-definite quadratic form. Consequently, the level sets of the Gaussian will always be ellipses.
A particular example of a two-dimensional Gaussian function is
Here the coefficient "A" is the amplitude, "x"o,yo is the center and σ"x", σ"y" are the "x" and "y" spreads of the blob. The figure on the right was created using "A" = 1, "x"o = 0, "y"o = 0, σ"x" = σ"y" = 1.
The volume under the Gaussian function is given by
In general, a two-dimensional elliptical Gaussian function is expressed as
where the matrix
is positive-definite.
Using this formulation, the figure on the right can be created using "A" = 1, ("x"o, "y"o) = (0, 0), "a" = "c" = 1/2, "b" = 0.
Meaning of parameters for the general equation.
For the general form of the equation the coefficient "A" is the height of the peak and ("x"o, "y"o) is the center of the blob.
If we set
then we rotate the blob by a clockwise angle formula_28 (for counterclockwise rotation invert the signs in the b coefficient). This can be seen in the following examples:
Using the following Octave code one can easily see the effect of changing the parameters
Such functions are often used in image processing and in computational models of visual system function—see the articles on scale space and affine shn.
Also see multivariate normal distribution.
Multi-dimensional Gaussian function.
In an formula_29-dimensional space a Gaussian function can be defined as
where formula_31 is a column of formula_29 coordinates, formula_33 is a positive-definite formula_34 matrix, and formula_35 denotes transposition.
The integral of this Gaussian function over the whole formula_29-dimensional space is given as
It can be easily calculated by diagonalizing the matrix formula_33 and changing the integration variables to the eigenvectors of formula_33.
More generally a shifted Gaussian function is defined as
where formula_41 is the shift vector and the matrix formula_33 can be assumed to be symmetric, formula_43, and positive-definite. The following integrals with this function can be calculated with the same technique,
Gaussian profile estimation.
A number of fields such as stellar photometry, Gaussian beam characterization, and emission/absorption line spectroscopy work with sampled Gaussian functions and need to accurately estimate the height, position, and width parameters of the function. These are formula_48, formula_49, and formula_50 for a 1D Gaussian function, formula_33, formula_52, and formula_53 for a 2D Gaussian function. The most common method for estimating the profile parameters is to take the logarithm of the data and fit a parabola to the resulting data set. While this provides a simple least squares fitting procedure, the resulting algorithm is biased by excessively weighting small data values, and this can produce large errors in the profile estimate. One can partially compensate for this through weighted least squares estimation, in which the small data values are given small weights, but this too can be biased by allowing the tail of the Gaussian to dominate the fit. In order to remove the bias, one can instead use an iterative procedure in which the weights are updated at each iteration (see Iteratively reweighted least squares).
Once one has an algorithm for estimating the Gaussian function parameters, it is also important to know how accurate those estimates are. While an estimation algorithm can provide numerical estimates for the variance of each parameter (i.e. the variance of the estimated height, position, and width of the function), one can use Cramér–Rao bound theory to obtain an analytical expression for the lower bound on the parameter variances, given some assumptions about the data.
When these assumptions are satisfied, the following covariance matrix K applies for the 1D profile parameters formula_48, formula_49, and formula_50 under i.i.d. Gaussian noise and under Poisson noise:
where formula_58 is the width of the pixels used to sample the function, formula_59 is the quantum efficiency of the detector, and formula_60 indicates the standard deviation of the measurement noise. Thus, the individual variances for the parameters are, in the Gaussian noise case,
and in the Poisson noise case,
For the 2D profile parameters giving the amplitude formula_33, position formula_52, and width formula_53 of the profile, the following covariance matrices apply:
where the individual parameter variances are given by the diagonal elements of the covariance matrix.
Discrete Gaussian.
One may ask for a discrete analog to the Gaussian;
this is necessary in discrete applications,
particularly digital signal processing.
A simple answer is to sample the continuous Gaussian, yielding the sampled Gaussian kernel. However, this discrete function does not have the discrete analogs of the properties of the continuous function, and can lead to undesired effects, as described in the article scale space implementation.
An alternative approach is to use discrete Gaussian kernel:
where formula_69 denotes the modified Bessel functions of integer order.
This is the discrete analog of the continuous Gaussian in that it is the solution to the discrete diffusion equation (discrete space, continuous time), just as the continuous Gaussian is the solution to the continuous diffusion equation.
Applications.
Gaussian functions appear in many contexts in the natural sciences, the social sciences, mathematics, and engineering. Some examples include:

</doc>
<doc id="12024" url="https://en.wikipedia.org/wiki?curid=12024" title="General relativity">
General relativity

General relativity, also known as the general theory of relativity, is the geometric theory of gravitation published by Albert Einstein in 1915 and the current description of gravitation in modern physics. General relativity generalizes special relativity and Newton's law of universal gravitation, providing a unified description of gravity as a geometric property of space and time, or spacetime. In particular, the curvature of spacetime is directly related to the energy and momentum of whatever matter and radiation are present. The relation is specified by the Einstein field equations, a system of partial differential equations.
Some predictions of general relativity differ significantly from those of classical physics, especially concerning the passage of time, the geometry of space, the motion of bodies in free fall, and the propagation of light. Examples of such differences include gravitational time dilation, gravitational lensing, the gravitational redshift of light, and the gravitational time delay. The predictions of general relativity have been confirmed in all observations and experiments to date. Although general relativity is not the only relativistic theory of gravity, it is the simplest theory that is consistent with experimental data. However, unanswered questions remain, the most fundamental being how general relativity can be reconciled with the laws of quantum physics to produce a complete and self-consistent theory of quantum gravity.
Einstein's theory has important astrophysical implications. For example, it implies the existence of black holes—regions of space in which space and time are distorted in such a way that nothing, not even light, can escape—as an end-state for massive stars. There is ample evidence that the intense radiation emitted by certain kinds of astronomical objects is due to black holes; for example, microquasars and active galactic nuclei result from the presence of stellar black holes and black holes of a much more massive type, respectively. The bending of light by gravity can lead to the phenomenon of gravitational lensing, in which multiple images of the same distant astronomical object are visible in the sky. General relativity also predicts the existence of gravitational waves, which have since been observed indirectly; a direct measurement is the aim of projects such as LIGO and NASA/ESA Laser Interferometer Space Antenna and various pulsar timing arrays. In addition, general relativity is the basis of current cosmological models of a consistently expanding universe.
History.
Soon after publishing the special theory of relativity in 1905, Einstein started thinking about how to incorporate gravity into his new relativistic framework. In 1907, beginning with a simple thought experiment involving an observer in free fall, he embarked on what would be an eight-year search for a relativistic theory of gravity. After numerous detours and false starts, his work culminated in the presentation to the Prussian Academy of Science in November 1915 of what are now known as the Einstein field equations. These equations specify how the geometry of space and time is influenced by whatever matter and radiation are present, and form the core of Einstein's general theory of relativity.
The Einstein field equations are nonlinear and very difficult to solve. Einstein used approximation methods in working out initial predictions of the theory. But as early as 1916, the astrophysicist Karl Schwarzschild found the first non-trivial exact solution to the Einstein field equations, the so-called Schwarzschild metric. This solution laid the groundwork for the description of the final stages of gravitational collapse, and the objects known today as black holes. In the same year, the first steps towards generalizing Schwarzschild's solution to electrically charged objects were taken, which eventually resulted in the Reissner–Nordström solution, now associated with electrically charged black holes. In 1917, Einstein applied his theory to the universe as a whole, initiating the field of relativistic cosmology. In line with contemporary thinking, he assumed a static universe, adding a new parameter to his original field equations—the cosmological constant—to match that observational presumption. By 1929, however, the work of Hubble and others had shown that our universe is expanding. This is readily described by the expanding cosmological solutions found by Friedmann in 1922, which do not require a cosmological constant. Lemaître used these solutions to formulate the earliest version of the Big Bang models, in which our universe has evolved from an extremely hot and dense earlier state. Einstein later declared the cosmological constant the biggest blunder of his life.
During that period, general relativity remained something of a curiosity among physical theories. It was clearly superior to Newtonian gravity, being consistent with special relativity and accounting for several effects unexplained by the Newtonian theory. Einstein himself had shown in 1915 how his theory explained the anomalous perihelion advance of the planet Mercury without any arbitrary parameters ("fudge factors"). Similarly, a 1919 expedition led by Eddington confirmed general relativity's prediction for the deflection of starlight by the Sun during the total solar eclipse of May 29, 1919, making Einstein instantly famous. Yet the theory entered the mainstream of theoretical physics and astrophysics only with the developments between approximately 1960 and 1975, now known as the golden age of general relativity. Physicists began to understand the concept of a black hole, and to identify quasars as one of these objects' astrophysical manifestations. Ever more precise solar system tests confirmed the theory's predictive power, and relativistic cosmology, too, became amenable to direct observational tests.
From classical mechanics to general relativity.
General relativity can be understood by examining its similarities with and departures from classical physics. The first step is the realization that classical mechanics and Newton's law of gravity admit a geometric description. The combination of this description with the laws of special relativity results in a heuristic derivation of general relativity.
Geometry of Newtonian gravity.
At the base of classical mechanics is the notion that a body's motion can be described as a combination of free (or inertial) motion, and deviations from this free motion. Such deviations are caused by external forces acting on a body in accordance with Newton's second law of motion, which states that the net force acting on a body is equal to that body's (inertial) mass multiplied by its acceleration. The preferred inertial motions are related to the geometry of space and time: in the standard reference frames of classical mechanics, objects in free motion move along straight lines at constant speed. In modern parlance, their paths are geodesics, straight world lines in curved spacetime.
Conversely, one might expect that inertial motions, once identified by observing the actual motions of bodies and making allowances for the external forces (such as electromagnetism or friction), can be used to define the geometry of space, as well as a time coordinate. However, there is an ambiguity once gravity comes into play. According to Newton's law of gravity, and independently verified by experiments such as that of Eötvös and its successors (see Eötvös experiment), there is a universality of free fall (also known as the weak equivalence principle, or the universal equality of inertial and passive-gravitational mass): the trajectory of a test body in free fall depends only on its position and initial speed, but not on any of its material properties. A simplified version of this is embodied in Einstein's elevator experiment, illustrated in the figure on the right: for an observer in a small enclosed room, it is impossible to decide, by mapping the trajectory of bodies such as a dropped ball, whether the room is at rest in a gravitational field, or in free space aboard a rocket that is accelerating at a rate equal to that of the gravitational field.
Given the universality of free fall, there is no observable distinction between inertial motion and motion under the influence of the gravitational force. This suggests the definition of a new class of inertial motion, namely that of objects in free fall under the influence of gravity. This new class of preferred motions, too, defines a geometry of space and time—in mathematical terms, it is the geodesic motion associated with a specific connection which depends on the gradient of the gravitational potential. Space, in this construction, still has the ordinary Euclidean geometry. However, space"time" as a whole is more complicated. As can be shown using simple thought experiments following the free-fall trajectories of different test particles, the result of transporting spacetime vectors that can denote a particle's velocity (time-like vectors) will vary with the particle's trajectory; mathematically speaking, the Newtonian connection is not integrable. From this, one can deduce that spacetime is curved. The result is a geometric formulation of Newtonian gravity using only covariant concepts, i.e. a description which is valid in any desired coordinate system. In this geometric description, tidal effects—the relative acceleration of bodies in free fall—are related to the derivative of the connection, showing how the modified geometry is caused by the presence of mass.
Relativistic generalization.
As intriguing as geometric Newtonian gravity may be, its basis, classical mechanics, is merely a limiting case of (special) relativistic mechanics. In the language of symmetry: where gravity can be neglected, physics is Lorentz invariant as in special relativity rather than Galilei invariant as in classical mechanics. (The defining symmetry of special relativity is the Poincaré group, which includes translations and rotations.) The differences between the two become significant when dealing with speeds approaching the speed of light, and with high-energy phenomena.
With Lorentz symmetry, additional structures come into play. They are defined by the set of light cones (see image). The light-cones define a causal structure: for each event A, there is a set of events that can, in principle, either influence or be influenced by A via signals or interactions that do not need to travel faster than light (such as event B in the image), and a set of events for which such an influence is impossible (such as event C in the image). These sets are observer-independent. In conjunction with the world-lines of freely falling particles, the light-cones can be used to reconstruct the space–time's semi-Riemannian metric, at least up to a positive scalar factor. In mathematical terms, this defines a conformal structure, or much better, a conformal geometry, as it is difficult to understand how space or time or space-time can have a structure.
Special relativity is defined in the absence of gravity, so for practical applications, it is a suitable model whenever gravity can be neglected. Bringing gravity into play, and assuming the universality of free fall, an analogous reasoning as in the previous section applies: there are no global inertial frames. Instead there are approximate inertial frames moving alongside freely falling particles. Translated into the language of spacetime: the straight time-like lines that define a gravity-free inertial frame are deformed to lines that are curved relative to each other, suggesting that the inclusion of gravity necessitates a change in spacetime geometry.
A priori, it is not clear whether the new local frames in free fall coincide with the reference frames in which the laws of special relativity hold—that theory is based on the propagation of light, and thus on electromagnetism, which could have a different set of preferred frames. But using different assumptions about the special-relativistic frames (such as their being earth-fixed, or in free fall), one can derive different predictions for the gravitational redshift, that is, the way in which the frequency of light shifts as the light propagates through a gravitational field (cf. below). The actual measurements show that free-falling frames are the ones in which light propagates as it does in special relativity. The generalization of this statement, namely that the laws of special relativity hold to good approximation in freely falling (and non-rotating) reference frames, is known as the Einstein equivalence principle, a crucial guiding principle for generalizing special-relativistic physics to include gravity.
The same experimental data shows that time as measured by clocks in a gravitational field—proper time, to give the technical term—does not follow the rules of special relativity. In the language of spacetime geometry, it is not measured by the Minkowski metric. As in the Newtonian case, this is suggestive of a more general geometry. At small scales, all reference frames that are in free fall are equivalent, and approximately Minkowskian. Consequently, we are now dealing with a curved generalization of Minkowski space. The metric tensor that defines the geometry—in particular, how lengths and angles are measured—is not the Minkowski metric of special relativity, it is a generalization known as a semi- or pseudo-Riemannian metric. Furthermore, each Riemannian metric is naturally associated with one particular kind of connection, the Levi-Civita connection, and this is, in fact, the connection that satisfies the equivalence principle and makes space locally Minkowskian (that is, in suitable locally inertial coordinates, the metric is Minkowskian, and its first partial derivatives and the connection coefficients vanish).
Einstein's equations.
Having formulated the relativistic, geometric version of the effects of gravity, the question of gravity's source remains. In Newtonian gravity, the source is mass. In special relativity, mass turns out to be part of a more general quantity called the energy–momentum tensor, which includes both energy and momentum densities as well as stress (that is, pressure and shear). Using the equivalence principle, this tensor is readily generalized to curved space-time. Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, which describes a particular class of tidal effects: the change in volume for a small cloud of test particles that are initially at rest, and then fall freely. In special relativity, conservation of energy–momentum corresponds to the statement that the energy–momentum tensor is divergence-free. This formula, too, is readily generalized to curved spacetime by replacing partial derivatives with their curved-manifold counterparts, covariant derivatives studied in differential geometry. With this additional condition—the covariant divergence of the energy–momentum tensor, and hence of whatever is on the other side of the equation, is zero— the simplest set of equations are what are called Einstein's (field) equations:
On the left-hand side is the Einstein tensor, a specific divergence-free combination of the Ricci tensor formula_1 and the metric. Where formula_2 is symmetric. In particular,
is the curvature scalar. The Ricci tensor itself is related to the more general Riemann curvature tensor as
On the right-hand side, "formula_5" is the energy–momentum tensor. All tensors are written in abstract index notation. Matching the theory's prediction to observational results for planetary orbits (or, equivalently, assuring that the weak-gravity, low-speed limit is Newtonian mechanics), the proportionality constant can be fixed as κ = 8π"G"/"c"4, with "G" the gravitational constant and "c" the speed of light. When there is no matter present, so that the energy–momentum tensor vanishes, the results are the vacuum Einstein equations,
There are alternatives to general relativity built upon the same premises, which include additional rules and/or constraints, leading to different field equations. Examples are Brans–Dicke theory, teleparallelism, and Einstein–Cartan theory.
Definition and basic applications.
The derivation outlined in the previous section contains all the information needed to define general relativity, describe its key properties, and address a question of crucial importance in physics, namely how the theory can be used for model-building.
Definition and basic properties.
General relativity is a metric theory of gravitation. At its core are Einstein's equations, which describe the relation between the geometry of a four-dimensional, pseudo-Riemannian manifold representing spacetime, and the energy–momentum contained in that spacetime. Phenomena that in classical mechanics are ascribed to the action of the force of gravity (such as free-fall, orbital motion, and spacecraft trajectories), correspond to inertial motion within a curved geometry of spacetime in general relativity; there is no gravitational force deflecting objects from their natural, straight paths. Instead, gravity corresponds to changes in the properties of space and time, which in turn changes the straightest-possible paths that objects will naturally follow. The curvature is, in turn, caused by the energy–momentum of matter. Paraphrasing the relativist John Archibald Wheeler, spacetime tells matter how to move; matter tells spacetime how to curve.
While general relativity replaces the scalar gravitational potential of classical physics by a symmetric rank-two tensor, the latter reduces to the former in certain limiting cases. For weak gravitational fields and slow speed relative to the speed of light, the theory's predictions converge on those of Newton's law of universal gravitation.
As it is constructed using tensors, general relativity exhibits general covariance: its laws—and further laws formulated within the general relativistic framework—take on the same form in all coordinate systems. Furthermore, the theory does not contain any invariant geometric background structures, i.e. it is background independent. It thus satisfies a more stringent general principle of relativity, namely that the laws of physics are the same for all observers. Locally, as expressed in the equivalence principle, spacetime is Minkowskian, and the laws of physics exhibit local Lorentz invariance.
Model-building.
The core concept of general-relativistic model-building is that of a solution of Einstein's equations. Given both Einstein's equations and suitable equations for the properties of matter, such a solution consists of a specific semi-Riemannian manifold (usually defined by giving the metric in specific coordinates), and specific matter fields defined on that manifold. Matter and geometry must satisfy Einstein's equations, so in particular, the matter's energy–momentum tensor must be divergence-free. The matter must, of course, also satisfy whatever additional equations were imposed on its properties. In short, such a solution is a model universe that satisfies the laws of general relativity, and possibly additional laws governing whatever matter might be present.
Einstein's equations are nonlinear partial differential equations and, as such, difficult to solve exactly. Nevertheless, a number of exact solutions are known, although only a few have direct physical applications. The best-known exact solutions, and also those most interesting from a physics point of view, are the Schwarzschild solution, the Reissner–Nordström solution and the Kerr metric, each corresponding to a certain type of black hole in an otherwise empty universe, and the Friedmann–Lemaître–Robertson–Walker and de Sitter universes, each describing an expanding cosmos. Exact solutions of great theoretical interest include the Gödel universe (which opens up the intriguing possibility of time travel in curved spacetimes), the Taub-NUT solution (a model universe that is homogeneous, but anisotropic), and anti-de Sitter space (which has recently come to prominence in the context of what is called the Maldacena conjecture).
Given the difficulty of finding exact solutions, Einstein's field equations are also solved frequently by numerical integration on a computer, or by considering small perturbations of exact solutions. In the field of numerical relativity, powerful computers are employed to simulate the geometry of spacetime and to solve Einstein's equations for interesting situations such as two colliding black holes. In principle, such methods may be applied to any system, given sufficient computer resources, and may address fundamental questions such as naked singularities. Approximate solutions may also be found by perturbation theories such as linearized gravity and its generalization, the post-Newtonian expansion, both of which were developed by Einstein. The latter provides a systematic approach to solving for the geometry of a spacetime that contains a distribution of matter that moves slowly compared with the speed of light. The expansion involves a series of terms; the first terms represent Newtonian gravity, whereas the later terms represent ever smaller corrections to Newton's theory due to general relativity. An extension of this expansion is the parametrized post-Newtonian (PPN) formalism, which allows quantitative comparisons between the predictions of general relativity and alternative theories.
Consequences of Einstein's theory.
General relativity has a number of physical consequences. Some follow directly from the theory's axioms, whereas others have become clear only in the course of many years of research that followed Einstein's initial publication.
Gravitational time dilation and frequency shift.
Assuming that the equivalence principle holds, gravity influences the passage of time. Light sent down into a gravity well is blueshifted, whereas light sent in the opposite direction (i.e., climbing out of the gravity well) is redshifted; collectively, these two effects are known as the gravitational frequency shift. More generally, processes close to a massive body run more slowly when compared with processes taking place farther away; this effect is known as gravitational time dilation.
Gravitational redshift has been measured in the laboratory and using astronomical observations. Gravitational time dilation in the Earth's gravitational field has been measured numerous times using atomic clocks, while ongoing validation is provided as a side effect of the operation of the Global Positioning System (GPS). Tests in stronger gravitational fields are provided by the observation of binary pulsars. All results are in agreement with general relativity. However, at the current level of accuracy, these observations cannot distinguish between general relativity and other theories in which the equivalence principle is valid.
Light deflection and gravitational time delay.
General relativity predicts that the path of light is bent in a gravitational field; light passing a massive body is deflected towards that body. This effect has been confirmed by observing the light of stars or distant quasars being deflected as it passes the Sun.
This and related predictions follow from the fact that light follows what is called a light-like or null geodesic—a generalization of the straight lines along which light travels in classical physics. Such geodesics are the generalization of the invariance of lightspeed in special relativity. As one examines suitable model spacetimes (either the exterior Schwarzschild solution or, for more than a single mass, the post-Newtonian expansion), several effects of gravity on light propagation emerge. Although the bending of light can also be derived by extending the universality of free fall to light, the angle of deflection resulting from such calculations is only half the value given by general relativity.
Closely related to light deflection is the gravitational time delay (or Shapiro delay), the phenomenon that light signals take longer to move through a gravitational field than they would in the absence of that field. There have been numerous successful tests of this prediction. In the parameterized post-Newtonian formalism (PPN), measurements of both the deflection of light and the gravitational time delay determine a parameter called γ, which encodes the influence of gravity on the geometry of space.
Gravitational waves.
One of several analogies between weak-field gravity and electromagnetism is that, analogous to electromagnetic waves, there are gravitational waves: ripples in the metric of spacetime that propagate at the speed of light. The simplest type of such a wave can be visualized by its action on a ring of freely floating particles. A sine wave propagating through such a ring towards the reader distorts the ring in a characteristic, rhythmic fashion (animated image to the right). Since Einstein's equations are non-linear, arbitrarily strong gravitational waves do not obey linear superposition, making their description difficult. However, for weak fields, a linear approximation can be made. Such linearized gravitational waves are sufficiently accurate to describe the exceedingly weak waves that are expected to arrive here on Earth from far-off cosmic events, which typically result in relative distances increasing and decreasing by formula_7 or less. Data analysis methods routinely make use of the fact that these linearized waves can be Fourier decomposed.
Some exact solutions describe gravitational waves without any approximation, e.g., a wave train traveling through empty space or so-called Gowdy universes, varieties of an expanding cosmos filled with gravitational waves. But for gravitational waves produced in astrophysically relevant situations, such as the merger of two black holes, numerical methods are presently the only way to construct appropriate models.
Orbital effects and the relativity of direction.
General relativity differs from classical mechanics in a number of predictions concerning orbiting bodies. It predicts an overall rotation (precession) of planetary orbits, as well as orbital decay caused by the emission of gravitational waves and effects related to the relativity of direction.
Precession of apsides.
In general relativity, the apsides of any orbit (the point of the orbiting body's closest approach to the system's center of mass) will precess—the orbit is not an ellipse, but akin to an ellipse that rotates on its focus, resulting in a rose curve-like shape (see image). Einstein first derived this result by using an approximate metric representing the Newtonian limit and treating the orbiting body as a test particle. For him, the fact that his theory gave a straightforward explanation of the anomalous perihelion shift of the planet Mercury, discovered earlier by Urbain Le Verrier in 1859, was important evidence that he had at last identified the correct form of the gravitational field equations.
The effect can also be derived by using either the exact Schwarzschild metric (describing spacetime around a spherical mass) or the much more general post-Newtonian formalism. It is due to the influence of gravity on the geometry of space and to the contribution of self-energy to a body's gravity (encoded in the nonlinearity of Einstein's equations). Relativistic precession has been observed for all planets that allow for accurate precession measurements (Mercury, Venus, and Earth), as well as in binary pulsar systems, where it is larger by five orders of magnitude.
Orbital decay.
According to general relativity, a binary system will emit gravitational waves, thereby losing energy. Due to this loss, the distance between the two orbiting bodies decreases, and so does their orbital period. Within the Solar System or for ordinary double stars, the effect is too small to be observable. This is not the case for a close binary pulsar, a system of two orbiting neutron stars, one of which is a pulsar: from the pulsar, observers on Earth receive a regular series of radio pulses that can serve as a highly accurate clock, which allows precise measurements of the orbital period. Because neutron stars are very compact, significant amounts of energy are emitted in the form of gravitational radiation.
The first observation of a decrease in orbital period due to the emission of gravitational waves was made by Hulse and Taylor, using the binary pulsar PSR1913+16 they had discovered in 1974. This was the first detection of gravitational waves, albeit indirect, for which they were awarded the 1993 Nobel Prize in physics. Since then, several other binary pulsars have been found, in particular the double pulsar PSR J0737-3039, in which both stars are pulsars.
Geodetic precession and frame-dragging.
Several relativistic effects are directly related to the relativity of direction. One is geodetic precession: the axis direction of a gyroscope in free fall in curved spacetime will change when compared, for instance, with the direction of light received from distant stars—even though such a gyroscope represents the way of keeping a direction as stable as possible ("parallel transport"). For the Moon–Earth system, this effect has been measured with the help of lunar laser ranging. More recently, it has been measured for test masses aboard the satellite Gravity Probe B to a precision of better than 0.3%.
Near a rotating mass, there are so-called gravitomagnetic or frame-dragging effects. A distant observer will determine that objects close to the mass get "dragged around". This is most extreme for rotating black holes where, for any object entering a zone known as the ergosphere, rotation is inevitable. Such effects can again be tested through their influence on the orientation of gyroscopes in free fall. Somewhat controversial tests have been performed using the LAGEOS satellites, confirming the relativistic prediction. Also the Mars Global Surveyor probe around Mars has been used.
Astrophysical applications.
Gravitational lensing.
The deflection of light by gravity is responsible for a new class of astronomical phenomena. If a massive object is situated between the astronomer and a distant target object with appropriate mass and relative distances, the astronomer will see multiple distorted images of the target. Such effects are known as gravitational lensing. Depending on the configuration, scale, and mass distribution, there can be two or more images, a bright ring known as an Einstein ring, or partial rings called arcs.
The earliest example was discovered in 1979; since then, more than a hundred gravitational lenses have been observed. Even if the multiple images are too close to each other to be resolved, the effect can still be measured, e.g., as an overall brightening of the target object; a number of such "microlensing events" have been observed.
Gravitational lensing has developed into a tool of observational astronomy. It is used to detect the presence and distribution of dark matter, provide a "natural telescope" for observing distant galaxies, and to obtain an independent estimate of the Hubble constant. Statistical evaluations of lensing data provide valuable insight into the structural evolution of galaxies.
Gravitational wave astronomy.
Observations of binary pulsars provide strong indirect evidence for the existence of gravitational waves (see Orbital decay, above). However, gravitational waves reaching us from the depths of the cosmos have not been detected directly. Such detection is a major goal of current relativity-related research. Several land-based gravitational wave detectors are currently in operation, most notably the interferometric detectors GEO 600, LIGO (two detectors), TAMA 300 and VIRGO. Various pulsar timing arrays are using millisecond pulsars to detect gravitational waves in the 10−9 to 10−6 Hertz frequency range, which originate from binary supermassive blackholes. European space-based detector, eLISA / NGO, is currently under development, with a precursor mission (LISA Pathfinder) due for launch in 2015.
Observations of gravitational waves promise to complement observations in the electromagnetic spectrum. They are expected to yield information about black holes and other dense objects such as neutron stars and white dwarfs, about certain kinds of supernova implosions, and about processes in the very early universe, including the signature of certain types of hypothetical cosmic string.
Black holes and other compact objects.
Whenever the ratio of an object's mass to its radius becomes sufficiently large, general relativity predicts the formation of a black hole, a region of space from which nothing, not even light, can escape. In the currently accepted models of stellar evolution, neutron stars of around 1.4 solar masses, and stellar black holes with a few to a few dozen solar masses, are thought to be the final state for the evolution of massive stars. Usually a galaxy has one supermassive black hole with a few million to a few billion solar masses in its center, and its presence is thought to have played an important role in the formation of the galaxy and larger cosmic structures.
Astronomically, the most important property of compact objects is that they provide a supremely efficient mechanism for converting gravitational energy into electromagnetic radiation. Accretion, the falling of dust or gaseous matter onto stellar or supermassive black holes, is thought to be responsible for some spectacularly luminous astronomical objects, notably diverse kinds of active galactic nuclei on galactic scales and stellar-size objects such as microquasars. In particular, accretion can lead to relativistic jets, focused beams of highly energetic particles that are being flung into space at almost light speed.
General relativity plays a central role in modelling all these phenomena, and observations provide strong evidence for the existence of black holes with the properties predicted by the theory.
Black holes are also sought-after targets in the search for gravitational waves (cf. Gravitational waves, above). Merging black hole binaries should lead to some of the strongest gravitational wave signals reaching detectors here on Earth, and the phase directly before the merger ("chirp") could be used as a "standard candle" to deduce the distance to the merger events–and hence serve as a probe of cosmic expansion at large distances. The gravitational waves produced as a stellar black hole plunges into a supermassive one should provide direct information about the supermassive black hole's geometry.
Cosmology.
The current models of cosmology are based on Einstein's field equations, which include the cosmological constant Λ since it has important influence on the large-scale dynamics of the cosmos,
where "formula_9" is the spacetime metric. Isotropic and homogeneous solutions of these enhanced equations, the Friedmann–Lemaître–Robertson–Walker solutions, allow physicists to model a universe that has evolved over the past 14 billion years from a hot, early Big Bang phase. Once a small number of parameters (for example the universe's mean matter density) have been fixed by astronomical observation, further observational data can be used to put the models to the test. Predictions, all successful, include the initial abundance of chemical elements formed in a period of primordial nucleosynthesis, the large-scale structure of the universe, and the existence and properties of a "thermal echo" from the early cosmos, the cosmic background radiation.
Astronomical observations of the cosmological expansion rate allow the total amount of matter in the universe to be estimated, although the nature of that matter remains mysterious in part. About 90% of all matter appears to be so-called dark matter, which has mass (or, equivalently, gravitational influence), but does not interact electromagnetically and, hence, cannot be observed directly. There is no generally accepted description of this new kind of matter, within the framework of known particle physics or otherwise. Observational evidence from redshift surveys of distant supernovae and measurements of the cosmic background radiation also show that the evolution of our universe is significantly influenced by a cosmological constant resulting in an acceleration of cosmic expansion or, equivalently, by a form of energy with an unusual equation of state, known as dark energy, the nature of which remains unclear.
A so-called inflationary phase, an additional phase of strongly accelerated expansion at cosmic times of around formula_10 seconds, was hypothesized in 1980 to account for several puzzling observations that were unexplained by classical cosmological models, such as the nearly perfect homogeneity of the cosmic background radiation. Recent measurements of the cosmic background radiation have resulted in the first evidence for this scenario. However, there is a bewildering variety of possible inflationary scenarios, which cannot be restricted by current observations. An even larger question is the physics of the earliest universe, prior to the inflationary phase and close to where the classical models predict the big bang singularity. An authoritative answer would require a complete theory of quantum gravity, which has not yet been developed (cf. the section on quantum gravity, below).
Time travel.
Kurt Gödel showed that solutions to Einstein's equations exist that contain closed timelike curves (CTCs), which allow for loops in time. The solutions require extreme physical conditions unlikely ever to occur in practice, and it remains an open question whether further laws of physics will eliminate them completely. Since then other—similarly impractical—GR solutions containing CTCs have been found, such as the Tipler cylinder and traversable wormholes.
Advanced concepts.
Causal structure and global geometry.
In general relativity, no material body can catch up with or overtake a light pulse. No influence from an event A can reach any other location X before light sent out at A to X. In consequence, an exploration of all light worldlines (null geodesics) yields key information about the spacetime's causal structure. This structure can be displayed using Penrose–Carter diagrams in which infinitely large regions of space and infinite time intervals are shrunk ("compactified") so as to fit onto a finite map, while light still travels along diagonals as in standard spacetime diagrams.
Aware of the importance of causal structure, Roger Penrose and others developed what is known as global geometry. In global geometry, the object of study is not one particular solution (or family of solutions) to Einstein's equations. Rather, relations that hold true for all geodesics, such as the Raychaudhuri equation, and additional non-specific assumptions about the nature of matter (usually in the form of so-called energy conditions) are used to derive general results.
Horizons.
Using global geometry, some spacetimes can be shown to contain boundaries called horizons, which demarcate one region from the rest of spacetime. The best-known examples are black holes: if mass is compressed into a sufficiently compact region of space (as specified in the hoop conjecture, the relevant length scale is the Schwarzschild radius), no light from inside can escape to the outside. Since no object can overtake a light pulse, all interior matter is imprisoned as well. Passage from the exterior to the interior is still possible, showing that the boundary, the black hole's "horizon", is not a physical barrier.
Early studies of black holes relied on explicit solutions of Einstein's equations, notably the spherically symmetric Schwarzschild solution (used to describe a static black hole) and the axisymmetric Kerr solution (used to describe a rotating, stationary black hole, and introducing interesting features such as the ergosphere). Using global geometry, later studies have revealed more general properties of black holes. In the long run, they are rather simple objects characterized by eleven parameters specifying energy, linear momentum, angular momentum, location at a specified time and electric charge. This is stated by the black hole uniqueness theorems: "black holes have no hair", that is, no distinguishing marks like the hairstyles of humans. Irrespective of the complexity of a gravitating object collapsing to form a black hole, the object that results (having emitted gravitational waves) is very simple.
Even more remarkably, there is a general set of laws known as black hole mechanics, which is analogous to the laws of thermodynamics. For instance, by the second law of black hole mechanics, the area of the event horizon of a general black hole will never decrease with time, analogous to the entropy of a thermodynamic system. This limits the energy that can be extracted by classical means from a rotating black hole (e.g. by the Penrose process). There is strong evidence that the laws of black hole mechanics are, in fact, a subset of the laws of thermodynamics, and that the black hole area is proportional to its entropy. This leads to a modification of the original laws of black hole mechanics: for instance, as the second law of black hole mechanics becomes part of the second law of thermodynamics, it is possible for black hole area to decrease—as long as other processes ensure that, overall, entropy increases. As thermodynamical objects with non-zero temperature, black holes should emit thermal radiation. Semi-classical calculations indicate that indeed they do, with the surface gravity playing the role of temperature in Planck's law. This radiation is known as Hawking radiation (cf. the quantum theory section, below).
There are other types of horizons. In an expanding universe, an observer may find that some regions of the past cannot be observed ("particle horizon"), and some regions of the future cannot be influenced (event horizon). Even in flat Minkowski space, when described by an accelerated observer (Rindler space), there will be horizons associated with a semi-classical radiation known as Unruh radiation.
Singularities.
Another general feature of general relativity is the appearance of spacetime boundaries known as singularities. Spacetime can be explored by following up on timelike and lightlike geodesics—all possible ways that light and particles in free fall can travel. But some solutions of Einstein's equations have "ragged edges"—regions known as spacetime singularities, where the paths of light and falling particles come to an abrupt end, and geometry becomes ill-defined. In the more interesting cases, these are "curvature singularities", where geometrical quantities characterizing spacetime curvature, such as the Ricci scalar, take on infinite values. Well-known examples of spacetimes with future singularities—where worldlines end—are the Schwarzschild solution, which describes a singularity inside an eternal static black hole, or the Kerr solution with its ring-shaped singularity inside an eternal rotating black hole. The Friedmann–Lemaître–Robertson–Walker solutions and other spacetimes describing universes have past singularities on which worldlines begin, namely Big Bang singularities, and some have future singularities (Big Crunch) as well.
Given that these examples are all highly symmetric—and thus simplified—it is tempting to conclude that the occurrence of singularities is an artifact of idealization. The famous singularity theorems, proved using the methods of global geometry, say otherwise: singularities are a generic feature of general relativity, and unavoidable once the collapse of an object with realistic matter properties has proceeded beyond a certain stage and also at the beginning of a wide class of expanding universes. However, the theorems say little about the properties of singularities, and much of current research is devoted to characterizing these entities' generic structure (hypothesized e.g. by the so-called BKL conjecture). The cosmic censorship hypothesis states that all realistic future singularities (no perfect symmetries, matter with realistic properties) are safely hidden away behind a horizon, and thus invisible to all distant observers. While no formal proof yet exists, numerical simulations offer supporting evidence of its validity.
Evolution equations.
Each solution of Einstein's equation encompasses the whole history of a universe — it is not just some snapshot of how things are, but a whole, possibly matter-filled, spacetime. It describes the state of matter and geometry everywhere and at every moment in that particular universe. Due to its general covariance, Einstein's theory is not sufficient by itself to determine the time evolution of the metric tensor. It must be combined with a coordinate condition, which is analogous to gauge fixing in other field theories.
To understand Einstein's equations as partial differential equations, it is helpful to formulate them in a way that describes the evolution of the universe over time. This is done in so-called "3+1" formulations, where spacetime is split into three space dimensions and one time dimension. The best-known example is the ADM formalism. These decompositions show that the spacetime evolution equations of general relativity are well-behaved: solutions always exist, and are uniquely defined, once suitable initial conditions have been specified. Such formulations of Einstein's field equations are the basis of numerical relativity.
Global and quasi-local quantities.
The notion of evolution equations is intimately tied in with another aspect of general relativistic physics. In Einstein's theory, it turns out to be impossible to find a general definition for a seemingly simple property such as a system's total mass (or energy). The main reason is that the gravitational field—like any physical field—must be ascribed a certain energy, but that it proves to be fundamentally impossible to localize that energy.
Nevertheless, there are possibilities to define a system's total mass, either using a hypothetical "infinitely distant observer" (ADM mass) or suitable symmetries (Komar mass). If one excludes from the system's total mass the energy being carried away to infinity by gravitational waves, the result is the so-called Bondi mass at null infinity. Just as in classical physics, it can be shown that these masses are positive. Corresponding global definitions exist for momentum and angular momentum. There have also been a number of attempts to define "quasi-local" quantities, such as the mass of an isolated system formulated using only quantities defined within a finite region of space containing that system. The hope is to obtain a quantity useful for general statements about isolated systems, such as a more precise formulation of the hoop conjecture.
Relationship with quantum theory.
If general relativity were considered to be one of the two pillars of modern physics, then quantum theory, the basis of understanding matter from elementary particles to solid state physics, would be the other. However, how to reconcile quantum theory with general relativity is still an open question.
Quantum field theory in curved spacetime.
Ordinary quantum field theories, which form the basis of modern elementary particle physics, are defined in flat Minkowski space, which is an excellent approximation when it comes to describing the behavior of microscopic particles in weak gravitational fields like those found on Earth. In order to describe situations in which gravity is strong enough to influence (quantum) matter, yet not strong enough to require quantization itself, physicists have formulated quantum field theories in curved spacetime. These theories rely on general relativity to describe a curved background spacetime, and define a generalized quantum field theory to describe the behavior of quantum matter within that spacetime. Using this formalism, it can be shown that black holes emit a blackbody spectrum of particles known as Hawking radiation, leading to the possibility that they evaporate over time. As briefly mentioned above, this radiation plays an important role for the thermodynamics of black holes.
Quantum gravity.
The demand for consistency between a quantum description of matter and a geometric description of spacetime, as well as the appearance of singularities (where curvature length scales become microscopic), indicate the need for a full theory of quantum gravity: for an adequate description of the interior of black holes, and of the very early universe, a theory is required in which gravity and the associated geometry of spacetime are described in the language of quantum physics. Despite major efforts, no complete and consistent theory of quantum gravity is currently known, even though a number of promising candidates exist.
Attempts to generalize ordinary quantum field theories, used in elementary particle physics to describe fundamental interactions, so as to include gravity have led to serious problems. At low energies, this approach proves successful, in that it results in an acceptable effective (quantum) field theory of gravity. At very high energies, however, the result are models devoid of all predictive power ("non-renormalizability").
One attempt to overcome these limitations is string theory, a quantum theory not of point particles, but of minute one-dimensional extended objects. The theory promises to be a unified description of all particles and interactions, including gravity; the price to pay is unusual features such as six extra dimensions of space in addition to the usual three. In what is called the second superstring revolution, it was conjectured that both string theory and a unification of general relativity and supersymmetry known as supergravity form part of a hypothesized eleven-dimensional model known as M-theory, which would constitute a uniquely defined and consistent theory of quantum gravity.
Another approach starts with the canonical quantization procedures of quantum theory. Using the initial-value-formulation of general relativity (cf. evolution equations above), the result is the Wheeler–deWitt equation (an analogue of the Schrödinger equation) which, regrettably, turns out to be ill-defined. However, with the introduction of what are now known as Ashtekar variables, this leads to a promising model known as loop quantum gravity. Space is represented by a web-like structure called a spin network, evolving over time in discrete steps.
Depending on which features of general relativity and quantum theory are accepted unchanged, and on what level changes are introduced, there are numerous other attempts to arrive at a viable theory of quantum gravity, some examples being dynamical triangulations, causal sets, twistor models or the path-integral based models of quantum cosmology.
All candidate theories still have major formal and conceptual problems to overcome. They also face the common problem that, as yet, there is no way to put quantum gravity predictions to experimental tests (and thus to decide between the candidates where their predictions vary), although there is hope for this to change as future data from cosmological observations and particle physics experiments becomes available.
Current status.
General relativity has emerged as a highly successful model of gravitation and cosmology, which has so far passed many unambiguous observational and experimental tests. However, there are strong indications the theory is incomplete. The problem of quantum gravity and the question of the reality of spacetime singularities remain open. Observational data that is taken as evidence for dark energy and dark matter could indicate the need for new physics. Even taken as is, general relativity is rich with possibilities for further exploration. Mathematical relativists seek to understand the nature of singularities and the fundamental properties of Einstein's equations, and increasingly powerful computer simulations (such as those describing merging black holes) are run. The race for the first direct detection of gravitational waves continues, in the hope of creating opportunities to test the theory's validity for much stronger gravitational fields than has been possible to date. A century after its publication, general relativity remains a highly active area of research.

</doc>
<doc id="178649" url="https://en.wikipedia.org/wiki?curid=178649" title="General topology">
General topology

In mathematics, general topology is the branch of topology that deals with the basic set-theoretic definitions and constructions used in topology. It is the foundation of most other branches of topology, including differential topology, geometric topology, and algebraic topology. Another name for general topology is point-set topology.
The fundamental concepts in point-set topology are "continuity", "compactness", and "connectedness": 
The words 'nearby', 'arbitrarily small', and 'far apart' can all be made precise by using open sets. If we change the definition of 'open set', we change what continuous functions, compact sets, and connected sets are. Each choice of definition for 'open set' is called a "topology". A set with a topology is called a "topological space".
"Metric spaces" are an important class of topological spaces where distances can be assigned a number called a "metric". Having a metric simplifies many proofs, and many of the most common topological spaces are metric spaces.
History.
General topology grew out of a number of areas, most importantly the following:
General topology assumed its present form around 1940. It captures, one might say, almost everything in the intuition of continuity, in a technically adequate form that can be applied in any area of mathematics.
A topology on a set.
Let "X" be a set and let "τ" be a family of subsets of "X". Then "τ" is called a "topology on X" if:
If "τ" is a topology on "X", then the pair ("X", "τ") is called a "topological space". The notation "Xτ" may be used to denote a set "X" endowed with the particular topology "τ".
The members of "τ" are called "open sets" in "X". A subset of "X" is said to be closed if its complement is in "τ" (i.e., its complement is open). A subset of "X" may be open, closed, both (clopen set), or neither. The empty set and "X" itself are always both closed and open.
Basis for a topology.
A base (or basis) "B" for a topological space "X" with topology "T" is a collection of open sets in "T" such that every open set in "T" can be written as a union of elements of "B". We say that the base "generates" the topology "T". Bases are useful because many properties of topologies can be reduced to statements about a base that generates that topology—and because many topologies are most easily defined in terms of a base that generates them.
Subspace and quotient.
Every subset of a topological space can be given the subspace topology in which the open sets are the intersections of the open sets of the larger space with the subset. For any indexed family of topological spaces, the product can be given the product topology, which is generated by the inverse images of open sets of the factors under the projection mappings. For example, in finite products, a basis for the product topology consists of all products of open sets. For infinite products, there is the additional requirement that in a basic open set, all but finitely many of its projections are the entire space.
A quotient space is defined as follows: if "X" is a topological space and "Y" is a set, and if "f" : "X"→ "Y" is a surjective function, then the quotient topology on "Y" is the collection of subsets of "Y" that have open inverse images under "f". In other words, the quotient topology is the finest topology on "Y" for which "f" is continuous. A common example of a quotient topology is when an equivalence relation is defined on the topological space "X". The map "f" is then the natural projection onto the set of equivalence classes.
Examples of topological spaces.
A given set may have many different topologies. If a set is given a different topology, it is viewed as a different topological space. Any set can be given the discrete topology in which every subset is open. The only convergent sequences or nets in this topology are those that are eventually constant. Also, any set can be given the trivial topology (also called the indiscrete topology), in which only the empty set and the whole space are open. Every sequence and net in this topology converges to every point of the space. This example shows that in general topological spaces, limits of sequences need not be unique. However, often topological spaces must be Hausdorff spaces where limit points are unique.
There are many ways to define a topology on R, the set of real numbers. The standard topology on R is generated by the open intervals. The set of all open intervals forms a base or basis for the topology, meaning that every open set is a union of some collection of sets from the base. In particular, this means that a set is open if there exists an open interval of non zero radius about every point in the set. More generally, the Euclidean spaces R"n" can be given a topology. In the usual topology on R"n" the basic open sets are the open balls. Similarly, C, the set of complex numbers, and C"n" have a standard topology in which the basic open sets are open balls.
Every metric space can be given a metric topology, in which the basic open sets are open balls defined by the metric. This is the standard topology on any normed vector space. On a finite-dimensional vector space this topology is the same for all norms.
Many sets of linear operators in functional analysis are endowed with topologies that are defined by specifying when a particular sequence of functions converges to the zero function.
Any local field has a topology native to it, and this can be extended to vector spaces over that field.
Every manifold has a natural topology since it is locally Euclidean. Similarly, every simplex and every simplicial complex inherits a natural topology from Rn.
The Zariski topology is defined algebraically on the spectrum of a ring or an algebraic variety. On R"n" or C"n", the closed sets of the Zariski topology are the solution sets of systems of polynomial equations.
A linear graph has a natural topology that generalises many of the geometric aspects of graphs with vertices and edges.
The Sierpiński space is the simplest non-discrete topological space. It has important relations to the theory of computation and semantics.
There exist numerous topologies on any given finite set. Such spaces are called finite topological spaces. Finite spaces are sometimes used to provide examples or counterexamples to conjectures about topological spaces in general.
Any set can be given the cofinite topology in which the open sets are the empty set and the sets whose complement is finite. This is the smallest T1 topology on any infinite set.
Any set can be given the cocountable topology, in which a set is defined as open if it is either empty or its complement is countable. When the set is uncountable, this topology serves as a counterexample in many situations.
The real line can also be given the lower limit topology. Here, the basic open sets are the half open intervals ["a", "b"). This topology on R is strictly finer than the Euclidean topology defined above; a sequence converges to a point in this topology if and only if it converges from above in the Euclidean topology. This example shows that a set may have many distinct topologies defined on it.
If Γ is an ordinal number, then the set Γ = [0, Γ) may be endowed with the order topology generated by the intervals ("a", "b"), [0, "b") and ("a", Γ) where "a" and "b" are elements of Γ.
Continuous functions.
Continuity is expressed in terms of neighborhoods: is continuous at some point if and only if for any neighborhood of , there is a neighborhood of such that . Intuitively, continuity means no matter how "small" becomes, there is always a containing that maps inside and whose image under contains . This is equivalent to the condition that the preimages of the open (closed) sets in are open (closed) in . In metric spaces, this definition is equivalent to the ε–δ-definition that is often used in analysis.
An extreme example: if a set is given the discrete topology, all functions 
to any topological space are continuous. On the other hand, if is equipped with the indiscrete topology and the space set is at least T0, then the only continuous functions are the constant functions. Conversely, any function whose range is indiscrete is continuous.
Alternative definitions.
Several equivalent definitions for a topological structure exist and thus there are several equivalent ways to define a continuous function.
Neighborhood definition.
Definitions based on preimages are often difficult to use directly. The following criterion expresses continuity in terms of neighborhoods: "f" is continuous at some point "x" ∈ "X" if and only if for any neighborhood "V" of "f"("x"), there is a neighborhood "U" of "x" such that "f"("U") ⊆ "V". Intuitively, continuity means no matter how "small" "V" becomes, there is always a "U" containing "x" that maps inside "V".
If "X" and "Y" are metric spaces, it is equivalent to consider the neighborhood system of open balls centered at "x" and "f"("x") instead of all neighborhoods. This gives back the above δ-ε definition of continuity in the context of metric spaces. However, in general topological spaces, there is no notion of nearness or distance.
Note, however, that if the target space is Hausdorff, it is still true that "f" is continuous at "a" if and only if the limit of "f" as "x" approaches "a" is "f"("a"). At an isolated point, every function is continuous.
Sequences and nets.
In several contexts, the topology of a space is conveniently specified in terms of limit points. In many instances, this is accomplished by specifying when a point is the limit of a sequence, but for some spaces that are too large in some sense, one specifies also when a point is the limit of more general sets of points indexed by a directed set, known as nets. A function is continuous only if it takes limits of sequences to limits of sequences. In the former case, preservation of limits is also sufficient; in the latter, a function may preserve all limits of sequences yet still fail to be continuous, and preservation of nets is a necessary and sufficient condition.
In detail, a function "f": "X" → "Y" is sequentially continuous if whenever a sequence ("x""n") in "X" converges to a limit "x", the sequence ("f"("x""n")) converges to "f"("x"). Thus sequentially continuous functions "preserve sequential limits". Every continuous function is sequentially continuous. If "X" is a first-countable space and countable choice holds, then the converse also holds: any function preserving sequential limits is continuous. In particular, if "X" is a metric space, sequential continuity and continuity are equivalent. For non first-countable spaces, sequential continuity might be strictly weaker than continuity. (The spaces for which the two properties are equivalent are called sequential spaces.) This motivates the consideration of nets instead of sequences in general topological spaces. Continuous functions preserve limits of nets, and in fact this property characterizes continuous functions.
Closure operator definition.
Instead of specifying the open subsets of a topological space, the topology can also be determined by a closure operator (denoted cl), which assigns to any subset "A" ⊆ "X" its closure, or an interior operator (denoted int), which assigns to any subset "A" of "X" its interior. In these terms, a function 
between topological spaces is continuous in the sense above if and only if for all subsets "A" of "X"
That is to say, given any element "x" of "X" that is in the closure of any subset "A", "f"("x") belongs to the closure of "f"("A"). This is equivalent to the requirement that for all subsets "A"' of "X"'
Moreover, 
is continuous if and only if 
for any subset "A" of "X".
Properties.
If "f": "X" → "Y" and "g": "Y" → "Z" are continuous, then so is the composition "g" ∘ "f": "X" → "Z". If "f": "X" → "Y" is continuous and
The possible topologies on a fixed set "X" are partially ordered: a topology τ1 is said to be coarser than another topology τ2 (notation: τ1 ⊆ τ2) if every open subset with respect to τ1 is also open with respect to τ2. Then, the identity map 
is continuous if and only if τ1 ⊆ τ2 (see also comparison of topologies). More generally, a continuous function
stays continuous if the topology τ"Y" is replaced by a coarser topology and/or τ"X" is replaced by a finer topology.
Homeomorphisms.
Symmetric to the concept of a continuous map is an open map, for which "images" of open sets are open. In fact, if an open map "f" has an inverse function, that inverse is continuous, and if a continuous map "g" has an inverse, that inverse is open. Given a bijective function "f" between two topological spaces, the inverse function "f"−1 need not be continuous. A bijective continuous function with continuous inverse function is called a "homeomorphism".
If a continuous bijection has as its domain a compact space and its codomain is Hausdorff, then it is a homeomorphism.
Defining topologies via continuous functions.
Given a function
where "X" is a topological space and "S" is a set (without a specified topology), the final topology on "S" is defined by letting the open sets of "S" be those subsets "A" of "S" for which "f"−1("A") is open in "X". If "S" has an existing topology, "f" is continuous with respect to this topology if and only if the existing topology is coarser than the final topology on "S". Thus the final topology can be characterized as the finest topology on "S" that makes "f" continuous. If "f" is surjective, this topology is canonically identified with the quotient topology under the equivalence relation defined by "f".
Dually, for a function "f" from a set "S" to a topological space, the initial topology on "S" has as open subsets "A" of "S" those subsets for which "f"("A") is open in "X". If "S" has an existing topology, "f" is continuous with respect to this topology if and only if the existing topology is finer than the initial topology on "S". Thus the initial topology can be characterized as the coarsest topology on "S" that makes "f" continuous. If "f" is injective, this topology is canonically identified with the subspace topology of "S", viewed as a subset of "X".
More generally, given a set "S", specifying the set of continuous functions 
into all topological spaces "X" defines a topology. Dually, a similar idea can be applied to maps
This is an instance of a universal property.
Compact sets.
Formally, a topological space "X" is called "compact" if each of its open covers has a finite subcover. Otherwise it is called "non-compact". Explicitly, this means that for every arbitrary collection
of open subsets of such that
there is a finite subset of such that
Some branches of mathematics such as algebraic geometry, typically influenced by the French school of Bourbaki, use the term "quasi-compact" for the general notion, and reserve the term "compact" for topological spaces that are both Hausdorff and "quasi-compact". A compact set is sometimes referred to as a "compactum", plural "compacta".
Every closed interval in R of finite length is compact. More is true: In Rn, a set is compact if and only if it is closed and bounded. (See Heine–Borel theorem).
Every continuous image of a compact space is compact.
A compact subset of a Hausdorff space is closed.
Every continuous bijection from a compact space to a Hausdorff space is necessarily a homeomorphism.
Every sequence of points in a compact metric space has a convergent subsequence.
Every compact finite-dimensional manifold can be embedded in some Euclidean space Rn.
Connected sets.
A topological space "X" is said to be disconnected if it is the union of two disjoint nonempty open sets. Otherwise, "X" is said to be connected. A subset of a topological space is said to be connected if it is connected under its subspace topology. Some authors exclude the empty set (with its unique topology) as a connected space, but this article does not follow that practice.
For a topological space "X" the following conditions are equivalent:
Every interval in R is connected.
The continuous image of a connected space is connected.
Connected components.
The maximal connected subsets (ordered by inclusion) of a nonempty topological space are called the connected components of the space.
The components of any topological space "X" form a partition of "X": they are disjoint, nonempty, and their union is the whole space.
Every component is a closed subset of the original space. It follows that, in the case where their number is finite, each component is also an open subset. However, if their number is infinite, this might not be the case; for instance, the connected components of the set of the rational numbers are the one-point sets, which are not open.
Let formula_14 be the connected component of "x" in a topological space "X", and formula_15 be the intersection of all open-closed sets containing "x" (called quasi-component of "x".) Then formula_16 where the equality holds if "X" is compact Hausdorff or locally connected.
Disconnected spaces.
A space in which all components are one-point sets is called totally disconnected. Related to this property, a space "X" is called totally separated if, for any two distinct elements "x" and "y" of "X", there exist disjoint open neighborhoods "U" of "x" and "V" of "y" such that "X" is the union of "U" and "V". Clearly any totally separated space is totally disconnected, but the converse does not hold. For example, take two copies of the rational numbers Q, and identify them at every point except zero. The resulting space, with the quotient topology, is totally disconnected. However, by considering the two copies of zero, one sees that the space is not totally separated. In fact, it is not even Hausdorff, and the condition of being totally separated is strictly stronger than the condition of being Hausdorff.
Path-connected sets.
A path from a point "x" to a point "y" in a topological space "X" is a continuous function "f" from the unit interval [0,1] to "X" with "f"(0) = "x" and "f"(1) = "y". A path-component of "X" is an equivalence class of "X" under the equivalence relation, which makes "x" equivalent to "y" if there is a path from "x" to "y". The space "X" is said to be path-connected (or pathwise connected or 0-connected) if there is at most one path-component, i.e. if there is a path joining any two points in "X". Again, many authors exclude the empty space.
Every path-connected space is connected. The converse is not always true: examples of connected spaces that are not path-connected include the extended long line "L"* and the "topologist's sine curve".
However, subsets of the real line R are connected if and only if they are path-connected; these subsets are the intervals of R.
Also, open subsets of R"n" or C"n" are connected if and only if they are path-connected.
Additionally, connectedness and path-connectedness are the same for finite topological spaces.
Products of spaces.
Given "X" such that
is the Cartesian product of the topological spaces "Xi", indexed by formula_18, and the canonical projections "pi" : "X" → "Xi", the product topology on "X" is defined as the coarsest topology (i.e. the topology with the fewest open sets) for which all the projections "pi" are continuous. The product topology is sometimes called the Tychonoff topology.
The open sets in the product topology are unions (finite or infinite) of sets of the form formula_19, where each "Ui" is open in "Xi" and "U""i" ≠ "X""i" only finitely many times. In particular, for a finite product (in particular, for the product of two topological spaces), the products of base elements of the "Xi" gives a basis for the product formula_20.
The product topology on "X" is the topology generated by sets of the form "pi"−1("U"), where "i" is in "I " and "U" is an open subset of "Xi". In other words, the sets {"pi"−1("U")} form a subbase for the topology on "X". A subset of "X" is open if and only if it is a (possibly infinite) union of intersections of finitely many sets of the form "pi"−1("U"). The "pi"−1("U") are sometimes called open cylinders, and their intersections are cylinder sets.
In general, the product of the topologies of each "Xi" forms a basis for what is called the box topology on "X". In general, the box topology is finer than the product topology, but for finite products they coincide.
Related to compactness is Tychonoff's theorem: the (arbitrary) product of compact spaces is compact.
Separation axioms.
Many of these names have alternative meanings in some of mathematical literature, as explained on History of the separation axioms; for example, the meanings of "normal" and "T4" are sometimes interchanged, similarly "regular" and "T3", etc. Many of the concepts also have several names; however, the one listed first is always least likely to be ambiguous.
Most of these axioms have alternative definitions with the same meaning; the definitions given here fall into a consistent pattern that relates the various notions of separation defined in the previous section. Other possible definitions can be found in the individual articles.
In all of the following definitions, "X" is again a topological space.
The Tietze extension theorem: In a normal space, every continuous real-valued function defined on a closed subspace can be extended to a continuous map defined on the whole space.
Countability axioms.
An axiom of countability is a property of certain mathematical objects (usually in a category) that requires the existence of a countable set with certain properties, while without it such sets might not exist.
Important countability axioms for topological spaces:
Relations:
Metric spaces.
A metric space is an ordered pair formula_21 where formula_22 is a set and formula_23 is a metric on formula_22, i.e., a function
such that for any formula_26, the following holds:
The function formula_23 is also called "distance function" or simply "distance". Often, formula_23 is omitted and one just writes formula_22 for a metric space if it is clear from the context what metric is used.
Every metric space is paracompact and Hausdorff, and thus normal.
The metrization theorems provide necessary and sufficient conditions for a topology to come from a metric.
Baire category theory.
The Baire category theorem says: If "X" is a complete metric space or a locally compact Hausdorff space, then the interior of every union of countably many nowhere dense sets is empty.
Any open subspace of a Baire space is itself a Baire space.
Main areas of research.
Continuum theory.
A continuum (pl "continua") is a nonempty compact connected metric space, or less frequently, a compact connected Hausdorff space. Continuum theory is the branch of topology devoted to the study of continua.
Pointless topology.
Pointless topology (also called point-free or pointfree topology) is an approach to topology that avoids mentioning points. The name 'pointless topology' is due to John von Neumann. The ideas of pointless topology are closely related to mereotopologies, in which regions (sets) are treated as foundational without explicit reference to underlying point sets.
Dimension theory.
Dimension theory is a branch of general topology dealing with dimensional invariants of topological spaces.
Topological algebras.
A topological algebra "A" over a topological field K is a topological vector space together with a continuous multiplication
that makes it an algebra over K. A unital associative topological algebra is a topological ring.
The term was coined by David van Dantzig; it appears in the title of his doctoral dissertation (1931).
Metrizability theory.
In topology and related areas of mathematics, a metrizable space is a topological space that is homeomorphic to a metric space. That is, a topological space formula_37 is said to be metrizable if there is a metric 
such that the topology induced by "d" is formula_39. Metrization theorems are theorems that give sufficient conditions for a topological space to be metrizable.
Set-theoretic topology.
Set-theoretic topology is a subject that combines set theory and general topology. It focuses on topological questions that are independent of Zermelo–Fraenkel set theory(ZFC). A famous problem is the normal Moore space question, a question in general topology that was the subject of intense research. The answer to the normal Moore space question was eventually proved to be independent of ZFC.
Further reading.
Some standard books on general topology include:
The arXiv subject code is math.GN.

</doc>
<doc id="2455731" url="https://en.wikipedia.org/wiki?curid=2455731" title="Generation (particle physics)">
Generation (particle physics)

In particle physics, a generation (or family) is a division of the elementary particles. Between generations, particles differ by their (flavour) quantum number and mass, but their interactions are identical.
There are three generations according to the Standard Model of particle physics. Each generation is divided into two types of leptons and two types of quarks. The two leptons may be classified into one with electric charge −1 (electron-like) and one neutral (neutrino); the two quarks may be classified into one with charge − (down-type) and one with charge + (up-type).
Overview.
Each member of a higher generation has greater mass than the corresponding particle of the previous generation, with the possible exception of the neutrinos (whose small but non-zero masses have not been accurately determined). For example, the first-generation electron has a mass of only , the second-generation muon has a mass of , and the third-generation tau has a mass of (almost twice as heavy as a proton). This mass hierarchy causes particles of higher generations to decay to the first generation, which explains why everyday matter (atoms) is made of particles from the first generation. Electrons surround a nucleus made of protons and neutrons, which contain up and down quarks. The second and third generations of charged particles do not occur in normal matter and are only seen in extremely high-energy environments such as cosmic rays or particle accelerators.
The term "generation" was first introduced by Haim Harari in Les Houches Summer School, 1976.
Neutrinos of all generations stream throughout the universe but rarely interact with normal matter. It is hoped that a comprehensive understanding of the relationship between the generations of the leptons may eventually explain the ratio of masses of the fundamental particles, and shed further light on the nature of mass generally, from a quantum perspective.
Fourth generation.
Fourth and further generations are considered to be unlikely. Some of the arguments against the possibility of a fourth generation are based on the subtle modifications of precision electroweak observables that extra generations would induce; such modifications are strongly disfavored by measurements. Furthermore, a fourth generation with a "light" neutrino (one with a mass less than about ) has been ruled out by measurements of the widths of the Z boson at CERN's Large Electron–Positron Collider (LEP). Nonetheless, searches at high-energy colliders for particles from a fourth generation continue, but as yet no evidence has been observed. In such searches, fourth-generation particles are denoted by the same symbols as third-generation ones with an added prime (e.g. "b′" and "t′").
According to the results of the statistical analysis by researchers from CERN, and Humboldt University of Berlin, the existence of further fermions can be excluded with a probability of 99.99999% (5.3 sigma). The researchers combined latest data collected by the particle accelerators LHC and Tevatron with many known measurements results relating to particles, such as the Z-boson or the top-quark. The most important data used for this analysis come from the discovery of the Higgs particle. In the Standard Model, the Higgs particle gives all other particles their mass. As additional fermions were not detected directly in accelerator experiments, they have to be heavier than the fermions known so far. Hence, these fermions would also interact with the Higgs particle more strongly. This interaction would have modified the properties of the Higgs particle such that this particle would not have been detected.

</doc>
<doc id="91096" url="https://en.wikipedia.org/wiki?curid=91096" title="Geodesic">
Geodesic

In mathematics, particularly differential geometry, a geodesic ( or ) is a generalization of the notion of a "straight line" to "curved spaces". In the presence of an affine connection, a geodesic is defined to be a curve whose tangent vectors remain parallel if they are transported along it. If this connection is the Levi-Civita connection induced by a Riemannian metric, then the geodesics are (locally) the shortest path between points in the space.
The term "geodesic" comes from "geodesy", the science of measuring the size and shape of Earth; in the original sense, a geodesic was the shortest route between two points on the Earth's surface, namely, a segment of a great circle. The term has been generalized to include measurements in much more general mathematical spaces; for example, in graph theory, one might consider a geodesic between two vertices/nodes of a graph.
Geodesics are of particular importance in general relativity. Timelike geodesics in general relativity describe the motion of inertial test particles.
Introduction.
The shortest path between two points in a curved space can be found by writing the equation for the length of a curve (a function "f" from an open interval of R to the manifold), and then minimizing this length using the calculus of variations. This has some minor technical problems, because there is an infinite dimensional space of different ways to parameterize the shortest path. It is simpler to demand not only that the curve locally minimizes length but also that it is parameterized "with constant velocity", meaning that the distance from "f"("s") to "f"("t") along the geodesic is proportional to |"s"−"t"|. Equivalently, a different quantity may be defined, termed the energy of the curve; minimizing the energy leads to the same equations for a geodesic (here "constant velocity" is a consequence of minimisation). Intuitively, one can understand this second formulation by noting that an elastic band stretched between two points will contract its length, and in so doing will minimize its energy. The resulting shape of the band is a geodesic.
In Riemannian geometry geodesics are not the same as "shortest curves" between two points, though the two concepts are closely related. The difference is that geodesics are only "locally" the shortest distance between points, and are parameterized with "constant velocity". Going the "long way round" on a great circle between two points on a sphere is a geodesic but not the shortest path between the points. The map "t" → "t"2 from the unit interval to itself gives the shortest path between 0 and 1, but is not a geodesic because the velocity of the corresponding motion of a point is not constant.
Geodesics are commonly seen in the study of Riemannian geometry and more generally metric geometry. In general relativity, geodesics describe the motion of point particles under the influence of gravity alone. In particular, the path taken by a falling rock, an orbiting satellite, or the shape of a planetary orbit are all geodesics in curved space-time. More generally, the topic of sub-Riemannian geometry deals with the paths that objects may take when they are not free, and their movement is constrained in various ways.
This article presents the mathematical formalism involved in defining, finding, and proving the existence of geodesics, in the case of Riemannian and pseudo-Riemannian manifolds. The article geodesic (general relativity) discusses the special case of general relativity in greater detail.
Examples.
The most familiar examples are the straight lines in Euclidean geometry. On a sphere, the images of geodesics are the great circles. The shortest path from point "A" to point "B" on a sphere is given by the shorter arc of the great circle passing through "A" and "B". If "A" and "B" are antipodal points, then there are "infinitely many" shortest paths between them. Geodesics on an ellipsoid behave in a more complicated way than on a sphere; in particular, they are not closed in general (see figure).
Metric geometry.
In metric geometry, a geodesic is a curve which is everywhere locally a distance minimizer. More precisely, a curve from an interval "I" of the reals to the metric space "M" is a geodesic if there is a constant such that for any there is a neighborhood "J" of "t" in "I" such that for any we have
This generalizes the notion of geodesic for Riemannian manifolds. However, in metric geometry the geodesic considered is often equipped with natural parameterization, i.e. in the above identity "v" = 1 and
If the last equality is satisfied for all , the geodesic is called a minimizing geodesic or shortest path.
In general, a metric space may have no geodesics, except constant curves. At the other extreme, any two points in a length metric space are joined by a minimizing sequence of rectifiable paths, although this minimizing sequence need not converge to a geodesic.
Riemannian geometry.
In a Riemannian manifold "M" with metric tensor "g", the length of a continuously differentiable curve γ : ["a","b"] → "M" is defined by
The distance "d"("p", "q") between two points "p" and "q" of "M" is defined as the infimum of the length taken over all continuous, piecewise continuously differentiable curves γ : ["a","b"] → "M" such that γ("a") = "p" and γ("b") = "q". With this definition of distance, geodesics in a Riemannian manifold are then the locally distance-minimizing paths.
The minimizing curves of "L" in a small enough open set of "M" can be obtained by techniques of calculus of variations. Typically, one introduces the following action or energy functional
It is then enough to minimize the functional "E", owing to the Cauchy–Schwarz inequality
with equality if and only if |dγ/dt| is constant.
The Euler–Lagrange equations of motion for the functional "E" are then given in local coordinates by
where formula_7 are the Christoffel symbols of the metric. This is the geodesic equation, discussed below.
Calculus of variations.
Techniques of the classical calculus of variations can be applied to examine the energy functional "E". The first variation of energy is defined in local coordinates by
The critical points of the first variation are precisely the geodesics. The second variation is defined by
In an appropriate sense, zeros of the second variation along a geodesic γ arise along Jacobi fields. Jacobi fields are thus regarded as variations through geodesics.
By applying variational techniques from classical mechanics, one can also regard geodesics as Hamiltonian flows. They are solutions of the associated Hamilton equations, with (pseudo-)Riemannian metric taken as Hamiltonian.
Affine geodesics.
A geodesic on a smooth manifold "M" with an affine connection ∇ is defined as a curve γ("t") such that parallel transport along the curve preserves the tangent vector to the curve, so
at each point along the curve, where formula_10 is the derivative with respect to formula_11. More precisely, in order to define the covariant derivative of formula_10 it is necessary first to extend formula_10 to a continuously differentiable vector field in an open set. However, the resulting value of () is independent of the choice of extension.
Using local coordinates on "M", we can write the geodesic equation (using the summation convention) as
where formula_15 are the coordinates of the curve γ("t") and formula_16 are the Christoffel symbols of the connection ∇. This is just an ordinary differential equation for the coordinates. It has a unique solution, given an initial position and an initial velocity. Therefore, from the point of view of classical mechanics, geodesics can be thought of as trajectories of free particles in a manifold. Indeed, the equation formula_17 means that the acceleration of the curve has no components in the direction of the surface (and therefore it is perpendicular to the tangent plane of the surface at each point of the curve). So, the motion is completely determined by the bending of the surface. This is also the idea of general relativity where particles move on geodesics and the bending is caused by the gravity.
Existence and uniqueness.
The "local existence and uniqueness theorem" for geodesics states that geodesics on a smooth manifold with an affine connection exist, and are unique. More precisely:
In general, "I" may not be all of R as for example for an open disc in R2. The proof of this theorem follows from the theory of ordinary differential equations, by noticing that the geodesic equation is a second-order ODE. Existence and uniqueness then follow from the Picard–Lindelöf theorem for the solutions of ODEs with prescribed initial conditions. γ depends smoothly on both "p" and "V".
Geodesic flow.
Geodesic flow is a local R-action on the tangent bundle "TM" of a manifold "M" defined in the following way 
where "t" ∈ R, "V" ∈ "TM" and formula_22 denotes the geodesic with initial data formula_23. Thus, "Gt"("V") = exp("tV") is the exponential map of the vector "tV". A closed orbit of the geodesic flow corresponds to a closed geodesic on "M".
On a (pseudo-)Riemannian manifold, the geodesic flow is identified with a Hamiltonian flow on the cotangent bundle. The Hamiltonian is then given by the inverse of the (pseudo-)Riemannian metric, evaluated against the canonical one-form. In particular the flow preserves the (pseudo-)Riemannian metric formula_24, i.e.
In particular, when "V" is a unit vector, formula_22 remains unit speed throughout, so the geodesic flow is tangent to the unit tangent bundle. Liouville's theorem implies invariance of a kinematic measure on the unit tangent bundle.
Geodesic spray.
The geodesic flow defines a family of curves in the tangent bundle. The derivatives of these curves define a vector field on the total space of the tangent bundle, known as the geodesic spray.
More precisely, an affine connection gives rise to a splitting of the double tangent bundle TT"M" into horizontal and vertical bundles:
The geodesic spray is the unique horizontal vector field "W" satisfying
at each point "v" ∈ T"M"; here π∗ : TT"M" → T"M" denotes the pushforward (differential) along the projection π : T"M" → "M" associated to the tangent bundle.
More generally, the same construction allows one to construct a vector field for any Ehresmann connection on the tangent bundle. For the resulting vector field to be a spray (on the deleted tangent bundle T"M" \ {0}) it is enough that the connection be equivariant under positive rescalings: it need not be linear. That is, (cf. Ehresmann connection#Vector bundles and covariant derivatives) it is enough that the horizontal distribution satisfy
for every "X" ∈ T"M" \ {0} and λ > 0. Here "d"("S"λ) is the pushforward along the scalar homothety formula_30 A particular case of a non-linear connection arising in this manner is that associated to a Finsler manifold.
Affine and projective geodesics.
Equation () is invariant under affine reparameterizations; that is, parameterizations of the form
where "a" and "b" are constant real numbers. Thus apart from specifying a certain class of embedded curves, the geodesic equation also determines a preferred class of parameterizations on each of the curves. Accordingly, solutions of () are called geodesics with affine parameter.
An affine connection is "determined by" its family of affinely parameterized geodesics, up to torsion . The torsion itself does not, in fact, affect the family of geodesics, since the geodesic equation depends only on the symmetric part of the connection. More precisely, if formula_32 are two connections such that the difference tensor
is skew-symmetric, then formula_34 and formula_35 have the same geodesics, with the same affine parameterizations. Furthermore, there is a unique connection having the same geodesics as formula_34, but with vanishing torsion.
Geodesics without a particular parameterization are described by a projective connection.
Computational methods.
Efficient solvers for the minimal geodesic problem on surfaces posed as Eikonal equations can be found in
Applications.
Geodesics serve as the basis to calculate:

</doc>
<doc id="2037563" url="https://en.wikipedia.org/wiki?curid=2037563" title="Geodesics in general relativity">
Geodesics in general relativity

In general relativity, a geodesic generalizes the notion of a "straight line" to curved spacetime. Importantly, the world line of a particle free from all external, non-gravitational force, is a particular type of geodesic. In other words, a freely moving or falling particle always moves along a geodesic.
In general relativity, gravity can be regarded as not a force but a consequence of a curved spacetime geometry where the source of curvature is the stress–energy tensor (representing matter, for instance). Thus, for example, the path of a planet orbiting around a star is the projection of a geodesic of the curved 4-D spacetime geometry around the star onto 3-D space.
Mathematical expression.
The full geodesic equation is this:
where "s" is a scalar parameter of motion (e.g. the proper time), and formula_2 are Christoffel symbols (sometimes called the affine connection or Levi-Civita connection) which is symmetric in the two lower indices. Greek indices take the values [0,1,2,3]. The quantity on the left-hand-side of this equation is the acceleration of a particle, and so this equation is analogous to Newton's laws of motion which likewise provide formulae for the acceleration of a particle. This equation of motion employs the Einstein notation, meaning that repeated indices are summed (i.e. from zero to three). The Christoffel symbols are functions of the four space-time coordinates, and so are independent of the velocity or acceleration or other characteristics of a test particle whose motion is described by the geodesic equation.
Equivalent mathematical expression using coordinate time as parameter.
So far the geodesic equation of motion has been written in terms of a scalar parameter "s". It can alternatively be written in terms of the time coordinate, formula_3 (here we have used the triple bar to signify a definition). The geodesic equation of motion then becomes:
This formulation of the geodesic equation of motion can be useful for computer calculations and to compare General Relativity with Newtonian Gravity. It is straightforward to derive this form of the geodesic equation of motion from the form which uses proper time as a parameter, using the chain rule. Notice that both sides of this last equation vanish when the mu index is set to zero. If the particle's velocity is small enough, then the geodesic equation reduces to this:
Here the Latin index "n" takes the values [1,2,3]. This equation simply means that all test particles at a particular place and time will have the same acceleration, which is a well-known feature of Newtonian gravity. For example, everything floating around in the international space station will undergo roughly the same acceleration due to gravity.
Derivation directly from the equivalence principle.
Physicist Steven Weinberg has presented a derivation of the geodesic equation of motion directly from the equivalence principle.
The first step in such a derivation is to suppose that no particles are accelerating in the neighborhood of a point-event with respect to a freely falling coordinate system (formula_6). Setting formula_7, we have the following equation that is locally applicable in free fall:
The next step is to employ the chain rule. We have:
Differentiating once more with respect to the time, we have:
Therefore:
Multiply both sides of this last equation by the following quantity:
Consequently, we have this:
As before, we can set formula_3. Using the chain rule, the parameter "T" can be eliminated in favor of the parameter "t" like so:
The geodesic equation of motion (using the coordinate time as parameter) follows immediately from this last equation, because the bracketed terms (which involve the relationship between local coordinates "X" and general coordinates "x") are functions of the general coordinates. The geodesic equation of motion can alternatively be derived using the concept of parallel transport.
Deriving the geodesic equation via an action.
We can (and this is the most common technique) derive the geodesic equation via the action principle.
Let the action be 
formula_16 
where formula_17 is the line element. To get the geodesic equation we must vary this action. To do this lets parameterize this action with respect a parameter formula_18. Doing this we get:
formula_19 
We can now go ahead and vary this action with respect to the curve formula_20. By the principle of least action we get:
formula_21
For concreteness lets parameterize this action w.r.t. the proper time, formula_22. Since the four-velocity is normalized to -1 (for time-like paths) we can say that the above is equivalent to the action:
formula_23
Using the product rule we get:
formula_24
Integrating by-parts the last term and dropping the total derivative (which equals to zero at the boundaries) we get that:
formula_25
Simplifying a bit we see that:
formula_26
so,
formula_27
multiplying this equation by formula_28 we get:
formula_29
So by Hamilton's principle we find that the Euler–Lagrange equation is
formula_30
Multiplying by the inverse metric tensor formula_31 we get that
formula_32
Thus we get the geodesic equation:
formula_33
with the Christoffel symbol defined in terms of the metric tensor as
formula_34
Equation of motion may follow from the field equations for empty space.
Albert Einstein believed that the geodesic equation of motion can be derived from the field equations for empty space, i.e. from the fact that the Ricci curvature vanishes. He wrote:
It has been shown that this law of motion — generalized to the case of arbitrarily large gravitating masses — can be derived from the field equations of empty space alone. According to this derivation the law of motion is implied by the condition that the field be singular nowhere outside its generating mass points.
Both physicists and philosophers have often repeated the assertion that the geodesic equation can be obtained from the field equations to describe the motion of a gravitational singularity, but this claim remains disputed. Less controversial is the notion that the field equations determine the motion of a fluid or dust, as distinguished from the motion of a point-singularity.
Extension to the case of a charged particle.
In deriving the geodesic equation from the equivalence principle, it was assumed that particles in a local inertial coordinate system are not accelerating. However, in real life, the particles may be charged, and therefore may be accelerating locally in accordance with the Lorentz force. That is:
with
The Minkowski tensor η is given by:
These last three equations can be used as the starting point for the derivation of an equation of motion in General Relativity, instead of assuming that acceleration is zero in free fall. Because the Minkowski tensor is involved here, it becomes necessary to introduce something called the "metric tensor" in General Relativity. The metric tensor "g" is symmetric, and locally reduces to the Minkowski tensor in free fall. The resulting equation of motion is as follows:
with
This last equation signifies that the particle is moving along a timelike geodesic; massless particles like the photon instead follow null geodesics (replace −1 with zero on the right-hand side of the last equation). It is important that the last two equations are consistent with each other, when the latter is differentiated with respect to proper time, and the following formula for the Christoffel symbols ensures that consistency:
This last equation does not involve the electromagnetic fields, and it is applicable even in the limit as the electromagnetic fields vanish. The letter "g" with superscripts refers to the inverse of the metric tensor. In General Relativity, indices of tensors are lowered and raised by contraction with the metric tensor or its inverse, respectively.
Geodesics as curves of stationary interval.
A geodesic between two events can also be described as the curve joining those two events which has a stationary interval (4-dimensional "length"). "Stationary" here is used in the sense in which that term is used in the calculus of variations, namely, that the interval along the curve varies minimally among curves that are nearby to the geodesic.
In Minkowski space there is only one time-like geodesic that connects any given pair of time-like separated events, and that geodesic is the curve with the longest proper time between the two events. But in curved spacetime, it's possible for a pair of widely-separated events to have more than one time-like geodesic that connects them. In such instances, the proper times along the various geodesics will not in general be the same. And for some geodesics in such instances, it's possible for a curve that connects the two events and is nearby to the geodesic to have either a longer or a shorter proper time than the geodesic.
For a space-like geodesic through two events, there are always nearby curves which go through the two events that have either a longer or a shorter proper length than the geodesic, even in Minkowski space. In Minkowski space, in an inertial frame of reference in which the two events are simultaneous, the geodesic will be the straight line between the two events at the time at which the events occur. Any curve that differs from the geodesic purely spatially in that frame of reference will have a longer proper length than the geodesic, but a curve that differs from the geodesic purely temporally in that frame of reference will have a shorter proper length.
The interval of a curve in spacetime is
Then, the Euler–Lagrange equation,
becomes, after some calculation, 
where formula_44
The goal being to find a curve for which the value of
is stationary, where
such goal can be accomplished by calculating the Euler–Lagrange equation for "f", which is 
Substituting the expression of "f" into the Euler–Lagrange equation (which makes the value of the integral "l" stationary), gives
Now calculate the derivatives:
formula_49
formula_50
formula_51
formula_52
formula_53
formula_54
formula_56
formula_57
This is just one step away from the geodesic equation.
If the parameter "s" is chosen to be affine, then the right side the above equation vanishes (because formula_58 is constant). Finally, we have the geodesic equation

</doc>
<doc id="621732" url="https://en.wikipedia.org/wiki?curid=621732" title="Geometric topology">
Geometric topology

In mathematics, geometric topology is the study of manifolds and maps between them, particularly embeddings of one manifold into another.
History.
Geometric topology as an area distinct from algebraic topology may be said to have originated in the 1935 classification of lens spaces by Reidemeister torsion, which required distinguishing spaces that are homotopy equivalent but not homeomorphic. This was the origin of "simple" homotopy theory.
Differences between low-dimensional and high-dimensional topology.
Manifolds differ radically in behavior in high and low dimension.
High-dimensional topology refers to manifolds of dimension 5 and above, or in relative terms, embeddings in codimension 3 and above. Low-dimensional topology is concerned with questions in dimensions up to 4, or embeddings in codimension up to 2.
Dimension 4 is special, in that in some respects (topologically), dimension 4 is high-dimensional, while in other respects (differentiably), dimension 4 is low-dimensional; this overlap yields phenomena exceptional to dimension 4, such as exotic differentiable structures on R4. Thus the topological classification of 4-manifolds is in principle easy, and the key questions are: does a topological manifold admit a differentiable structure, and if so, how many? Notably, the smooth case of dimension 4 is the last open case of the generalized Poincaré conjecture; see Gluck twists.
The distinction is because surgery theory works in dimension 5 and above (in fact, it works topologically in dimension 4, though this is very involved to prove), and thus the behavior of manifolds in dimension 5 and above is controlled algebraically by surgery theory. In dimension 4 and below (topologically, in dimension 3 and below), surgery theory does not work, and other phenomena occur.
Indeed, one approach to discussing low-dimensional manifolds is to ask "what would surgery theory predict to be true, were it to work?" – and then understand low-dimensional phenomena as deviations from this.
The precise reason for the difference at dimension 5 is because the Whitney embedding theorem, the key technical trick which underlies surgery theory, requires 2+1 dimensions. Roughly, the Whitney trick allows one to "unknot" knotted spheres – more precisely, remove self-intersections of immersions;
it does this via a homotopy of a disk – the disk has 2 dimensions, and the homotopy adds 1 more – and thus in codimension greater than 2, this can be done without intersecting itself; hence embeddings in codimension greater than 2 can be understood by surgery. In surgery theory, the key step is in the middle dimension, and thus when the middle dimension has codimension more than 2 (loosely, 2½ is enough, hence total dimension 5 is enough), the Whitney trick works. The key consequence of this is Smale's "h"-cobordism theorem, which works in dimension 5 and above, and forms the basis for surgery theory.
A modification of the Whitney trick can work in 4 dimensions, and is called Casson handles – because there are not enough dimensions, a Whitney disk introduces new kinks, which can be resolved by another Whitney disk, leading to a sequence ("tower") of disks. The limit of this tower yields a topological but not differentiable map, hence surgery works topologically but not differentiably in dimension 4.
Important tools in geometric topology.
Fundamental group.
In all dimensions, the fundamental group of a manifold is a very important invariant, and determines much of the structure; in dimensions 1, 2 and 3, the possible fundamental groups are restricted, while in every dimension 4 and above every finitely presented group is the fundamental group of a manifold (note that it is sufficient to show this for 4- and 5-dimensional manifolds, and then to take products with spheres to get higher ones).
Orientability.
A manifold is orientable if it has a consistent choice of orientation, and a connected orientable manifold has exactly two different possible orientations. In this setting, various equivalent formulations of orientability can be given, depending on the desired application and level of generality. Formulations applicable to general topological manifolds often employ methods of homology theory, whereas for differentiable manifolds more structure is present, allowing a formulation in terms of differential forms. An important generalization of the notion of orientability of a space is that of orientability of a family of spaces parameterized by some other space (a fiber bundle) for which an orientation must be selected in each of the spaces which varies continuously with respect to changes in the parameter values.
Handle decompositions.
A handle decomposition of an "m"-manifold "M" is a union
where each formula_2 is obtained from formula_3
by the attaching of formula_4-handles. A handle decomposition is to a manifold what a CW-decomposition is to a topological space—in many regards the purpose of a handle decomposition is to have a language analogous to CW-complexes, but adapted to the world of smooth manifolds. Thus an "i"-handle is the smooth analogue of an "i"-cell. Handle decompositions of manifolds arise naturally via Morse theory. The modification of handle structures is closely linked to Cerf theory.
Local flatness.
Local flatness is a property of a submanifold in a topological manifold of larger dimension. In the category of topological manifolds, locally flat submanifolds play a role similar to that of embedded submanifolds in the category of smooth manifolds.
Suppose a "d" dimensional manifold "N" is embedded into an "n" dimensional manifold "M" (where "d" < "n"). If formula_5 we say "N" is locally flat at "x" if there is a neighborhood formula_6 of "x" such that the topological pair formula_7 is homeomorphic to the pair formula_8, with a standard inclusion of formula_9 as a subspace of formula_10. That is, there exists a homeomorphism formula_11 such that the image of formula_12 coincides with formula_9.
Schönflies theorems.
The generalized Schoenflies theorem states that, if an ("n" − 1)-dimensional sphere "S" is embedded into the "n"-dimensional sphere "Sn" in a locally flat way (that is, the embedding extends to that of a thickened sphere), then the pair ("Sn", "S") is homeomorphic to the pair ("Sn", "S""n"−1), where "S""n"−1 is the equator of the "n"-sphere. Brown and Mazur received the Veblen Prize for their independent proofs of this theorem.
Branches of geometric topology.
Low-dimensional topology.
Low-dimensional topology includes:
each have their own theory, where there are some connections.
Low-dimensional topology is strongly geometric, as reflected in the uniformization theorem in 2 dimensions – every surface admits a constant curvature metric; geometrically, it has one of 3 possible geometries: positive curvature/spherical, zero curvature/flat, negative curvature/hyperbolic – and the geometrization conjecture (now theorem) in 3 dimensions – every 3-manifold can be cut into pieces, each of which has one of 8 possible geometries.
2-dimensional topology can be studied as complex geometry in one variable (Riemann surfaces are complex curves) – by the uniformization theorem every conformal class of metrics is equivalent to a unique complex one, and 4-dimensional topology can be studied from the point of view of complex geometry in two variables (complex surfaces), though not every 4-manifold admits a complex structure.
Knot theory.
Knot theory is the study of mathematical knots. While inspired by knots which appear in daily life in shoelaces and rope, a mathematician's knot differs in that the ends are joined together so that it cannot be undone. In mathematical language, a knot is an embedding of a circle in 3-dimensional Euclidean space, R3 (since we're using topology, a circle isn't bound to the classical geometric concept, but to all of its homeomorphisms). Two mathematical knots are equivalent if one can be transformed into the other via a deformation of R3 upon itself (known as an ambient isotopy); these transformations correspond to manipulations of a knotted string that do not involve cutting the string or passing the string through itself.
To gain further insight, mathematicians have generalized the knot concept in several ways. Knots can be considered in other three-dimensional spaces and objects other than circles can be used; see "knot (mathematics)". Higher-dimensional knots are "n"-dimensional spheres in "m"-dimensional Euclidean space.
High-dimensional geometric topology.
In high-dimensional topology, characteristic classes are a basic invariant, and surgery theory is a key theory.
A characteristic class is a way of associating to each principal bundle on a topological space "X" a cohomology class of "X". The cohomology class measures the extent to which the bundle is "twisted" — particularly, whether it possesses sections or not. In other words, characteristic classes are global invariants which measure the deviation of a local product structure from a global product structure. They are one of the unifying geometric concepts in algebraic topology, differential geometry and algebraic geometry.
Surgery theory is a collection of techniques used to produce one manifold from another in a 'controlled' way, introduced by . Surgery refers to cutting out parts of the manifold and replacing it with a part of another manifold, matching up along the cut or boundary. This is closely related to, but not identical with, handlebody decompositions. It is a major tool in the study and classification of manifolds of dimension greater than 3.
More technically, the idea is to start with a well-understood manifold "M" and perform surgery on it to produce a manifold "M "′ having some desired property, in such a way that the effects on the homology, homotopy groups, or other interesting invariants of the manifold are known.
The classification of exotic spheres by led to the emergence of surgery theory as a major tool in high-dimensional topology.

</doc>
<doc id="220642" url="https://en.wikipedia.org/wiki?curid=220642" title="Geometrization conjecture">
Geometrization conjecture

In mathematics, Thurston's geometrization conjecture states that certain three-dimensional topological spaces each have a unique geometric structure that can be associated with them. It is an analogue of the uniformization theorem for two-dimensional surfaces, which states that every simply-connected Riemann surface can be given one of three geometries (Euclidean, spherical, or hyperbolic).
In three dimensions, it is not always possible to assign a single geometry to a whole topological space. Instead, the geometrization conjecture states that every closed 3-manifold can be decomposed in a canonical way into pieces that each have one of eight types of geometric structure. The conjecture was proposed by , and implies several other conjectures, such as the Poincaré conjecture and Thurston's elliptization conjecture. 
Thurston's hyperbolization theorem implies that Haken manifolds satisfy the geometrization conjecture. Thurston announced a proof in the 1980s and since then several complete proofs have appeared in print.
Grigori Perelman sketched a proof of the full geometrization conjecture in 2003 using Ricci flow with surgery.
There are now several different manuscripts (see below) with details of the proof. The Poincaré conjecture and the spherical space form conjecture are corollaries of the geometrization conjecture, although there are shorter proofs of the former that do not lead to the geometrization conjecture.
The conjecture.
A 3-manifold is called closed if it is compact and has no boundary.
Every closed 3-manifold has a prime decomposition: this means it is the connected sum of prime three-manifolds (this decomposition is essentially unique except for a small problem in the case of non-orientable manifolds). This reduces much of the study of 3-manifolds to the case of prime 3-manifolds: those that cannot be written as a non-trivial connected sum. 
Here is a statement of Thurston's conjecture:
There are 8 possible geometric structures in 3 dimensions, described in the next section. There is a unique minimal way of cutting an irreducible oriented 3-manifold along tori into pieces that are Seifert manifolds or atoroidal called the JSJ decomposition, which is not quite the same as the decomposition in the geometrization conjecture, because some of the pieces in the JSJ decomposition might not have finite volume geometric structures. (For example, the mapping torus of an Anosov map of a torus has a finite volume solv structure, but its JSJ decomposition cuts it open along one torus to produce a product of a torus and a unit interval, and the interior of this has no finite volume geometric structure.)
For non-oriented manifolds the easiest way to state a geometrization conjecture is to first take the oriented double cover. It is also possible to work directly with non-orientable manifolds, but this gives some extra complications: it may be necessary to cut along projective planes and Klein bottles as well as spheres and tori, and manifolds with a projective plane boundary component usually have no geometric structure so this gives a minor extra complication. 
In 2 dimensions the analogous statement says that every surface (without boundary) has a geometric structure consisting of a metric with constant curvature; it is not necessary to cut the manifold up first.
The eight Thurston geometries.
A model geometry is a simply connected smooth manifold "X" together with a transitive action of a Lie group "G" on "X" with compact stabilizers.
A model geometry is called maximal if "G" is maximal among groups acting smoothly and transitively on "X" with compact stabilizers. Sometimes this condition is included in the definition of a model geometry. 
A geometric structure on a manifold "M" is a diffeomorphism from "M" to "X"/Γ for some model geometry "X", where Γ is a discrete subgroup of "G" acting freely on "X". If a given manifold admits a geometric structure, then it admits one whose model is maximal.
A 3-dimensional model geometry "X" is relevant to the geometrization conjecture if it is maximal and if there is at least one compact manifold with a geometric structure modelled on "X". Thurston classified the 8 model geometries satisfying these conditions; they are listed below and are sometimes called Thurston geometries. (There are also uncountably many model geometries without compact quotients.)
There is some connection with the Bianchi groups: the 3-dimensional Lie groups. Most Thurston geometries can be realized as a left invariant metric on a Bianchi group. However S2 × R cannot be, Euclidean space corresponds to two different Bianchi groups, and there are an uncountable number of solvable non-unimodular Bianchi groups, most of which give model geometries with no compact representatives.
Spherical geometry S3.
The point stabilizer is O(3, R), and the group "G" is the 6-dimensional Lie group O(4, R), with 2 components. The corresponding manifolds are exactly the closed 3-manifolds with finite fundamental group. Examples include the 3-sphere, the Poincaré homology sphere, Lens spaces. This geometry can be modeled as a left invariant metric on the Bianchi group of type IX. Manifolds with this geometry are all compact, orientable, and have the structure of a Seifert fiber space (often in several ways). The complete list of such manifolds is given in the article on Spherical 3-manifolds. Under Ricci flow manifolds with this geometry collapse to a point in finite time.
Euclidean geometry "E"3.
The point stabilizer is O(3, R), and the group "G" is the 6-dimensional Lie group R3 × O(3, R), with 2 components. Examples are the 3-torus, and more generally the mapping torus of a finite order automorphism of the 2-torus; see torus bundle. There are exactly 10 finite closed 3-manifolds with this geometry, 6 orientable and 4 non-orientable. This geometry can be modeled as a left invariant metric on the Bianchi groups of type I or VII0. Finite volume manifolds with this geometry are all compact, and have the structure of a Seifert fiber space (sometimes in two ways). The complete list of such manifolds is given in the article on Seifert fiber spaces. Under Ricci flow manifolds with Euclidean geometry remain invariant.
Hyperbolic geometry H3.
The point stabilizer is O(3, R), and the group "G" is the 6-dimensional Lie group O+(1, 3, R), with 2 components. There are enormous numbers of examples of these, and their classification is not completely understood. The example with smallest volume is the Weeks manifold. Other examples are given by the Seifert–Weber space, or "sufficiently complicated" Dehn surgeries on links, or most Haken manifolds. The geometrization conjecture implies that a closed 3-manifold is hyperbolic if and only if it is irreducible, atoroidal, and has infinite fundamental group. This geometry can be modeled as a left invariant metric on the Bianchi group of type V. Under Ricci flow manifolds with hyperbolic geometry expand.
The geometry of S2 × R.
The point stabilizer is O(2, R) × Z/2Z, and the group "G" is O(3, R) × R × Z/2Z, with 4 components. The four finite volume manifolds with this geometry are: S2 × S1, the mapping torus of the antipode map of S2, the connected sum of two copies of 3-dimensional projective space, and the product of S1 with two-dimensional projective space. The first two are mapping tori of the identity map and antipode map of the 2-sphere, and are the only examples of 3-manifolds that are prime but not irreducible. The third is the only example of a non-trivial connected sum with a geometric structure. This is the only model geometry that cannot be realized as a left invariant metric on a 3-dimensional Lie group. Finite volume manifolds with this geometry are all compact and have the structure of a Seifert fiber space (often in several ways). Under normalized Ricci flow manifolds with this geometry converge to a 1-dimensional manifold.
The geometry of H2 × R.
The point stabilizer is O(2, R) × Z/2Z, and the group "G" is O+(1, 2, R) × R × Z/2Z, with 4 components. Examples include the product of a hyperbolic surface with a circle, or more generally the mapping torus of an isometry of a hyperbolic surface. Finite volume manifolds with this geometry have the structure of a Seifert fiber space if they are orientable. (If they are not orientable the natural fibration by circles is not necessarily a Seifert fibration: the problem is that some fibers may "reverse orientation"; in other words their neighborhoods look like fibered solid Klein bottles rather than solid tori.) The classification of such (oriented) manifolds is given in the article on Seifert fiber spaces. This geometry can be modeled as a left invariant metric on the Bianchi group of type III. Under normalized Ricci flow manifolds with this geometry converge to a 2-dimensional manifold.
The geometry of the universal cover of SL(2, R).
formula_1 is the universal cover of SL(2, R), which fibers over H2. The point stabilizer is O(2, R). The group "G" has 2 components. Its identity component has the structure formula_2. Examples of these manifolds include: the manifold of unit vectors of the tangent bundle of a hyperbolic surface, and more generally the Brieskorn homology spheres (excepting the 3-sphere and the Poincare dodecahedral space). This geometry can be modeled as a left invariant metric on the Bianchi group of type VIII. Finite volume manifolds with this geometry are orientable and have the structure of a Seifert fiber space. The classification of such manifolds is given in the article on Seifert fiber spaces. Under normalized Ricci flow manifolds with this geometry converge to a 2-dimensional manifold.
Nil geometry.
This fibers over "E"2, and is the geometry of the Heisenberg group. The point stabilizer is O(2, R). The group "G" has 2 components, and is a semidirect product of the 3-dimensional Heisenberg group by the group O(2, R) of isometries of a circle. Compact manifolds with this geometry include the mapping torus of a Dehn twist of a 2-torus, or the quotient of the Heisenberg group by the "integral Heisenberg group". This geometry can be modeled as a left invariant metric on the Bianchi group of type II. Finite volume manifolds with this geometry are compact and orientable and have the structure of a Seifert fiber space. The classification of such manifolds is given in the article on Seifert fiber spaces. Under normalized Ricci flow, compact manifolds with this geometry converge to R2 with the flat metric.
Solv geometry.
This geometry fibers over the line with fiber the plane, and is the geometry of the identity component of the group "G". The point stabilizer is the dihedral group of order 8. The group "G" has 8 components, and is the group of maps from 2-dimensional Minkowski space to itself that are either isometries or multiply the metric by −1. The identity component has a normal subgroup R2 with quotient R, where R acts on R2 with 2 (real) eigenspaces, with distinct real eigenvalues of product 1. This is the Bianchi group of type VI0 and the geometry can be modeled as a left invariant metric on this group. All finite volume manifolds with solv geometry are compact. The compact manifolds with solv geometry are either the mapping torus of an Anosov map of the 2-torus (an automorphism of the 2-torus given by an invertible 2 by 2 matrix whose eigenvalues are real and distinct, such as formula_3, or quotients of these by groups of order at most 8. The eigenvalues of the automorphism of the torus generate an order of a real quadratic field, and the solv manifolds could in principle be classified in terms of the units and ideal classes of this order, though the details do not seem to be written down anywhere.
Under normalized Ricci flow compact manifolds with this geometry converge (rather slowly) to R1.
Uniqueness.
A closed 3-manifold has a geometric structure of at most one of the 8 types above, but finite volume non-compact 3-manifolds can occasionally have more than one type of geometric structure. (Nevertheless, a manifold can have many different geometric structures of the same type; for example, a surface of genus at least 2 has a continuum of different hyperbolic metrics.) More precisely, if "M" is a manifold with a finite volume geometric structure, then the type of geometric structure is almost determined as follows, in terms of the fundamental group π1("M"):
Infinite volume manifolds can have many different types of geometric structure: for example, R3 can have 6 of the different geometric structures listed above, as 6 of the 8 model geometries are homeomorphic to it. Moreover if the volume does not have to be finite there are an infinite number of new geometric structures with no compact models; for example, the geometry of almost any non-unimodular 3-dimensional Lie group.
There can be more than one way to decompose a closed 3-manifold into pieces with geometric structures. For example:
It is possible to choose a "canonical" decomposition into pieces with geometric structure, for example by first cutting the manifold into prime pieces in a minimal way, then cutting these up using the smallest possible number of tori. However this minimal decomposition is not necessarily the one produced by Ricci flow; if fact, the Ricci flow can cut up a manifold into geometric pieces in many inequivalent ways, depending on the choice of initial metric.
History.
The Fields Medal was awarded to Thurston in 1982 partially for his proof of the geometrization conjecture for Haken manifolds.
The case of 3-manifolds that should be spherical has been slower, but provided the spark needed for Richard Hamilton to develop his Ricci flow. In 1982, Hamilton showed that given a closed 3-manifold with a metric of positive Ricci curvature, the Ricci flow would collapse the manifold to a point in finite time, which proves the geometrization conjecture for this case as the metric becomes "almost round" just before the collapse. He later developed a program to prove the geometrization conjecture by Ricci flow with surgery. The idea is that the Ricci flow will in general produce singularities, but one may be able to continue the Ricci flow past the singularity by using surgery to change the topology of the manifold. Roughly speaking, the Ricci flow contracts positive curvature regions and expands negative curvature regions, so it should kill off the pieces of the manifold with the "positive curvature" geometries S3 and S2 × R, while what is left at large times should have a thick-thin decomposition into a "thick" piece with hyperbolic geometry and a "thin" graph manifold.
In 2003 Grigori Perelman sketched a proof of the geometrization conjecture by showing that the Ricci flow can indeed be continued past the singularities, and has the behavior described above. The main difficulty in verifying Perelman's proof of the Geometrization conjecture was a critical use of his Theorem 7.4 in the preprint 'Ricci Flow with surgery on three-manifolds'. This theorem was stated by Perelman without proof. There are now several different proofs of Perelman's Theorem 7.4, or variants of it which are sufficient to prove geometrization. There is the paper of Shioya and Yamaguchi that uses Perelman's stability theorem and a fibration theorem for Alexandrov spaces. This method, with full details leading to the proof of Geometrization, can be found in the exposition by B. Kleiner and J. Lott in 'Notes on Perelman's papers' in the journal Geometry & Topology.
A second route to the last part of Perelman's proof of Geometrization is the method of Bessières "et al.", which uses Thurston's hyperbolization theorem for Haken manifolds and Gromov's norm for 3-manifolds. A book by the same authors with complete details of their version of the proof has been published by the European Mathematical Society.
Also containing proofs of Perelman's Theorem 7.4, there is a paper of Morgan and Tian, another paper of Kleiner and Lott, and a paper by Cao and Ge.

</doc>
<doc id="18973446" url="https://en.wikipedia.org/wiki?curid=18973446" title="Geometry">
Geometry

Geometry (from the ; "geo-" "earth", "-metron" "measurement") is a branch of mathematics concerned with questions of shape, size, relative position of figures, and the properties of space. A mathematician who works in the field of geometry is called a geometer. Geometry arose independently in a number of early cultures as a body of practical knowledge concerning lengths, areas, and volumes, with elements of formal mathematical science emerging in the West as early as Thales (6th century BC). By the 3rd century BC, geometry was put into an axiomatic form by Euclid, whose treatment—Euclidean geometry—set a standard for many centuries to follow. Archimedes developed ingenious techniques for calculating areas and volumes, in many ways anticipating modern integral calculus. The field of astronomy, especially as it relates to mapping the positions of stars and planets on the celestial sphere and describing the relationship between movements of celestial bodies, served as an important source of geometric problems during the next one and a half millennia. In the classical world, both geometry and astronomy were considered to be part of the Quadrivium, a subset of the seven liberal arts considered essential for a free citizen to master.
The introduction of coordinates by René Descartes and the concurrent developments of algebra marked a new stage for geometry, since geometric figures such as plane curves could now be represented analytically in the form of functions and equations. This played a key role in the emergence of infinitesimal calculus in the 17th century. Furthermore, the theory of perspective showed that there is more to geometry than just the metric properties of figures: perspective is the origin of projective geometry. The subject of geometry was further enriched by the study of the intrinsic structure of geometric objects that originated with Euler and Gauss and led to the creation of topology and differential geometry.
In Euclid's time, there was no clear distinction between physical and geometrical space. Since the 19th-century discovery of non-Euclidean geometry, the concept of space has undergone a radical transformation and raised the question of which geometrical space best fits physical space.
With the rise of formal mathematics in the 20th century, 'space' (whether 'point', 'line', or 'plane') lost its intuitive contents, so today one has to distinguish between physical space, geometrical spaces (in which 'space', 'point' etc. still have their intuitive meanings) and abstract spaces.
Contemporary geometry considers manifolds, spaces that are considerably more abstract than the familiar Euclidean space, which they only approximately resemble at small scales. These spaces may be endowed with additional structure which allow one to speak about length. Modern geometry has many ties to physics as is exemplified by the links between pseudo-Riemannian geometry and general relativity. One of the youngest physical theories, string theory, is also very geometric in flavour.
While the visual nature of geometry makes it initially more accessible than other mathematical areas such as algebra or number theory, geometric language is also used in contexts far removed from its traditional, Euclidean provenance (for example, in fractal geometry and algebraic geometry).
Overview.
Because the recorded development of geometry spans more than two millennia, perceptions of what constitutes geometry have evolved throughout the ages:
Practical geometry.
Geometry originated as a practical science concerned with surveys, measurements, areas, and volumes. Among other highlights, notable accomplishments include formulas for lengths, areas and volumes, such as the Pythagorean theorem, circumference and area of a circle, area of a triangle, volume of a cylinder, sphere, and a pyramid. A method of computing certain inaccessible distances or heights based on similarity of geometric figures is attributed to Thales. The development of astronomy led to the emergence of trigonometry and spherical trigonometry, together with the attendant computational techniques.
Axiomatic geometry.
Euclid took a more abstract approach in his Elements, one of the most influential books ever written. Euclid introduced certain axioms, or postulates, expressing primary or self-evident properties of points, lines, and planes. He proceeded to rigorously deduce other properties by mathematical reasoning. The characteristic feature of Euclid's approach to geometry was its rigor, and it has come to be known as "axiomatic" or "synthetic" geometry. At the start of the 19th century, the discovery of non-Euclidean geometries by Nikolai Ivanovich Lobachevsky (1792–1856), János Bolyai (1802–1860) and Carl Friedrich Gauss (1777–1855) and others led to a revival of interest in this discipline, and in the 20th century, David Hilbert (1862–1943) employed axiomatic reasoning in an attempt to provide a modern foundation of geometry.
Geometric constructions.
Classical geometers paid special attention to constructing geometric objects that had been described in some other way. Classically, the only instruments allowed in geometric constructions are the compass and straightedge. Also, every construction had to be complete in a finite number of steps. However, some problems turned out to be difficult or impossible to solve by these means alone, and ingenious constructions using parabolas and other curves, as well as mechanical devices, were found.
Numbers in geometry.
In ancient Greece the Pythagoreans considered the role of numbers in geometry. However, the discovery of incommensurable lengths, which contradicted their philosophical views, made them abandon abstract numbers in favor of concrete geometric quantities, such as length and area of figures. Numbers were reintroduced into geometry in the form of coordinates by Descartes, who realized that the study of geometric shapes can be facilitated by their algebraic representation, and for whom the Cartesian plane is named. Analytic geometry applies methods of algebra to geometric questions, typically by relating geometric curves to algebraic equations. These ideas played a key role in the development of calculus in the 17th century and led to the discovery of many new properties of plane curves. Modern algebraic geometry considers similar questions on a vastly more abstract level.
Geometry of position.
Even in ancient times, geometers considered questions of relative position or spatial relationship of geometric figures and shapes. Some examples are given by inscribed and circumscribed circles of polygons, lines intersecting and tangent to conic sections, the Pappus and Menelaus configurations of points and lines. In the Middle Ages, new and more complicated questions of this type were considered: What is the maximum number of spheres simultaneously touching a given sphere of the same radius (kissing number problem)? What is the densest packing of spheres of equal size in space (Kepler conjecture)? Most of these questions involved 'rigid' geometrical shapes, such as lines or spheres. Projective, convex, and discrete geometry are three sub-disciplines within present day geometry that deal with these types of questions.
Leonhard Euler, in studying problems like the Seven Bridges of Königsberg, considered the most fundamental properties of geometric figures based solely on shape, independent of their metric properties. Euler called this new branch of geometry "geometria situs" (geometry of place), but it is now known as topology. Topology grew out of geometry, but turned into a large independent discipline. It does not differentiate between objects that can be continuously deformed into each other. The objects may nevertheless retain some geometry, as in the case of hyperbolic knots.
Geometry beyond Euclid.
In the nearly two thousand years since Euclid, while the range of geometrical questions asked and answered inevitably expanded, the basic understanding of space remained essentially the same. Immanuel Kant argued that there is only one, "absolute", geometry, which is known to be true "a priori" by an inner faculty of mind: Euclidean geometry was synthetic a priori. This dominant view was overturned by the revolutionary discovery of non-Euclidean geometry in the works of Bolyai, Lobachevsky, and Gauss (who never published his theory). They demonstrated that ordinary Euclidean space is only one possibility for development of geometry. A broad vision of the subject of geometry was then expressed by Riemann in his 1867 inauguration lecture "Über die Hypothesen, welche der Geometrie zu Grunde liegen" ("On the hypotheses on which geometry is based"), published only after his death. Riemann's new idea of space proved crucial in Einstein's general relativity theory, and Riemannian geometry, that considers very general spaces in which the notion of length is defined, is a mainstay of modern geometry.
Dimension.
Where the traditional geometry allowed dimensions 1 (a line), 2 (a plane) and 3 (our ambient world conceived of as three-dimensional space), mathematicians have used higher dimensions for nearly two centuries. Dimension has gone through stages of being any natural number "n", possibly infinite with the introduction of Hilbert space, and any positive real number in fractal geometry. Dimension theory is a technical area, initially within general topology, that discusses "definitions"; in common with most mathematical ideas, dimension is now defined rather than an intuition. Connected topological manifolds have a well-defined dimension; this is a theorem (invariance of domain) rather than anything "a priori".
The issue of dimension still matters to geometry, in the absence of complete answers to classic questions. Dimensions 3 of space and 4 of space-time are special cases in geometric topology. Dimension 10 or 11 is a key number in string theory. Research may bring a satisfactory "geometric" reason for the significance of 10 and 11 dimensions. 
Symmetry.
The theme of symmetry in geometry is nearly as old as the science of geometry itself. Symmetric shapes such as the circle, regular polygons and platonic solids held deep significance for many ancient philosophers and were investigated in detail before the time of Euclid. Symmetric patterns occur in nature and were artistically rendered in a multitude of forms, including the graphics of M. C. Escher. Nonetheless, it was not until the second half of 19th century that the unifying role of symmetry in foundations of geometry was recognized. Felix Klein's Erlangen program proclaimed that, in a very precise sense, symmetry, expressed via the notion of a transformation group, determines what geometry "is". Symmetry in classical Euclidean geometry is represented by congruences and rigid motions, whereas in projective geometry an analogous role is played by collineations, geometric transformations that take straight lines into straight lines. However it was in the new geometries of Bolyai and Lobachevsky, Riemann, Clifford and Klein, and Sophus Lie that Klein's idea to 'define a geometry via its symmetry group' proved most influential. Both discrete and continuous symmetries play prominent roles in geometry, the former in topology and geometric group theory, the latter in Lie theory and Riemannian geometry.
A different type of symmetry is the principle of duality in projective geometry (see Duality (projective geometry)) among other fields. This meta-phenomenon can roughly be described as follows: in any theorem, exchange "point" with "plane", "join" with "meet", "lies in" with "contains", and you will get an equally true theorem. A similar and closely related form of duality exists between a vector space and its dual space.
History.
The earliest recorded beginnings of geometry can be traced to ancient Mesopotamia and Egypt in the 2nd millennium BC. Early geometry was a collection of empirically discovered principles concerning lengths, angles, areas, and volumes, which were developed to meet some practical need in surveying, construction, astronomy, and various crafts. The earliest known texts on geometry are the Egyptian "Rhind Papyrus" (2000–1800 BC) and "Moscow Papyrus" (c. 1890 BC), the Babylonian clay tablets such as Plimpton 322 (1900 BC). For example, the Moscow Papyrus gives a formula for calculating the volume of a truncated pyramid, or frustum. South of Egypt the ancient Nubians established a system of geometry including early versions of sun clocks.
In the 7th century BC, the Greek mathematician Thales of Miletus used geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem. Pythagoras established the Pythagorean School, which is credited with the first proof of the Pythagorean theorem, though the statement of the theorem has a long history Eudoxus (408–c. 355 BC) developed the method of exhaustion, which allowed the calculation of areas and volumes of curvilinear figures, as well as a theory of ratios that avoided the problem of incommensurable magnitudes, which enabled subsequent geometers to make significant advances. Around 300 BC, geometry was revolutionized by Euclid, whose "Elements", widely considered the most successful and influential textbook of all time, introduced mathematical rigor through the axiomatic method and is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of the "Elements" were already known, Euclid arranged them into a single, coherent logical framework. The "Elements" was known to all educated people in the West until the middle of the 20th century and its contents are still taught in geometry classes today. Archimedes (c. 287–212 BC) of Syracuse used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave remarkably accurate approximations of Pi. He also studied the spiral bearing his name and obtained formulas for the volumes of surfaces of revolution.
Indian mathematicians also made many important contributions in geometry. The "Satapatha Brahmana" (9th century BC) contains rules for ritual geometric constructions that are similar to the "Sulba Sutras". According to , the "Śulba Sūtras" contain "the earliest extant verbal expression of the Pythagorean Theorem in the world, although it had already been known to the Old Babylonians. They contain lists of Pythagorean triples, which are particular cases of Diophantine equations.
In the Bakhshali manuscript, there is a handful of geometric problems (including problems about volumes of irregular solids). The Bakhshali manuscript also "employs a decimal place value system with a dot for zero." Aryabhata's "Aryabhatiya" (499) includes the computation of areas and volumes.
Brahmagupta wrote his astronomical work "" in 628. Chapter 12, containing 66 Sanskrit verses, was divided into two sections: "basic operations" (including cube roots, fractions, ratio and proportion, and barter) and "practical mathematics" (including mixture, mathematical series, plane figures, stacking bricks, sawing of timber, and piling of grain). In the latter section, he stated his famous theorem on the diagonals of a cyclic quadrilateral. Chapter 12 also included a formula for the area of a cyclic quadrilateral (a generalization of Heron's formula), as well as a complete description of rational triangles ("i.e." triangles with rational sides and rational areas).
In the Middle Ages, mathematics in medieval Islam contributed to the development of geometry, especially algebraic geometry and geometric algebra. Al-Mahani (b. 853) conceived the idea of reducing geometrical problems such as duplicating the cube to problems in algebra. Thābit ibn Qurra (known as Thebit in Latin) (836–901) dealt with arithmetic operations applied to ratios of geometrical quantities, and contributed to the development of analytic geometry. Omar Khayyám (1048–1131) found geometric solutions to cubic equations. The theorems of Ibn al-Haytham (Alhazen), Omar Khayyam and Nasir al-Din al-Tusi on quadrilaterals, including the Lambert quadrilateral and Saccheri quadrilateral, were early results in hyperbolic geometry, and along with their alternative postulates, such as Playfair's axiom, these works had a considerable influence on the development of non-Euclidean geometry among later European geometers, including Witelo (c. 1230–c. 1314), Gersonides (1288–1344), Alfonso, John Wallis, and Giovanni Girolamo Saccheri.
In the early 17th century, there were two important developments in geometry. The first was the creation of analytic geometry, or geometry with coordinates and equations, by René Descartes (1596–1650) and Pierre de Fermat (1601–1665). This was a necessary precursor to the development of calculus and a precise quantitative science of physics. The second geometric development of this period was the systematic study of projective geometry by Girard Desargues (1591–1661). Projective geometry is a geometry without measurement or parallel lines, just the study of how points are related to each other.
Two developments in geometry in the 19th century changed the way it had been studied previously. These were the discovery of non-Euclidean geometries by Nikolai Ivanovich Lobachevsky, János Bolyai and Carl Friedrich Gauss and of the formulation of symmetry as the central consideration in the Erlangen Programme of Felix Klein (which generalized the Euclidean and non-Euclidean geometries). Two of the master geometers of the time were Bernhard Riemann (1826–1866), working primarily with tools from mathematical analysis, and introducing the Riemann surface, and Henri Poincaré, the founder of algebraic topology and the geometric theory of dynamical systems. As a consequence of these major changes in the conception of geometry, the concept of "space" became something rich and varied, and the natural background for theories as different as complex analysis and classical mechanics.
Contemporary geometry.
Euclidean geometry.
Euclidean geometry has become closely connected with computational geometry, computer graphics, convex geometry, incidence geometry, finite geometry, discrete geometry, and some areas of combinatorics. Attention was given to further work on Euclidean geometry and the Euclidean groups by crystallography and the work of H. S. M. Coxeter, and can be seen in theories of Coxeter groups and polytopes. Geometric group theory is an expanding area of the theory of more general discrete groups, drawing on geometric models and algebraic techniques.
Differential geometry.
Differential geometry has been of increasing importance to mathematical physics due to Einstein's general relativity postulation that the universe is curved. Contemporary differential geometry is "intrinsic", meaning that the spaces it considers are smooth manifolds whose geometric structure is governed by a Riemannian metric, which determines how distances are measured near each point, and not "a priori" parts of some ambient flat Euclidean space.
Topology and geometry.
The field of topology, which saw massive development in the 20th century, is in a technical sense a type of transformation geometry, in which transformations are homeomorphisms. This has often been expressed in the form of the dictum 'topology is rubber-sheet geometry'. Contemporary geometric topology and differential topology, and particular subfields such as Morse theory, would be counted by most mathematicians as part of geometry. Algebraic topology and general topology have gone their own ways.
Algebraic geometry.
The field of algebraic geometry is the modern incarnation of the Cartesian geometry of co-ordinates. From late 1950s through mid-1970s it had undergone major foundational development, largely due to work of Jean-Pierre Serre and Alexander Grothendieck. This led to the introduction of schemes and greater emphasis on topological methods, including various cohomology theories. One of seven Millennium Prize problems, the Hodge conjecture, is a question in algebraic geometry.
The study of low-dimensional algebraic varieties, algebraic curves, algebraic surfaces and algebraic varieties of dimension 3 ("algebraic threefolds"), has been far advanced. Gröbner basis theory and real algebraic geometry are among more applied subfields of modern algebraic geometry. Arithmetic geometry is an active field combining algebraic geometry and number theory. Other directions of research involve moduli spaces and complex geometry. Algebro-geometric methods are commonly applied in string and brane theory.

</doc>
<doc id="2258117" url="https://en.wikipedia.org/wiki?curid=2258117" title="Global symmetry">
Global symmetry

In physics, a global symmetry is a symmetry that holds at all points in the spacetime under consideration, as opposed to a local symmetry which varies from point to point.
Global symmetries require conservation laws, but not forces, in physics.
An example of a global symmetry is the action of the formula_1 (for formula_2 a constant - making it a global transformation) group on the Dirac Lagrangian:
Under this transformation the wavefunction changes as formula_4 and formula_5 and so:

</doc>
<doc id="644391" url="https://en.wikipedia.org/wiki?curid=644391" title="Gluino">
Gluino

In supersymmetry, a gluino (symbol ) is the hypothetical supersymmetric partner of a gluon. Should they exist, gluinos are expected by supersymmetry theorists to be pair produced in particle accelerators such as the Large Hadron Collider.
In supersymmetric theories, gluinos are Majorana fermions and interact via the strong force as a color octet. Gluinos have a lepton number 0, baryon number 0, and spin 1/2. 
In models of supersymmetry that conserve R-parity, gluinos decay via the strong interaction to a squark and a quark, provided that an appropriate mass relation is satisfied.
The squark subsequently decays to another quark and the lightest supersymmetric particle, LSP (which leaves the detector unseen). This means that a typical signal for a gluino at a hadron collider would be four jets plus missing energy.
However if gluinos are lighter than squarks, 3-body decay of a gluino to a neutralino and a quark antiquark pair is kinematically accessible through an off-shell squark.

</doc>
<doc id="12666" url="https://en.wikipedia.org/wiki?curid=12666" title="Gluon">
Gluon

Gluons are elementary particles that act as the exchange particles (or gauge bosons) for the strong force between quarks, analogous to the exchange of photons in the electromagnetic force between two charged particles.
In technical terms, gluons are vector gauge bosons that mediate strong interactions of quarks in quantum chromodynamics (QCD). Gluons themselves carry the color charge of the strong interaction. This is unlike the photon, which mediates the electromagnetic interaction but lacks an electric charge. Gluons therefore participate in the strong interaction in addition to mediating it, making QCD significantly harder to analyze than QED (quantum electrodynamics).
Properties.
The gluon is a vector boson; like the photon, it has a spin of 1. While massive spin-1 particles have three polarization states, massless gauge bosons like the gluon have only two polarization states because gauge invariance requires the polarization to be transverse. In quantum field theory, unbroken gauge invariance requires that gauge bosons have zero mass (experiment limits the gluon's rest mass to less than a few meV/c2). The gluon has negative intrinsic parity.
Numerology of gluons.
Unlike the single photon of QED or the three W and Z bosons of the weak interaction, there are eight independent types of gluon in QCD.
This may be difficult to understand intuitively. Quarks carry three types of color charge; antiquarks carry three types of anticolor. Gluons may be thought of as carrying both color and anticolor, but to correctly understand how they are combined, it is necessary to consider the mathematics of color charge in more detail.
Color charge and superposition.
In quantum mechanics, the states of particles may be added according to the principle of superposition; that is, they may be in a "combined state" with a "probability", if some particular quantity is measured, of giving several different outcomes. A relevant illustration in the case at hand would be a gluon with a color state described by:
This is read as "red–antiblue plus blue–antired". (The factor of the square root of two is required for normalization, a detail that is not crucial to understand in this discussion.) If one were somehow able to make a direct measurement of the color of a gluon in this state, there would be a 50% chance of it having red-antiblue color charge and a 50% chance of blue-antired color charge.
Color singlet states.
It is often said that the stable strongly interacting particles (such as the proton and the neutron, i.e. hadrons) observed in nature are "colorless", but more precisely they are in a "color singlet" state, which is mathematically analogous to a "spin" singlet state. Such states allow interaction with other color singlets, but not with other color states; because long-range gluon interactions do not exist, this illustrates that gluons in the singlet state do not exist either.
The color singlet state is:
In words, if one could measure the color of the state, there would be equal probabilities of it being red-antired, blue-antiblue, or green-antigreen.
Eight gluon colors.
There are eight remaining independent color states, which correspond to the "eight types" or "eight colors" of gluons. Because states can be mixed together as discussed above, there are many ways of presenting these states, which are known as the "color octet". One commonly used list is:
These are equivalent to the Gell-Mann matrices; the translation between the two is that red-antired is the upper-left matrix entry, red-antiblue is the upper middle entry, blue-antigreen is the middle right entry, and so on. The critical feature of these particular eight states is that they are linearly independent, and also independent of the singlet state; there is no way to add any combination of states to produce any other. (It is also impossible to add them to make r, g, or b otherwise the forbidden singlet state could also be made.) There are many other possible choices, but all are mathematically equivalent, at least equally complex, and give the same physical results.
Group theory details.
Technically, QCD is a gauge theory with SU(3) gauge symmetry. Quarks are introduced as spinor fields in "N"f flavors, each in the fundamental representation (triplet, denoted 3) of the color gauge group, SU(3). The gluons are vector fields in the adjoint representation (octets, denoted 8) of color SU(3). For a general gauge group, the number of force-carriers (like photons or gluons) is always equal to the dimension of the adjoint representation. For the simple case of SU("N"), the dimension of this representation is .
In terms of group theory, the assertion that there are no color singlet gluons is simply the statement that quantum chromodynamics has an SU(3) rather than a U(3) symmetry. There is no known "a priori" reason for one group to be preferred over the other, but as discussed above, the experimental evidence supports SU(3). The U(1) group for electromagnetic field combines with a slightly more complicated group known as SU(2) – S stands for "special" – which means the corresponding matrices have determinant 1 in addition to being unitary.
Confinement.
Since gluons themselves carry color charge, they participate in strong interactions. These gluon-gluon interactions constrain color fields to string-like objects called "flux tubes", which exert constant force when stretched. Due to this force, quarks are confined within composite particles called hadrons. This effectively limits the range of the strong interaction to meters, roughly the size of an atomic nucleus. Beyond a certain distance, the energy of the flux tube binding two quarks increases linearly. At a large enough distance, it becomes energetically more favorable to pull a quark-antiquark pair out of the vacuum rather than increase the length of the flux tube.
Gluons also share this property of being confined within hadrons. One consequence is that gluons are not directly involved in the nuclear forces between hadrons. The force mediators for these are other hadrons called mesons.
Although in the normal phase of QCD single gluons may not travel freely, it is predicted that there exist hadrons that are formed entirely of gluons — called glueballs. There are also conjectures about other exotic hadrons in which real gluons (as opposed to virtual ones found in ordinary hadrons) would be primary constituents. Beyond the normal phase of QCD (at extreme temperatures and pressures), quark–gluon plasma forms. In such a plasma there are no hadrons; quarks and gluons become free particles.
Experimental observations.
Quarks and gluons (colored) manifest themselves by fragmenting into more quarks and gluons, which in turn hadronize into normal (colorless) particles, correlated in jets. As shown in 1978 summer conferences the PLUTO detector at the electron-positron collider DORIS (DESY) produced the first evidence that the hadronic decays of the very narrow resonance Υ(9.46) could be interpreted as three-jet event topologies produced by three gluons. Later published analyses by the same experiment confirmed this interpretation and also the spin 1 nature of the gluon (see also the recollection and PLUTO experiments).
In summer 1979 at higher energies at the electron-positron collider PETRA (DESY) again three-jet topologies were observed, now interpreted as q gluon bremsstrahlung, now clearly visible, by TASSO, 
MARK-J 
and PLUTO experiments (later in 1980 also by JADE). 
The spin 1 of the gluon was confirmed in 1980 by TASSO and PLUTO experiments (see also the review). In 1991 a subsequent experiment at the LEP storage ring at CERN again confirmed this result.
The gluons play an important role in the elementary strong interactions between quarks and gluons, described by QCD and studied particularly at the electron-proton collider HERA at DESY. The number and momentum distribution of the gluons in the proton (gluon density) have been measured by two experiments, H1 and ZEUS, in the years 1996 till today (2012). The gluon contribution to the proton spin has been studied by the HERMES experiment at HERA. The gluon density in the proton (when behaving hadronically) also has been measured.
Color confinement is verified by the failure of free quark searches (searches of fractional charges). Quarks are normally produced in pairs (quark + antiquark) to compensate the quantum color and flavor numbers; however at Fermilab single production of top quarks has been shown (technically this still involves a pair production, but quark and antiquark are of different flavor). No glueball has been demonstrated.
Deconfinement was claimed in 2000 at CERN SPS in heavy-ion collisions, and it implies a new state of matter: quark–gluon plasma, less interacting than in the nucleus, almost as in a liquid. It was found at the Relativistic Heavy Ion Collider (RHIC) at Brookhaven in the years 2004–2010 by four contemporaneous experiments. A quark–gluon plasma state has been confirmed at the CERN Large Hadron Collider (LHC) by the three experiments ALICE, ATLAS and CMS in 2010.
The Continuous Electron Beam Accelerator Facility at Jefferson Lab, also called the Thomas Jefferson National Accelerator Facility, in Newport News, Virginia is one of 10 Department of Energy facilities doing research on gluons. The Virginia lab is competing with another facility in Long Island, New York, Brookhaven National Laboratory, for funds to build a new electron-ion collider.

</doc>
<doc id="628183" url="https://en.wikipedia.org/wiki?curid=628183" title="Goldstone boson">
Goldstone boson

In particle and condensed matter physics, Goldstone bosons or Nambu–Goldstone bosons (NGBs) are bosons that appear necessarily in models exhibiting spontaneous breakdown of continuous symmetries. They were discovered by Yoichiro Nambu in the context of the BCS superconductivity mechanism, and subsequently elucidated by Jeffrey Goldstone, and systematically generalized in the context of quantum field theory.
These spinless bosons correspond to the spontaneously broken internal symmetry generators, and are characterized by the quantum numbers of these.
They transform nonlinearly (shift) under the action of these generators, and can thus be excited out of the asymmetric vacuum by these generators. Thus, they can be thought of as the excitations of the field in the broken symmetry directions in group space—and are massless if the spontaneously broken symmetry is "not also broken explicitly". 
If, instead, the symmetry is not exact, i.e. if it is "explicitly broken as well as spontaneously broken", then the Nambu–Goldstone bosons are not massless, though they typically remain relatively light; they are then called pseudo-Goldstone bosons or pseudo-Nambu–Goldstone bosons (abbreviated "PNGBs").
Goldstone's theorem.
Goldstone's theorem examines a generic continuous symmetry which is spontaneously broken; i.e., its currents are conserved, but the ground state is not invariant under the action of the corresponding charges. Then, necessarily, new massless (or light, if the symmetry is not exact) scalar particles appear in the spectrum of possible excitations. There is one scalar particle—called a Nambu–Goldstone boson—for each generator of the symmetry that is broken, i.e., that does not preserve the ground state. The Nambu–Goldstone mode is a long-wavelength fluctuation of the corresponding order parameter.
By virtue of their special properties in coupling to the vacuum of the respective symmetry-broken theory, vanishing momentum ("soft") Goldstone bosons involved in field-theoretic amplitudes make such amplitudes vanish ("Adler zeros").
In theories with gauge symmetry, the Goldstone bosons are "eaten" by the gauge bosons. The latter become massive and their new, longitudinal polarization is provided by the Goldstone boson.
Examples.
Theory.
Consider a complex scalar field , with the constraint that , a constant. One way to impose a constraint of this sort is by including a potential interaction term in its Lagrangian density, 
and taking the limit as (this is called the "Abelian nonlinear σ-model". It corresponds to the where the tip and the sides shoot to infinity, preserving the location of the minimum at its base).
The constraint, and the action, below, are invariant under a "U"(1) phase transformation, . The field can be redefined to give a real scalar field (i.e., a spin-zero particle) without any constraint by
where is the Nambu–Goldstone boson (actually is), and the "U"(1) symmetry transformation effects a shift on , namely 
but does not preserve the ground state , (i.e. the above infinitesimal transformation "does not annihilate it"—the hallmark of invariance), as evident in the charge of the current below.
Thus, the vacuum is degenerate and noninvariant under the action of the spontaneously broken symmetry.
The corresponding Lagrangian density is given by
and thus 
Note that the constant term in the Lagrangian density has no physical significance, and the other term in it is simply the kinetic term for a massless scalar.
The symmetry-induced conserved "U"(1) current is 
The charge, "Q", resulting from this current shifts and the ground state to a new, degenerate, ground state. Thus, a vacuum with will shift to a "different vacuum" with . The current connects the original vacuum with the Nambu–Goldstone boson state, .
In general, in a theory with several scalar fields, , the Nambu–Goldstone mode is massless, and parameterises the curve of possible (degenerate) vacuum states. Its hallmark under the broken symmetry transformation is nonvanishing vacuum expectation , an order parameter, for vanishing , at some ground state |0〉 chosen at the minimum of the potential, . Symmetry dictates that all variations of the potential with respect to the fields in all symmetry directions vanish. The vacuum value of the first order variation in any direction vanishes as just seen; while the vacuum value of the second order variation must also vanish, as follows. Vanishing vacuum values of field symmetry transformation increments add no new information. 
By contrast, however, "nonvanishing vacuum expectations of transformation increments", , specify the relevant (Goldstone) "null eigenvectors of the mass matrix",
and hence the corresponding zero-mass eigenvalues.
Goldstone's argument.
The principle behind Goldstone's argument is that the ground state is not unique. Normally, by current conservation, the charge operator for any symmetry current is 
time-independent,
Acting with the charge operator on the vacuum either "annihilates the vacuum", if that is symmetric; else, if "not", as is the case in spontaneous symmetry breaking, it produces a zero-frequency state out of it, through its shift transformation feature illustrated above. Actually, here, the charge itself is ill-defined. But its better 
behaved commutators with fields, so, then, the transformation shifts, are still time-invariant, , thus generating a in its Fourier transform.
Thus, if the vacuum is not invariant under the symmetry, action of the charge operator produces a state which is different from the vacuum chosen, but which has zero frequency. This is a long-wavelength oscillation of a field which is nearly stationary: there are physical states with zero frequency, , so that the theory cannot have a mass gap.
This argument is further clarified by taking the limit carefully. If an approximate charge operator acting in a huge but finite region "A" is applied to the vacuum,
a state with approximately vanishing time derivative is produced,
Assuming a nonvanishing mass gap "m"0, the frequency of any state like the above, which is orthogonal to the vacuum, is at least "m"0,
Letting "A" become large leads to a contradiction. Consequently "m"0 = 0.
Exception: This argument fails, however, when the symmetry is gauged, because then the symmetry generator is only performing a gauge transformation. A gauge transformed state is the same exact state, so that acting with a symmetry generator does not get one out of the vacuum. See Higgs mechanism.
Infraparticles.
There is an arguable loophole in the theorem. If one reads the theorem carefully, it only states that there exist non-vacuum states with arbitrarily small energies. Take for example a chiral N = 1 super QCD model with a nonzero squark VEV which is conformal in the IR. The chiral symmetry is a global symmetry which is (partially) spontaneously broken. Some of the "Goldstone bosons" associated with this spontaneous symmetry breaking are charged under the unbroken gauge group and hence, these composite bosons have a continuous mass spectrum with arbitrarily small masses but yet there is no Goldstone boson with exactly zero mass. In other words, the Goldstone bosons are infraparticles.
Nonrelativistic theories.
A version of Goldstone's theorem also applies to nonrelativistic theories (and also relativistic theories with spontaneously broken spacetime symmetries, such as Lorentz symmetry or conformal symmetry, rotational, or translational invariance).
It essentially states that, for each spontaneously broken symmetry, there corresponds some quasiparticle with no energy gap—the nonrelativistic version of the mass gap. (Note that the energy here is really "H"−"μN"−⋅ and not "H".) However, two "different" spontaneously broken generators may now give rise to the "same" Nambu–Goldstone boson. For example, in a superfluid, both the "U(1)" particle number symmetry and Galilean symmetry are spontaneously broken. However, the phonon is the Goldstone boson for both.
In general, the phonon is effectively the Nambu–Goldstone boson for spontaneously broken Galilean/Lorentz symmetry. However, in contrast to the case of internal symmetry breaking, when spacetime symmetries are broken, the order parameter "need not" be a scalar field, but may be a tensor field, and the corresponding independent massless modes may now be "fewer" than the number of spontaneously broken generators, because the 
Goldstone modes may now be linearly dependent among themselves: e.g., the Goldstone modes for some generators might be expressed as gradients of Goldstone modes for other broken generators.
Nambu–Goldstone fermions.
Spontaneously broken global fermionic symmetries, which occur in some supersymmetric models, lead to Nambu–Goldstone fermions, or "goldstinos". These have spin ½, instead of 0, and carry all quantum numbers of the respective supersymmetry generators broken spontaneously.
Spontaneous supersymmetry breaking smashes up ("reduces") supermultiplet structures into the characteristic nonlinear realizations of broken supersymmetry, so that goldstinos are superpartners of "all" particles in the theory, of "any spin", and the only superpartners, at that. That is, to say, two non-goldstino particles 
are connected to only goldstinos through supersymmetry transformations, and not to each other, even if they were so connected before the breaking of supersymmetry. As a result, the masses and spin multiplicities of such particles are then arbitrary.

</doc>
<doc id="12461" url="https://en.wikipedia.org/wiki?curid=12461" title="Gradient">
Gradient

In mathematics, the gradient is a generalization of the usual concept of derivative of a function in one dimension to a function in several dimensions. If is a differentiable, scalar-valued function of standard Cartesian coordinates in Euclidean space, its gradient is the vector whose components are the "n" partial derivatives of "f". It is thus a vector-valued function.
Similarly to the usual derivative, the gradient represents the slope of the tangent of the graph of the function. More precisely, the gradient points in the direction of the greatest rate of increase of the function and its magnitude is the slope of the graph in that direction. The components of the gradient in coordinates are the coefficients of the variables in the equation of the tangent space to the graph. This characterizing property of the gradient allows it to be defined independently of a choice of coordinate system, as a vector field whose components in a coordinate system will transform when going from one coordinate system to another.
The Jacobian is the generalization of the gradient for vector-valued functions of several variables and differentiable maps between Euclidean spaces or, more generally, manifolds. A further generalization for a function between Banach spaces is the Fréchet derivative.
Motivation.
Consider a room in which the temperature is given by a scalar field, , so at each point the temperature is . (We will assume that the temperature does not change over time.) At each point in the room, the gradient of "T" at that point will show the direction the temperature rises most quickly. The magnitude of the gradient will determine how fast the temperature rises in that direction.
Consider a surface whose height above sea level at a point ("x", "y") is "H"("x", "y"). The gradient of "H" at a point is a vector pointing in the direction of the steepest slope or grade at that point. The steepness of the slope at that point is given by the magnitude of the gradient vector.
The gradient can also be used to measure how a scalar field changes in other directions, rather than just the direction of greatest change, by taking a dot product. Suppose that the steepest slope on a hill is 40%. If a road goes directly up the hill, then the steepest slope on the road will also be 40%. If, instead, the road goes around the hill at an angle, then it will have a shallower slope. For example, if the angle between the road and the uphill direction, projected onto the horizontal plane, is 60°, then the steepest slope along the road will be 20%, which is 40% times the cosine of 60°.
This observation can be mathematically stated as follows. If the hill height function "H" is differentiable, then the gradient of "H" dotted with a unit vector gives the slope of the hill in the direction of the vector. More precisely, when "H" is differentiable, the dot product of the gradient of "H" with a given unit vector is equal to the directional derivative of "H" in the direction of that unit vector.
Definition.
The gradient (or gradient vector field) of a scalar function "f"("x"1, "x"2, "x"3, ..., "xn") is denoted ∇"f" or formula_1 where ∇ (the nabla symbol) denotes the vector differential operator, del. The notation "grad(f)" is also commonly used for the gradient. The gradient of "f" is defined as the unique vector field whose dot product with any vector v at each point "x" is the directional derivative of "f" along v. That is,
In a rectangular coordinate system, the gradient is the vector field whose components are the partial derivatives of "f":
where the e"i" are the orthogonal unit vectors pointing in the coordinate directions. When a function also depends on a parameter such as time, the gradient often refers simply to the vector of its spatial derivatives only.
In the three-dimensional Cartesian coordinate system, this is given by
where i, j, k are the standard unit vectors. For example, the gradient of the function
is:
In some applications it is customary to represent the gradient as a row vector or column vector of its components in a rectangular coordinate system.
Gradient and the derivative or differential.
Linear approximation to a function.
The gradient of a function "f" from the Euclidean space ℝ"n" to ℝ at any particular point "x"0 in ℝ"n" characterizes the best linear approximation to "f" at "x"0. The approximation is as follows:
for "x" close to "x"0, where formula_8 is the gradient of "f" computed at "x"0, and the dot denotes the dot product on ℝ"n". This equation is equivalent to the first two terms in the multi-variable Taylor Series expansion of "f" at "x"0.
Differential or (exterior) derivative.
The best linear approximation to a function
at a point "x" in ℝ"n" is a linear map from ℝ"n" to ℝ which is often denoted by d"fx" or "Df"("x") and called the differential or (total) derivative of "f" at "x". The gradient is therefore related to the differential by the formula
for any "v" ∈ ℝ"n". The function d"f", which maps "x" to d"f""x", is called the differential or exterior derivative of "f" and is an example of a differential 1-form.
If ℝ"n" is viewed as the space of (length "n") column vectors (of real numbers), then one can regard d"f" as the row vector with components
so that d"f""x"("v") is given by matrix multiplication. The gradient is then the corresponding column vector, i.e.,
Gradient as a derivative.
Let "U" be an open set in R"n". If the function is differentiable, then the differential of "f" is the (Fréchet) derivative of "f". Thus ∇"f" is a function from "U" to the space R such that
where ⋅ is the dot product.
As a consequence, the usual properties of the derivative hold for the gradient:
The gradient is linear in the sense that if "f" and "g" are two real-valued functions differentiable at the point , and α and β are two constants, then is differentiable at "a", and moreover
If "f" and "g" are real-valued functions differentiable at a point , then the product rule asserts that the product is differentiable at "a", and
Suppose that is a real-valued function defined on a subset "A" of R"n", and that "f" is differentiable at a point "a". There are two forms of the chain rule applying to the gradient. First, suppose that the function "g" is a parametric curve; that is, a function maps a subset into R"n". If "g" is differentiable at a point such that , then
where ∘ is the composition operator : (f ∘ g )(x) = f(g(x)).
More generally, if instead , then the following holds:
where ("Dg")T denotes the transpose Jacobian matrix.
For the second form of the chain rule, suppose that is a real valued function on a subset "I" of R, and that "h" is differentiable at the point . Then
Further properties and applications.
Level sets.
A level surface, or isosurface, is the set of all points where some function has a given value.
If "f" is differentiable, then the dot product of the gradient at a point "x" with a vector "v" gives the directional derivative of "f" at "x" in the direction "v". It follows that in this case the gradient of "f" is orthogonal to the level sets of "f". For example, a level surface in three-dimensional space is defined by an equation of the form . The gradient of "F" is then normal to the surface.
More generally, any embedded hypersurface in a Riemannian manifold can be cut out by an equation of the form such that d"F" is nowhere zero. The gradient of "F" is then normal to the hypersurface.
Similarly, an affine algebraic hypersurface may be defined by an equation , where "F" is a polynomial. The gradient of "F" is zero at a singular point of the hypersurface (this is the definition of a singular point). At a non-singular point, it is a nonzero normal vector.
Conservative vector fields and the gradient theorem.
The gradient of a function is called a gradient field. A (continuous) gradient field is always a conservative vector field: its line integral along any path depends only on the endpoints of the path, and can be evaluated by the gradient theorem (the fundamental theorem of calculus for line integrals). Conversely, a (continuous) conservative vector field is always the gradient of a function.
Riemannian manifolds.
For any smooth function f on a Riemannian manifold ("M","g"), the gradient of "f" is the vector field ∇"f" such that for any vector field "X",
where denotes the inner product of tangent vectors at "x" defined by the metric "g" and ∂"X""f" (sometimes denoted "X"("f")) is the function that takes any point to the directional derivative of "f" in the direction "X", evaluated at "x". In other words, in a coordinate chart φ from an open subset of "M" to an open subset of R"n", (∂"X""f")("x") is given by:
where "X""j" denotes the "j"th component of "X" in this coordinate chart.
So, the local form of the gradient takes the form:
Generalizing the case , the gradient of a function is related to its exterior derivative, since
More precisely, the gradient ∇"f" is the vector field associated to the differential 1-form d"f" using the musical isomorphism
(called "sharp") defined by the metric "g". The relation between the exterior derivative and the gradient of a function on R"n" is a special case of this in which the metric is the flat metric given by the dot product.
Cylindrical and spherical coordinates.
In cylindrical coordinates, the gradient is given by :
where ϕ is the azimuthal angle, "z" is the axial coordinate, and eρ, eφ and e"z" are unit vectors pointing along the coordinate directions.
In spherical coordinates :
where ϕ is the azimuth angle and θ is the zenith angle.
For the gradient in other orthogonal coordinate systems, see Orthogonal coordinates (Differential operators in three dimensions).
Generalizations.
Gradient of a vector.
In rectangular coordinates, the gradient of a vector field is defined by
where the Einstein summation notation is used and the product of the vectors ei, ek is a dyadic tensor of type (2,0), or the Jacobian matrix
In curvilinear coordinates, or more generally on a curved manifold, the gradient involves Christoffel symbols:
where "g""jk" are the components of the metric tensor and the e"i" are the coordinate vectors.
Expressed more invariantly, the gradient of a vector field f can be defined by the Levi-Civita connection and metric tensor:
where formula_30 is the connection.

</doc>
<doc id="12610" url="https://en.wikipedia.org/wiki?curid=12610" title="Grand Unified Theory">
Grand Unified Theory

A Grand Unified Theory (GUT) is a model in particle physics in which at high energy, the three gauge interactions of the Standard Model which define the electromagnetic, weak, and strong interactions or forces, are merged into one single force. This unified interaction is characterized by one larger gauge symmetry and thus several force carriers, but one unified coupling constant. If Grand Unification is realized in nature, there is the possibility of a grand unification epoch in the early universe in which the fundamental forces are not yet distinct.
Models that do not unify all interactions using one simple Lie group as the gauge symmetry, but do so using semisimple groups, can exhibit similar properties and are sometimes referred to as Grand Unified Theories as well.
Unifying gravity with the other three interactions would provide a theory of everything (TOE), rather than a GUT. Nevertheless, GUTs are often seen as an intermediate step towards a TOE.
The novel particles predicted by GUT models are expected to have masses around the GUT scale—just a few orders of magnitude below the Planck scale—and so will be well beyond the reach of any foreseen particle collider experiments. Therefore, the particles predicted by GUT models will be unable to be observed directly and instead the effects of grand unification might be detected through indirect observations such as proton decay, electric dipole moments of elementary particles, or the properties of neutrinos. Some grand unified theories predict the existence of magnetic monopoles.
, all GUT models which aim to be completely realistic are quite complicated, even compared to the Standard Model, because they need to introduce additional fields and interactions, or even additional dimensions of space. The main reason for this complexity lies in the difficulty of reproducing the observed fermion masses and mixing angles. Due to this difficulty, and due to the lack of any observed effect of grand unification so far, there is no generally accepted GUT model.
History.
Historically, the first true GUT which was based on the simple Lie group , was proposed by Howard Georgi and Sheldon Glashow in 1974. The Georgi–Glashow model was preceded by the semisimple Lie algebra Pati–Salam model by Abdus Salam and Jogesh Pati, who pioneered the idea to unify gauge interactions.
The acronym GUT was first coined in 1978 by CERN researchers John Ellis, Andrzej Buras, Mary K. Gaillard, and Dimitri Nanopoulos, however in the final version of their paper they opted for the less anatomical "GUM" (Grand Unification Mass). Nanopoulos later that year was the first to use the acronym in a paper.
Motivation.
The fact that the electric charges of electrons and protons seem to cancel each other exactly to extreme precision is essential for the existence of the macroscopic world as we know it, but this important property of elementary particles is not explained in the Standard Model of particle physics. While the description of strong and weak interactions within the Standard Model is based on gauge symmetries governed by the simple symmetry groups and which allow only discrete charges, the remaining component, the weak hypercharge interaction is described by an abelian symmetry which in principle allows for arbitrary charge assignments. The observed charge quantization, namely the fact that all known elementary particles carry electric charges which appear to be exact multiples of 1/3 of the "elementary" charge, has led to the idea that hypercharge interactions and possibly the strong and weak interactions might be embedded in one Grand Unified interaction described by a single, larger simple symmetry group containing the Standard Model. This would automatically predict the quantized nature and values of all elementary particle charges. Since this also results in a prediction for the relative strengths of the fundamental interactions which we observe, in particular the weak mixing angle, Grand Unification ideally reduces the number of independent input parameters, but is also constrained by observations.
Grand Unification is reminiscent of the unification of electric and magnetic forces by Maxwell's theory of electromagnetism in the 19th century, but its physical implications and mathematical structure are qualitatively different.
Unification of matter particles.
SU(5).
 is the simplest GUT. The smallest simple Lie group which contains the standard model, and upon which the first Grand Unified Theory was based, is
Such group symmetries allow the reinterpretation of several known particles as different states of a single particle field. However, it is not obvious that the simplest possible choices for the extended "Grand Unified" symmetry should yield the correct inventory of elementary particles. The fact that all currently known (2009) matter particles fit nicely into three copies of the smallest group representations of and immediately carry the correct observed charges, is one of the first and most important reasons why people believe that a Grand Unified Theory might actually be realized in nature.
The two smallest irreducible representations of are and . In the standard assignment, the contains the charge conjugates of the right-handed down-type quark color triplet and a left-handed lepton isospin doublet, while the contains the six up-type quark components, the left-handed down-type quark color triplet, and the right-handed electron. This scheme has to be replicated for each of the three known generations of matter. It is notable that the theory is anomaly free with this matter content.
The hypothetical right-handed neutrinos are a singlet of SU(5), which makes that its mass is not forbidden by any symmetry so it doesn't need a spontaneous symmetry breaking which explains why its mass would be heavy. (see seesaw mechanism).
SO(10).
The next simple Lie group which contains the standard model is
Here, the unification of matter is even more complete, since the irreducible spinor representation contains both the and of and a right-handed neutrino, and thus the complete particle content of one generation of the extended standard model with neutrino masses. This is already the largest simple group which achieves the unification of matter in a scheme involving only the already known matter particles (apart from the Higgs sector).
Since different standard model fermions are grouped together in larger representations, GUTs specifically predict relations among the fermion masses, such as between the electron and the down quark, the muon and the strange quark, and the tau lepton and the bottom quark for and . Some of these mass relations hold approximately, but most don't (see Georgi-Jarlskog mass relation).
The boson matrix for is found by taking the matrix from the representation of and adding an extra row and column for the right handed neutrino. The bosons are found by adding a partner to each of the 20 charged bosons (2 right-handed W bosons, 6 massive charged gluons and 12 X/Y type bosons) and adding an extra heavy neutral Z-boson to make 5 neutral bosons in total. The boson matrix will have a boson or its new partner in each row and column. These pairs combine to create the familiar 16D Dirac spinor matrices of .
SU(8).
Assuming 4 generations of fermions instead of 3 makes a total of types of particles. These can be put into representations of . This can be divided into which is the theory together with some heavy bosons which act on the generation number.
O(16).
Again assuming 4 generations of fermions, the 128 particles and anti-particles can be put into a single spinor representation of .
Symplectic groups and quaternion representations.
Symplectic gauge groups could also be considered. For example (which is called in the article symplectic group) has a representation in terms of quaternion unitary matrices which has a dimensional real representation and so might be considered as a candidate for a gauge group. has 32 charged bosons and 4 neutral bosons. Its subgroups include so can at least contain the gluons and photon of . Although it's probably not possible to have weak bosons acting on chiral fermions in this representation. A quaternion representation of the fermions might be:
A further complication with quaternion representations of fermions is that there are two types of multiplication: left multiplication and right multiplication which must be taken into account. It turns out that including left and right-handed quaternion matrices is equivalent to including a single right-multiplication by a unit quaternion which adds an extra SU(2) and so has an extra neutral boson and two more charged bosons. Thus the group of left and right handed quaternion matrcies is which does include the standard model bosons:
If formula_5 is a quaternion valued spinor, formula_6 is quaternion hermitian matrix coming from and formula_7 is a pure imaginary quaternion (both of which are 4-vector bosons) then the interaction term is:
E8 and octonion representations.
It can be noted that a generation of 16 fermions can be put into the form of an octonion with each element of the octonion being an 8-vector. If the 3 generations are then put in a 3x3 hermitian matrix with certain additions for the diagonal elements then these matrices form an exceptional (grassman-) Jordan algebra, which has the symmetry group of one of the exceptional Lie groups (F4, E6, E7 or E8) depending on the details.
Because they are fermions the anti-commutators of the Jordan algebra become commutators. It is known that E6 has subgroup and so is big enough to include the Standard Model. An E8 gauge group, for example, would have 8 neutral bosons, 120 charged bosons and 120 charged anti-bosons. To account for the 248 fermions in the lowest multiplet of E8, these would either have to include anti-particles (and so have baryogenesis), have new undiscovered particles, or have gravity-like (spin connection) bosons affecting elements of the particles spin direction. Each of these poses theoretical problems.
Beyond Lie groups.
Other structures have been suggested including Lie 3-algebras and Lie superalgebras. Neither of these fit with Yang–Mills theory. In particular Lie superalgebras would introduce bosons with the wrong statistics. Supersymmetry however does fit with Yang–Mills. For example N=4 Super Yang Mills Theory requires an gauge group.
Unification of forces and the role of supersymmetry.
The unification of forces is possible due to the energy scale dependence of force coupling parameters in quantum field theory called renormalization group running, which allows parameters with vastly different values at usual energies to converge to a single value at a much higher energy scale.
The renormalization group running of the three gauge couplings in the Standard Model has been found to nearly, but not quite, meet at the same point if the hypercharge is normalized so that it is consistent with or GUTs, which are precisely the GUT groups which lead to a simple fermion unification. This is a significant result, as other Lie groups lead to different normalizations. However, if the supersymmetric extension MSSM is used instead of the Standard Model, the match becomes much more accurate. In this case, the coupling constants of the strong and electroweak interactions meet at the grand unification energy, also known as the GUT scale:
It is commonly believed that this matching is unlikely to be a coincidence, and is often quoted as one of the main motivations to further investigate supersymmetric theories despite the fact that no supersymmetric partner particles have been experimentally observed (May 2015). Also, most model builders simply assume supersymmetry because it solves the hierarchy problem — i.e., it stabilizes the electroweak Higgs mass against radiative corrections.
Neutrino masses.
Since Majorana masses of the right-handed neutrino are forbidden by symmetry, GUTs predict the Majorana masses of right-handed neutrinos to be close to the GUT scale where the symmetry is spontaneously broken in those models. In supersymmetric GUTs, this scale tends to be larger than would be desirable to obtain realistic masses of the light, mostly left-handed neutrinos (see neutrino oscillation) via the seesaw mechanism.
Proposed theories.
Several such theories have been proposed, but none is currently universally accepted. An even more ambitious theory that includes "all" fundamental forces, including gravitation, is termed a theory of everything. Some common mainstream GUT models are:
Not quite GUTs:
"Note": These models refer to Lie algebras not to Lie groups. The Lie group could be , just to take a random example.
The most promising candidate is . (Minimal) does not contain any exotic fermions (i.e. additional fermions besides the Standard Model fermions and the right-handed neutrino), and it unifies each generation into a single irreducible representation. A number of other GUT models are based upon subgroups of . They are the minimal left-right model, , flipped and the Pati–Salam model. The GUT group E6 contains , but models based upon it are significantly more complicated. The primary reason for studying E6 models comes from heterotic string theory.
GUT models generically predict the existence of topological defects such as monopoles, cosmic strings, domain walls, and others. But none have been observed. Their absence is known as the monopole problem in cosmology. Most GUT models also predict proton decay, although not the Pati–Salam model; current experiments still haven't detected proton decay. This experimental limit on the proton's lifetime pretty much rules out minimal .
Some GUT theories like and suffer from what is called the doublet-triplet problem. These theories predict that for each electroweak Higgs doublet, there is a corresponding colored Higgs triplet field with a very small mass (many orders of magnitude smaller than the GUT scale here). In theory, unifying quarks with leptons, the Higgs doublet would also be unified with a Higgs triplet. Such triplets have not been observed. They would also cause extremely rapid proton decay (far below current experimental limits) and prevent the gauge coupling strengths from running together in the renormalization group.
Most GUT models require a threefold replication of the matter fields. As such, they do not explain why there are three generations of fermions. Most GUT models also fail to explain the little hierarchy between the fermion masses for different generations.
Ingredients.
A GUT model basically consists of a gauge group which is a compact Lie group, a connection form for that Lie group, a Yang–Mills action for that connection given by an invariant symmetric bilinear form over its Lie algebra (which is specified by a coupling constant for each factor), a Higgs sector consisting of a number of scalar fields taking on values within real/complex representations of the Lie group and chiral Weyl fermions taking on values within a complex rep of the Lie group. The Lie group contains the Standard Model group and the Higgs fields acquire VEVs leading to a spontaneous symmetry breaking to the Standard Model. The Weyl fermions represent matter.
Current status.
, there is still no hard evidence that nature is described by a Grand Unified Theory. The discovery of neutrino oscillations indicates that the Standard Model is incomplete and has led to renewed interest toward certain GUT such as . One of the few possible experimental tests of certain GUT is proton decay and also fermion masses. There are a few more special tests for supersymmetric GUT.
The gauge coupling strengths of QCD, the weak interaction and hypercharge seem to meet at a common length scale called the GUT scale and equal approximately to 1016 GeV, which is slightly suggestive. This interesting numerical observation is called the gauge coupling unification, and it works particularly well if one assumes the existence of superpartners of the Standard Model particles. Still it is possible to achieve the same by postulating, for instance, that ordinary (non supersymmetric) models break with an intermediate gauge scale, such as the one of Pati–Salam group

</doc>
<doc id="12401" url="https://en.wikipedia.org/wiki?curid=12401" title="Graph theory">
Graph theory

In mathematics and computer science, graph theory is the study of "graphs", which are mathematical structures used to model pairwise relations between objects. A graph in this context is made up of "vertices" or "nodes" or "points" and "edges" or "arcs" or "lines" that connect them. A graph may be "undirected", meaning that there is no distinction between the two vertices associated with each edge, or its edges may be "directed" from one vertex to another; see Graph (mathematics) for more detailed definitions and for other variations in the types of graph that are commonly considered. Graphs are one of the prime objects of study in discrete mathematics.
Refer to the glossary of graph theory for basic definitions in graph theory.
Definitions.
Definitions in graph theory vary. The following are some of the more basic ways of defining graphs and related mathematical structures.
Graph.
In the most common sense of the term, a graph is an ordered pair comprising a set "V" of "vertices" or "nodes" or "points" together with a set "E" of "edges" or "arcs" or "lines", which are 2-element subsets of "V" (i.e., an edge is related with two vertices, and the relation is represented as an unordered pair of the vertices with respect to the particular edge). To avoid ambiguity, this type of graph may be described precisely as undirected and simple.
Other senses of "graph" stem from different conceptions of the edge set. In one more generalized notion, "V" is a set together with a relation of "incidence" that associates with each edge two vertices. In another generalized notion, "E" is a multiset of unordered pairs of (not necessarily distinct) vertices. Many authors call this type of object a multigraph or pseudograph.
All of these variants and others are described more fully below.
The vertices belonging to an edge are called the "ends" or "end vertices" of the edge. A vertex may exist in a graph and not belong to an edge.
"V" and "E" are usually taken to be finite, and many of the well-known results are not true (or are rather different) for infinite graphs because many of the arguments fail in the infinite case. The "order" of a graph is |"V"|, its number of vertices. The "size" of a graph is |"E"|, its number of edges. The "degree" or "valency" of a vertex is the number of edges that connect to it, where an edge that connects a vertex to itself (a loop) is counted twice.
For an edge }, graph theorists usually use the somewhat shorter notation "xy".
Applications.
Graphs can be used to model many types of relations and processes in physical, biological, social and information systems. Many practical problems can be represented by graphs.
In computer science, graphs are used to represent networks of communication, data organization, computational devices, the flow of computation, etc. For instance, the link structure of a website can be represented by a directed graph, in which the vertices represent web pages and directed edges represent links from one page to another. A similar approach can be taken to problems in travel, biology, computer chip design, and many other fields. The development of algorithms to handle graphs is therefore of major interest in computer science. The transformation of graphs is often formalized and represented by graph rewrite systems. Complementary to graph transformation systems focusing on rule-based in-memory manipulation of graphs are graph databases geared towards transaction-safe, persistent storing and querying of graph-structured data.
Graph-theoretic methods, in various forms, have proven particularly useful in linguistics, since natural language often lends itself well to discrete structure. Traditionally, syntax and compositional semantics follow tree-based structures, whose expressive power lies in the principle of compositionality, modeled in a hierarchical graph. More contemporary approaches such as head-driven phrase structure grammar model the syntax of natural language using typed feature structures, which are directed acyclic graphs. Within lexical semantics, especially as applied to computers, modeling word meaning is easier when a given word is understood in terms of related words; semantic networks are therefore important in computational linguistics. Still other methods in phonology (e.g. optimality theory, which uses lattice graphs) and morphology (e.g. finite-state morphology, using finite-state transducers) are common in the analysis of language as a graph. Indeed, the usefulness of this area of mathematics to linguistics has borne organizations such as TextGraphs, as well as various 'Net' projects, such as WordNet, VerbNet, and others.
Graph theory is also used to study molecules in chemistry and physics. In condensed matter physics, the three-dimensional structure of complicated simulated atomic structures can be studied quantitatively by gathering statistics on graph-theoretic properties related to the topology of the atoms. In chemistry a graph makes a natural model for a molecule, where vertices represent atoms and edges bonds. This approach is especially used in computer processing of molecular structures, ranging from chemical editors to database searching. In statistical physics, graphs can represent local connections between interacting parts of a system, as well as the dynamics of a physical process on such
systems. Graphs are also used to represent the micro-scale channels of porous media, in which the vertices represent the pores and the edges represent the smaller channels connecting the pores.
Graph theory is also widely used in sociology as a way, for example, to measure actors' prestige or to explore rumor spreading, notably through the use of social network analysis software. Under the umbrella of social networks are many different types of graphs. Acquaintanceship and friendship graphs describe whether people know each other. Influence graphs model whether certain people can influence the behavior of others. Finally, collaboration graphs model whether two people work together in a particular way, such as acting in a movie together.
Likewise, graph theory is useful in biology and conservation efforts where a vertex can represent regions where certain species exist (or inhabit) and the edges represent migration paths, or movement between the regions. This information is important when looking at breeding patterns or tracking the spread of disease, parasites or how changes to the movement can affect other species.
In mathematics, graphs are useful in geometry and certain parts of topology such as knot theory. Algebraic graph theory has close links with group theory.
A graph structure can be extended by assigning a weight to each edge of the graph. Graphs with weights, or weighted graphs, are used to represent structures in which pairwise connections have some numerical values. For example, if a graph represents a road network, the weights could represent the length of each road.
History.
The paper written by Leonhard Euler on the "Seven Bridges of Königsberg" and published in 1736 is regarded as the first paper in the history of graph theory. This paper, as well as the one written by Vandermonde on the "knight problem," carried on with the "analysis situs" initiated by Leibniz. Euler's formula relating the number of edges, vertices, and faces of a convex polyhedron was studied and generalized by Cauchy and L'Huillier, and is at the origin of topology.
More than one century after Euler's paper on the bridges of Königsberg and while Listing introduced topology, Cayley was led by the study of particular analytical forms arising from differential calculus to study a particular class of graphs, the "trees". This study had many implications in theoretical chemistry. The involved techniques mainly concerned the enumeration of graphs having particular properties. Enumerative graph theory then rose from the results of Cayley and the fundamental results published by Pólya between 1935 and 1937 and the generalization of these by De Bruijn in 1959. Cayley linked his results on trees with the contemporary studies of chemical composition. The fusion of the ideas coming from mathematics with those coming from chemistry is at the origin of a part of the standard terminology of graph theory.
In particular, the term "graph" was introduced by Sylvester in a paper published in 1878 in "Nature", where he draws an analogy between "quantic invariants" and "co-variants" of algebra and molecular diagrams:
The first textbook on graph theory was written by Dénes Kőnig, and published in 1936. Another book by Frank Harary, published in 1969, was "considered the world over to be the definitive textbook on the subject", and enabled mathematicians, chemists, electrical engineers and social scientists to talk to each other. Harary donated all of the royalties to fund the Pólya Prize.
One of the most famous and stimulating problems in graph theory is the four color problem: "Is it true that any map drawn in the plane may have its regions colored with four colors, in such a way that any two regions having a common border have different colors?" This problem was first posed by Francis Guthrie in 1852 and its first written record is in a letter of De Morgan addressed to Hamilton the same year. Many incorrect proofs have been proposed, including those by Cayley, Kempe, and others. The study and the generalization of this problem by Tait, Heawood, Ramsey and Hadwiger led to the study of the colorings of the graphs embedded on surfaces with arbitrary genus. Tait's reformulation generated a new class of problems, the "factorization problems", particularly studied by Petersen and Kőnig. The works of Ramsey on colorations and more specially the results obtained by Turán in 1941 was at the origin of another branch of graph theory, "extremal graph theory".
The four color problem remained unsolved for more than a century. In 1969 Heinrich Heesch published a method for solving the problem using computers. A computer-aided proof produced in 1976 by Kenneth Appel and Wolfgang Haken makes fundamental use of the notion of "discharging" developed by Heesch. The proof involved checking the properties of 1,936 configurations by computer, and was not fully accepted at the time due to its complexity. A simpler proof considering only 633 configurations was given twenty years later by Robertson, Seymour, Sanders and Thomas.
The autonomous development of topology from 1860 and 1930 fertilized graph theory back through the works of Jordan, Kuratowski and Whitney. Another important factor of common development of graph theory and topology came from the use of the techniques of modern algebra. The first example of such a use comes from the work of the physicist Gustav Kirchhoff, who published in 1845 his Kirchhoff's circuit laws for calculating the voltage and current in electric circuits.
The introduction of probabilistic methods in graph theory, especially in the study of Erdős and Rényi of the asymptotic probability of graph connectivity, gave rise to yet another branch, known as "random graph theory", which has been a fruitful source of graph-theoretic results.
Graph drawing.
Graphs are represented visually by drawing a dot or circle for every vertex, and drawing an arc between two vertices if they are connected by an edge. If the graph is directed, the direction is indicated by drawing an arrow.
A graph drawing should not be confused with the graph itself (the abstract, non-visual structure) as there are several ways to structure the graph drawing. All that matters is which vertices are connected to which others by how many edges and not the exact layout. In practice it is often difficult to decide if two drawings represent the same graph. Depending on the problem domain some layouts may be better suited and easier to understand than others.
The pioneering work of W. T. Tutte was very influential in the subject of graph drawing. Among other achievements, he introduced the use of linear algebraic methods to obtain graph drawings.
Graph drawing also can be said to encompass problems that deal with the crossing number and its various generalizations. The crossing number of a graph is the minimum number of intersections between edges that a drawing of the graph in the plane must contain. For a planar graph, the crossing number is zero by definition.
Drawings on surfaces other than the plane are also studied.
Graph-theoretic data structures.
There are different ways to store graphs in a computer system. The data structure used depends on both the graph structure and the algorithm used for manipulating the graph. Theoretically one can distinguish between list and matrix structures but in concrete applications the best structure is often a combination of both. List structures are often preferred for sparse graphs as they have smaller memory requirements. Matrix structures on the other hand provide faster access for some applications but can consume huge amounts of memory.
List structures include the incidence list, an array of pairs of vertices, and the adjacency list, which separately lists the neighbors of each vertex: Much like the incidence list, each vertex has a list of which vertices it is adjacent to.
Matrix structures include the incidence matrix, a matrix of 0's and 1's whose rows represent vertices and whose columns represent edges, and the adjacency matrix, in which both the rows and columns are indexed by vertices. In both cases a 1 indicates two adjacent objects and a 0 indicates two non-adjacent objects. The Laplacian matrix is a modified form of the adjacency matrix that incorporates information about the degrees of the vertices, and is useful in some calculations such as Kirchhoff's theorem on the number of spanning trees of a graph.
The distance matrix, like the adjacency matrix, has both its rows and columns indexed by vertices, but rather than containing a 0 or a 1 in each cell it contains the length of a shortest path between two vertices.
Problems in graph theory.
Enumeration.
There is a large literature on graphical enumeration: the problem of counting graphs meeting specified conditions. Some of this work is found in Harary and Palmer (1973).
Subgraphs, induced subgraphs, and minors.
A common problem, called the subgraph isomorphism problem, is finding a fixed graph as a subgraph in a given graph. One reason to be interested in such a question is that many graph properties are "hereditary" for subgraphs, which means that a graph has the property if and only if all subgraphs have it too.
Unfortunately, finding maximal subgraphs of a certain kind is often an NP-complete problem. For example:
A similar problem is finding induced subgraphs in a given graph. Again, some important graph properties are hereditary with respect to induced subgraphs, which means that a graph has a property if and only if all induced subgraphs also have it. Finding maximal induced subgraphs of a certain kind is also often NP-complete. For example:
Still another such problem, the minor containment problem, is to find a fixed graph as a minor of a given graph. A minor or subcontraction of a graph is any graph obtained by taking a subgraph and contracting some (or no) edges. Many graph properties are hereditary for minors, which means that a graph has a property if and only if all minors have it too. For example:
Another class of problems has to do with the extent to which various species and generalizations of graphs are determined by their "point-deleted subgraphs". For example:
Graph coloring.
Many problems have to do with various ways of coloring graphs, for example:
Subsumption and unification.
Constraint modeling theories concern families of directed graphs related by a partial order. In these applications, graphs are ordered by specificity, meaning that more constrained graphs—which are more specific and thus contain a greater amount of information—are subsumed by those that are more general. Operations between graphs include evaluating the direction of a subsumption relationship between two graphs, if any, and computing graph unification. The unification of two argument graphs is defined as the most general graph (or the computation thereof) that is consistent with (i.e. contains all of the information in) the inputs, if such a graph exists; efficient unification algorithms are known.
For constraint frameworks which are strictly compositional, graph unification is the sufficient satisfiability and combination function. Well-known applications include automatic theorem proving and modeling the elaboration of linguistic structure.
Network flow.
There are numerous problems arising especially from applications that have to do with various notions of flows in networks, for example:
Covering problems.
Covering problems in graphs are specific instances of subgraph-finding problems, and they tend to be closely related to the clique problem or the independent set problem.
Decomposition problems.
Decomposition, defined as partitioning the edge set of a graph (with as many vertices as necessary accompanying the edges of each part of the partition), has a wide variety of question. Often, it is required to decompose a graph into subgraphs isomorphic to a fixed graph; for instance, decomposing a complete graph into Hamiltonian cycles. Other problems specify a family of graphs into which a given graph should be decomposed, for instance, a family of cycles, or decomposing a complete graph "K""n" into specified trees having, respectively, 1, 2, 3, …, edges.
Some specific decomposition problems that have been studied include:
Graph classes.
Many problems involve characterizing the members of various classes of graphs. Some examples of such questions are below:

</doc>
<doc id="38454" url="https://en.wikipedia.org/wiki?curid=38454" title="Gravitational constant">
Gravitational constant

The gravitational constant, approximately and denoted by letter , is an empirical physical constant involved in the calculation(s) of gravitational force between two bodies. It usually appears in Sir Isaac Newton's law of universal gravitation, and in Albert Einstein's general theory of relativity. It is also known as the universal gravitational constant, Newton's constant, and colloquially as Big G. It should not be confused with "small g" ("g"), which is the local gravitational field of the Earth (equivalent to the free-fall acceleration).
Laws and constants.
According to the law of universal gravitation, the attractive force ("F") between two bodies is directly proportional to the product of their masses ("m"1 and "m"2), and inversely proportional to the square of the distance, "r", (inverse-square law) between them:
The constant of proportionality, "G", is the gravitational constant.
The gravitational constant is a physical constant that is difficult to measure with high accuracy. In SI units, the 2014 CODATA-recommended value of the gravitational constant (with standard uncertainty in parentheses) is:
with relative standard uncertainty .
Dimensions, units, and magnitude.
The dimensions assigned to the gravitational constant in the equation above—length cubed, divided by mass, and by time squared (in SI units, meters cubed per kilogram per second squared)—are those needed to balance the units of measurements in gravitational equations. However, these dimensions have fundamental significance in terms of Planck units; when expressed in SI units, the gravitational constant is dimensionally and numerically equal to the cube of the Planck length divided by the product of the Planck mass and the square of Planck time.
In natural units, of which Planck units are a common example, "G" and other physical constants such as "c" (the speed of light) may be set equal to 1.
In many secondary school texts, the dimensions of "G" are derived from force in order to assist student comprehension:
In cgs, "G" can be written as:
Applying Kepler's 3rd law, in Newtonian form, to the Earth's orbit:
where distance is measured in astronomical units (AU), time in years, and mass in solar masses .
In other fields of astrophysics, where distances are measured in parsecs (pc), velocities in kilometers per second (km/s) and masses again in solar units , it is useful to express "G" as:
"G" can also be given as:
Given the fact that the period "P" of an object in circular orbit around a spherical object obeys
where "V" is the volume inside the radius of the orbit, we see that
This way of expressing "G" shows the relationship between the average density of a planet and the period of a satellite orbiting just above its surface.
The gravitational force is extremely weak compared with other fundamental forces. For example, the gravitational force between an electron and proton one meter apart is approximately , whereas the electromagnetic force between the same two particles is approximately . Both these forces are weak when compared with the forces we are able to experience directly, but the electromagnetic force in this example is some 39 orders of magnitude (i.e. 1039) greater than the force of gravity—roughly the same ratio as the mass of the Sun compared to a microgram.
History of measurement.
The gravitational constant appears in Newton's law of universal gravitation, but it was not measured until seventy-one years after Newton's death by Henry Cavendish with his Cavendish experiment, performed in 1798 ("Philosophical Transactions" 1798). Cavendish measured "G" implicitly, using a torsion balance invented by the geologist Rev. John Michell. He used a horizontal torsion beam with lead balls whose inertia (in relation to the torsion constant) he could tell by timing the beam's oscillation. Their faint attraction to other balls placed alongside the beam was detectable by the deflection it caused. Cavendish's aim was not actually to measure the gravitational constant, but rather to measure the Earth's density relative to water, through the precise knowledge of the gravitational interaction. In retrospect, the density that Cavendish calculated implies a value for "G" of .
The accuracy of the measured value of "G" has increased only modestly since the original Cavendish experiment. "G" is quite difficult to measure, as gravity is much weaker than other fundamental forces, and an experimental apparatus cannot be separated from the gravitational influence of other bodies. Furthermore, gravity has no established relation to other fundamental forces, so it does not appear possible to calculate it indirectly from other constants that can be measured more accurately, as is done in some other areas of physics. Published values of "G" have varied rather broadly, and some recent measurements of high precision are, in fact, mutually exclusive. This led to the 2010 CODATA value by NIST having 20% increased uncertainty than in 2006. For the 2014 update, CODATA reduced the uncertainty to less than half the 2010 value.
In the January 2007 issue of "Science", Fixler et al. described a new measurement of the gravitational constant by atom interferometry, reporting a value of . An improved cold atom measurement by Rosi et al. was published in 2014 of .
A 2015 study of all previous measurements of "G" led to the discovery that most of the mutually exclusive values can be explained by a periodic variation. This variation has a period of 5.9 years, similar to one observed in length of day (LOD) measurements, hinting at a common physical cause which is not necessarily a variation in "G". The observed behaviour can be confirmed by repeated measurements over the next half decade or so. Combined with new methods of measuring "G", such as quantum interferometry, this could help establish a more precise value of "G".
Under the assumption that the physics of type Ia supernovae are universal, analysis of observations of 580 type Ia supernovae has shown that the gravitational constant has varied by less than one part in ten billion per year over the last nine billion years.
The "GM" product.
The quantity "GM"—the product of the gravitational constant and the mass of a given astronomical body such as the Sun or the Earth—is known as the standard gravitational parameter and is denoted formula_10. Depending on the body concerned, it may also be called the geocentric or heliocentric gravitational constant, among other names.
This quantity gives a convenient simplification of various gravity-related formulas. Also, for celestial bodies such as the Earth and the Sun, the value of the product "GM" is known much more accurately than each factor independently. Indeed, the limited accuracy available for "G" limits the accuracy of scientific determination of such masses in the first place.
For Earth, using formula_11 as the symbol for the mass of the Earth, we have
For Sun, we have
Calculations in celestial mechanics can also be carried out using the unit of solar mass rather than the standard SI unit kilogram. In this case we use the Gaussian gravitational constant "k", where
and
If instead of mean solar day we use the sidereal year as our time unit, the value of "ks" is very close to 2π ().
The standard gravitational parameter "GM" appears as above in Newton's law of universal gravitation, as well as in formulas for the deflection of light caused by gravitational lensing, in Kepler's laws of planetary motion, and in the formula for escape velocity.

</doc>
<doc id="579026" url="https://en.wikipedia.org/wiki?curid=579026" title="Gravitational potential">
Gravitational potential

In classical mechanics, the gravitational potential at a location is equal to the work (energy transferred) per unit mass that would be done by the force of gravity if an object were moved from its location in space to a fixed reference location. It is analogous to the electric potential with mass playing the role of charge. The reference location, where the potential is zero, is by convention infinitely far away from any mass, resulting in a negative potential at any finite distance.
In mathematics the gravitational potential is also known as the Newtonian potential and is fundamental in the study of potential theory.
Potential energy.
The gravitational potential ("V") is the gravitational potential energy ("U") per unit mass:
where "m" is the mass of the object. Potential energy is equal (in magnitude, but negative) to the work done by the gravitational field moving a body to its given position in space from infinity. If the body has a mass of 1 unit, then the potential energy to be assigned to that body is equal to the gravitational potential. So the potential can be interpreted as the negative of the work done by the gravitational field moving a unit mass in from infinity.
In some situations, the equations can be simplified by assuming a field that is nearly independent of position. For instance, in daily life, in the region close to the surface of the Earth, the gravitational acceleration can be considered constant. In that case, the difference in potential energy from one height to another is to a good approximation linearly related to the difference in height:
Mathematical form.
The potential "V" of a unit mass "m" at a distance "x" from a point mass of mass "M" can be defined as the work "W" done by the gravitational field "F" bringing the unit mass in from infinity to that point:
where "G" is the gravitational constant. The potential has units of energy per unit mass, e.g., J/kg in the MKS system. By convention, it is always negative where it is defined, and as "x" tends to infinity, it approaches zero.
The gravitational field, and thus the acceleration of a small body in the space around the massive object, is the negative gradient of the gravitational potential. Thus the negative of a negative gradient yields positive acceleration toward a massive object. Because the potential has no angular components, its gradient is
where x is a vector of length "x" pointing from the point mass toward the small body and formula_5 is a unit vector pointing from the point mass toward the small body. The magnitude of the acceleration therefore follows an inverse square law:
The potential associated with a mass distribution is the superposition of the potentials of point masses. If the mass distribution is a finite collection of point masses, and if the point masses are located at the points x1, ..., x"n" and have masses "m"1, ..., "m""n", then the potential of the distribution at the point x is
If the mass distribution is given as a mass measure "dm" on three-dimensional Euclidean space R3, then the potential is the convolution of −G/|r| with "dm". In good cases this equals the integral
where |x − r| is the distance between the points x and r. If there is a function "ρ"(r) representing the density of the distribution at r, so that ", where "dv"(r) is the Euclidean volume element, then the gravitational potential is the volume integral
If "V" is a potential function coming from a continuous mass distribution "ρ"(r), then "ρ" can be recovered using the Laplace operator, Δ:
This holds pointwise whenever "ρ" is continuous and is zero outside of a bounded set. In general, the mass measure "dm" can be recovered in the same way if the Laplace operator is taken in the sense of distributions. As a consequence, the gravitational potential satisfies Poisson's equation. See also Green's function for the three-variable Laplace equation and Newtonian potential.
Spherical symmetry.
A spherically symmetric mass distribution behaves to an observer completely outside the distribution as though all of the mass were concentrated at the center, and thus effectively as a point mass, by the shell theorem. On the surface of the earth, the acceleration is given by so-called standard gravity "g", approximately 9.8 m/s2, although this value varies slightly with latitude and altitude: The magnitude of the acceleration is a little larger at the poles than at the equator because Earth is an oblate spheroid.
Within a spherically symmetric mass distribution, it is possible to solve Poisson's equation in spherical coordinates. Within a uniform spherical body of radius "R" and density ρ the gravitational force "g" inside the sphere varies linearly with distance "r" from the center, giving the gravitational potential inside the sphere, which is
which differentiably connects to the potential function for the outside of the sphere (see the figure at the top).
General relativity.
In general relativity, the gravitational potential is replaced by the metric tensor. When the gravitational field is weak and the sources are moving very slowly compared to light-speed, general relativity reduces to Newtonian gravity, and the metric tensor can be expanded in terms of the gravitational potential.
Multipole expansion.
The potential at a point x is given by
The potential can be expanded in a series of Legendre polynomials. Represent the points x and r as position vectors relative to the center of mass. The denominator in the integral is expressed as the square root of the square to give
V(\mathbf{x}) &= - \int_{\mathbb{R}^3} \frac{G}{ \sqrt
Compare the gravity at these locations.

</doc>
<doc id="10346" url="https://en.wikipedia.org/wiki?curid=10346" title="Gravitational redshift">
Gravitational redshift

In astrophysics, gravitational redshift or Einstein shift is the process by which electromagnetic radiation originating from a source that is in a gravitational field is reduced in frequency, or redshifted, when observed in a region of a weaker gravitational field. This is a direct result of gravitational time dilation - as one moves away from a source of gravitational field, the rate at which time passes is increased relative to the case when one is near the source. As frequency is inverse of time (specifically, time required for completing one wave oscillation), frequency of the electromagnetic radiation is reduced in an area of a lower gravitational field (i.e., a higher gravitational potential). There is a corresponding reduction in energy when electromagnetic radiation is red-shifted, as given by Planck's relation, due to the electromagnetic radiation propagating in opposition to the gravitational gradient. There also exists a corresponding blueshift when electromagnetic radiation propagates from an area of a weaker gravitational field to an area of a stronger gravitational field.
If applied to optical wavelengths, this manifests itself as a change in the colour of visible light as the wavelength of the light is increased toward the red part of the light spectrum. Since frequency and wavelength are inversely proportional, this is equivalent to saying that the frequency of the light is reduced towards the red part of the light spectrum, giving this phenomenon the name redshift.
Definition.
Redshift is often denoted with the dimensionless variable formula_1, defined as the fractional change of the wavelength
formula_2
where
formula_3 is the wavelength of the electromagnetic radiation (photon) as measured by the observer.
formula_4 is the wavelength of the electromagnetic radiation (photon) when measured at the source of emission.
The gravitational redshift of a photon can be calculated in the framework of general relativity (using the Schwarzschild metric) as
formula_5
with the Schwarzschild radius
formula_6,
where formula_7 denotes Newton's gravitational constant, formula_8 the mass of the gravitating body, formula_9 the speed of light, and formula_10 the distance between the center of mass of the gravitating body and the point at which the photon is emitted. The redshift is not defined for photons emitted inside the Schwarzschild radius, the distance from the body where the escape velocity is greater than the speed of light. Therefore this formula only applies when formula_10 is larger as formula_12. When the photon is emitted at a distance equal to the Schwarzschild radius, the redshift will be infinitely large and it can't escape to any finite distance from this Schwarzschild sphere! When the photon is emitted at an infinitely large distance, there is no redshift.
In the Newtonian limit, i.e. when formula_10 is sufficiently large compared to the Schwarzschild radius formula_12, the redshift can be approximated by a binomial expansion to become
formula_15
The redshift formula for the frequency formula_16 (and therefore also for the energy formula_17 of a photon) can simply deduced from the wavelength-formula above to be
formula_18
with formula_19 the emitted frequency at the emission point and formula_20 the frequency at distance formula_21 from the center of mass of the gravitating body causing this gravitational potential. Moreover we get from the law of energy conservation formula_22 the general case for a photon of frequency formula_23 emitted at distance formula_24 to observer distance formula_25 (measured as distances from the gravitational center of mass) the equation
formula_26
as long as formula_27 holds.
History.
The gravitational weakening of light from high-gravity stars was predicted by John Michell in 1783 and Pierre-Simon Laplace in 1796, using Isaac Newton's concept of light corpuscles (see: emission theory) and who predicted that some stars would have a gravity so strong that light would not be able to escape. The effect of gravity on light was then explored by Johann Georg von Soldner (1801), who calculated the amount of deflection of a light ray by the sun, arriving at the Newtonian answer which is half the value predicted by general relativity. All of this early work assumed that light could slow down and fall, which was inconsistent with the modern understanding of light waves.
Once it became accepted that light was an electromagnetic wave, it was clear that the frequency of light should not change from place to place, since waves from a source with a fixed frequency keep the same frequency everywhere. One way around this conclusion would be if time itself were altered—if clocks at different points had different rates.
This was precisely Einstein's conclusion in 1911. He considered an accelerating box, and noted that according to the special theory of relativity, the clock rate at the bottom of the box was slower than the clock rate at the top. Nowadays, this can be easily shown in accelerated coordinates. The metric tensor in units where the speed of light is one is:
and for an observer at a constant value of r, the rate at which a clock ticks, R(r), is the square root of the time coefficient, R(r)=r. The acceleration at position r is equal to the curvature of the hyperbola at fixed r, and like the curvature of the nested circles in polar coordinates, it is equal to 1/r.
So at a fixed value of g, the fractional rate of change of the clock-rate, the percentage change in the ticking at the top of an accelerating box vs at the bottom, is:
The rate is faster at larger values of R, away from the apparent direction of acceleration. The rate is zero at r=0, which is the location of the acceleration horizon.
Using the principle of equivalence, Einstein concluded that the same thing holds in any gravitational field, that the rate of clocks R at different heights was altered according to the gravitational field g. When g is slowly varying, it gives the fractional rate of change of the ticking rate. If the ticking rate is everywhere almost this same, the fractional rate of change is the same as the absolute rate of change, so that:
Since the rate of clocks and the gravitational potential have the same derivative, they are the same up to a constant. The constant is chosen to make the clock rate at infinity equal to 1. Since the gravitational potential is zero at infinity:
where the speed of light has been restored to make the gravitational potential dimensionless.
The coefficient of the formula_32 in the metric tensor is the square of the clock rate, which for small values of the potential is given by keeping only the linear term:
and the full metric tensor is:
where again the c's have been restored. This expression is correct in the full theory of general relativity, to lowest order in the gravitational field, and ignoring the variation of the space-space and space-time components of the metric tensor, which only affect fast moving objects.
Using this approximation, Einstein reproduced the incorrect Newtonian value for the deflection of light in 1909. But since a light beam is a fast moving object, the space-space components contribute too. After constructing the full theory of general relativity in 1916, Einstein solved for the space-space components in a post-Newtonian approximation, and calculated the correct amount of light deflection – double the Newtonian value. Einstein's prediction was confirmed by many experiments, starting with Arthur Eddington's 1919 solar eclipse expedition.
The changing rates of clocks allowed Einstein to conclude that light waves change frequency as they move, and the frequency/energy relationship for photons allowed him to see that this was best interpreted as the effect of the gravitational field on the mass–energy of the photon. To calculate the changes in frequency in a nearly static gravitational field, only the time component of the metric tensor is important, and the lowest order approximation is accurate enough for ordinary stars and planets, which are much bigger than their Schwarzschild radius.
Initial verification.
A number of experimenters initially claimed to have identified the effect using astronomical measurements, and the effect was eventually considered to have been finally identified in the spectral lines of the star Sirius B by W.S. Adams in 1925. However, measurements of the effect before the 1960s have been critiqued by ("e.g.", by C.M. Will), and the effect is now considered to have been definitively verified by the experiments of Pound, Rebka and Snider between 1959 and 1965.
The Pound–Rebka experiment of 1959 measured the gravitational redshift in spectral lines using a terrestrial 57Fe gamma source. This was documented by scientists of the Lyman Laboratory of Physics at Harvard University. A commonly cited experimental verification is the Pound–Snider experiment of 1965. James W. Brault, a graduate student of Robert Dicke at Princeton University, measured the gravitational redshift of the sun using optical methods in 1962.
More information can be seen at Tests of general relativity.
Application.
Gravitational redshift is studied in many areas of astrophysical research.
Exact Solutions.
A table of exact solutions of the Einstein field equations consists of the following:
The more often used exact equation for gravitational redshift applies to the case outside of a non-rotating, uncharged mass which is spherically symmetric. The equation is:
formula_36, where
Gravitational redshift versus gravitational time dilation.
When using special relativity's relativistic Doppler relationships to calculate the change in energy and frequency (assuming no complicating route-dependent effects such as those caused by the frame-dragging of rotating black holes), then the gravitational redshift and blueshift frequency ratios are the inverse of each other, suggesting that the "seen" frequency-change corresponds to the actual difference in underlying clockrate. Route-dependence due to frame-dragging may come into play, which would invalidate this idea and complicate the process of determining globally agreed differences in underlying clock rate.
While gravitational redshift refers to what is seen, gravitational time dilation refers to what is deduced to be "really" happening once observational effects are taken into account.

</doc>
<doc id="54244" url="https://en.wikipedia.org/wiki?curid=54244" title="Gravitational singularity">
Gravitational singularity

A gravitational singularity or spacetime singularity is a location where the quantities that are used to measure the gravitational field of a celestial body become infinite in a way that does not depend on the coordinate system. These quantities are the scalar invariant curvatures of spacetime, which includes a measure of the density of matter. The laws of normal spacetime could not exist within a singularity.
For the purposes of proving the Penrose–Hawking singularity theorems, a spacetime with a singularity is defined to be one that contains geodesics that cannot be extended in a smooth manner. The end of such a geodesic is considered to be the singularity. This is a different definition, useful for proving theorems.
The two most important types of spacetime singularities are "curvature singularities" and "conical singularities". Singularities can also be divided according to whether or not they are covered by an event horizon (naked singularities are not covered). According to modern general relativity, the initial state of the universe, at the beginning of the Big Bang, was a singularity. Both general relativity and quantum mechanics break down in describing the earliest moments of the Big Bang, but in general, quantum mechanics does not permit particles to inhabit a space smaller than their wavelengths. Another type of singularity predicted by general relativity is inside a black hole: any star collapsing beyond a certain point (the Schwarzschild radius) would form a black hole, inside which a singularity (covered by an event horizon) would be formed, as all the matter would flow into a certain point (or a circular line, if the black hole is rotating). This is again according to general relativity without quantum mechanics, which forbids wavelike particles entering a space smaller than their wavelength. These hypothetical singularities are also known as curvature singularities.
Interpretation.
Many theories in physics have mathematical singularities of one kind or another. Equations for these physical theories predict that the ball of mass of some quantity becomes infinite or increases without limit. This is generally a sign for a missing piece in the theory, as in the ultraviolet catastrophe, renormalization, and instability of a hydrogen atom predicted by the Larmor formula.
In supersymmetry, a singularity in the moduli space happens usually when there are additional massless degrees of freedom in that certain point. Similarly, it is thought that singularities in spacetime often mean that there are additional degrees of freedom that exist only within the vicinity of the singularity. The same fields related to the whole spacetime also exist; for example, the electromagnetic field. In known examples of string theory, the latter degrees of freedom are related to closed strings, while the degrees of freedom are "stuck" to the singularity and related either to open strings or to the twisted sector of an orbifold.
Some theories, such as the theory of loop quantum gravity suggest that singularities may not exist. The idea is that due to quantum gravity effects, there is a minimum distance beyond which the force of gravity no longer continues to increase as the distance between the masses becomes shorter.
The Einstein-Cartan-Sciama-Kibble theory of gravity naturally averts the gravitational singularity at the Big Bang. This theory extends general relativity to matter with intrinsic angular momentum (spin) by removing a constraint of the symmetry of the affine connection and regarding its antisymmetric part, the torsion tensor, as a variable in varying the action. The minimal coupling between torsion and Dirac spinors generates a spin–spin interaction in fermionic matter, which becomes dominant at extremely high densities and prevents the scale factor of the Universe from reaching zero. The Big Bang is replaced by a cusp-like Big Bounce at which the matter has an enormous but finite density and before which the Universe was contracting.
Types.
Curvature.
Solutions to the equations of general relativity or another theory of gravity (such as supergravity) often result in encountering points where the metric blows up to infinity. However, many of these points are completely regular, and the infinities are merely a result of using an inappropriate coordinate system at this point. In order to test whether there is a singularity at a certain point, one must check whether at this point diffeomorphism invariant quantities (i.e. scalars) become infinite. Such quantities are the same in every coordinate system, so these infinities will not "go away" by a change of coordinates.
An example is the Schwarzschild solution that describes a non-rotating, uncharged black hole. In coordinate systems convenient for working in regions far away from the black hole, a part of the metric becomes infinite at the event horizon. However, spacetime at the event horizon is regular. The regularity becomes evident when changing to another coordinate system (such as the Kruskal coordinates), where the metric is perfectly smooth. On the other hand, in the center of the black hole, where the metric becomes infinite as well, the solutions suggest a singularity exists. The existence of the singularity can be verified by noting that the Kretschmann scalar, being the square of the Riemann tensor i.e. formula_1, which is diffeomorphism invariant, is infinite.
While in a non-rotating black hole the singularity occurs at a single point in the model coordinates, called a "point singularity". In a rotating black hole, also known as a Kerr black hole, the singularity occurs on a ring (a circular line), known as a "ring singularity". Such a singularity may also theoretically become a wormhole.
More generally, a spacetime is considered singular if it is geodesically incomplete, meaning that there are freely-falling particles whose motion cannot be determined beyond a finite time, being after the point of reaching the singularity. For example, any observer inside the event horizon of a non-rotating black hole would fall into its center within a finite period of time. The classical version of the Big Bang cosmological model of the universe contains a causal singularity at the start of time ("t"=0), where all time-like geodesics have no extensions into the past. Extrapolating backward to this hypothetical time 0 results in a universe with all spatial dimensions of size zero, infinite density, infinite temperature, and infinite space-time curvature.
Conical.
A conical singularity occurs when there is a point where the limit of every diffeomorphism invariant quantity is finite, in which case spacetime is not smooth at the point of the limit itself. Thus, spacetime looks like a cone around this point, where the singularity is located at the tip of the cone. The metric can be finite everywhere if a suitable coordinate system is used.
An example of such a conical singularity is a cosmic string.
Naked.
Until the early 1990s, it was widely believed that general relativity hides every singularity behind an event horizon, making naked singularities impossible. This is referred to as the cosmic censorship hypothesis. However, in 1991, physicists Stuart Shapiro and Saul Teukolsky performed computer simulations of a rotating plane of dust that indicated that general relativity might allow for "naked" singularities. What these objects would actually look like in such a model is unknown. Nor is it known whether singularities would still arise if the simplifying assumptions used to make the simulation were removed.
Entropy.
Before Stephen Hawking came up with the concept of Hawking radiation, the question of black holes having entropy was avoided. However, this concept demonstrates that black holes can radiate energy, which conserves entropy and solves the incompatibility problems with the second law of thermodynamics. Entropy, however, implies heat and therefore temperature. The loss of energy also suggests that black holes do not last forever, but rather "evaporate" slowly. Small black holes tend to be hotter whereas larger ones tend to be colder. All known black hole candidates are so large that their temperature is far below that of the cosmic background radiation, so they are all gaining energy. They will not begin to lose energy until a cosmological redshift of more than one million is reached, rather than the thousand or so since the background radiation formed.

</doc>
<doc id="852089" url="https://en.wikipedia.org/wiki?curid=852089" title="Gravitational time dilation">
Gravitational time dilation

Gravitational time dilation is a form of time dilation, an actual difference of elapsed time between two events as measured by observers situated at varying distances from a gravitating mass. The stronger the gravitational potential (the closer the clock is to the source of gravitation), the slower time passes. Albert Einstein originally predicted this effect in his theory of relativity and it has since been confirmed by tests of general relativity.
This has been demonstrated by noting that atomic clocks at differing altitudes (and thus different gravitational potential) will eventually show different times. The effects detected in such earth-bound experiments are "extremely" small, with differences being measured in nanoseconds. Demonstrating greater effects would require greater distances from the earth and/or a larger gravitational source.
Gravitational time dilation was first described by Albert Einstein in 1907 as a consequence of special relativity in accelerated frames of reference. In general relativity, it is considered to be a difference in the passage of proper time at different positions as described by a metric tensor of spacetime. The existence of gravitational time dilation was first confirmed directly by the Pound–Rebka experiment in 1959.
Definition.
Clocks that are far from massive bodies (or at higher gravitational potentials) run more quickly, and clocks close to massive bodies (or at lower gravitational potentials) run more slowly. For example, considered over the total lifetime of the earth (4.6 Gyr), a clock set at the peak of Mount Everest would be about 39 hours ahead of a clock set at sea level. This is because gravitational time dilation is manifested in accelerated frames of reference or, by virtue of the equivalence principle, in the gravitational field of massive objects.
According to general relativity, inertial mass and gravitational mass are the same, and all accelerated reference frames (such as a uniformly rotating reference frame with its proper time dilation) are physically equivalent to a gravitational field of the same strength.
Let us consider a family of observers along a straight "vertical" line, each of whom experiences a constant g-force along this line (e.g., a long accelerating spacecraft, a skyscraper, a shaft on a planet). Let formula_1 be the dependence of g-force on "height", a coordinate along aforementioned line. The equation with respect to a base observer at formula_2 is
where formula_4 is the "total" time dilation at a distant position formula_5, formula_1 is the dependence of g-force on "height" formula_5, formula_8 is the speed of light, and formula_9 denotes exponentiation by e.
For simplicity, in a Rindler's family of observers in a flat space-time, the dependence would be
with constant formula_11, which yields
On the other hand, when formula_13 is nearly constant and formula_14 is much smaller than formula_15, the linear "weak field" approximation formula_16 may also be used.
See Ehrenfest paradox for application of the same formula to a rotating reference frame in flat space-time.
Outside a non-rotating sphere.
A common equation used to determine gravitational time dilation is derived from the Schwarzschild metric, which describes spacetime in the vicinity of a non-rotating massive spherically symmetric object. The equation is:
where
To illustrate then, without accounting for the effects of rotation, proximity to the Earth's gravitational well will cause a clock on the planet's surface to accumulate around 0.0219 fewer seconds over a period of one year than would a distant observer's clock. In comparison, a clock on the surface of the sun will accumulate around 66.4 fewer seconds in one year.
Circular orbits.
In the Schwarzschild metric, free-falling objects can be in circular orbits if the orbital radius is larger than formula_26 (the radius of the photon sphere). The formula for a clock at rest is given above; for a clock in a circular orbit, the formula is instead.
Experimental confirmation.
Gravitational time dilation has been experimentally measured using atomic clocks on airplanes. The clocks aboard the airplanes were slightly faster than clocks on the ground. The effect is significant enough that the Global Positioning System's artificial satellites need to have their clocks corrected.
Additionally, time dilations due to height differences of less than 1 meter have been experimentally verified in the laboratory.
Gravitational time dilation has also been confirmed by the Pound–Rebka experiment, observations of the spectra of the white dwarf Sirius B, and experiments with time signals sent to and from Viking 1 Mars lander.

</doc>
<doc id="633233" url="https://en.wikipedia.org/wiki?curid=633233" title="Gravitino">
Gravitino

In supergravity theories combining general relativity and supersymmetry, the gravitino () is the gauge fermion supersymmetric partner of the hypothesized graviton. It has been suggested as a candidate for dark matter.
If it exists, it is a fermion of spin and therefore obeys the Rarita-Schwinger equation. The gravitino field is conventionally written as ψμα with a four-vector index and a spinor index.
For one would get negative norm modes, as with every massless particle of spin 1 or higher. These modes are unphysical, and for consistency there must be a gauge symmetry which cancels these modes: where εα(x) is a spinor function of spacetime. This gauge symmetry is a local supersymmetry transformation, and the resulting theory is supergravity.
Thus the gravitino is the fermion mediating supergravity interactions, just as the photon is mediating electromagnetism, and the graviton is presumably mediating gravitation. Whenever supersymmetry is broken in supergravity theories, it acquires a mass which is determined by the scale at which supersymmetry is broken. This varies greatly between different models of supersymmetry breaking, but if supersymmetry is to solve the hierarchy problem of the Standard Model, the gravitino cannot be more massive than about 1 TeV/c2.
Gravitino cosmological problem.
If the gravitino indeed has a mass of the order of TeV, then it creates a problem in the standard model of cosmology, at least naïvely.
One option is that the gravitino is stable. This would be the case if the gravitino is the lightest supersymmetric particle and R-parity is conserved (or nearly so). In this case the gravitino is a candidate for dark matter; as such gravitinos will have been created in the very early universe. However, one may calculate the density of gravitinos and it turns out to be much higher than the observed dark matter density.
The other option is that the gravitino is unstable. Thus the gravitinos mentioned above would decay and will not contribute to the observed dark matter density. However, since they decay only through gravitational interactions, their lifetime would be very long, of the order of in natural units, where "Mpl" is the Planck mass and "m" is the mass of a gravitino. For a gravitino mass of the order of TeV this would be , much later than the era of nucleosynthesis. At least one possible channel of decay must include either a photon, a charged lepton or a meson, each of which would be energetic enough to destroy a nucleus if it strikes one. One can show that enough such energetic particles will be created in the decay as to destroy almost all the nuclei created in the era of nucleosynthesis, in contrast with observations. In fact, in such a case the universe would have been made of hydrogen alone, and star formation would probably be impossible.
One possible solution to the cosmological gravitino problem is the split supersymmetry model, where the gravitino mass is much higher than the TeV scale, but other fermionic supersymmetric partners of standard model particles already appear at this scale.
Another solution is that R-parity is slightly violated and the gravitino is the lightest supersymmetric particle. This causes almost all supersymmetric particles in the early Universe to decay into Standard Model particles via R-parity violating interactions well before the synthesis of primordial nuclei; a small fraction however decay into gravitinos, whose half-life is orders of magnitude greater than the age of the Universe due to the suppression of the decay rate by the Planck scale and the small R-parity violating couplings.

</doc>
<doc id="24794048" url="https://en.wikipedia.org/wiki?curid=24794048" title="Gravitoelectromagnetism">
Gravitoelectromagnetism

Gravitoelectromagnetism, abbreviated GEM, refers to a set of formal analogies between the equations for electromagnetism and relativistic gravitation; specifically: between Maxwell's field equations and an approximation, valid under certain conditions, to the Einstein field equations for general relativity. Gravitomagnetism is a widely used term referring specifically to the kinetic effects of gravity, in analogy to the magnetic effects of moving electric charge. The most common version of GEM is valid only far from isolated sources, and for slowly moving test particles.
The analogy and equations differing only by some small factors were first published in 1893, before general relativity, by Oliver Heaviside as a separate theory expanding Newton's law.
Background.
This approximate reformulation of gravitation as described by general relativity in the weak field limit makes an apparent field appear in a frame of reference different from that of a freely moving inertial body. This apparent field may be described by two components that act respectively like the electric and magnetic fields of electromagnetism, and by analogy these are called the "gravitoelectric" and "gravitomagnetic" fields, since these arise in the same way around a mass that a moving electric charge is the source of electric and magnetic fields. The main consequence of the "gravitomagnetic" field, or velocity-dependent acceleration, is that a moving object near a rotating massive object will experience acceleration not predicted by a purely Newtonian (gravitoelectric) gravity field. More subtle predictions, such as induced rotation of a falling object and precession of a spinning object are among the last basic predictions of general relativity to be directly tested.
Indirect validations of gravitomagnetic effects have been derived from analyses of relativistic jets. Roger Penrose had proposed a frame dragging mechanism for extracting energy and momentum from rotating black holes. Reva Kay Williams, University of Florida, developed a rigorous proof that validated Penrose's mechanism. Her model showed how the Lense–Thirring effect could account for the observed high energies and luminosities of quasars and active galactic nuclei; the collimated jets about their polar axis; and the asymmetrical jets (relative to the orbital plane). All of those observed properties could be explained in terms of gravitomagnetic effects. Williams' application of Penrose's mechanism can be applied to black holes of any size. Relativistic jets can serve as the largest and brightest form of validations for gravitomagnetism.
A group at Stanford University is currently analyzing data from the first direct test of GEM, the Gravity Probe B satellite experiment, to see if they are consistent with gravitomagnetism. The Apache Point Observatory Lunar Laser-ranging Operation also plans to observe gravitomagnetism effects.
Equations.
According to general relativity, the gravitational field produced by a rotating object (or any rotating mass–energy) can, in a particular limiting case, be described by equations that have the same form as in classical electromagnetism. Starting from the basic equation of general relativity, the Einstein field equation, and assuming a weak gravitational field or reasonably flat spacetime, the gravitational analogs to Maxwell's equations for electromagnetism, called the "GEM equations", can be derived. GEM equations compared to Maxwell's equations in SI units are:
where:
Lorentz force.
For a test particle whose mass "m" is "small", in a stationary system, the net (Lorentz) force acting on it due to a GEM field is described by the following GEM analog to the Lorentz force equation:
where:
Poynting vector.
The GEM Poynting vector compared to the electromagnetic Poynting vector is given by
Scaling of fields.
The literature does not adopt a consistent scaling for the gravitoelectric and gravitomagnetic fields, making comparison tricky. For example, to obtain agreement with Mashhoon's writings, all instances of Bg in the GEM equations must be multiplied by −1/2"c" and Eg by −1. These factors variously modify the analogues of the equations for the Lorentz force. No scaling choice allows all the GEM and EM equations to be perfectly analogous. The discrepancy in the factors arises because the source of the gravitational field is the second order stress–energy tensor, as opposed to the source of the electromagnetic field being the first order four-current tensor. This difference becomes clearer when one compares non-invariance of relativistic mass to electric charge invariance. This can be traced back to the spin-2 character of the gravitational field, in contrast to the electromagnetism being a spin-1 field. (See relativistic wave equations for more on "spin-1" and "spin-2" fields).
In Planck units.
From comparison of GEM equations and Maxwell's equations it is obvious that −1/(4π"G") is the gravitational analog of vacuum permittivity "ε"0. Adopting Planck units normalizes "G", "c" and 1/(4π"ε"0) to 1, thereby eliminating these constants from both sets of equations. The two sets of equations then become identical but for the minus sign preceding 4π in the GEM equations and a factor of four in Ampere's law. These minus signs stem from an essential difference between gravity and electromagnetism: electrostatic charges of identical sign repel each other, while masses attract each other. Hence the GEM equations are nearly Maxwell's equations with mass (or mass density) substituting for charge (or charge density), and −"G" replacing the Coulomb force constant 1/(4π"ε"0).
4π appears in both the GEM and Maxwell equations, because Planck units normalize "G" and 1/(4π"ε"0) to 1, and not 4π"G" and 1/"ε"0.
Higher-order effects.
Some higher-order gravitomagnetic effects can reproduce effects reminiscent of the interactions of more conventional polarized charges. For instance, if two wheels are spun on a common axis, the mutual gravitational attraction between the two wheels will be greater if they spin in opposite directions than in the same direction. This can be expressed as an attractive or repulsive gravitomagnetic component.
Gravitomagnetic arguments also predict that a flexible or fluid toroidal mass undergoing minor axis rotational acceleration (accelerating "smoke ring" rotation) will tend to pull matter through the throat (a case of rotational frame dragging, acting through the throat). In theory, this configuration might be used for accelerating objects (through the throat) without such objects experiencing any g-forces.
Consider a toroidal mass with two degrees of rotation (both major axis and minor-axis spin, both turning inside out and revolving). This represents a "special case" in which gravitomagnetic effects generate a chiral corkscrew-like gravitational field around the object. The reaction forces to dragging at the inner and outer equators would normally be expected to be equal and opposite in magnitude and direction respectively in the simpler case involving only minor-axis spin. When "both" rotations are applied simultaneously, these two sets of reaction forces can be said to occur at different depths in a radial Coriolis field that extends across the rotating torus, making it more difficult to establish that cancellation is complete.
Modelling this complex behaviour as a curved spacetime problem has yet to be done and is believed to be very difficult.
Gravitomagnetic fields of astronomical objects.
The formula for the gravitomagnetic field Bg near a rotating body can be derived from the GEM equations. It is given by:
where L is the angular momentum of the body. At the equatorial plane, r and L are perpendicular, so their dot product vanishes, and this formula reduces to:
The magnitude of angular momentum of a homogeneous ball-shaped body is:
where:
Earth.
Therefore, the magnitude of Earth's gravitomagnetic field at its equator is:
where formula_7 is Earth's gravity. The field direction coincides with the angular moment direction, i.e. north.
From this calculation it follows that Earth's equatorial gravitomagnetic field is about  Hz, or in units of standard gravity (9.81 m/s2) divided by the speed of light. Such a field is extremely weak and requires extremely sensitive measurements to be detected. One experiment to measure such field was the Gravity Probe B mission.
Pulsar.
If the preceding formula is used with the second fastest-spinning pulsar known, PSR J1748-2446ad (which rotates 716 times per second), assuming a radius of 16 km, and two solar masses, then
equals about 166 Hz. This would be easy to notice. However, the pulsar is spinning at a quarter of the speed of light at the equator, and its radius is only three times more than its Schwarzschild radius. When such fast motion and such strong gravitational fields exist in a system, the simplified approach of separating gravitomagnetic and gravitoelectric forces can be applied only as a very rough approximation.
Lack of invariance.
While Maxwell's equations are invariant under Lorentz transformations, the GEM equations were not. The fact that ρg and "j"g do not form a four-vector (instead they are merely a part of the stress–energy tensor) is the basis of this problem.
Although GEM may hold approximately in two different reference frames connected by a Lorentz boost, there is no way to calculate the GEM variables of one such frame from the GEM variables of the other, unlike the situation with the variables of electromagnetism. Indeed, their predictions (about what motion is free fall) will probably conflict with each other.
Note that the GEM equations are invariant under translations and spatial rotations, just not under boosts and more general curvilinear transformations. Maxwell's equations can be formulated in a way that makes them invariant under all of these coordinate transformations.

</doc>
<doc id="12100" url="https://en.wikipedia.org/wiki?curid=12100" title="Graviton">
Graviton

In physics, the graviton is a hypothetical elementary particle that mediates the force of gravitation in the framework of quantum field theory. If it exists, the graviton is expected to be massless (because the gravitational force appears to have unlimited range) and must be a spin-2 boson. The spin follows from the fact that the source of gravitation is the stress–energy tensor, a second-rank tensor (compared to electromagnetism's spin-1 photon, the source of which is the four-current, a first-rank tensor). Additionally, it can be shown that any massless spin-2 field would give rise to a force indistinguishable from gravitation, because a massless spin-2 field must couple to (interact with) the stress–energy tensor in the same way that the gravitational field does. Seeing as the graviton is hypothetical, its discovery would unite quantum theory with gravity. This result suggests that, if a massless spin-2 particle is discovered, it must be the graviton, so that the only experimental verification needed for the graviton may simply be the discovery of a massless spin-2 particle.
Theory.
The four other known forces of nature are mediated by elementary particles: electromagnetism by the photon, the strong interaction by the gluons, the Higgs field by the Higgs Boson, and the weak interaction by the W and Z bosons. The hypothesis is that the gravitational interaction is likewise mediated by an – as yet undiscovered – elementary particle, dubbed as "the graviton". In the classical limit, the theory would reduce to general relativity and conform to Newton's law of gravitation in the weak-field limit.
Gravitons and renormalization.
When describing graviton interactions, the classical theory (i.e., the tree diagrams) and semiclassical corrections (one-loop diagrams) behave normally, but Feynman diagrams with two (or more) loops lead to ultraviolet divergences; that is, infinite results that cannot be removed because the quantized general relativity is not renormalizable, unlike quantum electrodynamics. That is, the usual ways physicists calculate the probability that a particle will emit or absorb a graviton give nonsensical answers and the theory loses its predictive power. These problems, together with some conceptual puzzles, led many physicists to believe that a theory more complete than quantized general relativity must describe the behavior near the Planck scale.
Comparison with other forces.
Unlike the force carriers of the other forces, gravitation plays a special role in general relativity in defining the spacetime in which events take place. In some descriptions, matter modifies the 'shape' of spacetime itself, and gravity is a result of this shape, an idea which at first glance may appear hard to match with the idea of a force acting between particles. Because the diffeomorphism invariance of the theory does not allow any particular space-time background to be singled out as the "true" space-time background, general relativity is said to be background independent. In contrast, the Standard Model is "not" background independent, with Minkowski space enjoying a special status as the fixed background space-time. A theory of quantum gravity is needed in order to reconcile these differences. Whether this theory should be background independent is an open question. The answer to this question will determine our understanding of what specific role gravitation plays in the fate of the universe.
Gravitons in speculative theories.
String theory predicts the existence of gravitons and their well-defined interactions. A graviton in perturbative string theory is a closed string in a very particular low-energy vibrational state. The scattering of gravitons in string theory can also be computed from the correlation functions in conformal field theory, as dictated by the AdS/CFT correspondence, or from matrix theory.
A feature of gravitons in string theory is that, as closed strings without endpoints, they would not be bound to branes and could move freely between them. If we live on a brane (as hypothesized by brane theories) this "leakage" of gravitons from the brane into higher-dimensional space could explain why gravitation is such a weak force, and gravitons from other branes adjacent to our own could provide a potential explanation for dark matter. However, if gravitons were to move completely freely between branes this would dilute gravity too much, causing a violation of Newton's inverse square law. To combat this, Lisa Randall found that a three-brane (such as ours) would have a gravitational pull of its own, preventing gravitons from drifting freely, possibly resulting in the diluted gravity we observe while roughly maintaining Newton's inverse square law. See brane cosmology.
A theory by Ahmed Farag Ali and Saurya Das adds quantum mechanical corrections (using Bohm trajectories) to general relativistic geodesics. If gravitons are given a small but non-zero mass, it could explain the cosmological constant without need for dark energy and solve the smallness problem.
Experimental observation.
Unambiguous detection of individual gravitons, though not prohibited by any fundamental law, is impossible with any physically reasonable detector. The reason is the extremely low cross section for the interaction of gravitons with matter. For example, a detector with the mass of Jupiter and 100% efficiency, placed in close orbit around a neutron star, would only be expected to observe one graviton every 10 years, even under the most favorable conditions. It would be impossible to discriminate these events from the background of neutrinos, since the dimensions of the required neutrino shield would ensure collapse into a black hole.
However, experiments to detect gravitational waves, which may be viewed as coherent states of many gravitons, are underway (such as LIGO and VIRGO). Although these experiments cannot detect individual gravitons, they might provide information about certain properties of the graviton. For example, if gravitational waves were observed to propagate slower than "c" (the speed of light in a vacuum), that would imply that the graviton has mass (however, gravitational waves must propagate slower than "c" in a region with non-zero mass density if they are to be detectable).
Astronomical observations of the kinematics of galaxies, especially the galaxy rotation problem and modified Newtonian dynamics, might point toward gravitons having non-zero mass.
Difficulties and outstanding issues.
Most theories containing gravitons suffer from severe problems. Attempts to extend the Standard Model or other quantum field theories by adding gravitons run into serious theoretical difficulties at high energies (processes involving energies close to or above the Planck scale) because of infinities arising due to quantum effects (in technical terms, gravitation is nonrenormalizable). Since classical general relativity and quantum mechanics seem to be incompatible at such energies, from a theoretical point of view, this situation is not tenable. One possible solution is to replace particles with strings. String theories are quantum theories of gravity in the sense that they reduce to classical general relativity plus field theory at low energies, but are fully quantum mechanical, contain a graviton, and are believed to be mathematically consistent.

</doc>
<doc id="311001" url="https://en.wikipedia.org/wiki?curid=311001" title="Green's function">
Green's function

In mathematics, a Green's function is the impulse response of an inhomogeneous differential equation defined on a domain, with specified initial conditions or boundary conditions. Via the superposition principle, the convolution of a Green's function with an arbitrary function "f"("x") on that domain is the solution to the inhomogeneous differential equation for "f"("x").
In other words, given a linear ODE, formula_1, we can first solve formula_2, for each s, and realizing that since the source is a sum of delta functions, the solution is a sum of Green's functions as well, by linearity of L.
Green's functions are named after the British mathematician George Green, who first developed the concept in the 1830s. In the modern study of linear partial differential equations, Green's functions are studied largely from the point of view of fundamental solutions instead.
Under many-body theory, the term is also used in physics, specifically in quantum field theory, aerodynamics, aeroacoustics, electrodynamics and statistical field theory, to refer to various types of correlation functions, even those that do not fit the mathematical definition. In Quantum field theory, Green's functions take the roles of propagators.
Definition and uses.
A Green's function, "G"("x", "s"), of a linear differential operator "L" = "L"("x") acting on distributions over a subset of the Euclidean space R"n", at a point "s", is any solution of
where formula_3 is the Dirac delta function. This property of a Green's function can be exploited to solve differential equations of the form
If the kernel of "L" is non-trivial, then the Green's function is not unique. However, in practice, some combination of symmetry, boundary conditions and/or other externally imposed criteria will give a unique Green's function. Also, Green's functions in general are distributions, not necessarily proper functions.
Green's functions are also useful tools in solving wave equations and diffusion equations. In quantum mechanics, the Green's function of the Hamiltonian is a key concept with important links to the concept of density of states. As a side note, the Green's function as used in physics is usually defined with the opposite sign; that is,
This definition does not significantly change any of the properties of the Green's function.
If the operator is translation invariant, that is, when "L" has constant coefficients with respect to "x", then the Green's function can be taken to be a convolution operator, that is,
In this case, the Green's function is the same as the impulse response of linear time-invariant system theory.
Motivation.
Loosely speaking, if such a function "G" can be found for the operator "L", then if we multiply the equation (1) for the Green's function by "f"("s"), and then perform an integration in the "s" variable, we obtain:
The right-hand side is now given by the equation (2) to be equal to "L u"("x"), thus:
Because the operator "L" = "L"("x") is linear and acts on the variable "x" alone (not on the variable of integration "s"), we can take the operator "L" outside of the integration on the right-hand side, obtaining
which suggests
Thus, we can obtain the function "u"("x") through knowledge of the Green's function in equation (1) and the source term on the right-hand side in equation (2). This process relies upon the linearity of the operator "L".
In other words, the solution of equation (2), "u"("x"), can be determined by the integration given in equation (3). Although "f"("x") is known, this integration cannot be performed unless "G" is also known. The problem now lies in finding the Green's function "G" that satisfies equation (1). For this reason, the Green's function is also sometimes called the fundamental solution associated to the operator "L".
Not every operator "L" admits a Green's function. A Green's function can also be thought of as a right inverse of "L". Aside from the difficulties of finding a Green's function for a particular operator, the integral in equation (3) may be quite difficult to evaluate. However the method gives a theoretically exact result.
This can be thought of as an expansion of "f" according to a Dirac delta function basis (projecting "f" over δ("x" − "s")) and a superposition of the solution on each projection.) Such an integral equation is known as a Fredholm integral equation, the study of which constitutes Fredholm theory.
Green's functions for solving inhomogeneous boundary value problems.
The primary use of Green's functions in mathematics is to solve non-homogeneous boundary value problems. In modern theoretical physics, Green's functions are also usually used as propagators in Feynman diagrams (and the phrase "Green's function" is often used for any correlation function).
Framework.
Let "L" be the Sturm–Liouville operator, a linear differential operator of the form
and let "D" be the boundary conditions operator
Let "f"("x") be a continuous function in [0,"l"]. We shall also suppose that the problem
is regular (i.e., only the trivial solution exists for the homogeneous problem).
Theorem.
There is one and only one solution "u"("x") that satisfies
and it is given by
where "G"("x","s") is a Green's function satisfying the following conditions:
Advanced and retarded Green's functions.
Sometimes the Green's function can be split into a sum of two functions. One with the variable positive (+) and the other with the variable negative (-). These are the advanced and retarded Green's functions, and when the equation under study depends on time, one of the parts is causal and the other anti-causal. In these problems usually the causal part is the important one.
Finding Green's functions.
Eigenvalue expansions.
If a differential operator "L" admits a set of eigenvectors formula_23 (i.e., a set of functions formula_24 and scalars formula_25 such that formula_26) that is complete, then it is possible to construct a Green's function from these eigenvectors and eigenvalues.
"Complete" means that the set of functions formula_27 satisfies the following completeness relation:
Then the following holds:
where formula_30 represents complex conjugation.
Applying the operator "L" to each side of this equation results in the completeness relation, which was assumed true.
The general study of the Green's function written in the above form, and its relationship to the function spaces formed by the eigenvectors, is known as Fredholm theory.
There are several other methods for finding Green's functions, including the method of images, separation of variables, and Laplace transforms (Cole 2011).
Table of Green's functions.
The following table gives an overview of Green's functions of frequently appearing differential operators, where formula_31 is the Heaviside step function, formula_32 and formula_33.
Green's functions for the Laplacian.
Green's functions for linear differential operators involving the Laplacian may be readily put to use using the second of Green's identities.
To derive Green's theorem, begin with the divergence theorem (otherwise known as Gauss's theorem):
Let formula_35 and substitute into Gauss' law. Compute formula_36 and apply the product rule for the formula_37 operator:
Plugging this into the divergence theorem produces Green's theorem:
Suppose that the linear differential operator "L" is the Laplacian, formula_40, and that there is a Green's function "G" for the Laplacian. The defining property of the Green's function still holds:
Let formula_42 in Green's theorem. Then:
Using this expression, it is possible to solve Laplace's equation formula_44 or Poisson's equation formula_45, subject to either Neumann or Dirichlet boundary conditions. In other words, we can solve for formula_46 everywhere inside a volume where either (1) the value of formula_46 is specified on the bounding surface of the volume (Dirichlet boundary conditions), or (2) the normal derivative of formula_46 is specified on the bounding surface (Neumann boundary conditions).
Suppose the problem is to solve for formula_46 inside the region. Then the integral 
reduces to simply formula_46 due to the defining property of the Dirac delta function and we have:
This form expresses the well-known property of harmonic functions that if the value or normal derivative is known on a bounding surface, then the value of the function inside the volume is known everywhere.
In electrostatics, formula_46 is interpreted as the electric potential, formula_54 as electric charge density, and the normal derivative formula_55 as the normal component of the electric field.
If the problem is to solve a Dirichlet boundary value problem, the Green's function should be chosen such that formula_56 vanishes when either "x" or "x"′ is on the bounding surface. Thus only one of the two terms in the surface integral remains. If the problem is to solve a Neumann boundary value problem, the Green's function is chosen such that its normal derivative vanishes on the bounding surface, as it would seem to be the most logical choice. (See Jackson J.D. classical electrodynamics, page 39). However, application of Gauss's theorem to the differential equation defining the Green's function yields
meaning the normal derivative of formula_56 cannot vanish on the surface, because it must integrate to 1 on the surface. (Again, see Jackson J.D. classical electrodynamics, page 39 for this and the following argument). The simplest form the normal derivative can take is that of a constant, namely formula_59, where "S" is the surface area of the surface. The surface term in the solution becomes
where formula_61 is the average value of the potential on the surface. This number is not known in general, but is often unimportant, as the goal is often to obtain the electric field given by the gradient of the potential, rather than the potential itself.
With no boundary conditions, the Green's function for the Laplacian (Green's function for the three-variable Laplace equation) is:
Supposing that the bounding surface goes out to infinity, and plugging in this expression for the Green's function, this gives the familiar expression for electric potential in terms of electric charge density as
Example.
Example. Find the Green function for the following problem:
First step: The Green's function for the linear operator at hand is defined as the solution to
If formula_66, then the delta function gives zero, and the general solution is
For formula_68 implies
 if formula_70 and formula_71.
For formula_72, the boundary condition at formula_73 implies
The equation of formula_75 is skipped for similar reasons.
To summarize the results thus far:
Ensuring continuity in the Green's function at formula_78 implies
One can ensure proper discontinuity in the first derivative by integrating the defining differential equation from formula_80 to formula_81 and taking the limit as formula_82 goes to zero:
The two (dis)continuity equations can be solved for formula_84 and formula_77 to obtain
So the Green's function for this problem is:

</doc>
<doc id="225982" url="https://en.wikipedia.org/wiki?curid=225982" title="Ground state">
Ground state

The ground state of a quantum mechanical system is its lowest-energy state; the energy of the ground state is known as the zero-point energy of the system. An excited state is any state with energy greater than the ground state. The ground state of a quantum field theory is usually called the vacuum state or the vacuum.
If more than one ground state exists, they are said to be degenerate. Many systems have degenerate ground states. Degeneracy occurs whenever there exists a unitary operator which acts non-trivially on a ground state and commutes with the Hamiltonian of the system.
According to the third law of thermodynamics, a system at absolute zero temperature exists in its ground state; thus, its entropy is determined by the degeneracy of the ground state. Many systems, such as a perfect crystal lattice, have a unique ground state and therefore have zero entropy at absolute zero. It is also possible for the highest excited state to have absolute zero temperature for systems that exhibit negative temperature.
1D ground state has no nodes.
In 1D the ground state of the Schrödinger equation has no nodes. This can be proved considering an average energy in the state with a node at formula_1, i.e. formula_2. Consider the average energy in this state
formula_3
where formula_4 is the potential. Now consider a small interval around formula_1, i.e. formula_6. Take a new wavefunction formula_7 to be defined as formula_8 and formula_9 and constant for formula_6. If epsilon is small enough then this is always possible to do so that formula_7 is continuous. So assuming formula_12 around formula_1, we can write the new function as
formula_14
where formula_15 is the norm. Note that the kinetic energy density formula_16 everywhere because of the normalization. Now consider the potential energy. For definiteness let us choose formula_17. Then it is clear that outside the interval formula_6 the potential energy density is smaller for the formula_19 because formula_20 there. On the other hand, in the interval formula_6 we have
formula_22
which is correct to this order of formula_23 and formula_24 indicate higher order corrections. On the other hand, the potential energy in the formula_25 state is
formula_26
which is the same as that of the formula_19 state to the order shown.
Therefore, the potential energy unchanged to leading order in formula_23 by deforming the state with a node formula_25 into a state without a node formula_19. We can do this by removing all nodes thereby reducing the energy, which implies that the ground state energy must not have a node. This completes the proof.

</doc>
