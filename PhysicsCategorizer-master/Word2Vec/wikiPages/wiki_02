<doc id="39407" url="https://en.wikipedia.org/wiki?curid=39407" title="Dirac equation">
Dirac equation

In particle physics, the Dirac equation is a relativistic wave equation derived by British physicist Paul Dirac in 1928. In its free form, or including electromagnetic interactions, it describes all spin-½ massive particles such as electrons and quarks, for which parity is a symmetry, and is consistent with both the principles of quantum mechanics and the theory of special relativity, and was the first theory to account fully for special relativity in the context of quantum mechanics.
It accounted for the fine details of the hydrogen spectrum in a completely rigorous way. The equation also implied the existence of a new form of matter, "antimatter", previously unsuspected and unobserved and which was experimentally confirmed several years later. It also provided a "theoretical" justification for the introduction of several-component wave functions in Pauli's phenomenological theory of spin; the wave functions in the Dirac theory are vectors of four complex numbers (known as bispinors), two of which resemble the Pauli wavefunction in the non-relativistic limit, in contrast to the Schrödinger equation which described wave functions of only one complex value. Moreover, in the limit of zero mass, the Dirac equation reduces to the Weyl equation.
Although Dirac did not at first fully appreciate the importance of his results, the entailed explanation of spin as a consequence of the union of quantum mechanics and relativity—and the eventual discovery of the positron—represents one of the great triumphs of theoretical physics. This accomplishment has been described as fully on a par with the works of Newton, Maxwell, and Einstein before him. In the context of quantum field theory, the Dirac equation is reinterpreted to describe quantum fields corresponding to spin-½ particles.
Mathematical formulation.
The Dirac equation in the form originally proposed by Dirac is:
where is the wave function for the electron of rest mass with spacetime coordinates . The are the components of the momentum, understood to be the momentum operator in the Schrödinger equation. Also, is the speed of light, and is the Planck constant divided by . These fundamental physical constants reflect special relativity and quantum mechanics, respectively.
Dirac's purpose in casting this equation was to explain the behavior of the relativistically moving electron, and so to allow the atom to be treated in a manner consistent with relativity. His rather modest hope was that the corrections introduced this way might have bearing on the problem of atomic spectra. Up until that time, attempts to make the old quantum theory of the atom compatible with the theory of relativity, attempts based on discretizing the angular momentum stored in the electron's possibly non-circular orbit of the atomic nucleus, had failed – and the new quantum mechanics of Heisenberg, Pauli, Jordan, Schrödinger, and Dirac himself had not developed sufficiently to treat this problem. Although Dirac's original intentions were satisfied, his equation had far deeper implications for the structure of matter, and introduced new mathematical classes of objects that are now essential elements of fundamental physics.
The new elements in this equation are the 4 × 4 matrices and , and the four-component wave function . There are four components in because evaluation of it at any given point in configuration space is a bispinor. It is interpreted as a superposition of a spin-up electron, a spin-down electron, a spin-up positron, and a spin-down positron (see below for further discussion).
The 4 × 4 matrices and are all Hermitian and have squares equal to the identity matrix:
and they all mutually anticommute (if and are distinct):
The single symbolic equation thus unravels into four coupled linear first-order partial differential equations for the four quantities that make up the wave function. These matrices, and the form of the wave function, have a deep mathematical significance. The algebraic structure represented by the gamma matrices had been created some 50 years earlier by the English mathematician W. K. Clifford. In turn, Clifford's ideas had emerged from the mid-19th century work of the German mathematician Hermann Grassmann in his "Lineale Ausdehnungslehre" ("Theory of Linear Extensions"). The latter had been regarded as well-nigh incomprehensible by most of his contemporaries. The appearance of something so seemingly abstract, at such a late date, and in such a direct physical manner, is one of the most remarkable chapters in the history of physics.
Making the Schrödinger equation relativistic.
The Dirac equation is superficially similar to the Schrödinger equation for a massive free particle:
The left side represents the square of the momentum operator divided by twice the mass, which is the non-relativistic kinetic energy. Because relativity treats space and time as a whole, a relativistic generalization of this equation requires that space and time derivatives must enter symmetrically, as they do in the Maxwell equations that govern the behavior of light — the equations must be differentially of the "same order" in space and time. In relativity, the momentum and the energy are the space and time parts of a spacetime vector, the four-momentum, and they are related by the relativistically invariant relation
which says that the length of this four-vector is proportional to the rest mass . Substituting the operator equivalents of the energy and momentum from the Schrödinger theory, we get an equation describing the propagation of waves, constructed from relativistically invariant objects,
with the wave function being a relativistic scalar: a complex number which has the same numerical value in all frames of reference. The space and time derivatives both enter to second order. This has a telling consequence for the interpretation of the equation. Because the equation is second order in the time derivative, one must specify initial values both of the wave function itself and of its first time derivative in order to solve definite problems. Since both may be specified more or less arbitrarily, the wave function cannot maintain its former role of determining the probability density of finding the electron in a given state of motion. In the Schrödinger theory, the probability density is given by the positive definite expression
and this density is convected according to the probability current vector
with the conservation of probability current and density following from the continuity equation:
The fact that the density is positive definite and convected according to this continuity equation, implies that we may integrate the density over a certain domain and set the total to 1, and this condition will be maintained by the conservation law. A proper relativistic theory with a probability density current must also share this feature. Now, if we wish to maintain the notion of a convected density, then we must generalize the Schrödinger expression of the density and current so that the space and time derivatives again enter symmetrically in relation to the scalar wave function. We are allowed to keep the Schrödinger expression for the current, but must replace the probability density by the symmetrically formed expression
which now becomes the 4th component of a spacetime vector, and the entire probability 4-current density has the relativistically covariant expression
The continuity equation is as before. Everything is compatible with relativity now, but we see immediately that the expression for the density is no longer positive definite – the initial values of both and may be freely chosen, and the density may thus become negative, something that is impossible for a legitimate probability density. Thus we cannot get a simple generalization of the Schrödinger equation under the naive assumption that the wave function is a relativistic scalar, and the equation it satisfies, second order in time.
Although it is not a successful relativistic generalization of the Schrödinger equation, this equation is resurrected in the context of quantum field theory, where it is known as the Klein–Gordon equation, and describes a spinless particle field (e.g. pi meson). Historically, Schrödinger himself arrived at this equation before the one that bears his name, but soon discarded it. In the context of quantum field theory, the indefinite density is understood to correspond to the "charge" density, which can be positive or negative, and not the probability density.
Dirac's coup.
Dirac thus thought to try an equation that was "first order" in both space and time. One could, for example, formally take the relativistic expression for the energy
replace by its operator equivalent, expand the square root in an infinite series of derivative operators, set up an eigenvalue problem, then solve the equation formally by iterations. Most physicists had little faith in such a process, even if it were technically possible.
As the story goes, Dirac was staring into the fireplace at Cambridge, pondering this problem, when he hit upon the idea of taking the square root of the wave operator thus:
On multiplying out the right side we see that, in order to get all the cross-terms such as to vanish, we must assume
with
Dirac, who had just then been intensely involved with working out the foundations of Heisenberg's matrix mechanics, immediately understood that these conditions could be met if , , and are "matrices", with the implication that the wave function has "multiple components". This immediately explained the appearance of two-component wave functions in Pauli's phenomenological theory of spin, something that up until then had been regarded as mysterious, even to Pauli himself. However, one needs at least 4 × 4 matrices to set up a system with the properties required — so the wave function had "four" components, not two, as in the Pauli theory, or one, as in the bare Schrödinger theory. The four-component wave function represents a new class of mathematical object in physical theories that makes its first appearance here.
Given the factorization in terms of these matrices, one can now write down immediately an equation
with to be determined. Applying again the matrix operator on both side yields
On taking we find that all the components of the wave function "individually" satisfy the relativistic energy–momentum relation. Thus the sought-for equation that is first-order in both space and time is
Setting
we get the Dirac equation as written above.
Covariant form and relativistic invariance.
To demonstrate the relativistic invariance of the equation, it is advantageous to cast it into a form in which the space and time derivatives appear on an equal footing. New matrices are introduced as follows:
and the equation takes the form
where there is an implied summation over the values of the twice-repeated index . In practice one often writes the gamma matrices in terms of 2 × 2 sub-matrices taken from the Pauli matrices and the 2 × 2 identity matrix. Explicitly the standard representation is
The complete system is summarized using the Minkowski metric on spacetime in the form
where the bracket expression
denotes the anticommutator. These are the defining relations of a Clifford algebra over a pseudo-orthogonal 4-d space with metric signature . The specific Clifford algebra employed in the Dirac equation is known today as the Dirac algebra. Although not recognized as such by Dirac at the time the equation was formulated, in hindsight the introduction of this "geometric algebra" represents an enormous stride forward in the development of quantum theory.
The Dirac equation may now be interpreted as an eigenvalue equation, where the rest mass is proportional to an eigenvalue of the 4-momentum operator, the proportionality constant being the speed of light:
Using formula_26 (pronounced: "d-slash") in Feynman slash notation, which includes the gamma matrices as well as a summation over the spinor components in the derivative itself, the Dirac equation becomes:
In practice, physicists often use units of measure such that , known as natural units. The equation then takes the simple form
A fundamental theorem states that if two distinct sets of matrices are given that both satisfy the Clifford relations, then they are connected to each other by a similarity transformation:
If in addition the matrices are all unitary, as are the Dirac set, then itself is unitary;
The transformation is unique up to a multiplicative factor of absolute value 1. Let us now imagine a Lorentz transformation to have been performed on the space and time coordinates, and on the derivative operators, which form a covariant vector. For the operator to remain invariant, the gammas must transform among themselves as a contravariant vector with respect to their spacetime index. These new gammas will themselves satisfy the Clifford relations, because of the orthogonality of the Lorentz transformation. By the fundamental theorem, we may replace the new set by the old set subject to a unitary transformation. In the new frame, remembering that the rest mass is a relativistic scalar, the Dirac equation will then take the form
If we now define the transformed spinor
then we have the transformed Dirac equation in a way that demonstrates manifest relativistic invariance:
Thus, once we settle on any unitary representation of the gammas, it is final provided we transform the spinor according to the unitary transformation that corresponds to the given Lorentz transformation. The various representations of the Dirac matrices employed will bring into focus particular aspects of the physical content in the Dirac wave function (see below). The representation shown here is known as the "standard" representation – in it, the wave function's upper two components go over into Pauli's 2-spinor wave function in the limit of low energies and small velocities in comparison to light.
The considerations above reveal the origin of the gammas in "geometry", hearkening back to Grassmann's original motivation – they represent a fixed basis of unit vectors in spacetime. Similarly, products of the gammas such as represent "oriented surface elements", and so on. With this in mind, we can find the form of the unit volume element on spacetime in terms of the gammas as follows. By definition, it is
For this to be an invariant, the epsilon symbol must be a tensor, and so must contain a factor of , where is the determinant of the metric tensor. Since this is negative, that factor is "imaginary". Thus
This matrix is given the special symbol , owing to its importance when one is considering improper transformations of spacetime, that is, those that change the orientation of the basis vectors. In the standard representation it is
This matrix will also be found to anticommute with the other four Dirac matrices:
It takes a leading role when questions of "parity" arise, because the volume element as a directed magnitude changes sign under a spacetime reflection. Taking the positive square root above thus amounts to choosing a handedness convention on spacetime .
Conservation of probability current.
By defining the adjoint spinor
where is the conjugate transpose of , and noticing that
we obtain, by taking the Hermitian conjugate of the Dirac equation and multiplying from the right by , the adjoint equation:
where is understood to act to the left. Multiplying the Dirac equation by from the left, and the adjoint equation by from the right, and subtracting, produces the law of conservation of the Dirac current:
Now we see the great advantage of the first-order equation over the one Schrödinger had tried – this is the conserved current density required by relativistic invariance, only now its 4th component is "positive definite" and thus suitable for the role of a probability density:
Because the probability density now appears as the fourth component of a relativistic vector, and not a simple scalar as in the Schrödinger equation, it will be subject to the usual effects of the Lorentz transformations such as time dilation. Thus for example atomic processes that are observed as rates, will necessarily be adjusted in a way consistent with relativity, while those involving the measurement of energy and momentum, which themselves form a relativistic vector, will undergo parallel adjustment which preserves the relativistic covariance of the observed values.
Solutions.
See Dirac spinor for details of solutions to the Dirac equation. Note that since the Dirac operator acts on 4-tuples of square-integrable functions, its solutions should be members of the same Hilbert space. The fact that the energies of the solutions do not have a lower bound is unexpected – see the hole theory section below for more details.
Comparison with the Pauli theory.
The necessity of introducing half-integer spin goes back experimentally to the results of the Stern–Gerlach experiment. A beam of atoms is run through a strong inhomogeneous magnetic field, which then splits into parts depending on the intrinsic angular momentum of the atoms. It was found that for silver atoms, the beam was split in two—the ground state therefore could not be integer, because even if the intrinsic angular momentum of the atoms were as small as possible, 1, the beam would be split into three parts, corresponding to atoms with . The conclusion is that silver atoms have net intrinsic angular momentum of . Pauli set up a theory which explained this splitting by introducing a two-component wave function and a corresponding correction term in the Hamiltonian, representing a semi-classical coupling of this wave function to an applied magnetic field, as so in SI units: (Note that bold faced characters imply Euclidean vectors in 3 dimensions, where as the Minkowski four-vector formula_43 can be defined as formula_44.)
Here and represent the components of the electromagnetic four-potential in their standard SI units, and the three sigmas are the Pauli matrices. On squaring out the first term, a residual interaction with the magnetic field is found, along with the usual classical Hamiltonian of a charged particle interacting with an applied field in SI units:
This Hamiltonian is now a 2 × 2 matrix, so the Schrödinger equation based on it must use a two-component wave function. Pauli had introduced the 2 × 2 sigma matrices as pure "phenomenology"— Dirac now had a "theoretical argument" that implied that spin was somehow the consequence of the marriage of quantum mechanics to relativity. On introducing the external electromagnetic 4-vector potential into the Dirac equation in a similar way, known as minimal coupling, it takes the form (in natural units)
A second application of the Dirac operator will now reproduce the Pauli term exactly as before, because the spatial Dirac matrices multiplied by , have the same squaring and commutation properties as the Pauli matrices. What is more, the value of the gyromagnetic ratio of the electron, standing in front of Pauli's new term, is explained from first principles. This was a major achievement of the Dirac equation and gave physicists great faith in its overall correctness. There is more however. The Pauli theory may be seen as the low energy limit of the Dirac theory in the following manner. First the equation is written in the form of coupled equations for 2-spinors with the SI units restored:
so
Assuming the field is weak and the motion of the electron non-relativistic, we have the total energy of the electron approximately equal to its rest energy, and the momentum going over to the classical value,
and so the second equation may be written
which is of order - thus at typical energies and velocities, the bottom components of the Dirac spinor in the standard representation are much suppressed in comparison to the top components. Substituting this expression into the first equation gives after some rearrangement
The operator on the left represents the particle energy reduced by its rest energy, which is just the classical energy, so we recover Pauli's theory if we identify his 2-spinor with the top components of the Dirac spinor in the non-relativistic approximation. A further approximation gives the Schrödinger equation as the limit of the Pauli theory. Thus the Schrödinger equation may be seen as the far non-relativistic approximation of the Dirac equation when one may neglect spin and work only at low energies and velocities. This also was a great triumph for the new equation, as it traced the mysterious "i" that appears in it, and the necessity of a complex wave function, back to the geometry of spacetime through the Dirac algebra. It also highlights why the Schrödinger equation, although superficially in the form of a diffusion equation, actually represents the propagation of waves.
It should be strongly emphasized that this separation of the Dirac spinor into large and small components depends explicitly on a low-energy approximation. The entire Dirac spinor represents an "irreducible" whole, and the components we have just neglected to arrive at the Pauli theory will bring in new phenomena in the relativistic regime – antimatter and the idea of creation and annihilation of particles.
Comparison with the Weyl theory.
In the limit , the Dirac equation reduces to the Weyl equation, which describes relativistic massless spin-1/2 particles.
Dirac Lagrangian.
Both the Dirac equation and the Adjoint Dirac equation can be obtained from (varying) the action with a specific Lagrangian density that is given by:
formula_55
If one varies this with respect to one gets the Adjoint Dirac equation. Meanwhile, if one varies this with respect to one gets the Dirac equation.
Physical interpretation.
The Dirac theory, while providing a wealth of information that is accurately confirmed by experiments, nevertheless introduces a new physical paradigm that appears at first difficult to interpret and even paradoxical. Some of these issues of interpretation must be regarded as open questions.
Identification of observables.
The critical physical question in a quantum theory is—what are the physically observable quantities defined by the theory? According to general principles, such quantities are defined by Hermitian operators that act on the Hilbert space of possible states of a system. The eigenvalues of these operators are then the possible results of measuring the corresponding physical quantity. In the Schrödinger theory, the simplest such object is the overall Hamiltonian, which represents the total energy of the system. If we wish to maintain this interpretation on passing to the Dirac theory, we must take the Hamiltonian to be
where, as always, there is an implied summation over the twice-repeated index . This looks promising, because we see by inspection the rest energy of the particle and, in case , the energy of a charge placed in an electric potential . What about the term involving the vector potential? In classical electrodynamics, the energy of a charge moving in an applied potential is
Thus the Dirac Hamiltonian is fundamentally distinguished from its classical counterpart, and we must take great care to correctly identify what is an observable in this theory. Much of the apparent paradoxical behaviour implied by the Dirac equation amounts to a misidentification of these observables.
Hole theory.
The negative solutions to the equation are problematic, for it was assumed that the particle has a positive energy. Mathematically speaking, however, there seems to be no reason for us to reject the negative-energy solutions. Since they exist, we cannot simply ignore them, for once we include the interaction between the electron and the electromagnetic field, any electron placed in a positive-energy eigenstate would decay into negative-energy eigenstates of successively lower energy. Real electrons obviously do not behave in this way, or they would disappear by emitting energy in the form of photons.
To cope with this problem, Dirac introduced the hypothesis, known as hole theory, that the vacuum is the many-body quantum state in which all the negative-energy electron eigenstates are occupied. This description of the vacuum as a "sea" of electrons is called the Dirac sea. Since the Pauli exclusion principle forbids electrons from occupying the same state, any additional electron would be forced to occupy a positive-energy eigenstate, and positive-energy electrons would be forbidden from decaying into negative-energy eigenstates.
If an electron is forbidden from simultaneously occupying positive-energy and negative-energy eigenstates, then the feature known as Zitterbewegung, which arises from the interference of positive-energy and negative-energy states, would have to be considered to be an unphysical prediction of time-dependent Dirac theory. This conclusion may be inferred from the explanation of hole theory given in the preceding paragraph. Recent results have been published in Nature [R. Gerritsma, G. Kirchmair, F. Zaehringer, E. Solano, R. Blatt, and C. Roos, Nature 463, 68-71 (2010)] in which the Zitterbewegung feature was simulated in a trapped-ion experiment. This experiment impacts the hole interpretation if one infers that the physics-laboratory experiment is not merely a check on the mathematical correctness of a Dirac-equation solution but the measurement of a real effect whose detectability in electron physics is still beyond reach.
Dirac further reasoned that if the negative-energy eigenstates are incompletely filled, each unoccupied eigenstate – called a hole – would behave like a positively charged particle. The hole possesses a "positive" energy, since energy is required to create a particle–hole pair from the vacuum. As noted above, Dirac initially thought that the hole might be the proton, but Hermann Weyl pointed out that the hole should behave as if it had the same mass as an electron, whereas the proton is over 1800 times heavier. The hole was eventually identified as the positron, experimentally discovered by Carl Anderson in 1932.
It is not entirely satisfactory to describe the "vacuum" using an infinite sea of negative-energy electrons. The infinitely negative contributions from the sea of negative-energy electrons has to be canceled by an infinite positive "bare" energy and the contribution to the charge density and current coming from the sea of negative-energy electrons is exactly canceled by an infinite positive "jellium" background so that the net electric charge density of the vacuum is zero. In quantum field theory, a Bogoliubov transformation on the creation and annihilation operators (turning an occupied negative-energy electron state into an unoccupied positive energy positron state and an unoccupied negative-energy electron state into an occupied positive energy positron state) allows us to bypass the Dirac sea formalism even though, formally, it is equivalent to it.
In certain applications of condensed matter physics, however, the underlying concepts of "hole theory" are valid. The sea of conduction electrons in an electrical conductor, called a Fermi sea, contains electrons with energies up to the chemical potential of the system. An unfilled state in the Fermi sea behaves like a positively charged electron, though it is referred to as a "hole" rather than a "positron". The negative charge of the Fermi sea is balanced by the positively charged ionic lattice of the material.
In quantum field theory.
In quantum field theories such as quantum electrodynamics, the Dirac field is subject to a process of second quantization, which resolves some of the paradoxical features of the equation.
Other formulations.
The Dirac equation can be formulated in a number of other ways.
As a differential equation in one real component.
Generically (if a certain linear function of electromagnetic field does not vanish identically), three out of four components of the spinor function in the Dirac equation can be algebraically eliminated, yielding an equivalent fourth-order partial differential equation for just one component. Furthermore, this remaining component can be made real by a gauge transform.
Curved spacetime.
This article has developed the Dirac equation in flat spacetime according to special relativity. It is possible to formulate the Dirac equation in curved spacetime.
The algebra of physical space.
This article developed the Dirac equation using four vectors and Schrödinger operators. The Dirac equation in the algebra of physical space uses a Clifford algebra over the real numbers, a type of geometric algebra.
See also.
The Dirac equation appears on the floor of Westminster Abbey on the plaque commemorating Paul Dirac's life, which was inaugurated on November 13, 1995.

</doc>
<doc id="292800" url="https://en.wikipedia.org/wiki?curid=292800" title="Dirac spinor">
Dirac spinor

In quantum field theory, the Dirac spinor is the bispinor in the plane-wave solution
of the free Dirac equation,
where (in the units formula_3)
The Dirac spinor for the positive-frequency solution can be written as
where
Derivation from Dirac equation.
The Dirac equation has the form
In order to derive the form of the four-spinor formula_17 we have to first note the value of the matrices α and β:
These two 4×4 matrices are related to the Dirac gamma matrices. Note that 0 and I are 2×2 matrices here.
The next step is to look for solutions of the form
while at the same time splitting ω into two two-spinors:
Results.
Using all of the above information to plug into the Dirac equation results in
This matrix equation is really two coupled equations:
Solve the 2nd equation for formula_24 and one obtains 
Solve the 1st equation for formula_26 and one finds
This solution is useful for showing the relation between anti-particle and particle.
Details.
Two-spinors.
The most convenient definitions for the two-spinors are:
and
Pauli matrices.
The Pauli matrices are
Using these, one can calculate:
Four-spinors.
For particles.
Particles are defined as having "positive" energy. The normalization for the four-spinor ω is chosen so that formula_32 . These spinors are denoted as "u":
where "s" = 1 or 2 (spin "up" or "down")
Explicitly,
For anti-particles.
Anti-particles having "positive" energy formula_35 are defined as particles having "negative" energy and propagating backward in time. Hence changing the sign of formula_35 and formula_37 in the four-spinor for particles will give the four-spinor for anti-particles:
Here we choose the formula_39 solutions. Explicitly,
Completeness relations.
The completeness relations for the four-spinors "u" and "v" are
where
Dirac spinors and the Dirac algebra.
The Dirac matrices are a set of four 4×4 matrices that are used as spin and charge operators. 
Conventions.
There are several choices of signature and representation that are in common use in the physics literature. The Dirac matrices are typically written as formula_45 where formula_46 runs from 0 to 3. In this notation, 0 corresponds to time, and 1 through 3 correspond to x, y, and z.
The + − − − signature is sometimes called the west coast metric, while the − + + + is the east coast metric. At this time the + − − − signature is in more common use, and our example will use this signature. To switch from one example to the other, multiply all formula_47 by formula_48.
After choosing the signature, there are many ways of constructing a representation in the 4×4 matrices, and many are in common use. In order to make this example as general as possible we will not specify a representation until the final step. At that time we will substitute in the "chiral" or "Weyl" representation as used in the popular graduate textbook "An Introduction to Quantum Field Theory" by Michael E. Peskin and Daniel V. Schroeder.
Construction of Dirac spinor with a given spin direction and charge.
First we choose a spin direction for our electron or positron. As with the example of the Pauli algebra discussed above, the spin direction is defined by a unit vector in 3 dimensions, (a, b, c). Following the convention of Peskin & Schroeder, the spin operator for spin in the (a, b, c) direction is defined as the dot product of (a, b, c) with the vector
Note that the above is a root of unity, that is, it squares to 1. Consequently, we can make a projection operator from it that projects out the sub-algebra of the Dirac algebra that has spin oriented in the (a, b, c) direction:
Now we must choose a charge, +1 (positron) or −1 (electron). Following the conventions of Peskin & Schroeder, the operator for charge is formula_52, that is, electron states will take an eigenvalue of −1 with respect to this operator while positron states will take an eigenvalue of +1.
Note that formula_53 is also a square root of unity. Furthermore, formula_53 commutes with formula_55. They form a complete set of commuting operators for the Dirac algebra. Continuing with our example, we look for a representation of an electron with spin in the (a, b, c) direction. Turning formula_53 into a projection operator for charge = −1, we have
The projection operator for the spinor we seek is therefore the product of the two projection operators we've found:
The above projection operator, when applied to any spinor, will give that part of the spinor that corresponds to the electron state we seek. So we can apply it to a spinor with the value 1 in one of its components, and 0 in the others, which gives a column of the matrix. Continuing the example, we put (a, b, c) = (0, 0, 1) and have
and so our desired projection operator is
The 4×4 gamma matrices used in the Weyl representation are
for k = 1, 2, 3 and where formula_63 are the usual 2×2 Pauli matrices. Substituting these in for P gives
Our answer is any non-zero column of the above matrix. The division by two is just a normalization. The first and third columns give the same result:
More generally, for electrons and positrons with spin oriented in the (a, b, c) direction, the projection operator is
where the upper signs are for the electron and the lower signs are for the positron. The corresponding spinor can be taken as any non zero column. Since formula_67 the different columns are multiples of the same spinor. The representation of the resulting spinor in the Dirac basis can be obtained using the rule given in the bispinor article.

</doc>
<doc id="8811" url="https://en.wikipedia.org/wiki?curid=8811" title="Discrete Fourier transform">
Discrete Fourier transform

In mathematics, the discrete Fourier transform (DFT) converts a finite list of equally spaced samples of a function into the list of coefficients of a finite combination of complex sinusoids, ordered by their frequencies, that has those same sample values. It can be said to convert the sampled function from its original domain (often time or position along a line) to the frequency domain.
The input samples are complex numbers (in practice, usually real numbers), and the output coefficients are complex as well. The frequencies of the output sinusoids are integer multiples of a fundamental frequency, whose corresponding period is the length of the sampling interval. The combination of sinusoids obtained through the DFT is therefore periodic with that same period. The DFT differs from the discrete-time Fourier transform (DTFT) in that its input and output sequences are both finite; it is therefore said to be the Fourier analysis of finite-domain (or periodic) discrete-time functions.
The DFT is the most important discrete transform, used to perform Fourier analysis in many practical applications. In digital signal processing, the function is any quantity or signal that varies over time, such as the pressure of a sound wave, a radio signal, or daily temperature readings, sampled over a finite time interval (often defined by a window function). In image processing, the samples can be the values of pixels along a row or column of a raster image. The DFT is also used to efficiently solve partial differential equations, and to perform other operations such as convolutions or multiplying large integers.
Since it deals with a finite amount of data, it can be implemented in computers by numerical algorithms or even dedicated hardware. These implementations usually employ efficient fast Fourier transform (FFT) algorithms; so much so that the terms "FFT" and "DFT" are often used interchangeably. Prior to its current usage, the "FFT" initialism may have also been used for the ambiguous term "finite Fourier transform".
Definition.
The sequence of "N" complex numbers formula_1 is transformed into an "N"-periodic sequence of complex numbers:
Because of periodicity, the customary domain of k actually computed is ["0", "N"-1]. That is always the case when the DFT is implemented via the Fast Fourier transform algorithm. But other common domains are  [-"N"/2, "N"/2-1]  ("N" even)  and  [-("N"-1)/2, ("N"-1)/2]  ("N" odd), as when the left and right halves of an FFT output sequence are swapped.
The transform is sometimes denoted by the symbol formula_2, as in formula_3 or formula_4 or formula_5.
 can be interpreted or derived in various ways, for example:
The normalization factor multiplying the DFT and IDFT (here 1 and 1/"N") and the signs of the exponents are merely conventions, and differ in some treatments. The only requirements of these conventions are that the DFT and IDFT have opposite-sign exponents and that the product of their normalization factors be 1/"N".  A normalization of formula_12 for both the DFT and IDFT, for instance, makes the transforms unitary.
In the following discussion the terms "sequence" and "vector" will be considered interchangeable.
Using Euler's Formula, it can be derived further to the forms commonly used in Engineering and Computer Science.
Fourier Transform:
Inverse Fourier Transform:
Properties.
Completeness.
The discrete Fourier transform is an invertible, linear transformation
with formula_14 denoting the set of complex numbers. In other words, for any "N" > 0, an "N"-dimensional complex vector has a DFT and an IDFT which are in turn "N"-dimensional complex vectors.
Orthogonality.
The vectors formula_15 
form an orthogonal basis over the set of "N"-dimensional complex vectors:
where formula_17 is the Kronecker delta. (In the last step, the summation is trivial if formula_18, where it is 1+1+⋅⋅⋅="N", and otherwise is a geometric series that can be explicitly summed to obtain zero.) This orthogonality condition can be used to derive the formula for the IDFT from the definition of the DFT, and is equivalent to the unitarity property below.
The Plancherel theorem and Parseval's theorem.
If "X""k" and "Y""k" are the DFTs of "x""n" and "y""n" respectively then the Parseval's theorem states:
where the star denotes complex conjugation. Plancherel theorem is a special case of the Parseval's theorem and states:
These theorems are also equivalent to the unitary condition below.
Periodicity.
The periodicity can be shown directly from the definition:
Similarly, it can be shown that the IDFT formula leads to a periodic extension.
Shift theorem.
Multiplying formula_9 by a "linear phase" formula_23 for some integer "m" corresponds to a "circular shift" of the output formula_7: formula_7 is replaced by formula_26, where the subscript is interpreted modulo "N" (i.e., periodically). Similarly, a circular shift of the input formula_9 corresponds to multiplying the output formula_7 by a linear phase. Mathematically, if formula_29 represents the vector x then
Circular convolution theorem and cross-correlation theorem.
The convolution theorem for the discrete-time Fourier transform indicates that a convolution of two infinite sequences can be obtained as the inverse transform of the product of the individual transforms. An important simplification occurs when the sequences are of finite length, N. In terms of the DFT and inverse DFT, it can be written as follows:
which is the convolution of the formula_34 sequence with a formula_35 sequence extended by periodic summation:
Similarly, the cross-correlation of  formula_34  and  formula_38  is given by:
When either sequence contains a string of zeros, of length L,  L+1 of the circular convolution outputs are equivalent to values of  formula_40  Methods have also been developed to use this property as part of an efficient process that constructs  formula_41  with an formula_34 or formula_35 sequence potentially much longer than the practical transform size (N). Two such methods are called overlap-save and overlap-add. The efficiency results from the fact that a direct evaluation of either summation (above) requires formula_44 operations for an output sequence of length N.  An indirect method, using transforms, can take advantage of the formula_45 efficiency of the fast Fourier transform (FFT) to achieve much better performance. Furthermore, convolutions can be used to efficiently compute DFTs via Rader's FFT algorithm and Bluestein's FFT algorithm.
Convolution theorem duality.
It can also be shown that:
Trigonometric interpolation polynomial.
The trigonometric interpolation polynomial
where the coefficients "X""k" are given by the DFT of "x""n" above, satisfies the interpolation property formula_52 for formula_53.
For even "N", notice that the Nyquist component formula_54 is handled specially.
This interpolation is "not unique": aliasing implies that one could add "N" to any of the complex-sinusoid frequencies (e.g. changing formula_55 to formula_56 ) without changing the interpolation property, but giving "different" values in between the formula_9 points. The choice above, however, is typical because it has two useful properties. First, it consists of sinusoids whose frequencies have the smallest possible magnitudes: the interpolation is bandlimited. Second, if the formula_9 are real numbers, then formula_59 is real as well.
In contrast, the most obvious trigonometric interpolation polynomial is the one in which the frequencies range from 0 to formula_60 (instead of roughly formula_61 to formula_62 as above), similar to the inverse DFT formula. This interpolation does "not" minimize the slope, and is "not" generally real-valued for real formula_9; its use is a common mistake.
The unitary DFT.
Another way of looking at the DFT is to note that in the above discussion, the DFT can be expressed as a Vandermonde matrix, 
 in 1867,
where
is a primitive Nth root of unity.
The inverse transform is then given by the inverse of the above matrix,
With unitary normalization constants formula_67, the DFT becomes a unitary transformation, defined by a unitary matrix:
where "det()"  is the determinant function. The determinant is the product of the eigenvalues, which are always formula_71 or formula_72 as described below. In a real vector space, a unitary transformation can be thought of as simply a rigid rotation of the coordinate system, and all of the properties of a rigid rotation can be found in the unitary DFT.
The orthogonality of the DFT is now expressed as an orthonormality condition (which arises in many areas of mathematics as described in root of unity):
If X is defined as the unitary DFT of the vector x, then
and the Plancherel theorem is expressed as
If we view the DFT as just a coordinate transformation which simply specifies the components of a vector in a new coordinate system, then the above is just the statement that the dot product of two vectors is preserved under a unitary DFT transformation. For the special case formula_76, this implies that the length of a vector is preserved as well—this is just Parseval's theorem,
A consequence of the circular convolution theorem is that the DFT matrix diagonalizes any circulant matrix.
Expressing the inverse DFT in terms of the DFT.
A useful property of the DFT is that the inverse DFT can be easily expressed in terms of the (forward) DFT, via several well-known "tricks". (For example, in computations, it is often convenient to only implement a fast Fourier transform corresponding to one transform direction and then to get the other transform direction from the first.)
First, we can compute the inverse DFT by reversing the inputs (Duhamel "et al.", 1988):
Second, one can also conjugate the inputs and outputs:
Third, a variant of this conjugation trick, which is sometimes preferable because it requires no modification of the data values, involves swapping real and imaginary parts (which can be done on a computer simply by modifying pointers). Define swap(formula_9) as formula_9 with its real and imaginary parts swapped—that is, if formula_84 then swap(formula_9) is formula_86. Equivalently, swap(formula_9) equals formula_88. Then
That is, the inverse transform is the same as the forward transform with the real and imaginary parts swapped for both input and output, up to a normalization (Duhamel "et al.", 1988).
The conjugation trick can also be used to define a new transform, closely related to the DFT, that is involutory—that is, which is its own inverse. In particular, formula_90 is clearly its own inverse: formula_91. A closely related involutory transformation (by a factor of (1+"i") /√2) is formula_92, since the formula_93 factors in formula_94 cancel the 2. For real inputs formula_34, the real part of formula_96 is none other than the discrete Hartley transform, which is also involutory.
Eigenvalues and eigenvectors.
The eigenvalues of the DFT matrix are simple and well-known, whereas the eigenvectors are complicated, not unique, and are the subject of ongoing research.
Consider the unitary form formula_97 defined above for the DFT of length "N", where
This matrix satisfies the matrix polynomial equation:
This can be seen from the inverse properties above: operating formula_97 twice gives the original data in reverse order, so operating formula_97 four times gives back the original data and is thus the identity matrix. This means that the eigenvalues formula_102 satisfy the equation:
Therefore, the eigenvalues of formula_97 are the fourth roots of unity: formula_102 is +1, −1, +"i", or −"i".
Since there are only four distinct eigenvalues for this formula_106 matrix, they have some multiplicity. The multiplicity gives the number of linearly independent eigenvectors corresponding to each eigenvalue. (Note that there are "N" independent eigenvectors; a unitary matrix is never defective.)
The problem of their multiplicity was solved by McClellan and Parks (1972), although it was later shown to have been equivalent to a problem solved by Gauss (Dickinson and Steiglitz, 1982). The multiplicity depends on the value of "N" modulo 4, and is given by the following table:
Otherwise stated, the characteristic polynomial of formula_97 is:
No simple analytical formula for general eigenvectors is known. Moreover, the eigenvectors are not unique because any linear combination of eigenvectors for the same eigenvalue is also an eigenvector for that eigenvalue. Various researchers have proposed different choices of eigenvectors, selected to satisfy useful properties like orthogonality and to have "simple" forms (e.g., McClellan and Parks, 1972; Dickinson and Steiglitz, 1982; Grünbaum, 1982; Atakishiyev and Wolf, 1997; Candan "et al.", 2000; Hanna "et al.", 2004; Gurevich and Hadani, 2008).
A straightforward approach is to discretize an eigenfunction of the continuous Fourier transform,
of which the most famous is the Gaussian function.
Since periodic summation of the function means discretizing its frequency spectrum
and discretization means periodic summation of the spectrum,
the discretized and periodically summed Gaussian function yields an eigenvector of the discrete transform:
Two other simple closed-form analytical eigenvectors for special DFT period "N" were found (Kong, 2008):
For DFT period "N" = 2"L" + 1 = 4"K" +1, where "K" is an integer, the following is an eigenvector of DFT:
For DFT period "N" = 2"L" = 4"K", where "K" is an integer, the following is an eigenvector of DFT:
The choice of eigenvectors of the DFT matrix has become important in recent years in order to define a discrete analogue of the fractional Fourier transform—the DFT matrix can be taken to fractional powers by exponentiating the eigenvalues (e.g., Rubio and Santhanam, 2005). For the continuous Fourier transform, the natural orthogonal eigenfunctions are the Hermite functions, so various discrete analogues of these have been employed as the eigenvectors of the DFT, such as the Kravchuk polynomials (Atakishiyev and Wolf, 1997). The "best" choice of eigenvectors to define a fractional discrete Fourier transform remains an open question, however.
Uncertainty principle.
If the random variable is constrained by
then 
may be considered to represent a discrete probability mass function of , with an associated probability mass function constructed from the transformed variable,
For the case of continuous functions formula_115 and formula_116, the Heisenberg uncertainty principle states that
where formula_118 and formula_119 are the variances of formula_120 and formula_121 respectively, with the equality attained in the case of a suitably normalized Gaussian distribution. Although the variances may be analogously defined for the DFT, an analogous uncertainty principle is not useful, because the uncertainty will not be shift-invariant. Still, a meaningful uncertainty principle has been introduced by Massar and Spindel.
However, the Hirschman entropic uncertainty will have a useful analog for the case of the DFT. The Hirschman uncertainty principle is expressed in terms of the Shannon entropy of the two probability functions.
In the discrete case, the Shannon entropies are defined as
and
and the entropic uncertainty principle becomes
The equality is obtained for formula_125 equal to translations and modulations of a suitably normalized Kronecker comb of period formula_126 where formula_126 is any exact integer divisor of formula_128. The probability mass function formula_129 will then be proportional to a suitably translated Kronecker comb of period formula_130.
There is also a well-known deterministic uncertainty principle that uses signal sparsity (or the number of non-zero coefficients). Let formula_131 and formula_132 be the number of non-zero elements of the time and frequency sequences formula_133 and formula_134, respectively. Then, 
As an immediate consequence of the inequality of arithmetic and geometric means, one also has formula_136. Both uncertainty principles were shown to be tight for specifically-chosen "picket-fence" sequences (discrete impulse trains), and find practical use for signal recovery applications.
The real-input DFT.
If formula_137 are real numbers, as they often are in practical applications, then the DFT obeys the symmetry:
It follows that "X""0" and "X""N/2" are real-valued, and the remainder of the DFT is completely specified by just "N/2-1" complex numbers.
Generalized DFT (shifted and non-linear phase).
It is possible to shift the transform sampling in time and/or frequency domain by some real shifts "a" and "b", respectively. This is sometimes known as a generalized DFT (or GDFT), also called the shifted DFT or offset DFT, and has analogous properties to the ordinary DFT:
Most often, shifts of formula_141 (half a sample) are used.
While the ordinary DFT corresponds to a periodic signal in both time and frequency domains, formula_142 produces a signal that is anti-periodic in frequency domain (formula_143) and vice versa for formula_144.
Thus, the specific case of formula_145 is known as an "odd-time odd-frequency" discrete Fourier transform (or O2 DFT).
Such shifted transforms are most often used for symmetric data, to represent different boundary symmetries, and for real-symmetric data they correspond to different forms of the discrete cosine and sine transforms.
Another interesting choice is formula_146, which is called the centered DFT (or CDFT). The centered DFT has the useful property that, when "N" is a multiple of four, all four of its eigenvalues (see above) have equal multiplicities (Rubio and Santhanam, 2005)
The term GDFT is also used for the non-linear phase extensions of DFT. Hence, GDFT method provides a generalization for constant amplitude orthogonal block transforms including linear and non-linear phase types. GDFT is a framework 
to improve time and frequency domain properties of the traditional DFT, e.g. auto/cross-correlations, by the addition of the properly designed phase shaping function (non-linear, in general) to the original linear phase functions (Akansu and Agirman-Tosun, 2010).
The discrete Fourier transform can be viewed as a special case of the z-transform, evaluated on the unit circle in the complex plane; more general z-transforms correspond to "complex" shifts "a" and "b" above.
Multidimensional DFT.
The ordinary DFT transforms a one-dimensional sequence or array formula_9 that is a function of exactly one discrete variable "n". The multidimensional DFT of a multidimensional array formula_148 that is a function of "d" discrete variables formula_149 for formula_150 in formula_151 is defined by:
where formula_153 as above and the "d" output indices run from formula_154. This is more compactly expressed in vector notation, where we define formula_155 and formula_156 as "d"-dimensional vectors of indices from 0 to formula_157, which we define as formula_158:
where the division formula_160 is defined as formula_161 to be performed element-wise, and the sum denotes the set of nested summations above.
The inverse of the multi-dimensional DFT is, analogous to the one-dimensional case, given by:
As the one-dimensional DFT expresses the input formula_9 as a superposition of sinusoids, the multidimensional DFT expresses the input as a superposition of plane waves, or multidimensional sinusoids. The direction of oscillation in space is formula_164. The amplitudes are formula_165. This decomposition is of great importance for everything from digital image processing (two-dimensional) to solving partial differential equations. The solution is broken up into plane waves.
The multidimensional DFT can be computed by the composition of a sequence of one-dimensional DFTs along each dimension. In the two-dimensional case formula_166 the formula_167 independent DFTs of the rows (i.e., along formula_168) are computed first to form a new array formula_169. Then the formula_170 independent DFTs of "y" along the columns (along formula_171) are computed to form the final result formula_172. Alternatively the columns can be computed first and then the rows. The order is immaterial because the nested summations above commute.
An algorithm to compute a one-dimensional DFT is thus sufficient to efficiently compute a multidimensional DFT. This approach is known as the "row-column" algorithm. There are also intrinsically multidimensional FFT algorithms.
The real-input multidimensional DFT.
For input data formula_148 consisting of real numbers, the DFT outputs have a conjugate symmetry similar to the one-dimensional case above:
where the star again denotes complex conjugation and the formula_150-th subscript is again interpreted modulo formula_176 (for formula_177).
Applications.
The DFT has seen wide usage across a large number of fields; we only sketch a few examples below (see also the references at the end). All applications of the DFT depend crucially on the availability of a fast algorithm to compute discrete Fourier transforms and their inverses, a fast Fourier transform.
Spectral analysis.
When the DFT is used for signal spectral analysis, the formula_178 sequence usually represents a finite set of uniformly spaced time-samples of some signal formula_179, where "t" represents time. The conversion from continuous time to samples (discrete-time) changes the underlying Fourier transform of x(t) into a discrete-time Fourier transform (DTFT), which generally entails a type of distortion called aliasing. Choice of an appropriate sample-rate (see "Nyquist rate") is the key to minimizing that distortion. Similarly, the conversion from a very long (or infinite) sequence to a manageable size entails a type of distortion called "leakage", which is manifested as a loss of detail (a.k.a. resolution) in the DTFT. Choice of an appropriate sub-sequence length is the primary key to minimizing that effect. When the available data (and time to process it) is more than the amount needed to attain the desired frequency resolution, a standard technique is to perform multiple DFTs, for example to create a spectrogram. If the desired result is a power spectrum and noise or randomness is present in the data, averaging the magnitude components of the multiple DFTs is a useful procedure to reduce the variance of the spectrum (also called a periodogram in this context); two examples of such techniques are the Welch method and the Bartlett method; the general subject of estimating the power spectrum of a noisy signal is called spectral estimation.
A final source of distortion (or perhaps "illusion") is the DFT itself, because it is just a discrete sampling of the DTFT, which is a function of a continuous frequency domain. That can be mitigated by increasing the resolution of the DFT. That procedure is illustrated at Sampling the DTFT.
Filter bank.
See FFT filter banks and Sampling the DTFT.
Data compression.
The field of digital signal processing relies heavily on operations in the frequency domain (i.e. on the Fourier transform). For example, several lossy image and sound compression methods employ the discrete Fourier transform: the signal is cut into short segments, each is transformed, and then the Fourier coefficients of high frequencies, which are assumed to be unnoticeable, are discarded. The decompressor computes the inverse transform based on this reduced number of Fourier coefficients. (Compression applications often use a specialized form of the DFT, the discrete cosine transform or sometimes the modified discrete cosine transform.)
Some relatively recent compression algorithms, however, use wavelet transforms, which give a more uniform compromise between time and frequency domain than obtained by chopping data into segments and transforming each segment. In the case of JPEG2000, this avoids the spurious image features that appear when images are highly compressed with the original JPEG.
Partial differential equations.
Discrete Fourier transforms are often used to solve partial differential equations, where again the DFT is used as an approximation for the Fourier series (which is recovered in the limit of infinite "N"). The advantage of this approach is that it expands the signal in complex exponentials "e""inx", which are eigenfunctions of differentiation: "d"/"dx" "e""inx" = "in" "e""inx". Thus, in the Fourier representation, differentiation is simple—we just multiply by "i n". (Note, however, that the choice of "n" is not unique due to aliasing; for the method to be convergent, a choice similar to that in the trigonometric interpolation section above should be used.) A linear differential equation with constant coefficients is transformed into an easily solvable algebraic equation. One then uses the inverse DFT to transform the result back into the ordinary spatial representation. Such an approach is called a spectral method.
Polynomial multiplication.
Suppose we wish to compute the polynomial product "c"("x") = "a"("x") · "b"("x"). The ordinary product expression for the coefficients of "c" involves a linear (acyclic) convolution, where indices do not "wrap around." This can be rewritten as a cyclic convolution by taking the coefficient vectors for "a"("x") and "b"("x") with constant term first, then appending zeros so that the resultant coefficient vectors a and b have dimension "d" > deg("a"("x")) + deg("b"("x")). Then,
Where c is the vector of coefficients for "c"("x"), and the convolution operator formula_181 is defined so
But convolution becomes multiplication under the DFT:
Here the vector product is taken elementwise. Thus the coefficients of the product polynomial "c"("x") are just the terms 0, ..., deg("a"("x")) + deg("b"("x")) of the coefficient vector
With a fast Fourier transform, the resulting algorithm takes O ("N" log "N") arithmetic operations. Due to its simplicity and speed, the Cooley–Tukey FFT algorithm, which is limited to composite sizes, is often chosen for the transform operation. In this case, "d" should be chosen as the smallest integer greater than the sum of the input polynomial degrees that is factorizable into small prime factors (e.g. 2, 3, and 5, depending upon the FFT implementation).
Multiplication of large integers.
The fastest known algorithms for the multiplication of very large integers use the polynomial multiplication method outlined above. Integers can be treated as the value of a polynomial evaluated specifically at the number base, with the coefficients of the polynomial corresponding to the digits in that base. After polynomial multiplication, a relatively low-complexity carry-propagation step completes the multiplication.
Convolution.
When data is convolved with a function with wide support, such as for downsampling by a large sampling ratio, because of the Convolution theorem and the FFT algorithm, it may be faster to transform it, multiply pointwise by the transform of the filter and then reverse transform it. Alternatively, a good filter is obtained by simply truncating the transformed data and re-transforming the shortened data set.
Generalizations.
Representation theory.
The DFT can be interpreted as the complex-valued representation theory of the finite cyclic group. In other words, a sequence of "n" complex numbers can be thought of as an element of "n"-dimensional complex space C"n" or equivalently a function "f" from the finite cyclic group of order "n" to the complex numbers, Z"n" → C. So "f" is a class function on the finite cyclic group, and thus can be expressed as a linear combination of the irreducible characters of this group, which are the roots of unity.
From this point of view, one may generalize the DFT to representation theory generally, or more narrowly to the representation theory of finite groups.
More narrowly still, one may generalize the DFT by either changing the target (taking values in a field other than the complex numbers), or the domain (a group other than a finite cyclic group), as detailed in the sequel.
Other fields.
Many of the properties of the DFT only depend on the fact that formula_185 is a primitive root of unity, sometimes denoted formula_186 or formula_187 (so that formula_188). Such properties include the completeness, orthogonality, Plancherel/Parseval, periodicity, shift, convolution, and unitarity properties above, as well as many FFT algorithms. For this reason, the discrete Fourier transform can be defined by using roots of unity in fields other than the complex numbers, and such generalizations are commonly called "number-theoretic transforms" (NTTs) in the case of finite fields. For more information, see number-theoretic transform and discrete Fourier transform (general).
Other finite groups.
The standard DFT acts on a sequence "x"0, "x"1, …, "x""N"−1 of complex numbers, which can be viewed as a function {0, 1, …, "N" − 1} → C. The multidimensional DFT acts on multidimensional sequences, which can be viewed as functions
This suggests the generalization to Fourier transforms on arbitrary finite groups, which act on functions "G" → C where "G" is a finite group. In this framework, the standard DFT is seen as the Fourier transform on a cyclic group, while the multidimensional DFT is a Fourier transform on a direct sum of cyclic groups.
Alternatives.
 There are various alternatives to the DFT for various applications, prominent among which are wavelets. The analog of the DFT is the discrete wavelet transform (DWT). From the point of view of time–frequency analysis, a key limitation of the Fourier transform is that it does not include "location" information, only "frequency" information, and thus has difficulty in representing transients. As wavelets have location as well as frequency, they are better able to represent location, at the expense of greater difficulty representing frequency. For details, see comparison of the discrete wavelet transform with the discrete Fourier transform.

</doc>
<doc id="468536" url="https://en.wikipedia.org/wiki?curid=468536" title="Discrete group">
Discrete group

In mathematics, a discrete group is a group "G" equipped with the discrete topology. With this topology, "G" becomes a topological group. A discrete subgroup of a topological group "G" is a subgroup "H" whose relative topology is the discrete one. For example, the integers, Z, form a discrete subgroup of the reals, R (with the standard metric topology), but the rational numbers, Q, do not.
Any group can be given the discrete topology. Since every map from a discrete space is continuous, the topological homomorphisms between discrete groups are exactly the group homomorphisms between the underlying groups. Hence, there is an isomorphism between the category of groups and the category of discrete groups. Discrete groups can therefore be identified with their underlying (non-topological) groups.
There are some occasions when a topological group or Lie group is usefully endowed with the discrete topology, 'against nature'. This happens for example in the theory of the Bohr compactification, and in group cohomology theory of Lie groups.
A discrete isometry group is an isometry group such that for every point of the metric space the set of images of the point under the isometries is a discrete set. A discrete symmetry group is a symmetry group that is a discrete isometry group.
Properties.
Since topological groups are homogeneous, one need only look at a single point to determine if the topological group is discrete. In particular, a topological group is discrete if and only if the singleton containing the identity is an open set.
A discrete group is the same thing as a zero-dimensional Lie group (uncountable discrete groups are not second-countable so authors who require Lie groups to satisfy this axiom do not regard these groups as Lie groups). The identity component of a discrete group is just the trivial subgroup while the group of components is isomorphic to the group itself.
Since the only Hausdorff topology on a finite set is the discrete one, a finite Hausdorff topological group must necessarily be discrete. It follows that every finite subgroup of a Hausdorff group is discrete.
A discrete subgroup "H" of "G" is cocompact if there is a compact subset "K" of "G" such that "HK" = "G".
Discrete normal subgroups play an important role in the theory of covering groups and locally isomorphic groups. A discrete normal subgroup of a connected group "G" necessarily lies in the center of "G" and is therefore abelian.
"Other properties":

</doc>
<doc id="56061" url="https://en.wikipedia.org/wiki?curid=56061" title="Discrete space">
Discrete space

In topology, a discrete space is a particularly simple example of a topological space or similar structure, one in which the points form a "discontinuous sequence", meaning they are "isolated" from each other in a certain sense. The discrete topology is the finest topology that can be given on a set, i.e., it defines all subsets as open sets. In particular, each singleton is an open set in the discrete topology.
Definitions.
Given a set "X":
for any formula_3. In this case formula_4 is called a discrete metric space or a space of isolated points.
A metric space formula_16 is said to be "uniformly discrete" if there exists a "packing radius" formula_17 such that, for any formula_18, one has either formula_19 or formula_20. The topology underlying a metric space can be discrete, without the metric being uniformly discrete: for example the usual metric on the set {1, 1/2, 1/4, 1/8, ...} of real numbers.
Let X = {1, 1/2, 1/4, 1/8, ...}, consider this set using the usual metric on the real numbers. Then, X is a discrete space, since for each point 1/2n, we can surround it with the interval (1/2n - ɛ, 1/2n + ɛ), where ɛ = 1/2(1/2n - 1/2n+1) = 1/2n+2. The intersection (1/2n - ɛ, 1/2n + ɛ) ∩ {1/2n} is just the singleton {1/2n}. Since the intersection of two open sets is open, and singletons are open, it follows that X is a discrete space.
However, X cannot be uniformly discrete. To see why, suppose there exists an r>0 such that d(x,y)>r whenever x≠y. It suffices to show that there are at least two points x and y in X that are closer to each other than r. Since the distance between adjacent points 1/2n and 1/2n+1 is 1/2n+1, we need to find an n that satisfies this inequality:
formula_21
formula_22
formula_23
formula_24
formula_25
formula_26
Since there is always an n bigger than any given real number, it follows that there will always be at least two points in X that are closer to each other than any positive r, therefore X is not uniformly discrete.
Properties.
The underlying uniformity on a discrete metric space is the discrete uniformity, and the underlying topology on a discrete uniform space is the discrete topology.
Thus, the different notions of discrete space are compatible with one another.
On the other hand, the underlying topology of a non-discrete uniform or metric space can be discrete; an example is the metric space "X" := {1/"n" : "n" = 1,2,3...} (with metric inherited from the real line and given by d("x","y") = |"x" − "y"|).
Obviously, this is not the discrete metric; also, this space is not complete and hence not discrete as a uniform space.
Nevertheless, it is discrete as a topological space.
We say that "X" is "topologically discrete" but not "uniformly discrete" or "metrically discrete".
Additionally:
Any function from a discrete topological space to another topological space is continuous, and any function from a discrete uniform space to another uniform space is uniformly continuous. That is, the discrete space "X" is free on the set "X" in the category of topological spaces and continuous maps or in the category of uniform spaces and uniformly continuous maps. These facts are examples of a much broader phenomenon, in which discrete structures are usually free on sets.
With metric spaces, things are more complicated, because there are several categories of metric spaces, depending on what is chosen for the morphisms. Certainly the discrete metric space is free when the morphisms are all uniformly continuous maps or all continuous maps, but this says nothing interesting about the metric structure, only the uniform or topological structure. Categories more relevant to the metric structure can be found by limiting the morphisms to Lipschitz continuous maps or to short maps; however, these categories don't have free objects (on more than one element). However, the discrete metric space is free in the category of bounded metric spaces and Lipschitz continuous maps, and it is free in the category of metric spaces bounded by 1 and short maps. That is, any function from a discrete metric space to another bounded metric space is Lipschitz continuous, and any function from a discrete metric space to another metric space bounded by 1 is short.
Going the other direction, a function "f" from a topological space "Y" to a discrete space "X" is continuous if and only if it is "locally constant" in the sense that every point in "Y" has a neighborhood on which "f" is constant.
Uses.
A discrete structure is often used as the "default structure" on a set that doesn't carry any other natural topology, uniformity, or metric; discrete structures can often be used as "extreme" examples to test particular suppositions. For example, any group can be considered as a topological group by giving it the discrete topology, implying that theorems about topological groups apply to all groups. Indeed, analysts may refer to the ordinary, non-topological groups studied by algebraists as "discrete groups" . In some cases, this can be usefully applied, for example in combination with Pontryagin duality. A 0-dimensional manifold (or differentiable or analytical manifold) is nothing but a discrete topological space. We can therefore view any discrete group as a 0-dimensional Lie group.
A product of countably infinite copies of the discrete space of natural numbers is homeomorphic to the space of irrational numbers, with the homeomorphism given by the continued fraction expansion. A product of countably infinite copies of the discrete space {0,1} is homeomorphic to the Cantor set; and in fact uniformly homeomorphic to the Cantor set if we use the product uniformity on the product. Such a homeomorphism is given by using ternary notation of numbers. (See Cantor space.)
In the foundations of mathematics, the study of compactness properties of products of {0,1} is central to the topological approach to the ultrafilter principle, which is a weak form of choice.
Indiscrete spaces.
In some ways, the opposite of the discrete topology is the trivial topology (also called the "indiscrete topology"), which has the fewest possible open sets (just the empty set and the space itself). Where the discrete topology is initial or free, the indiscrete topology is final or cofree: every function "from" a topological space "to" an indiscrete space is continuous, etc.

</doc>
<doc id="55607" url="https://en.wikipedia.org/wiki?curid=55607" title="Discriminant">
Discriminant

In algebra, the discriminant of a polynomial is a function of its coefficients, typically denoted by a capital 'D' or the capital Greek letter Delta (Δ). It gives information about the nature of its roots. Typically, the discriminant is zero if and only if the polynomial has a multiple root. For example, the discriminant of the quadratic polynomial
is
Here for real a, b and c, if Δ > 0, the polynomial has two real roots, if Δ = 0, the polynomial has one real double root, and if Δ < 0, the two roots of the polynomial are complex conjugates.
The discriminant of the cubic polynomial
is
For higher degrees, the discriminant is always a polynomial function of the coefficients. It becomes significantly longer for the higher degrees. The discriminant of a "general" quartic has 16 terms, that of a quintic has 59 terms, that of a 6th degree polynomial has 246 terms,
and the number of terms increases exponentially with the degree.
A polynomial has a multiple root (i.e. a root with multiplicity greater than one) in the complex numbers if and only if its discriminant is zero.
The concept also applies if the polynomial has coefficients in a field which is not contained in the complex numbers. In this case, the discriminant vanishes if and only if the polynomial has a multiple root in any algebraically closed field containing the coefficients.
As the discriminant is a polynomial function of the coefficients, it is defined as long as the coefficients belong to an integral domain "R" and, in this case, the discriminant is in "R". In particular, the discriminant of a polynomial with integer coefficients is always an integer. This property is widely used in number theory.
The term "discriminant" was coined in 1851 by the British mathematician James Joseph Sylvester.
Definition.
In terms of the roots, the discriminant is given by
where formula_6 is the leading coefficient and formula_7 are the roots (counting multiplicity) of the polynomial in some splitting field. It is the square of the Vandermonde polynomial times formula_8.
As the discriminant is a symmetric function in the roots, it can also be expressed in terms of the coefficients of the polynomial, since the coefficients are the elementary symmetric polynomials in the roots; such a formula is given below.
Expressing the discriminant in terms of the roots makes its key property clear, namely that it vanishes if and only if there is a repeated root, but does not allow it to be calculated without factoring a polynomial, after which the information it provides is redundant (if one has the roots, one can tell if there are any duplicates). Hence the formula in terms of the coefficients allows the nature of the roots to be determined without factoring the polynomial.
Formulas for low degrees.
The discriminant of a linear polynomial (degree 1) is rarely considered. If needed, it is commonly defined to be equal to 1 (this is compatible with the usual conventions for the empty product and the determinant of the empty matrix). There is no common convention for the discriminant of a constant polynomial (degree 0).
The quadratic polynomial
has discriminant
The cubic polynomial
has discriminant
The quartic polynomial
has discriminant
These are homogeneous polynomials in the coefficients, respectively of degree 2, 4 and 6. They are also homogeneous in terms of the roots, of respective degrees 2, 6 and 12.
Simpler polynomials have simpler expressions for their discriminants. For example, the monic quadratic polynomial "x"2 + "bx" + "c" has discriminant Δ = "b"2 − 4"c".
The monic cubic polynomial without quadratic term "x"3 + "px" + "q" has discriminant Δ = −4"p"3 − 27"q"2.
In terms of the roots, these discriminants are homogeneous polynomials of respective degree 2 and 6.
Homogeneity.
The discriminant is a homogeneous polynomial in the coefficients; it is also a homogeneous polynomial in the roots.
In the coefficients, the discriminant is homogeneous of degree 2"n"−2; this can be seen two ways. In terms of the roots-and-leading-term formula, multiplying all the coefficients by λ does not change the roots, but multiplies the leading term by λ. In terms of the formula as a determinant of a (2"n"−1) ×(2"n"−1) matrix divided by "an", the determinant of the matrix is homogeneous of degree 2"n"−1 in the entries, and dividing by "an" makes the degree 2"n"−2; explicitly, multiplying the coefficients by λ multiplies all entries of the matrix by λ, hence multiplies the determinant by λ2"n"−1.
For a monic polynomial, the discriminant is a polynomial in the roots alone (as the "an" term is one), and is of degree "n"("n"−1) in the roots, as there are formula_15 terms in the product, each squared.
Let us consider the polynomial
It follows from what precedes that its discriminant is homogeneous of degree 2"n"−2 in the formula_17 and quasi-homogeneous of weight "n"("n"−1) if each formula_18 is given the weight "i". In other words, every monomial formula_19 appearing in the discriminant satisfies the two equations
and
These thus correspond to the partitions of "n"("n"−1) into at 2"n"−2 (non negative) parts of size at most n
This restricts the possible terms in the discriminant. For the quadratic polynomial formula_22 there are only two possibilities for formula_23 either [1,0,1] or [0,2,0], given the two monomials "ac" and "b"2.
For the cubic polynomial formula_24, these are the partitions of 6 into 4 parts of size at most 3:
All these five monomials occur effectively in the discriminant.
While this approach gives the possible terms, it does not determine the coefficients. Moreover, in general not all possible terms will occur in the discriminant. The first example is for the quartic polynomial formula_26, in which case formula_27 satisfies formula_28 and formula_29, even though the corresponding discriminant does not involve the monomial formula_30.
Quadratic formula.
The quadratic polynomial formula_31 has discriminant
which is the quantity under the square root sign in the quadratic formula. For real numbers "a", "b", "c", one has:
and its graph crosses the "x"-axis twice.
and its graph is tangent to the "x"-axis.
An alternative way to understand the discriminant of a quadratic is to use the characterization as "zero if and only if the polynomial has a repeated root".
In that case the polynomial is formula_36
The coefficients then satisfy formula_37 so formula_38
and a monic quadratic has a repeated root if and only if this is the case, in which case the root is formula_39 Putting both terms on one side and including a leading coefficient yields formula_40
Discriminant of a polynomial.
To find the formula for the discriminant of a polynomial in terms of its coefficients, it is easiest to introduce the resultant. Just as the discriminant of a single polynomial is the product of the square of the differences between distinct roots, the resultant of two polynomials is the product of the differences between their roots, and just as the discriminant vanishes if and only if the polynomial has a repeated root, the resultant vanishes if and only if the two polynomials share a root.
Since a polynomial formula_41 has a repeated root if and only if it shares a root with its derivative formula_42 the discriminant formula_43 and the resultant formula_44 both have the property that they vanish if and only if "p" has a repeated root, and they have almost the same degree (the degree of the resultant is one greater than the degree of the discriminant) and thus are equal up to a factor of degree one, which is, up to the sign, the leading coefficient of "p". 
The benefit of the resultant is that it can be computed as a determinant, namely as the determinant of the Sylvester matrix, a matrix, whose first rows contain the coefficients of "p" and the "n" last ones the coefficients of its derivative.
The resultant formula_44 of the general polynomial
is equal to the determinant of the Sylvester matrix:
The discriminant formula_43 of formula_41 is now given by the formula
For example, in the case , the above determinant is
The discriminant of the degree 4 polynomial is then obtained from this determinant upon dividing by formula_52.
In terms of the roots, the discriminant is equal to
where "r"1, ..., "r""n" are the complex roots (counting multiplicity) of the polynomial:
This second expression makes it clear that "p" has a multiple root if and only if the discriminant is zero. (This multiple root can be complex.)
The discriminant can be defined for polynomials over arbitrary fields, in exactly the same fashion as above. The product formula involving the roots "r""i" remains valid; the roots have to be taken in some splitting field of the polynomial. The discriminant can even be defined for polynomials over any commutative ring. However, if the ring is not an integral domain, above division of the resultant by formula_6 should be replaced by substituting formula_6 by 1 in the first column of the matrix.
Nature of the roots.
The discriminant gives additional information on the nature of the roots beyond simply whether there are any repeated roots: for polynomials with real coefficients, it also gives information on whether the roots are real or complex. This is most transparent and easily stated for quadratic and cubic polynomials; for polynomials of degree 4 or higher this is more difficult to state.
Quadratic.
Because the quadratic formula expressed the roots of a quadratic polynomial as a rational function in terms of the "square root" of the discriminant, the roots of a quadratic polynomial are in the same field as the coefficients if and only if the discriminant is a square in the field of coefficients: in other words, the polynomial factors over the field of coefficients if and only if the discriminant is a square.
As a real number has real square roots if and only if it is nonnegative, and these roots are distinct if and only if it is positive (not zero), the sign of the discriminant allows a complete description of the nature of the roots of a quadratic polynomial with real coefficients:
Further, for a quadratic polynomial with rational coefficients, it factors over the rationals if and only if the discriminant – which is necessarily a rational number, being a polynomial in the coefficients – is in fact a square.
Cubic.
For a cubic polynomial with real coefficients, the discriminant reflects the nature of the roots as follows:
If a cubic polynomial has a triple root, it is a root of its derivative and of its second derivative, which is linear. Thus to decide if a cubic polynomial has a triple root or not, one may compute the root of the second derivative and look if it is a root of the cubic and of its derivative.
Higher degrees.
More generally, for a polynomial of degree "n" with real coefficients, we have
Discriminant of a polynomial over a commutative ring.
The definition of the discriminant of a polynomial in terms of the resultant may easily be extended to polynomials whose coefficients belong to any commutative ring. However, as the division is not always defined in such a ring, instead of dividing the determinant by the leading coefficient, one substitutes the leading coefficient by 1 in the first column of the determinant. This generalized discriminant has the following property which is fundamental in algebraic geometry.
Let "f" be a polynomial with coefficients in a commutative ring "A" and "D" its discriminant. Let φ be a ring homomorphism of "A" into a field "K" and φ("f") be the polynomial over "K" obtained by replacing the coefficients of "f" by their images by φ. Then φ("D") = 0 if and only if either the difference of the degrees of "f" and φ("f") is at least 2 or φ("f") has a multiple root in an algebraic closure of "K". The first case may be interpreted by saying that φ("f") has a multiple root at infinity.
The typical situation where this property is applied is when "A" is a (univariate or multivariate) polynomial ring over a field "k" and φ is the substitution of the indeterminates in "A" by elements of a field extension "K" of "k".
For example, let "f" be a bivariate polynomial in "X" and "Y" with real coefficients, such that "f" = 0 is the implicit equation of a plane algebraic curve. Viewing "f" as a univariate polynomial in "Y" with coefficients depending on "X", then the discriminant is a polynomial in "X" whose roots are the "X"-coordinates of the singular points, of the points with a tangent parallel to the "Y"-axis and of some of the asymptotes parallel to the "Y"-axis. In other words the computation of the roots of the "Y"-discriminant and the "X"-discriminant allows to compute all remarkable points of the curve, except the inflection points.
Generalizations.
The concept of discriminant has been generalized to other algebraic structures besides polynomials of one variable, including conic sections, quadratic forms, and algebraic number fields. Discriminants in algebraic number theory are closely related, and contain information about ramification. In fact, the more geometric types of ramification are also related to more abstract types of discriminant, making this a central algebraic idea in many applications.
Discriminant of a conic section.
For a conic section defined in plane geometry by the real polynomial
the discriminant is equal to
and determines the shape of the conic section. If the discriminant is less than 0, the equation is of an ellipse or a circle. If the discriminant equals 0, the equation is that of a parabola. If the discriminant is greater than 0, the equation is that of a hyperbola. This formula will not work for degenerate cases (when the polynomial factors).
Discriminant of a quadratic form.
There is a substantive generalization to quadratic forms "Q" over any field "K" of . For characteristic 2, the corresponding invariant is the Arf invariant.
Given a quadratic form "Q", the discriminant or determinant is the determinant of a symmetric matrix "S" for "Q".
Change of variables by a matrix "A" changes the matrix of the symmetric form by "A"T"SA", which has determinant , so under change of variables, the discriminant changes by a non-zero square, and thus the class of the discriminant is well-defined in "K"/("K"×)2, i.e., up to non-zero squares. See also Quadratic residue.
Less intrinsically, by a theorem of Jacobi, quadratic forms on formula_61 can be expressed, after a linear change of variables, in diagonal form as
More precisely, a quadratic forms on "V" may be expressed as a sum
where the "L""i" are independent linear forms and "n" is the number of the variables (some of the "a""i" may be zero). Then the discriminant is the product of the "a""i", which is well-defined as a class in "K"/("K"×)2.
For , the real numbers, (R×)2 is the positive real numbers (any positive number is a square of a non-zero number), and thus the quotient R/(R×)2 has three elements: positive, zero, and negative. This is a cruder invariant than signature , where "n"0 is the number of 0s and "n"± is the number of ±1s in diagonal form. The discriminant is then zero if the form is degenerate (), and otherwise it is the parity of the number of negative coefficients, (−1)n−.
For , the complex numbers, (C×)2 is the non-zero complex numbers (any complex number is a square), and thus the quotient C/(C×)2 has two elements: non-zero and zero.
This definition generalizes the discriminant of a quadratic polynomial, as the polynomial formula_64 homogenizes to the quadratic form formula_65 which has symmetric matrix
whose determinant is formula_67. Up to a factor of −4, this is formula_68.
The invariance of the class of the discriminant of a real form (positive, zero, or negative) corresponds to the corresponding conic section being an ellipse, parabola, or hyperbola.
Alternating polynomials.
The discriminant is a symmetric polynomial in the roots; if one adjoins a square root of it (halves each of the powers: the Vandermonde polynomial) to the ring of symmetric polynomials in "n" variables formula_69, one obtains the ring of alternating polynomials, which is thus a quadratic extension of formula_69.

</doc>
<doc id="859275" url="https://en.wikipedia.org/wiki?curid=859275" title="Displacement (vector)">
Displacement (vector)

A displacement is the shortest distance from the initial to the final position of a point P.
Thus, it is the length of an imaginary straight path, typically distinct from the path actually travelled by P. A displacement vector represents the length and direction of this imaginary straight path.
A position vector expresses the position of a point P in space in terms of a displacement from an arbitrary reference point O (typically the origin of a coordinate system). Namely, it indicates both the distance and direction of an imaginary motion along a straight line from the reference position to the actual position of the point. 
A displacement may be also described as a 'relative position': the final position of a point (Rf) relative to its initial position (Ri), and a displacement vector can be mathematically defined as the difference between the final and initial position vectors:
In considering motions of objects over time the instantaneous velocity of the object is the rate of change of the displacement as a function of time. The velocity then is distinct from the instantaneous speed which is the time rate of change of the distance traveled along a specific path. The velocity may be equivalently defined as the time rate of change of the position vector. If one considers a moving initial position, or equivalently a moving origin (e.g. an initial position or origin which is fixed to a train wagon, which in turn moves with respect to its rail track), the velocity of P (e.g. a point representing the position of a passenger walking on the train) may be referred to as a relative velocity, as opposed to an absolute velocity, which is computed with respect to a point which is considered to be 'fixed in space' (such as, for instance, a point fixed on the floor of the train station).
For motion over a given interval of time, the displacement divided by the length of the time interval defines the average velocity. (Note that the average velocity, as a vector, differs from the average speed that is the ratio of the path length — a scalar — and the time interval.)
Rigid body.
In dealing with the motion of a rigid body, the term "displacement" may also include the rotations of the body. In this case, the displacement of a particle of the body is called linear displacement (displacement along a line), while the rotation of the body is called angular displacement.
Derivatives.
For a position vector s that is a function of time "t", the derivatives can be computed with respect to "t". These derivatives have common utility in the study of kinematics, control theory, vibration sensing and other sciences and engineering disciplines.
Velocity
Acceleration
Jerk
These common names correspond to terminology used in basic kinematics. By extension, the higher order derivatives can be computed in a similar fashion. Study of these higher order derivatives can improve approximations of the original displacement function. Such higher-order terms are required in order to accurately represent the displacement function as a sum of an infinite series, enabling several analytical techniques in engineering and physics. The fourth order derivative is called jounce, the fifth crackle, and the sixth pop.

</doc>
<doc id="39378" url="https://en.wikipedia.org/wiki?curid=39378" title="Distance">
Distance

Distance is a numerical description of how far apart objects are. In physics or everyday usage, distance may refer to a physical length, or an estimation based on other criteria (e.g. "two counties over"). In mathematics, a distance function or metric is a generalization of the concept of physical distance. A metric is a function that behaves according to a specific set of rules, and is a concrete way of describing what it means for elements of some space to be "close to" or "far away from" each other.
In most cases, "distance from A to B" is interchangeable with "distance between B and A".
Mathematics.
Geometry.
In analytic geometry, the distance between two points of the xy-plane can be found using the distance formula. The distance between ("x"1, "y"1) and ("x"2, "y"2) is given by:
Similarly, given points ("x"1, "y"1, "z"1) and ("x"2, "y"2, "z"2) in three-space, the distance between them is:
These formula are easily derived by constructing a right triangle with a leg on the hypotenuse of another (with the other leg orthogonal to the plane that contains the 1st triangle) and applying the Pythagorean theorem.
In the study of complicated geometries, we call this (most common) type of distance Euclidean distance, as it is derived from the Pythagorean theorem, which does not hold in Non-Euclidean geometries. This distance formula can also be expanded into the arc-length formula.
Distance in Euclidean space.
In the Euclidean space Rn, the distance between two points is usually given by the Euclidean distance (2-norm distance). Other distances, based on other norms, are sometimes used instead.
For a point ("x"1, "x"2, ...,"x""n") and a point ("y"1, "y"2, ...,"y""n"), the Minkowski distance of order p (p-norm distance) is defined as:
"p" need not be an integer, but it cannot be less than 1, because otherwise the triangle inequality does not hold.
The 2-norm distance is the Euclidean distance, a generalization of the Pythagorean theorem to more than two coordinates. It is what would be obtained if the distance between two points were measured with a ruler: the "intuitive" idea of distance.
The 1-norm distance is more colourfully called the "taxicab norm" or "Manhattan distance", because it is the distance a car would drive in a city laid out in square blocks (if there are no one-way streets).
The infinity norm distance is also called Chebyshev distance. In 2D, it is the minimum number of moves kings require to travel between two squares on a chessboard.
The "p"-norm is rarely used for values of "p" other than 1, 2, and infinity, but see super ellipse.
In physical space the Euclidean distance is in a way the most natural one, because in this case the length of a rigid body does not change with rotation.
Variational formulation of distance.
The Euclidean distance between two points in space (formula_3 and formula_4) may be written in a variational form where the distance is the minimum value of an integral:
Here formula_6 is the trajectory (path) between the two points. The value of the integral (D) represents the length of this trajectory. The distance is the minimal value of this integral and is obtained when formula_7 where formula_8 is the optimal trajectory. In the familiar Euclidean case (the above integral) this optimal trajectory is simply a straight line. It is well known that the shortest path between two points is a straight line. Straight lines can formally be obtained by solving the Euler–Lagrange equations for the above functional. In non-Euclidean manifolds (curved spaces) where the nature of the space is represented by a metric tensor formula_9 the integrand has be to modified to formula_10, where Einstein summation convention has been used.
Generalization to higher-dimensional objects.
The Euclidean distance between two objects may also be generalized to the case where the objects are no longer points but are higher-dimensional manifolds, such as space curves, so in addition to talking about distance between two points one can discuss concepts of distance between two strings. Since the new objects that are dealt with are extended objects (not points anymore) additional concepts such as non-extensibility, curvature constraints, and non-local interactions that enforce non-crossing become central to the notion of distance. The distance between the two manifolds is the scalar quantity that results from minimizing the generalized distance functional, which represents a transformation between the two manifolds:
The above double integral is the generalized distance functional between two polymer conformation. formula_12 is a spatial parameter and formula_13 is pseudo-time. This means that formula_14 is the polymer/string conformation at time formula_15 and is parameterized along the string length by formula_16. Similarly formula_17 is the trajectory of an infinitesimal segment of the string during transformation of the entire string from conformation formula_18 to conformation formula_19. The term with cofactor formula_20 is a Lagrange multiplier and its role is to ensure that the length of the polymer remains the same during the transformation. If two discrete polymers are inextensible, then the minimal-distance transformation between them no longer involves purely straight-line motion, even on a Euclidean metric. There is a potential application of such generalized distance to the problem of protein folding
This generalized distance is analogous to the Nambu-Goto action in string theory, however there is no exact correspondence because the Euclidean distance in 3-space is inequivalent to the space-time distance minimized for the classical relativistic string.
Algebraic distance.
This is a metric often used in computer vision that can be minimized by least squares estimation. For curves or surfaces given by the equation formula_21 (such as a conic in homogeneous coordinates), the algebraic distance from the point formula_22 to the curve is simply formula_23.
It may serve as an "initial guess" for geometric distance to refine estimations of the curve by more accurate methods, such as non-linear least squares.
General metric.
In mathematics, in particular geometry, a distance function on a given set "M" is a function , where R denotes the set of real numbers, that satisfies the following conditions:
For example, the usual definition of distance between two real numbers "x" and "y" is: . This definition satisfies the three conditions above, and corresponds to the standard topology of the real line. But distance on a given set is a definitional choice. Another possible choice is to define: if , and 1 otherwise. This also defines a metric, but gives a completely different topology, the "discrete topology"; with this definition numbers cannot be arbitrarily close.
Distances between sets and between a point and a set.
Various distance definitions are possible between objects. For example, between celestial bodies one should not confuse the surface-to-surface distance and the center-to-center distance. If the former is much less than the latter, as for a LEO, the first tends to be quoted (altitude), otherwise, e.g. for the Earth-Moon distance, the latter.
There are two common definitions for the distance between two non-empty subsets of a given set:
The distance between a point and a set is the infimum of the distances between the point and those in the set. This corresponds to the distance, according to the first-mentioned definition above of the distance between sets, from the set containing only this point to the other set.
In terms of this, the definition of the Hausdorff distance can be simplified: it is the larger of two values, one being the supremum, for a point ranging over one set, of the distance between the point and the set, and the other value being likewise defined but with the roles of the two sets swapped.
Graph theory.
In graph theory the distance between two vertices is the length of the shortest path between those vertices.
Distance versus directed distance and displacement.
Distance cannot be negative, and distance travelled never decreases. Distance is a scalar quantity or a magnitude, whereas displacement is a vector quantity with both magnitude and direction. Directed distance is a positive, zero, or negative scalar quantity.
The distance covered by a vehicle (for example as recorded by an odometer), person, animal, or object along a curved path from a point "A" to a point "B" should be distinguished from the straight-line distance from "A" to "B". For example, whatever the distance covered during a round trip from "A" to "B" and back to "A", the displacement is zero as start and end points coincide. In general the straight-line distance does not equal distance travelled, except for journeys in a straight line.
Directed distance.
Directed distances are distances with a directional sense. They can be determined along straight lines and along curved lines. A directed distance of a point "C" from point "A" in the direction of "B" on a line "AB" in a Euclidean vector space is the distance from "A" to "C" if "C" falls on the ray "AB", but is the negative of that distance if "C" falls on the ray "BA" (I.e., if "C" is not on the same side of "A" as "B" is).
A directed distance along a curved line is not a vector and is represented by a segment of that curved line defined by endpoints "A" and "B", with some specific information indicating the sense (or direction) of an ideal or real motion from one endpoint of the segment to the other (see figure). For instance, just labelling the two endpoints as "A" and "B" can indicate the sense, if the ordered sequence ("A", "B") is assumed, which implies that "A" is the starting point.
Displacement.
A displacement (see above) is a special kind of directed distance defined in mechanics. A directed distance is called displacement when it is the distance along a straight line (minimum distance) from "A" and "B", and when "A" and "B" are positions occupied by the "same particle" at two "different instants" of time. This implies motion of the particle. The distance traveled by a particle must always be greater than or equal to its displacement, with equality occurring only when the particle moves along a straight path.
Another kind of directed distance is that between two different particles or point masses at a given time. For instance, the distance from the center of gravity of the Earth "A" and the center of gravity of the Moon "B" (which does not strictly imply motion from "A" to "B") falls into this category.
Other "distances".
Circular distance is the distance traveled by a wheel. The circumference of the wheel is 2"π" × radius, and assuming the radius to be 1, then each revolution of the wheel is equivalent of the distance 2"π" radians. In engineering "ω" = 2"πƒ" is often used, where "ƒ" is the frequency.

</doc>
<doc id="831350" url="https://en.wikipedia.org/wiki?curid=831350" title="Distance matrix">
Distance matrix

In mathematics, computer science and especially graph theory, a distance matrix is a matrix (two-dimensional array) containing the distances, taken pairwise, between the elements of a set. Depending upon the application involved, the "distance" being used to define this matrix may or may not be a metric. If there are elements, this matrix will have size . In graph-theoretic applications the elements are more often referred to as points, nodes or vertices.
Metric distance.
When distance is defined as a metric, as for example in the Euclidean distance matrix, the distance matrix satisfies properties directly related to the defining properties of a metric. That is, if with is a distance matrix for a metric distance, then
Another common example of a distance matrix arises in coding theory when in a block code the elements are strings of fixed length over an alphabet and the distance between them is given by the Hamming distance metric. The smallest non-zero entry in the distance matrix measures the error correcting and error detecting capability of the code.
Non-metric distance.
In a network, a directed graph with weights assigned to the arcs, the distance between two nodes of the network can be defined as the minimum of the sums of the weights on the shortest paths joining the two nodes. This distance function, while well defined, is not a metric. There need be no restrictions on the weights other than the need to be able to combine and compare them, so negative weights are used in some applications. Since paths are directed, symmetry can not be guaranteed, and if cycles exist the distance matrix may not be hollow.
An algebraic formulation of the above can be obtained by using the min-plus algebra. Matrix multiplication in this system is defined as follows: Given two formula_1 matrices formula_2 and formula_3, their distance product formula_4 is defined as an formula_1 matrix such that formula_6.
If formula_7 is an formula_1 matrix containing the edge weights of a graph, then formula_9 (using this distance product) gives the distances between vertices using paths of length at most formula_10 edges, and formula_11 is the distance matrix of the graph. 
An arbitrary graph on vertices can be modeled as a weighted complete graph on vertices by assigning a weight of one to each edge of the complete graph that corresponds to an edge of and zero to all other edges. for this complete graph is the adjacency matrix of . The distance matrix of can be computed from as above, however, calculated by the usual matrix multiplication only encodes the number of paths between any two vertices of length at most . 
Applications.
Hierarchical Clustering.
A distance matrix is necessary for hierarchical clustering.
Phylogenetic Analysis.
Distance matrices are used in phylogenetic analysis.
Other uses.
In bioinformatics, distance matrices are used to represent protein structures in a coordinate-independent manner, as well as the pairwise distances between two sequences in sequence space. They are used in structural and sequential alignment, and for the determination of protein structures from NMR or X-ray crystallography.
Sometimes it is more convenient to express data as a similarity matrix.
Examples.
For example, suppose these data are to be analyzed, where pixel euclidean distance is the distance metric.
The distance matrix would be:
These data can then be viewed in graphic form as a heat map. In this image, black denotes a distance of 0 and white is maximal distance.

</doc>
<doc id="103118" url="https://en.wikipedia.org/wiki?curid=103118" title="Distributive property">
Distributive property

In abstract algebra and formal logic, the distributive property of binary operations generalizes the distributive law from elementary algebra. In propositional logic, distribution refers to two valid rules of replacement. The rules allow one to reformulate conjunctions and disjunctions within logical proofs.
For example, in arithmetic:
In the left-hand side of the first equation, the 2 multiplies the sum of 1 and 3; on the right-hand side, it multiplies the 1 and the 3 individually, with the products added afterwards.
Because these give the same final answer (8), it is said that multiplication by 2 "distributes" over addition of 1 and 3.
Since one could have put any real numbers in place of 2, 1, and 3 above, and still have obtained a true equation, we say that multiplication of real numbers "distributes" over addition of real numbers.
Definition.
Given a set and two binary operators ∗ and + on , we say that the operation: 
∗ is "left-distributive" over + if, given any elements , and of ,
∗ is "right-distributive" over + if, given any elements , and of ,
∗ is "distributive" over + if it is left- and right-distributive.
Notice that when ∗ is commutative, the three conditions above are logically equivalent.
Meaning.
The operators used for examples in this section are the binary operations of addition (formula_3) and multiplication (formula_4) of numbers.
There is a distinction between left-distributivity and right-distributivity:
In either case, the distributive property can be described in words as:
To multiply a sum (or difference) by a factor, each summand (or minuend and subtrahend) is multiplied by this factor and the resulting products are added (or subtracted).
If the operation outside the parentheses (in this case, the multiplication) is commutative, then left-distributivity implies right-distributivity and vice versa.
One example of an operation that is "only" right-distributive is division, which is not commutative:
In this case, left-distributivity does not apply:
The distributive laws are among the axioms for rings and fields. Examples of structures in which two operations are mutually related to each other by the distributive law are Boolean algebras such as the algebra of sets or the switching algebra. There are also combinations of operations that are not mutually distributive over each other; for example, addition is not distributive over multiplication.
Multiplying sums can be put into words as follows: When a sum is multiplied by a sum, multiply each summand of a sum with each summand of the other sums (keeping track of signs), and then adding up all of the resulting products.
Examples.
Real numbers.
In the following examples, the use of the distributive law on the set of real numbers formula_9 is illustrated. When multiplication is mentioned in elementary mathematics, it usually refers to this kind of multiplication. From the point of view of algebra, the real numbers form a field, which ensures the validity of the distributive law.
During mental arithmetic, distributivity is often used unconsciously:
Thus, to calculate in your head, you first multiply and and add the intermediate results. Written multiplication is also based on the distributive law.
Matrices.
The distributive law is valid for matrix multiplication. More precisely,
for all formula_17-matrices formula_18 and formula_19-matrices formula_20, as well as
for all formula_17-matrices formula_23 and formula_19-matrices formula_25. Because the commutative property does not hold for matrix multiplication, the second law does not follow from the first law. In this case, they are two different laws.
Propositional logic.
Rule of replacement.
In standard truth-functional propositional logic, "distribution" in logical proofs uses two valid rules of replacement to expand individual occurrences of certain logical connectives, within some formula, into separate applications of those connectives across subformulas of the given formula. The rules are:
and 
where "formula_28", also written ≡, is a metalogical symbol representing "can be replaced in a proof with" or "is logically equivalent to".
Truth functional connectives.
"Distributivity" is a property of some logical connectives of truth-functional propositional logic. The following logical equivalences demonstrate that distributivity is a property of particular connectives. The following are truth-functional tautologies.
Distributivity and rounding.
In practice, the distributive property of multiplication (and division) over addition may appear to be compromised or lost because of the limitations of arithmetic precision. For example, the identity appears to fail if the addition is conducted in decimal arithmetic; however, if many significant digits are used, the calculation will result in a closer approximation to the correct results. For example, if the arithmetical calculation takes the form: , this result is a closer approximation than if fewer significant digits had been used. Even when fractional numbers can be represented exactly in arithmetical form, errors will be introduced if those arithmetical values are rounded or truncated. For example, buying two books, each priced at £14.99 before a tax of 17.5%, in two separate transactions will actually save £0.01, over buying them together: to the nearest £0.01, giving a total expenditure of £35.22, but . Methods such as banker's rounding may help in some cases, as may increasing the precision used, but ultimately some calculation errors are inevitable.
Distributivity in rings.
Distributivity is most commonly found in rings and distributive lattices.
A ring has two binary operations (commonly called "+" and "∗"), and one of the requirements of a ring is that ∗ must distribute over +.
Most kinds of numbers (example 1) and matrices (example 4) form rings.
A lattice is another kind of algebraic structure with two binary operations, ∧ and ∨.
If either of these operations (say ∧) distributes over the other (∨), then ∨ must also distribute over ∧, and the lattice is called distributive. See also the article on distributivity (order theory).
Examples 4 and 5 are Boolean algebras, which can be interpreted either as a special kind of ring (a Boolean ring) or a special kind of distributive lattice (a Boolean lattice). Each interpretation is responsible for different distributive laws in the Boolean algebra. Examples 6 and 7 are distributive lattices which are not Boolean algebras.
Failure of one of the two distributive laws brings about near-rings and near-fields instead of rings and division rings respectively. The operations are usually configured to have the near-ring or near-field distributive on the right but not on the left.
Rings and distributive lattices are both special kinds of rigs, certain generalizations of rings.
Those numbers in example 1 that don't form rings at least form rigs.
Near-rigs are a further generalization of rigs that are left-distributive but not right-distributive; example 2 is a near-rig.
Generalizations of distributivity.
In several mathematical areas, generalized distributivity laws are considered. This may involve the weakening of the above conditions or the extension to infinitary operations. Especially in order theory one finds numerous important variants of distributivity, some of which include infinitary operations, such as the infinite distributive law; others being defined in the presence of only "one" binary operation, such as the according definitions and their relations are given in the article distributivity (order theory). This also includes the notion of a completely distributive lattice.
In the presence of an ordering relation, one can also weaken the above equalities by replacing = by either ≤ or ≥. Naturally, this will lead to meaningful concepts only in some situations. An application of this principle is the notion of sub-distributivity as explained in the article on interval arithmetic.
In category theory, if and are monads on a category "C", a distributive law is a natural transformation such that is a lax map of monads and is a colax map of monads . This is exactly the data needed to define a monad structure on : the multiplication map is and the unit map is "η"′"S"."η". See: distributive law between monads.
A generalized distributive law has also been proposed in the area of information theory.
Notions of antidistributivity.
The ubiquitous identity that relates inverses to the binary operation in any group, namely , which is taken as an axiom in the more general context of a semigroup with involution, has sometimes been called an antidistributive property (of inversion as a unary operation).
In the context of a near-ring, which removes the commutativity of the additively written group and assumes only one-sided distributivity, one can speak of (two-sided) distributive elements but also of antidistributive elements. The latter reverse the order of (the non-commutative) addition; assuming a left-nearring (i.e. one which all elements distribute when multiplied on the left), then an antidistributive element "a" reverses the order of addition when multiplied to the right: .
In the study of propositional logic and Boolean algebra, the term antidistributive law is sometimes used to denote the interchange between conjunction and disjunction when implication factors over them:
These two tautologies are a direct consequence of the duality in De Morgan's laws.

</doc>
<doc id="8328" url="https://en.wikipedia.org/wiki?curid=8328" title="Divergence">
Divergence

In vector calculus, divergence is a vector operator that measures the magnitude of a vector field's source or sink at a given point, in terms of a signed scalar. More technically, the divergence represents the volume density of the outward flux of a vector field from an infinitesimal volume around a given point.
For example:
consider air as it is heated or cooled. The relevant vector field for this example is the velocity of the moving air at a point. If air is heated in a region it will expand in all directions such that the velocity field points outward from that region. Therefore the divergence of the velocity field in that region would have a positive value, as the region is a source. If the air cools and contracts, the divergence has a negative value, as the region is a sink.
Definition of divergence.
In physical terms, the divergence of a three-dimensional vector field is the extent to which the vector field flow behaves like a source or a sink at a given point. It is a local measure of its "outgoingness"—the extent to which there is more exiting an infinitesimal region of space than entering it. If the divergence is nonzero at some point then there must be a source or sink at that position. (Note that we are imagining the vector field to be like the velocity vector field of a fluid (in motion) when we use the terms flow, sink and so on.)
More rigorously, the divergence of a vector field F at a point "p" is defined as the limit of the net flow of F across the smooth boundary of a three-dimensional region "V" divided by the volume of "V" as "V" shrinks to "p". Formally,
where |"V" | is the volume of "V", "S"("V") is the boundary of "V", and the integral is a surface integral with n being the outward unit normal to that surface. The result, div F, is a function of "p". From this definition it also becomes explicitly visible that div F can be seen as the "source density" of the flux of F.
In light of the physical interpretation, a vector field with constant zero divergence is called "incompressible" or "solenoidal" – in this case, no net flow can occur across any closed surface.
The intuition that the sum of all sources minus the sum of all sinks should give the net flow outwards of a region is made precise by the divergence theorem.
Application in Cartesian coordinates.
Let "x, y, z" be a system of Cartesian coordinates in 3-dimensional Euclidean space, and let i, j, k be the corresponding basis of unit vectors.
The divergence of a continuously differentiable vector field F = "U" i + "V" j + "W" k is equal to the scalar-valued function:
Although expressed in terms of coordinates, the result is invariant under orthogonal transformations, as the physical interpretation suggests.
The common notation for the divergence is a convenient mnemonic, where the dot denotes an operation reminiscent of the dot product: take the components of ∇ (see del), apply them to the components of F, and sum the results. Because applying an operator is different from multiplying the components, this is considered an abuse of notation.
The divergence of a continuously differentiable second-order tensor field formula_3 is a first-order tensor field:
Cylindrical coordinates.
For a vector expressed in cylindrical coordinates as
where ea is the unit vector in direction a, the divergence is
Spherical coordinates.
In spherical coordinates, with formula_7 the angle with the "z" axis and formula_8 the rotation around the "z" axis, the divergence reads
Decomposition theorem.
It can be shown that any stationary flux v(r) which is at least two times continuously differentiable in formula_10 and vanishes sufficiently fast for can be decomposed into an "irrotational part" E(r) and a "source-free part" B(r). Moreover, these parts are explicitly determined by the respective "source-densities" (see above) and "circulation densities" (see the article Curl):
For the irrotational part one has
with
The source-free part, B, can be similarly written: one only has to replace the "scalar potential" Φ(r) by a "vector potential" A(r) and the terms −∇Φ by +∇×A, and the source-density 
by the circulation-density ∇×v.
This "decomposition theorem" is in fact a by-product of the stationary case of electrodynamics. It is a special case of the more general Helmholtz decomposition which works in dimensions greater than three as well.
Properties.
The following properties can all be derived from the ordinary differentiation rules of calculus. Most importantly, the divergence is a linear operator, i.e.
for all vector fields F and G and all real numbers "a" and "b".
There is a product rule of the following type: if formula_14 is a scalar valued function and F is a vector field, then
or in more suggestive notation
Another product rule for the cross product of two vector fields F and G in three dimensions involves the curl and reads as follows:
or
The Laplacian of a scalar field is the divergence of the field's gradient:
The divergence of the curl of any vector field (in three dimensions) is equal to zero: 
If a vector field F with zero divergence is defined on a ball in R3, then there exists some vector field G on the ball with F = curl(G). For regions in R3 more complicated than this, the latter statement might be false (see Poincaré lemma). The degree of "failure" of the truth of the statement, measured by the homology of the chain complex
(where the first map is the gradient, the second is the curl, the third is the divergence) serves as a nice quantification of the complicatedness of the underlying region "U". These are the beginnings and main motivations of de Rham cohomology.
Relation with the exterior derivative.
One can express the divergence as a particular case of the exterior derivative, which takes a 2-form to a 3-form in R3.
Define the current two form 
It measures the amount of "stuff" flowing through a surface per unit time in a "stuff fluid" of density formula_26 moving with local velocity F. Its exterior derivative formula_27 is then given by
Thus, the divergence of the vector field F can be expressed as:
Here the superscript formula_30 is one of the two musical isomorphisms, and formula_31 is the Hodge dual. Note however that working with the current two form itself and the exterior derivative is usually easier than working with the vector field and divergence, because unlike the divergence, the exterior derivative commutes with a change of (curvilinear) coordinate system.
Generalizations.
The divergence of a vector field can be defined in any number of dimensions. If 
in a Euclidean coordinate system where formula_33 and formula_34, define
The appropriate expression is more complicated in curvilinear coordinates.
In the case of one dimension, a "vector field" is simply a regular function, and the divergence is simply the derivative.
For any "n", the divergence is a linear operator, and it satisfies the "product rule"
for any scalar-valued function formula_37.
The divergence can be defined on any manifold of dimension "n" with a volume form (or density) formula_38 e.g. a Riemannian or Lorentzian manifold. Generalising the construction of a two form for a vector field on formula_39, on such a manifold a vector field "X" defines a "n"−1 form formula_40 obtained by contracting "X" with formula_38. The divergence is then the function defined by
Standard formulas for the Lie derivative allow us to reformulate this as
This means that the divergence measures the rate of expansion of a volume element as we let it 
flow with the vector field.
On a Riemannian or Lorentzian manifold the divergence with respect to the metric volume form
can be computed in terms of the Levi Civita connection formula_44
where the second expression is the contraction of the vector field valued 1-form formula_46 with itself and the last expression is the traditional coordinate expression used by physicists.
An equivalent expression without using connection is
where formula_48 is the metric and formula_49 denotes partial derivative with respect to coordinate formula_50.
Divergence can also be generalised to tensors. In Einstein notation, the divergence of a contravariant vector formula_51 is given by
where formula_53 is the covariant derivative.
Equivalently, some authors define the divergence of any mixed tensor by using the "musical notation #":
If "T" is a ("p","q")-tensor ("p" for the contravariant vector and "q" for the covariant one), then we define the "divergence of T" to be the ("p","q"−1)-tensor
formula_54
that is we trace the covariant derivative on the "first two" covariant indices.

</doc>
<doc id="8724" url="https://en.wikipedia.org/wiki?curid=8724" title="Doppler effect">
Doppler effect

The Doppler effect (or Doppler shift) is the change in frequency of a wave (or other periodic event) for an observer moving relative to its source. It is named after the Austrian physicist Christian Doppler, who proposed it in 1842 in Prague. It is commonly heard when a vehicle sounding a siren or horn approaches, passes, and recedes from an observer. Compared to the emitted frequency, the received frequency is higher during the approach, identical at the instant of passing by, and lower during the recession.
When the source of the waves is moving toward the observer, each successive wave crest is emitted from a position closer to the observer than the previous wave. Therefore, each wave takes slightly less time to reach the observer than the previous wave. Hence, the time between the arrival of successive wave crests at the observer is reduced, causing an increase in the frequency. While they are travelling, the distance between successive wave fronts is reduced, so the waves "bunch together". Conversely, if the source of waves is moving away from the observer, each wave is emitted from a position farther from the observer than the previous wave, so the arrival time between successive waves is increased, reducing the frequency. The distance between successive wave fronts is then increased, so the waves "spread out".
For waves that propagate in a medium, such as sound waves, the velocity of the observer and of the source are relative to the medium in which the waves are transmitted. The total Doppler effect may therefore result from motion of the source, motion of the observer, or motion of the medium. Each of these effects is analyzed separately. For waves which do not require a medium, such as light or gravity in general relativity, only the relative difference in velocity between the observer and the source needs to be considered.
Developments.
Doppler first proposed this effect in 1842 in his treatise "Über das farbige Licht der Doppelsterne und einiger anderer Gestirne des Himmels" (On the coloured light of the binary stars and some other stars of the heavens). The hypothesis was tested for sound waves by Buys Ballot in 1845. He confirmed that the sound's pitch was higher than the emitted frequency when the sound source approached him, and lower than the emitted frequency when the sound source receded from him. Hippolyte Fizeau discovered independently the same phenomenon on electromagnetic waves in 1848 (in France, the effect is sometimes called "effet Doppler-Fizeau" but that name was not adopted by the rest of the world as Fizeau's discovery was six years after Doppler's proposal). In Britain, John Scott Russell made an experimental study of the Doppler effect (1848).
General.
In classical physics, where the speeds of source and the receiver relative to the medium are lower than the velocity of waves in the medium, the relationship between observed frequency formula_1 and emitted frequency formula_2 is given by:
The frequency is decreased if either is moving away from the other.
The above formula assumes that the source is either directly approaching or receding from the observer. If the source approaches the observer at an angle (but still with a constant velocity), the observed frequency that is first heard is higher than the object's emitted frequency. Thereafter, there is a monotonic decrease in the observed frequency as it gets closer to the observer, through equality when it is coming from a direction perpendicular to the relative motion (and was emitted at the point of closest approach; but when the wave is received, the source and observer will no longer be at their closest), and a continued monotonic decrease as it recedes from the observer. When the observer is very close to the path of the object, the transition from high to low frequency is very abrupt. When the observer is far from the path of the object, the transition from high to low frequency is gradual.
If the speeds formula_6 and formula_5 are small compared to the speed of the wave, the relationship between observed frequency formula_1 and emitted frequency formula_2 is approximately
Given formula_3
we divide for formula_14
formula_15
Since formula_16 we can substitute the geometric expansion:
formula_17
Analysis.
To understand what happens, consider the following analogy. Someone throws one ball every second at a man. Assume that balls travel with constant velocity. If the thrower is stationary, the man will receive one ball every second. However, if the thrower is moving towards the man, he will receive balls more frequently because the balls will be less spaced out. The inverse is true if the thrower is moving away from the man. So it is actually the "wavelength" which is affected; as a consequence, the received frequency is also affected. It may also be said that the velocity of the wave remains constant whereas wavelength changes; hence frequency also changes.
With an observer stationary relative to the medium, if a moving source is emitting waves with an actual frequency formula_2 (in this case, the wavelength is changed, the transmission velocity of the wave keeps constant formula_19 note that the "transmission velocity" of the wave does not depend on the "velocity of the source"), then the observer detects waves with a frequency formula_1 given by
A similar analysis for a moving "observer" and a stationary source (in this case, the wavelength keeps constant, but due to the motion, the rate at which the observer receives waves formula_19 and hence the "transmission velocity" of the wave [with respect to the observer] formula_19 is changed) yields the observed frequency:
These can be generalized into the equation that was presented in the previous section.
An interesting effect was predicted by Lord Rayleigh in his classic book on sound: if the source is moving at twice the speed of sound, a musical piece emitted by that source would be heard in correct time and tune, but "backwards". The Doppler effect with sound is only clearly heard with objects moving at high speed, as change in frequency of musical tone involves a speed of around 40 meters per second, and smaller changes in frequency can easily be confused by changes in the amplitude of the sounds from moving emitters. Neil A Downie has demonstrated how the Doppler effect can be made much more easily audible by using an ultrasonic (eg. 40 kHz) emitter on the moving object. The observer then uses a heterodyne frequency converter, as used in many bat detectors, to listen to a band around 40 kHz. In this case, with the bat detector tuned to give frequency for the stationary emitter of 2000 Hz, the observer will perceive a frequency shift of a whole tone, 240 Hz, if the emitter travels at 2 meters per second.
Application.
Sirens.
The siren on a passing emergency vehicle will start out higher than its stationary pitch, slide down as it passes, and continue lower than its stationary pitch as it recedes from the observer. Astronomer John Dobson explained the effect thus:
In other words, if the siren approached the observer directly, the pitch would remain constant until the vehicle hit him, and then immediately jump to a new lower pitch. Because the vehicle passes by the observer, the radial velocity does not remain constant, but instead varies as a function of the angle between his line of sight and the siren's velocity:
where formula_27 is the angle between the object's forward velocity and the line of sight from the object to the observer.
Astronomy.
The Doppler effect for electromagnetic waves such as light is of great use in astronomy and results in either a so-called redshift or blueshift. It has been used to measure the speed at which stars and galaxies are approaching or receding from us; that is, their radial velocities. This may be used to detect if an apparently single star is, in reality, a close binary, to measure the rotational speed of stars and galaxies, or to detect exoplanets. (Note that redshift is also used to measure the expansion of space, but that this is not truly a Doppler effect.)
The use of the Doppler effect for light in astronomy depends on our knowledge that the spectra of stars are not homogeneous. They exhibit absorption lines at well defined frequencies that are correlated with the energies required to excite electrons in various elements from one level to another. The Doppler effect is recognizable in the fact that the absorption lines are not always at the frequencies that are obtained from the spectrum of a stationary light source. Since blue light has a higher frequency than red light, the spectral lines of an approaching astronomical light source exhibit a blueshift and those of a receding astronomical light source exhibit a redshift.
Among the nearby stars, the largest radial velocities with respect to the Sun are +308 km/s (BD-15°4041, also known as LHS 52, 81.7 light-years away) and -260 km/s (Woolley 9722, also known as Wolf 1106 and LHS 64, 78.2 light-years away). Positive radial velocity means the star is receding from the Sun, negative that it is approaching.
Radar.
The Doppler effect is used in some types of radar, to measure the velocity of detected objects. A radar beam is fired at a moving target — e.g. a motor car, as police use radar to detect speeding motorists — as it approaches or recedes from the radar source. Each successive radar wave has to travel farther to reach the car, before being reflected and re-detected near the source. As each wave has to move farther, the gap between each wave increases, increasing the wavelength. In some situations, the radar beam is fired at the moving car as it approaches, in which case each successive wave travels a lesser distance, decreasing the wavelength. In either situation, calculations from the Doppler effect accurately determine the car's velocity. Moreover, the proximity fuze, developed during World War II, relies upon Doppler radar to detonate explosives at the correct time, height, distance, etc.
Because the doppler shift affects the wave incident upon the target as well as the wave reflected back to the radar, the change in frequency observed by a radar due to a target moving at relative velocity formula_28 is twice that from the same target emitting a wave:
Medical imaging and blood flow measurement.
An echocardiogram can, within certain limits, produce an accurate assessment of the direction of blood flow and the velocity of blood and cardiac tissue at any arbitrary point using the Doppler effect. One of the limitations is that the ultrasound beam should be as parallel to the blood flow as possible. Velocity measurements allow assessment of cardiac valve areas and function, any abnormal communications between the left and right side of the heart, any leaking of blood through the valves (valvular regurgitation), and calculation of the cardiac output. Contrast-enhanced ultrasound using gas-filled microbubble contrast media can be used to improve velocity or other flow-related medical measurements.
Although "Doppler" has become synonymous with "velocity measurement" in medical imaging, in many cases it is not the frequency shift (Doppler shift) of the received signal that is measured, but the phase shift ("when" the received signal arrives).
Velocity measurements of blood flow are also used in other fields of medical ultrasonography, such as obstetric ultrasonography and neurology. Velocity measurement of blood flow in arteries and veins based on Doppler effect is an effective tool for diagnosis of vascular problems like stenosis.
Flow measurement.
Instruments such as the laser Doppler velocimeter (LDV), and acoustic Doppler velocimeter (ADV) have been developed to measure velocities in a fluid flow. The LDV emits a light beam and the ADV emits an ultrasonic acoustic burst, and measure the Doppler shift in wavelengths of reflections from particles moving with the flow. The actual flow is computed as a function of the water velocity and phase. This technique allows non-intrusive flow measurements, at high precision and high frequency.
Velocity profile measurement.
Developed originally for velocity measurements in medical applications (blood flow), Ultrasonic Doppler Velocimetry (UDV) can measure in real time complete velocity profile in almost any liquids containing particles in suspension such as dust, gas bubbles, emulsions. Flows can be pulsating, oscillating, laminar or turbulent, stationary or transient. This technique is fully non-invasive.
Satellite communication.
Fast moving satellites can have a Doppler shift of dozens of kilohertz relative to a ground station. The speed, thus magnitude of Doppler effect, changes due to earth curvature. Dynamic Doppler compensation, where the frequency of a signal is changed multiple times during transmission, is used so the satellite receives a constant frequency signal.
Audio.
The Leslie speaker, associated with and predominantly used with the Hammond B-3 organ, takes advantage of the Doppler effect by using an electric motor to rotate an acoustic horn around a loudspeaker, sending its sound in a circle. This results at the listener's ear in rapidly fluctuating frequencies of a keyboard note.
Vibration measurement.
A laser Doppler vibrometer (LDV) is a non-contact method for measuring vibration. The laser beam from the LDV is directed at the surface of interest, and the vibration amplitude and frequency are extracted from the Doppler shift of the laser beam frequency due to the motion of the surface.
Developmental biology.
During the segmentation of vertebrate embroys, waves of gene expression sweep across the presomitic mesoderm, the tissue from which the precursors of the vertebrae (somites) are formed. A new somite is formed upon arrival of a wave at the anterior end of the presomitic mesoderm. In zebrafish, it has been shown that the shortening of the presomitic mesoderm during segmentation leads to a Doppler effect as the anterior end of the tissue moves into the waves. This Doppler effect contributes to the period of segmentation.
Inverse Doppler effect.
Since 1968 scientists such as Victor Veselago have speculated about the possibility of an inverse Doppler effect. The experiment that claimed to have detected this effect was conducted by Nigel Seddon and Trevor Bearpark in Bristol, United Kingdom in 2003.
Researchers from Swinburne University of Technology and the University of Shanghai for Science and Technology showed that this effect can be observed in optical frequencies as well. This was made possible by growing a photonic crystal and projecting a laser beam into the crystal. This made the crystal act like a super prism and the inverse Doppler effect could be observed.

</doc>
<doc id="157093" url="https://en.wikipedia.org/wiki?curid=157093" title="Dot product">
Dot product

In mathematics, the dot product or scalar product (sometimes inner product in the context of Euclidean space, or rarely projection product for emphasizing the geometric significance), is an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number. This operation can be defined either algebraically or geometrically. Algebraically, it is the sum of the products of the corresponding entries of the two sequences of numbers. Geometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them. The name "dot product" is derived from the centered dot " · " that is often used to designate this operation; the alternative name "scalar product" emphasizes that the result is a scalar (rather than a vector).
In three-dimensional space, the dot product contrasts with the cross product of two vectors, which produces a pseudovector as the result. The dot product is directly related to the cosine of the angle between two vectors in Euclidean space of any number of dimensions.
Definition.
The dot product is often defined in one of two ways: algebraically or geometrically. The geometric definition is based on the notions of angle and distance (magnitude of vectors). The equivalence of these two definitions relies on having a Cartesian coordinate system for Euclidean space.
In modern presentations of Euclidean geometry, the points of space are defined in terms of their Cartesian coordinates, and Euclidean space itself is commonly identified with the real coordinate space R"n". In such a presentation, the notions of length and angles are not primitive. They are defined by means of the dot product: the length of a vector is defined as the square root of the dot product of the vector by itself, and the cosine of the (non oriented) angle of two vectors of length one is defined as their dot product. So the equivalence of the two definitions of the dot product is a part of the equivalence of the classical and the modern formulations of Euclidean geometry.
Algebraic definition.
The dot product of two vectors and is defined as:
where Σ denotes summation notation and "n" is the dimension of the vector space. For instance, in three-dimensional space, the dot product of vectors and is:
Geometric definition.
In Euclidean space, a Euclidean vector is a geometrical object that possesses both a magnitude and a direction. A vector can be pictured as an arrow. Its magnitude is its length, and its direction is the direction that the arrow points. The magnitude of a vector A is denoted by formula_3. The dot product of two Euclidean vectors A and B is defined by
where θ is the angle between A and B.
In particular, if A and B are orthogonal, then the angle between them is 90° and
At the other extreme, if they are codirectional, then the angle between them is 0° and
This implies that the dot product of a vector A by itself is
which gives
the formula for the Euclidean length of the vector.
Scalar projection and first properties.
The scalar projection (or scalar component) of a Euclidean vector A in the direction of a Euclidean vector B is given by
where θ is the angle between A and B.
In terms of the geometric definition of the dot product, this can be rewritten
where formula_11 is the unit vector in the direction of B.
The dot product is thus characterized geometrically by
The dot product, defined in this manner, is homogeneous under scaling in each variable, meaning that for any scalar "α",
It also satisfies a distributive law, meaning that
These properties may be summarized by saying that the dot product is a bilinear form. Moreover, this bilinear form is positive definite, which means that 
formula_15
is never negative and is zero if and only if formula_16
Equivalence of the definitions.
If e1, ..., e"n" are the standard basis vectors in R"n", then we may write
The vectors e"i" are an orthonormal basis, which means that they have unit length and are at right angles to each other. Hence since these vectors have unit length
and since they form right angles with each other, if ,
Thus in general we can say that:
Where δ ij is the Kronecker delta.
Also, by the geometric definition, for any vector e"i" and a vector A, we note
where "A""i" is the component of vector A in the direction of e"i".
Now applying the distributivity of the geometric version of the dot product gives
which is precisely the algebraic definition of the dot product. So the (geometric) dot product equals the (algebraic) dot product.
Properties.
The dot product fulfills the following properties if a, b, and c are real vectors and "r" is a scalar.
Application to the cosine law.
Given two vectors a and b separated by angle "θ" (see image right), they form a triangle with a third side . The dot product of this with itself is:
which is the law of cosines.
Triple product expansion.
This is an identity (also known as Lagrange's formula) involving the dot- and cross-products. It is written as:
which may be remembered as "BAC minus CAB", keeping in mind which vectors are dotted together. This formula finds application in simplifying vector calculations in physics.
Physics.
In physics, vector magnitude is a scalar in the physical sense, i.e. a physical quantity independent of the coordinate system, expressed as the product of a numerical value and a physical unit, not just a number. The dot product is also a scalar in this sense, given by the formula, independent of the coordinate system. Examples include:
Generalizations.
Complex vectors.
For vectors with complex entries, using the given definition of the dot product would lead to quite different properties. For instance the dot product of a vector with itself would be an arbitrary complex number, and could be zero without the vector being the zero vector (such vectors are called isotropic); this in turn would have consequences for notions like length and angle. Properties such as the positive-definite norm can be salvaged at the cost of giving up the symmetric and bilinear properties of the scalar product, through the alternative definition
where "bi" is the complex conjugate of "bi". Then the scalar product of any vector with itself is a non-negative real number, and it is nonzero except for the zero vector. However this scalar product is thus sesquilinear rather than bilinear: it is conjugate linear and not linear in b, and the scalar product is not symmetric, since
The angle between two complex vectors is then given by
This type of scalar product is nevertheless useful, and leads to the notions of Hermitian form and of general inner product spaces.
Inner product.
The inner product generalizes the dot product to abstract vector spaces over a field of scalars, being either the field of real numbers formula_33 or the field of complex numbers formula_34. It is usually denoted by formula_35.
The inner product of two vectors over the field of complex numbers is, in general, a complex number, and is sesquilinear instead of bilinear. An inner product space is a normed vector space, and the inner product of a vector with itself is real and positive-definite.
Functions.
The dot product is defined for vectors that have a finite number of entries. Thus these vectors can be regarded as discrete functions: a length- vector is, then, a function with domain }, and is a notation for the image of by the function/vector .
This notion can be generalized to continuous functions: just as the inner product on vectors uses a sum over corresponding components, the inner product on functions is defined as an integral over some interval (also denoted ):
Generalized further to complex functions and , by analogy with the complex inner product above, gives
Weight function.
Inner products can have a weight function, i.e. a function which weight each term of the inner product with a value.
Dyadics and matrices.
Matrices have the Frobenius inner product, which is analogous to the vector inner product. It is defined as the sum of the products of the corresponding components of two matrices A and B having the same size:
Dyadics have a dot product and "double" dot product defined on them, see Dyadics (Product of dyadic and dyadic) for their definitions.
Tensors.
The inner product between a tensor of order "n" and a tensor of order "m" is a tensor of order , see tensor contraction for details.

</doc>
<doc id="1991441" url="https://en.wikipedia.org/wiki?curid=1991441" title="Double beta decay">
Double beta decay

Double beta decay is a radioactive decay process where a nucleus releases two beta rays as a single process.
History.
The idea of double beta decay was first proposed by Maria Goeppert-Mayer in 1935. In 1937 Ettore Majorana theoretically demonstrated that all results of beta decay theory remain unchanged if the neutrino is its own anti-particle, i.e. a Majorana particle. In 1939 Wendell H. Furry proposed if neutrinos are a Majorana particle, double beta decay can proceed without emission of any neutrino, via the process now called the neutrinoless beta decay.
In 1930–40s parity violation in weak interactions was not known and consequently calculations showed that neutrinoless double beta decay should be much more likely to occur than ordinary double beta decay (if neutrinos are Majorana particles). The predicted half-lives were on the order of 1015–16 years. Efforts to observe the process in laboratory date back to at least 1948 when Edward L. Fireman made the first attempt to measure the half-life of the isotope. Radiometric experiments through about 1960 produced negative results or false positives, not confirmed by later experiments. In 1950 for the first time the half-life of the was measured by geochemical methods, to be 1.4 years, reasonably close to the modern value.
In 1956 after the V-A nature of weak interactions was established it became clear the half-life of neutrinoless double beta decay would significantly exceed that of ordinary double beta decay. Despite significant progress in experimental techniques in 1960–70s, double beta decay was not observed in a laboratory until the 1980s. Experiments had only been able to establish the lower bound for the half-life—about 1021 years. On the other hand, geochemical experiments detected double beta decay of and .
Double beta decay was first observed in a laboratory in 1987 by the group of Michael Moe at UC Irvine on . Since then many experiments have observed ordinary double beta decay in other isotopes. None of those experiments has produced positive results for the neutrinoless process, raising the half-life lower bound to 1025 years. Geochemical experiments continued through the 1990s, producing positive results for a few more isotopes. Double beta decay is the rarest known kind of radioactive decay; as of 2012 it has been observed for only 12 isotopes (including double electron capture in observed in 2001), and all have a mean lifetime over 1018 yr (table below).
Ordinary double beta decay.
In double beta decay, two neutrons in the nucleus are converted to protons, and two electrons and two electron antineutrinos are emitted. The process can be thought as a sum of two beta minus decays. In order for (double) beta decay to be possible, the final nucleus must have a larger binding energy than the original nucleus. For some nuclei, such as germanium-76, the nucleus one atomic number higher has a smaller binding energy, preventing single beta decay. However, the nucleus with atomic number two higher, selenium-76, has a larger binding energy, so double beta decay is allowed.
For some nuclei, the process occurs as conversion of two protons to neutrons, emitting two electron neutrinos and absorbing two orbital electrons (double electron capture). If the mass difference between the parent and daughter atoms is more than 1.022 MeV/c2 (two electron masses), another decay is accessible, capture of one orbital electron and emission of one positron. When the mass difference is more than 2.044 MeV/c2 (four electron masses), emission of two positrons is possible. These theoretical decay branches have not been observed.
Known double beta decay isotopes.
There are 35 naturally occurring isotopes capable of double beta decay. The decay can be observed in practice if the single beta decay is forbidden by energy conservation. This happens for even-Z, even-N isotopes, which are more stable due to spin-coupling, seen by the pairing term in the semi-empirical mass formula.
Many isotopes are theoretically expected to double beta decay. In most cases, the double beta decay is so rare that it is nearly impossible to observe against the background. However, the double beta decay of (also an alpha emitter) has been measured radiochemically. Two of the nuclides ( and ) from the table below can also theoretically single beta decay but this is extremely suppressed and never been observed.
Eleven isotopes have been experimentally observed undergoing two-neutrino double beta decay. The table below contains nuclides with the latest experimentally measured half-lives, as of December 2012.
Neutrinoless double beta decay.
The processes described in the previous section are also known as two-neutrino double beta decay, as two neutrinos (or antineutrinos) are emitted. If the neutrino is a Majorana particle (meaning that the antineutrino and the neutrino are actually the same particle), and at least one type of neutrino has non-zero mass (which has been established by the neutrino oscillation experiments), then it is possible for neutrinoless double beta decay to occur. In the simplest theoretical treatment, light neutrino exchange, the two neutrinos annihilate each other, or equivalently, a nucleon absorbs the neutrino emitted by another nucleon.
The neutrinos in the above diagram are virtual particles. With only two electrons in the final state, the electrons total kinetic energy would be approximately the binding energy difference of the initial and final nuclei (with the nucleus recoil accounting for the rest). To a very good approximation, the electrons are emitted back-to-back.
The decay rate for this process is approximately given by
where formula_2 is the two-body phase-space factor, formula_3 is the nuclear matrix element, and mββ is the effective Majorana neutrino mass given by
In this expression, mi is the neutrino masses (of the ith mass eigenstate), and the Uei are elements of the lepton mixing Pontecorvo–Maki–Nakagawa–Sakata (PMNS) matrix. Therefore, observing neutrinoless double beta decay, in addition to confirming the Majorana neutrino nature, would give information on the absolute neutrino mass scale, potentially the neutrino mass hierarchy, and Majorana phases in the PMNS matrix.
The deep significance of the process stems from the "black-box theorem" which that observing neutrinoless double beta decay implies at least one neutrino is a Majorana particle, irrespective of whether the process is engendered by neutrino exchange.
Experiments.
Numerous experiments have searched for neutrinoless double beta decay. Recent and proposed experiments include:
Status.
Early experiments did claim neutrinoless decay, but modern searches have set limits disfavoring those results. Recent published lower bounds for germanium and xenon indicate no sign of neutrinoless decay.
Heidelberg-Moscow Controversy.
Heidelberg-Moscow collaboration initially released limits on neutrinoless beta decay in germanium-76. Then some members claimed detection in 2001. This claim was criticized by outside physicists as well as other members of the collaboration. In 2006 a refined estimate by the same authors stated the half-life was 2.3 years. More sensitive experiments are expected to resolve the controversy.
Current Results.
As of 2014, GERDA has reached much lower background, obtaining a half-life limit of 2.1 years with 21.6 kg*yr exposure. IGEX and HDM data increase the limit to 3 yr and rule out detection at high confidence.
Searches with 136Xe, Kamland-Zen and EXO-200, yielded a limit of 2.6 yr. Using the latest nuclear matrix elements, the 136Xe results also disfavor the HM claim.

</doc>
<doc id="2158646" url="https://en.wikipedia.org/wiki?curid=2158646" title="Double electron capture">
Double electron capture

Double electron capture is a decay mode of atomic nucleus. For a nuclide ("A", "Z") with number of nucleons "A" and atomic number "Z", double electron capture is only possible if the mass of the nuclide of ("A", "Z"−2) is lower.
In this mode of decay, two of the orbital electrons are captured by two protons in the nucleus, forming two neutrons. Two neutrinos are emitted in the process. Since the protons are changed to neutrons, the number of neutrons increases by 2, the number of protons "Z" decreases by 2, and the atomic mass number "A" remains unchanged. By changing the number of protons, double electron capture transforms the nuclide into a new element.
Example:
In most cases this decay mode is masked by more probable modes (single electron capture etc.), but when all these modes are forbidden or strongly suppressed, double electron capture becomes the main mode of decay. There exist 35 naturally occurring isotopes that can undergo double electron capture. There is, however, only one confirmed observation of this process (for barium-130). One reason is that the probability of double electron capture is enormously small (the theoretical predictions of half-lives for this mode lies well above 1020 years). A second reason is that the only detectable particles created in this process are X-rays and Auger electrons that are emitted by the excited atomic shell. In the range of their energies (~1–10 keV), the background is usually high. Thus, the experimental detection of double electron capture is more difficult than that for double beta decay.
Double electron capture can be accompanied by the excitation of the daughter nucleus. Its de-excitation, in turn, is accompanied by emission of photons with energies of hundreds of keV.
If the mass difference between the mother and daughter atoms is more than two masses of electron (1.022 MeV), the energy released in the process is enough to allow another mode of decay: electron capture with positron emission. It occurs simultaneously with double electron capture, their branching ratio depending on nuclear properties. When the mass difference is more than four electron masses (2.044 MeV), the third mode—double positron decay—is allowed. Only 6 naturally occurring nuclides can decay via these three modes simultaneously.
Neutrinoless double electron capture.
The above described process with capture of two electrons and emission of two neutrinos (two-neutrino double electron capture) is allowed by the Standard Model of particle physics: no conservation laws (including lepton number conservation) are violated. However, if the lepton number is not conserved, or the neutrino is its own antiparticle, another kind of the process can occur: the so-called neutrinoless double electron capture. In this case, two electrons are captured by nucleus, but neutrinos are not emitted. The energy released in this process is carried away by an internal bremsstrahlung gamma quantum. This mode of decay has never been observed experimentally, and would contradict the Standard Model if it were observed.
Example:

</doc>
<doc id="241027" url="https://en.wikipedia.org/wiki?curid=241027" title="Down quark">
Down quark

The down quark or d quark (symbol: d) is the second-lightest of all quarks, a type of elementary particle, and a major constituent of matter. Together with the up quark, it forms the neutrons (one up quark, two down quarks) and protons (two up quarks, one down quark) of atomic nuclei. It is part of the first generation of matter, has an electric charge of − "e" and a bare mass of . Like all quarks, the down quark is an elementary fermion with spin-, and experiences all four fundamental interactions: gravitation, electromagnetism, weak interactions, and strong interactions. The antiparticle of the down quark is the down antiquark (sometimes called "antidown quark" or simply "antidown"), which differs from it only in that some of its properties have equal magnitude but opposite sign.
Its existence (along with that of the up and strange quarks) was postulated in 1964 by Murray Gell-Mann and George Zweig to explain the "Eightfold Way" classification scheme of hadrons. The down quark was first observed by experiments at the Stanford Linear Accelerator Center in 1968.
History.
In the beginnings of particle physics (first half of the 20th century), hadrons such as protons, neutrons, and pions were thought to be elementary particles. However, as new hadrons were discovered, the 'particle zoo' grew from a few particles in the early 1930s and 1940s to several dozens of them in the 1950s. The relationships between each of them was unclear until 1961, when Murray Gell-Mann and Yuval Ne'eman (independently of each other) proposed a hadron classification scheme called the "Eightfold Way", or in more technical terms, SU(3) flavor symmetry.
This classification scheme organized the hadrons into isospin multiplets, but the physical basis behind it was still unclear. In 1964, Gell-Mann and George Zweig (independently of each other) proposed the quark model, then consisting only of up, down, and strange quarks. However, while the quark model explained the Eightfold Way, no direct evidence of the existence of quarks was found until 1968 at the Stanford Linear Accelerator Center. Deep inelastic scattering experiments indicated that protons had substructure, and that protons made of three more-fundamental particles explained the data (thus confirming the quark model).
At first people were reluctant to identify the three-bodies as quarks, instead preferring Richard Feynman's parton description, but over time the quark theory became accepted (see "November Revolution").
Mass.
Despite being extremely common, the bare mass of the down quark is not well determined, but probably lies between 4.5 and 5.3. Lattice QCD calculations give a more precise value: .
When found in mesons (particles made of one quark and one antiquark) or baryons (particles made of three quarks), the 'effective mass' (or 'dressed' mass) of quarks becomes greater because of the binding energy caused by the gluon field between quarks (see mass–energy equivalence). For example, the effective mass of down quarks in a proton is around . Because the bare mass of down quarks is so small, it cannot be straightforwardly calculated because relativistic effects have to be taken into account.

</doc>
<doc id="647994" url="https://en.wikipedia.org/wiki?curid=647994" title="E8 (mathematics)">
E8 (mathematics)

In mathematics, E8 is any of several closely related exceptional simple Lie groups, linear algebraic groups or Lie algebras of dimension 248; the same notation is used for the corresponding root lattice, which has rank 8. The designation E8 comes from the Cartan–Killing classification of the complex simple Lie algebras, which fall into four infinite series labeled A"n", B"n", C"n", D"n", and five exceptional cases labeled E6, E7, E8, F4, and G2. The E8 algebra is the largest and most complicated of these exceptional cases.
 discovered the complex Lie algebra E8 during his classification of simple compact Lie algebras, though he did not prove its existence, which was first shown by Élie Cartan. Cartan determined that a complex simple Lie algebra of type E8 admits three real forms. Each of them gives rise to a simple Lie group of dimension 248, exactly one of which is compact. introduced algebraic groups and Lie algebras of type E8 over other fields: for example, in the case of finite fields they lead to an infinite family of finite simple groups of Lie type.
Basic description.
The Lie group E8 has dimension 248. Its rank, which is the dimension of its maximal torus, is 8. Therefore the vectors of the root system are in eight-dimensional Euclidean space: they are described explicitly later in this article. The Weyl group of E8, which is the group of symmetries of the maximal torus which are induced by conjugations in the whole group, has order 21435527 = 696729600.
The compact group E8 is unique among simple compact Lie groups in that its non-trivial representation of smallest dimension is the adjoint representation (of dimension 248) acting on the Lie algebra E8 itself; it is also the unique one which has the following four properties: trivial center, compact, simply connected, and simply laced (all roots have the same length).
There is a Lie algebra E"n" for every integer "n" ≥ 3, which is infinite dimensional if "n" is greater than 8.
Real and complex forms.
There is a unique complex Lie algebra of type E8, corresponding to a complex group of complex dimension 248. The complex Lie group E8 of complex dimension 248 can be considered as a simple real Lie group of real dimension 496. This is simply connected, has maximal compact subgroup the compact form (see below) of E8, and has an outer automorphism group of order 2 generated by complex conjugation.
As well as the complex Lie group of type E8, there are three real forms of the Lie algebra, three real forms of the group with trivial center (two of which have non-algebraic double covers, giving two further real forms), all of real dimension 248, as follows:
For a complete list of real forms of simple Lie algebras, see the list of simple Lie groups.
E8 as an algebraic group.
By means of a Chevalley basis for the Lie algebra, one can define E8 as a linear algebraic group over the integers and, consequently, over any commutative ring and in particular over any field: this defines the so-called split (sometimes also known as “untwisted”) form of E8. Over an algebraically closed field, this is the only form; however, over other fields, there are often many other forms, or “twists” of E8, which are classified in the general framework of Galois cohomology (over a perfect field "k") by the set H1("k",Aut(E8)) which, because the Dynkin diagram of E8 (see below) has no automorphisms, coincides with H1("k",E8).
Over R, the real connected component of the identity of these algebraically twisted forms of E8 coincide with the three real Lie groups mentioned above, but with a subtlety concerning the fundamental group: all forms of E8 are simply connected in the sense of algebraic geometry, meaning that they admit no non-trivial algebraic coverings; the non-compact and simply connected real Lie group forms of E8 are therefore not algebraic and admit no faithful finite-dimensional representations.
Over finite fields, the Lang–Steinberg theorem implies that H1("k",E8)=0, meaning that E8 has no twisted forms: see below.
Representation theory.
The characters of finite dimensional representations of the real and complex Lie algebras and Lie groups are all given by the Weyl character formula. The dimensions of the smallest irreducible representations are :
The 248-dimensional representation is the adjoint representation. There are two non-isomorphic irreducible representations of dimension 8634368000 (it is not unique; however, the next integer with this property is 175898504162692612600853299200000 ). The fundamental representations are those with dimensions 3875, 6696000, 6899079264, 146325270, 2450240, 30380, 248 and 147250 (corresponding to the eight nodes in the Dynkin diagram in the order chosen for the Cartan matrix below, i.e., the nodes are read in the seven-node chain first, with the last node being connected to the third).
The coefficients of the character formulas for infinite dimensional irreducible representations of E8 depend on some large square matrices consisting of polynomials, the Lusztig–Vogan polynomials, an analogue of Kazhdan–Lusztig polynomials introduced for reductive groups in general by George Lusztig and David Kazhdan (1983). The values at 1 of the Lusztig–Vogan polynomials give the coefficients of the matrices relating the standard representations (whose characters are easy to describe) with the irreducible representations.
These matrices were computed after four years of collaboration by a group of 18 mathematicians and computer scientists, led by Jeffrey Adams, with much of the programming done by Fokko du Cloux. The most difficult case (for exceptional groups) is the split real form of E8 (see above), where the largest matrix is of size 453060×453060. The Lusztig–Vogan polynomials for all other exceptional simple groups have been known for some time; the calculation for the split form of "E"8 is far longer than any other case. The announcement of the result in March 2007 received extraordinary attention from the media (see the external links), to the surprise of the mathematicians working on it.
The representations of the E8 groups over finite fields are given by Deligne–Lusztig theory.
Constructions.
One can construct the (compact form of the) E8 group as the automorphism group of the corresponding e8 Lie algebra. This algebra has a 120-dimensional subalgebra so(16) generated by "J""ij" as well as 128 new generators "Q""a" that transform as a Weyl–Majorana spinor of spin(16). These statements determine the commutators
as well as
while the remaining commutator (not anticommutator!) is defined as
It is then possible to check that the Jacobi identity is satisfied.
Geometry.
The compact real form of E8 is the isometry group of the 128-dimensional exceptional compact Riemannian symmetric space EVIII (in Cartan's classification). It is known informally as the "octooctonionic projective plane" because it can be built using an algebra that is the tensor product of the octonions with themselves, and is also known as a Rosenfeld projective plane, though it does not obey the usual axioms of a projective plane. This can be seen systematically using a construction known as the "magic square", due to Hans Freudenthal and Jacques Tits .
E8 root system.
A root system of rank "r" is a particular finite configuration of vectors, called "roots", which span an "r"-dimensional Euclidean space and satisfy certain geometrical properties. In particular, the root system must be invariant under reflection through the hyperplane perpendicular to any root.
The E8 root system is a rank 8 root system containing 240 root vectors spanning R8. It is irreducible in the sense that it cannot be built from root systems of smaller rank. All the root vectors in E8 have the same length. It is convenient for a number of purposes to normalize them to have length √2. These 240 vectors are the vertices of a semi-regular polytope discovered by Thorold Gosset in 1900, sometimes known as the 421 polytope.
Construction.
In the so-called "even coordinate system", E8 is given as the set of all vectors in R8 with length squared equal to 2 such that coordinates are either all integers or all half-integers and the sum of the coordinates is even.
Explicitly, there are 112 roots with integer entries obtained from
by taking an arbitrary combination of signs and an arbitrary permutation of coordinates, and 128 roots with half-integer entries obtained from
by taking an even number of minus signs (or, equivalently, requiring that the sum of all the eight coordinates be even). There are 240 roots in all.
The 112 roots with integer entries form a D8 root system. The E8 root system also contains a copy of A8 (which has 72 roots) as well as E6 and E7 (in fact, the latter two are usually "defined" as subsets of E8).
In the "odd coordinate system", E8 is given by taking the roots in the even coordinate system and changing the sign of any one coordinate. The roots with integer entries are the same while those with half-integer entries have an odd number of minus signs rather than an even number.
Dynkin diagram.
The Dynkin diagram for E8 is given by .
This diagram gives a concise visual summary of the root structure. Each node of this diagram represents a simple root. A line joining two simple roots indicates that they are at an angle of 120° to each other. Two simple roots which are not joined by a line are orthogonal.
Cartan matrix.
The Cartan matrix of a rank "r" root system is an "r × r" matrix whose entries are derived from the simple roots. Specifically, the entries of the Cartan matrix are given by
where (−,−) is the Euclidean inner product and "αi" are the simple roots. The entries are independent of the choice of simple roots (up to ordering).
The Cartan matrix for E8 is given by
The determinant of this matrix is equal to 1.
Simple roots.
A set of simple roots for a root system Φ is a set of roots that form a basis for the Euclidean space spanned by Φ with the special property that each root has components with respect to this basis that are either all nonnegative or all nonpositive.
Given the E8 Cartan matrix (above) and a Dynkin diagram node ordering of: 
One choice of simple roots is given by the rows of the following matrix:
Weyl group.
The Weyl group of E8 is of order 696729600, and can be described as O(2): it is of the form 2."G".2 (that is, a stem extension by the cyclic group of order 2 of an extension of the cyclic group of order 2 by a group "G") where "G" is the unique simple group of order 174182400 (which can be described as PSΩ8+(2)).
E8 root lattice.
The integral span of the E8 root system forms a lattice in R8 naturally called the E8 root lattice. This lattice is rather remarkable in that it is the only (nontrivial) even, unimodular lattice with rank less than 16.
Simple subalgebras of E8.
The Lie algebra E8 contains as subalgebras all the exceptional Lie algebras as well as many other important Lie algebras in mathematics and physics. The height of the Lie algebra on the diagram approximately corresponds to the rank of the algebra. A line from an algebra down to a lower algebra indicates that the lower algebra is a subalgebra of the higher algebra.
Chevalley groups of type E8.
 showed that the points of the (split) algebraic group E8 (see above) over a finite field with "q" elements form a finite Chevalley group, generally written E8("q"), which is simple for any "q", and constitutes one of the infinite families addressed by the classification of finite simple groups. Its number of elements is given by the formula :
The first term in this sequence, the order of E8(2), namely ≈ 3.38×1074, is already larger than the size of the Monster group. This group E8(2) is the last one described (but without its character table) in the ATLAS of Finite Groups.
The Schur multiplier of E8("q") is trivial, and its outer automorphism group is that of field automorphisms (i.e., cyclic of order "f" if "q"="pf" where "p" is prime).
 described the unipotent representations of finite groups of type "E"8.
Subgroups.
The smaller exceptional groups E7 and E6 sit inside E8. In the compact group, both E7×SU(2)/(−1,−1) and E6×SU(3)/(Z/3Z) are maximal subgroups of E8.
The 248-dimensional adjoint representation of E8 may be considered in terms of its restricted representation to the first of these subgroups. It transforms under E7×SU(2) as a sum of tensor product representations, which may be labelled as a pair of dimensions as (3,1) + (1,133) + (2,56) (since there is a quotient in the product, these notations may strictly be taken as indicating the infinitesimal (Lie algebra) representations). Since the adjoint representation can be described by the roots together with the generators in the Cartan subalgebra, we may see that decomposition by looking at these. In this description,
The 248-dimensional adjoint representation of E8, when similarly restricted, transforms under E6×SU(3) as: (8,1) + (1,78) + (3,27) + (3,27). We may again see the decomposition by looking at the roots together with the generators in the Cartan subalgebra. In this description,
The finite quasisimple groups that can embed in (the compact form of) E8 were found by .
The Dempwolff group is a subgroup of (the compact form of) E8. It is contained in the Thompson sporadic group, which acts on the underlying vector space of the Lie group E8 but does not preserve the Lie bracket. The Thompson group fixes a lattice and does preserve the Lie bracket of this lattice mod 3, giving an embedding of the Thompson group into E8(F3).
Applications.
The E8 Lie group has applications in theoretical physics and especially in string theory and supergravity. E8×E8 is the gauge group of one of the two types of heterotic string and is one of two anomaly-free gauge groups that can be coupled to the "N" = 1 supergravity in ten dimensions. E8 is the U-duality group of supergravity on an eight-torus (in its split form).
One way to incorporate the standard model of particle physics into heterotic string theory is the symmetry breaking of E8 to its maximal subalgebra SU(3)×E6.
In 1982, Michael Freedman used the E8 lattice to construct an example of a topological 4-manifold, the E8 manifold, which has no smooth structure.
Antony Garrett Lisi's incomplete "An Exceptionally Simple Theory of Everything" attempts to describe all known fundamental interactions in physics as part of the E8 Lie algebra.
 reported an experiment where the electron spins of a cobalt-niobium crystal exhibited, under certain conditions, two of the eight peaks related to E8 that were predicted by .
External links.
Lusztig–Vogan polynomial calculation
Other links

</doc>
<doc id="1239472" url="https://en.wikipedia.org/wiki?curid=1239472" title="Eccentricity (mathematics)">
Eccentricity (mathematics)

In mathematics, the eccentricity, denoted "e" or formula_1, is a parameter associated with every conic section. It can be thought of as a measure of how much the conic section deviates from being circular.
In particular,
Furthermore, two conic sections are similar (identically shaped) if and only if they have the same eccentricity.
Definitions.
Any conic section can be defined as the locus of points whose distances to a point (the focus) and a line (the directrix) are in a constant ratio. That ratio is called eccentricity, commonly denoted as "e".
The eccentricity can also be defined in terms of the intersection of a plane and a double-napped cone associated with the conic section. If the cone is oriented with its axis vertical, the eccentricity is
where β is the angle between the plane and the horizontal and α is the angle between the cone's slant generator and the horizontal. For formula_3 the plane section is a circle, for formula_4 a parabola. (The plane must not meet the vertex of the cone.)
The linear eccentricity of an ellipse or hyperbola , denoted "c" (or sometimes "f" or "e"), is the distance between its center and either of its two foci. The eccentricity can be defined as the ratio of the linear eccentricity to the semimajor axis "a": that is, formula_5. (Lacking a center the linear eccentricity for parabolas is not defined.)
Alternative names.
The eccentricity is sometimes called first eccentricity to distinguish it from the second eccentricity and third eccentricity defined for ellipses (see below). The eccentricity is also sometimes called numerical eccentricity.
In the case of ellipses and hyperbolas the linear eccentricity is sometimes called half-focal separation.
Notation.
Three notational conventions are in common use:
This article makes use of the first notation.
Values.
where, for the ellipse and the hyperbola, "a" is the length of the semi-major axis and "b" is the length of the semi-minor axis.
When the conic section is given in the general quadratic form
the following formula gives the eccentricity "e" if the conic section is not a parabola (which has eccentricity equal to 1), not a degenerate hyperbola or degenerate ellipse, and not an imaginary ellipse:
where formula_10 if the determinant of the 3×3 matrix
is negative or formula_12 if that determinant is positive.
Ellipses.
The eccentricity of an ellipse is strictly less than 1. When circles (which have eccentricity 0) are counted as ellipses, the eccentricity of an ellipse is greater than or equal to 0; if circles are given a special category and are excluded from the category of ellipses, then the eccentricity of an ellipse is strictly greater than 0.
For any ellipse, let "a" be the length of its semi-major axis and "b" be the length of its semi-minor axis.
We define a number of related additional concepts (only for ellipses):
Other formulas for the eccentricity of an ellipse.
The eccentricity of an ellipse is, most simply, the ratio of the distance between the center of the ellipse and each focus to the length of the semimajor axis.
The eccentricity is also the ratio of the semimajor axis "a" to the distance "d" from the center to the directrix:
The eccentricity can be expressed in terms of the flattening "g" (defined as "g" = 1 – "b"/"a" for semimajor axis "a" and semiminor axis "b"):
Define the maximum and minimum radii formula_15 and formula_16 as the maximum and minimum distances from either focus to the ellipse (that is, the distances from either focus to the two ends of the major axis). Then with semimajor axis "a", the eccentricity is given by
which is the distance between the foci divided by the length of the major axis.
Hyperbolas.
The eccentricity of a hyperbola can be any real number greater than 1, with no upper bound. The eccentricity of a rectangular hyperbola is formula_18.
Quadrics.
The eccentricity of a three-dimensional quadric is the eccentricity of a designated section of it. For example, on a triaxial ellipsoid, the "meridional eccentricity" is that of the ellipse formed by a section containing both the longest and the shortest axes (one of which will be the polar axis), and the "equatorial eccentricity" is the eccentricity of the ellipse formed by a section through the centre, perpendicular to the polar axis (i.e. in the equatorial plane). But: conic sections may occur on surfaces of higher order, too (see image).
Celestial mechanics.
In celestial mechanics, for bound orbits in a spherical potential, the definition above is informally generalized. When the apocenter distance is close to the pericenter distance, the orbit is said to have low eccentricity; when they are very different, the orbit is said be eccentric or having eccentricity near unity. This definition coincides with the mathematical definition of eccentricity for ellipses, in Keplerian, i.e., formula_19 potentials.
Analogous classifications.
A number of classifications in mathematics use derived terminology from the classification of conic sections by eccentricity:

</doc>
<doc id="681582" url="https://en.wikipedia.org/wiki?curid=681582" title="Effective field theory">
Effective field theory

In physics, an effective field theory is a type of approximation to (or effective theory for) an underlying physical theory, such as a quantum field theory or a statistical mechanics model. An effective field theory includes the appropriate degrees of freedom to describe physical phenomena occurring at a chosen length scale or energy scale, while ignoring substructure and degrees of freedom at shorter distances (or, equivalently, at higher energies). Intuitively, one averages over the behavior of the underlying theory at shorter length scales to derive a hopefully simplified model at longer length scales. Effective field theories typically work best when there is a large separation between length scale of interest and the length scale of the underlying dynamics. Effective field theories have found use in particle physics, statistical mechanics, condensed matter physics, general relativity, and hydrodynamics. They simplify calculations, and allow treatment of Dissipation and Radiation effects
The renormalization group.
Presently, effective field theories are discussed in the context of the renormalization group (RG) where the process of "integrating out" short distance degrees of freedom is made systematic. Although this method is not sufficiently concrete to allow the actual construction of effective field theories, the gross understanding of their usefulness becomes clear through a RG analysis. This method also lends credence to the main technique of constructing effective field theories, through the analysis of symmetries. If there is a single mass scale M in the "microscopic" theory, then the effective field theory can be seen as an expansion in 1/M. The construction of an effective field theory accurate to some power of 1/M requires a new set of free parameters at each order of the expansion in 1/M. This technique is useful for scattering or other processes where the maximum momentum scale k satisfies the condition k/M≪1. Since effective field theories are not valid at small length scales, they need not be renormalizable. Indeed, the ever expanding number of parameters at each order in 1/M required for an effective field theory means that they are generally not renormalizable in the same sense as quantum electrodynamics which requires only the renormalization of two parameters.
Examples of effective field theories.
Fermi theory of beta decay.
The best-known example of an effective field theory is the Fermi theory of beta decay. This theory was developed during the early study of weak decays of nuclei when only the hadrons and leptons undergoing weak decay were known. The typical reactions studied were:
This theory posited a pointlike interaction between the four fermions involved in these reactions. The theory had great phenomenological success and was eventually understood to arise from the gauge theory of electroweak interactions, which forms a part of the standard model of particle physics. In this more fundamental theory, the interactions are mediated by a flavour-changing gauge boson, the W±. The immense success of the Fermi theory was because the W particle has mass of about 80 GeV, whereas the early experiments were all done at an energy scale of less than 10 MeV. Such a separation of scales, by over 3 orders of magnitude, has not been met in any other situation as yet.
BCS theory of superconductivity.
Another famous example is the BCS theory of superconductivity. Here the underlying theory is of electrons in a metal interacting with lattice vibrations called phonons. The phonons cause attractive interactions between some electrons, causing them to form Cooper pairs. The length scale of these pairs is much larger than the wavelength of phonons, making it possible to neglect the dynamics of phonons and construct a theory in which two electrons effectively interact at a point. This theory has had remarkable success in describing and predicting the results of experiments on superconductivity.
Effective Field Theories in Gravity.
General relativity itself is expected to be the low energy effective field theory of a full theory of quantum gravity, such as string theory. The expansion scale is the Planck mass.
Effective field theories have also been used to simplify problems in General Relativity, in particular in calculating the gravitational wave signature of inspiralling finite-sized objects. The most common EFT in GR is "Non-Relativistic General Relativity" (NRGR), which is similar to the post-Newtonian expansion. Another common GR EFT is the Extreme Mass Ratio (EMR), which in the context of the inspiralling problem is called EMRI.
Other examples.
Presently, effective field theories are written for many situations.

</doc>
<doc id="228601" url="https://en.wikipedia.org/wiki?curid=228601" title="Eigenfunction">
Eigenfunction

In mathematics, an eigenfunction of a linear operator, , defined on some function space, is any non-zero function in that space that returns from the operator exactly as is, except for a multiplicative scaling factor. More precisely, one has
for some scalar, , the corresponding eigenvalue. The solution of the differential eigenvalue problem also depends on any boundary conditions required of . In each case there are only certain eigenvalues that admit a corresponding solution for (with each belonging to the eigenvalue ) when combined with the boundary conditions. Eigenfunctions are used to analyze .
For example, is an eigenfunction for the differential operator
for any value of , with corresponding eigenvalue . Similarly, the functions and , have eigenvalue . If a boundary condition is applied to this system (e.g., or ), then even fewer pairs of eigenfunctions and eigenvalues satisfy both the definition of an eigenfunction, and the boundary conditions.
Specifically, in the study of signals and systems, the eigenfunction of a system is the signal which when input into the system, produces a response with the complex constant .
Examples.
Derivative operator.
A widely used class of linear operators acting on function spaces are the differential operators on function spaces. As an example, on the space of infinitely differentiable real functions of a real argument , the process of differentiation is a linear operator since
The eigenvalue equation for a linear differential operator in is then a differential equation
The functions that satisfy this equation are commonly called eigenfunctions. For the derivative operator , an eigenfunction is a function that, when differentiated, yields a constant times the original function. That is,
for all . This equation can be solved for any value of . The solution is an exponential function
The derivative operator is defined also for complex-valued functions of a complex argument. In the complex version of the space , the eigenvalue equation has a solution for any complex constant . The spectrum of the operator is therefore the whole complex plane. This is an example of a continuous spectrum.
Applications.
Vibrating strings.
Let denote the sideways displacement of a stressed elastic chord, such as the vibrating strings of a string instrument, as a function of the position along the string and of time . From the laws of mechanics, applied to infinitesimal portions of the string, one can deduce that the function satisfies the partial differential equation
which is called the (one-dimensional) wave equation. Here is a constant that depends on the tension and mass of the string.
This problem is amenable to the method of separation of variables. If we assume that can be written as the product of the form , we can form a pair of ordinary differential equations:
Each of these is an eigenvalue equation, for eigenvalues formula_9 and , respectively. For any values of and , the equations are satisfied by the functions
where and are arbitrary real constants. If we impose boundary conditions (that the ends of the string are fixed with at and , for example) we can constrain the eigenvalues. For those boundary conditions, we find , and so the phase angle and
Thus, the constant is constrained to take one of the values , where is any integer. Thus, the clamped string supports a family of standing waves of the form
From the point of view of our musical instrument, the frequency is the frequency of the -th harmonic, which is called the -th overtone.
Quantum mechanics.
Eigenfunctions play an important role in many branches of physics. An important example is quantum mechanics, where the Schrödinger equation
with 
has solutions of the form
where are eigenfunctions of the operator formula_17 with eigenvalues . The fact that only certain eigenvalues with associated eigenfunctions satisfy Schrödinger's equation leads to a natural basis for quantum mechanics and the periodic table of the elements, with each an allowable energy state of the system. The success of this equation in explaining the spectral characteristics of hydrogen is considered one of the greatest triumphs of 20th century physics.
Since the Hamiltonian operator formula_17 is a Hermitian Operator, its eigenfunctions are orthogonal functions. This is not necessarily the case for eigenfunctions of other operators (such as the example mentioned above). Orthogonal functions have the property that
where is the complex conjugate of .
whenever , in which case the set is said to be orthogonal. Also, it is linearly independent.

</doc>
<doc id="2161429" url="https://en.wikipedia.org/wiki?curid=2161429" title="Eigenvalues and eigenvectors">
Eigenvalues and eigenvectors

In linear algebra, an eigenvector or characteristic vector of a square matrix is a vector that does not change its direction under the associated linear transformation. In other words—if v is a vector that is not zero, then it is an eigenvector of a square matrix "A" if "A"v is a scalar multiple of v. This condition could be written as the equation:
where "λ" is a scalar known as the eigenvalue or characteristic value associated with the eigenvector v. Geometrically, an eigenvector corresponding to a real, nonzero eigenvalue points in a direction that is stretched by the transformation and the eigenvalue is the factor by which it is stretched. If the eigenvalue is negative, the direction is reversed.
There is a correspondence between "n" by "n" square matrices and linear transformations from an "n"-dimensional vector space to itself. For this reason, it is equivalent to define eigenvalues and eigenvectors using either the language of matrices or the language of linear transformations.
Overview.
If a two-dimensional space is visualized as a rubber sheet, a linear map with two eigenvectors and associated eigenvalues λ1 and λ2 may be envisioned as stretching/compressing the sheet simultaneously along the two directions of the eigenvectors with the factors given by the eigenvalues. Thereby only the directions of the eigenvectors do not change. For example, the sheet could be stretched by a factor λ1 along the "x"-axis and λ2 along the "y"-axis, assuming the eigendirections being given by the directions of the coordinate axes. In two dimensions, there can be two such independent stretching directions, but they do not have to be at right angles to each other. A rotation in two dimensions is a linear map with no eigenvectors, and a shear, as in the photo, has only one eigenvector, with eigenvalue 1. Other vectors besides eigenvectors change their directions, unless the two eigenvalues are equal, in which case all vectors are eigenvectors with that eigenvalue, yielding a magnification—i.e., a linear map that alters neither shape nor direction, but only magnitude. A reflection may be viewed as stretching a line perpendicular to the axis of reflection by a factor of −1 while stretching the axis of reflection by a factor of 1. For 3D rotations, the axis of rotation is an eigenvector of eigenvalue 1.
A three-coordinate vector may be seen as an arrow in three-dimensional space starting at the origin. In that case, an eigenvector formula_1 is an arrow whose direction is either preserved or exactly reversed after multiplication by formula_2. The corresponding eigenvalue determines how the length of the arrow is changed by the operation, and whether its direction is reversed or not, determined by whether the eigenvalue is negative or positive.
In abstract linear algebra, these concepts are naturally extended to more general situations, where the set of real scalar factors is replaced by any field of scalars (such as algebraic or complex numbers); the set of Cartesian vectors formula_3 is replaced by any vector space (such as the continuous functions, the polynomials or the trigonometric series), and multiplication of a vector by a matrix is replaced by any linear operator that maps vectors to vectors (such as the derivative from calculus). In such cases, the "vector" in "eigenvector" may be replaced by a more specific term, such as "eigenfunction", "eigenmode", "eigenface", or "eigenstate". Thus, for example, the exponential function formula_4 is an eigenfunction of the derivative operator, formula_5 , with eigenvalue formula_6, since its derivative is formula_7.
The set of all eigenvectors of a matrix (or linear operator), each paired with its corresponding eigenvalue, is called the eigensystem of that matrix. Any nonzero scalar multiple of an eigenvector is also an eigenvector corresponding to the same eigenvalue. An eigenspace or characteristic space of a matrix formula_2 is the set of all eigenvectors of formula_2 corresponding to the same eigenvalue, together with the zero vector. An eigenbasis for formula_2 is any basis for the set of all vectors that consists of linearly independent eigenvectors of formula_2. Not every matrix has an eigenbasis, but every symmetric matrix does.
The prefix eigen- is adopted from the German word "eigen" for "own-", "unique to", "peculiar to", or "belonging to" in the sense of "idiosyncratic" in relation to the originating matrix.
Eigenvalues and eigenvectors have many applications in both pure and applied mathematics. They are used in matrix factorization, in quantum mechanics, and in many other areas.
History.
Eigenvalues are often introduced in the context of linear algebra or matrix theory. Historically, however, they arose in the study of quadratic forms and differential equations.
In the 18th century Euler studied the rotational motion of a rigid body and discovered the importance of the principal axes. Lagrange realized that the principal axes are the eigenvectors of the inertia matrix. In the early 19th century, Cauchy saw how their work could be used to classify the quadric surfaces, and generalized it to arbitrary dimensions. Cauchy also coined the term "racine caractéristique" (characteristic root) for what is now called "eigenvalue"; his term survives in "characteristic equation".
Fourier used the work of Laplace and Lagrange to solve the heat equation by separation of variables in his famous 1822 book "Théorie analytique de la chaleur". Sturm developed Fourier's ideas further and brought them to the attention of Cauchy, who combined them with his own ideas and arrived at the fact that real symmetric matrices have real eigenvalues. This was extended by Hermite in 1855 to what are now called Hermitian matrices. Around the same time, Brioschi proved that the eigenvalues of orthogonal matrices lie on the unit circle, and Clebsch found the corresponding result for skew-symmetric matrices. Finally, Weierstrass clarified an important aspect in the stability theory started by Laplace by realizing that defective matrices can cause instability.
In the meantime, Liouville studied eigenvalue problems similar to those of Sturm; the discipline that grew out of their work is now called "Sturm–Liouville theory". Schwarz studied the first eigenvalue of Laplace's equation on general domains towards the end of the 19th century, while Poincaré studied Poisson's equation a few years later.
At the start of the 20th century, Hilbert studied the eigenvalues of integral operators by viewing the operators as infinite matrices. He was the first to use the German word "eigen", which means "own", to denote eigenvalues and eigenvectors in 1904, though he may have been following a related usage by Helmholtz. For some time, the standard term in English was "proper value", but the more distinctive term "eigenvalue" is standard today.
The first numerical algorithm for computing eigenvalues and eigenvectors appeared in 1929, when Von Mises published the power method. One of the most popular methods today, the QR algorithm, was proposed independently by John G.F. Francis and Vera Kublanovskaya in 1961.
Real matrices.
Consider "n"-dimensional vectors that are formed as a list of "n" real numbers, such as the three dimensional vectors,
These vectors are said to be scalar multiples of each other, also parallel or collinear, if there is a scalar λ, such that
In this case "λ" = −1/20.
Now consider the linear transformation of "n"-dimensional vectors defined by an "n"×"n" matrix "A", that is,
or
where, for each index formula_16,
If it occurs that w and v are scalar multiples, that is if
then v is an eigenvector of the linear transformation "A" and the scale factor λ is the eigenvalue corresponding to that eigenvector.
Two dimensional example.
Consider the transformation matrix "A", given by,
The figure on the right shows the effect of this transformation on point coordinates in the plane.
The eigenvectors v of this transformation satisfy the equation,
Rearrange this equation to obtain
which has a solution only when its determinant equals zero. 
Set the determinant to zero to obtain the polynomial equation,
known as the characteristic polynomial of the matrix "A". In this case, it has the roots and .
For , the equation becomes,
which has the solution,
For , the equation becomes, 
which has the solution,
Thus, the vectors v and w are eigenvectors of "A" associated with the eigenvalues and , respectively.
Three dimensional example.
The eigenvectors v of the 3×3 matrix "A",
satisfy the equation
This equation has solutions only if the determinant equals zero, which yields the characteristic polynomial,
with the roots , , and .
Associated with the roots , , and are the respective eigenvectors,
Diagonal matrices.
Matrices with entries only along the main diagonal are called "diagonal matrices". It is easy to see that the eigenvalues of a diagonal matrix are the diagonal elements themselves. Consider the matrix A,
The characteristic polynomial of "A" is given by
which has the roots , , and .
Associated with these roots are the eigenvectors,
respectively.
Triangular matrices.
A matrix with elements above the main diagonal that are all zeros is described as a "triangular matrix", or in this case, "lower triangular." If the elements below the main diagonal are all zeros then the matrix is "upper triangular." The eigenvalues of triangular matrices are the elements of the main diagonal, in the same way as for diagonal matrices.
Consider the lower triangular matrix "A",
The characteristic polynomial of "A" is given by
which has the roots , , and .
Associated with these roots are the eigenvectors,
respectively.
Eigenvector basis.
In this section, it is shown that a change of coordinates of a matrix "A" to a basis formed by its eigenvectors results in a diagonal matrix.
Let "A" be an "n"×"n" linear transformation 
that has "n" linearly independent eigenvectors vi, and consider the change of coordinates of "A" so that it is defined relative to its eigenvector basis.
Recall that the eigenvectors vi of "A" satisfy the eigenvalue equation,
Assemble these eigenvectors into the matrix V, which is invertible because these vectors are assumed linearly independent. This means the coordinates of x and X relative to the basis vi can be computed to be,
This yields the change of coordinates
To see the effect of this change of coordinates on "A", introduce "I=VV−1" into the eigenvalue equation
and multiply both side by "V−1" to obtain
Notice that
which is the natural basis vector. Thus,
and the matrix K is found to be a diagonal matrix with the eigenvalues λi as its diagonal elements.
This shows that a matrix "A" with a linearly independent system of eigenvectors is similar to a diagonal matrix formed from its eigenvalues.
Matrices.
Characteristic polynomial.
The eigenvalue equation for a matrix formula_2 is
which is equivalent to
where formula_48 is the formula_49 identity matrix. It is a fundamental result of linear algebra that an equation formula_50 has a non-zero solution formula_1 if, and only if, the determinant formula_52 of the matrix formula_53 is zero. It follows that the eigenvalues of formula_2 are precisely the real numbers formula_6 that satisfy the equation
The left-hand side of this equation can be seen (using Leibniz' rule for the determinant) to be a polynomial function of the variable formula_6. The degree of this polynomial is formula_58, the order of the matrix. Its coefficients depend on the entries of formula_2, except that its term of degree formula_58 is always formula_61. This polynomial is called the "characteristic polynomial" of formula_2; and the above equation is called the "characteristic equation" (or, less often, the "secular equation") of formula_2.
For example, let formula_2 be the matrix
The characteristic polynomial of formula_2 is
which is
The roots of this polynomial are 2, 1, and 11. Indeed these are the only three eigenvalues of formula_2, corresponding to the eigenvectors formula_70 formula_71 and formula_72 (or any non-zero multiples thereof).
Real domain.
Since the eigenvalues are roots of the characteristic polynomial, an formula_49 matrix has at most formula_58 eigenvalues. If the matrix has real entries, the coefficients of the characteristic polynomial are all real; but it may have fewer than formula_58 real roots, or no real roots at all.
For example, consider the cyclic permutation matrix
This matrix shifts the coordinates of the vector up by one position, and moves the first coordinate to the bottom. Its characteristic polynomial is formula_77 which has one real root formula_78. Any vector with three equal non-zero coordinates is an eigenvector for this eigenvalue. For example,
Complex domain.
The fundamental theorem of algebra implies that the characteristic polynomial of an formula_49 matrix formula_2, being a polynomial of degree formula_58, has exactly formula_58 complex roots. More precisely, it can be factored into the product of formula_58 linear terms,
where each formula_86 is a complex number. The numbers formula_87, formula_88, ... formula_89, (which may not be all distinct) are roots of the polynomial, and are precisely the eigenvalues of formula_2.
Even if the entries of formula_2 are all real numbers, the eigenvalues may still have non-zero imaginary parts (and the coordinates of the corresponding eigenvectors therefore also have non-zero imaginary parts). Also, the eigenvalues may be irrational numbers even if all the entries of formula_2 are rational numbers, or all are integers. However, if the entries of formula_2 are algebraic numbers (which include the rationals), the eigenvalues are (complex) algebraic numbers too.
The non-real roots of a real polynomial with real coefficients can be grouped into pairs of complex conjugate values, namely with the two members of each pair having the same real part and imaginary parts that differ only in sign. If the degree is odd, then by the intermediate value theorem at least one of the roots is real. Therefore, any real matrix with odd order has at least one real eigenvalue—whereas a real matrix with even order may have no real eigenvalues.
In the example of the 3×3 cyclic permutation matrix formula_2, above, the characteristic polynomial formula_77 has two additional non-real roots, namely
where formula_98 is the imaginary unit. Note that formula_99, formula_100, and formula_101. Then
Therefore, the vectors formula_104 and formula_105 are eigenvectors of formula_2, with eigenvalues formula_88, and formula_108, respectively.
Algebraic multiplicity.
"For a definition of "Geometric multiplicity", referred to in this section, see below".
Let formula_86 be an eigenvalue of an formula_49 matrix formula_2. The "algebraic multiplicity" formula_112 of formula_86 is its multiplicity as a root of the characteristic polynomial, that is, the largest integer formula_114 such that formula_115 divides evenly that polynomial.
Like the geometric multiplicity formula_116, we have formula_117; and the sum formula_118 of formula_112 over all "distinct" eigenvalues also cannot exceed formula_58. If complex eigenvalues are considered, formula_118 is exactly formula_58.
It can be proved that the geometric multiplicity formula_116 of an eigenvalue never exceeds its algebraic multiplicity formula_112. Therefore, formula_125 is at most formula_118.
If formula_127, then formula_86 is said to be a "simple eigenvalue".
If formula_129, then formula_86 is said to be a "semisimple eigenvalue".
Example.
For the matrix:
formula_131
The roots of this polynomial, and hence the eigenvalues, are 2 and 3.
The "algebraic multiplicity" of each eigenvalue is 2; in other words they are both double roots.
On the other hand, the "geometric multiplicity" of the eigenvalue 2 is only 1, because its eigenspace is spanned by the vector formula_134, and is therefore 1-dimensional.
Similarly, the geometric multiplicity of the eigenvalue 3 is 1 because its eigenspace is spanned by formula_135. Hence, the total algebraic multiplicity of A, denoted formula_136, is 4, which is the most it could be for a 4 by 4 matrix. The geometric multiplicity formula_137 is 2, which is the smallest it could be for a matrix with two distinct eigenvalues.
Diagonalization and eigendecomposition.
If the sum formula_125 of the geometric multiplicities of all eigenvalues is exactly formula_58, then formula_2 has a set of formula_58 linearly independent eigenvectors. Let formula_142 be a square matrix whose columns are those eigenvectors, in any order. Then we have formula_143, where formula_144 is the diagonal matrix such that formula_145 is the eigenvalue associated to column formula_16 of formula_142. Since the columns of formula_142 are linearly independent, the matrix formula_142 is invertible. Premultiplying both sides by formula_150 we get formula_151. By definition, therefore, the matrix formula_2 is diagonalizable.
Conversely, if formula_2 is diagonalizable, let formula_142 be a non-singular square matrix such that formula_155 is some diagonal matrix formula_156. Multiplying both sides on the left by formula_142 we get formula_158. Therefore each column of formula_142 must be an eigenvector of formula_2, whose eigenvalue is the corresponding element on the diagonal of formula_156. Since the columns of formula_142 must be linearly independent, it follows that formula_163. Thus formula_125 is equal to formula_58 if and only if formula_2 is diagonalizable.
If formula_2 is diagonalizable, the space of all formula_58-coordinate vectors can be decomposed into the direct sum of the eigenspaces of formula_2. This decomposition is called the eigendecomposition of formula_2, and it is preserved under change of coordinates.
A matrix that is not diagonalizable is said to be defective. For defective matrices, the notion of eigenvector can be generalized to generalized eigenvectors, and that of diagonal matrix to a Jordan form matrix. Over an algebraically closed field, any matrix formula_2 has a Jordan form and therefore admits a basis of generalized eigenvectors, and a decomposition into generalized eigenspaces.
Further properties.
Let formula_2 be an arbitrary formula_49 matrix of complex numbers with eigenvalues formula_87, formula_88, ... formula_89. (Here it is understood that an eigenvalue with algebraic multiplicity formula_177 occurs formula_177 times in this list.) Then
Left and right eigenvectors.
The use of matrices with a single column (rather than a single row) to represent vectors is traditional in many disciplines. For that reason, the word "eigenvector" almost always means a right eigenvector, namely a "column" vector that must be placed to the "right" of the matrix formula_2 in the defining equation
There may be also single-"row" vectors that are unchanged when they occur on the "left" side of a product with a square matrix formula_2; that is, which satisfy the equation
Any such row vector formula_202 is called a left eigenvector of formula_2.
The left eigenvectors of formula_2 are transposes of the right eigenvectors of the transposed matrix formula_205, since their defining equation is equivalent to
It follows that, if formula_2 is Hermitian, its left and right eigenvectors are complex conjugates. In particular if formula_2 is a real symmetric matrix, they are the same except for transposition.
Variational characterization.
In the Hermitian case, eigenvalues can be given a variational characterization. The largest eigenvalue of formula_209 is the maximum value of the quadratic form formula_210. A value of formula_211 that realizes that maximum, is an eigenvector.
General definition.
The concept of eigenvectors and eigenvalues extends naturally to abstract linear transformations on abstract vector spaces. Namely, let formula_212 be any vector space over some field formula_213 of scalars, and let formula_214 be a linear transformation mapping formula_212 into formula_212. We say that a non-zero vector formula_1 of formula_212 is an eigenvector of formula_214 if (and only if) there is a scalar formula_6 in formula_213 such that
This equation is called the eigenvalue equation for formula_214, and the scalar formula_6 is the eigenvalue of formula_214 corresponding to the eigenvector formula_1. Note that formula_227 means the result of applying the operator formula_214 to the vector formula_1, while formula_230 means the product of the scalar formula_6 by formula_1.
The matrix-specific definition is a special case of this abstract definition. Namely, the vector space formula_212 is the set of all column vectors of a certain size formula_58×1, and formula_214 is the linear transformation that consists in multiplying a vector by the given formula_49 matrix formula_2.
Some authors allow formula_1 to be the zero vector in the definition of eigenvector. This is reasonable as long as we define eigenvalues and eigenvectors carefully: If we would like the zero vector to be an eigenvector, then we must first define an eigenvalue of formula_239 as a scalar formula_240 in formula_213 such that there is a "nonzero" vector formula_242 in formula_212 with formula_244. We then define an eigenvector to be a vector formula_242 in formula_212 such that there is an eigenvalue formula_240 in formula_213 with formula_244. This way, we ensure that it is not the case that every scalar is an eigenvalue corresponding to the zero vector.
Geometric multiplicity.
The geometric multiplicity formula_250 of an eigenvalue formula_6 is the dimension of the eigenspace associated with formula_6, i.e., the maximum number of vectors in any linearly independent set of eigenvectors with that eigenvalue. It is clear from the definition of eigenvalue in the eigenvalue equation () that we always have formula_253
Eigenspace and spectrum.
If formula_1 is an eigenvector of formula_214, with eigenvalue formula_6, then any scalar multiple formula_257 of formula_1 with nonzero formula_259 is also an eigenvector with eigenvalue formula_6, since formula_261. Moreover, if formula_202 and formula_1 are eigenvectors with the same eigenvalue formula_6 and formula_265, then formula_266 is also an eigenvector with the same eigenvalue formula_6. Therefore, the set of all eigenvectors with the same eigenvalue formula_6, together with the zero vector, is a linear subspace of formula_212, called the eigenspace of formula_214 associated to formula_6. If that subspace has dimension 1, it is sometimes called an eigenline.
The eigenspaces of "T" always form a direct sum (and as a consequence any family of eigenvectors for different eigenvalues is always linearly independent). Therefore the sum of the dimensions of the eigenspaces cannot exceed the dimension "n" of the space on which "T" operates, and in particular there cannot be more than "n" distinct eigenvalues.
Any subspace spanned by eigenvectors of formula_214 is an invariant subspace of formula_214, and the restriction of "T" to such a subspace is diagonalizable.
The set of eigenvalues of formula_214 is sometimes called the spectrum of formula_214.
Eigenbasis.
An eigenbasis for a linear operator formula_214 that operates on a vector space formula_212 is a basis for formula_212 that consists entirely of eigenvectors of formula_214 (possibly with different eigenvalues). Such a basis exists precisely if the direct sum of the eigenspaces equals the whole space, in which case one can take the union of bases chosen in each of the eigenspaces as eigenbasis. The matrix of "T" in a given basis is diagonal precisely when that basis is an eigenbasis for "T", and for this reason "T" is called diagonalizable if it admits an eigenbasis.
Dynamic equations.
The simplest difference equations have the form
The solution of this equation for "x" in terms of "t" is found by using its characteristic equation
which can be found by stacking into matrix form a set of equations consisting of the above difference equation and the "k"–1 equations formula_282 giving a "k"-dimensional system of the first order in the stacked variable vector formula_283 in terms of its once-lagged value, and taking the characteristic equation of this system's matrix. This equation gives "k" characteristic roots formula_284 for use in the solution equation
A similar procedure is used for solving a differential equation of the form
Calculation.
Eigenvalues.
The eigenvalues of a matrix formula_2 can be determined by finding the roots of the characteristic polynomial. Explicit algebraic formulas for the roots of a polynomial exist only if the degree formula_58 is 4 or less. According to the Abel–Ruffini theorem there is no general, explicit and exact algebraic formula for the roots of a polynomial with degree 5 or more.
It turns out that any polynomial with degree formula_58 is the characteristic polynomial of some companion matrix of order formula_58. Therefore, for matrices of order 5 or more, the eigenvalues and eigenvectors cannot be obtained by an explicit algebraic formula, and must therefore be computed by approximate numerical methods.
In theory, the coefficients of the characteristic polynomial can be computed exactly, since they are sums of products of matrix elements; and there are algorithms that can find all the roots of a polynomial of arbitrary degree to any required accuracy. However, this approach is not viable in practice because the coefficients would be contaminated by unavoidable round-off errors, and the roots of a polynomial can be an extremely sensitive function of the coefficients (as exemplified by Wilkinson's polynomial).
Efficient, accurate methods to compute eigenvalues and eigenvectors of arbitrary matrices were not known until the advent of the QR algorithm in 1961.
 Combining the Householder transformation with the LU decomposition results in an algorithm with better convergence than the QR algorithm. For large Hermitian sparse matrices, the Lanczos algorithm is one example of an efficient iterative method to compute eigenvalues and eigenvectors, among several other possibilities.
Eigenvectors.
Once the (exact) value of an eigenvalue is known, the corresponding eigenvectors can be found by finding non-zero solutions of the eigenvalue equation, that becomes a system of linear equations with known coefficients. For example, once it is known that 6 is an eigenvalue of the matrix
we can find its eigenvectors by solving the equation formula_292, that is
This matrix equation is equivalent to two linear equations
Both equations reduce to the single linear equation formula_296. Therefore, any vector of the form formula_297, for any non-zero real number formula_298, is an eigenvector of formula_2 with eigenvalue formula_300.
The matrix formula_2 above has another eigenvalue formula_302. A similar calculation shows that the corresponding eigenvectors are the non-zero solutions of formula_303, that is, any vector of the form formula_304, for any non-zero real number formula_305.
Some numeric methods that compute the eigenvalues of a matrix also determine a set of corresponding eigenvectors as a by-product of the computation.
Generalizations to infinite-dimensional spaces.
The definition of eigenvalue of a linear transformation formula_214 remains valid even if the underlying space formula_212 is an infinite dimensional Hilbert or Banach space. Namely, a scalar formula_6 is an eigenvalue if and only if there is some nonzero vector formula_1 such that formula_310.
Eigenfunctions.
A widely used class of linear operators acting on infinite dimensional spaces are the differential operators on function spaces. Let formula_156 be a linear differential operator on the space formula_312 of infinitely differentiable real functions of a real argument formula_313. The eigenvalue equation for formula_156 is the differential equation
The functions that satisfy this equation are commonly called eigenfunctions of formula_156. For the derivative operator formula_317, an eigenfunction is a function that, when differentiated, yields a constant times the original function. The solution is an exponential function
including when formula_6 is zero when it becomes a constant function. Eigenfunctions are an essential tool in the solution of differential equations and many other applied and theoretical fields. For instance, the exponential functions are eigenfunctions of the shift operators. This is the basis of Fourier transform methods for solving problems.
Spectral theory.
If formula_6 is an eigenvalue of formula_214, then the operator formula_322 is not one-to-one, and therefore its inverse formula_323 does not exist. The converse is true for finite-dimensional vector spaces, but not for infinite-dimensional vector spaces. In general, the operator formula_324 may not have an inverse, even if formula_6 is not an eigenvalue.
For this reason, in functional analysis one defines the spectrum of a linear operator formula_214 as the set of all scalars formula_6 for which the operator formula_322 has no bounded inverse. Thus the spectrum of an operator always contains all its eigenvalues, but is not limited to them.
Associative algebras and representation theory.
More algebraically, rather than generalizing the vector space to an infinite dimensional space, one can generalize the algebraic object that is acting on the space, replacing a single operator acting on a vector space with an algebra representation – an associative algebra acting on a module. The study of such actions is the field of representation theory.
A closer analog of eigenvalues is given by the representation-theoretical concept of weight, with the analogs of eigenvectors and eigenspaces being "weight vectors" and "weight spaces."
Applications.
Eigenvalues of geometric transformations.
The following table presents some example transformations in the plane along with their 2×2 matrices, eigenvalues, and eigenvectors.
Note that the characteristic equation for a rotation is a quadratic equation with discriminant formula_329, which is a negative number whenever formula_330 is not an integer multiple of 180°. Therefore, except for these special cases, the two eigenvalues are complex numbers, formula_331; and all eigenvectors have non-real entries. Indeed, except for those special cases, a rotation changes the direction of every nonzero vector in the plane.
Schrödinger equation.
An example of an eigenvalue equation where the transformation formula_214 is represented in terms of a differential operator is the time-independent Schrödinger equation in quantum mechanics:
where formula_334, the Hamiltonian, is a second-order differential operator and formula_335, the wavefunction, is one of its eigenfunctions corresponding to the eigenvalue formula_336, interpreted as its energy.
However, in the case where one is interested only in the bound state solutions of the Schrödinger equation, one looks for formula_335 within the space of square integrable functions. Since this space is a Hilbert space with a well-defined scalar product, one can introduce a basis set in which formula_335 and formula_334 can be represented as a one-dimensional array and a matrix respectively. This allows one to represent the Schrödinger equation in a matrix form.
The bra–ket notation is often used in this context. A vector, which represents a state of the system, in the Hilbert space of square integrable functions is represented by formula_340. In this notation, the Schrödinger equation is:
where formula_340 is an eigenstate of formula_334 and formula_336 represents the eigenvalue. It is an observable self adjoint operator, the infinite dimensional analog of Hermitian matrices. As in the matrix case, in the equation above formula_345 is understood to be the vector obtained by application of the transformation formula_334 to formula_340.
Molecular orbitals.
In quantum mechanics, and in particular in atomic and molecular physics, within the Hartree–Fock theory, the atomic and molecular orbitals can be defined by the eigenvectors of the Fock operator. The corresponding eigenvalues are interpreted as ionization potentials via Koopmans' theorem. In this case, the term eigenvector is used in a somewhat more general meaning, since the Fock operator is explicitly dependent on the orbitals and their eigenvalues. If one wants to underline this aspect one speaks of nonlinear eigenvalue problem. Such equations are usually solved by an iteration procedure, called in this case self-consistent field method. In quantum chemistry, one often represents the Hartree–Fock equation in a non-orthogonal basis set. This particular representation is a generalized eigenvalue problem called Roothaan equations.
Geology and glaciology.
In geology, especially in the study of glacial till, eigenvectors and eigenvalues are used as a method by which a mass of information of a clast fabric's constituents' orientation and dip can be summarized in a 3-D space by six numbers. In the field, a geologist may collect such data for hundreds or thousands of clasts in a soil sample, which can only be compared graphically such as in a Tri-Plot (Sneed and Folk) diagram, or as a Stereonet on a Wulff Net.
The output for the orientation tensor is in the three orthogonal (perpendicular) axes of space. The three eigenvectors are ordered formula_348 by their eigenvalues formula_349; formula_350 then is the primary orientation/dip of clast, formula_351 is the secondary and formula_352 is the tertiary, in terms of strength. The clast orientation is defined as the direction of the eigenvector, on a compass rose of 360°. Dip is measured as the eigenvalue, the modulus of the tensor: this is valued from 0° (no dip) to 90° (vertical). The relative values of formula_353, formula_354, and formula_355 are dictated by the nature of the sediment's fabric. If formula_356, the fabric is said to be isotropic. If formula_357, the fabric is said to be planar. If formula_358, the fabric is said to be linear.
Principal component analysis.
The eigendecomposition of a symmetric positive semidefinite (PSD) matrix yields an orthogonal basis of eigenvectors, each of which has a nonnegative eigenvalue. The orthogonal decomposition of a PSD matrix is used in multivariate analysis, where the sample covariance matrices are PSD. This orthogonal decomposition is called principal components analysis (PCA) in statistics. PCA studies linear relations among variables. PCA is performed on the covariance matrix or the correlation matrix (in which each variable is scaled to have its sample variance equal to one). For the covariance or correlation matrix, the eigenvectors correspond to principal components and the eigenvalues to the variance explained by the principal components. Principal component analysis of the correlation matrix provides an orthonormal eigen-basis for the space of the observed data: In this basis, the largest eigenvalues correspond to the principal components that are associated with most of the covariability among a number of observed data.
Principal component analysis is used to study large data sets, such as those encountered in bioinformatics, data mining, chemical research, psychology, and in marketing. PCA is popular especially in psychology, in the field of psychometrics. In Q methodology, the eigenvalues of the correlation matrix determine the Q-methodologist's judgment of "practical" significance (which differs from the statistical significance of hypothesis testing; cf. criteria for determining the number of factors). More generally, principal component analysis can be used as a method of factor analysis in structural equation modeling.
Vibration analysis.
Eigenvalue problems occur naturally in the vibration analysis of mechanical structures with many degrees of freedom. The eigenvalues are the natural frequencies (or eigenfrequencies) of vibration, and the eigenvectors are the shapes of these vibrational modes. In particular, undamped vibration is governed by
or
that is, acceleration is proportional to position (i.e., we expect formula_361 to be sinusoidal in time).
In formula_58 dimensions, formula_363 becomes a mass matrix and formula_114 a stiffness matrix. Admissible solutions are then a linear combination of solutions to the generalized eigenvalue problem
where formula_366 is the eigenvalue and formula_367 is the angular frequency. Note that the principal vibration modes are different from the principal compliance modes, which are the eigenvectors of formula_114 alone. Furthermore, damped vibration, governed by
leads to a so-called quadratic eigenvalue problem,
This can be reduced to a generalized eigenvalue problem by clever use of algebra at the cost of solving a larger system.
The orthogonality properties of the eigenvectors allows decoupling of the differential equations so that the system can be represented as linear summation of the eigenvectors. The eigenvalue problem of complex structures is often solved using finite element analysis, but neatly generalize the solution to scalar-valued vibration problems.
Eigenfaces.
In image processing, processed images of faces can be seen as vectors whose components are the brightnesses of each pixel. The dimension of this vector space is the number of pixels. The eigenvectors of the covariance matrix associated with a large set of normalized pictures of faces are called eigenfaces; this is an example of principal components analysis. They are very useful for expressing any face image as a linear combination of some of them. In the facial recognition branch of biometrics, eigenfaces provide a means of applying data compression to faces for identification purposes. Research related to eigen vision systems determining hand gestures has also been made.
Similar to this concept, eigenvoices represent the general direction of variability in human pronunciations of a particular utterance, such as a word in a language. Based on a linear combination of such eigenvoices, a new voice pronunciation of the word can be constructed. These concepts have been found useful in automatic speech recognition systems for speaker adaptation.
Tensor of moment of inertia.
In mechanics, the eigenvectors of the moment of inertia tensor define the principal axes of a rigid body. The tensor of moment of inertia is a key quantity required to determine the rotation of a rigid body around its center of mass.
Stress tensor.
In solid mechanics, the stress tensor is symmetric and so can be decomposed into a diagonal tensor with the eigenvalues on the diagonal and eigenvectors as a basis. Because it is diagonal, in this orientation, the stress tensor has no shear components; the components it does have are the principal components.
Graphs.
In spectral graph theory, an eigenvalue of a graph is defined as an eigenvalue of the graph's adjacency matrix formula_2, or (increasingly) of the graph's Laplacian matrix due to its Discrete Laplace operator, which is either formula_372 (sometimes called the "combinatorial Laplacian") or formula_373 (sometimes called the "normalized Laplacian"), where formula_214 is a diagonal matrix with formula_375 equal to the degree of vertex formula_376, and in formula_377, the formula_16th diagonal entry is formula_379. The formula_114th principal eigenvector of a graph is defined as either the eigenvector corresponding to the formula_114th largest or formula_114th smallest eigenvalue of the Laplacian. The first principal eigenvector of the graph is also referred to merely as the principal eigenvector.
The principal eigenvector is used to measure the centrality of its vertices. An example is Google's PageRank algorithm. The principal eigenvector of a modified adjacency matrix of the World Wide Web graph gives the page ranks as its components. This vector corresponds to the stationary distribution of the Markov chain represented by the row-normalized adjacency matrix; however, the adjacency matrix must first be modified to ensure a stationary distribution exists. The second smallest eigenvector can be used to partition the graph into clusters, via spectral clustering. Other methods are also available for clustering.
Basic reproduction number.
The basic reproduction number (formula_383) is a fundamental number in the study of how infectious diseases spread. If one infectious person is put into a population of completely susceptible people, then formula_383 is the average number of people that one typical infectious person will infect. The generation time of an infection is the time, formula_385, from one person becoming infected to the next person becoming infected. In a heterogeneous population, the next generation matrix defines how many people in the population will become infected after time formula_385 has passed. formula_383 is then the largest eigenvalue of the next generation matrix.

</doc>
<doc id="424540" url="https://en.wikipedia.org/wiki?curid=424540" title="Einstein field equations">
Einstein field equations

The Einstein field equations (EFE; also known as "Einstein's equations") are the set of 10 equations in Albert Einstein's general theory of relativity that describes the fundamental interaction of gravitation as a result of spacetime being curved by matter and energy. First published by Einstein in 1915 as a tensor equation, the EFE equate local spacetime curvature (expressed by the Einstein tensor) with the local energy and momentum within that spacetime (expressed by the stress–energy tensor).
Similar to the way that electromagnetic fields are determined using charges and currents via Maxwell's equations, the EFE are used to determine the spacetime geometry resulting from the presence of mass–energy and linear momentum, that is, they determine the metric tensor of spacetime for a given arrangement of stress–energy in the spacetime. The relationship between the metric tensor and the Einstein tensor allows the EFE to be written as a set of non-linear partial differential equations when used in this way. The solutions of the EFE are the components of the metric tensor. The inertial trajectories of particles and radiation (geodesics) in the resulting geometry are then calculated using the geodesic equation.
As well as obeying local energy–momentum conservation, the EFE reduce to Newton's law of gravitation where the gravitational field is weak and velocities are much less than the speed of light.
Exact solutions for the EFE can only be found under simplifying assumptions such as symmetry. Special classes of exact solutions are most often studied as they model many gravitational phenomena, such as rotating black holes and the expanding universe. Further simplification is achieved in approximating the actual spacetime as flat spacetime with a small deviation, leading to the linearised EFE. These equations are used to study phenomena such as gravitational waves.
Mathematical form.
The Einstein field equations (EFE) may be written in the form:
where formula_1 is the Ricci curvature tensor, formula_2 is the scalar curvature, formula_3 is the metric tensor, formula_4 is the cosmological constant, formula_5 is Newton's gravitational constant, formula_6 is the speed of light in vacuum, and formula_7 is the stress–energy tensor.
The EFE is a tensor equation relating a set of symmetric 4×4 tensors. Each tensor has 10 independent components. The four Bianchi identities reduce the number of independent equations from 10 to 6, leaving the metric with four gauge fixing degrees of freedom, which correspond to the freedom to choose a coordinate system.
Although the Einstein field equations were initially formulated in the context of a four-dimensional theory, some theorists have explored their consequences in "n" dimensions. The equations in contexts outside of general relativity are still referred to as the Einstein field equations. The vacuum field equations (obtained when "T" is identically zero) define Einstein manifolds.
Despite the simple appearance of the equations they are actually quite complicated. Given a specified distribution of matter and energy in the form of a stress–energy tensor, the EFE are understood to be equations for the metric tensor formula_8, as both the Ricci tensor and scalar curvature depend on the metric in a complicated nonlinear manner. In fact, when fully written out, the EFE are a system of 10 coupled, nonlinear, hyperbolic-elliptic partial differential equations.
One can write the EFE in a more compact form by defining the Einstein tensor
which is a symmetric second-rank tensor that is a function of the metric. The EFE can then be written as
Using geometrized units where "G" = "c" = 1, this can be rewritten as
The expression on the left represents the curvature of spacetime as determined by the metric; the expression on the right represents the matter/energy content of spacetime. The EFE can then be interpreted as a set of equations dictating how matter/energy determines the curvature of spacetime.
These equations, together with the geodesic equation, which dictates how freely-falling matter moves through space-time, form the core of the mathematical formulation of general relativity.
Sign convention.
The above form of the EFE is the standard established by Misner, Thorne, and Wheeler. The authors analyzed all conventions that exist and classified according to the following three signs (S1, S2, S3):
The third sign above is related to the choice of convention for the Ricci tensor:
With these definitions Misner, Thorne, and Wheeler classify themselves as formula_14, whereas Weinberg (1972) is formula_15, Peebles (1980) and Efstathiou (1990) are formula_16 while Peacock (1994), Rindler (1977), Atwater (1974), Collins Martin & Squires (1989) are formula_17.
Authors including Einstein have used a different sign in their definition for the Ricci tensor which results in the sign of the constant on the right side being negative
The sign of the (very small) cosmological term would change in both these versions, if the +−−− metric sign convention is used rather than the MTW −+++ metric sign convention adopted here.
Equivalent formulations.
Taking the trace with respect to the metric of both sides of the EFE one gets
where formula_20 is the spacetime dimension. This expression can be rewritten as
If one adds formula_22 times this to the EFE, one gets the following equivalent "trace-reversed" form
For example, in formula_24 dimensions this reduces to
Reversing the trace again would restore the original EFE. The trace-reversed form may be more convenient in some cases (for example, when one is interested in weak-field limit and can replace formula_26 in the expression on the right with the Minkowski metric without significant loss of accuracy).
The cosmological constant.
Einstein modified his original field equations to include a cosmological constant term formula_27 proportional to the metric
Since formula_27 is constant, the energy conservation law is unaffected.
The cosmological constant term was originally introduced by Einstein to allow for a universe that is not expanding or contracting. This effort was unsuccessful because:
So, Einstein abandoned formula_27, calling it the "biggest blunder [he] ever made".
Despite Einstein's motivation for introducing the cosmological constant term, there is nothing inconsistent with the presence of such a term in the equations. For many years the cosmological constant was almost universally considered to be 0.
However, recent improved astronomical techniques have found that a positive value of formula_27 is needed to explain the accelerating universe.
Einstein thought of the cosmological constant as an independent parameter, but its term in the field equation can also be moved algebraically to the other side, written as part of the stress–energy tensor:
The resulting vacuum energy is constant and given by
The existence of a cosmological constant is thus equivalent to the existence of a non-zero vacuum energy. Thus, the terms "cosmological constant" and "vacuum energy" are now used interchangeably in general relativity.
Features.
Conservation of energy and momentum.
General relativity is consistent with the local conservation of energy and momentum expressed as
which expresses the local conservation of stress–energy. This conservation law is a physical requirement. With his field equations Einstein ensured that general relativity is consistent with this conservation condition.
Nonlinearity.
The nonlinearity of the EFE distinguishes general relativity from many other fundamental physical theories. For example, Maxwell's equations of electromagnetism are linear in the electric and magnetic fields, and charge and current distributions (i.e. the sum of two solutions is also a solution); another example is Schrödinger's equation of quantum mechanics which is linear in the wavefunction.
The correspondence principle.
The EFE reduce to Newton's law of gravity by using both the weak-field approximation and the slow-motion approximation. In fact, the constant "G" appearing in the EFE is determined by making these two approximations.
Vacuum field equations.
If the energy-momentum tensor formula_35 is zero in the region under consideration, then the field equations are also referred to as the vacuum field equations. By setting formula_36 in the trace-reversed field equations, the vacuum equations can be written as
In the case of nonzero cosmological constant, the equations are
The solutions to the vacuum field equations are called vacuum solutions. Flat Minkowski space is the simplest example of a vacuum solution. Nontrivial examples include the Schwarzschild solution and the Kerr solution.
Manifolds with a vanishing Ricci tensor, formula_39, are referred to as Ricci-flat manifolds and manifolds with a Ricci tensor proportional to the metric as Einstein manifolds.
Einstein–Maxwell equations.
If the energy-momentum tensor formula_35 is that of an electromagnetic field in free space, i.e. if the electromagnetic stress–energy tensor
is used, then the Einstein field equations are called the "Einstein–Maxwell equations" (with cosmological constant Λ, taken to be zero in conventional relativity theory):
Additionally, the covariant Maxwell Equations are also applicable in free space:
where the semicolon represents a covariant derivative, and the brackets denote anti-symmetrization. The first equation asserts that the 4-divergence of the two-form "F" is zero, and the second that its exterior derivative is zero. From the latter, it follows by the Poincaré lemma that in a coordinate chart it is possible to introduce an electromagnetic field potential "A"α such that
in which the comma denotes a partial derivative. This is often taken as equivalent to the covariant Maxwell equation from which it is derived. However, there are global solutions of the equation which may lack a globally defined potential.
Solutions.
The solutions of the Einstein field equations are metrics of spacetime. These metrics describe the structure of the spacetime including the inertial motion of objects in the spacetime. As the field equations are non-linear, they cannot always be completely solved (i.e. without making approximations). For example, there is no known complete solution for a spacetime with two massive bodies in it (which is a theoretical model of a binary star system, for example). However, approximations are usually made in these cases. These are commonly referred to as post-Newtonian approximations. Even so, there are numerous cases where the field equations have been solved completely, and those are called exact solutions.
The study of exact solutions of Einstein's field equations is one of the activities of cosmology. It leads to the prediction of black holes and to different models of evolution of the universe.
One can also discover new solutions of the Einstein field equations via the method of orthonormal frames as pioneered by Ellis and MacCallum. In this approach, the Einstein field equations are reduced to a set of coupled, nonlinear, ordinary differential equations. As discussed by Hsu and Wainwright, self-similar solutions to the Einstein field equations are fixed points of the resulting dynamical system. New solutions have been discovered using these methods by LeBlanc and Kohli and Haslam.
The linearised EFE.
The nonlinearity of the EFE makes finding exact solutions difficult. One way of solving the field equations is to make an approximation, namely, that far from the source(s) of gravitating matter, the gravitational field is very weak and the spacetime approximates that of Minkowski space. The metric is then written as the sum of the Minkowski metric and a term representing the deviation of the true metric from the Minkowski metric, with terms that are quadratic in or higher powers of the deviation being ignored. This linearisation procedure can be used to investigate the phenomena of gravitational radiation.
Polynomial form.
One might think that EFE are non-polynomial since they contain the inverse of the metric tensor. However, the equations can be arranged so that they contain only the metric tensor and not its inverse. First, the determinant of the metric in 4 dimensions can be written:
using the Levi-Civita symbol; and the inverse of the metric in 4 dimensions can be written as:
Substituting this definition of the inverse of the metric into the equations then multiplying both sides by det("g") until there are none left in the denominator results in polynomial equations in the metric tensor and its first and second derivatives. The action from which the equations are derived can also be written in polynomial form by suitable redefinitions of the fields.
References.
See General relativity resources.

</doc>
<doc id="855776" url="https://en.wikipedia.org/wiki?curid=855776" title="Einstein–Hilbert action">
Einstein–Hilbert action

The Einstein–Hilbert action (also referred to as Hilbert action) in general relativity is the action that yields the Einstein field equations through the principle of least action. With the metric signature, the gravitational part of the action is given as
where formula_2 is the determinant of the metric tensor matrix, formula_3 is the Ricci scalar, and formula_4, where formula_5 is the gravitational constant and formula_6 is the speed of light in vacuum. The integral is taken over the whole spacetime if it converges. If it does not converge, formula_7 is no longer well-defined, but a modified definition where one integrates over arbitrarily large, relatively compact domains, still yields the Einstein equation as the Euler–Lagrange equation of the Einstein–Hilbert action.
The action was first proposed by David Hilbert in 1915.
Discussion.
The derivation of equations from an action has several advantages. First of all, it allows for easy unification of general relativity with other classical field theories (such as Maxwell theory), which are also formulated in terms of an action. In the process the derivation from an action identifies a natural candidate for the source term coupling the metric to matter fields. Moreover, the action allows for the easy identification of conserved quantities through Noether's theorem by studying symmetries of the action.
In general relativity, the action is usually assumed to be a functional of the metric (and matter fields), and the connection is given by the Levi-Civita connection. The Palatini formulation of general relativity assumes the metric and connection to be independent, and varies with respect to both independently, which makes it possible to include fermionic matter fields with non-integral spin.
The Einstein equations in the presence of matter are given by adding the matter action to the Hilbert–Einstein action.
Derivation of Einstein's field equations.
Suppose that the full action of the theory is given by the Einstein–Hilbert term plus a term formula_8 describing any matter fields appearing in the theory.
The action principle then tells us that the variation of this action with respect to the inverse metric is zero, yielding 
Since this equation should hold for any variation formula_11, it implies that
is the equation of motion for the metric field. The right hand side of this equation is (by definition) proportional to the stress–energy tensor,
To calculate the left hand side of the equation we need the variations of the Ricci scalar R and the determinant of the metric. These can be obtained by standard text book calculations such as the one given below, which is strongly based on the one given in .
Variation of the Riemann tensor, the Ricci tensor, and the Ricci scalar.
To calculate the variation of the Ricci scalar we calculate first the variation of the Riemann curvature tensor, and then the variation of the Ricci tensor. So, the Riemann curvature tensor is defined as,
Since the Riemann curvature depends only on the Levi-Civita connection formula_15, the variation of the Riemann tensor can be calculated as,
Now, since formula_17 is the difference of two connections, it is a tensor and we can thus calculate its covariant derivative, 
We can now observe that the expression for the variation of Riemann curvature tensor above is equal to the difference of two such terms, 
We may now obtain the variation of the Ricci curvature tensor simply by contracting two indices of the variation of the Riemann tensor, and get the Palatini identity:
The Ricci scalar is defined as
Therefore, its variation with respect to the inverse metric formula_22 is given by
In the second line we used the previously obtained result for the variation of the Ricci curvature and the metric compatibility of the covariant derivative, formula_24.
The last term, formula_25,
multiplied by formula_26 becomes a total derivative, since
and thus by Stokes' theorem only yields a boundary term when integrated. Hence when the variation of the metric formula_11 vanishes at infinity, this term does not contribute to the variation of the action. And we thus obtain,
Variation of the determinant.
Jacobi's formula, the rule for differentiating a determinant, gives:
or one could transform to a coordinate system where formula_31 is diagonal and then apply the product rule to differentiate the product of factors on the main diagonal.
Using this we get
In the last equality we used the fact that
which follows from the rule for differentiating the inverse of a matrix
Thus we conclude that
Equation of motion.
Now that we have all the necessary variations at our disposal, we can insert them into the equation of motion for the metric field to obtain,
which is Einstein's field equation and 
has been chosen such that the non-relativistic limit yields the usual form of Newton's gravity law, where "G" is the gravitational constant (see here for details).
Cosmological constant.
When a cosmological constant Λ is included in the Lagrangian, the action 
yields the field equations:

</doc>
<doc id="29344127" url="https://en.wikipedia.org/wiki?curid=29344127" title="Einstein–Maxwell–Dirac equations">
Einstein–Maxwell–Dirac equations

Einstein–Maxwell–Dirac equations (EMD) are related to quantum field theory. The current Big Bang Model is a quantum field theory in a curved spacetime. Unfortunately, no such theory is mathematically well-defined; in spite of this, theoreticians claim to extract information from this hypothetical theory. On the other hand, the super-classical limit of the not mathematically well-defined QED in a curved spacetime is the mathematically well-defined Einstein–Maxwell–Dirac system. (One could get a similar system for the Standard Model.) As a super theory, EMD violates the positivity condition in the Penrose–Hawking singularity theorems. Thus, it is possible that there would be complete solutions without any singularities – Yau has in fact constructed some. Furthermore, it is known that the Einstein–Maxwell–Dirac system admits of solitonic solutions, i.e., classical electrons and photons. This is the kind of theory Einstein was hoping for. In fact, in 1929 Weyl wrote Einstein that any unified theory would need to include the metric tensor, a gauge field, and a matter field. Einstein considered the Einstein–Maxwell–Dirac system by 1930. He probably didn't develop it because he was unable to geometricize it. It can now be geometricized as a non-commutative geometry; here, the charge e and the mass m of the electron are geometric invariants of the non-commutative geometry analogous to pi.
The Einstein–Yang–Mills–Dirac Equations provide an alternative approach to a Cyclic Universe which Penrose has recently been advocating. They also imply that the massive compact objects now classified as Black Holes are actually Quark Stars, possibly with event horizons, but without singularities.
One way of trying to construct a rigorous QED and beyond is to attempt to apply the deformation quantization program to MD, and more generally, EMD. This would involve the following.
Program for SCESM.
"The Super-Classical Einstein-Standard Model":

</doc>
<doc id="9804" url="https://en.wikipedia.org/wiki?curid=9804" title="Electric charge">
Electric charge

Electric charge is the physical property of matter that causes it to experience a force when placed in an electromagnetic field. There are two types of electric charges: positive and negative. Positively charged substances are repelled from other positively charged substances, but attracted to negatively charged substances; negatively charged substances are repelled from negative and attracted to positive. An object is negatively charged if it has an excess of electrons, and is otherwise positively charged or uncharged. The SI derived unit of electric charge is the coulomb (C), although in electrical engineering it is also common to use the ampere-hour (Ah), and in chemistry it is common to use the elementary charge ("e") as a unit. The symbol "Q" is often used to denote charge. The early knowledge of how charged substances interact is now called classical electrodynamics, and is still very accurate if quantum effects do not need to be considered.
The "electric charge" is a fundamental conserved property of some subatomic particles, which determines their electromagnetic interaction. Electrically charged matter is influenced by, and produces, electromagnetic fields. The interaction between a moving charge and an electromagnetic field is the source of the electromagnetic force, which is one of the four fundamental forces (See also: magnetic field).
Twentieth-century experiments demonstrated that electric charge is "quantized"; that is, it comes in integer multiples of individual small units called the elementary charge, "e", approximately equal to (except for particles called quarks, which have charges that are integer multiples of "e/3"). The proton has a charge of +"e", and the electron has a charge of −"e". The study of charged particles, and how their interactions are mediated by photons, is called quantum electrodynamics.
Overview.
Charge is the fundamental property of forms of matter that exhibit electrostatic attraction or repulsion in the presence of other matter.
Electric charge is a characteristic property of many subatomic particles. The charges of free-standing particles are integer multiples of the elementary charge "e"; we say that electric charge is "quantized". Michael Faraday, in his electrolysis experiments, was the first to note the discrete nature of electric charge. Robert Millikan's oil-drop experiment demonstrated this fact directly, and measured the elementary charge.
By convention, the charge of an electron is −1, while that of a proton is +1. Charged particles whose charges have the same sign repel one another, and particles whose charges have different signs attract. Coulomb's law quantifies the electrostatic force between two particles by asserting that the force is proportional to the product of their charges, and inversely proportional to the square of the distance between them.
The charge of an antiparticle equals that of the corresponding particle, but with opposite sign. Quarks have fractional charges of either − or +, but free-standing quarks have never been observed (the theoretical reason for this fact is asymptotic freedom).
The electric charge of a macroscopic object is the sum of the electric charges of the particles that make it up. This charge is often small, because matter is made of atoms, and atoms typically have equal numbers of protons and electrons, in which case their charges cancel out, yielding a net charge of zero, thus making the atom neutral.
An "ion" is an atom (or group of atoms) that has lost one or more electrons, giving it a net positive charge (cation), or that has gained one or more electrons, giving it a net negative charge (anion). "Monatomic ions" are formed from single atoms, while "polyatomic ions" are formed from two or more atoms that have been bonded together, in each case yielding an ion with a positive or negative net charge.
During formation of macroscopic objects, constituent atoms and ions usually combine to form structures composed of neutral "ionic compounds" electrically bound to neutral atoms. Thus macroscopic objects tend toward being neutral overall, but macroscopic objects are rarely perfectly net neutral.
Sometimes macroscopic objects contain ions distributed throughout the material, rigidly bound in place, giving an overall net positive or negative charge to the object. Also, macroscopic objects made of conductive elements, can more or less easily (depending on the element) take on or give off electrons, and then maintain a net negative or positive charge indefinitely. When the net electric charge of an object is non-zero and motionless, the phenomenon is known as static electricity. This can easily be produced by rubbing two dissimilar materials together, such as rubbing amber with fur or glass with silk. In this way non-conductive materials can be charged to a significant degree, either positively or negatively. Charge taken from one material is moved to the other material, leaving an opposite charge of the same magnitude behind. The law of "conservation of charge" always applies, giving the object from which a negative charge has been taken a positive charge of the same magnitude, and vice versa.
Even when an object's net charge is zero, charge can be distributed non-uniformly in the object (e.g., due to an external electromagnetic field, or bound polar molecules). In such cases the object is said to be polarized. The charge due to polarization is known as bound charge, while charge on an object produced by electrons gained or lost from outside the object is called "free charge". The motion of electrons in conductive metals in a specific direction is known as electric current.
Units.
The SI unit of quantity of electric charge is the coulomb, which is equivalent to about ("e" is the charge of a proton). Hence, the charge of an electron is approximately . The coulomb is defined as the quantity of charge that has passed through the cross section of an electrical conductor carrying one ampere within one second. The symbol "Q" is often used to denote a quantity of electricity or charge. The quantity of electric charge can be directly measured with an electrometer, or indirectly measured with a ballistic galvanometer.
After finding the quantized character of charge, in 1891 George Stoney proposed the unit 'electron' for this fundamental unit of electrical charge. This was before the discovery of the particle by J.J. Thomson in 1897. The unit is today treated as nameless, referred to as "elementary charge", "fundamental unit of charge", or simply as "e". A measure of charge should be a multiple of the elementary charge "e", even if at large scales charge seems to behave as a real quantity. In some contexts it is meaningful to speak of fractions of a charge; for example in the charging of a capacitor, or in the fractional quantum Hall effect.
In systems of units other than SI such as cgs, electric charge is expressed as combination of only three fundamental quantities such as length, mass and time and not four as in SI where electric charge is a combination of length, mass, time and electric current.
History.
As reported by the ancient Greek mathematician Thales of Miletus around 600 BC, charge (or "electricity") could be accumulated by rubbing fur on various substances, such as amber. The Greeks noted that the charged amber buttons could attract light objects such as hair. They also noted that if they rubbed the amber for long enough, they could even get an electric spark to jump. This property derives from the triboelectric effect.
In 1600, the English scientist William Gilbert returned to the subject in "De Magnete", and coined the New Latin word "electricus" from (ēlektron), the Greek word for "amber", which soon gave rise to the English words "electric" and "electricity." He was followed in 1660 by Otto von Guericke, who invented what was probably the first electrostatic generator. Other European pioneers were Robert Boyle, who in 1675 stated that electric attraction and repulsion can act across a vacuum; Stephen Gray, who in 1729 classified materials as conductors and insulators; and C. F. du Fay, who proposed in 1733 that electricity comes in two varieties that cancel each other, and expressed this in terms of a two-fluid theory. When glass was rubbed with silk, du Fay said that the glass was charged with "vitreous electricity", and, when amber was rubbed with fur, the amber was said to be charged with "resinous electricity". In 1839, Michael Faraday showed that the apparent division between static electricity, current electricity, and bioelectricity was incorrect, and all were a consequence of the behavior of a single kind of electricity appearing in opposite polarities. It is arbitrary which polarity is called positive and which is called negative. Positive charge can be defined as the charge left on a glass rod after being rubbed with silk.
One of the foremost experts on electricity in the 18th century was Benjamin Franklin, who argued in favour of a one-fluid theory of electricity. Franklin imagined electricity as being a type of invisible fluid present in all matter; for example, he believed that it was the glass in a Leyden jar that held the accumulated charge. He posited that rubbing insulating surfaces together caused this fluid to change location, and that a flow of this fluid constitutes an electric current. He also posited that when matter contained too little of the fluid it was "negatively" charged, and when it had an excess it was "positively" charged. For a reason that was not recorded, he identified the term "positive" with vitreous electricity and "negative" with resinous electricity. William Watson arrived at the same explanation at about the same time.
Static electricity and electric current.
Static electricity and electric current are two separate phenomena. They both involve electric charge, and may occur simultaneously in the same object. Static electricity refers to the electric charge of an object and the related electrostatic discharge when two objects are brought together that are not at equilibrium. An electrostatic discharge creates a change in the charge of each of the two objects. In contrast, electric current is the flow of electric charge through an object, which produces no net loss or gain of electric charge.
Electrification by friction.
When a piece of glass and a piece of resin—neither of which exhibit any electrical properties—are rubbed together and left with the rubbed surfaces in contact, they still exhibit no electrical properties. When separated, they attract each other.
A second piece of glass rubbed with a second piece of resin, then separated and suspended near the former pieces of glass and resin causes these phenomena:
This attraction and repulsion is an "electrical phenomena," and the bodies that exhibit them are said to be "electrified", or "electrically charged". Bodies may be electrified in many other ways, as well as by friction. The electrical properties of the two pieces of glass are similar to each other but opposite to those of the two pieces of resin: The glass attracts what the resin repels and repels what the resin attracts.
If a body electrified in any manner whatsoever behaves as the glass does, that is, if it repels the glass and attracts the resin, the body is said to be 'vitreously' electrified, and if it attracts the glass and repels the resin it is said to be 'resinously' electrified. All electrified bodies are found to be either vitreously or resinously electrified.
It is the established convention of the scientific community to define the vitreous electrification as positive, and the resinous electrification as negative. The exactly opposite properties of the two kinds of electrification justify our indicating them by opposite signs, but the application of the positive sign to one rather than to the other kind must be considered as a matter of arbitrary convention, just as it is a matter of convention in mathematical diagram to reckon positive distances towards the right hand.
No force, either of attraction or of repulsion, can be observed between an electrified body and a body not electrified.
Actually, all bodies are electrified, but may appear not to be so by the relative similar charge of neighboring objects in the environment. An object further electrified + or – creates an equivalent or opposite charge by default in neighboring objects, until those charges can equalize. The effects of attraction can be observed in high-voltage experiments, while lower voltage effects are merely weaker and therefore less obvious. The attraction and repulsion forces are codified by Coulomb's Law (attraction falls off at the square of the distance, which has a corollary for acceleration in a gravitational field, suggesting that gravitation may be merely electrostatic phenomenon between relatively weak charges in terms of scale). See also the Casimir effect.
It is now known that the Franklin/Watson model was fundamentally correct. There is only one kind of electrical charge, and only one variable is required to keep track of the amount of charge. On the other hand, just knowing the charge is not a complete description of the situation. Matter is composed of several kinds of electrically charged particles, and these particles have many properties, not just charge.
The most common charge carriers are the positively charged proton and the negatively charged electron. The movement of any of these charged particles constitutes an electric current. In many situations, it suffices to speak of the "conventional current" without regard to whether it is carried by positive charges moving in the direction of the conventional current or by negative charges moving in the opposite direction. This macroscopic viewpoint is an approximation that simplifies electromagnetic concepts and calculations.
At the opposite extreme, if one looks at the microscopic situation, one sees there are many ways of carrying an electric current, including: a flow of electrons; a flow of electron "holes" that act like positive particles; and both negative and positive particles (ions or other charged particles) flowing in opposite directions in an electrolytic solution or a plasma.
Beware that, in the common and important case of metallic wires, the direction of the conventional current is opposite to the drift velocity of the actual charge carriers, i.e., the electrons. This is a source of confusion for beginners.
Properties.
Aside from the properties described in articles about electromagnetism, charge is a relativistic invariant. This means that any particle that has charge "Q", no matter how fast it goes, always has charge "Q". This property has been experimentally verified by showing that the charge of "one" helium nucleus (two protons and two neutrons bound together in a nucleus and moving around at high speeds) is the same as "two" deuterium nuclei (one proton and one neutron bound together, but moving much more slowly than they would if they were in a helium nucleus).
Conservation of electric charge.
The total electric charge of an isolated system remains constant regardless of changes within the system itself. This law is inherent to all processes known to physics and can be derived in a local form from gauge invariance of the wave function. The conservation of charge results in the charge-current continuity equation. More generally, the net change in charge density "ρ" within a volume of integration "V" is equal to the area integral over the current density J through the closed surface "S" = ∂"V", which is in turn equal to the net current "I":
Thus, the conservation of electric charge, as expressed by the continuity equation, gives the result:
The charge transferred between times formula_2 and formula_3 is obtained by integrating both sides:
where "I" is the net outward current through a closed surface and "Q" is the electric charge contained within the volume defined by the surface.

</doc>
<doc id="6207" url="https://en.wikipedia.org/wiki?curid=6207" title="Electric current">
Electric current

An electric current is a flow of electric charge. In electric circuits this charge is often carried by moving electrons in a wire. It can also be carried by ions in an electrolyte, or by both ions and electrons such as in a plasma.
The SI unit for measuring an electric current is the ampere, which is the flow of electric charge across a surface at the rate of one coulomb per second. Electric current is measured using a device called an ammeter.
Electric currents cause Joule heating, which creates light in incandescent light bulbs. They also create magnetic fields, which are used in motors, inductors and generators.
The particles that carry the charge in an electric current are called charge carriers. In metals, one or more electrons from each atom are loosely bound to the atom, and can move freely about within the metal. These conduction electrons are the charge carriers in metal conductors.
Symbol.
The conventional symbol for current is , which originates from the French phrase "intensité de courant", meaning "current intensity". Current intensity is often referred to simply as "current". The symbol was used by André-Marie Ampère, after whom the unit of electric current is named, in formulating the eponymous Ampère's force law, which he discovered in 1820. The notation travelled from France to Great Britain, where it became standard, although at least one journal did not change from using to until 1896.
Conventions.
In metals, which make up the wires and other conductors in most electrical circuits, the positively charged atomic nuclei are held in a fixed position, and the electrons are free to move, carrying their charge from one place to another. In other materials, notably the semiconductors, the charge carriers can be positive "or" negative, depending on the dopant used. Positive and negative charge carriers may even be present at the same time, as happens in an electrochemical cell.
A flow of positive charges gives the same electric current, and has the same effect in a circuit, as an equal flow of negative charges in the opposite direction. Since current can be the flow of either positive or negative charges, or both, a convention is needed for the direction of current that is independent of the type of charge carriers. The direction of "conventional current" is arbitrarily defined as the same direction as positive charges flow.
The consequence of this convention is that electrons, the charge carriers in metal wires and most other parts of electric circuits, flow in the opposite direction of conventional current flow in an electrical circuit.
Reference direction.
Since the current in a wire or component can flow in either direction, when a variable is defined to represent that current, the direction representing positive current must be specified, usually by an arrow on the circuit schematic diagram. This is called the "reference direction" of current . If the current flows in the opposite direction, the variable has a negative value.
When analyzing electrical circuits, the actual direction of current through a specific circuit element is usually unknown. Consequently, the reference directions of currents are often assigned arbitrarily. When the circuit is solved, a negative value for the variable means that the actual direction of current through that circuit element is opposite that of the chosen reference direction.
In electronic circuits, the reference current directions are often chosen so that all currents are toward ground. This often corresponds to the actual current direction, because in many circuits the power supply voltage is positive with respect to ground.
Ohm's law.
Ohm's law states that the current through a conductor between two points is directly proportional to the potential difference across the two points. Introducing the constant of proportionality, the resistance, one arrives at the usual mathematical equation that describes this relationship:
where "I" is the current through the conductor in units of amperes, "V" is the potential difference measured "across" the conductor in units of volts, and "R" is the resistance of the conductor in units of ohms. More specifically, Ohm's law states that the "R" in this relation is constant, independent of the current.
AC and DC.
The abbreviations "AC" and "DC" are often used to mean simply "alternating" and "direct", as when they modify "current" or "voltage".
Direct current.
Direct current (DC) is the unidirectional flow of electric charge. Direct current is produced by sources such as batteries, thermocouples, solar cells, and commutator-type electric machines of the dynamo type. Direct current may flow in a conductor such as a wire, but can also flow through semiconductors, insulators, or even through a vacuum as in electron or ion beams. The electric charge flows in a constant direction, distinguishing it from alternating current (AC). A term formerly used for "direct current" was galvanic current.
Alternating current.
In alternating current (AC, also ac), the movement of electric charge periodically reverses direction. In direct current (DC, also dc), the flow of electric charge is only in one direction.
AC is the form of electric power delivered to businesses and residences. The usual waveform of an AC power circuit is a sine wave. Certain applications use different waveforms, such as triangular or square waves. Audio and radio signals carried on electrical wires are also examples of alternating current. An important goal in these applications is recovery of information encoded (or "modulated") onto the AC signal.
Occurrences.
Natural observable examples of electrical current include lightning, static electricity, and the solar wind, the source of the polar auroras.
Man-made occurrences of electric current include the flow of conduction electrons in metal wires such as the overhead power lines that deliver electrical energy across long distances and the smaller wires within electrical and electronic equipment. Eddy currents are electric currents that occur in conductors exposed to changing magnetic fields. Similarly, electric currents occur, particularly in the surface, of conductors exposed to electromagnetic waves. When oscillating electric currents flow at the correct voltages within radio antennas, radio waves are generated.
In electronics, other forms of electric current include the flow of electrons through resistors or through the vacuum in a vacuum tube, the flow of ions inside a battery or a neuron, and the flow of holes within a semiconductor.
Current measurement.
Current can be measured using an ammeter.
At the circuit level, there are various techniques that can be used to measure current:
Resistive heating.
Joule heating, also known as "ohmic heating" and "resistive heating", is the process by which the passage of an electric current through a conductor releases heat. It was first studied by James Prescott Joule in 1841. Joule immersed a length of wire in a fixed mass of water and measured the temperature rise due to a known current through the wire for a 30 minute period. By varying the current and the length of the wire he deduced that the heat produced was proportional to the square of the current multiplied by the electrical resistance of the wire.
This relationship is known as Joule's First Law. The SI unit of energy was subsequently named the joule and given the symbol "J". The commonly known unit of power, the watt, is equivalent to one joule per second.
Electromagnetism.
Electromagnet.
Electric current produces a magnetic field. The magnetic field can be visualized as a pattern of circular field lines surrounding the wire that persists as long as there is current.
Magnetism can also produce electric currents. When a changing magnetic field is applied to a conductor, an Electromotive force (EMF) is produced, and when there is a suitable path, this causes current.
Electric current can be directly measured with a galvanometer, but this method involves breaking the electrical circuit, which is sometimes inconvenient. Current can also be measured without breaking the circuit by detecting the magnetic field associated with the current. Devices used for this include Hall effect sensors, current clamps, current transformers, and Rogowski coils.
Radio waves.
When an electric current flows in a suitably shaped conductor at radio frequencies radio waves can be generated. These travel at the speed of light and can cause electric currents in distant conductors.
Conduction mechanisms in various media.
In metallic solids, electric charge flows by means of electrons, from lower to higher electrical potential. In other media, any stream of charged objects (ions, for example) may constitute an electric current. To provide a definition of current independent of the type of charge carriers, "conventional current" is defined as moving in the same direction as the positive charge flow. So, in metals where the charge carriers (electrons) are negative, conventional current is in the opposite direction as the electrons. In conductors where the charge carriers are positive, conventional current is in the same direction as the charge carriers.
In a vacuum, a beam of ions or electrons may be formed. In other conductive materials, the electric current is due to the flow of both positively and negatively charged particles at the same time. In still others, the current is entirely due to positive charge flow. For example, the electric currents in electrolytes are flows of positively and negatively charged ions. In a common lead-acid electrochemical cell, electric currents are composed of positive hydrogen ions (protons) flowing in one direction, and negative sulfate ions flowing in the other. Electric currents in sparks or plasma are flows of electrons as well as positive and negative ions. In ice and in certain solid electrolytes, the electric current is entirely composed of flowing ions.
Metals.
A solid conductive metal contains mobile, or free electrons, which function as conduction electrons. These electrons are bound to the metal lattice but no longer to an individual atom. Metals are particularly conductive because there are a large number of these free electrons, typically one per atom in the lattice. Even with no external electric field applied, these electrons move about randomly due to thermal energy but, on average, there is zero net current within the metal. At room temperature, the average speed of these random motions is 106 metres per second. Given a surface through which a metal wire passes, electrons move in both directions across the surface at an equal rate. As George Gamow wrote in his popular science book, "One, Two, Three...Infinity" (1947), "The metallic substances differ from all other materials by the fact that the outer shells of their atoms are bound rather loosely, and often let one of their electrons go free. Thus the interior of a metal is filled up with a large number of unattached electrons that travel aimlessly around like a crowd of displaced persons. When a metal wire is subjected to electric force applied on its opposite ends, these free electrons rush in the direction of the force, thus forming what we call an electric current."
When a metal wire is connected across the two terminals of a DC voltage source such as a battery, the source places an electric field across the conductor. The moment contact is made, the free electrons of the conductor are forced to drift toward the positive terminal under the influence of this field. The free electrons are therefore the charge carrier in a typical solid conductor.
For a steady flow of charge through a surface, the current "I" (in amperes) can be calculated with the following equation:
where "Q" is the electric charge transferred through the surface over a time "t". If "Q" and "t" are measured in coulombs and seconds respectively, "I" is in amperes.
More generally, electric current can be represented as the rate at which charge flows through a given surface as:
Electrolytes.
Electric currents in electrolytes are flows of electrically charged particles (ions). For example, if an electric field is placed across a solution of Na+ and Cl− (and conditions are right) the sodium ions move towards the negative electrode (cathode), while the chloride ions move towards the positive electrode (anode). Reactions take place at both electrode surfaces, absorbing each ion.
Water-ice and certain solid electrolytes called proton conductors contain positive hydrogen ions ("protons") that are mobile. In these materials, electric currents are composed of moving protons, as opposed to the moving electrons in metals.
In certain electrolyte mixtures, brightly coloured ions are the moving electric charges. The slow progress of the colour makes the current visible.
Gases and plasmas.
In air and other ordinary gases below the breakdown field, the dominant source of electrical conduction is via relatively few mobile ions produced by radioactive gases, ultraviolet light, or cosmic rays. Since the electrical conductivity is low, gases are dielectrics or insulators. However, once the applied electric field approaches the breakdown value, free electrons become sufficiently accelerated by the electric field to create additional free electrons by colliding, and ionizing, neutral gas atoms or molecules in a process called avalanche breakdown. The breakdown process forms a plasma that contains enough mobile electrons and positive ions to make it an electrical conductor. In the process, it forms a light emitting conductive path, such as a spark, arc or lightning.
Plasma is the state of matter where some of the electrons in a gas are stripped or "ionized" from their molecules or atoms. A plasma can be formed by high temperature, or by application of a high electric or alternating magnetic field as noted above. Due to their lower mass, the electrons in a plasma accelerate more quickly in response to an electric field than the heavier positive ions, and hence carry the bulk of the current. The free ions recombine to create new chemical compounds (for example, breaking atmospheric oxygen into single oxygen [O2 → 2O], which then recombine creating ozone [O3]).
Vacuum.
Since a "perfect vacuum" contains no charged particles, it normally behaves as a perfect insulator. However, metal electrode surfaces can cause a region of the vacuum to become conductive by injecting free electrons or ions through either field electron emission or thermionic emission. Thermionic emission occurs when the thermal energy exceeds the metal's work function, while field electron emission occurs when the electric field at the surface of the metal is high enough to cause tunneling, which results in the ejection of free electrons from the metal into the vacuum. Externally heated electrodes are often used to generate an electron cloud as in the filament or indirectly heated cathode of vacuum tubes. Cold electrodes can also spontaneously produce electron clouds via thermionic emission when small incandescent regions (called cathode spots or anode spots) are formed. These are incandescent regions of the electrode surface that are created by a localized high current. These regions may be initiated by field electron emission, but are then sustained by localized thermionic emission once a vacuum arc forms. These small electron-emitting regions can form quite rapidly, even explosively, on a metal surface subjected to a high electrical field. Vacuum tubes and sprytrons are some of the electronic switching and amplifying devices based on vacuum conductivity.
Superconductivity.
Superconductivity is a phenomenon of exactly zero electrical resistance and expulsion of magnetic fields occurring in certain materials when cooled below a characteristic critical temperature. It was discovered by Heike Kamerlingh Onnes on April 8, 1911 in Leiden. Like ferromagnetism and atomic spectral lines, superconductivity is a quantum mechanical phenomenon. It is characterized by the Meissner effect, the complete ejection of magnetic field lines from the interior of the superconductor as it transitions into the superconducting state. The occurrence of the Meissner effect indicates that superconductivity cannot be understood simply as the idealization of "perfect conductivity" in classical physics.
Semiconductor.
In a semiconductor it is sometimes useful to think of the current as due to the flow of positive "holes" (the mobile positive charge carriers that are places where the semiconductor crystal is missing a valence electron). This is the case in a p-type semiconductor. A semiconductor has electrical conductivity intermediate in magnitude between that of a conductor and an insulator. This means a conductivity roughly in the range of 10−2 to 104 siemens per centimeter (S⋅cm−1).
In the classic crystalline semiconductors, electrons can have energies only within certain bands (i.e. ranges of levels of energy). Energetically, these bands are located between the energy of the ground state, the state in which electrons are tightly bound to the atomic nuclei of the material, and the free electron energy, the latter describing the energy required for an electron to escape entirely from the material. The energy bands each correspond to a large number of discrete quantum states of the electrons, and most of the states with low energy (closer to the nucleus) are occupied, up to a particular band called the "valence band". Semiconductors and insulators are distinguished from metals because the valence band in any given metal is nearly filled with electrons under usual operating conditions, while very few (semiconductor) or virtually none (insulator) of them are available in the "conduction band", the band immediately above the valence band.
The ease of exciting electrons in the semiconductor from the valence band to the conduction band depends on the band gap between the bands. The size of this energy band gap serves as an arbitrary dividing line (roughly 4 eV) between semiconductors and insulators.
With covalent bonds, an electron moves by hopping to a neighboring bond. The Pauli exclusion principle requires that the electron be lifted into the higher anti-bonding state of that bond. For delocalized states, for example in one dimension – that is in a nanowire, for every energy there is a state with electrons flowing in one direction and another state with the electrons flowing in the other. For a net current to flow, more states for one direction than for the other direction must be occupied. For this to occur, energy is required, as in the semiconductor the next higher states lie above the band gap. Often this is stated as: full bands do not contribute to the electrical conductivity. However, as a semiconductor's temperature rises above absolute zero, there is more energy in the semiconductor to spend on lattice vibration and on exciting electrons into the conduction band. The current-carrying electrons in the conduction band are known as "free electrons", though they are often simply called "electrons" if that is clear in context.
Current density and Ohm's law.
Current density is a measure of the density of an electric current. It is defined as a vector whose magnitude is the electric current per cross-sectional area. In SI units, the current density is measured in amperes per square metre.
where "formula_6" is current in the conductor, formula_7 is the current density, and formula_8 is the differential cross-sectional area vector.
The current density (current per unit area) "formula_7" in materials with finite resistance is directly proportional to the electric field formula_10 in the medium. The proportionality constant is called the conductivity "formula_11" of the material, whose value depends on the material concerned and, in general, is dependent on the temperature of the material:
The reciprocal of the conductivity "formula_11" of the material is called the resistivity "formula_14" of the material and the above equation, when written in terms of resistivity becomes:
Conduction in semiconductor devices may occur by a combination of drift and diffusion, which is proportional to diffusion constant formula_17 and charge density formula_18. The current density is then:
with formula_20 being the elementary charge and formula_21 the electron density. The carriers move in the direction of decreasing concentration, so for electrons a positive current results for a positive density gradient. If the carriers are holes, replace electron density formula_21 by the negative of the hole density formula_23.
In linear anisotropic materials, "σ", "ρ" and "D" are tensors.
In linear materials such as metals, and under low frequencies, the current density across the conductor surface is uniform. In such conditions, Ohm's law states that the current is directly proportional to the potential difference between two ends (across) of that metal (ideal) resistor (or other ohmic device):
where formula_6 is the current, measured in amperes; formula_26 is the potential difference, measured in volts; and formula_27 is the resistance, measured in ohms. For alternating currents, especially at higher frequencies, skin effect causes the current to spread unevenly across the conductor cross-section, with higher density near the surface, thus increasing the apparent resistance.
Drift speed.
The mobile charged particles within a conductor move constantly in random directions, like the particles of a gas. To create a net flow of charge, the particles must also move together with an average drift rate. Electrons are the charge carriers in metals and they follow an erratic path, bouncing from atom to atom, but generally drifting in the opposite direction of the electric field. The speed they drift at can be calculated from the equation:
where
Typically, electric charges in solids flow slowly. For example, in a copper wire of cross-section 0.5 mm2, carrying a current of 5 A, the drift velocity of the electrons is on the order of a millimetre per second. To take a different example, in the near-vacuum inside a cathode ray tube, the electrons travel in near-straight lines at about a tenth of the speed of light.
Any accelerating electric charge, and therefore any changing electric current, gives rise to an electromagnetic wave that propagates at very high speed outside the surface of the conductor. This speed is usually a significant fraction of the speed of light, as can be deduced from Maxwell's Equations, and is therefore many times faster than the drift velocity of the electrons. For example, in AC power lines, the waves of electromagnetic energy propagate through the space between the wires, moving from a source to a distant load, even though the electrons in the wires only move back and forth over a tiny distance.
The ratio of the speed of the electromagnetic wave to the speed of light in free space is called the velocity factor, and depends on the electromagnetic properties of the conductor and the insulating materials surrounding it, and on their shape and size.
The magnitudes (but, not the natures) of these three velocities can be illustrated by an analogy with the three similar velocities associated with gases.

</doc>
<doc id="30876071" url="https://en.wikipedia.org/wiki?curid=30876071" title="Electric dipole moment">
Electric dipole moment

In physics, the electric dipole moment is a measure of the separation of positive and negative electrical charges in a system of electric charges, that is, a measure of the charge system's overall polarity. The SI units are Coulomb-meter (C m). This article is limited to static phenomena, and does not describe time-dependent or dynamic polarization. The magnitude of dipole moment determines the electric field strength.
Elementary definition.
In the simple case of two point charges, one with charge +"q" and the other one with charge −"q", the electric dipole moment p is: 
where d is the displacement vector pointing from the negative charge to the positive charge. Thus, the electric dipole moment vector p points from the negative charge to the positive charge. 
An idealization of this two-charge system is the electrical point dipole consisting of two (infinite) charges only infinitesimally separated, but with a finite p.
Torque.
An object with an electric dipole moment is subject to a torque τ when placed in an external electric field. The torque tends to align the dipole with the field. A dipole aligned parallel to an electric field has lower potential energy than a dipole making some angle with it. For a spatially uniform electric field E, the torque is given by:
where p is the dipole moment, and the symbol "×" refers to the vector cross product. The field vector and the dipole vector define a plane, and the torque is directed normal to that plane with the direction given by the right-hand rule.
A dipole placed parallel or anti-parallel to the direction in which a non-uniform electric field is increasing will not experience a torque, only a force in the direction of its dipole moment. It can be shown that this force will always be parallel to the dipole moment regardless of parallel or anti-parallel orientation of the dipole.
Expression (general case).
More generally, for a continuous distribution of charge confined to a volume "V", the corresponding expression for the dipole moment is:
where r locates the point of observation and "d"3r0 denotes an elementary volume in "V". For an array of point charges, the charge density becomes a sum of Dirac delta functions:
where each r"i" is a vector from some reference point to the charge "qi". Substitution into the above integration formula provides:
This expression is equivalent to the previous expression in the case of charge neutrality and "N" = 2. For two opposite charges, denoting the location of the positive charge of the pair as r+ and the location of the negative charge as r− :
showing that the dipole moment vector is directed from the negative charge to the positive charge because the position vector of a point is directed outward from the origin to that point.
The dipole moment is most easily understood when the system has an overall neutral charge; for example, a pair of opposite charges, or a neutral conductor in a uniform electric field. For a system of charges with no net charge, visualized as an array of paired opposite charges, the relation for electric dipole moment is:
which is the vector sum of the individual dipole moments of the neutral charge pairs. (Because of overall charge neutrality, the dipole moment is independent of the observer's position r.) Thus, the value of p is independent of the choice of reference point, provided the overall charge of the system is zero.
When discussing the dipole moment of a non-neutral system, such as the dipole moment of the proton, a dependence on the choice of reference point arises. In such cases it is conventional to choose the reference point to be the center of mass of the system, not some arbitrary origin. 
For a charged molecule the center of charge should be the reference point instead of the center of mass. For neutral systems the references point is not important. The dipole moment is an intrinsic property of the system.
Potential and field of an electric dipole.
An ideal dipole consists of two opposite charges with infinitesimal separation. The potential and field of such an ideal dipole are found next as a limiting case of an example of two opposite charges at non-zero separation.
Two closely spaced opposite charges have a potential of the form:
with charge separation, d, defined as
The position relative to their center of mass (assuming equal masses), R, and the unit vector in the direction of R are given by:
Taylor expansion in "d"/"R" (see multipole expansion and quadrupole) allows this potential to be expressed as a series.
where higher order terms in the series are vanishing at large distances, "R", compared to "d". Here, the electric dipole moment p is, as above:
The result for the dipole potential also can be expressed as:
which relates the dipole potential to that of a point charge. A key point is that the potential of the dipole falls off faster with distance "R" than that of the point charge.
The electric field of the dipole is the negative gradient of the potential, leading to:
Thus, although two closely spaced opposite charges are "not quite" an ideal electric dipole (because their potential at short distances is not that of a dipole), at distances much larger than their separation, their dipole moment p appears directly in their potential and field.
As the two charges are brought closer together ("d" is made smaller), the dipole term in the multipole expansion based on the ratio "d"/"R" becomes the only significant term at ever closer distances "R", and in the limit of infinitesimal separation the dipole term in this expansion is all that matters. As "d" is made infinitesimal, however, the dipole charge must be made to increase to hold p constant. This limiting process results in a "point dipole".
Dipole moment density and polarization density.
The dipole moment of an array of charges, 
determines the degree of polarity of the array, but for a neutral array it is simply a vector property of the array with no information about the array's absolute location. The dipole moment "density" of the array p(r) contains both the location of the array and its dipole moment. When it comes time to calculate the electric field in some region containing the array, Maxwell's equations are solved, and the information about the charge array is contained in the "polarization density" P(r) of Maxwell's equations. Depending upon how fine-grained an assessment of the electric field is required, more or less information about the charge array will have to be expressed by P(r). As explained below, sometimes it is sufficiently accurate to take P(r) = p(r). Sometimes a more detailed description is needed (for example, supplementing the dipole moment density with an additional quadrupole density) and sometimes even more elaborate versions of P(r) are necessary.
It now is explored just in what way the polarization density P(r) that enters Maxwell's equations is related to the dipole moment p of an overall neutral array of charges, and also to the "dipole moment density" p(r) (which describes not only the dipole moment, but also the array location). Only static situations are considered in what follows, so P(r) has no time dependence, and there is no displacement current. First is some discussion of the polarization density P(r). That discussion is followed with several particular examples.
A formulation of Maxwell's equations based upon division of charges and currents into "free" and "bound" charges and currents leads to introduction of the D- and P-fields:
where P is called the polarization density. In this formulation, the divergence of this equation yields:
and as the divergence term in E is the "total" charge, and "ρf" is "free charge", we are left with the relation:
with "ρb" as the bound charge, by which is meant the difference between the total and the free charge densities.
As an aside, in the absence of magnetic effects, Maxwell's equations specify that
which implies
Applying Helmholtz decomposition:
for some scalar potential "φ", and:
Suppose the charges are divided into free and bound, and the potential is divided into
Satisfaction of the boundary conditions upon "φ" may be divided arbitrarily between "φf" and "φb" because only the sum "φ" must satisfy these conditions. It follows that P is simply proportional to the electric field due to the charges selected as bound, with boundary conditions that prove convenient. In particular, when "no" free charge is present, one possible choice is P = "ε"0 E.
Next is discussed how several different dipole-moment descriptions of a medium relate to the polarization entering Maxwell's equations.
Medium with charge and dipole densities.
As described next, a model for polarization moment density p(r) results in a polarization
restricted to the same model. For a smoothly varying dipole moment distribution p(r), the corresponding bound charge density is simply
as we will establish shortly via integration by parts. However, if p(r) exhibits an abrupt step in dipole moment at a boundary between two regions, ∇•p(r) results in a surface charge component of bound charge. This surface charge can be treated through a surface integral, or by using discontinuity conditions at the boundary, as illustrated in the various examples below.
As a first example relating dipole moment to polarization, consider a medium made up of a continuous charge density "ρ"(r) and a continuous dipole moment distribution p(r). The potential at a position r is:
which, at the center of a spherical bounding surface is not zero (the "fields" of negative and positive charges on opposite sides of the center add because both fields point the same way) but is instead :
If we suppose the polarization of the dipoles was induced by an external field, the polarization field opposes the applied field and sometimes is called a "depolarization field". In the case when the polarization is "outside" a spherical cavity, the field in the cavity due to the surrounding dipoles is in the "same" direction as the polarization.
In particular, if the electric susceptibility is introduced through the approximation:
where E, in this case and in the following, represent the "external field" which induces the polarization.
Then:
Whenever "χ"(r) is used to model a step discontinuity at the boundary between two regions, the step produces a surface charge layer. For example, integrating along a normal to the bounding surface from a point just interior to one surface to another point just exterior:
where "A"n, "Ω"n indicate the area and volume of an elementary region straddling the boundary between the regions, and formula_32 a unit normal to the surface. The right side vanishes as the volume shrinks, inasmuch as ρb is finite, indicating a discontinuity in E, and therefore a surface charge. That is, where the modeled medium includes a step in permittivity, the polarization density corresponding to the dipole moment density
necessarily includes the contribution of a surface charge.
A physically more realistic modeling of p(r) would have the dipole moment density drop off rapidly, but smoothly to zero at the boundary of the confining region, rather than making a sudden step to zero density. Then the surface charge will not concentrate in an infinitely thin surface, but instead, being the divergence of a smoothly varying dipole moment density, will distribute itself throughout a thin, but finite transition layer.
Dielectric sphere in uniform external electric field.
The above general remarks about surface charge are made more concrete by considering the example of a dielectric sphere in a uniform electric field. The sphere is found to adopt a surface charge related to the dipole moment of its interior.
A uniform external electric field is supposed to point in the "z"-direction, and spherical-polar coordinates are introduced so the potential created by this field is:
The sphere is assumed to be described by a dielectric constant "κ", that is,
and inside the sphere the potential satisfies Laplace's equation. Skipping a few details, the solution inside the sphere is:
while outside the sphere:
At large distances, φ> → φ∞ so "B" = -"E∞ ". Continuity of potential and of the radial component of displacement D = κε0E determine the other two constants. Supposing the radius of the sphere is "R",
As a consequence, the potential is:
which is the potential due to applied field and, in addition, a dipole in the direction of the applied field (the "z"-direction) of dipole moment:
or, per unit volume:
The factor ("κ"-1)/("κ"+2) is called the Clausius-Mossotti factor and shows that the induced polarization flips sign if "κ" < 1. Of course, this cannot happen in this example, but in an example with two different dielectrics "κ" is replaced by the ratio of the inner to outer region dielectric constants, which can be greater or smaller than one. The potential inside the sphere is:
leading to the field inside the sphere:
showing the depolarizing effect of the dipole. Notice that the field inside the sphere is "uniform" and parallel to the applied field. The dipole moment is uniform throughout the interior of the sphere. The surface charge density on the sphere is the difference between the radial field components:
This linear dielectric example shows that the dielectric constant treatment is equivalent to the uniform dipole-moment model and leads to zero charge everywhere except for the surface charge at the boundary of the sphere.
General media.
If observation is confined to regions sufficiently remote from a system of charges, a multipole expansion of the exact polarization density can be made. By truncating this expansion (for example, retaining only the dipole terms, or only the dipole and quadrupole terms, or "etc."), the results of the previous section are regained. In particular, truncating the expansion at the dipole term, the result is indistinguishable from the polarization density generated by a uniform dipole moment confined to the charge region. To the accuracy of this dipole approximation, as shown in the previous section, the dipole moment "density" p(r) (which includes not only p but the location of p) serves as P(r).
At locations "inside" the charge array, to connect an array of paired charges to an approximation involving only a dipole moment density p(r) requires additional considerations. The simplest approximation is to replace the charge array with a model of ideal (infinitesimally spaced) dipoles. In particular, as in the example above that uses a constant dipole moment density confined to a finite region, a surface charge and depolarization field results. A more general version of this model (which allows the polarization to vary with position) is the customary approach using electric susceptibility or electrical permittivity.
A more complex model of the point charge array introduces an effective medium by averaging the microscopic charges; for example, the averaging can arrange that only dipole fields play a role. A related approach is to divide the charges into those nearby the point of observation, and those far enough away to allow a multipole expansion. The nearby charges then give rise to "local field effects". In a common model of this type, the distant charges are treated as a homogeneous medium using a dielectric constant, and the nearby charges are treated only in a dipole approximation. The approximation of a medium or an array of charges by only dipoles and their associated dipole moment density is sometimes called the "point dipole" approximation, the "discrete dipole approximation", or simply the "dipole approximation".
Dipole moments of fundamental particles.
Much experimental work is continuing on measuring the electric dipole moments (EDM) of fundamental and composite particles, namely those of the electron and neutron, respectively. As EDMs violate both the parity (P) and time-reversal (T) symmetries, their values yield a mostly model-independent measure of CP-violation in nature (assuming CPT symmetry is valid). Therefore, values for these EDMs place strong constraints upon the scale of CP-violation that extensions to the standard model of particle physics may allow. Current generations of experiments are designed to be sensitive to the supersymmetry range of EDMs, providing complementary experiments to those done at the LHC.
Indeed, many theories are inconsistent with the current limits and have effectively been ruled out, and established theory permits a much larger value than these limits, leading to the strong CP problem and prompting searches for new particles such as the axion.
Dipole moments of molecules.
Dipole moments in molecules are responsible for the behavior of a substance in the presence of external electric fields. The dipoles tend to be aligned to the external field which can be constant or time-dependent. This effect forms the basis of a modern experimental technique called dielectric spectroscopy.
Dipole moments can be found in common molecules such as water and also in biomolecules such as proteins.
By means of the total dipole moment of some material one can compute the dielectric constant which is related to the more intuitive concept of conductivity. If formula_45 is the total dipole moment of the sample, then the dielectric
constant is given by,
where "k" is a constant and formula_47 is the time correlation function of the total dipole moment. In general the total dipole moment have contributions coming
from translations and rotations of the molecules in the sample,
Therefore, the dielectric constant (and the conductivity) has contributions from both terms. This approach can be generalized to compute the frequency dependent dielectric function.
The dipole moment of a molecule can also be calculated based on the molecular structure using the concept of group contribution methods.

</doc>
<doc id="41092" url="https://en.wikipedia.org/wiki?curid=41092" title="Electric field">
Electric field

The electric field is a component of the electromagnetic field. It is a vector field and is generated by electric charges or time-varying magnetic fields, as described by Maxwell's equations. The concept of an electric field was introduced by Michael Faraday.
Definition.
The electric field formula_1 at a given point is defined as the (vectorial) force formula_2 that would be exerted on a stationary test particle of unit charge by electromagnetic forces (i.e. the Lorentz force). A particle of charge formula_3 would be subject to a force formula_4.
Its SI units are newtons per coulomb (N⋅C−1) or, equivalently, volts per metre (V⋅m−1), which in terms of SI base units are kg⋅m⋅s−3⋅A−1.
Sources of electric field.
Causes and description.
Electric fields are caused by electric charges or varying magnetic fields. The former effect is described by Gauss's law, the latter by Faraday's law of induction, which together are enough to define the behavior of the electric field as a function of charge repartition and magnetic field. However, since the magnetic field is described as a function of electric field, the equations of both fields are coupled and together form Maxwell's equations that describe both fields as a function of charges and currents.
In the special case of a steady state (stationary charges and currents), the Maxwell-Faraday inductive effect disappears. The resulting two equations (Gauss's law formula_5 and Faraday's law with no induction term formula_6), taken together, are equivalent to Coulomb's law, written as formula_7 for a charge density
formula_8 (formula_9 denotes the position in space). Notice that formula_10, the permittivity of vacuum, must be substituted if charges are considered in non-empty media.
Continuous vs. discrete charge repartition.
The equations of electromagnetism are best described in a continuous description. However, charges are sometimes best described as discrete points; for example, some models may describe electrons as punctual sources where charge density is infinite on an infinitesimal section of space.
A charge formula_3 located in formula_12 can be described mathematically as a charge density formula_13, where the Dirac delta function (in three dimensions) is used. Conversely, a charge distribution can be approximated by many small punctual charge.
Superposition principle.
Electric fields satisfy the superposition principle, because Maxwell's equations are linear. As a result, if formula_14 and formula_15 are the electric fields resulting from distribution of charges formula_16 and formula_17, a distribution of charges formula_18 will create an electric field formula_19; for instance, Coulomb's law is linear in charge density as well.
This principle is useful to calculate the field created by multiple point charges. If charges formula_20 are stationary in space at formula_21, in the absence of currents, the superposition principle proves that the resulting field is the sum of fields generated by each particle as described by Coulomb's law:
This suggests similarities between the electric field E and the gravitational field g, or their associated potentials. Mass is sometimes called "gravitational charge" because of that similarity.
Electrostatic and gravitational forces both are central, conservative and obey an inverse-square law.
Uniform fields.
A uniform field is one in which the electric field is constant at every point. It can be approximated by placing two conducting plates parallel to each other and maintaining a voltage (potential difference) between them; it is only an approximation because of boundary effects (near the edge of the planes, electric field is distorted because the plane does not continue). Assuming infinite planes, the magnitude of the electric field "E" is:
where Δ"ϕ" is the potential difference between the plates and "d" is the distance separating the plates. The negative sign arises as positive charges repel, so a positive charge will experience a force away from the positively charged plate, in the opposite direction to that in which the voltage increases. In micro- and nanoapplications, for instance in relation to semiconductors, a typical magnitude of an electric field is in the order of , achieved by applying a voltage of the order of 1 volt between conductors spaced 1 µm apart.
Electrodynamic fields.
Electrodynamic fields are E-fields which do change with time, for instance when charges are in motion.
The electric field cannot be described independently of the magnetic field in that case. If A is the magnetic vector potential, defined so that formula_24, one can still define an electric potential formula_25 such that:
One can recover Faraday's law of induction by taking the curl of that equation
which justifies, a posteriori, the previous form for E.
Energy in the electric field.
If the magnetic field B is nonzero,
The total energy per unit volume stored by the electromagnetic field is
where "ε" is the permittivity of the medium in which the field exists, formula_29 its magnetic permeability, and E and B are the electric and magnetic field vectors.
As E and B fields are coupled, it would be misleading to split this expression into "electric" and "magnetic" contributions. However, in the steady-state case, the fields are no longer coupled (see Maxwell's equations). It makes sense in that case to compute the electrostatic energy per unit volume:
The total energy "U" stored in the electric field in a given volume "V" is therefore
Further extensions.
Definitive equation of vector fields.
In the presence of matter, it is helpful in electromagnetism to extend the notion of the electric field into three vector fields, rather than just one:
where P is the electric polarization – the volume density of electric dipole moments, and D is the electric displacement field. Since E and P are defined separately, this equation can be used to define D. The physical interpretation of D is not as clear as E (effectively the field applied to the material) or P (induced field due to the dipoles in the material), but still serves as a convenient mathematical simplification, since Maxwell's equations can be simplified in terms of free charges and currents.
Constitutive relation.
The E and D fields are related by the permittivity of the material, "ε".
For linear, homogeneous, isotropic materials E and D are proportional and constant throughout the region, there is no position dependence: For inhomogeneous materials, there is a position dependence throughout the material:
For anisotropic materials the E and D fields are not parallel, and so E and D are related by the permittivity tensor (a 2nd order tensor field), in component form:
For non-linear media, E and D are not proportional. Materials can have varying extents of linearity, homogeneity and isotropy.

</doc>
<doc id="1209000" url="https://en.wikipedia.org/wiki?curid=1209000" title="Electric flux">
Electric flux

In electromagnetism, electric flux is the measure of flow of the electric field through a given area. Electric flux is proportional to the number of electric field lines going through a normally perpendicular surface. If the electric field is uniform, the electric flux passing through a surface of vector area is
where is the electric field (having units of ), is its magnitude, is the area of the surface, and is the angle between the electric field lines and the normal (perpendicular) to .
For a non-uniform electric field, the electric flux through a small surface area is given by
(the electric field, , multiplied by the component of area parallel to the field). The electric flux over a surface is therefore given by the surface integral:
where is the electric field and is a differential area on the closed surface with an outward facing surface normal defining its direction.
For a closed Gaussian surface, electric flux is given by:
where
This relation is known as Gauss' law for electric field in its integral form and it is one of the four Maxwell's equations.
While the electric flux is not affected by charges that are not within the closed surface, the net electric field, , in the Gauss' Law equation, can be affected by charges that lie outside the closed surface. While Gauss' Law holds for all situations, it is only useful for "by hand" calculations when high degrees of symmetry exist in the electric field. Examples include spherical and cylindrical symmetry.
Electrical flux has SI units of volt metres (), or, equivalently, newton metres squared per coulomb (). Thus, the SI base units of electric flux are .
Its dimensional formula is .
See also.
related websites are following:
http://www.citycollegiate.com/coulomb4_XII.htm

</doc>
<doc id="59615" url="https://en.wikipedia.org/wiki?curid=59615" title="Electric potential">
Electric potential

An electric potential (also called the "electric field potential" or the "electrostatic potential") is the amount of electric potential energy that a unitary point electric charge would have if located at any point in space, and is equal to the work done by an electric field in carrying a unit of positive charge from infinity to that point. 
According to theoretical electromagnetics, electric potential is a scalar quantity denoted by (Phi), or , equal to the electric potential energy of any charged particle at any location (measured in joules) divided by the charge of that particle (measured in coulombs). By dividing out the charge on the particle a remainder is obtained that is a property of the electric field itself. 
This value can be calculated in either a static (time-invariant) or a dynamic (varying with time) electric field at a specific time in units of joules per coulomb (), or volts (). The electric potential at infinity is assumed to be zero. 
A generalized electric scalar potential is also used in electrodynamics when time-varying electromagnetic fields are present, but this can not be so simply calculated. The electric potential and the magnetic vector potential together form a four vector, so that the two kinds of potential are mixed under Lorentz transformations.
Introduction.
Classical mechanics explores concepts such as force, energy, potential etc. Force and potential energy are directly related. A net force acting on any object will cause it to accelerate. As an object moves in the direction in which the force accelerates it, its potential energy decreases: the gravitational potential energy of a cannonball at the top of a hill is greater than at the base of the hill. As it rolls downhill its potential energy decreases, being translated to motion, inertial (kinetic) energy. 
It is possible to define the potential of certain force fields so that the potential energy of an object in that field depends only on the position of the object with respect to the field. Two such force fields are the gravitational field and an electric field (in the absence of time-varying magnetic fields). Such fields must affect objects due to the intrinsic properties of the object (e.g., mass or charge) and the position of the object. 
Objects may possess a property known as electric charge and an electric field exerts a force on charged objects. If the charged object has a positive charge the force will be in the direction of the electric field vector at that point while if the charge is negative the force will be in the opposite direction. The magnitude of the force is given by the quantity of the charge multiplied by the magnitude of the electric field vector.
Electrostatics.
The electric potential at a point r in a static electric field E is given by the line integral
where "C" is an arbitrary path connecting the point with zero potential to r. When the curl is zero, the line integral above does not depend on the specific path "C" chosen but only on its endpoints. In this case, the electric field is conservative and determined by the gradient of the potential:
Then, by Gauss's law, the potential satisfies Poisson's equation:
where "ρ" is the total charge density (including bound charge) and ∇· denotes the divergence.
The concept of electric potential is closely linked with potential energy. A test charge "q" has an electric potential energy "U"E given by
The potential energy and hence also the electric potential is only defined up to an additive constant: one must arbitrarily choose a position where the potential energy and the electric potential are zero.
These equations cannot be used if the curl , i.e., in the case of a "nonconservative electric field" (caused by a changing magnetic field; see Maxwell's equations). The generalization of electric potential to this case is described below.
Electric potential due to a point charge.
The electric potential created by a point charge "Q", at a distance "r" from the charge (relative to the potential at infinity), can be shown to be
where "ε"0 is the electric constant (permittivity of vacuum). This is known as the Coulomb potential.
The electric potential due to a system of point charges is equal to the sum of the point charges' individual potentials. This fact simplifies calculations significantly, since addition of potential (scalar) fields is much easier than addition of the electric (vector) fields.
The equation given above for the electric potential (and all the equations used here) are in the forms required by SI units. In some other (less common) systems of units, such as CGS-Gaussian, many of these equations would be altered.
Generalization to electrodynamics.
When time-varying magnetic fields are present (which is true whenever there are time-varying electric fields and vice versa), it is not possible to describe the electric field simply in terms of a scalar potential "V" because the electric field is no longer conservative: formula_4 is path-dependent because formula_5 (Faraday's law of induction).
Instead, one can still define a scalar potential by also including the magnetic vector potential A. In particular, A is defined to satisfy:
where B is the magnetic field. Because the divergence of the magnetic field is always zero due to the absence of magnetic monopoles, such an A can always be found. Given this, the quantity
"is" a conservative field by Faraday's law and one can therefore write
where "V" is the scalar potential defined by the conservative field F.
The electrostatic potential is simply the special case of this definition where A is time-invariant. On the other hand, for time-varying fields, 
unlike electrostatics.
Units.
The SI unit of electric potential is the volt (in honor of Alessandro Volta), which is why a difference in electric potential between two points is known as voltage. Older units are rarely used today. Variants of the centimeter gram second system of units included a number of different units for electric potential, including the abvolt and the statvolt.
Galvani potential versus electrochemical potential.
Inside metals (and other solids and liquids), the energy of an electron is affected not only by the electric potential, but also by the specific atomic environment that it is in. When a voltmeter is connected between two different types of metal, it measures not the electric potential difference, but instead the potential difference corrected for the different atomic environments. The quantity measured by a voltmeter is called electrochemical potential or fermi level, while the pure unadjusted electric potential is sometimes called Galvani potential. The terms "voltage" and "electric potential" are a bit ambiguous in that, in practice, they can refer to "either" of these in different contexts.

</doc>
<doc id="1975956" url="https://en.wikipedia.org/wiki?curid=1975956" title="Electric potential energy">
Electric potential energy

Electric potential energy, or electrostatic potential energy, is a potential energy (measured in joules) that results from conservative Coulomb forces and is associated with the configuration of a particular set of point charges within a defined system. An "object" may have electric potential energy by virtue of two key elements: its own electric charge and its relative position to other electrically charged "objects".
The term "electric potential energy" is used to describe the potential energy in systems with time-variant electric fields, while the term "electrostatic potential energy" is used to describe the potential energy in systems with time-invariant electric fields.
Definition.
We define the electric potential energy of a system of point charges as the work required assembling this system of charges by bringing them close together, as in the system from an infinite distance.
The electrostatic potential energy can also be defined from the electric potential as follows:
Units.
The SI unit of electric potential energy is the joule (named after the English physicist James Prescott Joule). In the CGS system the erg is the unit of energy, being equal to 10−7 J. Also electronvolts may be used, 1 eV = 1.602×10−19 J.
Electrostatic potential energy of one point charge.
One point charge "q" in the presence of one point charge "Q".
The electrostatic potential energy, "UE", of one point charge "q" at position r in the presence of a point charge "Q", taking an infinite separation between the charges as the reference position, is:
where formula_3 is Coulomb's constant, "r" is the distance between the point charges "q" & "Q", and "q" & "Q" are the signed values of the charges (not the modules of the charges. For example, an electron would have a negative value of charge when placed in the formula). The following outline of proof states the derivation from the definition of electric potential energy and Coulomb's law to this formula.
One point charge "q" in the presence of "n" point charges "Qi".
The electrostatic potential energy, "UE", of one point charge "q" in the presence of "n" point charges "Qi", taking an infinite separation between the charges as the reference position, is:
where formula_3 is Coulomb's constant, "ri" is the distance between the point charges "q" & "Qi", and "q" & "Qi" are the signed values of the charges.
Electrostatic potential energy stored in a system of point charges.
The electrostatic potential energy "U"E stored in a system of "N" charges "q"1, "q"2, ..., "qN" at positions r1, r2, ..., r"N" respectively, is:
where, for each "i" value, Φ(r"i") is the electrostatic potential due to all point charges except the one at r"i", and is equal to:
formula_5,
where rij is the distance between qj and ri.
Energy stored in a system of one point charge.
The electrostatic potential energy of a system containing only one point charge is zero, as there are no other sources of electrostatic potential against which an external agent must do work in moving the point charge from infinity to its final location.
Energy stored in a system of two point charges.
Consider bringing a point charge, "q", into its final position in the vicinity of a point charge, "Q"1. The electrostatic potential Φ(r) due to "Q"1 is
Hence we obtain, the electric potential energy of "q" in the potential of "Q"1 as
where "r"1is the separation between the two point charges.
Energy stored in a system of three point charges.
The electrostatic potential energy of a system of three charges should not be confused with the electrostatic potential energy of "Q"1 due to two charges "Q"2 and "Q"3, because the latter doesn't include the electrostatic potential energy of the system of the two charges "Q"2 and "Q"3.
The electrostatic potential energy stored in the system of three charges is:
Energy stored in an electrostatic field distribution.
The energy density, or energy per unit volume, formula_9, of the electrostatic field of a continuous charge distribution is:
Energy in electronic elements.
Some elements in a circuit can convert energy from one form to another. For example, a resistor converts electrical energy to heat, this is known as the Joule effect. A capacitor stores it in its electric field. The total electric potential energy stored in a capacitor is given by
where "C" is the capacitance, "V" is the electric potential difference, and "Q" the charge stored in the capacitor.

</doc>
<doc id="41957" url="https://en.wikipedia.org/wiki?curid=41957" title="Electrical impedance">
Electrical impedance

Electrical impedance is the measure of the opposition that a circuit presents to a current when a voltage is applied.
In quantitative terms, it is the complex ratio of the voltage to the current in an alternating current (AC) circuit. Impedance extends the concept of resistance to AC circuits, and possesses both magnitude and phase, unlike resistance, which has only magnitude. When a circuit is driven with direct current (DC), there is no distinction between impedance and resistance; the latter can be thought of as impedance with zero phase angle.
It is necessary to introduce the concept of impedance in AC circuits because there are two additional impeding mechanisms to be taken into account besides the normal resistance of DC circuits: the induction of voltages in conductors self-induced by the magnetic fields of currents (inductance), and the electrostatic storage of charge induced by voltages between conductors (capacitance). The impedance caused by these two effects is collectively referred to as reactance and forms the imaginary part of complex impedance whereas resistance forms the real part.
The symbol for impedance is usually and it may be represented by writing its magnitude and phase in the form . However, cartesian complex number representation is often more powerful for circuit analysis purposes.
The term "impedance" was coined by Oliver Heaviside in July 1886. Arthur Kennelly was the first to represent impedance with complex numbers in 1893.
Impedance is defined as the frequency domain ratio of the voltage to the current. In other words, it is the voltage–current ratio for a single complex exponential at a particular frequency . In general, impedance will be a complex number, with the same units as resistance, for which the SI unit is the ohm (). For a sinusoidal current or voltage input, the polar form of the complex impedance relates the amplitude and phase of the voltage and current. In particular:
The reciprocal of impedance is admittance (i.e., admittance is the current-to-voltage ratio, and it conventionally carries units of siemens, formerly called mhos).
Complex impedance.
Impedance is represented as a complex quantity formula_1 and the term "complex impedance" may be used interchangeably.
The polar form conveniently captures both magnitude and phase characteristics as
where the magnitude formula_3 represents the ratio of the voltage difference amplitude to the current amplitude, while the argument formula_4 (commonly given the symbol formula_5) gives the phase difference between voltage and current. formula_6 is the imaginary unit, and is used instead of formula_7 in this context to avoid confusion with the symbol for electric current.
In Cartesian form, impedance is defined as
where the real part of impedance is the resistance formula_9 and the imaginary part is the reactance formula_10.
Where it is needed to add or subtract impedances, the cartesian form is more convenient; but when quantities are multiplied or divided, the calculation becomes simpler if the polar form is used. A circuit calculation, such as finding the total impedance of two impedances in parallel, may require conversion between forms several times during the calculation. Conversion between the forms follows the normal conversion rules of complex numbers.
Ohm's law.
The meaning of electrical impedance can be understood by substituting it into Ohm's law.
The magnitude of the impedance formula_3 acts just like resistance, giving the drop in voltage amplitude across an impedance formula_1 for a given current formula_14. The phase factor tells us that the current lags the voltage by a phase of formula_15 (i.e., in the time domain, the current signal is shifted formula_16 later with respect to the voltage signal).
Just as impedance extends Ohm's law to cover AC circuits, other results from DC circuit analysis, such as voltage division, current division, Thévenin's theorem and Norton's theorem, can also be extended to AC circuits by replacing resistance with impedance.
Complex voltage and current.
In order to simplify calculations, sinusoidal voltage and current waves are commonly represented as complex-valued functions of time denoted as formula_17 and formula_14.
Impedance is defined as the ratio of these quantities.
Substituting these into Ohm's law, we have
Noting that this must hold for all formula_22, we may equate the magnitudes and phases to obtain
The magnitude equation is the familiar Ohm's law applied to the voltage and current amplitudes, while the second equation defines the phase relationship.
Validity of complex representation.
This representation using complex exponentials may be justified by noting that (by Euler's formula):
The real-valued sinusoidal function representing either voltage or current may be broken into two complex-valued functions. By the principle of superposition, we may analyse the behaviour of the sinusoid on the left-hand side by analysing the behaviour of the two complex terms on the right-hand side. Given the symmetry, we only need to perform the analysis for one right-hand term; the results will be identical for the other. At the end of any calculation, we may return to real-valued sinusoids by further noting that
Phasors.
A phasor is a constant complex number, usually expressed in exponential form, representing the complex amplitude (magnitude and phase) of a sinusoidal function of time. Phasors are used by electrical engineers to simplify computations involving sinusoids, where they can often reduce a differential equation problem to an algebraic one.
The impedance of a circuit element can be defined as the ratio of the phasor voltage across the element to the phasor current through the element, as determined by the relative amplitudes and phases of the voltage and current. This is identical to the definition from Ohm's law given above, recognising that the factors of formula_26 cancel.
Device examples.
The impedance of an ideal resistor is purely real and is referred to as a "resistive impedance":
In this case, the voltage and current waveforms are proportional and in phase.
Ideal inductors and capacitors have a purely imaginary "reactive impedance":
the impedance of inductors increases as frequency increases;
the impedance of capacitors decreases as frequency increases;
In both cases, for an applied sinusoidal voltage, the resulting current is also sinusoidal, but in quadrature, 90 degrees out of phase with the voltage. However, the phases have opposite signs: in an inductor, the current is "lagging"; in a capacitor the current is "leading".
Note the following identities for the imaginary unit and its reciprocal:
Thus the inductor and capacitor impedance equations can be rewritten in polar form:
The magnitude gives the change in voltage amplitude for a given current amplitude through the impedance, while the exponential factors give the phase relationship.
Deriving the device-specific impedances.
What follows below is a derivation of impedance for each of the three basic circuit elements: the resistor, the capacitor, and the inductor. Although the idea can be extended to define the relationship between the voltage and current of any arbitrary signal, these derivations will assume sinusoidal signals, since any arbitrary signal can be approximated as a sum of sinusoids through Fourier analysis.
Resistor.
For a resistor, there is the relation
which is Ohm's law.
Considering the voltage signal to be
it follows that
This says that the ratio of AC voltage amplitude to alternating current (AC) amplitude across a resistor is formula_9, and that the AC voltage leads the current across a resistor by 0 degrees.
This result is commonly expressed as
Capacitor.
For a capacitor, there is the relation:
Considering the voltage signal to be
it follows that
and thus
This says that the ratio of AC voltage amplitude to AC current amplitude across a capacitor is formula_41, and that the AC voltage lags the AC current across a capacitor by 90 degrees (or the AC current leads the AC voltage across a capacitor by 90 degrees).
This result is commonly expressed in polar form as
or, by applying Euler's formula, as
Inductor.
For the inductor, we have the relation:
This time, considering the current signal to be:
it follows that:
and thus:
This says that the ratio of AC voltage amplitude to AC current amplitude across an inductor is formula_48, and that the AC voltage leads the AC current across an inductor by 90 degrees.
This result is commonly expressed in polar form as
or, using Euler's formula, as
Generalised s-plane impedance.
Impedance defined in terms of "jω" can strictly be applied only to circuits that are driven with a steady-state AC signal. The concept of impedance can be extended to a circuit energised with any arbitrary signal by using complex frequency instead of "jω". Complex frequency is given the symbol "s" and is, in general, a complex number. Signals are expressed in terms of complex frequency by taking the Laplace transform of the time domain expression of the signal. The impedance of the basic circuit elements in this more general notation is as follows:
For a DC circuit, this simplifies to . For a steady-state sinusoidal AC signal .
Resistance vs reactance.
Resistance and reactance together determine the magnitude and phase of the impedance through the following relations:
In many applications, the relative phase of the voltage and current is not critical so only the magnitude of the impedance is significant.
Resistance.
Resistance formula_9 is the real part of impedance; a device with a purely resistive impedance exhibits no phase shift between the voltage and current.
Reactance.
Reactance formula_10 is the imaginary part of the impedance; a component with a finite reactance induces a phase shift formula_55 between the voltage across it and the current through it.
A purely reactive component is distinguished by the sinusoidal voltage across the component being in quadrature with the sinusoidal current through the component. This implies that the component alternately absorbs energy from the circuit and then returns energy to the circuit. A pure reactance will not dissipate any power.
Capacitive reactance.
A capacitor has a purely reactive impedance which is inversely proportional to the signal frequency. A capacitor consists of two conductors separated by an insulator, also known as a dielectric.
At low frequencies, a capacitor is an open circuit so no charge flows in the dielectric.
A DC voltage applied across a capacitor causes charge to accumulate on one side; the electric field due to the accumulated charge is the source of the opposition to the current. When the potential associated with the charge exactly balances the applied voltage, the current goes to zero.
Driven by an AC supply, a capacitor will only accumulate a limited amount of charge before the potential difference changes sign and the charge dissipates. The higher the frequency, the less charge will accumulate and the smaller the opposition to the current.
Inductive reactance.
Inductive reactance formula_58 is proportional to the signal frequency formula_59 and the inductance formula_60.
An inductor consists of a coiled conductor. Faraday's law of electromagnetic induction gives the back emfformula_62 (voltage opposing current) due to a rate-of-change of magnetic flux density formula_63 through a current loop.
Or explicitly in real and imaginary terms:
Parallel combination.
For components connected in parallel, the voltage across each circuit element is the same; the ratio of currents through any two elements is the inverse ratio of their impedances.
Hence the inverse total impedance is the sum of the inverses of the component impedances:
or, when n = 2:
The equivalent impedance formula_69 can be calculated in terms of the equivalent series resistance formula_70 and reactance formula_71.
Measurement.
The measurement of the impedance of devices and transmission lines is a practical problem in radio technology and other fields. Measurements of impedance may be carried out at one frequency, or the variation of device impedance over a range of frequencies may be of interest. The impedance may be measured or displayed directly in ohms, or other values related to impedance may be displayed; for example, in a radio antenna, the standing wave ratio or reflection coefficient may be more useful than the impedance alone. The measurement of impedance requires the measurement of the magnitude of voltage and current, and the phase difference between them. Impedance is often measured by "bridge" methods, similar to the direct-current Wheatstone bridge; a calibrated reference impedance is adjusted to balance off the effect of the impedance of the device under test. Impedance measurement in power electronic devices may require simultaneous measurement and provision of power to the operating device.
The impedance of a device can be calculated by complex division of the voltage and current. The impedance of the device can be calculated by applying a sinusoidal voltage to the device in series with a resistor, and measuring the voltage across the resistor and across the device. Performing this measurement by sweeping the frequencies of the applied signal provides the impedance phase and magnitude.
The use of an impulse response may be used in combination with the fast Fourier transform (FFT) to rapidly measure the electrical impedance of various electrical devices.
The LCR meter (Inductance (L), Capacitance (C), and Resistance (R)) is a device commonly used to measure the inductance, resistance and capacitance of a component; from these values, the impedance at any frequency can be calculated.
Variable impedance.
In general, neither impedance nor admittance can be time varying as they are defined for complex exponentials for –∞ < "t" < +∞. If the complex exponential voltage–current ratio changes over time or amplitude, the circuit element cannot be described using the frequency domain. However, many systems (e.g., varicaps that are used in radio tuners) may exhibit non-linear or time-varying voltage–current ratios that appear to be linear time-invariant (LTI) for small signals over small observation windows; hence, they can be roughly described as having a time-varying impedance. That is, this description is an approximation; over large signal swings or observation windows, the voltage–current relationship is non-LTI and cannot be described by impedance.

</doc>
<doc id="9550" url="https://en.wikipedia.org/wiki?curid=9550" title="Electricity">
Electricity

Electricity is the set of physical phenomena associated with the presence and flow of electric charge. Electricity gives a wide variety of well-known effects, such as lightning, static electricity, electromagnetic induction and electric current. In addition, electricity permits the creation and reception of electromagnetic radiation such as radio waves.
In electricity, charges produce electromagnetic fields which act on other charges. Electricity occurs due to several types of physics:
In electrical engineering, electricity is used for:
Electrical phenomena have been studied since antiquity, though progress in theoretical understanding remained slow until the seventeenth and eighteenth centuries. Even then, practical applications for electricity were few, and it would not be until the late nineteenth century that engineers were able to put it to industrial and residential use. The rapid expansion in electrical technology at this time transformed industry and society. Electricity's extraordinary versatility means it can be put to an almost limitless set of applications which include transport, heating, lighting, communications, and computation. Electrical power is now the backbone of modern industrial society.
History.
Long before any knowledge of electricity existed people were aware of shocks from electric fish. Ancient Egyptian texts dating from 2750 BC referred to these fish as the "Thunderer of the Nile", and described them as the "protectors" of all other fish. Electric fish were again reported millennia later by ancient Greek, Roman and Arabic naturalists and physicians. Several ancient writers, such as Pliny the Elder and Scribonius Largus, attested to the numbing effect of electric shocks delivered by catfish and torpedo rays, and knew that such shocks could travel along conducting objects. Patients suffering from ailments such as gout or headache were directed to touch electric fish in the hope that the powerful jolt might cure them. Possibly the earliest and nearest approach to the discovery of the identity of lightning, and electricity from any other source, is to be attributed to the Arabs, who before the 15th century had the Arabic word for lightning ("raad") applied to the electric ray.
Ancient cultures around the Mediterranean knew that certain objects, such as rods of amber, could be rubbed with cat's fur to attract light objects like feathers. Thales of Miletus made a series of observations on static electricity around 600 BC, from which he believed that friction rendered amber magnetic, in contrast to minerals such as magnetite, which needed no rubbing. Thales was incorrect in believing the attraction was due to a magnetic effect, but later science would prove a link between magnetism and electricity. According to a controversial theory, the Parthians may have had knowledge of electroplating, based on the 1936 discovery of the Baghdad Battery, which resembles a galvanic cell, though it is uncertain whether the artifact was electrical in nature.
Electricity would remain little more than an intellectual curiosity for millennia until 1600, when the English scientist William Gilbert made a careful study of electricity and magnetism, distinguishing the lodestone effect from static electricity produced by rubbing amber. He coined the New Latin word "electricus" ("of amber" or "like amber", from ἤλεκτρον, "elektron", the Greek word for "amber") to refer to the property of attracting small objects after being rubbed. This association gave rise to the English words "electric" and "electricity", which made their first appearance in print in Thomas Browne's "Pseudodoxia Epidemica" of 1646.
Further work was conducted by Otto von Guericke, Robert Boyle, Stephen Gray and C. F. du Fay. In the 18th century, Benjamin Franklin conducted extensive research in electricity, selling his possessions to fund his work. In June 1752 he is reputed to have attached a metal key to the bottom of a dampened kite string and flown the kite in a storm-threatened sky. A succession of sparks jumping from the key to the back of his hand showed that lightning was indeed electrical in nature. He also explained the apparently paradoxical behavior of the Leyden jar as a device for storing large amounts of electrical charge in terms of electricity consisting of both positive and negative charges.
In 1791, Luigi Galvani published his discovery of bioelectricity, demonstrating that electricity was the medium by which nerve cells passed signals to the muscles. Alessandro Volta's battery, or voltaic pile, of 1800, made from alternating layers of zinc and copper, provided scientists with a more reliable source of electrical energy than the electrostatic machines previously used. The recognition of electromagnetism, the unity of electric and magnetic phenomena, is due to Hans Christian Ørsted and
André-Marie Ampère in 1819-1820; Michael Faraday invented the electric motor in 1821, and Georg Ohm mathematically analysed the electrical circuit in 1827. Electricity and magnetism (and light) were definitively linked by James Clerk Maxwell, in particular in his "On Physical Lines of Force" in 1861 and 1862.
While the early 19th century had seen rapid progress in electrical science, the late 19th century would see the greatest progress in electrical engineering. Through such people as Alexander Graham Bell, Ottó Bláthy, Thomas Edison, Galileo Ferraris, Oliver Heaviside, Ányos Jedlik, Lord Kelvin, Sir Charles Parsons, Ernst Werner von Siemens, Joseph Swan, Nikola Tesla and George Westinghouse, electricity turned from a scientific curiosity into an essential tool for modern life, becoming a driving force of the Second Industrial Revolution.
In 1887, Heinrich Hertz discovered that electrodes illuminated with ultraviolet light create electric sparks more easily. In 1905 Albert Einstein published a paper that explained experimental data from the photoelectric effect as being the result of light energy being carried in discrete quantized packets, energising electrons. This discovery led to the quantum revolution. Einstein was awarded the Nobel Prize in 1921 for "his discovery of the law of the photoelectric effect". The photoelectric effect is also employed in photocells such as can be found in solar panels and this is frequently used to make electricity commercially.
The first solid-state device was the "cat's whisker" detector, first used in the 1900s in radio receivers. A whisker-like wire is placed lightly in contact with a solid crystal (such as a germanium crystal) in order to detect a radio signal by the contact junction effect. In a solid-state component, the current is confined to solid elements and compounds engineered specifically to switch and amplify it. Current flow can be understood in two forms: as negatively charged electrons, and as positively charged electron deficiencies called holes. These charges and holes are understood in terms of quantum physics. The building material is most often a crystalline semiconductor.
The solid-state device came into its own with the invention of the transistor in 1947. Common solid-state devices include transistors, microprocessor chips, and RAM. A specialized type of RAM called flash RAM is used in flash drives and more recently, solid state drives to replace mechanically rotating magnetic disc hard drives. Solid state devices became prevalent in the 1950s and the 1960s, during the transition from vacuum tubes to semiconductor diodes, transistors, integrated circuit (IC) and the light-emitting diode (LED).
Concepts.
Electric charge.
The presence of charge gives rise to an electrostatic force: charges exert a force on each other, an effect that was known, though not understood, in antiquity. A lightweight ball suspended from a string can be charged by touching it with a glass rod that has itself been charged by rubbing with a cloth. If a similar ball is charged by the same glass rod, it is found to repel the first: the charge acts to force the two balls apart. Two balls that are charged with a rubbed amber rod also repel each other. However, if one ball is charged by the glass rod, and the other by an amber rod, the two balls are found to attract each other. These phenomena were investigated in the late eighteenth century by Charles-Augustin de Coulomb, who deduced that charge manifests itself in two opposing forms. This discovery led to the well-known axiom: "like-charged objects repel and opposite-charged objects attract".
The force acts on the charged particles themselves, hence charge has a tendency to spread itself as evenly as possible over a conducting surface. The magnitude of the electromagnetic force, whether attractive or repulsive, is given by Coulomb's law, which relates the force to the product of the charges and has an inverse-square relation to the distance between them. The electromagnetic force is very strong, second only in strength to the strong interaction, but unlike that force it operates over all distances. In comparison with the much weaker gravitational force, the electromagnetic force pushing two electrons apart is 1042 times that of the gravitational attraction pulling them together.
Study has shown that the origin of charge is from certain types of subatomic particles which have the property of electric charge. Electric charge gives rise to and interacts with the electromagnetic force, one of the four fundamental forces of nature. The most familiar carriers of electrical charge are the electron and proton. Experiment has shown charge to be a conserved quantity, that is, the net charge within an isolated system will always remain constant regardless of any changes taking place within that system. Within the system, charge may be transferred between bodies, either by direct contact, or by passing along a conducting material, such as a wire. The informal term static electricity refers to the net presence (or 'imbalance') of charge on a body, usually caused when dissimilar materials are rubbed together, transferring charge from one to the other.
The charge on electrons and protons is opposite in sign, hence an amount of charge may be expressed as being either negative or positive. By convention, the charge carried by electrons is deemed negative, and that by protons positive, a custom that originated with the work of Benjamin Franklin. The amount of charge is usually given the symbol "Q" and expressed in coulombs; each electron carries the same charge of approximately −1.6022×10−19 coulomb. The proton has a charge that is equal and opposite, and thus +1.6022×10−19  coulomb. Charge is possessed not just by matter, but also by antimatter, each antiparticle bearing an equal and opposite charge to its corresponding particle.
Charge can be measured by a number of means, an early instrument being the gold-leaf electroscope, which although still in use for classroom demonstrations, has been superseded by the electronic electrometer.
Electric current.
The movement of electric charge is known as an electric current, the intensity of which is usually measured in amperes. Current can consist of any moving charged particles; most commonly these are electrons, but any charge in motion constitutes a current.
By historical convention, a positive current is defined as having the same direction of flow as any positive charge it contains, or to flow from the most positive part of a circuit to the most negative part. Current defined in this manner is called conventional current. The motion of negatively charged electrons around an electric circuit, one of the most familiar forms of current, is thus deemed positive in the "opposite" direction to that of the electrons. However, depending on the conditions, an electric current can consist of a flow of charged particles in either direction, or even in both directions at once. The positive-to-negative convention is widely used to simplify this situation.
The process by which electric current passes through a material is termed electrical conduction, and its nature varies with that of the charged particles and the material through which they are travelling. Examples of electric currents include metallic conduction, where electrons flow through a conductor such as metal, and electrolysis, where ions (charged atoms) flow through liquids, or through plasmas such as electrical sparks. While the particles themselves can move quite slowly, sometimes with an average drift velocity only fractions of a millimetre per second, the electric field that drives them itself propagates at close to the speed of light, enabling electrical signals to pass rapidly along wires.
Current causes several observable effects, which historically were the means of recognising its presence. That water could be decomposed by the current from a voltaic pile was discovered by Nicholson and Carlisle in 1800, a process now known as electrolysis. Their work was greatly expanded upon by Michael Faraday in 1833. Current through a resistance causes localised heating, an effect James Prescott Joule studied mathematically in 1840. One of the most important discoveries relating to current was made accidentally by Hans Christian Ørsted in 1820, when, while preparing a lecture, he witnessed the current in a wire disturbing the needle of a magnetic compass. He had discovered electromagnetism, a fundamental interaction between electricity and magnetics. The level of electromagnetic emissions generated by electric arcing is high enough to produce electromagnetic interference, which can be detrimental to the workings of adjacent equipment.
In engineering or household applications, current is often described as being either direct current (DC) or alternating current (AC). These terms refer to how the current varies in time. Direct current, as produced by example from a battery and required by most electronic devices, is a unidirectional flow from the positive part of a circuit to the negative. If, as is most common, this flow is carried by electrons, they will be travelling in the opposite direction. Alternating current is any current that reverses direction repeatedly; almost always this takes the form of a sine wave. Alternating current thus pulses back and forth within a conductor without the charge moving any net distance over time. The time-averaged value of an alternating current is zero, but it delivers energy in first one direction, and then the reverse. Alternating current is affected by electrical properties that are not observed under steady state direct current, such as inductance and capacitance. These properties however can become important when circuitry is subjected to transients, such as when first energised.
Electric field.
The concept of the electric field was introduced by Michael Faraday. An electric field is created by a charged body in the space that surrounds it, and results in a force exerted on any other charges placed within the field. The electric field acts between two charges in a similar manner to the way that the gravitational field acts between two masses, and like it, extends towards infinity and shows an inverse square relationship with distance. However, there is an important difference. Gravity always acts in attraction, drawing two masses together, while the electric field can result in either attraction or repulsion. Since large bodies such as planets generally carry no net charge, the electric field at a distance is usually zero. Thus gravity is the dominant force at distance in the universe, despite being much weaker.
An electric field generally varies in space, and its strength at any one point is defined as the force (per unit charge) that would be felt by a stationary, negligible charge if placed at that point. The conceptual charge, termed a 'test charge', must be vanishingly small to prevent its own electric field disturbing the main field and must also be stationary to prevent the effect of magnetic fields. As the electric field is defined in terms of force, and force is a vector, so it follows that an electric field is also a vector, having both magnitude and direction. Specifically, it is a vector field.
The study of electric fields created by stationary charges is called electrostatics. The field may be visualised by a set of imaginary lines whose direction at any point is the same as that of the field. This concept was introduced by Faraday, whose term 'lines of force' still sometimes sees use. The field lines are the paths that a point positive charge would seek to make as it was forced to move within the field; they are however an imaginary concept with no physical existence, and the field permeates all the intervening space between the lines. Field lines emanating from stationary charges have several key properties: first, that they originate at positive charges and terminate at negative charges; second, that they must enter any good conductor at right angles, and third, that they may never cross nor close in on themselves.
A hollow conducting body carries all its charge on its outer surface. The field is therefore zero at all places inside the body. This is the operating principal of the Faraday cage, a conducting metal shell which isolates its interior from outside electrical effects.
The principles of electrostatics are important when designing items of high-voltage equipment. There is a finite limit to the electric field strength that may be withstood by any medium. Beyond this point, electrical breakdown occurs and an electric arc causes flashover between the charged parts. Air, for example, tends to arc across small gaps at electric field strengths which exceed 30 kV per centimetre. Over larger gaps, its breakdown strength is weaker, perhaps 1 kV per centimetre. The most visible natural occurrence of this is lightning, caused when charge becomes separated in the clouds by rising columns of air, and raises the electric field in the air to greater than it can withstand. The voltage of a large lightning cloud may be as high as 100 MV and have discharge energies as great as 250 kWh.
The field strength is greatly affected by nearby conducting objects, and it is particularly intense when it is forced to curve around sharply pointed objects. This principle is exploited in the lightning conductor, the sharp spike of which acts to encourage the lightning stroke to develop there, rather than to the building it serves to protect
Electric potential.
The concept of electric potential is closely linked to that of the electric field. A small charge placed within an electric field experiences a force, and to have brought that charge to that point against the force requires work. The electric potential at any point is defined as the energy required to bring a unit test charge from an infinite distance slowly to that point. It is usually measured in volts, and one volt is the potential for which one joule of work must be expended to bring a charge of one coulomb from infinity. This definition of potential, while formal, has little practical application, and a more useful concept is that of electric potential difference, and is the energy required to move a unit charge between two specified points. An electric field has the special property that it is "conservative", which means that the path taken by the test charge is irrelevant: all paths between two specified points expend the same energy, and thus a unique value for potential difference may be stated. The volt is so strongly identified as the unit of choice for measurement and description of electric potential difference that the term voltage sees greater everyday usage.
For practical purposes, it is useful to define a common reference point to which potentials may be expressed and compared. While this could be at infinity, a much more useful reference is the Earth itself, which is assumed to be at the same potential everywhere. This reference point naturally takes the name earth or ground. Earth is assumed to be an infinite source of equal amounts of positive and negative charge, and is therefore electrically uncharged—and unchargeable.
Electric potential is a scalar quantity, that is, it has only magnitude and not direction. It may be viewed as analogous to height: just as a released object will fall through a difference in heights caused by a gravitational field, so a charge will 'fall' across the voltage caused by an electric field. As relief maps show contour lines marking points of equal height, a set of lines marking points of equal potential (known as equipotentials) may be drawn around an electrostatically charged object. The equipotentials cross all lines of force at right angles. They must also lie parallel to a conductor's surface, otherwise this would produce a force that will move the charge carriers to even the potential of the surface.
The electric field was formally defined as the force exerted per unit charge, but the concept of potential allows for a more useful and equivalent definition: the electric field is the local gradient of the electric potential. Usually expressed in volts per metre, the vector direction of the field is the line of greatest slope of potential, and where the equipotentials lie closest together.
Electromagnets.
Ørsted's discovery in 1821 that a magnetic field existed around all sides of a wire carrying an electric current indicated that there was a direct relationship between electricity and magnetism. Moreover, the interaction seemed different from gravitational and electrostatic forces, the two forces of nature then known. The force on the compass needle did not direct it to or away from the current-carrying wire, but acted at right angles to it. Ørsted's slightly obscure words were that "the electric conflict acts in a revolving manner." The force also depended on the direction of the current, for if the flow was reversed, then the force did too.
Ørsted did not fully understand his discovery, but he observed the effect was reciprocal: a current exerts a force on a magnet, and a magnetic field exerts a force on a current. The phenomenon was further investigated by Ampère, who discovered that two parallel current-carrying wires exerted a force upon each other: two wires conducting currents in the same direction are attracted to each other, while wires containing currents in opposite directions are forced apart. The interaction is mediated by the magnetic field each current produces and forms the basis for the international definition of the ampere.
This relationship between magnetic fields and currents is extremely important, for it led to Michael Faraday's invention of the electric motor in 1821. Faraday's homopolar motor consisted of a permanent magnet sitting in a pool of mercury. A current was allowed through a wire suspended from a pivot above the magnet and dipped into the mercury. The magnet exerted a tangential force on the wire, making it circle around the magnet for as long as the current was maintained.
Experimentation by Faraday in 1831 revealed that a wire moving perpendicular to a magnetic field developed a potential difference between its ends. Further analysis of this process, known as electromagnetic induction, enabled him to state the principle, now known as Faraday's law of induction, that the potential difference induced in a closed circuit is proportional to the rate of change of magnetic flux through the loop. Exploitation of this discovery enabled him to invent the first electrical generator in 1831, in which he converted the mechanical energy of a rotating copper disc to electrical energy. Faraday's disc was inefficient and of no use as a practical generator, but it showed the possibility of generating electric power using magnetism, a possibility that would be taken up by those that followed on from his work.
Electrochemistry.
The ability of chemical reactions to produce electricity, and conversely the ability of electricity to drive chemical reactions has a wide array of uses.
Electrochemistry has always been an important part of electricity. From the initial invention of the Voltaic pile, electrochemical cells have evolved into the many different types of batteries, electroplating and electrolysis cells. Aluminium is produced in vast quantities this way, and many portable devices are electrically powered using rechargeable cells.
Electric circuits.
An electric circuit is an interconnection of electric components such that electric charge is made to flow along a closed path (a circuit), usually to perform some useful task.
The components in an electric circuit can take many forms, which can include elements such as resistors, capacitors, switches, transformers and electronics. Electronic circuits contain active components, usually semiconductors, and typically exhibit non-linear behaviour, requiring complex analysis. The simplest electric components are those that are termed passive and linear: while they may temporarily store energy, they contain no sources of it, and exhibit linear responses to stimuli.
The resistor is perhaps the simplest of passive circuit elements: as its name suggests, it resists the current through it, dissipating its energy as heat. The resistance is a consequence of the motion of charge through a conductor: in metals, for example, resistance is primarily due to collisions between electrons and ions. Ohm's law is a basic law of circuit theory, stating that the current passing through a resistance is directly proportional to the potential difference across it. The resistance of most materials is relatively constant over a range of temperatures and currents; materials under these conditions are known as 'ohmic'. The ohm, the unit of resistance, was named in honour of Georg Ohm, and is symbolised by the Greek letter Ω. 1 Ω is the resistance that will produce a potential difference of one volt in response to a current of one amp.
The capacitor is a development of the Leyden jar and is a device that can store charge, and thereby storing electrical energy in the resulting field. It consists of two conducting plates separated by a thin insulating dielectric layer; in practice, thin metal foils are coiled together, increasing the surface area per unit volume and therefore the capacitance. The unit of capacitance is the farad, named after Michael Faraday, and given the symbol "F": one farad is the capacitance that develops a potential difference of one volt when it stores a charge of one coulomb. A capacitor connected to a voltage supply initially causes a current as it accumulates charge; this current will however decay in time as the capacitor fills, eventually falling to zero. A capacitor will therefore not permit a steady state current, but instead blocks it.
The inductor is a conductor, usually a coil of wire, that stores energy in a magnetic field in response to the current through it. When the current changes, the magnetic field does too, inducing a voltage between the ends of the conductor. The induced voltage is proportional to the time rate of change of the current. The constant of proportionality is termed the inductance. The unit of inductance is the henry, named after Joseph Henry, a contemporary of Faraday. One henry is the inductance that will induce a potential difference of one volt if the current through it changes at a rate of one ampere per second. The inductor's behaviour is in some regards converse to that of the capacitor: it will freely allow an unchanging current, but opposes a rapidly changing one.
Electric power.
Electric power is the rate at which electric energy is transferred by an electric circuit. The SI unit of power is the watt, one joule per second.
Electric power, like mechanical power, is the rate of doing work, measured in watts, and represented by the letter "P". The term "wattage" is used colloquially to mean "electric power in watts." The electric power in watts produced by an electric current "I" consisting of a charge of "Q" coulombs every "t" seconds passing through an electric potential (voltage) difference of "V" is
where
Electricity generation is often done with electric generators, but can also be supplied by chemical sources such as electric batteries or by other means from a wide variety of sources of energy. Electric power is generally supplied to businesses and homes by the electric power industry. Electricity is usually sold by the kilowatt hour (3.6 MJ) which is the product of power in kilowatts multiplied by running time in hours. Electric utilities measure power using electricity meters, which keep a running total of the electric energy delivered to a customer.
Electronics.
Electronics deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes and integrated circuits, and associated passive interconnection technologies. The nonlinear behaviour of active components and their ability to control electron flows makes amplification of weak signals possible and electronics is widely used in information processing, telecommunications, and signal processing. The ability of electronic devices to act as switches makes digital information processing possible. Interconnection technologies such as circuit boards, electronics packaging technology, and other varied forms of communication infrastructure complete circuit functionality and transform the mixed components into a regular working system.
Today, most electronic devices use semiconductor components to perform electron control. The study of semiconductor devices and related technology is considered a branch of solid state physics, whereas the design and construction of electronic circuits to solve practical problems come under electronics engineering.
Electromagnetic wave.
Faraday's and Ampère's work showed that a time-varying magnetic field acted as a source of an electric field, and a time-varying electric field was a source of a magnetic field. Thus, when either field is changing in time, then a field of the other is necessarily induced. Such a phenomenon has the properties of a wave, and is naturally referred to as an electromagnetic wave. Electromagnetic waves were analysed theoretically by James Clerk Maxwell in 1864. Maxwell developed a set of equations that could unambiguously describe the interrelationship between electric field, magnetic field, electric charge, and electric current. He could moreover prove that such a wave would necessarily travel at the speed of light, and thus light itself was a form of electromagnetic radiation. Maxwell's Laws, which unify light, fields, and charge are one of the great milestones of theoretical physics.
Thus, the work of many researchers enabled the use of electronics to convert signals into high frequency oscillating currents, and via suitably shaped conductors, electricity permits the transmission and reception of these signals via radio waves over very long distances.
Production and uses.
Generation and transmission.
In the 6th century BC, the Greek philosopher Thales of Miletus experimented with amber rods and these experiments were the first studies into the production of electrical energy. While this method, now known as the triboelectric effect, can lift light objects and generate sparks, it is extremely inefficient. It was not until the invention of the voltaic pile in the eighteenth century that a viable source of electricity became available. The voltaic pile, and its modern descendant, the electrical battery, store energy chemically and make it available on demand in the form of electrical energy. The battery is a versatile and very common power source which is ideally suited to many applications, but its energy storage is finite, and once discharged it must be disposed of or recharged. For large electrical demands electrical energy must be generated and transmitted continuously over conductive transmission lines.
Electrical power is usually generated by electro-mechanical generators driven by steam produced from fossil fuel combustion, or the heat released from nuclear reactions; or from other sources such as kinetic energy extracted from wind or flowing water. The modern steam turbine invented by Sir Charles Parsons in 1884 today generates about 80 percent of the electric power in the world using a variety of heat sources. Such generators bear no resemblance to Faraday's homopolar disc generator of 1831, but they still rely on his electromagnetic principle that a conductor linking a changing magnetic field induces a potential difference across its ends. The invention in the late nineteenth century of the transformer meant that electrical power could be transmitted more efficiently at a higher voltage but lower current. Efficient electrical transmission meant in turn that electricity could be generated at centralised power stations, where it benefited from economies of scale, and then be despatched relatively long distances to where it was needed.
Since electrical energy cannot easily be stored in quantities large enough to meet demands on a national scale, at all times exactly as much must be produced as is required. This requires electricity utilities to make careful predictions of their electrical loads, and maintain constant co-ordination with their power stations. A certain amount of generation must always be held in reserve to cushion an electrical grid against inevitable disturbances and losses.
Demand for electricity grows with great rapidity as a nation modernises and its economy develops. The United States showed a 12% increase in demand during each year of the first three decades of the twentieth century, a rate of growth that is now being experienced by emerging economies such as those of India or China. Historically, the growth rate for electricity demand has outstripped that for other forms of energy.
Environmental concerns with electricity generation have led to an increased focus on generation from renewable sources, in particular from wind and hydropower. While debate can be expected to continue over the environmental impact of different means of electricity production, its final form is relatively clean
Applications.
Electricity is a very convenient way to transfer energy, and it has been adapted to a huge, and growing, number of uses. The invention of a practical incandescent light bulb in the 1870s led to lighting becoming one of the first publicly available applications of electrical power. Although electrification brought with it its own dangers, replacing the naked flames of gas lighting greatly reduced fire hazards within homes and factories. Public utilities were set up in many cities targeting the burgeoning market for electrical lighting.
The Joule heating effect employed in the light bulb also sees more direct use in electric heating. While this is versatile and controllable, it can be seen as wasteful, since most electrical generation has already required the production of heat at a power station. A number of countries, such as Denmark, have issued legislation restricting or banning the use of electric heating in new buildings. Electricity is however a highly practical energy source for refrigeration, with air conditioning representing a growing sector for electricity demand, the effects of which electricity utilities are increasingly obliged to accommodate.
Electricity is used within telecommunications, and indeed the electrical telegraph, demonstrated commercially in 1837 by Cooke and Wheatstone, was one of its earliest applications. With the construction of first intercontinental, and then transatlantic, telegraph systems in the 1860s, electricity had enabled communications in minutes across the globe. Optical fibre and satellite communication have taken a share of the market for communications systems, but electricity can be expected to remain an essential part of the process.
The effects of electromagnetism are most visibly employed in the electric motor, which provides a clean and efficient means of motive power. A stationary motor such as a winch is easily provided with a supply of power, but a motor that moves with its application, such as an electric vehicle, is obliged to either carry along a power source such as a battery, or to collect current from a sliding contact such as a pantograph.
Electronic devices make use of the transistor, perhaps one of the most important inventions of the twentieth century, and a fundamental building block of all modern circuitry. A modern integrated circuit may contain several billion miniaturised transistors in a region only a few centimetres square.
Electricity is also used to fuel public transportation, including electric buses and trains.
Electricity and the natural world.
Physiological effects.
A voltage applied to a human body causes an electric current through the tissues, and although the relationship is non-linear, the greater the voltage, the greater the current. The threshold for perception varies with the supply frequency and with the path of the current, but is about 0.1 mA to 1 mA for mains-frequency electricity, though a current as low as a microamp can be detected as an electrovibration effect under certain conditions. If the current is sufficiently high, it will cause muscle contraction, fibrillation of the heart, and tissue burns. The lack of any visible sign that a conductor is electrified makes electricity a particular hazard. The pain caused by an electric shock can be intense, leading electricity at times to be employed as a method of torture. Death caused by an electric shock is referred to as electrocution. Electrocution is still the means of judicial execution in some jurisdictions, though its use has become rarer in recent times.
Electrical phenomena in nature.
Electricity is not a human invention, and may be observed in several forms in nature, a prominent manifestation of which is lightning. Many interactions familiar at the macroscopic level, such as touch, friction or chemical bonding, are due to interactions between electric fields on the atomic scale. The Earth's magnetic field is thought to arise from a natural dynamo of circulating currents in the planet's core. Certain crystals, such as quartz, or even sugar, generate a potential difference across their faces when subjected to external pressure. This phenomenon is known as piezoelectricity, from the Greek "piezein" (πιέζειν), meaning to press, and was discovered in 1880 by Pierre and Jacques Curie. The effect is reciprocal, and when a piezoelectric material is subjected to an electric field, a small change in physical dimensions takes place.
Some organisms, such as sharks, are able to detect and respond to changes in electric fields, an ability known as electroreception, while others, termed electrogenic, are able to generate voltages themselves to serve as a predatory or defensive weapon. The order Gymnotiformes, of which the best known example is the electric eel, detect or stun their prey via high voltages generated from modified muscle cells called electrocytes. All animals transmit information along their cell membranes with voltage pulses called action potentials, whose functions include communication by the nervous system between neurons and muscles. An electric shock stimulates this system, and causes muscles to contract. Action potentials are also responsible for coordinating activities in certain plants.
Cultural perception.
In 1850, William Gladstone asked the scientist Michael Faraday why electricity was valuable. Faraday answered, “One day sir, you may tax it.”
In the 19th and early 20th century, electricity was not part of the everyday life of many people, even in the industrialised Western world. The popular culture of the time accordingly often depicts it as a mysterious, quasi-magical force that can slay the living, revive the dead or otherwise bend the laws of nature. This attitude began with the 1771 experiments of Luigi Galvani in which the legs of dead frogs were shown to twitch on application of animal electricity. "Revitalization" or resuscitation of apparently dead or drowned persons was reported in the medical literature shortly after Galvani's work. These results were known to Mary Shelley when she authored "Frankenstein" (1819), although she does not name the method of revitalization of the monster. The revitalization of monsters with electricity later became a stock theme in horror films.
As the public familiarity with electricity as the lifeblood of the Second Industrial Revolution grew, its wielders were more often cast in a positive light, such as the workers who "finger death at their gloves' end as they piece and repiece the living wires" in Rudyard Kipling's 1907 poem "Sons of Martha". Electrically powered vehicles of every sort featured large in adventure stories such as those of Jules Verne and the "Tom Swift" books. The masters of electricity, whether fictional or real—including scientists such as Thomas Edison, Charles Steinmetz or Nikola Tesla—were popularly conceived of as having wizard-like powers.
With electricity ceasing to be a novelty and becoming a necessity of everyday life in the later half of the 20th century, it required particular attention by popular culture only when it "stops" flowing, an event that usually signals disaster. The people who "keep" it flowing, such as the nameless hero of Jimmy Webb’s song "Wichita Lineman" (1968), are still often cast as heroic, wizard-like figures.

</doc>
<doc id="9735" url="https://en.wikipedia.org/wiki?curid=9735" title="Electromagnetic field">
Electromagnetic field

An electromagnetic field (also EMF or EM field) is a physical field produced by electrically charged objects. It affects the behavior of charged objects in the vicinity of the field. The electromagnetic field extends indefinitely throughout space and describes the electromagnetic interaction. It is one of the four fundamental forces of nature (the others are gravitation, weak interaction and strong interaction).
The field can be viewed as the combination of an electric field and a magnetic field. The electric field is produced by stationary charges, and the magnetic field by moving charges (currents); these two are often described as the sources of the field. The way in which charges and currents interact with the electromagnetic field is described by Maxwell's equations and the Lorentz force law.
From a classical perspective in the history of electromagnetism, the electromagnetic field can be regarded as a smooth, continuous field, propagated in a wavelike manner; whereas from the perspective of quantum field theory, the field is seen as quantized, being composed of individual particles.
Structure.
The electromagnetic field may be viewed in two distinct ways: a continuous structure or a discrete structure.
Continuous structure.
Classically, electric and magnetic fields are thought of as being produced by smooth motions of charged objects. For example, oscillating charges produce electric and magnetic fields that may be viewed in a 'smooth', continuous, wavelike fashion. In this case, energy is viewed as being transferred continuously through the electromagnetic field between any two locations. For instance, the metal atoms in a radio transmitter appear to transfer energy continuously. This view is useful to a certain extent (radiation of low frequency), but problems are found at high frequencies (see ultraviolet catastrophe).
Discrete structure.
The electromagnetic field may be thought of in a more 'coarse' way. Experiments reveal that in some circumstances electromagnetic energy transfer is better described as being carried in the form of packets called quanta (in this case, photons) with a fixed frequency. Planck's relation links the energy "E" of a photon to its frequency ν through the equation:
where "h" is Planck's constant, and ν is the frequency of the photon . Although modern quantum optics tells us that there also is a semi-classical explanation of the photoelectric effect—the emission of electrons from metallic surfaces subjected to electromagnetic radiation—the photon was historically (although not strictly necessarily) used to explain certain observations. It is found that increasing the intensity of the incident radiation (so long as one remains in the linear regime) increases only the number of electrons ejected, and has almost no effect on the energy distribution of their ejection. Only the frequency of the radiation is relevant to the energy of the ejected electrons.
This quantum picture of the electromagnetic field (which treats it as analogous to harmonic oscillators) has proved very successful, giving rise to quantum electrodynamics, a quantum field theory describing the interaction of electromagnetic radiation with charged matter. It also gives rise to quantum optics, which is different from quantum electrodynamics in that the matter itself is modelled using quantum mechanics rather than quantum field theory.
Dynamics.
In the past, electrically charged objects were thought to produce two different, unrelated types of field associated with their charge property. An electric field is produced when the charge is stationary with respect to an observer measuring the properties of the charge, and a magnetic field as well as an electric field is produced when the charge moves, creating an electric current with respect to this observer. Over time, it was realized that the electric and magnetic fields are better thought of as two parts of a greater whole — the electromagnetic field. Recall that until 1820 (when the Danish physicist H.C. Ørsted discovered the effect of electricity through a wire on a compass needle), electricity and magnetism had been viewed as unrelated phenomena . In 1831, Michael Faraday, one of the great thinkers of his time, made the seminal observation that time-varying magnetic fields could induce electric currents and then, in 1864, James Clerk Maxwell published his famous paper on a dynamical theory of the electromagnetic field.
Once this electromagnetic field has been produced from a given charge distribution, other charged objects in this field will experience a force in a similar way that planets experience a force in the gravitational field of the sun. If these other charges and currents are comparable in size to the sources producing the above electromagnetic field, then a new net electromagnetic field will be produced. Thus, the electromagnetic field may be viewed as a dynamic entity that causes other charges and currents to move, and which is also affected by them. These interactions are described by Maxwell's equations and the Lorentz force law. This discussion ignores the radiation reaction force.
Feedback loop.
The behavior of the electromagnetic field can be divided into four different parts of a loop:
A common misunderstanding is that (a) the quanta of the fields act in the same manner as (b) the charged particles that generate the fields. In our everyday world, charged particles, such as electrons, move slowly through matter with a drift velocity of a fraction of a centimeter (or inch) per second, but fields propagate at the speed of light - approximately 300 thousand kilometers (or 186 thousand miles) a second. The mundane speed difference between charged particles and field quanta is on the order of one to a million, more or less. Maxwell's equations relate (a) the presence and movement of charged particles with (b) the generation of fields. Those fields can then affect the force on, and can then move other slowly moving charged particles. Charged particles can move at relativistic speeds nearing field propagation speeds, but, as Einstein showed, this requires enormous field energies, which are not present in our everyday experiences with electricity, magnetism, matter, and time and space.
The feedback loop can be summarized in a list, including phenomena belonging to each part of the loop:
Mathematical description.
There are different mathematical ways of representing the electromagnetic field. The first one views the electric and magnetic fields as three-dimensional vector fields. These vector fields each have a value defined at every point of space and time and are thus often regarded as functions of the space and time coordinates. As such, they are often written as E(x, y, z, t) (electric field) and B(x, y, z, t) (magnetic field).
If only the electric field (E) is non-zero, and is constant in time, the field is said to be an electrostatic field. Similarly, if only the magnetic field (B) is non-zero and is constant in time, the field is said to be a magnetostatic field. However, if either the electric or magnetic field has a time-dependence, then both fields must be considered together as a coupled electromagnetic field using Maxwell's equations.
With the advent of special relativity, physical laws became susceptible to the formalism of tensors. Maxwell's equations can be written in tensor form, generally viewed by physicists as a more elegant means of expressing physical laws.
The behaviour of electric and magnetic fields, whether in cases of electrostatics, magnetostatics, or electrodynamics (electromagnetic fields), is governed by Maxwell's equations. In the vector field formalism, these are:
where formula_6 is the charge density, which can (and often does) depend on time and position, formula_7 is the permittivity of free space, formula_8 is the permeability of free space, and J is the current density vector, also a function of time and position. The units used above are the standard SI units. Inside a linear material, Maxwell's equations change by switching the permeability and permittivity of free space with the permeability and permittivity of the linear material in question. Inside other materials which possess more complex responses to electromagnetic fields, these terms are often represented by complex numbers, or tensors.
The Lorentz force law governs the interaction of the electromagnetic field with charged matter.
When a field travels across to different media, the properties of the field change according to the various boundary conditions. These equations are derived from Maxwell's equations.
The tangential components of the electric and magnetic fields as they relate on the boundary of two media are as follows:
The angle of refraction of an electric field between media is related to the permittivity formula_13 of each medium:
The angle of refraction of a magnetic field between media is related to the permeability formula_15 of each medium:
Properties of the field.
Reciprocal behavior of electric and magnetic fields.
The two Maxwell equations, Faraday's Law and the Ampère-Maxwell Law, illustrate a very practical feature of the electromagnetic field. Faraday's Law may be stated roughly as 'a changing magnetic field creates an electric field'. This is the principle behind the electric generator.
Ampere's Law roughly states that 'a changing electric field creates a magnetic field'. Thus, this law can be applied to generate a magnetic field and run an electric motor.
Light as an electromagnetic disturbance.
Maxwell's equations take the form of an electromagnetic wave in a volume of space not containing charges or currents (free space) – that is, where formula_6 and J are zero. Under these conditions, the electric and magnetic fields satisfy the electromagnetic wave equation:
James Clerk Maxwell was the first to obtain this relationship by his completion of Maxwell's equations with the addition of a displacement current term to Ampere's Circuital law.
Relation to and comparison with other physical fields.
Being one of the four fundamental forces of nature, it is useful to compare the electromagnetic field with the gravitational, strong and weak fields. The word 'force' is sometimes replaced by 'interaction' because modern particle physics models electromagnetism as an exchange of particles known as gauge bosons.
Electromagnetic and gravitational fields.
Sources of electromagnetic fields consist of two types of charge – positive and negative. This contrasts with the sources of the gravitational field, which are masses. Masses are sometimes described as "gravitational charges", the important feature of them being that there are only positive masses and no negative masses. Further, gravity differs from electromagnetism in that positive masses attract other positive masses whereas same charges in electromagnetism repel each other.
The relative strengths and ranges of the four interactions and other information are tabulated below:
Applications.
Static E and M fields and static EM fields.
When an EM field (see electromagnetic tensor) is not varying in time, it may be seen as a purely electrical field or a purely magnetic field, or a mixture of both. However the general case of a static EM field with both electric and magnetic components present, is the case that appears to most observers. Observers who see only an electric or magnetic field component of a static EM field, have the other (electric or magnetic) component suppressed, due to the special case of the immobile state of the charges that produce the EM field in that case. In such cases the other component becomes manifest in other observer frames.
A consequence of this, is that any case that seems to consist of a "pure" static electric or magnetic field, can be converted to an EM field, with both E and M components present, by simply moving the observer into a frame of reference which is moving with regard to the frame in which only the “pure” electric or magnetic field appears. That is, a pure static electric field will show the familiar magnetic field associated with a current, in any frame of reference where the charge moves. Likewise, any new motion of a charge in a region that seemed previously to contain only a magnetic field, will show that that the space now contains an electric field as well, which will be found to produces an additional Lorentz force upon the moving charge.
Thus, electrostatics, as well as magnetism and magnetostatics, are now seen as studies of the static EM field when a particular frame has been selected to suppress the other type of field, and since an EM field with both electric and magnetic will appear in any other frame, these "simpler" effects are merely the observer's. The "applications" of all such non-time varying (static) fields are discussed in the main articles linked in this section.
Time-varying EM fields in Maxwell’s equations.
An EM field that varies in time has two “causes” in Maxwell’s equations. One is charges and currents (so-called “sources”), and the other cause for an E or M field is a change in the other type of field (this last cause also appears in “free space” very far from currents and charges).
An electromagnetic field very far from currents and charges (sources) is called electromagnetic radiation (EMR) since it radiates from the charges and currents in the source, and has no "feedback" effect on them, and is also not affected directly by them in the present time (rather, it is indirectly produced by a sequences of changes in fields radiating out from them in the past). EMR consists of the radiations in the electromagnetic spectrum, including radio waves, microwave, infrared, visible light, ultraviolet light, X-rays, and gamma rays. The many commercial applications of these radiations are discussed in the named and linked articles.
A notable application of visible light is that this type of energy from the Sun powers all life on Earth that either makes or uses oxygen.
A changing electromagnetic field which is physically close to currents and charges (see near and far field for a definition of “close”) will have a dipole characteristic that is dominated by either a changing electric dipole, or a changing magnetic dipole. This type of dipole field near sources is called an electromagnetic "near-field".
Changing "electric" dipole fields, as such, are used commercially as near-fields mainly as a source of dielectric heating. Otherwise, they appear parasitically around conductors which absorb EMR, and around antennas which have the purpose of generating EMR at greater distances.
Changing "magnetic" dipole fields (i.e., magnetic near-fields) are used commercially for many types of magnetic induction devices. These include motors and electrical transformers at low frequencies, and devices such as metal detectors and MRI scanner coils at higher frequencies. Sometimes these high-frequency magnetic fields change at radio frequencies without being far-field waves and thus radio waves; see RFID tags.
See also near-field communication.
Further uses of near-field EM effects commercially, may be found in the article on virtual photons, since at the quantum level, these fields are represented by these particles. Far-field effects (EMR) in the quantum picture of radiation, are represented by ordinary photons.
Health and safety.
The potential health effects of the very low frequency EMFs surrounding power lines and electrical devices are the subject of on-going research and a significant amount of public debate. The US National Institute for Occupational Safety and Health (NIOSH) and other US government agencies do not consider EMFs a proven health hazard. NIOSH has issued some cautionary advisories but stresses that the data is currently too limited to draw good conclusions.
The potential effects of electromagnetic fields on human health vary widely depending on the frequency and intensity of the fields. For more information on the health effects due to specific parts of the electromagnetic spectrum, see the following articles:

</doc>
<doc id="4769216" url="https://en.wikipedia.org/wiki?curid=4769216" title="Electromagnetic mass">
Electromagnetic mass

Electromagnetic mass was initially a concept of classical mechanics, denoting as to how much the electromagnetic field, or the self-energy, is contributing to the mass of charged particles. It was first derived by J. J. Thomson in 1881 and was for some time also considered as a dynamical explanation of inertial mass "per se". Today, the relation of mass, momentum, velocity and all forms of energy, including electromagnetic energy, is analyzed on the basis of Albert Einstein's special relativity and mass–energy equivalence. As to the cause of mass of elementary particles, the Higgs mechanism in the framework of the relativistic Standard Model is currently used. In addition, some problems concerning the electromagnetic mass and self-energy of charged particles are still studied.
Charged particles.
Rest mass and energy.
It was recognized by J. J. Thomson in 1881 that a charged sphere moving in a space filled with a medium of a specific inductive capacity (the electromagnetic aether of James Clerk Maxwell), is harder to set in motion than an uncharged body. (Similar considerations were already made by George Gabriel Stokes (1843) with respect to hydrodynamics, who showed that the inertia of a body moving in an incompressible perfect fluid is increased.) So due to this self-induction effect, electrostatic energy behaves as having some sort of momentum and "apparent" electromagnetic mass, which can increase the ordinary mechanical mass of the bodies, or in more modern terms, the increase should arise from their electromagnetic self-energy. This idea was worked out in more detail by Oliver Heaviside (1889), Thomson (1893), George Frederick Charles Searle (1897), Max Abraham (1902), Hendrik Lorentz (1892, 1904), and was directly applied to the electron by using the Abraham–Lorentz force. Now, the electrostatic energy formula_1 and mass formula_2 of an electron at rest was calculated to be 
where formula_4 is the charge, uniformly distributed on the surface of a sphere, and formula_5 is the classical electron radius, which must be nonzero to avoid infinite energy accumulation. Thus the formula for this electromagnetic energy–mass relation is
This was discussed in connection with the proposal of the electrical origin of matter, so Wilhelm Wien (1900), and Max Abraham (1902), came to the conclusion that the total mass of the bodies is identical to its electromagnetic mass. Wien stated, that if it is assumed that gravitation is an electromagnetic effect too, then there has to be a proportionality between electromagnetic energy, inertial mass, and gravitational mass. When one body attracts another one, the electromagnetic energy store of gravitation is according to Wien diminished by the amount (where formula_7 is the attracted mass, formula_8 the gravitational constant, formula_9 the distance):
Henri Poincaré in 1906 argued that when mass is in fact the product of the electromagnetic field in the aether – implying that no "real" mass exists – and because matter is inseparably connected with mass, then also matter doesn't exist at all and electrons are only concavities in the aether.
Mass and speed.
Thomson and Searle.
Thomson (1893) noticed that electromagnetic momentum and energy of charged bodies and therefore their masses depend on the speed of the bodies as well. He wrote:
In 1897, Searle gave a more precise formula for the electromagnetic energy of charged sphere in motion:
and like Thomson he concluded:
Longitudinal and transverse mass.
From Searle's formula, Walter Kaufmann (1901) and Abraham (1902) derived the formula for the electromagnetic mass of moving bodies:
However, it was shown by Abraham (1902), that this value is only valid in the longitudinal direction ("longitudinal mass"), i.e., that the electromagnetic mass also depends on the direction of the moving bodies with respect to the aether. Thus Abraham also derived the "transverse mass":
On the other hand, already in 1899 Lorentz assumed that the electrons undergo length contraction in the line of motion, which leads to results for the acceleration of moving electrons that differ from those given by Abraham. Lorentz obtained factors of formula_14 parallel to the direction of motion and formula_15 perpendicular to the direction of motion, where formula_16 and formula_17 is an undetermined factor. Lorentz expanded his 1899 ideas in his famous 1904 paper, where he set the factor formula_17 to unity, thus:
So, eventually Lorentz arrived at the same conclusion as Thomson in 1893: no body can reach the speed of light because the mass becomes infinitely large at this velocity.
Additionally, a third electron model was developed by Alfred Bucherer and Paul Langevin, in which the electron contracts in the line of motion, and expands perpendicular to it, so that the volume remains constant. This gives:
Kaufmann's experiments.
The predictions of the theories of Abraham and Lorentz were supported by the experiments of Walter Kaufmann (1901), but the experiments were not precise enough to distinguish between them. In 1905 Kaufmann conducted another series of experiments (Kaufmann–Bucherer–Neumann experiments) which confirmed Abraham's and Bucherer's predictions, but contradicted Lorentz's theory and the "fundamental assumption of Lorentz and Einstein", "i.e.", the relativity principle. In the following years experiments by Alfred Bucherer (1908), Gunther Neumann (1914) and others seemed to confirm Lorentz's mass formula. It was later pointed out that the Bucherer–Neumann experiments were also not precise enough to distinguish between the theories – it lasted until 1940 when the precision required was achieved to eventually prove Lorentz's formula and to refute Abraham's by these kinds of experiments. (However, other experiments of different kind already refuted Abraham's and Bucherer's formulas long before.)
Poincaré stresses and 4/3 problem.
The idea of an electromagnetic nature of matter, however, had to be given up. Abraham (1904, 1905) argued that non-electromagnetic forces were necessary to prevent Lorentz's contractile electrons from exploding. He also showed that different results for the longitudinal electromagnetic mass can be obtained in Lorentz's theory, depending on whether the mass is calculated from its energy or its momentum, so a non-electromagnetic potential (corresponding to 1/3 of the electron's electromagnetic energy) was necessary to render these masses equal. Abraham doubted whether it was possible to develop a model satisfying all of these properties.
To solve those problems, Henri Poincaré in 1905 and 1906 introduced some sort of pressure ("Poincaré stresses") of non-electromagnetic nature. As required by Abraham, these stresses contribute non-electromagnetic energy to the electrons, amounting to 1/4 of their total energy or to 1/3 of their electromagnetic energy. So, the Poincaré stresses remove the contradiction in the derivation of the longitudinal electromagnetic mass, they prevent the electron from exploding, they remain unaltered by a Lorentz transformation ("i.e." they are Lorentz invariant), and were also thought as a dynamical explanation of length contraction. However, Poincaré still assumed that only the electromagnetic energy contributes to the mass of the bodies.
As it was later noted, the problem lies in the 4/3 factor of electromagnetic rest mass – given above as formula_21 when derived from the Abraham–Lorentz equations. However, when it is derived from the electron's electrostatic energy alone, we have formula_22 where the 4/3 factor is missing. This can be solved by adding the non-electromagnetic energy formula_23 of the Poincaré stresses to formula_1, the electron's total energy formula_25 now becomes:
Thus the missing 4/3 factor is restored when the mass is related to its electromagnetic energy, and it disappears when the total energy is considered.
Inertia of energy and radiation paradoxes.
Radiation pressure.
Another way of deriving some sort of electromagnetic mass was based on the concept of radiation pressure. These pressures or tensions in the electromagnetic field were derived by James Clerk Maxwell (1874) and Adolfo Bartoli (1876). Lorentz recognized in 1895 that those tensions also arise in his theory of the stationary aether. So if the electromagnetic field of the aether is able to set bodies in motion, the action/reaction principle demands that the aether must be set in motion by matter as well. However, Lorentz pointed out that any tension in the aether requires the mobility of the aether parts, which in not possible since in his theory the aether is immobile. This represents a violation of the reaction principle that was accepted by Lorentz consciously. He continued by saying, that one can only speak about "fictitious" tensions, since they are only mathematical models in his theory to ease the description of the electrodynamic interactions.
Mass of the fictitious electromagnetic fluid.
In 1900 Poincaré studied the conflict between the action/reaction principle and Lorentz's theory. He tried to determine whether the center of gravity still moves with a uniform velocity when electromagnetic fields and radiation are involved. He noticed that the action/reaction principle does not hold for matter alone, but that the electromagnetic field has its own momentum (such a momentum was also derived by Thomson in 1893 in a more complicated way). Poincaré concluded, the electromagnetic field energy behaves like a fictitious fluid („fluide fictif“) with a mass density of formula_27 (in other words formula_28). Now, if the center of mass frame (COM-frame) is defined by both the mass of matter "and" the mass of the fictitious fluid, and if the fictitious fluid is indestructible – it is neither created or destroyed – then the motion of the center of mass frame remains uniform.
But this electromagnetic fluid is not indestructible, because it can be absorbed by matter (which according to Poincaré was the reason why he regarded the em-fluid as "fictitious" rather than "real"). Thus the COM-principle would be violated again. As it was later done by Einstein, an easy solution of this would be to assume that the mass of em-field is transferred to matter in the absorption process. But Poincaré created another solution: He assumed that there exists an immobile non-electromagnetic energy fluid at each point in space, also carrying a mass proportional to its energy. When the fictitious em-fluid is destroyed or absorbed, its electromagnetic energy and mass is not carried away by moving matter, but is transferred into the non-electromagnetic fluid and remains at exactly the same place in that fluid. (Poincaré added that one should not be too surprised by these assumptions, since they are only mathematical fictions.) In this way, the motion of the COM-frame, incl. matter, fictitious em-fluid, and fictitious non-em-fluid, at least "theoretically" remains uniform.
However, since only matter and electromagnetic energy are directly observable by experiment (not the non-em-fluid), Poincaré's resolution still violates the reaction principle and the COM-theorem, when an emission/absorption process is "practically" considered. This leads to a paradox when changing frames: if waves are radiated in a certain direction, the device will suffer a recoil from the momentum of the fictitious fluid. Then, Poincaré performed a Lorentz boost (to first order in v/c) to the frame of the moving source. He noted that energy conservation holds in both frames, but that the law of conservation of momentum is violated. This would allow perpetual motion, a notion which he abhorred. The laws of nature would have to be different in the frames of reference, and the relativity principle would not hold. Therefore he argued that also in this case there has to be another compensating mechanism in the ether.
Poincaré came back to this topic in 1904. This time he rejected his own solution that motions in the ether can compensate the motion of matter, because any such motion is unobservable and therefore scientifically worthless. He also abandoned the concept that energy carries mass and wrote in connection to the above mentioned recoil:
Momentum and cavity radiation.
However, Poincaré's idea of momentum and mass associated with radiation proved to be fruitful, when Max Abraham introduced the term „electromagnetic momentum“, having a field density of formula_27 per cm3 and formula_30 per cm2. Contrary to Lorentz and Poincaré, who considered momentum as a fictitious force, he argued that it is a real physical entity, and therefore conservation of momentum is guaranteed.
In 1904, Friedrich Hasenöhrl specifically associated inertia with "radiation" by studying the dynamics of a moving cavity. Hasenöhrl suggested that part of the mass of a body (which he called "apparent mass") can be thought of as radiation bouncing around a cavity. The apparent mass of radiation depends on the temperature (because every heated body emits radiation) and is proportional to its energy, and he first concluded that formula_31. However, in 1905 Hasenöhrl published a summary of a letter, which was written by Abraham to him. Abraham concluded that Hasenöhrl's formula of the apparent mass of radiation is not correct, and on the basis of his definition of electromagnetic momentum and longitudinal electromagnetic mass Abraham changed it to formula_32, the same value for the electromagnetic mass for a body at rest. Hasenöhrl recalculated his own derivation and verified Abraham's result. He also noticed the similarity between the apparent mass and the electromagnetic mass. However, Hasenöhrl stated that this energy-apparent-mass relation "only" holds as long a body radiates, i.e. if the temperature of a body is greater than 0 K.
Modern view.
Mass–energy equivalence.
The idea that the principal relations between mass, energy, momentum and velocity can only be considered on the basis of dynamical interactions of matter was superseded, when Albert Einstein found out in 1905 that considerations based on the special principle of relativity require that all forms of energy (not only electromagnetic) contribute to the mass of bodies (mass–energy equivalence). That is, the entire mass of a body is a measure of its energy content by formula_33, and Einstein's considerations were independent from assumptions about the constitution of matter. By this equivalence, Poincaré's radiation paradox can be solved without using "compensating forces", because the mass of matter itself (not the non-electromagnetic aether fluid as suggested by Poincaré) is increased or diminished by the mass of electromagnetic energy in the course of the emission/absorption process. Also the idea of an electromagnetic explanation of gravitation was superseded in the course of developing general relativity.
So every theory dealing with the mass of a body must be formulated in a relativistic way from the outset. This is for example the case in the current quantum field explanation of mass of elementary particles in the framework of the Standard Model, the Higgs mechanism. Because of this, the idea that any form of mass is "completely" caused by interactions with electromagnetic fields, is not relevant any more.
Relativistic mass.
The concepts of longitudinal and transverse mass (equivalent to those of Lorentz) were also used by Einstein in his first papers on relativity. However, in special relativity they apply to the entire mass of matter, not only to the electromagnetic part. Later it was shown by physicists like Richard Chace Tolman that expressing mass as the ratio of force and acceleration is not advantageous. Therefore a similar concept without direction dependent terms, in which force is defined as formula_34, was used as relativistic mass
This concept is sometimes still used in modern physics textbooks, although the term 'mass' is now considered by many to refer to invariant mass, see mass in special relativity.
Self-energy.
When the special case of the electromagnetic self-energy or self-force of charged particles is discussed, also in modern texts some sort of "effective" electromagnetic mass is sometimes introduced – not as an explanation of mass "per se", but in addition to the ordinary mass of bodies. Many different reformulations of the Abraham–Lorentz force have been derived – for instance, in order to deal with the 4/3-problem (see next section) and other problems that arose form this concept. Such questions are discussed in connection with renormalization, and on the basis of quantum mechanics and quantum field theory, which must be applied when the electron is considered physically point-like. At distances located in the classical domain, the classical concepts again come into play. A rigorous derivation of the electromagnetic self-force, including the contribution to the mass of the body, was published by Gralla et al. (2009).
4/3 problem.
Max von Laue in 1911 also used the Abraham–Lorentz equations of motion in his development of special relativistic dynamics, so that also in special relativity the 4/3-factor is present when the electromagnetic mass of a charged sphere is calculated. This contradicts the mass–energy equivalence formula, which requires the relation formula_28 without the 4/3 factor, or in other words, four-momentum doesn't properly transform like a four-vector when the 4/3 factor is present. Laue found a solution equivalent to Poincaré's introduction of a non-electromagnetic potential (Poincaré stresses), but Laue showed its deeper, relativistic meaning by employing and advancing Hermann Minkowski's spacetime formalism. Laue's formalism required that there are additional components and forces, which guarantee that spatially extended systems (where both electromagnetic and non-electromagnetic energies are combined) are forming a stable or "closed system" and transform as a four-vector. That is, the 4/3 factor arises only with respect to electromagnetic mass, while the closed system has total rest mass and energy of formula_37.
Another solution was found by authors such as Enrico Fermi (1922), Paul Dirac (1938) Fritz Rohrlich (1960), or Julian Schwinger (1983), who pointed out that the electron's stability and the 4/3-problem are two different things. They showed that the preceding definitions of four-momentum are non-relativistic "per se", and by changing the definition into a relativistic form, the electromagnetic mass can simply written as formula_28 and thus the 4/3 factor doesn't appear at all. So every part of the system, not only "closed" systems, properly transforms as a four-vector. However, binding forces like the Poincaré stresses are still necessary to prevent the electron from exploding due to Coulomb repulsion. But on the basis of the Fermi–Rohrlich definition, this is only a dynamical problem and has nothing to do with the transformation properties any more.

</doc>
<doc id="9426" url="https://en.wikipedia.org/wiki?curid=9426" title="Electromagnetic radiation">
Electromagnetic radiation

Electromagnetic radiation (EM radiation or EMR) is the radiant energy released by certain electromagnetic processes. Visible light is one type of electromagnetic radiation; other familiar forms are invisible electromagnetic radiations, such as radio waves, infrared light and X rays.
Classically, electromagnetic radiation consists of electromagnetic waves, which are synchronized oscillations of electric and magnetic fields that propagate at the speed of light through a vacuum. The oscillations of the two fields are perpendicular to each other and perpendicular to the direction of energy and wave propagation, forming a transverse wave. Electromagnetic waves can be characterized by either the frequency or wavelength of their oscillations to form the electromagnetic spectrum, which includes, in order of increasing frequency and decreasing wavelength: radio waves, microwaves, infrared radiation, visible light, ultraviolet radiation, X-rays and gamma rays.
Electromagnetic waves are produced whenever charged particles are accelerated, and these waves can subsequently interact with any charged particles. EM waves carry energy, momentum and angular momentum away from their source particle and can impart those quantities to matter with which they interact. Quanta of EM waves are called photons, which are massless, but they are still affected by gravity. Electromagnetic radiation is associated with those EM waves that are free to propagate themselves ("radiate") without the continuing influence of the moving charges that produced them, because they have achieved sufficient distance from those charges. Thus, EMR is sometimes referred to as the far field. In this jargon, the "near field" refers to EM fields near the charges and current that directly produced them, specifically, electromagnetic induction and electrostatic induction phenomena.
In the quantum theory of electromagnetism, EMR consists of photons, the elementary particles responsible for all electromagnetic interactions. Quantum effects provide additional sources of EMR, such as the transition of electrons to lower energy levels in an atom and black-body radiation. The energy of an individual photon is quantized and is greater for photons of higher frequency. This relationship is given by Planck's equation E=hν, where E is the energy per photon, ν is the frequency of the photon, and h is Planck's constant. A single gamma ray photon, for example, might carry ~100,000 times the energy of a single photon of visible light.
The effects of EMR upon biological systems (and also to many other chemical systems, under standard conditions) depend both upon the radiation's power and its frequency. For EMR of visible frequencies or lower (i.e., radio, microwave, infrared), the damage done to cells and other materials is determined mainly by power and caused primarily by heating effects from the combined energy transfer of many photons. By contrast, for ultraviolet and higher frequencies (i.e., X-rays and gamma rays), chemical materials and living cells can be further damaged beyond that done by simple heating, since individual photons of such high frequency have enough energy to cause direct molecular damage.
Physics.
Theory.
Maxwell’s equations.
James Clerk Maxwell first formally postulated "electromagnetic waves". These were subsequently confirmed by Heinrich Hertz. Maxwell derived a wave form of the electric and magnetic equations, thus uncovering the wave-like nature of electric and magnetic fields and their symmetry. Because the speed of EM waves predicted by the wave equation coincided with the measured speed of light, Maxwell concluded that light itself is an EM wave.
According to Maxwell's equations, a spatially varying electric field is always associated with a magnetic field that changes over time. Likewise, a spatially varying magnetic field is associated with specific changes over time in the electric field. In an electromagnetic wave, the changes in the electric field are always accompanied by a wave in the magnetic field in one direction, and vice versa. This relationship between the two occurs without either type field causing the other; rather, they occur together in the same way that time and space changes occur together and are interlinked in special relativity. In fact, magnetic fields may be viewed as relativistic distortions of electric fields, so the close relationship between space and time changes here is more than an analogy. Together, these fields form a propagating electromagnetic wave, which moves out into space and need never again affect the source. The distant EM field formed in this way by the acceleration of a charge carries energy with it that "radiates" away through space, hence the term.
Near and far fields.
Maxwell's equations established that some charges and currents ("sources") produce a local type of electromagnetic field near them that does "not" have the behaviour of EMR. Currents directly produce a magnetic field, but it is of a magnetic dipole type that dies out with distance from the current. In a similar manner, moving charges pushed apart in a conductor by a changing electrical potential (such as in an antenna) produce an electric dipole type electrical field, but this also declines with distance. These fields make up the near-field near the EMR source. Neither of these behaviours are responsible for EM radiation. Instead, they cause electromagnetic field behaviour that only efficiently transfers power to a receiver very close to the source, such as the magnetic induction inside a transformer, or the feedback behaviour that happens close to the coil of a metal detector. Typically, near-fields have a powerful effect on their own sources, causing an increased “load” (decreased electrical reactance) in the source or transmitter, whenever energy is withdrawn from the EM field by a receiver. Otherwise, these fields do not “propagate” freely out into space, carrying their energy away without distance-limit, but rather oscillate, returning their energy to the transmitter if it is not received by a receiver.
By contrast, the EM far-field is composed of "radiation" that is free of the transmitter in the sense that (unlike the case in an electrical transformer) the transmitter requires the same power to send these changes in the fields out, whether the signal is immediately picked up or not. This distant part of the electromagnetic field "is" "electromagnetic radiation" (also called the far-field). The far-fields propagate (radiate) without allowing the transmitter to affect them. This causes them to be independent in the sense that their existence and their energy, after they have left the transmitter, is completely independent of both transmitter and receiver. Because such waves conserve the amount of energy they transmit through any spherical boundary surface drawn around their source, and because such surfaces have an area that is defined by the square of the distance from the source, the power of EM radiation always varies according to an inverse-square law. This is in contrast to dipole parts of the EM field close to the source (the near-field), which varies in power according to an inverse cube power law, and thus does "not" transport a conserved amount of energy over distances, but instead fades with distance, with its energy (as noted) rapidly returning to the transmitter or absorbed by a nearby receiver (such as a transformer secondary coil).
The far-field (EMR) depends on a different mechanism for its production than the near-field, and upon different terms in Maxwell’s equations. Whereas the magnetic part of the near-field is due to currents in the source, the magnetic field in EMR is due only to the local change in the electric field. In a similar way, while the electric field in the near-field is due directly to the charges and charge-separation in the source, the electric field in EMR is due to a change in the local magnetic field. Both processes for producing electric and magnetic EMR fields have a different dependence on distance than do near-field dipole electric and magnetic fields. That is why the EMR type of EM field becomes dominant in power “far” from sources. The term “far from sources” refers to how far from the source (moving at the speed of light) any portion of the outward-moving EM field is located, by the time that source currents are changed by the varying source potential, and the source has therefore begun to generate an outwardly moving EM field of a different phase.
A more compact view of EMR is that the far-field that composes EMR is generally that part of the EM field that has traveled sufficient distance from the source, that it has become completely disconnected from any feedback to the charges and currents that were originally responsible for it. Now independent of the source charges, the EM field, as it moves farther away, is dependent only upon the accelerations of the charges that produced it. It no longer has a strong connection to the direct fields of the charges, or to the velocity of the charges (currents).
In the Liénard–Wiechert potential formulation of the electric and magnetic fields due to motion of a single particle (according to Maxwell's equations), the terms associated with acceleration of the particle are those that are responsible for the part of the field that is regarded as electromagnetic radiation. By contrast, the term associated with the changing static electric field of the particle and the magnetic term that results from the particle's uniform velocity, are both associated with the electromagnetic near-field, and do not comprise EM radiation.
Properties.
The physics of electromagnetic radiation is electrodynamics. Electromagnetism is the physical phenomenon associated with the theory of electrodynamics. Electric and magnetic fields obey the properties of superposition. Thus, a field due to any particular particle or time-varying electric or magnetic field contributes to the fields present in the same space due to other causes. Further, as they are vector fields, all magnetic and electric field vectors add together according to vector addition. For example, in optics two or more coherent lightwaves may interact and by constructive or destructive interference yield a resultant irradiance deviating from the sum of the component irradiances of the individual lightwaves.
Since light is an oscillation it is not affected by travelling through static electric or magnetic fields in a linear medium such as a vacuum. However, in nonlinear media, such as some crystals, interactions can occur between light and static electric and magnetic fields — these interactions include the Faraday effect and the Kerr effect.
In refraction, a wave crossing from one medium to another of different density alters its speed and direction upon entering the new medium. The ratio of the refractive indices of the media determines the degree of refraction, and is summarized by Snell's law. Light of composite wavelengths (natural sunlight) disperses into a visible spectrum passing through a prism, because of the wavelength-dependent refractive index of the prism material (dispersion); that is, each component wave within the composite light is bent a different amount.
EM radiation exhibits both wave properties and particle properties at the same time (see wave-particle duality). Both wave and particle characteristics have been confirmed in many experiments. Wave characteristics are more apparent when EM radiation is measured over relatively large timescales and over large distances while particle characteristics are more evident when measuring small timescales and distances. For example, when electromagnetic radiation is absorbed by matter, particle-like properties will be more obvious when the average number of photons in the cube of the relevant wavelength is much smaller than 1. It is not too difficult to experimentally observe non-uniform deposition of energy when light is absorbed, however this alone is not evidence of "particulate" behavior. Rather, it reflects the quantum nature of "matter". Demonstrating that the light itself is quantized, not merely its interaction with matter, is a more subtle affair.
Some experiments display both the wave and particle natures of electromagnetic waves, such as the self-interference of a single photon. When a single photon is sent through an interferometer, it passes through both paths, interfering with itself, as waves do, yet is detected by a photomultiplier or other sensitive detector only once.
A quantum theory of the interaction between electromagnetic radiation and matter such as electrons is described by the theory of quantum electrodynamics.
Wave model.
Electromagnetic radiation is a transverse wave, meaning that its oscillations are perpendicular to the direction of energy transfer and travel. The electric and magnetic parts of the field stand in a fixed ratio of strengths in order to satisfy the two Maxwell equations that specify how one is produced from the other. These E and B fields are also in phase, with both reaching maxima and minima at the same points in space (see illustrations). A common misconception is that the E and B fields in electromagnetic radiation are out of phase because a change in one produces the other, and this would produce a phase difference between them as sinusoidal functions (as indeed happens in electromagnetic induction, and in the near-field close to antennas). However, in the far-field EM radiation which is described by the two source-free Maxwell curl operator equations, a more correct description is that a time-change in one type of field is proportional to a space-change in the other. These derivatives require that the E and B fields in EMR are in-phase (see math section below).
An important aspect of light's nature is its frequency. The frequency of a wave is its rate of oscillation and is measured in hertz, the SI unit of frequency, where one hertz is equal to one oscillation per second. Light usually has multiple frequencies that sum to form the resultant wave. Different frequencies undergo different angles of refraction, a phenomenon known as dispersion.
A wave consists of successive troughs and crests, and the distance between two adjacent crests or troughs is called the wavelength. Waves of the electromagnetic spectrum vary in size, from very long radio waves the size of buildings to very short gamma rays smaller than atom nuclei. Frequency is inversely proportional to wavelength, according to the equation:
where "v" is the speed of the wave ("c" in a vacuum, or less in other media), "f" is the frequency and λ is the wavelength. As waves cross boundaries between different media, their speeds change but their frequencies remain constant.
Electromagnetic waves in free space must be solutions of Maxwell's electromagnetic wave equation. Two main classes of solutions are known, namely plane waves and spherical waves. The plane waves may be viewed as the limiting case of spherical waves at a very large (ideally infinite) distance from the source. Both types of waves can have a waveform which is an arbitrary time function (so long as it is sufficiently differentiable to conform to the wave equation). As with any time function, this can be decomposed by means of Fourier analysis into its frequency spectrum, or individual sinusoidal components, each of which contains a single frequency, amplitude and phase. Such a component wave is said to be "monochromatic". A monochromatic electromagnetic wave can be characterized by its frequency or wavelength, its peak amplitude, its phase relative to some reference phase, its direction of propagation and its polarization.
Interference is the superposition of two or more waves resulting in a new wave pattern. If the fields have components in the same direction, they constructively interfere, while opposite directions cause destructive interference. An example of interference caused by EMR is electromagnetic interference (EMI) or as it is more commonly known as, radio-frequency interference (RFI).
The energy in electromagnetic waves is sometimes called radiant energy.
Particle model and quantum theory.
An anomaly arose in the late 19th century involving a contradiction between the wave theory of light and measurements of the electromagnetic spectra that were being emitted by thermal radiators known as black bodies. Physicists struggled with this problem, which later became known as the ultraviolet catastrophe, unsuccessfully for many years. In 1900, Max Planck developed a new theory of black-body radiation that explained the observed spectrum. Planck's theory was based on the idea that black bodies emit light (and other electromagnetic radiation) only as discrete bundles or packets of energy. These packets were called quanta. Later, Albert Einstein proposed that light quanta be regarded as real particles. Later the particle of light was given the name photon, to correspond with other particles being described around this time, such as the electron and proton. A photon has an energy, "E", proportional to its frequency, "f", by
where "h" is Planck's constant, formula_3 is the wavelength and "c" is the speed of light. This is sometimes known as the Planck–Einstein equation. In quantum theory (see first quantization) the energy of the photons is thus directly proportional to the frequency of the EMR wave.
Likewise, the momentum "p" of a photon is also proportional to its frequency and inversely proportional to its wavelength:
The source of Einstein's proposal that light was composed of particles (or could act as particles in some circumstances) was an experimental anomaly not explained by the wave theory: the photoelectric effect, in which light striking a metal surface ejected electrons from the surface, causing an electric current to flow across an applied voltage. Experimental measurements demonstrated that the energy of individual ejected electrons was proportional to the "frequency", rather than the "intensity", of the light. Furthermore, below a certain minimum frequency, which depended on the particular metal, no current would flow regardless of the intensity. These observations appeared to contradict the wave theory, and for years physicists tried in vain to find an explanation. In 1905, Einstein explained this puzzle by resurrecting the particle theory of light to explain the observed effect. Because of the preponderance of evidence in favor of the wave theory, however, Einstein's ideas were met initially with great skepticism among established physicists. Eventually Einstein's explanation was accepted as new particle-like behavior of light was observed, such as the Compton effect.
As a photon is absorbed by an atom, it excites the atom, elevating an electron to a higher energy level (one that is on average farther from the nucleus). When an electron in an excited molecule or atom descends to a lower energy level, it emits a photon of light at a frequency corresponding to the energy difference. Since the energy levels of electrons in atoms are discrete, each element and each molecule emits and absorbs its own characteristic frequencies. Immediate photon emission is called fluorescence, a type of photoluminescence. An example is visible light emitted from fluorescent paints, in response to ultraviolet (blacklight). Many other fluorescent emissions are known in spectral bands other than visible light. Delayed emission is called phosphorescence.
Wave–particle duality.
The modern theory that explains the nature of light includes the notion of wave–particle duality. More generally, the theory states that everything has both a particle nature and a wave nature, and various experiments can be done to bring out one or the other. The particle nature is more easily discerned using an object with a large mass. A bold proposition by Louis de Broglie in 1924 led the scientific community to realize that electrons also exhibited wave–particle duality.
Wave and particle effects of electromagnetic radiation.
Together, wave and particle effects fully explain the emission and absorption spectra of EM radiation. The matter-composition of the medium through which the light travels determines the nature of the absorption and emission spectrum. These bands correspond to the allowed energy levels in the atoms. Dark bands in the absorption spectrum are due to the atoms in an intervening medium between source and observer. The atoms absorb certain frequencies of the light between emitter and detector/eye, then emit them in all directions. A dark band appears to the detector, due to the radiation scattered out of the beam. For instance, dark bands in the light emitted by a distant star are due to the atoms in the star's atmosphere. A similar phenomenon occurs for emission, which is seen when an emitting gas glows due to excitation of the atoms from any mechanism, including heat. As electrons descend to lower energy levels, a spectrum is emitted that represents the jumps between the energy levels of the electrons, but lines are seen because again emission happens only at particular energies after excitation. An example is the emission spectrum of nebulae. Rapidly moving electrons are most sharply accelerated when they encounter a region of force, so they are responsible for producing much of the highest frequency electromagnetic radiation observed in nature.
These phenomena can aid various chemical determinations for the composition of gases lit from behind (absorption spectra) and for glowing gases (emission spectra). Spectroscopy (for example) determines what chemical elements comprise a particular star. Spectroscopy is also used in the determination of the distance of a star, using the red shift.
Propagation speed.
Any electric charge that accelerates, or any changing magnetic field, produces electromagnetic radiation. Electromagnetic information about the charge travels at the speed of light. Accurate treatment thus incorporates a concept known as retarded time, which adds to the expressions for the electrodynamic electric field and magnetic field. These extra terms are responsible for electromagnetic radiation.
When any wire (or other conducting object such as an antenna) conducts alternating current, electromagnetic radiation is propagated at the same frequency as the current. In many such situations it is possible to identify an electrical dipole moment that arises from separation of charges due to the exciting electrical potential, and this dipole moment oscillates in time, as the charges move back and forth. This oscillation at a given frequency gives rise to changing electric and magnetic fields, which then set the electromagnetic radiation in motion.
At the quantum level, electromagnetic radiation is produced when the wavepacket of a charged particle oscillates or otherwise accelerates. Charged particles in a stationary state do not move, but a superposition of such states may result in a transition state that has an electric dipole moment that oscillates in time. This oscillating dipole moment is responsible for the phenomenon of radiative transition between quantum states of a charged particle. Such states occur (for example) in atoms when photons are radiated as the atom shifts from one stationary state to another.
As a wave, light is characterized by a velocity (the speed of light), wavelength, and frequency. As particles, light is a stream of photons. Each has an energy related to the frequency of the wave given by Planck's relation "E = hf", where "E" is the energy of the photon, "h" = 6.626 × 10−34 J·s is Planck's constant, and "f" is the frequency of the wave.
One rule is obeyed regardless of circumstances: EM radiation in a vacuum travels at the speed of light, "relative to the observer", regardless of the observer's velocity. (This observation led to Einstein's development of the theory of special relativity.)
In a medium (other than vacuum), velocity factor or refractive index are considered, depending on frequency and application. Both of these are ratios of the speed in a medium to speed in a vacuum.
Special theory of relativity.
By the late nineteenth century, various experimental anomalies could not be explained by the simple wave theory. One of these anomalies involved a controversy over the speed of light. The speed of light and other EMR predicted by Maxwell's equations did not appear unless the equations were modified in a way first suggested by FitzGerald and Lorentz (see history of special relativity), or else otherwise that speed would depend on the speed of observer relative to the "medium" (called luminiferous aether) which supposedly "carried" the electromagnetic wave (in a manner analogous to the way air carries sound waves). Experiments failed to find any observer effect. In 1905, Einstein proposed that space and time appeared to be velocity-changeable entities for light propagation and all other processes and laws. These changes accounted for the constancy of the speed of light and all electromagnetic radiation, from the viewpoints of all observers—even those in relative motion.
History of discovery.
Electromagnetic radiation of wavelengths other than those of visible light were discovered in the early 19th century. The discovery of infrared radiation is ascribed to astronomer William Herschel, who published his results in 1800 before the Royal Society of London. Herschel used a glass prism to refract light from the Sun and detected invisible rays that caused heating beyond the red part of the spectrum, through an increase in the temperature recorded with a thermometer. These "calorific rays" were later termed infrared.
In 1801, German physicist Johann Wilhelm Ritter discovered ultraviolet in an experiment similar to Hershel's, using sunlight and a glass prism. Ritter noted that invisible rays near the violet edge of a solar spectrum dispersed by a triangular prism darkened silver chloride preparations more quickly than did the nearby violet light. Ritter's experiments were an early precursor to what would become photography. Ritter noted that the ultraviolet rays (which at first were called "chemical rays") were capable of causing chemical reactions.
In 1862-4 James Clerk Maxwell developed equations for the electromagnetic field which suggested that waves in the field would travel with a speed that was very close to the known speed of light. Maxwell therefore suggested that visible light (as well as invisible infrared and ultraviolet rays by inference) all consisted of propagating disturbances (or radiation) in the electromagnetic field. Radio waves were first produced deliberately by Heinrich Hertz in 1887, using electrical circuits calculated to produce oscillations at a much lower frequency than that of visible light, following recipes for producing oscillating charges and currents suggested by Maxwell's equations. Hertz also developed ways to detect these waves, and produced and characterized what were later termed radio waves and microwaves.
Wilhelm Röntgen discovered and named X-rays. After experimenting with high voltages applied to an evaccuated tube on 8 November 1895, he noticed a fluorescence on a nearby plate of coated glass. In one month, he discovered X-rays' main properties.
The last portion of the EM spectrum to be discovered was associated with radioactivity. Henri Becquerel found that uranium salts caused fogging of an unexposed photographic plate through a covering paper in a manner similar to X-rays, and Marie Curie discovered that only certain elements gave off these rays of energy, soon discovering the intense radiation of radium. The radiation from pitchblende was differentiated into alpha rays (alpha particles) and beta rays (beta particles) by Ernest Rutherford through simple experimentation in 1899, but these proved to be charged particulate types of radiation. However, in 1900 the French scientist Paul Villard discovered a third neutrally charged and especially penetrating type of radiation from radium, and after he described it, Rutherford realized it must be yet a third type of radiation, which in 1903 Rutherford named gamma rays. In 1910 British physicist William Henry Bragg demonstrated that gamma rays are electromagnetic radiation, not particles, and in 1914 Rutherford and Edward Andrade measured their wavelengths, finding that they were similar to X-rays but with shorter wavelengths and higher frequency, although a 'cross-over' between X and gamma rays makes it possible to have X-rays with a higher energy (and hence shorter wavelength) than gamma rays and vice versa. The origin of the ray differentiates them, gamma rays tend to be a natural phenomena originating from the unstable nucleus of an atom and X-rays are electrically generated (and hence man-made) unless they are as a result of bremsstrahlung X-radiation caused by the interaction of fast moving particles (such as beta particles) colliding with certain materials, usually of higher atomic numbers.
Electromagnetic spectrum.
EM radiation (the designation 'radiation' excludes static electric and magnetic and near fields) is classified by wavelength into radio, microwave, infrared, visible, ultraviolet, X-rays and gamma rays. Arbitrary electromagnetic waves can be expressed by Fourier analysis in terms of sinusoidal monochromatic waves, which in turn can each be classified into these regions of the EMR spectrum.
For certain classes of EM waves, the waveform is most usefully treated as "random", and then spectral analysis must be done by slightly different mathematical techniques appropriate to random or stochastic processes. In such cases, the individual frequency components are represented in terms of their "power" content, and the phase information is not preserved. Such a representation is called the power spectral density of the random process. Random electromagnetic radiation requiring this kind of analysis is, for example, encountered in the interior of stars, and in certain other very wideband forms of radiation such as the Zero point wave field of the electromagnetic vacuum.
The behavior of EM radiation depends on its frequency. Lower frequencies have longer wavelengths, and higher frequencies have shorter wavelengths, and are associated with photons of higher energy. There is no fundamental limit known to these wavelengths or energies, at either end of the spectrum, although photons with energies near the Planck energy or exceeding it (far too high to have ever been observed) will require new physical theories to describe.
Soundwaves are not electromagnetic radiation. At the lower end of the electromagnetic spectrum, about 20 Hz to about 20 kHz, are frequencies that might be considered in the audio range. However, electromagnetic waves cannot be directly perceived by human ears. Sound waves are instead the oscillating compression of molecules. To be heard, electromagnetic radiation must be converted to pressure waves of the fluid in which the ear is located (whether the fluid is air, water or something else).
Interactions as a function of frequency.
When EM radiation interacts with matter, its behavior changes qualitatively as its frequency changes.
Radio and microwave.
At radio and microwave frequencies, EMR interacts with matter largely as a bulk collection of charges which are spread out over large numbers of affected atoms. In electrical conductors, such induced bulk movement of charges (electric currents) results in absorption of the EMR, or else separations of charges that cause generation of new EMR (effective reflection of the EMR). An example is absorption or emission of radio waves by antennas, or absorption of microwaves by water or other molecules with an electric dipole moment, as for example inside a microwave oven. These interactions produce either electric currents or heat, or both. 
Infrared.
Like radio and microwave, infrared also is reflected by metals (as is most EMR into the ultraviolet). However, unlike lower-frequency radio and microwave radiation, Infrared EMR commonly interacts with dipoles present in single molecules, which change as atoms vibrate at the ends of a single chemical bond. It is consequently absorbed by a wide range of substances, causing them to increase in temperature as the vibrations dissipate as heat. The same process, run in reverse, causes bulk substances to radiate in the infrared spontaneously (see thermal radiation section below).
Visible light.
As frequency increases into the visible range, photons of EMR have enough energy to change the bond structure of some individual molecules. It is not a coincidence that this happens in the "visible range," as the mechanism of vision involves the change in bonding of a single molecule (retinal) which absorbs light in the rhodopsin in the retina of the human eye. Photosynthesis becomes possible in this range as well, for similar reasons, as a single molecule of chlorophyll is excited by a single photon. Animals that detect infrared make use of small packets of water that change temperature, in an essentially thermal process that involves many photons (see infrared sensing in snakes). For this reason, infrared, microwaves and radio waves are thought to damage molecules and biological tissue only by bulk heating, not excitation from single photons of the radiation.
Visible light is able to affect a few molecules with single photons, but usually not in a permanent or damaging way, in the absence of power high enough to increase temperature to damaging levels. However, in plant tissues that conduct photosynthesis, carotenoids act to quench electronically excited chlorophyll produced by visible light in a process called non-photochemical quenching, in order to prevent reactions that would otherwise interfere with photosynthesis at high light levels. Limited evidence indicate that some reactive oxygen species are created by visible light in skin, and that these may have some role in photoaging, in the same manner as ultraviolet A.
Ultraviolet.
As frequency increases into the ultraviolet, photons now carry enough energy (about three electron volts or more) to excite certain doubly bonded molecules into permanent chemical rearrangement. In DNA, this causes lasting damage. DNA is also indirectly damaged by reactive oxygen species produced by ultraviolet A (UVA), which has energy too low to damage DNA directly. This is why ultraviolet at all wavelengths can damage DNA, and is capable of causing cancer, and (for UVB) skin burns (sunburn) that are far worse than would be produced by simple heating (temperature increase) effects. This property of causing molecular damage that is out of proportion to heating effects, is characteristic of all EMR with frequencies at the visible light range and above. These properties of high-frequency EMR are due to quantum effects that permanently damage materials and tissues at the molecular level.
At the higher end of the ultraviolet range, the energy of photons becomes large enough to impart enough energy to electrons to cause them to be liberated from the atom, in a process called photoionisation. The energy required for this is always larger than about 10 electron volts (eV) corresponding with wavelengths smaller than 124 nm (some sources suggest a more realistic cutoff of 33 eV, which is the energy required to ionize water). This high end of the ultraviolet spectrum with energies in the approximate ionization range, is sometimes called "extreme UV." Ionizing UV is strongly filtered by the Earth's atmosphere).
X-rays and gamma rays.
Electromagnetic radiation composed of photons that carry minimum-ionization energy, or more, (which includes the entire spectrum with shorter wavelengths), is therefore termed ionizing radiation. (Many other kinds of ionizing radiation are made of non-EM particles). Electromagnetic-type ionizing radiation extends from the extreme ultraviolet to all higher frequencies and shorter wavelengths, which means that all X-rays and gamma rays qualify. These are capable of the most severe types of molecular damage, which can happen in biology to any type of biomolecule, including mutation and cancer, and often at great depths below the skin, since the higher end of the X-ray spectrum, and all of the gamma ray spectrum, penetrate matter. This type of damage causes these types of radiation to be especially carefully monitored, due to their hazard, even at comparatively low-energies, to all living organisms.
Atmosphere and magnetosphere.
Most UV and X-rays are blocked by absorption first from molecular nitrogen, and then (for wavelengths in the upper UV) from the electronic excitation of dioxygen and finally ozone at the mid-range of UV. Only 30% of the Sun's ultraviolet light reaches the ground, and almost all of this is well transmitted.
Visible light is well transmitted in air, as it is not energetic enough to excite nitrogen, oxygen, or ozone, but too energetic to excite molecular vibrational frequencies of water vapor.
Absorption bands in the infrared are due to modes of vibrational excitation in water vapor. However, at energies too low to excite water vapor, the atmosphere becomes transparent again, allowing free transmission of most microwave and radio waves.
Finally, at radio wavelengths longer than 10 meters or so (about 30 MHz), the air in the lower atmosphere remains transparent to radio, but plasma in certain layers of the ionosphere begins to interact with radio waves (see skywave). This property allows some longer wavelengths (100 meters or 3 MHz) to be reflected and results in shortwave radio beyond line-of-sight. However, certain ionospheric effects begin to block incoming radiowaves from space, when their frequency is less than about 10 MHz (wavelength longer than about 30 meters).
Types and sources, classed by spectral band.
See electromagnetic spectrum
Radio waves.
When radio waves impinge upon a conductor, they couple to the conductor, travel along it and induce an electric current on the conductor surface by moving the electrons of the conducting material in correlated bunches of charge. Such effects can cover macroscopic distances in conductors (including as radio antennas), since the wavelength of radiowaves is long.
Microwaves.
Microwaves are a form of electromagnetic radiation with wavelengths ranging from as long as one meter to as short as one millimeter; with frequencies between 300 MHz (0.3 GHz) and 300 GHz.
Visible light.
Natural sources produce EM radiation across the spectrum. EM radiation with a wavelength between approximately 400 nm and 700 nm is directly detected by the human eye and perceived as visible light. Other wavelengths, especially nearby infrared (longer than 700 nm) and ultraviolet (shorter than 400 nm) are also sometimes referred to as light.
Thermal radiation and electromagnetic radiation as a form of heat.
The basic structure of matter involves charged particles bound together. When electromagnetic radiation impinges on matter, it causes the charged particles to oscillate and gain energy. The ultimate fate of this energy depends on the context. It could be immediately re-radiated and appear as scattered, reflected, or transmitted radiation. It may get dissipated into other microscopic motions within the matter, coming to thermal equilibrium and manifesting itself as thermal energy in the material. With a few exceptions related to high-energy photons (such as fluorescence, harmonic generation, photochemical reactions, the photovoltaic effect for ionizing radiations at far ultraviolet, X-ray and gamma radiation), absorbed electromagnetic radiation simply deposits its energy by heating the material. This happens for infrared, microwave and radio wave radiation. Intense radio waves can thermally burn living tissue and can cook food. In addition to infrared lasers, sufficiently intense visible and ultraviolet lasers can easily set paper afire. 
Ionizing radiation creates high-speed electrons in a material and breaks chemical bonds, but after these electrons collide many times with other atoms eventually most of the energy becomes thermal energy all in a tiny fraction of a second. This process makes ionizing radiation far more dangerous per unit of energy than non-ionizing radiation. This caveat also applies to UV, even though almost all of it is not ionizing, because UV can damage molecules due to electronic excitation, which is far greater per unit energy than heating effects.
Infrared radiation in the spectral distribution of a black body is usually considered a form of heat, since it has an equivalent temperature and is associated with an entropy change per unit of thermal energy. However, "heat" is a technical term in physics and thermodynamics and is often confused with thermal energy. Any type of electromagnetic energy can be transformed into thermal energy in interaction with matter. Thus, "any" electromagnetic radiation can "heat" (in the sense of increase the thermal energy termperature of) a material, when it is absorbed.
The inverse or time-reversed process of absorption is thermal radiation. Much of the thermal energy in matter consists of random motion of charged particles, and this energy can be radiated away from the matter. The resulting radiation may subsequently be absorbed by another piece of matter, with the deposited energy heating the material.
The electromagnetic radiation in an opaque cavity at thermal equilibrium is effectively a form of thermal energy, having maximum radiation entropy.
Biological effects.
Bioelectromagnetics is the study of the interactions and effects of EM radiation on living organisms. The effects of electromagnetic radiation upon living cells, including those in humans, depends upon the radiation's power and frequency. For low-frequency radiation (radio waves to visible light) the best-understood effects are those due to radiation power alone, acting through heating when radiation is absorbed. For these thermal effects, frequency is important only as it affects penetration into the organism (for example, microwaves penetrate better than infrared). Initially, it was believed that low frequency fields that were too weak to cause significant heating could not possibly have any biological effect.
Despite this opinion among researchers, evidence has accumulated that supports the existence of complex biological effects of weaker "non-thermal" electromagnetic fields, (including weak ELF magnetic fields, although the latter does not strictly qualify as EM radiation), and modulated RF and microwave fields. Fundamental mechanisms of the interaction between biological material and electromagnetic fields at non-thermal levels are not fully understood.
The World Health Organization has classified radio frequency electromagnetic radiation as Group 2B - possibly carcinogenic. This group contains possible carcinogens that have weaker evidence, at the same level as coffee and automobile exhaust. For example, epidemiological studies looking for a relationship between cell phone use and brain cancer development, have been largely inconclusive, save to demonstrate that the effect, if it exists, cannot be a large one.
At higher frequencies (visible and beyond), the effects of individual photons begin to become important, as these now have enough energy individually to directly or indirectly damage biological molecules. All UV frequences have been classed as Group 1 carcinogens by the World Health Organization. Ultraviolet radiation from sun exposure is the primary cause of skin cancer.
Thus, at UV frequencies and higher (and probably somewhat also in the visible range), electromagnetic radiation does more damage to biological systems than simple heating predicts. This is most obvious in the "far" (or "extreme") ultraviolet. UV, with X-ray and gamma radiation, are referred to as ionizing radiation due to the ability of photons of this radiation to produce ions and free radicals in materials (including living tissue). Since such radiation can severely damage life at energy levels that produce little heating, it is considered far more dangerous (in terms of damage-produced per unit of energy, or power) than the rest of the electromagnetic spectrum.
Derivation from electromagnetic theory.
Electromagnetic waves were predicted by the classical laws of electricity and magnetism, known as Maxwell's equations. Inspection of Maxwell's equations without sources (charges or currents) results in nontrivial solutions of changing electric and magnetic fields. Beginning with Maxwell's equations in free space:
One solution,
is trivial.
For a more useful solution, we utilize vector identities, which work for any vector, as follows:
The curl of equation (2):
Evaluating the left hand side:
Evaluating the right hand side:
Equations (6) and (7) are equal, so this results in a vector-valued differential equation for the electric field, namely
Applying a similar pattern results in similar differential equation for the magnetic field:
These differential equations are equivalent to the wave equation:
for a generic wave traveling in the formula_19 direction.
This form will satisfy the wave equation.
The first of Maxwell's equations implies that the electric field is orthogonal to the direction the wave propagates.
The second of Maxwell's equations yields the magnetic field. The remaining equations will be satisfied by this choice of formula_24.
The electric and magnetic field waves in the far-field travel at the speed of light. They have a special restricted orientation and proportional magnitudes, formula_25, which can be seen immediately from the Poynting vector. The electric field, magnetic field, and direction of wave propagation are all orthogonal, and the wave propagates in the same direction as formula_26. Also, E and B far-fields in free space, which as wave solutions depend primarily on these two Maxwell equations, are in-phase with each other. This is guaranteed since the generic wave solution is first order in both space and time, and the curl operator on one side of these equations results in first-order spatial derivatives of the wave solution, while the time-derivative on the other side of the equations, which gives the other field, is first order in time, resulting in the same phase shift for both fields in each mathematical operation.
From the viewpoint of an electromagnetic wave traveling forward, the electric field might be oscillating up and down, while the magnetic field oscillates right and left. This picture can be rotated with the electric field oscillating right and left and the magnetic field oscillating down and up. This is a different solution that is traveling in the same direction. This arbitrariness in the orientation with respect to propagation direction is known as polarization. On a quantum level, it is described as photon polarization. The direction of the polarization is defined as the direction of the electric field.
More general forms of the second-order wave equations given above are available, allowing for both non-vacuum propagation media and sources. Many competing derivations exist, all with varying levels of approximation and intended applications. One very general example is a form of the electric field equation, which was factorized into a pair of explicitly directional wave equations, and then efficiently reduced into a single uni-directional wave equation by means of a simple slow-evolution approximation.

</doc>
<doc id="1912367" url="https://en.wikipedia.org/wiki?curid=1912367" title="Electromagnetic tensor">
Electromagnetic tensor

In electromagnetism, the electromagnetic tensor or electromagnetic field tensor (sometimes called the field strength tensor, Faraday tensor or Maxwell bivector) is a mathematical object that describes the electromagnetic field of a physical system. The field tensor was first used after the 4-dimensional tensor formulation of special relativity was introduced by Hermann Minkowski. The tensor allows some physical laws to be written in a very concise form.
Definition.
The electromagnetic tensor, conventionally labelled "F", is defined as the exterior derivative of the electromagnetic four-potential, "A", a differential 1-form:
Therefore "F" is a differential 2-form—that is, an antisymmetric rank-2 tensor field—on Minkowski space. In component form,
SI units for Maxwell's equations and the particle physicist's sign convention for the signature of Minkowski space (+,−,−,−), will be used throughout this article.
Relationship with the classical fields.
The electromagnetic tensor is completely isomorphic to the electric and magnetic fields, though the electric and magnetic fields change with the choice of the reference frame, while the electromagnetic tensor does not. In general, the relationship is quite complicated, but in Cartesian coordinates, using the coordinate system's own reference frame, the relationship is very simple.
where "c" is the speed of light, and
where formula_5 is the Levi-Civita symbol.
In contravariant matrix form,
The covariant form is given by index lowering,
The mixed-variance form appears in the Lorentz force equation when using the contravariant four-velocity: formula_8, where
From now on in this article, when the electric or magnetic fields are mentioned, a Cartesian coordinate system is being assumed, and the electric and magnetic fields are with respect to coordinate system's own reference frame, as in the equations above.
Properties.
The matrix form of the field tensor yields the following properties:
Significance.
This tensor simplifies and reduces Maxwell's equations as four vector calculus equations into two tensor field equations. In electrostatics and electrodynamics, Gauss's law and Ampère's circuital law are respectively:
and reduce to the inhomogeneous Maxwell equation:
where
is the 4-current. In magnetostatics and magnetodynamics, Gauss's law for magnetism and Maxwell–Faraday equation are respectively:
which reduce to Bianchi identity:
or using the index notation with square brackets for the antisymmetric part of the tensor:
Relativity.
The field tensor derives its name from the fact that the electromagnetic field is found to obey the tensor transformation law, this general property of (non-gravitational) physical laws being recognised after the advent of special relativity. This theory stipulated that all the (non-gravitational) laws of physics should take the same form in all coordinate systems - this led to the introduction of tensors. The tensor formalism also leads to a mathematically simpler presentation of physical laws.
The inhomogeneous Maxwell equation leads to the continuity equation:
implying conservation of charge.
Maxwell's laws above can be generalised to curved spacetime by simply replacing partial derivatives with covariant derivatives:
where the semi-colon notation represents a covariant derivative, as opposed to a partial derivative. These equations are sometimes referred to as the curved space Maxwell equations. Again, the second equation implies charge conservation (in curved spacetime):
Lagrangian formulation of classical electromagnetism.
Classical electromagnetism and Maxwell's equations can be derived from the action:
where
This means the Lagrangian density is
The two middle terms in the parentheses are the same, as are the two outer terms, so the Lagrangian density is
Substituting this into the Euler–Lagrange equation of motion for a field:
So the Euler–Lagrange equation becomes:
The quantity in parentheses above is just the field tensor, so this finally simplifies to
That equation is another way of writing the two inhomogeneous Maxwell's equations (namely, Gauss's law and Ampère's circuital law) using the substitutions:
where "i, j, k" take the values 1, 2, and 3.
Quantum electrodynamics and field theory.
The Lagrangian of quantum electrodynamics extends beyond the classical Lagrangian established in relativity, from formula_29  to incorporate the creation and annihilation of photons (and electrons).
In quantum field theory it is used as the template for the gauge field strength tensor. By being employed in addition to the local interaction Lagrangian it reprises its usual role in QED.

</doc>
<doc id="9532" url="https://en.wikipedia.org/wiki?curid=9532" title="Electromagnetism">
Electromagnetism

Electromagnetism is a branch of physics which involves the study of the electromagnetic force, a type of physical interaction that occurs between electrically charged particles. The electromagnetic force usually shows electromagnetic fields, such as electric fields, magnetic fields, and light. The electromagnetic force is one of the four fundamental interactions in nature. The other three fundamental interactions are the strong interaction, the weak interaction, and gravitation.
The word "electromagnetism" is a compound form of two Greek terms, ἤλεκτρον, "ēlektron", "amber", and μαγνῆτις λίθος "magnētis lithos", which means "magnesian stone", a type of iron ore. The science of electromagnetic phenomena is defined in terms of the electromagnetic force, sometimes called the Lorentz force, which includes both electricity and magnetism as elements of one phenomenon.
The electromagnetic force plays a major role in determining the internal properties of most objects encountered in daily life. Ordinary matter takes its form as a result of intermolecular forces between individual molecules in matter. Electrons are bound by electromagnetic wave mechanics into orbitals around atomic nuclei to form atoms, which are the building blocks of molecules. This governs the processes involved in chemistry, which arise from interactions between the electrons of neighboring atoms, which are in turn determined by the interaction between electromagnetic force and the momentum of the electrons.
There are numerous mathematical descriptions of the electromagnetic field. In classical electrodynamics, electric fields are described as electric potential and electric current in Ohm's law, magnetic fields are associated with electromagnetic induction and magnetism, and Maxwell's equations describe how electric and magnetic fields are generated and altered by each other and by charges and currents.
The theoretical implications of electromagnetism, in particular the establishment of the speed of light based on properties of the "medium" of propagation (permeability and permittivity), led to the development of special relativity by Albert Einstein in 1905.
Although electromagnetism is considered one of the four fundamental forces, at high energy the weak force and electromagnetism are unified. In the history of the universe, during the quark epoch, the electroweak force split into the electromagnetic and weak forces.
History of the theory.
Originally electricity and magnetism were thought of as two separate forces. This view changed, however, with the publication of James Clerk Maxwell's 1873 "A Treatise on Electricity and Magnetism" in which the interactions of positive and negative charges were shown to be regulated by one force. There are four main effects resulting from these interactions, all of which have been clearly demonstrated by experiments:
While preparing for an evening lecture on 21 April 1820, Hans Christian Ørsted made a surprising observation. As he was setting up his materials, he noticed a compass needle deflected away from magnetic north when the electric current from the battery he was using was switched on and off. This deflection convinced him that magnetic fields radiate from all sides of a wire carrying an electric current, just as light and heat do, and that it confirmed a direct relationship between electricity and magnetism.
At the time of discovery, Ørsted did not suggest any satisfactory explanation of the phenomenon, nor did he try to represent the phenomenon in a mathematical framework. However, three months later he began more intensive investigations. Soon thereafter he published his findings, proving that an electric current produces a magnetic field as it flows through a wire. The CGS unit of magnetic induction (oersted) is named in honor of his contributions to the field of electromagnetism.
His findings resulted in intensive research throughout the scientific community in electrodynamics. They influenced French physicist André-Marie Ampère's developments of a single mathematical form to represent the magnetic forces between current-carrying conductors. Ørsted's discovery also represented a major step toward a unified concept of energy.
This unification, which was observed by Michael Faraday, extended by James Clerk Maxwell, and partially reformulated by Oliver Heaviside and Heinrich Hertz, is one of the key accomplishments of 19th century mathematical physics. It had far-reaching consequences, one of which was the understanding of the nature of light. Unlike what was proposed in Electromagnetism, light and other electromagnetic waves are at the present seen as taking the form of quantized, self-propagating oscillatory electromagnetic field disturbances which have been called photons. Different frequencies of oscillation give rise to the different forms of electromagnetic radiation, from radio waves at the lowest frequencies, to visible light at intermediate frequencies, to gamma rays at the highest frequencies.
Ørsted was not the only person to examine the relationship between electricity and magnetism. In 1802, Gian Domenico Romagnosi, an Italian legal scholar, deflected a magnetic needle using electrostatic charges. Actually, no galvanic current existed in the setup and hence no electromagnetism was present. An account of the discovery was published in 1802 in an Italian newspaper, but it was largely overlooked by the contemporary scientific community.
Fundamental forces.
The electromagnetic force is one of the four known fundamental forces. The other fundamental forces are:
All other forces (e.g., friction) are derived from these four fundamental forces (including momentum which is carried by the movement of particles).
The electromagnetic force is the one responsible for practically all the phenomena one encounters in daily life above the nuclear scale, with the exception of gravity. Roughly speaking, all the forces involved in interactions between atoms can be explained by the electromagnetic force acting on the electrically charged atomic nuclei and electrons inside and around the atoms, together with how these particles carry momentum by their movement. This includes the forces we experience in "pushing" or "pulling" ordinary material objects, which come from the intermolecular forces between the individual molecules in our bodies and those in the objects. It also includes all forms of chemical phenomena.
A necessary part of understanding the intra-atomic to intermolecular forces is the effective force generated by the momentum of the electrons' movement, and that electrons move between interacting atoms, carrying momentum with them. As a collection of electrons becomes more confined, their minimum momentum necessarily increases due to the Pauli exclusion principle. The behaviour of matter at the molecular scale including its density is determined by the balance between the electromagnetic force and the force generated by the exchange of momentum carried by the electrons themselves.
Classical electrodynamics.
The scientist William Gilbert proposed, in his "De Magnete" (1600), that electricity and magnetism, while both capable of causing attraction and repulsion of objects, were distinct effects. Mariners had noticed that lightning strikes had the ability to disturb a compass needle, but the link between lightning and electricity was not confirmed until Benjamin Franklin's proposed experiments in 1752. One of the first to discover and publish a link between man-made electric current and magnetism was Romagnosi, who in 1802 noticed that connecting a wire across a voltaic pile deflected a nearby compass needle. However, the effect did not become widely known until 1820, when Ørsted performed a similar experiment. Ørsted's work influenced Ampère to produce a theory of electromagnetism that set the subject on a mathematical foundation.
A theory of electromagnetism, known as classical electromagnetism, was developed by various physicists over the course of the 19th century, culminating in the work of James Clerk Maxwell, who unified the preceding developments into a single theory and discovered the electromagnetic nature of light. In classical electromagnetism, the electromagnetic field obeys a set of equations known as Maxwell's equations, and the electromagnetic force is given by the Lorentz force law.
One of the peculiarities of classical electromagnetism is that it is difficult to reconcile with classical mechanics, but it is compatible with special relativity. According to Maxwell's equations, the speed of light in a vacuum is a universal constant, dependent only on the electrical permittivity and magnetic permeability of free space. This violates Galilean invariance, a long-standing cornerstone of classical mechanics. One way to reconcile the two theories (electromagnetism and classical mechanics) is to assume the existence of a luminiferous aether through which the light propagates. However, subsequent experimental efforts failed to detect the presence of the aether. After important contributions of Hendrik Lorentz and Henri Poincaré, in 1905, Albert Einstein solved the problem with the introduction of special relativity, which replaces classical kinematics with a new theory of kinematics that is compatible with classical electromagnetism. (For more information, see History of special relativity.)
In addition, relativity theory shows that in moving frames of reference a magnetic field transforms to a field with a nonzero electric component and vice versa; thus firmly showing that they are two sides of the same coin, and thus the term "electromagnetism". (For more information, see Classical electromagnetism and special relativity and Covariant formulation of classical electromagnetism.
Quantum mechanics.
Photoelectric effect.
In another paper published in 1905, Albert Einstein undermined the very foundations of classical electromagnetism. In his theory of the photoelectric effect (for which he won the Nobel prize in physics) and inspired by the idea of Max Planck's "quanta", he posited that light could exist in discrete particle-like quantities as well, which later came to be known as photons. Einstein's theory of the photoelectric effect extended the insights that appeared in the solution of the ultraviolet catastrophe presented by Max Planck in 1900. In his work, Planck showed that hot objects emit electromagnetic radiation in discrete packets ("quanta"), which leads to a finite total energy emitted as black body radiation. Both of these results were in direct contradiction with the classical view of light as a continuous wave. Planck's and Einstein's theories were progenitors of quantum mechanics, which, when formulated in 1925, necessitated the invention of a quantum theory of electromagnetism. This theory, completed in the 1940s-1950s, is known as quantum electrodynamics (or "QED"), and, in situations where perturbation theory is applicable, is one of the most accurate theories known to physics.
Quantum electrodynamics.
All electromagnetic phenomena are underpinned by quantum mechanics, specifically by quantum electrodynamics (which includes classical electrodynamics as a limiting case) and this accounts for almost all physical phenomena observable to the unaided human senses, including light and other electromagnetic radiation, all of chemistry, most of mechanics (excepting gravitation), and, of course, magnetism and electricity.
Electroweak interaction.
The electroweak interaction is the unified description of two of the four known fundamental interactions of nature: electromagnetism and the weak interaction. Although these two forces appear very different at everyday low energies, the theory models them as two different aspects of the same force. Above the unification energy, on the order of 100 GeV, they would merge into a single electroweak force. Thus if the universe is hot enough (approximately 1015 K, a temperature exceeded until shortly after the Big Bang) then the electromagnetic force and weak force merge into a combined electroweak force. During the electroweak epoch, the electroweak force separated from the strong force. During the quark epoch, the electroweak force split into the electromagnetic and weak force.
Quantities and units.
Electromagnetic units are part of a system of electrical units based primarily upon the magnetic properties of electric currents, the fundamental SI unit being the ampere. The units are:
In the electromagnetic cgs system, electric current is a fundamental quantity defined via Ampère's law and takes the permeability as a dimensionless quantity (relative permeability) whose value in a vacuum is unity. As a consequence, the square of the speed of light appears explicitly in some of the equations interrelating quantities in this system.
Formulas for physical laws of electromagnetism (such as Maxwell's equations) need to be adjusted depending on what system of units one uses. This is because there is no one-to-one correspondence between electromagnetic units in SI and those in CGS, as is the case for mechanical units. Furthermore, within CGS, there are several plausible choices of electromagnetic units, leading to different unit "sub-systems", including Gaussian, "ESU", "EMU", and Heaviside–Lorentz. Among these choices, Gaussian units are the most common today, and in fact the phrase "CGS units" is often used to refer specifically to CGS-Gaussian units.

</doc>
<doc id="9476" url="https://en.wikipedia.org/wiki?curid=9476" title="Electron">
Electron

The electron is a subatomic particle, symbol or , with a negative elementary electric charge. Electrons belong to the first generation of the lepton particle family, and are generally thought to be elementary particles because they have no known components or substructure. The electron has a mass that is approximately 1/1836 that of the proton. Quantum mechanical properties of the electron include an intrinsic angular momentum (spin) of a half-integer value in units of "ħ", which means that it is a fermion. Being fermions, no two electrons can occupy the same quantum state, in accordance with the Pauli exclusion principle. Like all matter, electrons have properties of both particles and waves, and so can collide with other particles and can be diffracted like light. The wave properties of electrons are easier to observe with experiments than those of other particles like neutrons and protons because electrons have a lower mass and hence a higher De Broglie wavelength for typical energies.
Many physical phenomena involve electrons in an essential role, such as electricity, magnetism, and thermal conductivity, and they also participate in gravitational, electromagnetic and weak interactions. An electron generates an electric field surrounding it. An electron moving relative to an observer generates a magnetic field. External magnetic fields deflect an electron. Electrons radiate or absorb energy in the form of photons when accelerated. Laboratory instruments are capable of containing and observing individual electrons as well as electron plasma using electromagnetic fields, whereas dedicated telescopes can detect electron plasma in outer space. Electrons have many applications, including electronics, welding, cathode ray tubes, electron microscopes, radiation therapy, lasers, gaseous ionization detectors and particle accelerators.
Interactions involving electrons and other subatomic particles are of interest in fields such as chemistry and nuclear physics. The Coulomb force interaction between positive protons inside atomic nuclei and negative electrons composes atoms. Ionization or changes in the proportions of particles changes the binding energy of the system. The exchange or sharing of the electrons between two or more atoms is the main cause of chemical bonding. British natural philosopher Richard Laming first hypothesized the concept of an indivisible quantity of electric charge to explain the chemical properties of atoms in 1838; Irish physicist George Johnstone Stoney named this charge 'electron' in 1891, and J. J. Thomson and his team of British physicists identified it as a particle in 1897. Electrons can also participate in nuclear reactions, such as nucleosynthesis in stars, where they are known as beta particles. Electrons may be created through beta decay of radioactive isotopes and in high-energy collisions, for instance when cosmic rays enter the atmosphere. The antiparticle of the electron is called the positron; it is identical to the electron except that it carries electrical and other charges of the opposite sign. When an electron collides with a positron, both particles may be totally annihilated, producing gamma ray photons.
History.
The ancient Greeks noticed that amber attracted small objects when rubbed with fur. Along with lightning, this phenomenon is one of humanity's earliest recorded experiences with electricity.
 In his 1600 treatise , the English scientist William Gilbert coined the New Latin term , to refer to this property of attracting small objects after being rubbed.
 Both "electric" and "electricity" are derived from the Latin ' (also the root of the alloy of the same name), which came from the Greek word for amber, (').
In the early 1700s, Francis Hauksbee and French chemist Charles François de Fay independently discovered what they believed were two kinds of frictional electricity—one generated from rubbing glass, the other from rubbing resin. From this, Du Fay theorized that electricity consists of two electrical fluids, "vitreous" and "resinous", that are separated by friction, and that neutralize each other when combined. A decade later Benjamin Franklin proposed that electricity was not from different types of electrical fluid, but the same electrical fluid under different pressures. He gave them the modern charge nomenclature of positive and negative respectively. Franklin thought of the charge carrier as being positive, but he did not correctly identify which situation was a surplus of the charge carrier, and which situation was a deficit.
Between 1838 and 1851, British natural philosopher Richard Laming developed the idea that an atom is composed of a core of matter surrounded by subatomic particles that had unit electric charges. Beginning in 1846, German physicist William Weber theorized that electricity was composed of positively and negatively charged fluids, and their interaction was governed by the inverse square law. After studying the phenomenon of electrolysis in 1874, Irish physicist George Johnstone Stoney suggested that there existed a "single definite quantity of electricity", the charge of a monovalent ion. He was able to estimate the value of this elementary charge "e" by means of Faraday's laws of electrolysis. However, Stoney believed these charges were permanently attached to atoms and could not be removed. In 1881, German physicist Hermann von Helmholtz argued that both positive and negative charges were divided into elementary parts, each of which "behaves like atoms of electricity".
Stoney initially coined the term "electrolion" in 1881. Ten years later, he switched to "electron" to describe these elementary charges, writing in 1894: "... an estimate was made of the actual amount of this most remarkable fundamental unit of electricity, for which I have since ventured to suggest the name "electron"". A 1906 proposal to change to "electrion" failed because Hendrik Lorentz preferred to keep "electron". The word "electron" is a combination of the words "electric" and "ion". The suffix -"on" which is now used to designate other subatomic particles, such as a proton or neutron, is in turn derived from electron.
Discovery.
The German physicist Johann Wilhelm Hittorf studied electrical conductivity in rarefied gases: in 1869, he discovered a glow emitted from the cathode that increased in size with decrease in gas pressure. In 1876, the German physicist Eugen Goldstein showed that the rays from this glow cast a shadow, and he dubbed the rays cathode rays. During the 1870s, the English chemist and physicist Sir William Crookes developed the first cathode ray tube to have a high vacuum inside. He then showed that the luminescence rays appearing within the tube carried energy and moved from the cathode to the anode. Furthermore, by applying a magnetic field, he was able to deflect the rays, thereby demonstrating that the beam behaved as though it were negatively charged. In 1879, he proposed that these properties could be explained by what he termed 'radiant matter'. He suggested that this was a fourth state of matter, consisting of negatively charged molecules that were being projected with high velocity from the cathode.
The German-born British physicist Arthur Schuster expanded upon Crookes' experiments by placing metal plates parallel to the cathode rays and applying an electric potential between the plates. The field deflected the rays toward the positively charged plate, providing further evidence that the rays carried negative charge. By measuring the amount of deflection for a given level of current, in 1890 Schuster was able to estimate the charge-to-mass ratio of the ray components. However, this produced a value that was more than a thousand times greater than what was expected, so little credence was given to his calculations at the time.
In 1892 Hendrik Lorentz suggested that the mass of these particles (electrons) could be a consequence of their electric charge.
In 1896, the British physicist J. J. Thomson, with his colleagues John S. Townsend and H. A. Wilson, performed experiments indicating that cathode rays really were unique particles, rather than waves, atoms or molecules as was believed earlier. Thomson made good estimates of both the charge "e" and the mass "m", finding that cathode ray particles, which he called "corpuscles," had perhaps one thousandth of the mass of the least massive ion known: hydrogen. He showed that their charge to mass ratio, "e"/"m", was independent of cathode material. He further showed that the negatively charged particles produced by radioactive materials, by heated materials and by illuminated materials were universal. The name electron was again proposed for these particles by the Irish physicist George F. Fitzgerald, and the name has since gained universal acceptance.
While studying naturally fluorescing minerals in 1896, the French physicist Henri Becquerel discovered that they emitted radiation without any exposure to an external energy source. These radioactive materials became the subject of much interest by scientists, including the New Zealand physicist Ernest Rutherford who discovered they emitted particles. He designated these particles alpha and beta, on the basis of their ability to penetrate matter. In 1900, Becquerel showed that the beta rays emitted by radium could be deflected by an electric field, and that their mass-to-charge ratio was the same as for cathode rays. This evidence strengthened the view that electrons existed as components of atoms.
The electron's charge was more carefully measured by the American physicists Robert Millikan and Harvey Fletcher in their oil-drop experiment of 1909, the results of which were published in 1911. This experiment used an electric field to prevent a charged droplet of oil from falling as a result of gravity. This device could measure the electric charge from as few as 1–150 ions with an error margin of less than 0.3%. Comparable experiments had been done earlier by Thomson's team, using clouds of charged water droplets generated by electrolysis, and in 1911 by Abram Ioffe, who independently obtained the same result as Millikan using charged microparticles of metals, then published his results in 1913. However, oil drops were more stable than water drops because of their slower evaporation rate, and thus more suited to precise experimentation over longer periods of time.
Around the beginning of the twentieth century, it was found that under certain conditions a fast-moving charged particle caused a condensation of supersaturated water vapor along its path. In 1911, Charles Wilson used this principle to devise his cloud chamber so he could photograph the tracks of charged particles, such as fast-moving electrons.
Atomic theory.
By 1914, experiments by physicists Ernest Rutherford, Henry Moseley, James Franck and Gustav Hertz had largely established the structure of an atom as a dense nucleus of positive charge surrounded by lower-mass electrons. In 1913, Danish physicist Niels Bohr postulated that electrons resided in quantized energy states, with the energy determined by the angular momentum of the electron's orbits about the nucleus. The electrons could move between these states, or orbits, by the emission or absorption of photons at specific frequencies. By means of these quantized orbits, he accurately explained the spectral lines of the hydrogen atom. However, Bohr's model failed to account for the relative intensities of the spectral lines and it was unsuccessful in explaining the spectra of more complex atoms.
Chemical bonds between atoms were explained by Gilbert Newton Lewis, who in 1916 proposed that a covalent bond between two atoms is maintained by a pair of electrons shared between them. Later, in 1927, Walter Heitler and Fritz London gave the full explanation of the electron-pair formation and chemical bonding in terms of quantum mechanics. In 1919, the American chemist Irving Langmuir elaborated on the Lewis' static model of the atom and suggested that all electrons were distributed in successive "concentric (nearly) spherical shells, all of equal thickness". The shells were, in turn, divided by him in a number of cells each containing one pair of electrons. With this model Langmuir was able to qualitatively explain the chemical properties of all elements in the periodic table, which were known to largely repeat themselves according to the periodic law.
In 1924, Austrian physicist Wolfgang Pauli observed that the shell-like structure of the atom could be explained by a set of four parameters that defined every quantum energy state, as long as each state was inhabited by no more than a single electron. (This prohibition against more than one electron occupying the same quantum energy state became known as the Pauli exclusion principle.) The physical mechanism to explain the fourth parameter, which had two distinct possible values, was provided by the Dutch physicists Samuel Goudsmit and George Uhlenbeck. In 1925, Goudsmit and Uhlenbeck suggested that an electron, in addition to the angular momentum of its orbit, possesses an intrinsic angular momentum and magnetic dipole moment. The intrinsic angular momentum became known as spin, and explained the previously mysterious splitting of spectral lines observed with a high-resolution spectrograph; this phenomenon is known as fine structure splitting.
Quantum mechanics.
In his 1924 dissertation "" (Research on Quantum Theory), French physicist Louis de Broglie hypothesized that all matter possesses a de Broglie wave similar to light. That is, under the appropriate conditions, electrons and other matter would show properties of either particles or waves. The corpuscular properties of a particle are demonstrated when it is shown to have a localized position in space along its trajectory at any given moment. Wave-like nature is observed, for example, when a beam of light is passed through parallel slits and creates interference patterns. In 1927, the interference effect was found in a beam of electrons by English physicist George Paget Thomson with a thin metal film and by American physicists Clinton Davisson and Lester Germer using a crystal of nickel.
De Broglie's prediction of a wave nature for electrons led Erwin Schrödinger to postulate a wave equation for electrons moving under the influence of the nucleus in the atom. In 1926, this equation, the Schrödinger equation, successfully described how electron waves propagated. Rather than yielding a solution that determined the location of an electron over time, this wave equation also could be used to predict the probability of finding an electron near a position, especially a position near where the electron was bound in space, for which the electron wave equations did not change in time. This approach led to a second formulation of quantum mechanics (the first being by Heisenberg in 1925), and solutions of Schrödinger's equation, like Heisenberg's, provided derivations of the energy states of an electron in a hydrogen atom that were equivalent to those that had been derived first by Bohr in 1913, and that were known to reproduce the hydrogen spectrum. Once spin and the interaction between multiple electrons were considered, quantum mechanics later made it possible to predict the configuration of electrons in atoms with higher atomic numbers than hydrogen.
In 1928, building on Wolfgang Pauli's work, Paul Dirac produced a model of the electron – the Dirac equation, consistent with relativity theory, by applying relativistic and symmetry considerations to the hamiltonian formulation of the quantum mechanics of the electro-magnetic field. To resolve some problems within his relativistic equation, in 1930 Dirac developed a model of the vacuum as an infinite sea of particles having negative energy, which was dubbed the Dirac sea. This led him to predict the existence of a positron, the antimatter counterpart of the electron. This particle was discovered in 1932 by Carl Anderson, who proposed calling standard electrons "negatrons", and using "electron" as a generic term to describe both the positively and negatively charged variants.
In 1947 Willis Lamb, working in collaboration with graduate student Robert Retherford, found that certain quantum states of hydrogen atom, which should have the same energy, were shifted in relation to each other, the difference being the Lamb shift. About the same time, Polykarp Kusch, working with Henry M. Foley, discovered the magnetic moment of the electron is slightly larger than predicted by Dirac's theory. This small difference was later called anomalous magnetic dipole moment of the electron. This difference was later explained by the theory of quantum electrodynamics, developed by Sin-Itiro Tomonaga, Julian Schwinger and
Richard Feynman in the late 1940s.
Particle accelerators.
With the development of the particle accelerator during the first half of the twentieth century, physicists began to delve deeper into the properties of subatomic particles. The first successful attempt to accelerate electrons using electromagnetic induction was made in 1942 by Donald Kerst. His initial betatron reached energies of 2.3 MeV, while subsequent betatrons achieved 300 MeV. In 1947, synchrotron radiation was discovered with a 70 MeV electron synchrotron at General Electric. This radiation was caused by the acceleration of electrons, moving near the speed of light, through a magnetic field.
With a beam energy of 1.5 GeV, the first high-energy
particle collider was ADONE, which began operations in 1968. This device accelerated electrons and positrons in opposite directions, effectively doubling the energy of their collision when compared to striking a static target with an electron. The Large Electron–Positron Collider (LEP) at CERN, which was operational from 1989 to 2000, achieved collision energies of 209 GeV and made important measurements for the Standard Model of particle physics.
Confinement of individual electrons.
Individual electrons can now be easily confined in ultra small (, ) CMOS transistors operated at cryogenic temperature over a range of −269 °C (4 K) to about −258 °C (15 K). The electron wavefunction spreads in a semiconductor lattice and negligibly interacts with the valence band electrons, so it can be treated in the single particle formalism, by replacing its mass with the effective mass tensor.
Characteristics.
Classification.
In the Standard Model of particle physics, electrons belong to the group of subatomic particles called leptons, which are believed to be fundamental or elementary particles. Electrons have the lowest mass of any charged lepton (or electrically charged particle of any type) and belong to the first-generation of fundamental particles. The second and third generation contain charged leptons, the muon and the tau, which are identical to the electron in charge, spin and interactions, but are more massive. Leptons differ from the other basic constituent of matter, the quarks, by their lack of strong interaction. All members of the lepton group are fermions, because they all have half-odd integer spin; the electron has spin .
Fundamental properties.
The invariant mass of an electron is approximately kilograms, or atomic mass units. On the basis of Einstein's principle of mass–energy equivalence, this mass corresponds to a rest energy of 0.511 MeV. The ratio between the mass of a proton and that of an electron is about 1836. Astronomical measurements show that the proton-to-electron mass ratio has held the same value for at least half the age of the universe, as is predicted by the Standard Model.
Electrons have an electric charge of coulomb, which is used as a standard unit of charge for subatomic particles, and is also called the elementary charge. This elementary charge has a relative standard uncertainty of . Within the limits of experimental accuracy, the electron charge is identical to the charge of a proton, but with the opposite sign. As the symbol "e" is used for the elementary charge, the electron is commonly symbolized by , where the minus sign indicates the negative charge. The positron is symbolized by because it has the same properties as the electron but with a positive rather than negative charge.
The electron has an intrinsic angular momentum or spin of . This property is usually stated by referring to the electron as a spin- particle. For such particles the spin magnitude is  "ħ". while the result of the measurement of a projection of the spin on any axis can only be ±. In addition to spin, the electron has an intrinsic magnetic moment along its spin axis. It is approximately equal to one Bohr magneton,=\frac{e\hbar}{2m_{\mathrm{e}}}.</math>|group=note}} which is a physical constant equal to . The orientation of the spin with respect to the momentum of the electron defines the property of elementary particles known as helicity.
The electron has no known substructure. and it is assumed to be a point particle with a point charge and no spatial extent. In classical physics, the angular momentum and magnetic moment of an object depend upon its physical dimensions. Hence, the concept of a dimensionless electron possessing these properties might seem paradoxical and inconsistent to experimental observations in Penning traps which point to finite non-zero radius of the electron. A possible explanation of this paradoxical situation is given below in the "Virtual particles" subsection by taking into consideration the Foldy-Wouthuysen transformation. The issue of the radius of the electron is a challenging problem of the modern theoretical physics. The admission of the hypothesis of a finite radius of the electron is incompatible to the premises of the theory of relativity. On the other hand, a point-like electron (zero radius) generates serious mathematical difficulties due to the self-energy of the electron tending to infinity. These aspects have been analyzed in detail by Dmitri Ivanenko and Arseny Sokolov.
Observation of a single electron in a Penning trap shows the upper limit of the particle's radius is 10−22 meters. There "is" a physical constant called the "classical electron radius", with the much larger value of , greater than the radius of the proton. However, the terminology comes from a simplistic calculation that ignores the effects of quantum mechanics; in reality, the so-called classical electron radius has little to do with the true fundamental structure of the electron.
There are elementary particles that spontaneously decay into less massive particles. An example is the muon, which decays into an electron, a neutrino and an antineutrino, with a mean lifetime of seconds. However, the electron is thought to be stable on theoretical grounds: the electron is the least massive particle with non-zero electric charge, so its decay would violate charge conservation. The experimental lower bound for the electron's mean lifetime is years, at a 90% confidence level.
Quantum properties.
As with all particles, electrons can act as waves. This is called the wave–particle duality and can be demonstrated using the double-slit experiment.
The wave-like nature of the electron allows it to pass through two parallel slits simultaneously, rather than just one slit as would be the case for a classical particle. In quantum mechanics, the wave-like property of one particle can be described mathematically as a complex-valued function, the wave function, commonly denoted by the Greek letter psi ("ψ"). When the absolute value of this function is squared, it gives the probability that a particle will be observed near a location—a probability density.
Electrons are identical particles because they cannot be distinguished from each other by their intrinsic physical properties. In quantum mechanics, this means that a pair of interacting electrons must be able to swap positions without an observable change to the state of the system. The wave function of fermions, including electrons, is antisymmetric, meaning that it changes sign when two electrons are swapped; that is, , where the variables "r"1 and "r"2 correspond to the first and second electrons, respectively. Since the absolute value is not changed by a sign swap, this corresponds to equal probabilities. Bosons, such as the photon, have symmetric wave functions instead.
In the case of antisymmetry, solutions of the wave equation for interacting electrons result in a zero probability that each pair will occupy the same location or state. This is responsible for the Pauli exclusion principle, which precludes any two electrons from occupying the same quantum state. This principle explains many of the properties of electrons. For example, it causes groups of bound electrons to occupy different orbitals in an atom, rather than all overlapping each other in the same orbit.
Virtual particles.
In a simplified picture, every photon spends some time as a combination of a virtual electron plus its antiparticle, the virtual positron, which rapidly annihilate each other shortly thereafter. The combination of the energy variation needed to create these particles, and the time during which they exist, fall under the threshold of detectability expressed by the Heisenberg uncertainty relation, Δ"E" · Δ"t" ≥ "ħ". In effect, the energy needed to create these virtual particles, Δ"E", can be "borrowed" from the vacuum for a period of time, Δ"t", so that their product is no more than the reduced Planck constant, . Thus, for a virtual electron, Δ"t" is at most .
While an electron–positron virtual pair is in existence, the coulomb force from the ambient electric field surrounding an electron causes a created positron to be attracted to the original electron, while a created electron experiences a repulsion. This causes what is called vacuum polarization. In effect, the vacuum behaves like a medium having a dielectric permittivity more than unity. Thus the effective charge of an electron is actually smaller than its true value, and the charge decreases with increasing distance from the electron. This polarization was confirmed experimentally in 1997 using the Japanese TRISTAN particle accelerator. Virtual particles cause a comparable shielding effect for the mass of the electron.
The interaction with virtual particles also explains the small (about 0.1%) deviation of the intrinsic magnetic moment of the electron from the Bohr magneton (the anomalous magnetic moment). The extraordinarily precise agreement of this predicted difference with the experimentally determined value is viewed as one of the great achievements of quantum electrodynamics.
The apparent paradox (mentioned above in the properties subsection) of a point particle electron having intrinsic angular momentum and magnetic moment can be explained by the formation of virtual photons in the electric field generated by the electron. These photons cause the electron to shift about in a jittery fashion (known as zitterbewegung), which results in a net circular motion with precession. This motion produces both the spin and the magnetic moment of the electron. In atoms, this creation of virtual photons explains the Lamb shift observed in spectral lines.
Interaction.
An electron generates an electric field that exerts an attractive force on a particle with a positive charge, such as the proton, and a repulsive force on a particle with a negative charge. The strength of this force is determined by Coulomb's inverse square law. When an electron is in motion, it generates a magnetic field. The Ampère-Maxwell law relates the magnetic field to the mass motion of electrons (the current) with respect to an observer. This property of induction supplies the magnetic field that drives an electric motor. The electromagnetic field of an arbitrary moving charged particle is expressed by the Liénard–Wiechert potentials, which are valid even when the particle's speed is close to that of light (relativistic).
When an electron is moving through a magnetic field, it is subject to the Lorentz force that acts perpendicularly to the plane defined by the magnetic field and the electron velocity. This centripetal force causes the electron to follow a helical trajectory through the field at a radius called the gyroradius. The acceleration from this curving motion induces the electron to radiate energy in the form of synchrotron radiation. The energy emission in turn causes a recoil of the electron, known as the Abraham–Lorentz–Dirac Force, which creates a friction that slows the electron. This force is caused by a back-reaction of the electron's own field upon itself.
Photons mediate electromagnetic interactions between particles in quantum electrodynamics. An isolated electron at a constant velocity cannot emit or absorb a real photon; doing so would violate conservation of energy and momentum. Instead, virtual photons can transfer momentum between two charged particles. This exchange of virtual photons, for example, generates the Coulomb force. Energy emission can occur when a moving electron is deflected by a charged particle, such as a proton. The acceleration of the electron results in the emission of Bremsstrahlung radiation.
An inelastic collision between a photon (light) and a solitary (free) electron is called Compton scattering. This collision results in a transfer of momentum and energy between the particles, which modifies the wavelength of the photon by an amount called the Compton shift. The maximum magnitude of this wavelength shift is "h"/"m"e"c", which is known as the Compton wavelength. For an electron, it has a value of . When the wavelength of the light is long (for instance, the wavelength of the visible light is 0.4–0.7 μm) the wavelength shift becomes negligible. Such interaction between the light and free electrons is called Thomson scattering or Linear Thomson scattering.
The relative strength of the electromagnetic interaction between two charged particles, such as an electron and a proton, is given by the fine-structure constant. This value is a dimensionless quantity formed by the ratio of two energies: the electrostatic energy of attraction (or repulsion) at a separation of one Compton wavelength, and the rest energy of the charge. It is given by "α" ≈ , which is approximately equal to .
When electrons and positrons collide, they annihilate each other, giving rise to two or more gamma ray photons. If the electron and positron have negligible momentum, a positronium atom can form before annihilation results in two or three gamma ray photons totalling 1.022 MeV. On the other hand, high-energy photons may transform into an electron and a positron by a process called pair production, but only in the presence of a nearby charged particle, such as a nucleus.
In the theory of electroweak interaction, the left-handed component of electron's wavefunction forms a weak isospin doublet with the electron neutrino. This means that during weak interactions, electron neutrinos behave like electrons. Either member of this doublet can undergo a charged current interaction by emitting or absorbing a and be converted into the other member. Charge is conserved during this reaction because the W boson also carries a charge, canceling out any net change during the transmutation. Charged current interactions are responsible for the phenomenon of beta decay in a radioactive atom. Both the electron and electron neutrino can undergo a neutral current interaction via a exchange, and this is responsible for neutrino-electron elastic scattering.
Atoms and molecules.
An electron can be "bound" to the nucleus of an atom by the attractive Coulomb force. A system of one or more electrons bound to a nucleus is called an atom. If the number of electrons is different from the nucleus' electrical charge, such an atom is called an ion. The wave-like behavior of a bound electron is described by a function called an atomic orbital. Each orbital has its own set of quantum numbers such as energy, angular momentum and projection of angular momentum, and only a discrete set of these orbitals exist around the nucleus. According to the Pauli exclusion principle each orbital can be occupied by up to two electrons, which must differ in their spin quantum number.
Electrons can transfer between different orbitals by the emission or absorption of photons with an energy that matches the difference in potential. Other methods of orbital transfer include collisions with particles, such as electrons, and the Auger effect. To escape the atom, the energy of the electron must be increased above its binding energy to the atom. This occurs, for example, with the photoelectric effect, where an incident photon exceeding the atom's ionization energy is absorbed by the electron.
The orbital angular momentum of electrons is quantized. Because the electron is charged, it produces an orbital magnetic moment that is proportional to the angular momentum. The net magnetic moment of an atom is equal to the vector sum of orbital and spin magnetic moments of all electrons and the nucleus. The magnetic moment of the nucleus is negligible compared with that of the electrons. The magnetic moments of the electrons that occupy the same orbital (so called, paired electrons) cancel each other out.
The chemical bond between atoms occurs as a result of electromagnetic interactions, as described by the laws of quantum mechanics. The strongest bonds are formed by the sharing or transfer of electrons between atoms, allowing the formation of molecules. Within a molecule, electrons move under the influence of several nuclei, and occupy molecular orbitals; much as they can occupy atomic orbitals in isolated atoms. A fundamental factor in these molecular structures is the existence of electron pairs. These are electrons with opposed spins, allowing them to occupy the same molecular orbital without violating the Pauli exclusion principle (much like in atoms). Different molecular orbitals have different spatial distribution of the electron density. For instance, in bonded pairs (i.e. in the pairs that actually bind atoms together) electrons can be found with the maximal probability in a relatively small volume between the nuclei. On the contrary, in non-bonded pairs electrons are distributed in a large volume around nuclei.
Conductivity.
If a body has more or fewer electrons than are required to balance the positive charge of the nuclei, then that object has a net electric charge. When there is an excess of electrons, the object is said to be negatively charged. When there are fewer electrons than the number of protons in nuclei, the object is said to be positively charged. When the number of electrons and the number of protons are equal, their charges cancel each other and the object is said to be electrically neutral. A macroscopic body can develop an electric charge through rubbing, by the triboelectric effect.
Independent electrons moving in vacuum are termed "free" electrons. Electrons in metals also behave as if they were free. In reality the particles that are commonly termed electrons in metals and other solids are quasi-electrons—quasiparticles, which have the same electrical charge, spin and magnetic moment as real electrons but may have a different mass. When free electrons—both in vacuum and metals—move, they produce a net flow of charge called an electric current, which generates a magnetic field. Likewise a current can be created by a changing magnetic field. These interactions are described mathematically by Maxwell's equations.
At a given temperature, each material has an electrical conductivity that determines the value of electric current when an electric potential is applied. Examples of good conductors include metals such as copper and gold, whereas glass and Teflon are poor conductors. In any dielectric material, the electrons remain bound to their respective atoms and the material behaves as an insulator. Most semiconductors have a variable level of conductivity that lies between the extremes of conduction and insulation. On the other hand, metals have an electronic band structure containing partially filled electronic bands. The presence of such bands allows electrons in metals to behave as if they were free or delocalized electrons. These electrons are not associated with specific atoms, so when an electric field is applied, they are free to move like a gas (called Fermi gas) through the material much like free electrons.
Because of collisions between electrons and atoms, the drift velocity of electrons in a conductor is on the order of millimeters per second. However, the speed at which a change of current at one point in the material causes changes in currents in other parts of the material, the velocity of propagation, is typically about 75% of light speed. This occurs because electrical signals propagate as a wave, with the velocity dependent on the dielectric constant of the material.
Metals make relatively good conductors of heat, primarily because the delocalized electrons are free to transport thermal energy between atoms. However, unlike electrical conductivity, the thermal conductivity of a metal is nearly independent of temperature. This is expressed mathematically by the Wiedemann–Franz law, which states that the ratio of thermal conductivity to the electrical conductivity is proportional to the temperature. The thermal disorder in the metallic lattice increases the electrical resistivity of the material, producing a temperature dependence for electric current.
When cooled below a point called the critical temperature, materials can undergo a phase transition in which they lose all resistivity to electric current, in a process known as superconductivity. In BCS theory, this behavior is modeled by pairs of electrons entering a quantum state known as a Bose–Einstein condensate. These Cooper pairs have their motion coupled to nearby matter via lattice vibrations called phonons, thereby avoiding the collisions with atoms that normally create electrical resistance. (Cooper pairs have a radius of roughly 100 nm, so they can overlap each other.) However, the mechanism by which higher temperature superconductors operate remains uncertain.
Electrons inside conducting solids, which are quasi-particles themselves, when tightly confined at temperatures close to absolute zero, behave as though they had split into three other quasiparticles: spinons, orbitons and holons. The former carries spin and magnetic moment, the next carries its orbital location while the latter electrical charge.
Motion and energy.
According to Einstein's theory of special relativity, as an electron's speed approaches the speed of light, from an observer's point of view its relativistic mass increases, thereby making it more and more difficult to accelerate it from within the observer's frame of reference. The speed of an electron can approach, but never reach, the speed of light in a vacuum, "c". However, when relativistic electrons—that is, electrons moving at a speed close to "c"—are injected into a dielectric medium such as water, where the local speed of light is significantly less than "c", the electrons temporarily travel faster than light in the medium. As they interact with the medium, they generate a faint light called Cherenkov radiation.
The effects of special relativity are based on a quantity known as the Lorentz factor, defined as formula_1 where "v" is the speed of the particle. The kinetic energy "K"e of an electron moving with velocity "v" is:
where "m"e is the mass of electron. For example, the Stanford linear accelerator can accelerate an electron to roughly 51 GeV.
Since an electron behaves as a wave, at a given velocity it has a characteristic de Broglie wavelength. This is given by "λ"e = "h"/"p" where "h" is the Planck constant and "p" is the momentum. For the 51 GeV electron above, the wavelength is about , small enough to explore structures well below the size of an atomic nucleus.
Formation.
The Big Bang theory is the most widely accepted scientific theory to explain the early stages in the evolution of the Universe. For the first millisecond of the Big Bang, the temperatures were over 10 billion Kelvin and photons had mean energies over a million electronvolts. These photons were sufficiently energetic that they could react with each other to form pairs of electrons and positrons. Likewise, positron-electron pairs annihilated each other and emitted energetic photons:
An equilibrium between electrons, positrons and photons was maintained during this phase of the evolution of the Universe. After 15 seconds had passed, however, the temperature of the universe dropped below the threshold where electron-positron formation could occur. Most of the surviving electrons and positrons annihilated each other, releasing gamma radiation that briefly reheated the universe.
For reasons that remain uncertain, during the process of leptogenesis there was an excess in the number of electrons over positrons. Hence, about one electron in every billion survived the annihilation process. This excess matched the excess of protons over antiprotons, in a condition known as baryon asymmetry, resulting in a net charge of zero for the universe. The surviving protons and neutrons began to participate in reactions with each other—in the process known as nucleosynthesis, forming isotopes of hydrogen and helium, with trace amounts of lithium. This process peaked after about five minutes. Any leftover neutrons underwent negative beta decay with a half-life of about a thousand seconds, releasing a proton and electron in the process,
For about the next –, the excess electrons remained too energetic to bind with atomic nuclei. What followed is a period known as recombination, when neutral atoms were formed and the expanding universe became transparent to radiation.
Roughly one million years after the big bang, the first generation of stars began to form. Within a star, stellar nucleosynthesis results in the production of positrons from the fusion of atomic nuclei. These antimatter particles immediately annihilate with electrons, releasing gamma rays. The net result is a steady reduction in the number of electrons, and a matching increase in the number of neutrons. However, the process of stellar evolution can result in the synthesis of radioactive isotopes. Selected isotopes can subsequently undergo negative beta decay, emitting an electron and antineutrino from the nucleus. An example is the cobalt-60 (60Co) isotope, which decays to form nickel-60 ().
At the end of its lifetime, a star with more than about 20 solar masses can undergo gravitational collapse to form a black hole. According to classical physics, these massive stellar objects exert a gravitational attraction that is strong enough to prevent anything, even electromagnetic radiation, from escaping past the Schwarzschild radius. However, quantum mechanical effects are believed to potentially allow the emission of Hawking radiation at this distance. Electrons (and positrons) are thought to be created at the event horizon of these stellar remnants.
When pairs of virtual particles (such as an electron and positron) are created in the vicinity of the event horizon, the random spatial distribution of these particles may permit one of them to appear on the exterior; this process is called quantum tunnelling. The gravitational potential of the black hole can then supply the energy that transforms this virtual particle into a real particle, allowing it to radiate away into space. In exchange, the other member of the pair is given negative energy, which results in a net loss of mass-energy by the black hole. The rate of Hawking radiation increases with decreasing mass, eventually causing the black hole to evaporate away until, finally, it explodes.
Cosmic rays are particles traveling through space with high energies. Energy events as high as have been recorded. When these particles collide with nucleons in the Earth's atmosphere, a shower of particles is generated, including pions. More than half of the cosmic radiation observed from the Earth's surface consists of muons. The particle called a muon is a lepton produced in the upper atmosphere by the decay of a pion.
A muon, in turn, can decay to form an electron or positron.
Observation.
Remote observation of electrons requires detection of their radiated energy. For example, in high-energy environments such as the corona of a star, free electrons form a plasma that radiates energy due to Bremsstrahlung radiation. Electron gas can undergo plasma oscillation, which is waves caused by synchronized variations in electron density, and these produce energy emissions that can be detected by using radio telescopes.
The frequency of a photon is proportional to its energy. As a bound electron transitions between different energy levels of an atom, it absorbs or emits photons at characteristic frequencies. For instance, when atoms are irradiated by a source with a broad spectrum, distinct absorption lines appear in the spectrum of transmitted radiation. Each element or molecule displays a characteristic set of spectral lines, such as the hydrogen spectral series. Spectroscopic measurements of the strength and width of these lines allow the composition and physical properties of a substance to be determined.
In laboratory conditions, the interactions of individual electrons can be observed by means of particle detectors, which allow measurement of specific properties such as energy, spin and charge. The development of the Paul trap and Penning trap allows charged particles to be contained within a small region for long durations. This enables precise measurements of the particle properties. For example, in one instance a Penning trap was used to contain a single electron for a period of 10 months. The magnetic moment of the electron was measured to a precision of eleven digits, which, in 1980, was a greater accuracy than for any other physical constant.
The first video images of an electron's energy distribution were captured by a team at Lund University in Sweden, February 2008. The scientists used extremely short flashes of light, called attosecond pulses, which allowed an electron's motion to be observed for the first time.
The distribution of the electrons in solid materials can be visualized by angle-resolved photoemission spectroscopy (ARPES). This technique employs the photoelectric effect to measure the reciprocal space—a mathematical representation of periodic structures that is used to infer the original structure. ARPES can be used to determine the direction, speed and scattering of electrons within the material.
Plasma applications.
Particle beams.
Electron beams are used in welding. They allow energy densities up to across a narrow focus diameter of and usually require no filler material. This welding technique must be performed in a vacuum to prevent the electrons from interacting with the gas before reaching their target, and it can be used to join conductive materials that would otherwise be considered unsuitable for welding.
Electron-beam lithography (EBL) is a method of etching semiconductors at resolutions smaller than a micrometer. This technique is limited by high costs, slow performance, the need to operate the beam in the vacuum and the tendency of the electrons to scatter in solids. The last problem limits the resolution to about 10 nm. For this reason, EBL is primarily used for the production of small numbers of specialized integrated circuits.
Electron beam processing is used to irradiate materials in order to change their physical properties or sterilize medical and food products. Electron beams fluidise or quasi-melt glasses without significant increase of temperature on intensive irradiation: e.g. intensive electron radiation causes a many orders of magnitude decrease of viscosity and stepwise decrease of its activation energy.
Linear particle accelerators generate electron beams for treatment of superficial tumors in radiation therapy. Electron therapy can treat such skin lesions as basal-cell carcinomas because an electron beam only penetrates to a limited depth before being absorbed, typically up to 5 cm for electron energies in the range 5–20 MeV. An electron beam can be used to supplement the treatment of areas that have been irradiated by X-rays.
Particle accelerators use electric fields to propel electrons and their antiparticles to high energies. These particles emit synchrotron radiation as they pass through magnetic fields. The dependency of the intensity of this radiation upon spin polarizes the electron beam—a process known as the Sokolov–Ternov effect. Polarized electron beams can be useful for various experiments. Synchrotron radiation can also cool the electron beams to reduce the momentum spread of the particles. Electron and positron beams are collided upon the particles' accelerating to the required energies; particle detectors observe the resulting energy emissions, which particle physics studies .
Imaging.
Low-energy electron diffraction (LEED) is a method of bombarding a crystalline material with a collimated beam of electrons and then observing the resulting diffraction patterns to determine the structure of the material. The required energy of the electrons is typically in the range 20–200 eV. The reflection high-energy electron diffraction (RHEED) technique uses the reflection of a beam of electrons fired at various low angles to characterize the surface of crystalline materials. The beam energy is typically in the range 8–20 keV and the angle of incidence is 1–4°.
The electron microscope directs a focused beam of electrons at a specimen. Some electrons change their properties, such as movement direction, angle, and relative phase and energy as the beam interacts with the material. Microscopists can record these changes in the electron beam to produce atomically resolved images of the material. In blue light, conventional optical microscopes have a diffraction-limited resolution of about 200 nm. By comparison, electron microscopes are limited by the de Broglie wavelength of the electron. This wavelength, for example, is equal to 0.0037 nm for electrons accelerated across a 100,000-volt potential. The Transmission Electron Aberration-Corrected Microscope is capable of sub-0.05 nm resolution, which is more than enough to resolve individual atoms. This capability makes the electron microscope a useful laboratory instrument for high resolution imaging. However, electron microscopes are expensive instruments that are costly to maintain.
Two main types of electron microscopes exist: transmission and scanning. Transmission electron microscopes function like overhead projectors, with a beam of electrons passing through a slice of material then being projected by lenses on a photographic slide or a charge-coupled device. Scanning electron microscopes rasteri a finely focused electron beam, as in a TV set, across the studied sample to produce the image. Magnifications range from 100× to 1,000,000× or higher for both microscope types. The scanning tunneling microscope uses quantum tunneling of electrons from a sharp metal tip into the studied material and can produce atomically resolved images of its surface.
Other applications.
In the free-electron laser (FEL), a relativistic electron beam passes through a pair of undulators that contain arrays of dipole magnets whose fields point in alternating directions. The electrons emit synchrotron radiation that coherently interacts with the same electrons to strongly amplify the radiation field at the resonance frequency. FEL can emit a coherent high-brilliance electromagnetic radiation with a wide range of frequencies, from microwaves to soft X-rays. These devices may find manufacturing, communication and various medical applications, such as soft tissue surgery.
Electrons are important in cathode ray tubes, which have been extensively used as display devices in laboratory instruments, computer monitors and television sets. In a photomultiplier tube, every photon striking the photocathode initiates an avalanche of electrons that produces a detectable current pulse. Vacuum tubes use the flow of electrons to manipulate electrical signals, and they played a critical role in the development of electronics technology. However, they have been largely supplanted by solid-state devices such as the transistor.

</doc>
<doc id="82381" url="https://en.wikipedia.org/wiki?curid=82381" title="Electron capture">
Electron capture

Electron capture (K-electron capture, also K-capture, or L-electron capture, L-capture) is a process in which the proton-rich nucleus of an electrically neutral atom absorbs an inner atomic electron, usually from the K or L electron shell. This process thereby changes a nuclear proton to a neutron and simultaneously causes the emission of an electron neutrino. 
The daughter nuclide, if it is in an excited state, then transitions to its ground state. Usually, a gamma ray is emitted during this transition, but nuclear de-excitation may also take place by internal conversion.
Following capture of an inner electron from the atom, an outer electron replaces the electron that was captured and one or more characteristic X-ray photons is emitted in this process. Electron capture sometimes also results in the Auger effect, where an electron is ejected from the atom's electron shell due to interactions between the atom's electrons in the process of seeking a lower energy electron state. 
Following electron capture, the atomic number is reduced by one, the neutron number is increased by one, and there is no change in atomic mass. Simple electron capture results in a neutral atom, since the loss of the electron in the electron shell is balanced by a loss of positive nuclear charge. However, a positive atomic ion may result from further Auger electron emission. 
Electron capture is an example of weak interaction, one of the four fundamental forces.
Electron capture is the primary decay mode for isotopes with a relative superabundance of protons in the nucleus, but with insufficient energy difference between the isotope and its prospective daughter (the isobar with one less positive charge) for the nuclide to decay by emitting a positron. Electron capture is always an alternate decay mode for radioactive isotopes that do not have sufficient energy to decay by positron emission. It is sometimes called inverse beta decay, though this term can also refer to the interaction of an electron antineutrino with a proton.
If the energy difference between the parent atom and the daughter atom is less than 1.022 MeV, positron emission is forbidden as not enough decay energy is available to allow it, and thus electron capture is the sole decay mode. For example, rubidium-83 (37 protons, 46 neutrons) will decay to krypton-83 (36 protons, 47 neutrons) solely by electron capture (the energy difference, or decay energy, is about 0.9 MeV).
A free proton cannot normally be changed to a free neutron by this process; the proton and neutron must be part of a larger nucleus.
History.
The theory of electron capture was first discussed by Gian-Carlo Wick in a 1934 paper, and then developed by Hideki Yukawa and others. K-electron capture was first observed by Luis Alvarez, in vanadium-48. He reported it in a 1937 paper in "Physical Review". Alvarez went on to study electron capture in gallium-67 and other nuclides.
Reaction details.
Examples:
The electron that is captured is one of the atom's own electrons, and not a new, incoming electron, as might be suggested by the way the above reactions are written. Radioactive isotopes that decay by pure electron capture can be inhibited from radioactive decay if they are fully ionized ("stripped" is sometimes used to describe such ions). It is hypothesized that such elements, if formed by the r-process in exploding supernovae, are ejected fully ionized and so do not undergo radioactive decay as long as they do not encounter electrons in outer space. Anomalies in elemental distributions are thought to be partly a result of this effect on electron capture. Inverse decays can also be induced by full ionisation; for instance, 163Ho decays into 163Dy by electron capture; however, a fully ionised 163Dy decays into a bound state of 163Ho by the process of bound-state β− decay.
Chemical bonds can also affect the rate of electron capture to a small degree (in general, less than 1%) depending on the proximity of electrons to the nucleus. For example in 7Be, a difference of 0.9% has been observed between half-lives in metallic and insulating environments. This relatively large effect is due to the fact that beryllium is a small atom whose valence electrons are close to the nucleus.
Around the elements in the middle of the periodic table, isotopes that are lighter than stable isotopes of the same element tend to decay through electron capture, while isotopes heavier than the stable ones decay by electron emission. Electron capture happens most often in the heavier neutron-deficient elements where the mass change is smallest and positron emission isn't always possible. When the loss of mass in a nuclear reaction is greater than zero but less than 2m[0-1e-], the process cannot occur by positron emission but is spontaneous for electron capture.
Common examples.
Some common radioisotopes that decay by electron capture include:
For a full list, see the table of nuclides.

</doc>
<doc id="142534" url="https://en.wikipedia.org/wiki?curid=142534" title="Electron hole">
Electron hole

In physics, chemistry, and electronic engineering, an electron hole is the lack of an electron at a position where one could exist in an atom or atomic lattice. It is different from the positron, which is an actual particle of antimatter.
If an electron is excited into a higher state it leaves a hole in its old state. This meaning is used in Auger electron spectroscopy (and other x-ray techniques), in computational chemistry, and to explain the low electron-electron scattering-rate in crystals (metals, semiconductors).
In crystals, electronic band structure calculations lead to an effective mass for the electrons, which typically is negative at the top of a band. The negative mass is an unintuitive concept, and in these situations a more familiar picture is found by considering a positive charge with a positive mass.
Solid-state physics.
In solid-state physics, an electron hole (usually referred to simply as a hole) is the absence of an electron from a full valence band. A hole is essentially a way to conceptualize the interactions of the electrons within a nearly "full" system, which is "missing" just a few electrons. In some ways, the behavior of a hole within a semiconductor crystal lattice is comparable to that of the bubble in a full bottle of water.
Simplified analogy: Empty seat in an auditorium.
Hole conduction in a valence band can be explained by the following analogy. Imagine a row of people seated in an auditorium, where there are no spare chairs. Someone in the middle of the row wants to leave, so he jumps over the back of the seat into an empty row, and walks out. The empty row is analogous to the conduction band, and the person walking out is analogous to a free electron.
Now imagine someone else comes along and wants to sit down. The empty row has a poor view; so he does not want to sit there. Instead, a person in the crowded row moves into the empty seat the first person left behind. The empty seat moves one spot closer to the edge and the person waiting to sit down. The next person follows, and the next, et cetera. One could say that the empty seat moves towards the edge of the row. Once the empty seat reaches the edge, the new person can sit down.
In the process everyone in the row has moved along. If those people were negatively charged (like electrons), this movement would constitute conduction. If the seats themselves were positively charged, then only the vacant seat would be positive. This is a very simple model of how hole conduction works.
In reality, due to the crystal structure properties, the hole is not localized to a single position as described in the previous example. Rather, the hole spans an area in the crystal lattice covering many hundreds of unit cells. This is equivalent to being unable to tell which broken bond corresponds to the "missing" electron.
Instead of analyzing the movement of an empty state in the valence band as the movement of many separate electrons, a single equivalent imaginary particle called a "hole" is considered. In an applied electric field, the electrons move in one direction, corresponding to the hole moving in the other. If a hole associates itself with a neutral atom, that atom loses an electron and becomes positive. Therefore, the hole is taken to have positive charge of +e, precisely the opposite of the electron charge.
Detailed picture: A hole is the absence of a negative-mass electron.
The analogy above is quite simplified, and cannot explain why holes create an opposite effect to electrons in the Hall effect and Seebeck effect. A more precise and detailed explanation follows.
A dispersion relation is the relationship between wavevector (k-vector) and energy in a band, part of the electronic band structure. In quantum mechanics, the electrons are waves, and energy is the wave frequency. A localized electron is a wavepacket, and the motion of an electron is given by the formula for the group velocity of a wave. An electric field affects an electron by gradually shifting all the wavevectors in the wavepacket, and the electron moves because its wave group velocity changes. Therefore, again, the way an electron responds to forces is entirely determined by its dispersion relation. An electron floating in space has the dispersion relation "E"=ℏ2"k"2/(2"m"), where "m" is the (real) electron mass and ℏ is reduced Planck constant. In the conduction band of a semiconductor, the dispersion relation is instead "E"=ℏ2"k"2/(2"m"*) ("m"* is the "effective mass"), so a conduction-band electron responds to forces "as if" it had the mass "m"*.
The dispersion relation near the top of the valence band is "E"=ℏ2k2/(2"m"*) with "negative" effective mass. So electrons near the top of the valence band behave like they have negative mass. When a force pulls the electrons to the right, these electrons actually move left. This is solely due to the shape of the valence band, and is unrelated to whether the band is full or empty. If you could somehow empty out the valence band and just put one electron near the valence band maximum (an unstable situation), this electron would move the "wrong way" in response to forces.
A perfectly full band always has zero current. One way to think about this fact is that the electron states near the top of the band have negative effective mass, and those near the bottom of the band have positive effective mass, so the net motion is exactly zero. If an otherwise-almost-full valence band has a state "without" an electron in it, we say that this state is occupied by a hole. There is a mathematical shortcut for calculating the current due to every electron in the whole valence band: Start with zero current (the total if the band were full), and "subtract" the current due to the electrons that "would" be in each hole state if it wasn't a hole. Since "subtracting" the current caused by a "negative" charge in motion is the same as "adding" the current caused by a "positive" charge moving on the same path, the mathematical shortcut is to pretend that each hole state is carrying a positive charge, while ignoring every other electron state in the valence band.
This fact follows from the discussion and definition above. This is an example where the auditorium analogy above is misleading. When a person moves left in a full auditorium, an empty seat moves right. But in this section we are imagining how electrons move through k-space, not real space, and the effect of a force is to move all the electrons through k-space in the same direction at the same time. So a better analogy is a bubble underwater in a river: The bubble moves the same direction as the water, not opposite. Since force = mass × acceleration, a negative-effective-mass electron near the top of the valence band would move the opposite direction as a positive-effective-mass electron near the bottom of the conduction band, in response to a given electric or magnetic force.
From the above, a hole (1) carries a positive charge, and (2) responds to electric and magnetic fields as if it had a positive charge and positive mass. (The latter is because a particle with positive charge and positive mass responds to electric and magnetic fields in the same way as a particle with negative charge and negative mass.) That explains why holes can be treated in all situations as ordinary positively charged quasiparticles.
Role in semiconductor technology.
In some semiconductors, such as silicon, the hole's effective mass is dependent on direction (anisotropic), however a value averaged over all directions can be used for some macroscopic calculations.
In most semiconductors, the effective mass of a hole is much larger than that of an electron. This results in lower mobility for holes under the influence of an electric field and this may slow down the speed of the electronic device made of that semiconductor. This is one major reason for adopting electrons as the primary charge carriers, whenever possible in semiconductor devices, rather than holes.
However, in many semiconductor devices, both electrons "and" holes play an essential role. Examples include p–n diodes and bipolar transistors.
Holes in quantum chemistry.
An alternate meaning for the term electron hole is used in computational chemistry. In coupled cluster methods, the ground (or lowest energy) state of a molecule is interpreted as the "vacuum state"—conceptually, in this state there are no electrons. In this scheme, the absence of an electron from a normally filled state is called a "hole" and is treated as a particle, and the presence of an electron in a normally empty state is simply called an "electron". This terminology is almost identical to that used in solid-state physics.

</doc>
<doc id="241034" url="https://en.wikipedia.org/wiki?curid=241034" title="Electron neutrino">
Electron neutrino

The electron neutrino () is a subatomic lepton elementary particle which has no net electric charge. Together with the electron it forms the first generation of leptons, hence its name "electron neutrino". It was first hypothesized by Wolfgang Pauli in 1930, to account for missing momentum and missing energy in beta decay, and was discovered in 1956 by a team led by Clyde Cowan and Frederick Reines (see Cowan–Reines neutrino experiment).
Proposal.
In the early 1900s, theories predicted that the electrons resulting from beta decay should have been emitted at a specific energy. However, in 1914, James Chadwick showed that electrons were instead emitted in a continuous spectrum.
In 1930, Wolfgang Pauli theorized that an undetected particle was carrying away the observed difference between the energy, momentum, and angular momentum of the initial and final particles.
Pauli's letter.
On 4 December 1930, Pauli wrote a letter to the Physical Institute of the Federal Institute of Technology, Zürich, in which he proposed the electron neutrino as a potential solution to solve the problem of the continuous beta decay spectrum. An excerpt of the letter reads:
Dear radioactive ladies and gentlemen,
As the bearer of these lines [...] will explain more exactly, considering the 'false' statistics of N-14 and Li-6 nuclei, as well as the continuous "β"-spectrum, I have hit upon a desperate remedy to save the "exchange theorem" of statistics and the energy theorem. Namely [there is] the possibility that there could exist in the nuclei electrically neutral particles that I wish to call neutrons, which have spin 1/2 and obey the exclusion principle, and additionally differ from light quanta in that they do not travel with the velocity of light: The mass of the neutron must be of the same order of magnitude as the electron mass and, in any case, not larger than 0.01 proton mass. The continuous "β"-spectrum would then become understandable by the assumption that in "β" decay a neutron is emitted together with the electron, in such a way that the sum of the energies of neutron and electron is constant.
But I don't feel secure enough to publish anything about this idea, so I first turn confidently to you, dear radioactives, with a question as to the situation concerning experimental proof of such a neutron, if it has something like about 10 times the penetrating capacity of a "γ" ray.
I admit that my remedy may appear to have a small "a priori" probability because neutrons, if they exist, would probably have long ago been seen. However, only those who wager can win, and the seriousness of the situation of the continuous "β"-spectrum can be made clear by the saying of my honored predecessor in office, Mr. Debye, [...] "One does best not to think about that at all, like the new taxes." [...] So, dear radioactives, put it to test and set it right. [...]
With many greetings to you, also to Mr. Back, your devoted servant,
A translated reprint of the full letter can be found in the September 1978 issue of "Physics Today".
Discovery.
The electron neutrino was discovered by Clyde Cowan and Frederick Reines in 1956.
Name.
Pauli originally named his proposed light particle a "neutron". When James Chadwick discovered a much more massive nuclear particle in 1932 and also named it a neutron, this left the two particles with the same name. Enrico Fermi, who developed the theory of beta decay, coined the term "neutrino" in 1934 to resolve the confusion. It was a pun on "neutrone", the Italian equivalent of "neutron": the "-one" ending can be an augmentative in Italian, so "neutrone" could be read as the "large neutral thing"; "-ino" replaces the augmentative suffix with a diminutive one.
Upon the prediction and discovery of a second neutrino, it became important to distinguish between different types of neutrinos. Pauli's neutrino is now identified as the "electron neutrino", while the second neutrino is identified as the "muon neutrino".
Electron antineutrino.
Like all fermions, the electron neutrino has a corresponding antiparticle, the electron antineutrino (), which differs only in that some of its properties have equal magnitude but opposite sign. The process of beta decay produces both beta particles and electron antineutrinos. Wolfgang Pauli proposed the existence of these particles, in 1930, to ensure that beta decay conserved energy (the electrons in beta decay have a continuum of energies and momentum (the momentum of the electron and recoil nucleus – in beta decay – do not add up to zero).

</doc>
<doc id="19916615" url="https://en.wikipedia.org/wiki?curid=19916615" title="Electron shell">
Electron shell

In chemistry and atomic physics, an electron shell, or a principal energy level, may be thought of as an orbit followed by electrons around an atom's nucleus. The closest shell to the nucleus is called the " shell" (also called "K shell"), followed by the " shell" (or "L shell"), then the " shell" (or "M shell"), and so on farther and farther from the nucleus. The shells correspond with the principal quantum numbers ("n" = 1, 2, 3, 4 ...) or are labeled alphabetically with letters used in the X-ray notation (K, L, M, …).
Each shell can contain only a fixed number of electrons: The first shell can hold up to two electrons, the second shell can hold up to eight (2 + 6) electrons, the third shell can hold up to 18 (2 + 6 + 10) and so on. The general formula is that the "n"th shell can in principle hold up to 2("n"2) electrons. Since electrons are electrically attracted to the nucleus, an atom's electrons will generally occupy outer shells only if the more inner shells have already been completely filled by other electrons. However, this is not a strict requirement: atoms may have two or even three incomplete outer shells. (See Madelung rule for more details.) For an explanation of why electrons exist in these shells see electron configuration.
The electrons in the outermost occupied shell (or shells) determine the chemical properties of the atom; it is called the "valence shell".
Each shell consists of one or more "subshells", and each subshell consists of one or more atomic orbitals.
History.
The shell terminology comes from Arnold Sommerfeld's modification of the Bohr model. Sommerfeld retained Bohr's planetary model, but added mildly elliptical orbits (characterized by additional quantum numbers "" and "m") to explain the fine spectroscopic structure of some elements. The multiple electrons with the same principal quantum number ("n") had close orbits that formed a "shell" of positive thickness instead of the infinitely thin circular orbit of Bohr's model.
The existence of electron shells was first observed experimentally in Charles Barkla's and Henry Moseley's X-ray absorption studies. Barkla labeled them with the letters K, L, M, N, O, P, and Q. The origin of this terminology was alphabetic. A "J" series was also suspected, though later experiments indicated that the K absorption lines are produced by the innermost electrons. These letters were later found to correspond to the "n" values 1, 2, 3, etc. They are used in the spectroscopic Siegbahn notation.
The physical chemist Gilbert Lewis was responsible for much of the early development of the theory of the participation of valence shell electrons in chemical bonding. Linus Pauling later generalized and extended the theory while applying insights from quantum mechanics.
Shells.
The electron shells are labeled K, L, M, N, O, P, and Q; or 1, 2, 3, 4, 5, 6, and 7; going from innermost shell outwards. Electrons in outer shells have higher average energy and travel farther from the nucleus than those in inner shells. This makes them more important in determining how the atom reacts chemically and behaves as a conductor, because the pull of the atom's nucleus upon them is weaker and more easily broken. In this way, a given element's reactivity is highly dependent upon its electronic configuration.
Subshells.
Each shell is composed of one or more subshells, which are themselves composed of atomic orbitals. For example, the first (K) shell has one subshell, called ; the second (L) shell has two subshells, called and ; the third shell has , , and ; the fourth shell has , , and ; the fifth shell has , , , and and can theoretically hold more but the subshell, although partially occupied in actinides, is not filled in any element occurring naturally. The various possible subshells are shown in the following table:
Although it is commonly stated that all the electrons in a shell have the same energy, this is an approximation. However, the electrons in one "subshell" do have exactly the same level of energy, with later subshells having more energy per electron than earlier ones. This effect is great enough that the energy ranges associated with shells can overlap (see "valence shells" and "Aufbau principle").
Number of electrons in each shell.
Each subshell is constrained to hold electrons at most, namely:
Therefore, the K shell, which contains only an subshell, can hold up to 2 electrons; the L shell, which contains an and a , can hold up to 2 + 6 = 8 electrons, and so forth; in general, the "n"th shell can hold up to 2"n"2 electrons.
Although that formula gives the maximum in principle, in fact that maximum is only "achieved" (by known elements) for the first four shells (K, L, M, N). No known element has more than 32 electrons in any one shell. This is because the subshells are filled according to the Aufbau principle. The first elements to have more than 32 electrons in one shell would belong to the g-block of period 8 of the periodic table. These elements would have some electrons in their subshell and thus have more than 32 electrons in the O shell (fifth principal shell).
Valence shells.
The valence shell is the outermost shell of an atom. It is usually (and misleadingly) said that the electrons in this shell make up its valence electrons, that is, the electrons that determine how the atom behaves in chemical reactions. Just as atoms with complete valence shells (noble gases) are the most chemically non-reactive, those with only one electron in their valence shells (alkali metals) or just missing one electron from having a complete shell (halogens) are the most reactive.
However, this is a simplification of the truth. The electrons that determine how an atom reacts chemically are those that travel farthest from the nucleus, that is, those with the highest energy. For the transition elements, the partially filled ("n" − 1) energy level is very close in energy to the "n" level and hence the electrons in transition metals behave as valence electrons although they are not in the so-called valence shell.
List of elements with electrons per shell.
The list below gives the elements arranged by increasing atomic number and shows the number of electrons per shell. At a glance, one can see that subsets of the list show obvious patterns. In particular, the seven elements (in ) before a noble gas (group 18, in ) higher than helium have the number of electrons in the valence shell in arithmetic progression. (However, this pattern may break down in the seventh period due to relativistic effects.)
Sorting the table by chemical group shows additional patterns, especially with respect to the last two outermost shells. (Elements 57 to 71 belong to the lanthanides, while 89 to 103 are the actinides.)
The list below is primarily consistent with the Aufbau principle. However, there are a number of exceptions to the rule; for example palladium (atomic number 46) has no electrons in the fifth shell, unlike other atoms with "lower" atomic number. Some entries in the table are uncertain, when experimental data is unavailable. (For example, some atoms have such short half-life that it is impossible to measure their electron configurations).

</doc>
<doc id="368328" url="https://en.wikipedia.org/wiki?curid=368328" title="Electrostatics">
Electrostatics

Electrostatics is a branch of physics that deals with the phenomena and properties of stationary or slow-moving electric charges with no acceleration.
Since classical physics, it has been known that some materials such as amber attract lightweight particles after rubbing. The Greek word for amber, "", was the source of the word 'electricity'. Electrostatic phenomena arise from the forces that electric charges exert on each other. Such forces are described by Coulomb's law.
Even though electrostatically induced forces seem to be rather weak, the electrostatic force between e.g. an electron and a proton, that together make up a hydrogen atom, is about 36 orders of magnitude stronger than the gravitational force acting between them.
There are many examples of electrostatic phenomena, from those as simple as the attraction of the plastic wrap to your hand after you remove it from a package, and the attraction of paper to a charged scale, to the apparently spontaneous explosion of grain silos, the damage of electronic components during manufacturing, and the operation of photocopiers. Electrostatics involves the buildup of charge on the surface of objects due to contact with other surfaces. Although charge exchange happens whenever any two surfaces contact and separate, the effects of charge exchange are usually only noticed when at least one of the surfaces has a high resistance to electrical flow. This is because the charges that transfer to or from the highly resistive surface are more or less trapped there for a long enough time for their effects to be observed. These charges then remain on the object until they either bleed off to ground or are quickly neutralized by a discharge: e.g., the familiar phenomenon of a static 'shock' is caused by the neutralization of charge built up in the body from contact with insulated surfaces.
Coulomb's law.
We begin with the magnitude of the electrostatic force (in newtons) between two point charges formula_1 and formula_2 (in coulombs). It is convenient to label one of these charges, formula_1, as a test charge, and call formula_2 a source charge. As we develop the theory, more source charges will be added. If formula_5 is the distance (in meters) between two charges, then the force is:
where ε0 is the vacuum permittivity, or permittivity of free space:
The SI units of ε0 are equivalently  A2s4 kg−1m−3 or C2N−1m−2 or F m−1. Coulomb's constant is:
The use of ε0 instead of k0 in expressing Coulomb's Law is related to the fact that the force is inversely proportional to the surface area of a sphere with radius equal to the separation between the two charges.
A single proton has a charge of "e", and the electron has a charge of −"e", where,
These physical constants (ε0, k0, e) are currently defined so that ε0 and k0 are exactly defined, and "e" is a measured quantity.
Electric field.
Electric field lines are useful for visualizing the electric field. Field lines begin on positive charge and terminate on negative charge. Electric field lines are parallel to the direction of the electric field, and the density of these field lines is a measure of the magnitude of the electric field at any given point. The electric field, formula_10, (in units of volts per meter) is a vector field that can be defined everywhere, except at the location of point charges (where it diverges to infinity). It is convenient to place a hypothetical test charge at a point (where no charges are present). By Coulomb's Law, this test charge will experience a force that can be used to define the electric field as follow
Consider a collection of formula_12 particles of charge formula_13, located at points formula_14 (called "source points"), the electric field at formula_15 (called the "field point") is:
where formula_17 is the displacement vector from a "source point" 
formula_14 to the "field point" 
formula_15, and 
formula_20 
is a unit vector that indicates the direction of the field. For a single point charge at the origin, the magnitude of this electric field is formula_21 and points away from that charge is positive. That fact that the force (and hence the field) can be calculated by summing over all the contributions due to individual source particles is an example of the superposition principle. The electric field produced by a distribution of charges is given by the volume charge density formula_22 and can be obtained by converting this sum into a triple integral:
Gauss's law.
Gauss's law states that " the total electric flux through any closed surface in free space of any shape drawn in an electric field is proportional to the total electric charge enclosed by the surface." Mathematically, Gauss's law takes the form of an integral equation:
where formula_25 is a volume element. If the charge is distributed over a surface or along a line, replace formula_26 by formula_27 or formula_28. The Divergence Theorem allows Gauss's Law to be written in differential form:
where formula_30 is the divergence operator.
Poisson and Laplace equations.
The definition of electrostatic potential, combined with the differential form of Gauss's law (above), provides a relationship between the potential Φ and the charge density ρ:
This relationship is a form of Poisson's equation. In the absence of unpaired electric charge, the equation becomes Laplace's equation:
Electrostatic approximation.
The validity of the electrostatic approximation rests on the assumption that the electric field is irrotational:
From Faraday's law, this assumption implies the absence or near-absence of time-varying magnetic fields:
In other words, electrostatics does not require the absence of magnetic fields or electric currents. Rather, if magnetic fields or electric currents "do" exist, they must not change with time, or in the worst-case, they must change with time only "very slowly". In some problems, both electrostatics and magnetostatics may be required for accurate predictions, but the coupling between the two can still be ignored.
Electrostatic potential.
Because the electric field is irrotational, it is possible to express the electric field as the gradient of a scalar function,formula_35, called the electrostatic potential (also known as the voltage). An electric field, formula_36, points from regions of high electric potential to regions of low electric potential, expressed mathematically as
The Gradient Theorem can be used to establish that the electrostatic potential is the amount of work per unit charge required to move a charge from point formula_38 to point formula_39 is the following line integral:
From these equations, we see that the electric potential is constant in any region for which the electric field vanishes (such as occurs inside a conducting object).
Electrostatic energy.
A single test particle's potential energy, formula_41, can be calculated from a line integral of the work, formula_42. We integrate from a point at infinity, and assume a collection of formula_12 particles of charge formula_44, are already situated at the points formula_14. This potential energy (in Joules) is:
where formula_47 is the distance of each charge formula_13 from the test charge formula_1, which situated at the point formula_15, and formula_51 is the electric potential that would be at formula_15 if the test charge were not present. If only two charges are present, the potential energy is formula_53. The total electric potential energy due a collection of "N" charges is calculating by assembling these particles one at a time:
where the following sum from, "j = 1" to "N", excludes "i = j":
This electric potential, formula_56 is what would be measured at formula_14 if the charge formula_13 were missing. This formula obviously excludes the (infinite) energy that would be required to assemble each point charge from a disperse cloud of charge. The sum over charges can be converted into an integral over charge density using the prescription formula_59:
This second expression for electrostatic energy uses the fact that the electric field is the negative gradient of the electric potential, as well as vector calculus identities in a way that resembles integration by parts. These two integrals for electric field energy seem to indicate two mutually exclusive formulas for electrostatic energy density, namely formula_61 and formula_62; they yield equal values for the total electrostatic energy only if both are integrated over all space.
Electrostatic pressure.
On a conductor, a surface charge will experience a force in the presence of an electric field. This force is the average of the discontinuous electric field at the surface charge. This average in terms of the field just outside the surface amounts to:
This pressure tends to draw the conductor into the field, regardless of the sign of the surface charge.
Triboelectric series.
The triboelectric effect is a type of contact electrification in which certain materials become electrically charged when they are brought into contact with a different material and then separated. One of the materials acquires a positive charge, and the other acquires an equal negative charge. The polarity and strength of the charges produced differ according to the materials, surface roughness, temperature, strain, and other properties. Amber, for example, can acquire an electric charge by friction with a material like wool. This property, first recorded by Thales of Miletus, was the first electrical phenomenon investigated by man. Other examples of materials that can acquire a significant charge when rubbed together include glass rubbed with silk, and hard rubber rubbed with fur.
Electrostatic generators.
The presence of surface charge imbalance means that the objects will exhibit attractive or repulsive forces. This surface charge imbalance, which yields static electricity, can be generated by touching two differing surfaces together and then separating them due to the phenomena of contact electrification and the triboelectric effect. Rubbing two nonconductive objects generates a great amount of static electricity. This is not just the result of friction; two nonconductive surfaces can become charged by just being placed one on top of the other. Since most surfaces have a rough texture, it takes longer to achieve charging through contact than through rubbing. Rubbing objects together increases amount of adhesive contact between the two surfaces. Usually insulators, e.g., substances that do not conduct electricity, are good at both generating, and holding, a surface charge. Some examples of these substances are rubber, plastic, glass, and pith. Conductive objects only rarely generate charge imbalance except, for example, when a metal surface is impacted by solid or liquid nonconductors. The charge that is transferred during contact electrification is stored on the surface of each object. Static electric generators, devices which produce very high voltage at very low current and used for classroom physics demonstrations, rely on this effect.
Note that the presence of electric current does not detract from the electrostatic forces nor from the sparking, from the corona discharge, or other phenomena. Both phenomena can exist simultaneously in the same system.
Charge neutralization.
Natural electrostatic phenomena are most familiar as an occasional annoyance in seasons of low humidity, but can be destructive and harmful in some situations (e.g. electronics manufacturing). When working in direct contact with integrated circuit electronics (especially delicate MOSFETs), or in the presence of flammable gas, care must be taken to avoid accumulating and suddenly discharging a static charge (see electrostatic discharge).
Charge induction.
Charge induction occurs when a negatively charged object repels (the negatively charged) electrons from the surface of a second object. This creates a region in the second object that is more positively charged. An attractive force is then exerted between the objects. For example, when a balloon is rubbed, the balloon will stick to the wall as an attractive force is exerted by two oppositely charged surfaces (the surface of the wall gains an electric charge due to charge induction, as the free electrons at the surface of the wall are repelled by the negative balloon, creating a positive wall surface, which is subsequently attracted to the surface of the balloon). You can explore the effect with a simulation of the balloon and static electricity.
'Static' electricity.
Before the year 1832, when Michael Faraday published the results of his experiment on the identity of electricities, physicists thought "static electricity" was somehow different from other electrical charges. Michael Faraday proved that the electricity induced from the magnet, voltaic electricity produced by a battery, and static electricity are all the same.
Static electricity is usually caused when certain materials are rubbed against each other, like wool on plastic or the soles of shoes on carpet. The process causes electrons to be pulled from the surface of one material and relocated on the surface of the other material.
A static shock occurs when the surface of the second material, negatively charged with electrons, touches a positively charged conductor, or vice versa.
Static electricity is commonly used in xerography, air filters, and some automotive paints.
Static electricity is a buildup of electric charges on two objects that have become separated from each other.
Small electrical components can easily be damaged by static electricity. Component manufacturers use a number of antistatic devices to avoid this.
Static electricity and chemical industry.
When different materials are brought together and then separated, an accumulation of electric charge can occur which leaves one material positively charged while the other becomes negatively charged. The mild shock that you receive when touching a grounded object after walking on carpet is an example of excess electrical charge accumulating in your body from frictional charging between your shoes and the carpet. The resulting charge build-up upon your body can generate a strong electrical discharge. Although experimenting with static electricity may be fun, similar sparks create severe hazards in those industries dealing with flammable substances, where a small electrical spark may ignite explosive mixtures with devastating consequences.
A similar charging mechanism can occur within low conductivity fluids flowing through pipelines—a process called flow electrification. Fluids which have low electrical conductivity (below 50 picosiemens per meter), are called accumulators. Fluids having conductivities above 50 pS/m are called non-accumulators. In non-accumulators, charges recombine as fast as they are separated and hence electrostatic charge generation is not significant. In the petrochemical industry, 50 pS/m is the recommended minimum value of electrical conductivity for adequate removal of charge from a fluid.
An important concept for insulating fluids is the static relaxation time. This is similar to the time constant (tau) within an RC circuit. For insulating materials, it is the ratio of the static dielectric constant divided by the electrical conductivity of the material. For hydrocarbon fluids, this is sometimes approximated by dividing the number 18 by the electrical conductivity of the fluid. Thus a fluid that has an electrical conductivity of 1 pS/cm (100 pS/m) will have an estimated relaxation time of about 18 seconds. The excess charge within a fluid will be almost completely dissipated after 4 to 5 times the relaxation time, or 90 seconds for the fluid in the above example.
Charge generation increases at higher fluid velocities and larger pipe diameters, becoming quite significant in pipes or larger. Static charge generation in these systems is best controlled by limiting fluid velocity. The British standard BS PD CLC/TR 50404:2003 (formerly BS-5958-Part 2) Code of Practice for Control of Undesirable Static Electricity prescribes velocity limits. Because of its large impact on dielectric constant, the recommended velocity for hydrocarbon fluids containing water should be limited to 1 m/s.
Bonding and earthing are the usual ways by which charge buildup can be prevented. For fluids with electrical conductivity below 10 pS/m, bonding and earthing are not adequate for charge dissipation, and anti-static additives may be required.
Applicable standards.
1.BS PD CLC/TR 50404:2003 Code of Practice for Control of Undesirable Static Electricity
2.NFPA 77 (2007) Recommended Practice on Static Electricity
3.API RP 2003 (1998) Protection Against Ignitions Arising Out of Static, Lightning, and Stray Currents
Electrostatic induction in commercial applications.
The principle of electrostatic induction has been harnessed to beneficial effect in industry for many years, beginning with the introduction of electrostatic industrial painting systems for the economical and even application of enamel and polyurethane paints to consumer goods, including automobiles, bicycles, and other products.

</doc>
<doc id="10103" url="https://en.wikipedia.org/wiki?curid=10103" title="Electroweak interaction">
Electroweak interaction

In particle physics, the electroweak interaction is the unified description of two of the four known fundamental interactions of nature: electromagnetism and the weak interaction. Although these two forces appear very different at everyday low energies, the theory models them as two different aspects of the same force. Above the unification energy, on the order of 100 GeV, they would merge into a single electroweak force. Thus, if the universe is hot enough (approximately 1015 K, a temperature exceeded until shortly after the Big Bang), then the electromagnetic force and weak force merge into a combined electroweak force. During the electroweak epoch, the electroweak force separated from the strong force. During the quark epoch, the electroweak force split into the electromagnetic and weak force.
Sheldon Glashow, Abdus Salam, and Steven Weinberg were awarded the 1979 Nobel Prize in Physics for their contributions to the unification of the weak and electromagnetic interaction between elementary particles. The existence of the electroweak interactions was experimentally established in two stages, the first being the discovery of neutral currents in neutrino scattering by the Gargamelle collaboration in 1973, and the second in 1983 by the UA1 and the UA2 collaborations that involved the discovery of the W and Z gauge bosons in proton–antiproton collisions at the converted Super Proton Synchrotron. In 1999, Gerardus 't Hooft and Martinus Veltman were awarded the Nobel prize for showing that the electroweak theory is renormalizable.
Formulation.
Mathematically, the unification is accomplished under an "SU"(2) × "U"(1) gauge group. The corresponding gauge bosons are the three W bosons of weak isospin from SU(2) (W1, W2, and W3, and the B boson of weak hypercharge from U(1), respectively, all of which are massless.
In the Standard Model, the and bosons, and the photon, are produced by the spontaneous symmetry breaking of the electroweak symmetry from "SU"(2) × "U"(1)"Y" to "U"(1)em, caused by the Higgs mechanism (see also Higgs boson). "U"(1)"Y" and "U"(1)em are different copies of "U"(1); the generator of "U"(1)em is given by "Q" = "Y"/2 + "I"3, where "Y" is the generator of "U"(1)"Y" (called the weak hypercharge), and "I"3 is one of the "SU"(2) generators (a component of weak isospin).
The spontaneous symmetry breaking causes the W3 and B bosons to coalesce together into two different bosons – the boson, and the photon (γ) as follows:
Where θW is the "weak mixing angle". The axes representing the particles have essentially just been rotated, in the (W3, B) plane, by the angle θW. This also introduces a discrepancy between the mass of the and the mass of the particles (denoted as MZ and MW, respectively);
W1 and W2 bosons, in turn, combine to give massive charged bosons
The distinction between electromagnetism and the weak force arises because there is a (nontrivial) linear combination of "Y" and "I"3 that vanishes for the Higgs boson (it is an eigenstate of both "Y" and "I"3, so the coefficients may be taken as −"I"3 and "Y"): "U"(1)em is defined to be the group generated by this linear combination, and is unbroken because it does not interact with the Higgs.
Lagrangian.
Before electroweak symmetry breaking.
The Lagrangian for the electroweak interactions is divided into four parts before electroweak symmetry breaking
The formula_5 term describes the interaction between the three W particles and the B particle.
where formula_7 (formula_8) and formula_9 are the field strength tensors for the weak isospin and weak hypercharge fields.
formula_10 is the kinetic term for the Standard Model fermions. The interaction of the gauge bosons and the fermions are through the gauge covariant derivative.
where the subscript formula_12 runs over the three generations of fermions, formula_13, formula_14, and formula_15 are the left-handed doublet, right-handed singlet up, and right handed singlet down quark fields, and formula_16 and formula_17 are the left-handed doublet and right-handed singlet electron fields.
The "h" term describes the Higgs field F.
The "y" term gives the Yukawa interaction that generates the fermion masses after the Higgs acquires a vacuum expectation value.
After electroweak symmetry breaking.
The Lagrangian reorganizes itself after the Higgs boson acquires a vacuum expectation value. Due to its complexity, this Lagrangian is best described by breaking it up into several parts as follows.
The kinetic term formula_21 contains all the quadratic terms of the Lagrangian, which include the dynamic terms (the partial derivatives) and the mass terms (conspicuously absent from the Lagrangian before symmetry breaking)
where the sum runs over all the fermions of the theory (quarks and leptons), and the fields formula_23, formula_24, formula_25, and formula_26 are given as
The neutral current formula_28 and charged current formula_29 components of the Lagrangian contain the interactions between the fermions and gauge bosons.
where the electromagnetic current formula_31 and the neutral weak current formula_32 are
and
formula_35 and formula_36 are the fermions' electric charges and weak isospin.
The charged current part of the Lagrangian is given by
formula_38 contains the Higgs three-point and four-point self interaction terms.
formula_40 contains the Higgs interactions with gauge vector bosons.
formula_42 contains the gauge three-point self interactions.
formula_44 contains the gauge four-point self interactions
and formula_46 contains the Yukawa interactions between the fermions and the Higgs field.
Note the formula_48 factors in the weak couplings: these factors project out the left handed components of the spinor fields. This is why electroweak theory (after symmetry breaking) is commonly said to be a chiral theory.

</doc>
<doc id="174945" url="https://en.wikipedia.org/wiki?curid=174945" title="Elementary charge">
Elementary charge

The elementary charge, usually denoted as or sometimes , is the electric charge carried by a single proton, or equivalently, the negation (opposite) of the electric charge carried by a single electron. This elementary charge is a fundamental physical constant. To avoid confusion over its sign, "e" is sometimes called the elementary positive charge. This charge has a measured value of approximately In the cgs system, "e" is .
The magnitude of the elementary charge was first measured in Robert A. Millikan's noted oil drop experiment in 1909.
As a unit.
In some natural unit systems, such as the system of atomic units, "e" functions as the unit of electric charge, that is "e" is equal to 1 e in those unit systems. The use of elementary charge as a unit was promoted by George Johnstone Stoney in 1874 for the first system of natural units, called Stoney units. Later, he proposed the name "electron" for this unit. At the time, the particle we now call the electron was not yet discovered and the difference between the particle "electron" and the unit of charge "electron" was still blurred. Later, the name "electron" was assigned to the particle and the unit of charge "e" lost its name. However, the unit of energy electronvolt reminds us that the elementary charge was once called "electron".
The maximum capacity of each pixel in a charge-coupled device image sensor, known as the "well depth", is typically given in units of electrons, commonly around 105 e– per pixel.
Quantization.
"Charge quantization" is the principle that the charge of any object is an integer multiple of the elementary charge. Thus, an object's charge can be exactly 0 "e", or exactly 1 "e", −1 "e", 2 "e", etc., but not, say,  "e", or −3.8 "e", etc. (There may be exceptions to this statement, depending on how "object" is defined; see below.)
This is the reason for the terminology "elementary charge": it is meant to imply that it is an indivisible unit of charge.
Charges less than an elementary charge.
There are two known sorts of exceptions to the indivisibility of the elementary charge: quarks and quasiparticles.
What is the quantum of charge?
All known elementary particles, including quarks, have charges that are integer multiples of  "e". Therefore, one can say that the "quantum of charge" is  "e". In this case, one says that the "elementary charge" is three times as large as the "quantum of charge".
On the other hand, all "isolatable" particles have charges that are integer multiples of "e". (Quarks cannot be isolated, except in combinations like protons that have total charges that are integer multiples of "e".) Therefore, one can say that the "quantum of charge" is "e", with the proviso that quarks are not to be included. In this case, "elementary charge" would be synonymous with the "quantum of charge".
In fact, both terminologies are used. For this reason, phrases like "the quantum of charge" or "the indivisible unit of charge" can be ambiguous, unless further specification is given. On the other hand, the term "elementary charge" is unambiguous: it universally refers to a quantity of charge equal to that of a proton.
Experimental measurements of the elementary charge.
In terms of the Avogadro constant and Faraday constant.
If the Avogadro constant "N"A and the Faraday constant "F" are independently known, the value of the elementary charge can be deduced, using the formula
In practice, this method is "not" how the "most accurate" values are measured today: Nevertheless, it is a legitimate and still quite accurate method, and experimental methodologies are described below:
The value of the Avogadro constant "N"A was first approximated by Johann Josef Loschmidt who, in 1865, estimated the average diameter of the molecules in air by a method that is equivalent to calculating the number of particles in a given volume of gas. Today the value of "N"A can be measured at very high accuracy by taking an extremely pure crystal (in practice, often silicon), measuring how far apart the atoms are spaced using X-ray diffraction or another method, and accurately measuring the density of the crystal. From this information, one can deduce the mass ("m") of a single atom; and since the molar mass ("M") is known, the number of atoms in a mole can be calculated: "N"A = "M"/"m".
The value of "F" can be measured directly using Faraday's laws of electrolysis. Faraday's laws of electrolysis are quantitative relationships based on the electrochemical researches published by Michael Faraday in 1834. In an electrolysis experiment, there is a one-to-one correspondence between the electrons passing through the anode-to-cathode wire and the ions that plate onto or off of the anode or cathode. Measuring the mass change of the anode or cathode, and the total charge passing through the wire (which can be measured as the time-integral of electric current), and also taking into account the molar mass of the ions, one can deduce "F".
The limit to the precision of the method is the measurement of "F": the best experimental value has a relative uncertainty of 1.6 ppm, about thirty times higher than other modern methods of measuring or calculating the elementary charge.
Oil-drop experiment.
A famous method for measuring "e" is Millikan's oil-drop experiment. A small drop of oil in an electric field would move at a rate that balanced the forces of gravity, viscosity (of traveling through the air), and electric force. The forces due to gravity and viscosity could be calculated based on the size and velocity of the oil drop, so electric force could be deduced. Since electric force, in turn, is the product of the electric charge and the known electric field, the electric charge of the oil drop could be accurately computed. By measuring the charges of many different oil drops, it can be seen that the charges are all integer multiples of a single small charge, namely "e".
The necessity of measuring the size of the oil droplets can be eliminated by using tiny plastic spheres of a uniform size. The force due to viscosity can be eliminated by adjusting the strength of the electric field so that the sphere hovers motionless.
Shot noise.
Any electric current will be associated with noise from a variety of sources, one of which is shot noise. Shot noise exists because a current is not a smooth continual flow; instead, a current is made up of discrete electrons that pass by one at a time. By carefully analyzing the noise of a current, the charge of an electron can be calculated. This method, first proposed by Walter H. Schottky, can give only a value of "e" accurate to a few percent. However, it was used in the first direct observation of Laughlin quasiparticles, implicated in the fractional quantum Hall effect.
From the Josephson and von Klitzing constants.
Another accurate method for measuring the elementary charge is by inferring it from measurements of two effects in quantum mechanics: The Josephson effect, voltage oscillations that arise in certain superconducting structures; and the quantum Hall effect, a quantum effect of electrons at low temperatures, strong magnetic fields, and confinement into two dimensions. The Josephson constant is
(where "h" is the Planck constant). It can be measured directly using the Josephson effect.
The von Klitzing constant is
It can be measured directly using the quantum Hall effect.
From these two constants, the elementary charge can be deduced:
CODATA method.
In the most recent CODATA adjustments, the elementary charge is not an independently defined quantity. Instead, a value is derived from the relation 
where "h" is the Planck constant, "α" is the fine structure constant, "μ"0 is the magnetic constant, "ε"0 is the electric constant and "c" is the speed of light. The uncertainty in the value of "e" is currently determined entirely by the uncertainty in the Planck constant.
The most precise values of the Planck constant come from watt balance experiments, which are currently used to measure the product "K""R"K. The most precise values of the fine structure constant come from comparisons of the measured and calculated value of the gyromagnetic ratio of the electron.

</doc>
<doc id="5897031" url="https://en.wikipedia.org/wiki?curid=5897031" title="Elementary matrix">
Elementary matrix

In mathematics, an elementary matrix is a matrix which differs from the identity matrix by one single elementary row operation. The elementary matrices generate the general linear group of invertible matrices. Left multiplication (pre-multiplication) by an elementary matrix represents elementary row operations, while right multiplication (post-multiplication) represents elementary column operations. The acronym "ERO" is commonly used for "elementary row operations".
Elementary row operations are used in Gaussian elimination to reduce a matrix to row echelon form. They are also used in Gauss-Jordan elimination to further reduce the matrix to reduced row echelon form.
Operations.
There are three types of elementary matrices, which correspond to three types of row operations (respectively, column operations):
If "E" is an elementary matrix, as described below, to apply the elementary row operation to a matrix "A", one multiplies the elementary matrix on the left, "E⋅A". The elementary matrix for any row operation is obtained by executing the operation on the identity matrix.
Row-switching transformations.
The first type of row operation on a matrix "A" switches all matrix elements on row "i" with their counterparts on row "j". The corresponding elementary matrix is obtained by swapping row "i" and row "j" of the identity matrix. 
Row-multiplying transformations.
The next type of row operation on a matrix "A" multiplies all elements on row "i" by "m" where "m" is a non-zero scalar (usually a real number). The corresponding elementary matrix is a diagonal matrix, with diagonal entries 1 everywhere except in the "i"th position, where it is "m".
Row-addition transformations.
The final type of row operation on a matrix "A" adds row "j" multiplied by a scalar "m" to row "i". The corresponding elementary matrix is the identity matrix but with an "m" in the ("i,j") position.

</doc>
<doc id="11274" url="https://en.wikipedia.org/wiki?curid=11274" title="Elementary particle">
Elementary particle

In particle physics, an elementary particle or fundamental particle is a whose substructure is unknown, thus it is unknown whether it is composed of other particles. Known elementary particles include the fundamental fermions (quarks, leptons, antiquarks, and antileptons), which generally are "matter particles" and "antimatter particles", as well as the fundamental bosons (gauge bosons and Higgs boson), which generally are "force particles" that mediate interactions among fermions. A particle containing two or more elementary particles is a "composite particle".
Everyday matter is composed of atoms, once presumed to be matter's elementary particles—"atom" meaning "indivisible" in Greek—although the atom's existence remained controversial until about 1910, as some leading physicists regarded molecules as mathematical illusions, and matter as ultimately composed of energy. Soon, subatomic constituents of the atom were identified. As the 1930s opened, the electron and the proton had been observed, along with the photon, the particle of electromagnetic radiation. At that time, the recent advent of quantum mechanics was radically altering the conception of particles, as a single particle could seemingly span a field as would a wave, a paradox still eluding satisfactory explanation.
Via quantum theory, protons and neutrons were found to contain quarks—up quarks and down quarks—now considered elementary particles. And within a molecule, the electron's three degrees of freedom (charge, spin, orbital) can separate via wavefunction into three quasiparticles (holon, spinon, orbiton). Yet a free electron—which, not orbiting an atomic nucleus, lacks orbital motion—appears unsplittable and remains regarded as an elementary particle.
Around 1980, an elementary particle's status as indeed elementary—an "ultimate constituent" of substance—was mostly discarded for a more practical outlook, embodied in particle physics' Standard Model, science's most experimentally successful theory. Many elaborations upon and theories beyond the Standard Model, including the extremely popular supersymmetry, double the number of elementary particles by hypothesizing that each known particle associates with a "shadow" partner far more massive, although all such superpartners remain undiscovered. Meanwhile, an elementary boson mediating gravitation—the graviton—remains hypothetical.
Overview.
All elementary particles are—depending on their "spin"—either bosons or fermions. These are differentiated via the spin–statistics theorem of quantum statistics. Particles of "half-integer" spin exhibit Fermi–Dirac statistics and are fermions. Particles of "integer" spin, in other words full-integer, exhibit Bose–Einstein statistics and are bosons.
A particle's mass is quantified in units of energy versus the electron's (electronvolts). Through conversion of energy into mass, any particle can be produced through collision of other particles at high energy, although the output particle might not contain the input particles, for instance matter creation from colliding photons. Likewise, the composite fermions protons were collided at nearly light speed to produce the relatively more massive Higgs boson. The most massive elementary particle, the top quark, rapidly decays, but apparently does not contain, lighter particles.
When probed at energies available in experiments, particles exhibit spherical sizes. In operating particle physics' Standard Model, elementary particles are usually represented for predictive utility as point particles, which, as zero-dimensional, lack spatial extension. Though extremely successful, the Standard Model is limited to the microcosm by its omission of gravitation, and has some parameters arbitrarily added but unexplained. Seeking to resolve those shortcomings, string theory posits that elementary particles are ultimately composed of one-dimensional energy strings whose absolute minimum size is the Planck length.
Common elementary particles.
According to the current models of big bang nucleosynthesis, the primordial composition of visible matter of the universe should be about 75% hydrogen and 25% helium-4 (in mass). Neutrons are made up of one up and two down quark, while protons are made of two up and one down quark. Since the other common elementary particles (such as electrons, neutrinos, or weak bosons) are so light or so rare when compared to atomic nuclei, we can neglect their mass contribution to the observable universe's total mass. Therefore, one can conclude that most of the visible mass of the universe consists of protons and neutrons, which, like all baryons, in turn consist of up quarks and down quarks.
Some estimates imply that there are roughly baryons (almost entirely protons and neutrons) in the observable universe.
The number of protons in the observable universe is called the Eddington number.
In terms of number of particles, some estimates imply that nearly all the matter, excluding dark matter, occurs in neutrinos, and that roughly elementary particles of matter exist in the visible universe, mostly neutrinos. Other estimates imply that roughly elementary particles exist in the visible universe (not including dark matter), mostly photons, gravitons, and other massless force carriers.
Standard Model.
The Standard Model of particle physics contains 12 flavors of elementary fermions, plus their corresponding antiparticles, as well as elementary bosons that mediate the forces and the Higgs boson, which was reported on July 4, 2012, as having been likely detected by the two main experiments at the LHC (ATLAS and CMS). However, the Standard Model is widely considered to be a provisional theory rather than a truly fundamental one, since it is not known if it is compatible with Einstein's general relativity. There may be hypothetical elementary particles not described by the Standard Model, such as the graviton, the particle that would carry the gravitational force, and sparticles, supersymmetric partners of the ordinary particles.
Fundamental fermions.
The 12 fundamental fermionic flavours are divided into three generations of four particles each. Six of the particles are quarks. The remaining six are leptons, three of which are neutrinos, and the remaining three of which have an electric charge of −1: the electron and its two cousins, the muon and the tau.
Antiparticles.
There are also 12 fundamental fermionic antiparticles that correspond to these 12 particles. For example, the antielectron (positron) "" is the electron's antiparticle and has an electric charge of +1.
Quarks.
Isolated quarks and antiquarks have never been detected, a fact explained by confinement. Every quark carries one of three color charges of the strong interaction; antiquarks similarly carry anticolor. Color-charged particles interact via gluon exchange in the same way that charged particles interact via photon exchange. However, gluons are themselves color-charged, resulting in an amplification of the strong force as color-charged particles are separated. Unlike the electromagnetic force, which diminishes as charged particles separate, color-charged particles feel increasing force.
However, color-charged particles may combine to form color neutral composite particles called hadrons. A quark may pair up with an antiquark: the quark has a color and the antiquark has the corresponding anticolor. The color and anticolor cancel out, forming a color neutral meson. Alternatively, three quarks can exist together, one quark being "red", another "blue", another "green". These three colored quarks together form a color-neutral baryon. Symmetrically, three antiquarks with the colors "antired", "antiblue" and "antigreen" can form a color-neutral antibaryon.
Quarks also carry fractional electric charges, but, since they are confined within hadrons whose charges are all integral, fractional charges have never been isolated. Note that quarks have electric charges of either +2/3 or −1/3, whereas antiquarks have corresponding electric charges of either −2/3 or +1/3.
Evidence for the existence of quarks comes from deep inelastic scattering: firing electrons at nuclei to determine the distribution of charge within nucleons (which are baryons). If the charge is uniform, the electric field around the proton should be uniform and the electron should scatter elastically. Low-energy electrons do scatter in this way, but, above a particular energy, the protons deflect some electrons through large angles. The recoiling electron has much less energy and a jet of particles is emitted. This inelastic scattering suggests that the charge in the proton is not uniform but split among smaller charged particles: quarks.
Fundamental bosons.
In the Standard Model, vector (spin-1) bosons (gluons, photons, and the W and Z bosons) mediate forces, whereas the Higgs boson (spin-0) is responsible for the intrinsic mass of particles. Bosons differ from fermions in the fact that multiple bosons can occupy the same quantum state (Pauli exclusion principle). Also, bosons can be either elementary, like photons, or a combination, like mesons. The spin of bosons are integers instead of half integers.
Gluons.
Gluons mediate the strong interaction, which join quarks and thereby form hadrons, which are either baryons (three quarks) or mesons (one quark and one antiquark). Protons and neutrons are baryons, joined by gluons to form the atomic nucleus. Like quarks, gluons exhibit colour and anticolour—unrelated to the concept of visual color—sometimes in combinations, altogether eight variations of gluons.
Electroweak bosons.
There are three weak gauge bosons: "W+", "W−", and "Z0"; these mediate the weak interaction. The W bosons are known for their mediation in nuclear decay. The "W−" converts a neutron into a proton then decay into an electron and electron antineutrino pair. The "Z0" does not convert charge but rather changes momentum and is the only mechanism for elastically scattering neutrinos. The weak gauge bosons were discovered due to momentum change in electrons from neutrino-Z exchange. The massless photon mediates the electromagnetic interaction. These four gauge bosons form the electroweak interaction among elementary particles.
Higgs boson.
Although the weak and electromagnetic forces appear quite different to us at everyday energies, the two forces are theorized to unify as a single electroweak force at high energies. This prediction was clearly confirmed by measurements of cross-sections for high-energy electron-proton scattering at the HERA collider at DESY. The differences at low energies is a consequence of the high masses of the "W" and "Z" bosons, which in turn are a consequence of the Higgs mechanism. Through the process of spontaneous symmetry breaking, the Higgs selects a special direction in electroweak space that causes three electroweak particles to become very heavy (the weak bosons) and one to remain massless (the photon). On 4 July 2012, after many years of experimentally searching for evidence of its existence, the Higgs boson was announced to have been observed at CERN's Large Hadron Collider. Peter Higgs who first posited the existence of the Higgs boson was present at the announcement. The Higgs boson is believed to have a mass of approximately 125 GeV. The statistical significance of this discovery was reported as 5-sigma, which implies a certainty of roughly 99.99994%. In particle physics, this is the level of significance required to officially label experimental observations as a discovery. Research into the properties of the newly discovered particle continues.
Graviton.
The graviton is hypothesized to mediate gravitation, but remains undiscovered and yet is sometimes included in tables of elementary particles. Its spin would be two—thus a boson—and it would lack charge or mass. Besides mediating an extremely feeble force, the graviton would have its own antiparticle and rapidly annihilate, rendering its detection extremely difficult even if it exists.
Beyond the Standard Model.
Although experimental evidence overwhelmingly confirms the predictions derived from the Standard Model, some of its parameters were added arbitrarily, not determined by a particular explanation, which remain mysteries, for instance the hierarchy problem. Theories beyond the Standard Model attempt to resolve these shortcomings.
Grand unification.
One extension of the Standard Model attempts to combine the electroweak interaction with the strong interaction into a single 'grand unified theory' (GUT). Such a force would be spontaneously broken into the three forces by a Higgs-like mechanism. The most dramatic prediction of grand unification is the existence of X and Y bosons, which cause proton decay. However, the non-observation of proton decay at the Super-Kamiokande neutrino observatory rules out the simplest GUTs, including SU(5) and SO(10).
Supersymmetry.
Supersymmetry extends the Standard Model by adding another class of symmetries to the Lagrangian. These symmetries exchange fermionic particles with bosonic ones. Such a symmetry predicts the existence of supersymmetric particles, abbreviated as "sparticles", which include the sleptons, squarks, neutralinos, and charginos. Each particle in the Standard Model would have a superpartner whose spin differs by 1/2 from the ordinary particle. Due to the breaking of supersymmetry, the sparticles are much heavier than their ordinary counterparts; they are so heavy that existing particle colliders would not be powerful enough to produce them. However, some physicists believe that sparticles will be detected by the Large Hadron Collider at CERN.
String theory.
String theory is a model of physics where all "particles" that make up matter are composed of strings (measuring at the Planck length) that exist in an 11-dimensional (according to M-theory, the leading version) universe. These strings vibrate at different frequencies that determine mass, electric charge, color charge, and spin. A string can be open (a line) or closed in a loop (a one-dimensional sphere, like a circle). As a string moves through space it sweeps out something called a "world sheet". String theory predicts 1- to 10-branes (a 1-brane being a string and a 10-brane being a 10-dimensional object) that prevent tears in the "fabric" of space using the uncertainty principle (E.g., the electron orbiting a hydrogen atom has the probability, albeit small, that it could be anywhere else in the universe at any given moment).
String theory proposes that our universe is merely a 4-brane, inside which exist the 3 space dimensions and the 1 time dimension that we observe. The remaining 6 theoretical dimensions either are very tiny and curled up (and too small to be macroscopically accessible) or simply do not/cannot exist in our universe (because they exist in a grander scheme called the "multiverse" outside our known universe).
Some predictions of the string theory include existence of extremely massive counterparts of ordinary particles due to vibrational excitations of the fundamental string and existence of a massless spin-2 particle behaving like the graviton.
Technicolor.
Technicolor theories try to modify the Standard Model in a minimal way by introducing a new QCD-like interaction. This means one adds a new theory of so-called Techniquarks, interacting via so called Technigluons. The main idea is that the Higgs-Boson is not an elementary particle but a bound state of these objects.
Preon theory.
According to preon theory there are one or more orders of particles more fundamental than those (or most of those) found in the Standard Model. The most fundamental of these are normally called preons, which is derived from "pre-quarks". In essence, preon theory tries to do for the Standard Model what the Standard Model did for the particle zoo that came before it. Most models assume that almost everything in the Standard Model can be explained in terms of three to half a dozen more fundamental particles and the rules that govern their interactions. Interest in preons has waned since the simplest models were experimentally ruled out in the 1980s.
Acceleron theory.
Accelerons are the hypothetical subatomic particles that integrally link the newfound mass of the neutrino and to the dark energy conjectured to be accelerating the expansion of the universe.
In theory, neutrinos are influenced by a new force resulting from their interactions with accelerons. Dark energy results as the universe tries to pull neutrinos apart.
External links.
The most important address about the current experimental and theoretical knowledge about elementary particle physics is the Particle Data Group, where different international institutions collect all experimental data and give short reviews over the contemporary theoretical understanding.
other pages are:

</doc>
<doc id="9277" url="https://en.wikipedia.org/wiki?curid=9277" title="Ellipse">
Ellipse

In mathematics, an ellipse is a curve on a plane that surrounds two focal points such that the sum of the distances to the two focal points is constant for every point on the curve. As such, it is a generalization of a circle, which is a special type of an ellipse that has both focal points at the same location. The shape of an ellipse (how 'elongated' it is) is represented by its eccentricity, which for an ellipse can be any number from 0 (the limiting case of a circle) to arbitrarily close to but less than 1.
Ellipses are the closed type of conic section: a plane curve that results from the intersection of a cone by a plane. (See figure to the right.) Ellipses have many similarities with the other two forms of conic sections: the parabolas and the hyperbolas, both of which are open and unbounded. The cross section of a cylinder is an ellipse, unless the section is parallel to the axis of the cylinder.
Analytically, an ellipse can also be defined as the set of points such that the ratio of the distance of each point on the curve from a given point (called a focus or focal point) to the distance from that same point on the curve to a given line (called the directrix) is a constant, called the eccentricity of the ellipse.
Ellipses are common in physics, astronomy and engineering. For example, the orbit of each planet in the solar system is an ellipse with the barycenter of the planet-Sun pair at one of the focal points. The same is true for moons orbiting planets and all other systems having two astronomical bodies. The shape of planets and stars are often well described by ellipsoids. Ellipses also arise as images of a circle under parallel projection and the bounded cases of perspective projection, which are simply intersections of the projective cone with the plane of projection. It is also the simplest Lissajous figure, formed when the horizontal and vertical motions are sinusoids with the same frequency. A similar effect leads to elliptical polarization of light in optics.
The name, ἔλλειψις (élleipsis, "omission"), was given by Apollonius of Perga in his "Conics", emphasizing the connection of the curve with "application of areas".
Elements of an ellipse.
Ellipses have two perpendicular axes about which the ellipse is symmetric. These axes intersect at the center of the ellipse due to this symmetry. The larger of these two axes, which corresponds to the largest distance between antipodal points on the ellipse, is called the major axis. (On the figure to the right it is represented by the line segment between the point labeled −a and the point labeled a.) The smaller of these two axes, and the smallest distance across the ellipse, is called the minor axis.
The semi-major axis (denoted by "a" in the figure) and the semi-minor axis (denoted by "b" in the figure) are one half of the major and minor axes, respectively. These are sometimes called (especially in technical fields) the major and minor semi-axes, the major and minor semiaxes, or major radius and minor radius.
The four points where these axes cross the ellipse are the vertices and are marked as a, −a, b, and −b. In addition to being at the largest and smallest distance from the center, these points are where the curvature of the ellipse is maximum and minimum.
The two foci (the term focal points is also used) of an ellipse are two special points "F1" and "F2" on the ellipse's major axis that are equidistant from the center point. The sum of the distances from any point P on the ellipse to those two foci is constant and equal to the major axis ("PF1" + "PF2" = 2"a"). (On the figure to the right this corresponds to the sum of the two green lines equaling the length of the major axis that goes from −a to a.)
The distance to the focal point from the center of the ellipse is sometimes called the linear eccentricity, "f", of the ellipse. Here it is denoted by "f", but it is often denoted by "c". Due to the Pythagorean theorem and the definition of the ellipse explained in the previous paragraph: "f"2 = "a"2 −"b"2.
A second equivalent method of constructing an ellipse using a directrix is shown on the plot as the three blue lines. (See the Directrix section of this article for more information about this method). The dashed blue line is the directrix of the ellipse shown.
The eccentricity of an ellipse, usually denoted by "ε" or "e", is the ratio of the distance between the two foci, to the length of the major axis or "e" = 2"f"/2"a" = "f"/"a". For an ellipse the eccentricity is between 0 and 1 (0 < "e" < 1). When the eccentricity is 0 the foci coincide with the center point and the figure is a circle. As the eccentricity tends toward 1, the ellipse gets a more elongated shape. It tends towards a line segment (see below) if the two foci remain a finite distance apart and a parabola if one focus is kept fixed as the other is allowed to move arbitrarily far away. The eccentricity is also equal to the ratio of the distance (such as the (blue) line "PF2") from any particular point on an ellipse to one of the foci to the perpendicular distance to the directrix from the same point (line "PD"), "e" = "PF2"/"PD".
Drawing ellipses.
Pins-and-string method.
The characterization of an ellipse as the locus of points so that sum of the distances to the foci is constant leads to a method of drawing one using two drawing pins, a length of string, and a pencil. In this method, pins are pushed into the paper at two points, which become the ellipse's foci. A string tied at each end to the two pins and the tip of a pen pulls the loop taut to form a triangle. The tip of the pen then traces an ellipse if it is moved while keeping the string taut. Using two pegs and a rope, gardeners use this procedure to outline an elliptical flower bed—thus it is called the "gardener's ellipse".
Trammel method.
An ellipse can also be drawn using a ruler, a set square, and a pencil:
The trammel of Archimedes, or ellipsograph, is a mechanical device that implements this principle. The ruler is replaced by a rod with a pencil holder (point "C") at one end, and two adjustable side pins (points "A" and "B") that slide into two perpendicular slots cut into a metal plate. The mechanism can be used with a router to cut ellipses from board material. The mechanism is also used in a toy called the "nothing grinder".
Parallelogram method.
In the parallelogram method, an ellipse is constructed point by point using equally spaced points on two horizontal lines and equally spaced points on two vertical lines. It is based on Steiner's theorem on the generation of conic sections. Similar methods exist for the parabola and hyperbola.
Mathematical definitions and properties.
In Euclidean geometry.
Definition.
In Euclidean geometry, the ellipse is usually defined as the bounded case of a conic section, or as the set of points such that the sum of the distances to two fixed points (the foci) is constant. The ellipse can also be defined as the set of points such that the distance from any point in that set to a given point in the plane (a focus) is a constant positive fraction less than 1 (the eccentricity) of the perpendicular distance of the point in the set to a given line (called the directrix). Yet another equivalent definition of the ellipse is that it is the set of points that are equidistant from one point in the plane (a focus) and a particular circle, the directrix circle (whose center is the other focus).
The equivalence of these definitions can be proved using the Dandelin spheres.
Equations.
The equation of an ellipse whose major and minor axes coincide with the Cartesian axes is
formula_1. This can be explained as follows:
If we define formula_2 as the angle at the origin from the positive horizontal axis to a ray to a point on the ellipse, then
and
Then plotting "x" and "y" values for all angles of θ between 0 and 2π results in an ellipse (e.g. at θ = 0, "x" = a, "y" = 0 and at θ = π/2, "y" = b, "x" = 0). This can be seen as follows.
Squaring both equations gives:
And
Dividing these two equations by "a"2 and "b"2 respectively gives:
Reflexive property.
When a ray of light originating from one focus reflects off the inner surface of an ellipse, it always passes through the other focus.
In projective geometry.
In a projective geometry defined over a field, a conic section can be defined as the set of all points of intersection between corresponding lines of two pencils of lines in a plane that are related by a projective, but not perspective, map (see Steiner's theorem). By projective duality, a conic section can also be defined as the envelope of all lines that connect corresponding points of two lines related by a projective, but not perspective, map.
In a pappian projective plane (one defined over a field), all conic sections are equivalent to each other, and the different types of conic sections are determined by how they intersect the line at infinity, denoted by Ω. An ellipse is a conic section that does not intersect this line. A parabola is a conic section that is tangent to Ω, and a hyperbola is one that crosses Ω twice. Since an ellipse does not intersect the line at infinity, it properly belongs to the affine plane determined by removing the line at infinity and all of its points from the projective plane.
Affine space.
An ellipse is also the result of projecting a circle, sphere, or ellipse in a three dimensional affine space onto a plane (flat), by parallel lines. This is a special case of conical (perspective) projection of any of those geometric objects in the affine space from a point "O" onto a plane "P", when the point "O" lies in the plane at infinity of the affine space. In the setting of pappian projective planes, the image of an ellipse by any affine map (a projective map that leaves the line at infinity invariant) is an ellipse, and, more generally, the image of an ellipse by any projective map "M" such that the line "M"−1(Ω) does not touch or cross the ellipse is an ellipse.
In analytic geometry.
General ellipse.
In analytic geometry, the ellipse is defined as the set of points formula_9 of the Cartesian plane that, in non-degenerate cases, satisfy the implicit equation
provided formula_11
To distinguish the degenerate cases from the non-degenerate case, let "∆" be the determinant
that is,
Then the ellipse is a non-degenerate real ellipse if and only if "C∆" < 0. If "C∆" > 0, we have an imaginary ellipse, and if "∆" = 0, we have a point ellipse.
The general equation's coefficients can be obtained from known semi-major axis formula_14, semi-minor axis formula_15, center coordinates formula_16 and rotation angle formula_17 using the following formulae:
These expressions can be derived from the canonical equation (see next section) by substituting the coordinates with expressions for rotation and translation of the coordinate system:
Canonical form.
Let formula_22. Through change of coordinates (a rotation of axes and a translation of axes) the general ellipse can be described by the canonical implicit equation
Here formula_24 are the point coordinates in the canonical system, whose origin is the center formula_25 of the ellipse, whose formula_26-axis is the unit vector formula_27 coinciding with the major axis, and whose formula_28-axis is the perpendicular vector formula_29 coinciding with the minor axis. That is, formula_30 and formula_31.
In this system, the center is the origin formula_32 and the foci are formula_33 and formula_34.
Any ellipse can be obtained by rotation and translation of a canonical ellipse with the proper semi-diameters. The expression of an ellipse centered at formula_25 is
Moreover, any canonical ellipse can be obtained by scaling the unit circle of formula_37, defined by the equation
by factors "a" and "b" along the two axes.
For an ellipse in canonical form, we have
The distances from a point formula_9 on the ellipse to the left and right foci are formula_41 and formula_42, respectively.
Line segment as a type of degenerate ellipse.
A line segment is a degenerate ellipse with semi-minor axis = 0 and eccentricity = 1, and with the focal points at the ends. Although the eccentricity is 1 this is not a parabola. A radial elliptic trajectory is a non-trivial special case of an elliptic orbit, where the ellipse is a line segment.
In trigonometry.
General parametric form.
An ellipse in general position can be expressed parametrically as the path of a point formula_43, where
as the parameter "t" varies from 0 to 2"π". Here formula_25 is the center of the ellipse, and formula_47 is the angle between the formula_48-axis and the major axis of the ellipse.
Parametric form in canonical position.
For an ellipse in canonical position (center at origin, major axis along the "X"-axis), the equation simplifies to 
Note that the parameter "t" (called the eccentric anomaly in astronomy) is "not" the angle of formula_43 with the "X"-axis (see diagram at right).
For a given point on an ellipse, formulae connecting the tangential angle formula_52, the polar angle from the ellipse center formula_2, and the parametric angle "t" are:
Polar form relative to center.
In polar coordinates, with the origin at the center of the ellipse and with the angular coordinate formula_2 measured from the major axis, the ellipse's equation is
Polar form relative to focus.
If instead we use polar coordinates with the origin at one focus, with the angular coordinate formula_58 still measured from the major axis, the ellipse's equation is
where the sign in the denominator is negative if the reference direction formula_58 points towards the center (as illustrated on the right), and positive if that direction points away from the center.
In the slightly more general case of an ellipse with one focus at the origin and the other focus at angular coordinate formula_52, the polar form is
The angle formula_2 in these formulas is called the true anomaly of the point. The numerator formula_64 of these formulas is the semi-latus rectum of the ellipse, usually denoted formula_65. It is the distance from a focus of the ellipse to the ellipse itself, measured along a line perpendicular to the major axis.
General polar form.
The following equation on the polar coordinates ("r", "θ") describes a general ellipse with semidiameters "a" and "b", centered at a point ("r"0, "θ"0), with the "a" axis rotated by "φ" relative to the polar axis:
where "r" is the radius or central distance, and
Angular eccentricity.
The angular eccentricity formula_70 is the angle whose sine is the eccentricity "e"; that is,
Degrees of freedom.
An ellipse in the plane has five degrees of freedom (the same as a general conic section), defining its vertical and horizontal position, orientation, shape, and scale. In comparison, circles have only three degrees of freedom (horizontal position, vertical position and scale), while parabolae have four. Said another way, the set of all ellipses in the plane, with any natural metric (such as the Hausdorff distance) is a five-dimensional manifold. 
The five degrees of freedom can be identified with, for example, the coefficients "A","B","C","D","E" of the implicit equation, or with the coefficients "X"c, "Y"c, "φ", "a", "b" of the general parametric form. Thus an ellipse is uniquely determined by any five points lying on it.
Applications.
Ellipses in physics.
Elliptical reflectors and acoustics.
If the water's surface is disturbed at one focus of an elliptical water tank, the circular waves of that disturbance, after reflecting off the walls, converge simultaneously to a single point: the "second focus". This is a consequence of the total travel length being the same along any wall-bouncing path between the two foci.
Similarly, if a light source is placed at one focus of an elliptic mirror, all light rays on the plane of the ellipse are reflected to the second focus. Since no other smooth curve has such a property, it can be used as an alternative definition of an ellipse. (In the special case of a circle with a source at its center all light would be reflected back to the center.) If the ellipse is rotated along its major axis to produce an ellipsoidal mirror (specifically, a prolate spheroid), this property holds for all rays out of the source. Alternatively, a cylindrical mirror with elliptical cross-section can be used to focus light from a linear fluorescent lamp along a line of the paper; such mirrors are used in some document scanners.
Sound waves are reflected in a similar way, so in a large elliptical room a person standing at one focus can hear a person standing at the other focus remarkably well. The effect is even more evident under a vaulted roof shaped as a section of a prolate spheroid. Such a room is called a "whisper chamber". The same effect can be demonstrated with two reflectors shaped like the end caps of such a spheroid, placed facing each other at the proper distance. Examples are the National Statuary Hall at the United States Capitol (where John Quincy Adams is said to have used this property for eavesdropping on political matters); the Mormon Tabernacle at Temple Square in Salt Lake City, Utah; at an exhibit on sound at the Museum of Science and Industry in Chicago; in front of the University of Illinois at Urbana-Champaign Foellinger Auditorium; and also at a side chamber of the Palace of Charles V, in the Alhambra.
Planetary orbits.
In the 17th century, Johannes Kepler discovered that the orbits along which the planets travel around the Sun are ellipses with the Sun [approximately] at one focus, in his first law of planetary motion. Later, Isaac Newton explained this as a corollary of his law of universal gravitation.
More generally, in the gravitational two-body problem, if the two bodies are bound to each other (i.e., the total energy is negative), their orbits are similar ellipses with the common barycenter being one of the foci of each ellipse. The other focus of either ellipse has no known physical significance. Interestingly, the orbit of either body in the reference frame of the other is also an ellipse, with the other body at the same focus.
Keplerian elliptical orbits are the result of any radially directed attraction force whose strength is inversely proportional to the square of the distance. Thus, in principle, the motion of two oppositely charged particles in empty space would also be an ellipse. (However, this conclusion ignores losses due to electromagnetic radiation and quantum effects, which become significant when the particles are moving at high speed.)
For elliptical orbits, useful relations involving the eccentricity formula_72 are:
where
Also, in terms of formula_74 and formula_75, the semi-major axis formula_14 is their arithmetic mean, the semi-minor axis formula_15 is their geometric mean, and the semi-latus rectum formula_65 is their harmonic mean. In other words,
Harmonic oscillators.
The general solution for a harmonic oscillator in two or more dimensions is also an ellipse. Such is the case, for instance, of a long pendulum that is free to move in two dimensions; of a mass attached to a fixed point by a perfectly elastic spring; or of any object that moves under influence of an attractive force that is directly proportional to its distance from a fixed attractor. Unlike Keplerian orbits, however, these "harmonic orbits" have the center of attraction at the geometric center of the ellipse, and have fairly simple equations of motion.
Phase visualization.
In electronics, the relative phase of two sinusoidal signals can be compared by feeding them to the vertical and horizontal inputs of an oscilloscope. If the display is an ellipse, rather than a straight line, the two signals are out of phase.
Elliptical gears.
Two non-circular gears with the same elliptical outline, each pivoting around one focus and positioned at the proper angle, turn smoothly while maintaining contact at all times. Alternatively, they can be connected by a link chain or timing belt, or in the case of a bicycle the main chainring may be elliptical, or an ovoid similar to an ellipse in form. Such elliptical gears may be used in mechanical equipment to produce variable angular speed or torque from a constant rotation of the driving axle, or in the case of a bicycle to allow a varying crank rotation speed with inversely varying mechanical advantage.
Elliptical bicycle gears make it easier for the chain to slide off the cog when changing gears.
An example gear application would be a device that winds thread onto a conical bobbin on a spinning machine. The bobbin would need to wind faster when the thread is near the apex than when it is near the base.
Ellipses in statistics and finance.
In statistics, a bivariate random vector ("X", "Y") is jointly elliptically distributed if its iso-density contours — loci of equal values of the density function — are ellipses. The concept extends to an arbitrary number of elements of the random vector, in which case in general the iso-density contours are ellipsoids. A special case is the multivariate normal distribution. The elliptical distributions are important in finance because if rates of return on assets are jointly elliptically distributed then all portfolios can be characterized completely by their mean and variance — that is, any two portfolios with identical mean and variance of portfolio return have identical distributions of portfolio return.
Ellipses in computer graphics.
Drawing an ellipse as a graphics primitive is common in standard display libraries, such as the MacIntosh QuickDraw API, and Direct2D on Windows. Jack Bresenham at IBM is most famous for the invention of 2D drawing primitives, including line and circle drawing, using only fast integer operations such as addition and branch on carry bit. M. L. V. Pitteway extended Bresenham's algorithm for lines to conics in 1967. Another efficient generalization to draw ellipses was invented in 1984 by Jerry Van Aken.
In 1970 Danny Cohen presented at the "Computer Graphics 1970" conference in England a linear algorithm for drawing ellipses and circles. In 1971, L. B. Smith published similar algorithms for all conic sections and proved them to have good properties. These algorithms need only a few multiplications and additions to calculate each vector.
It is beneficial to use a parametric formulation in computer graphics because the density of points is greatest where there is the most curvature. Thus, the change in slope between each successive point is small, reducing the apparent "jaggedness" of the approximation.
Drawing with Bézier paths.
Composite Bézier curves may also be used to draw an ellipse to sufficient accuracy, since any ellipse may be construed as an affine transformation of a circle. The spline methods used to draw a circle may be used to draw an ellipse, since the constituent Bézier curves behave appropriately under such transformations.
Drawing with three points of a parallelogram.
Rytz’s construction can be used to find the minor and major axes and their angle of an ellipse from conjugate diameters (which can be seen as three points of a parallelogram). The method uses the conjugate diameters of an ellipse to map the ellipse to an unit circle under affine transformation and calculate the ellipse parameters from that.
Ellipses in optimization theory.
It is sometimes useful to find the minimum bounding ellipse on a set of points. The ellipsoid method is quite useful for attacking this problem.

</doc>
<doc id="10225" url="https://en.wikipedia.org/wiki?curid=10225" title="Elliptic curve">
Elliptic curve

In mathematics, an elliptic curve is a plane algebraic curve defined by an equation of the form
that is non-singular; that is, its graph has no cusps or self-intersections. (When the characteristic of the coefficient field is equal to 2 or 3, the above equation is not quite general enough to comprise all non-singular cubic curves; see below for a more precise definition.) 
Formally, an elliptic curve is a smooth, projective algebraic curve of genus one, on which there is a specified point "O". An elliptic curve is in fact an abelian variety – that is, it has a multiplication defined algebraically, with respect to which it is a (necessarily commutative) group – and "O" serves as the identity element. Often the curve itself, without "O" specified, is called an elliptic curve. The point "O" is actually the "point at infinity" in the projective plane.
If "y"2 = "P"("x"), where "P" is any polynomial of degree three in "x" with no repeated roots, then we obtain a nonsingular plane curve of genus one, which is thus an elliptic curve. If "P" has degree four and is square-free this equation again describes a plane curve of genus one; however, it has no natural choice of identity element. More generally, any algebraic curve of genus one, for example from the intersection of two quadric surfaces embedded in three-dimensional projective space, is called an elliptic curve, provided that it has at least one rational point to act as the identity.
Using the theory of elliptic functions, it can be shown that elliptic curves defined over the complex numbers correspond to embeddings of the torus into the complex projective plane. The torus is also an abelian group, and in fact this correspondence is also a group isomorphism.
Elliptic curves are especially important in number theory, and constitute a major area of current research; for example, they were used in the proof, by Andrew Wiles (assisted by Richard Taylor), of Fermat's Last Theorem. They also find applications in elliptic curve cryptography (ECC) and integer factorization.
An elliptic curve is "not" an ellipse: see elliptic integral for the origin of the term. Topologically, a complex elliptic curve is a torus.
Elliptic curves over the real numbers.
Although the formal definition of an elliptic curve is fairly technical and requires some background in algebraic geometry, it is possible to describe some features of elliptic curves over the real numbers using only high school algebra and geometry.
In this context, an elliptic curve is a plane curve defined by an equation of the form
where "a" and "b" are real numbers. This type of equation is called a Weierstrass equation.
The definition of elliptic curve also requires that the curve be non-singular. Geometrically, this means that the graph has no cusps, self-intersections, or isolated points. Algebraically, this involves calculating the discriminant
The curve is non-singular if and only if the discriminant is not equal to zero. (Although the factor −16 is irrelevant to whether or not the curve is non-singular, this definition of the discriminant is useful in a more advanced study of elliptic curves.)
The (real) graph of a non-singular curve has "two" components if its discriminant is positive, and "one" component if it is negative. For example, in the graphs shown in figure to the right, the discriminant in the first case is 64, and in the second case is −368.
The group law.
When working in the projective plane, we can define a group structure on any smooth cubic curve. In Weierstrass normal form, such a curve will have an additional point at infinity, "O", at the homogeneous coordinates [0:1:0] which serves as the identity of the group.
Since the curve is symmetrical about the x-axis, given any point "P", we can take "-P" to be the point opposite it. We take "-O" to be just "O".
If "P" and "Q" are two points on the curve, then we can uniquely describe a third point, "P + Q", in the following way. First, draw the line between "P" and "Q". This will generally intersect the cubic at a third point, "R". We then take "P + Q" to be "-R", the point opposite "R".
This definition for addition works except in a few special cases related to the point at infinity and intersection multiplicity. The first is when one of the points is "O". Here, we define "P + O = P = O + P", making "O" the identity of the group. Next, if "P" and "Q" are opposites of each other, we define "P + Q = O". Lastly "P = Q", we only have one point, and we can't define the line between them. In this case, we use the tangent line to the curve at this point as our line. In most cases, the tangent will intersect a second point "R" and we can take its opposite. However, if "P" happens to be an inflection point (a point where the concavity of the curve changes), we take "R" to be "P" itself and "P + P" is simply the point opposite itself.
For a cubic curve not in Weierstrass normal form, we can still define a group structure by designating one of its nine inflection points as the identity "O". In the projective plane, each line will intersect a cubic at three points when accounting for multiplicity. For a point "P", "-P" is defined as the unique third point passing through "O" and "P". Then, for any "P" and "Q", "P + Q" is defined as "-R" where "R" is the unique third point on the line containing "P" and "Q".
Let "K" be a field over which the curve is defined (i.e., the coefficients of the defining equation or equations of the curve are in "K") and denote the curve by "E". Then the "K"-rational points of "E" are the points on "E" whose coordinates all lie in "K", including the point at infinity. The set of "K"-rational points is denoted by "E"("K"). It, too, forms a group, because properties of polynomial equations show that if "P" is in "E"("K"), then −"P" is also in "E"("K"), and if two of "P", "Q", and "R" are in "E"("K"), then so is the third. Additionally, if "K" is a subfield of "L", then "E"("K") is a subgroup of "E"("L").
The above group can be described algebraically as well as geometrically. Given the curve "y"2 = "x"3 − "px" − "q" over the field "K" (whose characteristic we assume to be neither 2 nor 3), and points "P" = ("xP", "yP") and "Q" = ("xQ", "yQ") on the curve, assume first that "xP" ≠ "xQ". Let "s" be the slope of the line containing "P" and "Q"; i.e.,
Since "K" is a field, "s" is well-defined. Then we can define "R" = "P" + "Q" = ("xR", "−yR") by
If "xP" = "xQ" (third and fourth panes above), then there are two options: if "yP" = −"yQ", including the case where "yP" = "yQ" = 0, then the sum is defined as 0; thus, the inverse of each point on the curve is found by reflecting it across the "x"-axis. If "yP" = "yQ" ≠ 0 (second pane), then "R" = "P" + "P" = 2"P" = ("x""R", "−y""R") is given by
Associativity.
All of the group laws except associativity follow immediately from the geometrical definition of the group operation. This animation illustrates geometrically the associativity law.
Notice that the sum of the three values on any of the six lines is zero. The location of all nine points is determined by the elliptic curve together with the locations of zero, "a", "b" and "c". The central point of the nine lies on the line through "a" and "b + c", and also on the line through "a + b" and "c". Associativity of the addition law is equivalent to the fact that the curve passes through the central point in the grid. From this fact, the equality of −("a" + ("b" + "c")) and −(("a" + "b") + "c") follows. This idea can be formalized using the Cayley–Bacharach theorem.
The elliptic curve and the point zero are fixed in this animation while "a", "b" and "c" move independently of each other.
Elliptic curves over the complex numbers.
The formulation of elliptic curves as the embedding of a torus in the complex projective plane follows naturally from a curious property of Weierstrass's elliptic functions. These functions and their first derivative are related by the formula
Here, "g"2 and "g"3 are constants; formula_8 is the Weierstrass elliptic function and formula_9 its derivative. It should be clear that this relation is in the form of an elliptic curve (over the complex numbers). The Weierstrass functions are doubly periodic; that is, they are periodic with respect to a lattice Λ; in essence, the Weierstrass functions are naturally defined on a torus "T" = C/Λ. This torus may be embedded in the complex projective plane by means of the map
This map is a group isomorphism, carrying the natural group structure of the torus into the projective plane. It is also an isomorphism of Riemann surfaces, so topologically, a given elliptic curve looks like a torus. If the lattice Λ is related by multiplication by a non-zero complex number "c" to a lattice "c"Λ, then the corresponding curves are isomorphic. Isomorphism classes of elliptic curves are specified by the j-invariant.
The isomorphism classes can be understood in a simpler way as well. The constants "g"2 and "g"3, called the modular invariants, are uniquely determined by the lattice, that is, by the structure of the torus. However, the complex numbers form the splitting field for polynomials with real coefficients, and so the elliptic curve may be written as
One finds that
and
so that the modular discriminant is
Here, λ is sometimes called the modular lambda function.
Note that the uniformization theorem implies that every compact Riemann surface of genus one can be represented as a torus.
This also allows an easy understanding of the torsion points on an elliptic curve: if the lattice Λ is spanned by the fundamental periods ω1 and ω2, then the "n"-torsion points are the (equivalence classes of) points of the form
for "a" and "b" integers in the range from 0 to "n"−1.
Over the complex numbers, every elliptic curve has nine inflection points. Every line through two of these points also passes through a third inflection point; the nine points and 12 lines formed in this way form a realization of the Hesse configuration.
Elliptic curves over the rational numbers.
A curve "E" defined over the field of rational numbers is also defined over the field of real numbers. Therefore the law of addition (of points with real coordinates) by the tangent and secant method can be applied to "E". The explicit formulae show that the sum of two points "P" and "Q" with rational coordinates has again rational coordinates, since the line joining "P" and "Q" has rational coefficients. This way, one shows that the set of rational points of "E" forms a subgroup of the group of real points of "E". As this group, it is an abelian group, that is, "P" + "Q" = "Q" + "P".
The structure of rational points.
The most important result is that all points can be constructed by the method of tangents and secants starting with a "finite" number of points. More precisely the Mordell–Weil theorem states that the group "E"(Q) is a finitely generated (abelian) group. By the fundamental theorem of finitely generated abelian groups it is therefore a finite direct sum of copies of Z and finite cyclic groups.
The proof of that theorem rests on two ingredients: first, one shows that for any integer "m" > 1, the quotient group "E"(Q)/"mE"(Q) is finite (weak Mordell–Weil theorem). Second, introducing a height function "h" on the rational points "E"(Q) defined by "h"("P"0) = 0 and if "P" (unequal to the point at infinity "P"0) has as abscissa the rational number "x" = (with coprime "p" and "q"). This height function "h" has the property that "h"("mP") grows roughly like the square of "m". Moreover, only finitely many rational points with height smaller than any constant exist on "E".
The proof of the theorem is thus a variant of the method of infinite descent and relies on the repeated application of Euclidean divisions on "E": let "P" ∈ "E"(Q) be a rational point on the curve, writing "P" as the sum 2"P"1 + "Q"1 where "Q"1 is a fixed representant of "P" in "E"(Q)/2"E"(Q), the height of "P"1 is about of the one of "P" (more generally, replacing 2 by any "m" > 1, and by ). Redoing the same with "P"1, that is to say "P"1 = 2"P"2 + "Q"2, then "P"2 = 2"P"3 + "Q"3, etc. finally expresses "P" as an integral linear combination of points "Qi" and of points whose height is bounded by a fixed constant chosen in advance: by the weak Mordell–Weil theorem and the second property of the height function "P" is thus expressed as an integral linear combination of a finite number of fixed points.
So far, the theorem is not effective since there is no known general procedure for determining the representants of "E"(Q)/"mE"(Q).
The rank of "E"(Q), that is the number of copies of Z in "E"(Q) or, equivalently, the number of independent points of infinite order, is called the "rank" of "E". The Birch and Swinnerton-Dyer conjecture is concerned with determining the rank. One conjectures that it can be arbitrarily large, even if only examples with relatively small rank are known. The elliptic curve with biggest exactly known rank is
It has rank 19, found by Noam Elkies in 2009. Curves of rank at least 28 are known, but their rank is not exactly known.
As for the groups constituting the torsion subgroup of "E"(Q), the following is known: the torsion subgroup of "E"(Q) is one of the 15 following groups (a theorem due to Barry Mazur): Z/"NZ for "N" = 1, 2, ..., 10, or 12, or Z/2Z × Z/2"NZ with "N" = 1, 2, 3, 4. Examples for every case are known. Moreover, elliptic curves whose Mordell–Weil groups over Q have the same torsion groups belong to a parametrized family.
The Birch and Swinnerton-Dyer conjecture.
The "Birch and Swinnerton-Dyer conjecture" (BSD) is one of the Millennium problems of the Clay Mathematics Institute. The conjecture relies on analytic and arithmetic objects defined by the elliptic curve in question.
At the analytic side, an important ingredient is a function of a complex variable, "L", the Hasse–Weil zeta function of "E" over Q. This function is a variant of the Riemann zeta function and Dirichlet L-functions. It is defined as an Euler product, with one factor for every prime number "p".
For a curve "E" over Q given by a minimal equation
with integral coefficients "ai", reducing the coefficients modulo "p" defines an elliptic curve over the finite field F"p" (except for a finite number of primes "p", where the reduced curve has a singularity and thus fails to be elliptic, in which case "E" is said to be of bad reduction at "p").
The zeta function of an elliptic curve over a finite field F"p" is, in some sense, a generating function assembling the information of the number of points of "E" with values in the finite field extensions of F"p", F"pn". It is given,
The interior sum of the exponential resembles the development of the logarithm and, in fact, the so-defined zeta function is a rational function:
The Hasse–Weil zeta function of "E" over Q is then defined by collecting this information together, for all primes "p". It is defined by
where ε("p") = 1 if "E" has good reduction at "p" and 0 otherwise (in which case "ap" is defined differently than above).
This product converges for Re("s") > 3/2 only. Hasse's conjecture affirms that the "L"-function admits an analytic continuation to the whole complex plane and satisfies a functional equation relating, for any "s", "L"("E", "s") to "L"("E", 2 − "s"). In 1999 this was shown to be a consequence of the proof of the Shimura–Taniyama–Weil conjecture, which asserts that every elliptic curve over "Q" is a modular curve, which implies that its "L"-function is the "L"-function of a modular form whose analytic continuation is known.
One can therefore speak about the values of "L"("E", "s") at any complex number "s". The Birch-Swinnerton-Dyer conjecture relates the arithmetic of the curve to the behavior of its "L"-function at "s" = 1. More precisely, it affirms that the order of the "L"-function at "s" = 1 equals the rank of "E" and predicts the leading term of the Laurent series of "L"("E", "s") at that point in terms of several quantities attached to the elliptic curve.
Much like the Riemann hypothesis, this conjecture has multiple consequences, including the following two:
The modularity theorem and its application to Fermat's Last Theorem.
The modularity theorem, once known as the Taniyama–Shimura–Weil conjecture, states that every elliptic curve "E" over Q is a modular curve, that is to say, its Hasse–Weil zeta function is the "L"-function of a modular form of weight 2 and level "N", where "N" is the conductor of "E" (an integer divisible by the same prime numbers as the discriminant of "E", Δ("E").) In other words, if, for Re("s") > 3/2, one writes the "L"-function in the form
the expression
defines a parabolic modular newform of weight 2 and level "N". For prime numbers ℓ not dividing "N", the coefficient "a"(ℓ) of the form equals ℓ – the number of solutions of the minimal equation of the curve modulo ℓ.
For example, to the elliptic curve formula_26 with discriminant (and conductor) 37, is associated the form
For prime numbers ℓ not equal to 37, one can verify the property about the coefficients. Thus, for ℓ = 3, the solutions of the equation modulo 3 are (0, 0), (0, 1), (2, 0), (1, 0), (1, 1), (2, 1), as and "a"(3) = 3 − 6 = −3.
The conjecture, going back to the fifties, was completely proven by 1999 using ideas of Andrew Wiles, who proved it in 1994 for a large family of elliptic curves.
There are several formulations of the conjecture. Showing that they are equivalent is difficult and was a main topic of number theory in the second half of the 20th century. The modularity of an elliptic curve "E" of conductor "N" can be expressed also by saying that there is a non-constant rational map defined over Q, from the modular curve "X"0("N") to "E". In particular, the points of "E" can be parametrized by modular functions.
For example, a modular parametrization of the curve formula_28 is given by
where, as above, "q" = exp(2π"iz"). The functions "x(z)" and "y(z)" are modular of weight 0 and level 37; in other words they are meromorphic, defined on the upper half-plane Im("z") > 0 and satisfy
and likewise for "y(z)" for all integers "a, b, c, d" with "ad" − "bc" = 1 and 37|"c".
Another formulation depends on the comparison of Galois representations attached on the one hand to elliptic curves, and on the other hand to modular forms. The latter formulation has been used in the proof the conjecture. Dealing with the level of the forms (and the connection to the conductor of the curve) is particularly delicate.
The most spectacular application of the conjecture is the proof of Fermat's Last Theorem (FLT). Suppose that for a prime "p" > 5, the Fermat equation
has a solution with non-zero integers, hence a counter-example to FLT. Then the elliptic curve
of discriminant
cannot be modular. Thus, the proof of the Taniyama–Shimura–Weil conjecture for this family of elliptic curves (called Hellegouarch–Frey curves) implies FLT. The proof of the link between these two statements, based on an idea of Gerhard Frey (1985), is difficult and technical. It was established by Kenneth Ribet in 1987.
Integral points.
This section is concerned with points "P" = ("x", "y") of "E" such that "x" is an integer. The following theorem is due to C. L. Siegel: the set of points "P" = ("x", "y") of "E"(Q) such that "x" is an integer is finite. This theorem can be generalized to points whose "x" coordinate has a denominator divisible only by a fixed finite set of prime numbers.
The theorem can be formulated effectively. For example, if the Weierstrass equation of "E" has integer coefficients bounded by a constant "H", the coordinates ("x", "y") of a point of "E" with both "x" and "y" integer satisfy:
The Sato–Tate conjecture is a statement about how the error term formula_35 in Hasse's theorem varies with the different primes "q", if an elliptic curve E over Q is reduced modulo q. It was proven (for almost all such curves) in 2006 due to the results of Taylor, Harris and Shepherd-Barron, and says that the error terms are equidistributed.
Elliptic curves over finite fields are notably applied in cryptography and for the factorization of large integers. These algorithms often make use of the group structure on the points of "E". Algorithms that are applicable to general groups, for example the group of invertible elements in finite fields, F*"q", can thus be applied to the group of points on an elliptic curve. For example, the discrete logarithm is such an algorithm. The interest in this is that choosing an elliptic curve allows for more flexibility than choosing "q" (and thus the group of units in F"q"). Also, the group structure of elliptic curves is generally more complicated.
Algorithms that use elliptic curves.
Elliptic curves over finite fields are used in some cryptographic applications as well as for integer factorization. Typically, the general idea in these applications is that a known algorithm which makes use of certain finite groups is rewritten to use the groups of rational points of elliptic curves. For more see also:
References.
Serge Lang, in the introduction to the book cited below, stated that "It is possible to write endlessly on elliptic curves. (This is not a threat.)" The following short list is thus at best a guide to the vast expository literature available on the theoretical, algorithmic, and cryptographic aspects of elliptic curves.

</doc>
<doc id="550137" url="https://en.wikipedia.org/wiki?curid=550137" title="Elliptic operator">
Elliptic operator

In the theory of partial differential equations, elliptic operators are differential operators that generalize the Laplace operator. They are defined by the condition that the coefficients of the highest-order derivatives be positive, which implies the key property that the principal symbol is invertible, or equivalently that there are no real characteristic directions.
Elliptic operators are typical of potential theory, and they appear frequently in electrostatics and continuum mechanics. Elliptic regularity implies that their solutions tend to be smooth functions (if the coefficients in the operator are smooth). Steady-state solutions to hyperbolic and parabolic equations generally solve elliptic equations.
Definitions.
A linear differential operator "L" of order "m" on a domain formula_1 in R"d" given by
(where formula_3 is a multi-index, and formula_4) is called "elliptic" if for every "x" in formula_1 and every non-zero formula_6 in R"d",
where formula_8.
In many applications, this condition is not strong enough, and instead a "uniform ellipticity condition" may be imposed for operators of degree "m = 2k":
where "C" is a positive constant. Note that ellipticity only depends on the highest-order terms.
A nonlinear operator
is elliptic if its first-order Taylor expansion with respect to "u" and its derivatives about any point is a linear elliptic operator.
Elliptic regularity theorem.
Let "L" be an elliptic operator of order "2k" with coefficients having "2k" continuous derivatives. The Dirichlet problem for "L" is to find a function "u", given a function "f" and some appropriate boundary values, such that "Lu = f" and such that "u" has the appropriate boundary values and normal derivatives. The existence theory for elliptic operators, using Gårding's inequality and the Lax–Milgram lemma, only guarantees that a weak solution "u" exists in the Sobolev space "H""k".
This situation is ultimately unsatisfactory, as the weak solution "u" might not have enough derivatives for the expression "Lu" to even make sense.
The "elliptic regularity theorem" guarantees that, provided "f" is square-integrable, "u" will in fact have "2k" square-integrable weak derivatives. In particular, if "f" is infinitely-often differentiable, then so is "u".
Any differential operator exhibiting this property is called a hypoelliptic operator; thus, every elliptic operator is hypoelliptic. The property also means that every fundamental solution of an elliptic operator is infinitely differentiable in any neighborhood not containing 0.
As an application, suppose a function formula_17 satisfies the Cauchy-Riemann equations. Since the Cauchy-Riemann equations form an elliptic operator, it follows that formula_17 is smooth.
General definition.
Let formula_19 be a (possibly nonlinear) differential operator between vector bundles of any rank. Take its principal symbol formula_20 with respect to a one-form formula_6. (Basically, what we are doing is replacing the highest order covariant derivatives formula_22 by vector fields formula_6.)
We say formula_19 is "weakly elliptic" if formula_20 is a linear isomorphism for every non-zero formula_6.
We say formula_19 is (uniformly) "strongly elliptic" if for some constant formula_28,
for all formula_30 and all formula_31. It is important to note that the definition of ellipticity in the previous part of the article is "strong ellipticity". Here formula_32 is an inner product. Notice that the formula_6 are covector fields or one-forms, but the formula_31 are elements of the vector bundle upon which formula_19 acts.
The quintessential example of a (strongly) elliptic operator is the Laplacian (or its negative, depending upon convention). It is not hard to see that formula_19 needs to be of even order for strong ellipticity to even be an option. Otherwise, just consider plugging in both formula_6 and its negative. On the other hand, a weakly elliptic first-order operator, such as the Dirac operator can square to become a strongly elliptic operator, such as the Laplacian. The composition of weakly elliptic operators is weakly elliptic.
Weak ellipticity is nevertheless strong enough for the Fredholm alternative, Schauder estimates, and the Atiyah–Singer index theorem. On the other hand, we need strong ellipticity for the maximum principle, and to guarantee that the eigenvalues are discrete, and their only limit point is infinity.

</doc>
<doc id="9649" url="https://en.wikipedia.org/wiki?curid=9649" title="Energy">
Energy

In physics, energy is a property of objects which can be transferred to other objects or converted into different forms, but cannot be created or destroyed. The "ability of a system to perform work" is a common description, but it is difficult to give one single comprehensive definition of energy because of its many forms. For instance, in SI units, energy is measured in joules, and one joule is defined "mechanically", being the energy transferred to an object by the mechanical work of moving it a distance of 1 metre against a force of 1 newton. However, there are many other definitions of energy, depending on the context, such as thermal energy, radiant energy, electromagnetic, nuclear, etc., where definitions are derived that are the most convenient.
Common energy forms include the kinetic energy of a moving object, the radiant energy carried by light, the potential energy stored by an object's position in a force field (gravitational, electric or magnetic), elastic energy stored by stretching solid objects, chemical energy released when a fuel burns, and the thermal energy due to an object's temperature. All of the many forms of energy are convertible to other kinds of energy, and obey the law of conservation of energy which says that energy can be neither created nor be destroyed; however, it can change from one form to another.
For "closed systems" with no external source or sink of energy, the first law of thermodynamics states that a system's energy is constant unless energy is transferred in or out by mechanical work or heat, and that no energy is lost in transfer. This means that it is impossible to create or destroy energy. The second law of thermodynamics states that all systems doing work always lose some energy as waste heat. This creates a limit to the amount of energy that can do work by a heating process, a limit called the available energy. Mechanical and other forms of energy can be transformed in the other direction into thermal energy without such limitations. The total energy of a system can be calculated by adding up all forms of energy in the system.
Examples of energy transformation include generating electric energy from heat energy via a steam turbine, or lifting an object against gravity using electrical energy driving a crane motor. Lifting against gravity performs mechanical work on the object and stores gravitational potential energy In the object. If the object falls to the ground, gravity does mechanical work on the object which transforms the potential energy in the gravitational field to the kinetic energy released as heat on impact with the ground. Our Sun transforms nuclear potential energy to other forms of energy; its total mass does not decrease due to that in itself (since it still contains the same total energy even if in different forms), but its mass does decrease when the energy escapes out to its surroundings, largely as radiant energy.
Mass and energy are closely related. According to the theory of mass–energy equivalence, any object that has mass when stationary in a frame of reference (called rest mass) also has an equivalent amount of energy whose form is called rest energy in that frame, and any additional energy acquired by the object above that rest energy will increase an object's mass. For example, if you had a sensitive enough scale, you could measure an increase in mass after heating an object.
Living organisms require available energy to stay alive, such as the energy humans get from food. Civilisation gets the energy it needs from energy resources such as fossil fuels. The processes of Earth's climate and ecosystem are driven by the radiant energy Earth receives from the sun and the geothermal energy contained within the earth.
Forms.
The total energy of a system can be subdivided and classified in various ways. For example, classical mechanics distinguishes between kinetic energy, which is determined by an object's movement through space, and potential energy, which is a function of the position of an object within a field. It may also be convenient to distinguish gravitational energy, thermal energy, several types of nuclear energy (which utilize potentials from the nuclear force and the weak force), electric energy (from the electric field), and magnetic energy (from the magnetic field), among others. Many of these classifications overlap; for instance, thermal energy usually consists partly of kinetic and partly of potential energy.
Some types of energy are a varying mix of both potential and kinetic energy. An example is mechanical energy which is the sum of (usually macroscopic) kinetic and potential energy in a system. Elastic energy in materials is also dependent upon electrical potential energy (among atoms and molecules), as is chemical energy, which is stored and released from a reservoir of electrical potential energy between electrons, and the molecules or atomic nuclei that attract them. .The list is also not necessarily complete. Whenever physical scientists discover that a certain phenomenon appears to violate the law of energy conservation, new forms are typically added that account for the discrepancy.
Heat and work are special cases in that they are not properties of systems, but are instead properties of "processes" that transfer energy. In general we cannot measure how much heat or work are present in an object, but rather only how much energy is transferred among objects in certain ways during the occurrence of a given process. Heat and work are measured as positive or negative depending on which side of the transfer we view them from.
Potential energies are often measured as positive or negative depending on whether they are greater or less than the energy of a specified base state or configuration such as two interacting bodies being infinitely far apart. Wave energies (such as radiant or sound energy), kinetic energy, and rest energy are each greater than or equal to zero because they are measured in comparison to a base state of zero energy: "no wave", "no motion", and "no inertia", respectively.
The distinctions between different kinds of energy is not always clear-cut. As Richard Feynman points out:
Some examples of different kinds of energy:
History.
The word "energy" derives from the , which possibly appears for the first time in the work of Aristotle in the 4th century BC. In contrast to the modern definition, energeia was a qualitative philosophical concept, broad enough to include ideas such as happiness and pleasure.
In the late 17th century, Gottfried Leibniz proposed the idea of the , or living force, which defined as the product of the mass of an object and its velocity squared; he believed that total "vis viva" was conserved. To account for slowing due to friction, Leibniz theorized that thermal energy consisted of the random motion of the constituent parts of matter, a view shared by Isaac Newton, although it would be more than a century until this was generally accepted. The modern analog of this property, kinetic energy, differs from "vis viva" only by a factor of two.
In 1807, Thomas Young was possibly the first to use the term "energy" instead of "vis viva", in its modern sense. Gustave-Gaspard Coriolis described "kinetic energy" in 1829 in its modern sense, and in 1853, William Rankine coined the term "potential energy". The law of conservation of energy, was also first postulated in the early 19th century, and applies to any isolated system. It was argued for some years whether heat was a physical substance, dubbed the caloric, or merely a physical quantity, such as momentum. In 1845 James Prescott Joule discovered the link between mechanical work and the generation of heat.
These developments led to the theory of conservation of energy, formalized largely by William Thomson (Lord Kelvin) as the field of thermodynamics. Thermodynamics aided the rapid development of explanations of chemical processes by Rudolf Clausius, Josiah Willard Gibbs, and Walther Nernst. It also led to a mathematical formulation of the concept of entropy by Clausius and to the introduction of laws of radiant energy by Jožef Stefan. According to Noether's theorem, the conservation of energy is a consequence of the fact that the laws of physics do not change over time. Thus, since 1918, theorists have understood that the law of conservation of energy is the direct mathematical consequence of the translational symmetry of the quantity conjugate to energy, namely time.
Units of measure.
In 1843 James Prescott Joule independently discovered the mechanical equivalent in a series of experiments. The most famous of them used the "Joule apparatus": a descending weight, attached to a string, caused rotation of a paddle immersed in water, practically insulated from heat transfer. It showed that the gravitational potential energy lost by the weight in descending was equal to the internal energy gained by the water through friction with the paddle.
In the International System of Units (SI), the unit of energy is the joule, named after James Prescott Joule. It is a derived unit. It is equal to the energy expended (or work done) in applying a force of one newton through a distance of one metre. However energy is also expressed in many other units not part of the SI, such as ergs, calories, British Thermal Units, kilowatt-hours and kilocalories, which require a conversion factor when expressed in SI units.
The SI unit of energy rate (energy per unit time) is the watt, which is a joule per second. Thus, one joule is one watt-second, and 3600 joules equal one watt-hour. The CGS energy unit is the erg and the imperial and US customary unit is the foot pound. Other energy units such as the electronvolt, food calorie or thermodynamic kcal (based on the temperature change of water in a heating process), and BTU are used in specific areas of science and commerce.
Scientific use.
Classical mechanics.
In classical mechanics, energy is a conceptually and mathematically useful property, as it is a conserved quantity. Several formulations of mechanics have been developed using energy as a core concept.
Work, a form of energy, is force times distance.
This says that the work (formula_2) is equal to the line integral of the force F along a path "C"; for details see the mechanical work article. Work and thus energy is frame dependent. For example, consider a ball being hit by a bat. In the center-of-mass reference frame, the bat does no work on the ball. But, in the reference frame of the person swinging the bat, considerable work is done on the ball.
The total energy of a system is sometimes called the Hamiltonian, after William Rowan Hamilton. The classical equations of motion can be written in terms of the Hamiltonian, even for highly complex or abstract systems. These classical equations have remarkably direct analogs in nonrelativistic quantum mechanics.
Another energy-related concept is called the Lagrangian, after Joseph-Louis Lagrange. This formalism is as fundamental as the Hamiltonian, and both can be used to derive the equations of motion or be derived from them. It was invented in the context of classical mechanics, but is generally useful in modern physics. The Lagrangian is defined as the kinetic energy "minus" the potential energy. Usually, the Lagrange formalism is mathematically more convenient than the Hamiltonian for non-conservative systems (such as systems with friction).
Noether's theorem (1918) states that any differentiable symmetry of the action of a physical system has a corresponding conservation law. Noether's theorem has become a fundamental tool of modern theoretical physics and the calculus of variations. A generalisation of the seminal formulations on constants of motion in Lagrangian and Hamiltonian mechanics (1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian; for example, dissipative systems with continuous symmetries need not have a corresponding conservation law.
Chemistry.
In the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structure, it is invariably accompanied by an increase or decrease of energy of the substances involved. Some energy is transferred between the surroundings and the reactants of the reaction in the form of heat or light; thus the products of a reaction may have more or less energy than the reactants. A reaction is said to be exergonic if the final state is lower on the energy scale than the initial state; in the case of endergonic reactions the situation is the reverse. Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy. The "speed" of a chemical reaction (at given temperature "T") is related to the activation energy "E", by the Boltzmann's population factor e−"E"/"kT"that is the probability of molecule to have energy greater than or equal to "E" at the given temperature "T". This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation.The activation energy necessary for a chemical reaction can be in the form of thermal energy.
Biology.
In biology, energy is an attribute of all biological systems from the biosphere to the smallest living organism. Within an organism it is responsible for growth and development of a biological cell or an organelle of a biological organism. Energy is thus often said to be stored by cells in the structures of molecules of substances such as carbohydrates (including sugars), lipids, and proteins, which release energy when reacted with oxygen in respiration. In human terms, the human equivalent (H-e) (Human energy conversion) indicates, for a given amount of energy expenditure, the relative quantity of energy needed for human metabolism, assuming an average human energy expenditure of 12,500kJ per day and a basal metabolic rate of 80 watts. For example, if our bodies run (on average) at 80 watts, then a light bulb running at 100 watts is running at 1.25 human equivalents (100 ÷ 80) i.e. 1.25 H-e. For a difficult task of only a few seconds' duration, a person can put out thousands of watts, many times the 746 watts in one official horsepower. For tasks lasting a few minutes, a fit human can generate perhaps 1,000 watts. For an activity that must be sustained for an hour, output drops to around 300; for an activity kept up all day, 150 watts is about the maximum. The human equivalent assists understanding of energy flows in physical and biological systems by expressing energy units in human terms: it provides a "feel" for the use of a given amount of energy
Sunlight is also captured by plants as "chemical potential energy" in photosynthesis, when carbon dioxide and water (two low-energy compounds) are converted into the high-energy compounds carbohydrates, lipids, and proteins. Plants also release oxygen during photosynthesis, which is utilized by living organisms as an electron acceptor, to release the energy of carbohydrates, lipids, and proteins. Release of the energy stored during photosynthesis as heat or light may be triggered suddenly by a spark, in a forest fire, or it may be made available more slowly for animal or human metabolism, when these molecules are ingested, and catabolism is triggered by enzyme action.
Any living organism relies on an external source of energy—radiation from the Sun in the case of green plants; chemical energy in some form in the case of animals—to be able to grow and reproduce. The daily 1500–2000 Calories (6–8 MJ) recommended for a human adult are taken as a combination of oxygen and food molecules, the latter mostly carbohydrates and fats, of which glucose (C6H12O6) and stearin (C57H110O6) are convenient examples. The food molecules are oxidised to carbon dioxide and water in the mitochondria
and some of the energy is used to convert ADP into ATP
The rest of the chemical energy in the carbohydrate or fat is converted into heat: the ATP is used as a sort of "energy currency", and some of the chemical energy it contains when split and reacted with water, is used for other metabolism (at each stage of a metabolic pathway, some chemical energy is converted into heat). Only a tiny fraction of the original chemical energy is used for work:
It would appear that living organisms are remarkably inefficient (in the physical sense) in their use of the energy they receive (chemical energy or radiation), and it is true that most real machines manage higher efficiencies. In growing organisms the energy that is converted to heat serves a vital purpose, as it allows the organism tissue to be highly ordered with regard to the molecules it is built from. The second law of thermodynamics states that energy (and matter) tends to become more evenly spread out across the universe: to concentrate energy (or matter) in one specific place, it is necessary to spread out a greater amount of energy (as heat) across the remainder of the universe ("the surroundings"). Simpler organisms can achieve higher energy efficiencies than more complex ones, but the complex organisms can occupy ecological niches that are not available to their simpler brethren. The conversion of a portion of the chemical energy to heat at each step in a metabolic pathway is the physical reason behind the pyramid of biomass observed in ecology: to take just the first step in the food chain, of the estimated 124.7 Pg/a of carbon that is fixed by photosynthesis, 64.3 Pg/a (52%) are used for the metabolism of green plants, i.e. reconverted into carbon dioxide and heat.
Earth sciences.
In geology, continental drift, mountain ranges, volcanoes, and earthquakes are phenomena that can be explained in terms of energy transformations in the Earth's interior., while meteorological phenomena like wind, rain, hail, snow, lightning, tornadoes and hurricanes, are all a result of energy transformations brought about by solar energy on the atmosphere of the planet Earth.
Sunlight may be stored as gravitational potential energy after it strikes the Earth, as (for example) water evaporates from oceans and is deposited upon mountains (where, after being released at a hydroelectric dam, it can be used to drive turbines or generators to produce electricity). Sunlight also drives many weather phenomena, save those generated by volcanic events. An example of a solar-mediated weather event is a hurricane, which occurs when large unstable areas of warm ocean, heated over months, give up some of their thermal energy suddenly to power a few days of violent air movement.
In a slower process, radioactive decay of atoms in the core of the Earth releases heat. This thermal energy drives plate tectonics and may lift mountains, via orogenesis. This slow lifting represents a kind of gravitational potential energy storage of the thermal energy, which may be later released to active kinetic energy in landslides, after a triggering event. Earthquakes also release stored elastic potential energy in rocks, a store that has been produced ultimately from the same radioactive heat sources. Thus, according to present understanding, familiar events such as landslides and earthquakes release energy that has been stored as potential energy in the Earth's gravitational field or elastic strain (mechanical potential energy) in rocks. Prior to this, they represent release of energy that has been stored in heavy atoms since the collapse of long-destroyed supernova stars created these atoms.
Cosmology.
In cosmology and astronomy the phenomena of stars, nova, supernova, quasars and gamma-ray bursts are the universe's highest-output energy transformations of matter. All stellar phenomena (including solar activity) are driven by various kinds of energy transformations. Energy in such transformations is either from gravitational collapse of matter (usually molecular hydrogen) into various classes of astronomical objects (stars, black holes, etc.), or from nuclear fusion (of lighter elements, primarily hydrogen). The nuclear fusion of hydrogen in the Sun also releases another store of potential energy which was created at the time of the Big Bang. At that time, according to theory, space expanded and the universe cooled too rapidly for hydrogen to completely fuse into heavier elements. This meant that hydrogen represents a store of potential energy that can be released by fusion. Such a fusion process is triggered by heat and pressure generated from gravitational collapse of hydrogen clouds when they produce stars, and some of the fusion energy is then transformed into sunlight.
Quantum mechanics.
In quantum mechanics, energy is defined in terms of the energy operator
as a time derivative of the wave function. The Schrödinger equation equates the energy operator to the full energy of a particle or a system. Its results can be considered as a definition of measurement of energy in quantum mechanics. The Schrödinger equation describes the space- and time-dependence of a slowly changing (non-relativistic) wave function of quantum systems. The solution of this equation for a bound system is discrete (a set of permitted states, each characterized by an energy level) which results in the concept of quanta. In the solution of the Schrödinger equation for any oscillator (vibrator) and for electromagnetic waves in a vacuum, the resulting energy states are related to the frequency by Planck's relation: formula_3 (where formula_4 is Planck's constant and formula_5 the frequency). In the case of an electromagnetic wave these energy states are called quanta of light or photons.
Relativity.
When calculating kinetic energy (work to accelerate a mass from zero speed to some finite speed) relativistically – using Lorentz transformations instead of Newtonian mechanics – Einstein discovered an unexpected by-product of these calculations to be an energy term which does not vanish at zero speed. He called it rest mass energy: energy which every mass must possess even when being at rest. The amount of energy is directly proportional to the mass of body:
where
For example, consider electron–positron annihilation, in which the rest mass of individual particles is destroyed, but the inertia equivalent of the system of the two particles (its invariant mass) remains (since all energy is associated with mass), and this inertia and invariant mass is carried off by photons which individually are massless, but as a system retain their mass. This is a reversible process – the inverse process is called pair creation – in which the rest mass of particles is created from energy of two (or more) annihilating photons. In this system the matter (electrons and positrons) is destroyed and changed to non-matter energy (the photons). However, the total system mass and energy do not change during this interaction.
In general relativity, the stress–energy tensor serves as the source term for the gravitational field, in rough analogy to the way mass serves as the source term in the non-relativistic Newtonian approximation.
It is not uncommon to hear that energy is "equivalent" to mass. It would be more accurate to state that every energy has an inertia and gravity equivalent, and because mass is a form of energy, then mass too has inertia and gravity associated with it.
In classical physics, energy is a scalar quantity, the canonical conjugate to time. In special relativity energy is also a scalar (although not a Lorentz scalar but a time component of the energy–momentum 4-vector). In other words, energy is invariant with respect to rotations of space, but not invariant with respect to rotations of space-time (= boosts).
Transformation.
Energy may be transformed between different forms at various efficiencies. Items that transform between these forms are called transducers. Examples of transducers include a battery, from chemical energy to electric energy; a dam: gravitational potential energy to kinetic energy of moving water (and the blades of a turbine) and ultimately to electric energy through an electric generator.
There are strict limits to how efficiently energy can be converted into other forms of energy via work, and heat as described by Carnot's theorem and the second law of thermodynamics. These limits are especially evident when an engine is used to perform work. However, some energy transformations can be quite efficient. The direction of transformations in energy (what kind of energy is transformed to what other kind) is often determined by entropy (equal energy spread among all available degrees of freedom) considerations. In practice all energy transformations are permitted on a small scale, but certain larger transformations are not permitted because it is statistically unlikely that energy or matter will randomly move into more concentrated forms or smaller spaces.
Energy transformations in the universe over time are characterized by various kinds of potential energy that has been available since the Big Bang, later being "released" (transformed to more active types of energy such as kinetic or radiant energy), when a triggering mechanism is available. Familiar examples of such processes include nuclear decay, in which energy is released that was originally "stored" in heavy isotopes (such as uranium and thorium), by nucleosynthesis, a process ultimately using the gravitational potential energy released from the gravitational collapse of supernovae, to store energy in the creation of these heavy elements before they were incorporated into the solar system and the Earth. This energy is triggered and released in nuclear fission bombs or in civil nuclear power generation. Similarly, in the case of a chemical explosion, chemical potential energy is transformed to kinetic energy and thermal energy in a very short time. Yet another example is that of a pendulum. At its highest points the kinetic energy is zero and the gravitational potential energy is at maximum. At its lowest point the kinetic energy is at maximum and is equal to the decrease of potential energy. If one (unrealistically) assumes that there is no friction or other losses, the conversion of energy between these processes would be perfect, and the pendulum would continue swinging forever.
Energy is also transferred from potential energy (formula_7) to kinetic energy (formula_8) and then back to potential energy constantly. This is referred to as conservation of energy. In this closed system, energy cannot be created or destroyed; therefore, the initial energy and the final energy will be equal to each other. This can be demonstrated by the following:
The equation can then be simplified further since formula_9 (mass times acceleration due to gravity times the height) and formula_10 (half mass times velocity squared). Then the total amount of energy can be found by adding formula_11.
Conservation of energy and mass in transformation.
Energy gives rise to weight when it is trapped in a system with zero momentum, where it can be weighed. It is also equivalent to mass, and this mass is always associated with it. Mass is also equivalent to a certain amount of energy, and likewise always appears associated with it, as described in mass-energy equivalence. The formula "E" = "mc"², derived by Albert Einstein (1905) quantifies the relationship between rest-mass and rest-energy within the concept of special relativity. In different theoretical frameworks, similar formulas were derived by J. J. Thomson (1881), Henri Poincaré (1900), Friedrich Hasenöhrl (1904) and others (see Mass-energy equivalence#History for further information).
Matter may be converted to energy (and vice versa), but mass cannot ever be destroyed; rather, mass/energy equivalence remains a constant for both the matter and the energy, during any process when they are converted into each other. However, since formula_12 is extremely large relative to ordinary human scales, the conversion of ordinary amount of matter (for example, 1 kg) to other forms of energy (such as heat, light, and other radiation) can liberate tremendous amounts of energy (~formula_13 joules = 21 megatons of TNT), as can be seen in nuclear reactors and nuclear weapons. Conversely, the mass equivalent of a unit of energy is minuscule, which is why a loss of energy (loss of mass) from most systems is difficult to measure by weight, unless the energy loss is very large. Examples of energy transformation into matter (i.e., kinetic energy into particles with rest mass) are found in high-energy nuclear physics.
Reversible and non-reversible transformations.
Thermodynamics divides energy transformation into two kinds: reversible processes and irreversible processes. An irreversible process is one in which energy is dissipated (spread) into empty energy states available in a volume, from which it cannot be recovered into more concentrated forms (fewer quantum states), without degradation of even more energy. A reversible process is one in which this sort of dissipation does not happen. For example, conversion of energy from one type of potential field to another, is reversible, as in the pendulum system described above. In processes where heat is generated, quantum states of lower energy, present as possible excitations in fields between atoms, act as a reservoir for part of the energy, from which it cannot be recovered, in order to be converted with 100% efficiency into other forms of energy. In this case, the energy must partly stay as heat, and cannot be completely recovered as usable energy, except at the price of an increase in some other kind of heat-like increase in disorder in quantum states, in the universe (such as an expansion of matter, or a randomisation in a crystal).
As the universe evolves in time, more and more of its energy becomes trapped in irreversible states (i.e., as heat or other kinds of increases in disorder). This has been referred to as the inevitable thermodynamic heat death of the universe. In this heat death the energy of the universe does not change, but the fraction of energy which is available to do work through a heat engine, or be transformed to other usable forms of energy (through the use of generators attached to heat engines), grows less and less.
Conservation of energy.
According to conservation of energy, energy can neither be created (produced) nor destroyed by itself. It can only be transformed. The total inflow of energy into a system must equal the total outflow of energy from the system, plus the change in the energy contained within the system. Energy is subject to a strict global conservation law; that is, whenever one measures (or calculates) the total energy of a system of particles whose interactions do not depend explicitly on time, it is found that the total energy of the system always remains constant.
Richard Feynman said during a 1961 lecture:
Most kinds of energy (with gravitational energy being a notable exception) are subject to strict local conservation laws as well. In this case, energy can only be exchanged between adjacent regions of space, and all observers agree as to the volumetric density of energy in any given space. There is also a global law of conservation of energy, stating that the total energy of the universe cannot change; this is a corollary of the local law, but not vice versa.
This law is a fundamental principle of physics. As shown rigorously by Noether's theorem, the conservation of energy is a mathematical consequence of translational symmetry of time, a property of most phenomena below the cosmic scale that makes them independent of their locations on the time coordinate. Put differently, yesterday, today, and tomorrow are physically indistinguishable. This is because energy is the quantity which is canonical conjugate to time. This mathematical entanglement of energy and time also results in the uncertainty principle - it is impossible to define the exact amount of energy during any definite time interval. The uncertainty principle should not be confused with energy conservation - rather it provides mathematical limits to which energy can in principle be defined and measured.
Each of the basic forces of nature is associated with a different type of potential energy, and all types of potential energy (like all other types of energy) appears as system mass, whenever present. For example, a compressed spring will be slightly more massive than before it was compressed. Likewise, whenever energy is transferred between systems by any mechanism, an associated mass is transferred with it.
In quantum mechanics energy is expressed using the Hamiltonian operator. On any time scales, the uncertainty in the energy is by
which is similar in form to the Heisenberg Uncertainty Principle (but not really mathematically equivalent thereto, since "H" and "t" are not dynamically conjugate variables, neither in classical nor in quantum mechanics).
In particle physics, this inequality permits a qualitative understanding of virtual particles which carry momentum, exchange by which and with real particles, is responsible for the creation of all known fundamental forces (more accurately known as fundamental interactions). Virtual photons (which are simply lowest quantum mechanical energy state of photons) are also responsible for electrostatic interaction between electric charges (which results in Coulomb law), for spontaneous radiative decay of exited atomic and nuclear states, for the Casimir force, for van der Waals bond forces and some other observable phenomena.
Energy transfer.
Closed systems.
Energy transfer usually refers to movements of energy between systems which are closed to transfers of matter. The portion of the energy which is transferred by conservative forces over a distance is measured as the work the source system does on the receiving system. The portion of the energy which does not do work doing during the transfer is called heat. Energy can be transferred between systems in a variety of ways. Examples include the transmission of electromagnetic energy via photons, physical collisions which transfer kinetic energy, and the conductive transfer of thermal energy.
Energy is strictly conserved and is also locally conserved wherever it can be defined. Mathematically, the process of energy transfer is described by the first law of thermodynamics:
where formula_15 is the amount of energy transferred, formula_2  represents the work done on the system, and formula_17 represents the heat flow into the system. As a simplification, the heat term, formula_17, is sometimes ignored, especially when the thermal efficiency of the transfer is high.
This simplified equation is the one used to define the joule, for example.
Open systems.
There are other ways in which an open system can gain or lose energy. In chemical systems, energy can be added to a system by means of adding substances with different chemical potentials, which potentials are then extracted (both of these process are illustrated by fueling an auto, a system which gains in energy thereby, without addition of either work or heat). These terms may be added to the above equation, or they can generally be subsumed into a quantity called "energy addition term formula_15" which refers to "any" type of energy carried over the surface of a control volume or system volume. Examples may be seen above, and many others can be imagined (for example, the kinetic energy of a stream of particles entering a system, or energy from a laser beam adds to system energy, without either being either work-done or heat-added, in the classic senses).
Where formula_15 in this general equation represents other additional advected energy terms not covered by work done on a system, or heat added to it.
Thermodynamics.
Internal energy.
Internal energy is the sum of all microscopic forms of energy of a system. It is the energy needed to create the system. It is related to the potential energy, e.g., molecular structure, crystal structure, and other geometric aspects, as well as the motion of the particles, in form of kinetic energy. Thermodynamics is chiefly concerned with changes in internal energy and not its absolute value, which is impossible to determine with thermodynamics alone.
First law of thermodynamics.
The first law of thermodynamics asserts that energy (but not necessarily thermodynamic free energy) is always conserved and that heat flow is a form of energy transfer. For homogeneous systems, with a well-defined temperature and pressure, a commonly used corollary of the first law is that, for a system subject only to pressure forces and heat transfer (e.g., a cylinder-full of gas), the differential change in the internal energy of the system (with a "gain" in energy signified by a positive quantity) is given as
where the first term on the right is the heat transferred into the system, expressed in terms of temperature "T" and entropy "S" (in which entropy increases and the change d"S" is positive when the system is heated), and the last term on the right hand side is identified as work done on the system, where pressure is "P" and volume "V" (the negative sign results since compression of the system requires work to be done on it and so the volume change, d"V", is negative when work is done on the system).
This equation is highly specific, ignoring all chemical, electrical, nuclear, and gravitational forces, effects such as advection of any form of energy other than heat and pV-work. The general formulation of the first law (i.e., conservation of energy) is valid even in situations in which the system is not homogeneous. For these cases the change in internal energy of a "closed" system is expressed in a general form by
where formula_23 is the heat supplied to the system and formula_24 is the work applied to the system.
Equipartition of energy.
The energy of a mechanical harmonic oscillator (a mass on a spring) is alternatively kinetic and potential. At two points in the oscillation cycle it is entirely kinetic, and alternatively at two other points it is entirely potential. Over the whole cycle, or over many cycles, net energy is thus equally split between kinetic and potential. This is called equipartition principle; total energy of a system with many degrees of freedom is equally split among all available degrees of freedom.
This principle is vitally important to understanding the behaviour of a quantity closely related to energy, called entropy. Entropy is a measure of evenness of a distribution of energy between parts of a system. When an isolated system is given more degrees of freedom (i.e., given new available energy states that are the same as existing states), then total energy spreads over all available degrees equally without distinction between "new" and "old" degrees. This mathematical result is called the second law of thermodynamics.

</doc>
<doc id="10274" url="https://en.wikipedia.org/wiki?curid=10274" title="Enthalpy">
Enthalpy

Enthalpy is defined as a thermodynamic state function, designated by the letter "H", that consists of the internal energy of the system (U) plus the product of pressure (p) and volume (V) of the system:
Since U, p and V are all functions of the state of the thermodynamic system, enthalpy is a state function.
The unit of measurement for enthalpy in the International System of Units (SI) is the joule, but other historical, conventional units are still in use, such as the British thermal unit and the calorie.
The enthalpy is the preferred expression of system energy changes in many chemical, biological, and physical measurements at constant pressure, because it simplifies the description of energy transfer. At constant pressure, the enthalpy change equals the energy transferred from the environment through heating or work other than expansion work.
The total enthalpy, "H", of a system cannot be measured directly. The same situation exists in classical mechanics: only a change or difference in energy carries physical meaning. Enthalpy itself is a thermodynamic potential, so in order to measure the enthalpy of a system, we must refer to a defined reference point; therefore what we measure is the change in enthalpy, Δ"H". The Δ"H" is a positive change in endothermic reactions, and negative in heat-releasing exothermic processes. 
For processes under constant pressure, Δ"H" is equal to the change in the internal energy of the system, plus the pressure-volume work that the system has done on its surroundings. This means that the change in enthalpy under such conditions is the heat absorbed (or released) by the material through a chemical reaction or by external heat transfer. Enthalpies for chemical substances at constant pressure assume standard state: most commonly 1 bar pressure. Standard state does not, strictly speaking, specify a temperature (see standard state), but expressions for enthalpy generally reference the standard heat of formation at 25 °C.
Enthalpy of ideal gases and incompressible solids and liquids does not depend on pressure, unlike entropy and Gibbs energy. Real materials at common temperatures and pressures usually closely approximate this behavior, which greatly simplifies enthalpy calculation and use in practical designs and analyses.
Origins.
The word "enthalpy" is based on the Greek "enthalpein" (ἐνθάλπειν), which means "to warm in". It comes from the Classical Greek prefix ἐν- "en-", meaning "to put into", and the verb θάλπειν "thalpein", meaning "to heat". The word "enthalpy" is often incorrectly attributed to Benoît Paul Émile Clapeyron and Rudolf Clausius through the 1850 publication of their Clausius–Clapeyron relation. This misconception was popularized by the 1927 publication of "The Mollier Steam Tables and Diagrams". However, neither the concept, the word, nor the symbol for enthalpy existed until well after Clapeyron's death.
The earliest writings to contain the concept of enthalpy did not appear until 1875,
when Josiah Willard Gibbs introduced "a heat function for constant pressure". However, Gibbs did not use the word "enthalpy" in his writings.
The actual word first appears in the scientific literature in a 1909 publication by J. P. Dalton. According to that publication, Heike Kamerlingh Onnes (1853-1926) actually coined the word.
Over the years, many different symbols were used to denote enthalpy. It was not until 1922 that Alfred W. Porter proposed the symbol "H" as the accepted standard, thus finalizing the terminology still in use today.
Formal definition.
The enthalpy of a homogeneous system is defined as:
where
The enthalpy is an extensive property. This means that, for homogeneous systems, the enthalpy is proportional to the size of the system. It is convenient to introduce the specific enthalpy "h" ="H/m" where "m" is the mass of the system, or the molar enthalpy "H"m = "H/n", where "n" is the number of moles ("h" and "H"m are intensive properties). For inhomogeneous systems the enthalpy is the sum of the enthalpies of the composing subsystems
where the label "k" refers to the various subsystems. In case of continuously varying "p", "T", and/or composition the summation becomes an integral:
where "ρ" is the density.
The enthalpy of homogeneous systems can be viewed as function "H"("S,p") of the entropy "S" and the pressure "p", and a differential relation for it can be derived as follows. We start from the first law of thermodynamics for closed systems for an infinitesimal process
Here, δ"Q" is a small amount of heat added to the system and δ"W" a small amount of work performed by the system. In a homogeneous system only reversible processes can take place so the second law of thermodynamics gives δ"Q" = "T"d"S" with "T" the absolute temperature of the system. Furthermore, if only pV work is done, δ"W" = "p"d"V". As a result
Adding d("pV") to both sides of this expression gives
or
So
Other expressions.
The above expression of d"H" in terms of entropy and pressure may be unfamiliar to some readers. However, there are expressions in terms of more familiar variables such as temperature and pressure
formula_10
Here "C"p is the heat capacity at constant pressure and "α" is the coefficient of (cubic) thermal expansion
formula_11
With this expression one can, in principle, determine the enthalpy if "C"p and "V" are known as functions of "p" and "T".
Notice that for an ideal gas, formula_12, so that:
formula_13
In a more general form, the first law describes the internal energy with additional terms involving the chemical potential and the number of particles of various types. The differential statement for d"H" then becomes:
where "μ""i" is the chemical potential per particle for an "i"-type particle, and "N""i" is the number of such particles. The last term can also be written as "μ"id"n""i" (with d"n""i" the number of moles of component "i" added to the system and, in this case, "μ"i the molar chemical potential) or as "μ"id"m""i" (with d"m""i" the mass of component "i" added to the system and, in this case, "μ"i the specific chemical potential).
Physical interpretation.
The "U" term can be interpreted as the energy required to create the system, and the "pV" term as the energy that would be required to "make room" for the system if the pressure of the environment remained constant. When a system, for example, "n" moles of a gas of volume "V" at pressure "p" and temperature "T", is created or brought to its present state from absolute zero, energy must be supplied equal to its internal energy "U" plus "pV", where "pV" is the work done in pushing against the ambient (atmospheric) pressure.
In basic physics and statistical mechanics it may be more interesting to study the internal properties of the system and therefore the internal energy is used. In basic chemistry, experiments are often conducted at constant atmospheric pressure, and the pressure-volume work represents an energy exchange with the atmosphere that cannot be accessed or controlled, so that "ΔH" is the expression chosen for the heat of reaction.
Relationship to heat.
In order to discuss the relation between the enthalpy increase and heat supply we return to the first law for closed systems: d"U" = δ"Q" - δ"W". We apply it to the special case that the pressure at the surface is uniform. In this case the work term can be split into two contributions, the so-called pV work, given by "p"d"V" (where here "p" is the pressure at the surface, d"V" is the increase of the volume of the system) and all other types of work δ"W" ', such as by a shaft or by electromagnetic interaction. So we write δ"W" = "p"d"V"+δ"W" '. In this case the first law reads
or
From this relation we see that the increase in enthalpy of a system is equal to the added heat
provided that the system is under constant pressure (d"p" = 0) and that the only work done by the system is expansion work (δ"W" = 0)
Applications.
In thermodynamics, one can calculate enthalpy by determining the requirements for creating a system from "nothingness"; the mechanical work required, "pV", differs based upon the conditions that obtain during the creation of the thermodynamic system.
Energy must be supplied to remove particles from the surroundings to make space for the creation of the system, assuming that the pressure "p" remains constant; this is the "pV" term. The supplied energy must also provide the change in internal energy, "U", which includes activation energies, ionization energies, mixing energies, vaporization energies, chemical bond energies, and so forth. Together, these constitute the change in the enthalpy "U" + "pV". For systems at constant pressure, with no external work done other than the "pV" work, the change in enthalpy is the heat received by the system.
For a simple system, with a constant number of particles, the difference in enthalpy is the maximum amount of thermal energy derivable from a thermodynamic process in which the pressure is held constant.
Heat of reaction.
The total enthalpy of a system cannot be measured directly; the "enthalpy change" of a system is measured instead. Enthalpy change is defined by the following equation:
formula_18formula_19 formula_20 formula_21 formula_22 formula_23
where
formula_24 is the "enthalpy change"
formula_21 is the final enthalpy of the system, expressed in joules. In a chemical reaction,formula_21 is the enthalpy of the products.
formula_23 is the initial enthalpy of the system, expressed in joules. In a chemical reaction,formula_23 is the enthalpy of the reactants.
For an exothermic reaction at constant pressure, the system's change in enthalpy equals the energy released in the reaction, including the energy retained in the system and lost through expansion against its surroundings. In a similar manner, for an endothermic reaction, the system's change in enthalpy is equal to the energy "absorbed" in the reaction, including the energy "lost by" the system and "gained" from compression from its surroundings. A relatively easy way to determine whether or not a reaction is exothermic or endothermic is to determine the sign of Δ"H". If Δ"H" is positive, the reaction is endothermic, that is heat is absorbed by the system due to the products of the reaction having a greater enthalpy than the reactants. On the other hand if Δ"H" is negative, the reaction is exothermic, that is the overall decrease in enthalpy is achieved by the generation of heat.
Specific enthalpy.
The specific enthalpy of a uniform system is defined as "h" = "H"/"m" where "m" is the mass of the system. The SI unit for specific enthalpy is joule per kilogram. It can be expressed in other specific quantities by "h" = "u" + "pv", where "u" is the specific internal energy, "p" is the pressure, and "v" is specific volume, which is equal to 1/"ρ", where "ρ" is the density.
Enthalpy changes.
An enthalpy change describes the change in enthalpy observed in the constituents of a thermodynamic system when undergoing a transformation or chemical reaction. It is the difference between the enthalpy after the process has completed, i.e. the enthalpy of the products, and the initial enthalpy of the system, i.e. the reactants. These processes are reversible and the enthalpy for the reverse process is the negative value of the forward change.
A common standard enthalpy change is the enthalpy of formation, which has been determined for a large number of substances. Enthalpy changes are routinely measured and compiled in chemical and physical reference works, such as the CRC Handbook of Chemistry and Physics. The following is a selection of enthalpy changes commonly recognized in thermodynamics.
When used in these recognized terms the qualifier "change" is usually dropped and the property is simply termed "enthalpy of 'process"'. Since these properties are often used as reference values it is very common to quote them for a standardized set of environmental parameters, or standard conditions, including: 
For such standardized values the name of the enthalpy is commonly prefixed with the term "standard", e.g. "standard enthalpy of formation".
Chemical properties:
Physical properties:
Open systems.
In thermodynamic open systems, matter may flow in and out of the system boundaries. The first law of thermodynamics for open systems states: The increase in the internal energy of a system is equal to the amount of energy added to the system by matter flowing in and by heating, minus the amount lost by matter flowing out and in the form of work done by the system. The first law for open systems is given by:
where formula_30 is the average internal energy entering the system and formula_31 is the average internal energy leaving the system.
The region of space enclosed by open system boundaries is usually called a control volume, and it may or may not correspond to physical walls. If we choose the shape of the control volume such that all flow in or out occurs perpendicular to its surface, then the flow of matter into the system performs work as if it were a piston of fluid pushing mass into the system, and the system performs work on the flow of matter out as if it were driving a piston of fluid. There are then two types of work performed: "flow work" described above, which is performed on the fluid (this is also often called "pV work"), and "shaft work", which may be performed on some mechanical device.
These two types of work are expressed in the equation:
Substitution into the equation above for the control volume cv yields:
The definition of enthalpy, formula_19, permits us to use this thermodynamic potential to account for both internal energy and formula_35 work in fluids for open systems:
This expression is described by Fig.1. If we allow also the system boundary to move (e.g. due to moving pistons) we get a rather general form of the first law for open systems. In terms of time derivatives it reads
where formula_38 represent algebraic sums and the indices "k" refer to the various places where heat is supplied, matter flows into the system, and boundaries are moving. The formula_39 terms represent enthalpy flows, which can be written as
with formula_41 the mass flow and formula_42 the molar flow at position "k" respectively. The term d"V"k/d"t" represents the rate of change of the system volume at position "k" that results in pV power done by the system. The parameter "P" represents all other forms of power done by the system such as shaft power, but it can also be e.g. electric power produced by an electrical power plant. Note that the previous expression holds true only if the kinetic energy flow rate is conserved between system inlet and outlet. Otherwise, it has to be included in the enthalpy balance. During steady-state operation of a device ("see turbine, pump, and engine"), the average d"U"/d"t" may be set equal to zero. This yields a useful expression for the average power generation for these devices in the absence of chemical reactions
where the angle brackets denote time averages. The technical importance of the enthalpy is directly related to its presence in the first law for open systems, as formulated above.
Diagrams.
Nowadays the enthalpy values of important substances can be obtained via commercial software. Practically all relevant material properties can be obtained either in tabular or in graphical form. There are many types of diagrams, such as hT diagrams, which give the specific enthalpy as function of temperature for various pressures and hp diagrams, which give "h" as function of "p" for various "T". One of the most common diagrams is the temperature-entropy diagram (Ts-diagram). An example is "Fig.2", which is the Ts-diagram of nitrogen. It gives the melting curve and saturated liquid and vapor values together with isobars and isenthalps. These diagrams are powerful tools in the hands of the thermal engineer.
Some basic applications.
The points a through h in Fig.2 play a role in the discussion in this Section.
Throttling.
One of the simple applications of the concept of enthalpy is the so-called throttling process, also known as Joule-Thomson expansion. It concerns a steady adiabatic flow of a fluid through a flow resistance (valve, porous plug, or any other type of flow resistance) as shown in Fig.3a. This process is very important since it is at the heart of domestic refrigerators where it is responsible for the temperature drop between ambient temperature and the interior of the fridge. It is also the final stage in many types of liquefiers.
In the first law for open systems (see above), applied to the system in Fig.3a, all terms are zero except the terms for the enthalpy flow. Hence
Since the mass flow is constant the specific enthalpies at the two sides of the flow resistance are the same
that is, the enthalpy per unit mass does not change during the throttling. The consequences of this relation can be demonstrated using Fig.2. Point c in Fig.2 is at 200 bar and room temperature (300 K). A Joule-Thomson expansion from 200 bar to 1 bar follows a curve of constant enthalpy of roughly 425 kJ/kg (not shown in Fig.2) lying between the 400 and 450 kJ/kg isenthalps and ends in point d, which is at a temperature of about 270 K. Hence the expansion from 200 bar to 1 bar cools nitrogen from 300 K to 270 K. In the valve there is a lot of friction and a lot of entropy is produced, but still the final temperature is below the starting value!
Point e is chosen so that it is on the saturated liquid line with "h" = 100 kJ/kg. It corresponds roughly with "p" = 13 bar and "T" = 108 K. Throttling from this point to a pressure of one bar ends in the two-phase region (point f). This means that a mixture of gas and liquid leaves the throttling valve. Since the enthalpy is an extensive parameter the enthalpy in f ("h"f) is equal to the enthalpy in g ("h"g) multiplied with the liquid fraction in f ("x"f) plus the enthalpy in h ("h"h) multiplied with the gas fraction in f (1-"x"f). So
With numbers: 100 = "x"f 28 + (1 – "x"f)230 so "x"f = 0.64. This means that the mass fraction of the liquid in the liquid–gas mixture that leaves the throttling valve is 64%.
Compressors.
Fig.3b is a schematic drawing of a compressor. A power "P" is applied e.g. as electrical power. If the compression is adiabatic the gas temperature goes up. In the reversible case it would be at constant entropy, which corresponds with a vertical line in Fig.2. E.g. compressing nitrogen from 1 bar (point a) to 2 bar (point b) would result in a temperature increase from 300 K to 380 K. In order to let the compressed gas exit at ambient temperature "T"a heat exchange, e.g. by cooling water, is necessary. In the ideal case the compression is isothermal. The average heat flow to the surroundings is formula_47. Since the system is in the steady state the first law gives
The minimum power, needed for the compression is realized if the compression is reversible. In that case the second law of thermodynamics for open systems gives
Eliminating formula_47 gives for the minimum power
E.g. compressing 1 kg of nitrogen from 1 bar to 200 bar costs at least ("h"c - "h"a) - "T"a("s"c-"s"a). With the data, obtained with Fig.2, we find a value of (430–461) – 300 (5.16–6.85) = 476 kJ/kg.
The relation for the power can be further simplified by writing it as
With d"h" = "T"d"s" + "v"d"p" this results in the final relation

</doc>
<doc id="9891" url="https://en.wikipedia.org/wiki?curid=9891" title="Entropy">
Entropy

In thermodynamics, entropy (usual symbol "S") is a measure of the number of specific realizations or microstates which may realize a thermodynamic system in a defined state specified by macroscopic observables. Entropy is commonly understood as a measure of disorder. According to the second law of thermodynamics the entropy of an isolated system never decreases; such a system will spontaneously proceed towards thermodynamic equilibrium, the configuration with maximum entropy. Systems that are not isolated may decrease in entropy, provided they increase the entropy of their environment by at least that same amount. Since entropy is a state function, the change in the entropy of a system is the same for any process that goes from a given initial state to a given final state, whether the process is reversible or irreversible. However, irreversible processes increase the combined entropy of the system and its environment.
The change in entropy (Δ"S") of a system was originally defined for a thermodynamically reversible process as
where is the absolute temperature of the system, dividing an incremental reversible transfer of heat into that system (). (If heat is transferred out the sign would be reversed giving a decrease in entropy of the system.) The above definition is sometimes called the macroscopic definition of entropy because it can be used without regard to any microscopic description of the contents of a system. The concept of entropy has been found to be generally useful and has several other formulations. Entropy was discovered when it was noticed to be a quantity that behaves as a function of state, as a consequence of the second law of thermodynamics.
Entropy is an extensive property. It has the dimension of energy divided by temperature, which has a unit of joules per kelvin (J K−1) in the International System of Units (or kg m2 s−2 K−1 in terms of base units). But the entropy of a pure substance is usually given as an intensive property — either entropy per unit mass (SI unit: J K−1 kg−1) or entropy per unit amount of substance (SI unit: J K−1 mol−1).
The "absolute" entropy ("S" rather than Δ"S") was defined later, using either statistical mechanics or the third law of thermodynamics.
In the modern microscopic interpretation of entropy in statistical mechanics, entropy is the amount of additional information needed to specify the exact physical state of a system, given its thermodynamic specification. Understanding the role of thermodynamic entropy in various processes requires an understanding of how and why that information changes as the system evolves from its initial to its final condition. It is often said that entropy is an expression of the disorder, or randomness of a system, or of our lack of information about it. The second law is now often seen as an expression of the fundamental postulate of statistical mechanics through the modern definition of entropy.
History.
The French mathematician Lazare Carnot proposed in his 1803 paper "Fundamental Principles of Equilibrium and Movement" that in any machine the accelerations and shocks of the moving parts represent losses of "moment of activity". In other words, in any natural process there exists an inherent tendency towards the dissipation of useful energy. Building on this work, in 1824 Lazare's son Sadi Carnot published "Reflections on the Motive Power of Fire" which posited that in all heat-engines, whenever "caloric" (what is now known as heat) falls through a temperature difference, work or motive power can be produced from the actions of its fall from a hot to cold body. He made the analogy with that of how water falls in a water wheel. This was an early insight into the second law of thermodynamics. Carnot based his views of heat partially on the early 18th century "Newtonian hypothesis" that both heat and light were types of indestructible forms of matter, which are attracted and repelled by other matter, and partially on the contemporary views of Count Rumford who showed (1789) that heat could be created by friction as when cannon bores are machined. Carnot reasoned that if the body of the working substance, such as a body of steam, is returned to its original state at the end of a complete engine cycle, that "no change occurs in the condition of the working body".
The first law of thermodynamics, deduced from the heat-friction experiments of James Joule in 1843, expresses the concept of energy, and its conservation in all processes; the first law, however, is unable to quantify the effects of friction and dissipation.
In the 1850s and 1860s, German physicist Rudolf Clausius objected to the supposition that no change occurs in the working body, and gave this "change" a mathematical interpretation by questioning the nature of the inherent loss of usable heat when work is done, e.g. heat produced by friction. Clausius described entropy as the "transformation-content", i.e. dissipative energy use, of a thermodynamic system or working body of chemical species during a change of state. This was in contrast to earlier views, based on the theories of Isaac Newton, that heat was an indestructible particle that had mass.
Later, scientists such as Ludwig Boltzmann, Josiah Willard Gibbs, and James Clerk Maxwell gave entropy a statistical basis. In 1877 Boltzmann visualized a probabilistic way to measure the entropy of an ensemble of ideal gas particles, in which he defined entropy to be proportional to the logarithm of the number of microstates such a gas could occupy. Henceforth, the essential problem in statistical thermodynamics, i.e. according to Erwin Schrödinger, has been to determine the distribution of a given amount of energy E over N identical systems.
Carathéodory linked entropy with a mathematical definition of irreversibility, in terms of trajectories and integrability.
Definitions and descriptions.
There are two related definitions of entropy: the thermodynamic definition and the statistical mechanics definition. Historically, the classical thermodynamics definition developed first. In the classical thermodynamics viewpoint, the system is composed of very large numbers of constituents (atoms, molecules) and the state of the system is described by the average thermodynamic properties of those constituents; the details of the system's constituents are not directly considered, but their behavior is described by macroscopically averaged properties, e.g. temperature, pressure, entropy, heat capacity. The early classical definition of the properties of the system assumed equilibrium. The classical thermodynamic definition of entropy has more recently been extended into the area of non-equilibrium thermodynamics. Later, the thermodynamic properties, including entropy, were given an alternative definition in terms of the statistics of the motions of the microscopic constituents of a system — modeled at first classically, e.g. Newtonian particles constituting a gas, and later quantum-mechanically (photons, phonons, spins, etc.). The statistical mechanics description of the behavior of a system is necessary as the definition of the properties of a system using classical thermodynamics become an increasingly unreliable method of predicting the final state of a system that is subject to some process. 
Function of state.
There are many thermodynamic properties that are functions of state. This means that at a particular thermodynamic state (which should not be confused with the microscopic state of a system), these properties have a certain value. Often, if two properties of the system are determined, then the state is determined and the other properties' values can also be determined. For instance, a gas at a particular temperature and pressure has its state fixed by those values, and has a particular volume that is determined by those values. As another instance, a system composed of a pure substance of a single phase at a particular uniform temperature and pressure is determined (and is thus a particular state) and is at not only a particular volume but also at a particular entropy. The fact that entropy is a function of state is one reason it is useful. In the Carnot cycle, the working fluid returns to the same state it had at the start of the cycle, hence the line integral of any state function, such as entropy, over the cycle is zero.
Reversible process.
Entropy is defined for a reversible process and for a system that, at all times, can be treated as being at a uniform state and thus at a uniform temperature. Reversibility is an ideal that some real processes approximate and that is often presented in study exercises. For a reversible process, entropy behaves as a conserved quantity and no change occurs in total entropy. More specifically, total entropy is conserved in a reversible process and not conserved in an irreversible process. One has to be careful about system boundaries. For example, in the Carnot cycle, while the heat flow from the hot reservoir to the cold reservoir represents an increase in entropy, the work output, if reversibly and perfectly stored in some energy storage mechanism, represents a decrease in entropy that could be used to operate the heat engine in reverse and return to the previous state, thus the "total" entropy change is still zero at all times if the entire process is reversible. Any process that does not meet the requirements of a reversible process must be treated as an irreversible process, which is usually a complex task. An irreversible process increases entropy.
Heat "transfer" situations require two or more non-isolated systems in thermal contact. In irreversible heat transfer, heat energy is irreversibly transferred from the higher temperature system to the lower temperature system, and the combined entropy of the systems increases. Each system, by definition, must have its own absolute temperature applicable within all areas in each respective system in order to calculate the entropy transfer. Thus, when a system at higher temperature transfers heat to a system of lower temperature , the former loses entropy and the latter gains entropy . Since , it follows that , whence there is a net gain in the combined entropy.
Carnot cycle.
The concept of entropy arose from Rudolf Clausius's study of the Carnot cycle. In a Carnot cycle, heat formula_2 is absorbed at temperature formula_3 from a 'hot' reservoir (an isothermal process), and given up as heat formula_4 to a 'cold' reservoir (isothermal process) at formula_5. According to Carnot's principle, work can only be produced by the system when there is a temperature difference, and the work should be some function of the difference in temperature and the heat absorbed (formula_2). Carnot did not distinguish between formula_2 and formula_4, since he was using the incorrect hypothesis that caloric theory was valid, and hence heat was conserved (the incorrect assumption that formula_2 and formula_4 were equal) when, in fact, formula_11. Through the efforts of Clausius and Kelvin, it is now known that the maximum work that a system can produce is the product of the Carnot efficiency and the heat of the hot reservoir:
Equation 1:
In order to derive the Carnot efficiency, formula_13 (a number less than one), Kelvin had to evaluate the ratio of the work output to the heat absorbed during the isothermal expansion with the help of the Carnot-Clapeyron equation which contained an unknown function, known as the Carnot function. The possibility that the Carnot function could be the temperature as measured from a zero temperature, was suggested by Joule in a letter to Kelvin. This allowed Kelvin to establish his absolute temperature scale.
It is also known that the work produced by the system is the difference between the heat of the hot reservoir and the heat of the cold reservoir:
Equation 2:
Since the latter is valid over the entire cycle, this gave Clausius the hint that at each stage of the cycle, work and heat would not be equal, but rather their difference would be a state function that would vanish upon completion of the cycle. The state function was called the internal energy and it became the first law of thermodynamics.
Now equating the two expressions gives
or
This implies that there is a function of state which is conserved over a complete cycle of the Carnot cycle. Clausius called this state function "entropy". One can see that entropy was discovered through mathematics rather than through laboratory results. It is a mathematical construct and has no easy physical analogy. This makes the concept somewhat obscure or abstract, akin to how the concept of energy arose.
Clausius then asked what would happen if there should be less work produced by the system than that predicted by Carnot's principle. The right-hand side of the first equation would be the upper bound of the work output by the system, which would now be converted into an inequality
formula_17
When the second equation is used to express the work as a difference in heats, we get
formula_18
or
formula_19
So more heat is given off to the cold reservoir than in the Carnot cycle. If we denote the entropies by formula_20 for the two states, then the above inequality can be written as a decrease in the entropy
or
In other words, the entropy that leaves the system is greater than the entropy that entered the system, implying that some irreversible process prevented the cycle from outputting the maximum amount of work as predicted by the Carnot equation.
The Carnot cycle and efficiency are invaluable as they define the upper bound of the possible work output and the efficiency of any classical thermodynamic system. Other cycles such as the Otto cycle, Diesel cycle, Brayton cycle etcetera can be analyzed from the standpoint of the Carnot cycle. Any machine or process that is claimed to produce an efficiency greater than the Carnot efficiency is not viable as it would violate the second law of thermodynamics. For very small numbers of particles in the system, statistical thermodynamics must be used. The efficiency of devices such as photovoltaic cells require an analysis from the standpoint of quantum mechanics.
Classical thermodynamics.
The thermodynamic definition of entropy was developed in the early 1850s by Rudolf Clausius and essentially describes how to measure the entropy of an isolated system in thermodynamic equilibrium with its parts. Clausius created the term entropy as an extensive thermodynamic variable that was shown to be useful in characterizing the Carnot cycle. Heat transfer along the isotherm steps of the Carnot cycle was found to be proportional to the temperature of a system (known as its absolute temperature). This relationship was expressed in increments of entropy equal to the ratio of incremental heat transfer divided by temperature, which was found to vary in the thermodynamic cycle but eventually return to the same value at the end of every cycle. Thus it was found to be a function of state, specifically a thermodynamic state of the system. Clausius wrote that he "intentionally formed the word Entropy as similar as possible to the word Energy", basing the term on the Greek ἡ τροπή "tropē", "transformation".
While Clausius based his definition on a reversible process, there are also irreversible processes that change entropy. Following the second law of thermodynamics, entropy of an isolated system always increases. The difference between an isolated system and closed system is that heat may "not" flow to and from an isolated system, but heat flow to and from a closed system is possible. Nevertheless, for both closed and isolated systems, and indeed, also in open systems, irreversible thermodynamics processes may occur.
According to the Clausius equality, for a reversible cyclic process:
formula_23
This means the line integral formula_24 is path-independent.
So we can define a state function "S" called entropy, which satisfies
formula_25
To find the entropy difference between any two states of a system, the integral must be evaluated for some reversible path between the initial and final states. Since entropy is a state function, the entropy change of the system for an irreversible path will be the same as for a reversible path between the same two states. However, the entropy change of the surroundings will be different.
We can only obtain the change of entropy by integrating the above formula. To obtain the absolute value of the entropy, we need the third law of thermodynamics, which states that "S" = 0 at absolute zero for perfect crystals.
From a macroscopic perspective, in classical thermodynamics the entropy is interpreted as a state function of a thermodynamic system: that is, a property depending only on the current state of the system, independent of how that state came to be achieved. In any process where the system gives up energy Δ"E", and its entropy falls by Δ"S", a quantity at least "T"R Δ"S" of that energy must be given up to the system's surroundings as unusable heat ("T"R is the temperature of the system's external surroundings). Otherwise the process will not go forward. In classical thermodynamics, the entropy of a system is defined only if it is in thermodynamic equilibrium.
Statistical mechanics.
The statistical definition was developed by Ludwig Boltzmann in the 1870s by analyzing the statistical behavior of the microscopic components of the system. Boltzmann showed that this definition of entropy was equivalent to the thermodynamic entropy to within a constant number which has since been known as Boltzmann's constant. In summary, the thermodynamic definition of entropy provides the experimental definition of entropy, while the statistical definition of entropy extends the concept, providing an explanation and a deeper understanding of its nature.
The interpretation of entropy in statistical mechanics is the measure of uncertainty, or "mixedupness" in the phrase of Gibbs, which remains about a system after its observable macroscopic properties, such as temperature, pressure and volume, have been taken into account. For a given set of macroscopic variables, the entropy measures the degree to which the probability of the system is spread out over different possible microstates. In contrast to the macrostate, which characterizes plainly observable average quantities, a microstate specifies all molecular details about the system including the position and velocity of every molecule. The more such states available to the system with appreciable probability, the greater the entropy. In statistical mechanics, entropy is a measure of the number of ways in which a system may be arranged, often taken to be a measure of "disorder" (the higher the entropy, the higher the disorder). This definition describes the entropy as being proportional to the natural logarithm of the number of possible microscopic configurations of the individual atoms and molecules of the system (microstates) which could give rise to the observed macroscopic state (macrostate) of the system. The constant of proportionality is the Boltzmann constant.
Specifically, entropy is a logarithmic measure of the number of states with significant probability of being occupied:
where "k"B is the Boltzmann constant, equal to .
The summation is over all the possible microstates of the system, and "pi" is the probability that the system is in the "i"-th microstate. This definition assumes that the basis set of states has been picked so that there is no information on their relative phases. In a different basis set, the more general expression is
where formula_28 is the density matrix, formula_29 is trace (linear algebra) and formula_30 is the matrix logarithm.
This density matrix formulation is not needed in cases of thermal equilibrium so long as the basis states are chosen to be energy eigenstates. For most practical purposes, this can be taken as the fundamental definition of entropy since all other formulas for "S" can be mathematically derived from it, but not vice versa.
In what has been called "the fundamental assumption of statistical thermodynamics" or "the fundamental postulate in statistical mechanics", the occupation of any microstate is assumed to be equally probable (i.e. "P""i" = 1/Ω, where Ω is the number of microstates); this assumption is usually justified for an isolated system in equilibrium. Then the previous equation reduces to
In thermodynamics, such a system is one in which the volume, number of molecules, and internal energy are fixed (the microcanonical ensemble).
The most general interpretation of entropy is as a measure of our uncertainty about a system. The equilibrium state of a system maximizes the entropy because we have lost all information about the initial conditions except for the conserved variables; maximizing the entropy maximizes our ignorance about the details of the system. This uncertainty is not of the everyday subjective kind, but rather the uncertainty inherent to the experimental method and interpretative model.
The interpretative model has a central role in determining entropy. The qualifier "for a given set of macroscopic variables" above has deep implications: if two observers use different sets of macroscopic variables, they will observe different entropies. For example, if observer A uses the variables "U", "V" and "W", and observer B uses "U", "V", "W", "X", then, by changing "X", observer B can cause an effect that looks like a violation of the second law of thermodynamics to observer A. In other words: the set of macroscopic variables one chooses must include everything that may change in the experiment, otherwise one might see decreasing entropy!
Entropy can be defined for any Markov processes with reversible dynamics and the detailed balance property.
In Boltzmann's 1896 "Lectures on Gas Theory", he showed that this expression gives a measure of entropy for systems of atoms and molecules in the gas phase, thus providing a measure for the entropy of classical thermodynamics.
Entropy of a system.
Entropy is the above-mentioned unexpected and, to some, obscure integral that arises directly from the Carnot cycle. It is reversible heat divided by temperature. It is, remarkably, a function of state and it is fundamental and very useful.
In a thermodynamic system, pressure, density, and temperature tend to become uniform over time because this equilibrium state has higher probability (more possible combinations of microstates) than any other; see statistical mechanics. As an example, for a glass of ice water in air at room temperature, the difference in temperature between a warm room (the surroundings) and cold glass of ice and water (the system and not part of the room), begins to be equalized as portions of the thermal energy from the warm surroundings spread to the cooler system of ice and water. Over time the temperature of the glass and its contents and the temperature of the room become equal. The entropy of the room has decreased as some of its energy has been dispersed to the ice and water. However, as calculated in the example, the entropy of the system of ice and water has increased more than the entropy of the surrounding room has decreased. In an isolated system such as the room and ice water taken together, the dispersal of energy from warmer to cooler always results in a net increase in entropy. Thus, when the "universe" of the room and ice water system has reached a temperature equilibrium, the entropy change from the initial state is at a maximum. The entropy of the thermodynamic system is a measure of how far the equalization has progressed.
Thermodynamic entropy is a non-conserved state function that is of great importance in the sciences of physics and chemistry. Historically, the concept of entropy evolved in order to explain why some processes (permitted by conservation laws) occur spontaneously while their time reversals (also permitted by conservation laws) do not; systems tend to progress in the direction of increasing entropy. For isolated systems, entropy never decreases. This fact has several important consequences in science: first, it prohibits "perpetual motion" machines; and second, it implies the arrow of entropy has the same direction as the arrow of time. Increases in entropy correspond to irreversible changes in a system, because some energy is expended as waste heat, limiting the amount of work a system can do.
Unlike many other functions of state, entropy cannot be directly observed but must be calculated. Entropy can be calculated for a substance as the standard molar entropy from absolute zero (also known as absolute entropy) or as a difference in entropy from some other reference state which is defined as zero entropy. Entropy has the dimension of energy divided by temperature, which has a unit of joules per kelvin (J/K) in the International System of Units. While these are the same units as heat capacity, the two concepts are distinct. Entropy is not a conserved quantity: for example, in an isolated system with non-uniform temperature, heat might irreversibly flow and the temperature become more uniform such that entropy increases. The second law of thermodynamics, states that a closed system has entropy which may increase or otherwise remain constant. Chemical reactions cause changes in entropy and entropy plays an important role in determining in which direction a chemical reaction spontaneously proceeds.
One dictionary definition of entropy is that it is "a measure of thermal energy per unit temperature that is not available for useful work". For instance, a substance at uniform temperature is at maximum entropy and cannot drive a heat engine. A substance at non-uniform temperature is at a lower entropy (than if the heat distribution is allowed to even out) and some of the thermal energy can drive a heat engine.
A special case of entropy increase, the entropy of mixing, occurs when two or more different substances are mixed. If the substances are at the same temperature and pressure, there will be no net exchange of heat or work – the entropy change will be entirely due to the mixing of the different substances. At a statistical mechanical level, this results due to the change in available volume per particle with mixing.
Second law of thermodynamics.
The second law of thermodynamics requires that, in general, the total entropy of any system will not decrease other than by increasing the entropy of some other system. Hence, in a system isolated from its environment, the entropy of that system will tend not to decrease. It follows that heat will not flow from a colder body to a hotter body without the application of work (the imposition of order) to the colder body. Secondly, it is impossible for any device operating on a cycle to produce net work from a single temperature reservoir; the production of net work requires flow of heat from a hotter reservoir to a colder reservoir, or a single expanding reservoir undergoing adiabatic cooling, which performs adiabatic work. As a result, there is no possibility of a perpetual motion system. It follows that a reduction in the increase of entropy in a specified process, such as a chemical reaction, means that it is energetically more efficient.
It follows from the second law of thermodynamics that the entropy of a system that is not isolated may decrease. An air conditioner, for example, may cool the air in a room, thus reducing the entropy of the air of that system. The heat expelled from the room (the system), which the air conditioner transports and discharges to the outside air, will always make a bigger contribution to the entropy of the environment than will the decrease of the entropy of the air of that system. Thus, the total of entropy of the room plus the entropy of the environment increases, in agreement with the second law of thermodynamics.
In mechanics, the second law in conjunction with the fundamental thermodynamic relation places limits on a system's ability to do useful work. The entropy change of a system at temperature "T" absorbing an infinitesimal amount of heat δ"q"
in a reversible way, is given by "δq/T". More explicitly, an energy "TR S" is not available to do useful work, where "TR" is the temperature of the coldest accessible reservoir or heat sink external to the system. For further discussion, see "Exergy".
Statistical mechanics demonstrates that entropy is governed by probability, thus allowing for a decrease in disorder even in an isolated system. Although this is possible, such an event has a small probability of occurring, making it unlikely.
Applications.
The fundamental thermodynamic relation.
The entropy of a system depends on its internal energy and the external parameters, such as the volume. In the thermodynamic limit this fact leads to an equation relating the change in the internal energy to changes in the entropy and the external parameters. This relation is known as the fundamental thermodynamic relation. If the volume is the only external parameter, this relation is:
Since the internal energy is fixed when one specifies the entropy and the volume, this relation is valid even if the change from one state of thermal equilibrium to another with infinitesimally larger entropy and volume happens in a non-quasistatic way (so during this change the system may be very far out of thermal equilibrium and then the entropy, pressure and temperature may not exist).
The fundamental thermodynamic relation implies many thermodynamic identities that are valid in general, independent of the microscopic details of the system. Important examples are the Maxwell relations and the relations between heat capacities.
Entropy in chemical thermodynamics.
Thermodynamic entropy is central in chemical thermodynamics, enabling changes to be quantified and the outcome of reactions predicted. The second law of thermodynamics states that entropy in an isolated system – the combination of a subsystem under study and its surroundings – increases during all spontaneous chemical and physical processes. The Clausius equation of δ"q"rev/"T" = Δ"S" introduces the measurement of entropy change, Δ"S". Entropy change describes the direction and quantifies the magnitude of simple changes such as heat transfer between systems – always from hotter to cooler spontaneously.
The thermodynamic entropy therefore has the dimension of energy divided by temperature, and the unit joule per kelvin (J/K) in the International System of Units (SI).
Thermodynamic entropy is an extensive property, meaning that it scales with the size or extent of a system. In many processes it is useful to specify the entropy as an intensive property independent of the size, as a specific entropy characteristic of the type of system studied. Specific entropy may be expressed relative to a unit of mass, typically the kilogram (unit: ). Alternatively, in chemistry, it is also referred to one mole of substance, in which case it is called the "molar entropy" with a unit of .
Thus, when one mole of substance at about is warmed by its surroundings to , the sum of the incremental values of "q"rev/"T" constitute each element's or compound's standard molar entropy, an indicator of the amount of energy stored by a substance at . Entropy change also measures the mixing of substances as a summation of their relative quantities in the final mixture.
Entropy is equally essential in predicting the extent and direction of complex chemical reactions. For such applications, Δ"S" must be incorporated in an expression that includes both the system and its surroundings, Δ"S"universe = Δ"S"surroundings + Δ"S" system. This expression becomes, via some steps, the Gibbs free energy equation for reactants and products in the system: Δ"G" [the Gibbs free energy change of the system] = Δ"H" [the enthalpy change] −"T" Δ"S" [the entropy change].
Entropy balance equation for open systems.
In chemical engineering, the principles of thermodynamics are commonly applied to "open systems", i.e. those in which heat, work, and mass flow across the system boundary. Flows of both heat (formula_33) and work, i.e. formula_34 (shaft work) and "P(dV/dt)" (pressure-volume work), across the system boundaries, in general cause changes in the entropy of the system. Transfer as heat entails entropy transfer formula_35 where "T" is the absolute thermodynamic temperature of the system at the point of the heat flow. If there are mass flows across the system boundaries, they will also influence the total entropy of the system. This account, in terms of heat and work, is valid only for cases in which the work and heat transfers are by paths physically distinct from the paths of entry and exit of matter from the system.
To derive a generalized entropy balanced equation, we start with the general balance equation for the change in any extensive quantity Θ in a thermodynamic system, a quantity that may be either conserved, such as energy, or non-conserved, such as entropy. The basic generic balance expression states that dΘ/dt, i.e. the rate of change of Θ in the system, equals the rate at which Θ enters the system at the boundaries, minus the rate at which Θ leaves the system across the system boundaries, plus the rate at which Θ is generated within the system. For an open thermodynamic system in which heat and work are transferred by paths separate from the paths for transfer of matter, using this generic balance equation, with respect to the rate of change with time of the extensive quantity entropy "S", the entropy balance equation is:
where
Note, also, that if there are multiple heat flows, the term formula_41 will be replaced by formula_42 where formula_43 is the heat flow and formula_44 is the temperature at the "jth" heat flow port into the system.
Entropy change formulas for simple processes.
For certain simple transformations in systems of constant composition, the entropy changes are given by simple formulas.
Isothermal expansion or compression of an ideal gas.
For the expansion (or compression) of an ideal gas from an initial volume formula_45 and pressure formula_46 to a final volume formula_47 and pressure formula_48 at any constant temperature, the change in entropy is given by:
Here formula_50 is the number of moles of gas and formula_51 is the ideal gas constant. These equations also apply for expansion into a finite vacuum or a throttling process, where the temperature, internal energy and enthalpy for an ideal gas remain constant.
Cooling and heating.
For heating or cooling of any system (gas, liquid or solid) at constant pressure from an initial temperature formula_52 to a final temperature formula_53, the entropy change is
provided that the constant-pressure molar heat capacity (or specific heat) CP is constant and that no phase transition occurs in this temperature interval.
Similarly at constant volume, the entropy change is
where the constant-volume heat capacity Cv is constant and there is no phase change.
At low temperatures near absolute zero, heat capacities of solids quickly drop off to near zero, so the assumption of constant heat capacity does not apply.
Since entropy is a state function, the entropy change of any process in which temperature and volume both vary is the same as for a path divided into two steps - heating at constant volume and expansion at constant temperature. For an ideal gas, the total entropy change is
Similarly if the temperature and pressure of an ideal gas both vary,
Phase transitions.
Reversible phase transitions occur at constant temperature and pressure. The reversible heat is the enthalpy change for the transition, and the entropy change is the enthalpy change divided by the thermodynamic temperature. For fusion (melting) of a solid to a liquid at the melting point "T"m, the entropy of fusion is
Similarly, for vaporization of a liquid to a gas at the boiling point "T"b, the entropy of vaporization is
Approaches to understanding entropy.
As a fundamental aspect of thermodynamics and physics, several different approaches to entropy beyond that of Clausius and Boltzmann are valid.
Standard textbook definitions.
The following is a list of additional definitions of entropy from a collection of textbooks:
In Boltzmann's definition, entropy is a measure of the number of possible microscopic states (or microstates) of a system in thermodynamic equilibrium. Consistent with the Boltzmann definition, the second law of thermodynamics needs to be re-worded as such that entropy increases over time, though the underlying principle remains the same.
Order and disorder.
Entropy has often been loosely associated with the amount of order or disorder, or of chaos, in a thermodynamic system. The traditional qualitative description of entropy is that it refers to changes in the status quo of the system and is a measure of "molecular disorder" and the amount of wasted energy in a dynamical energy transformation from one state or form to another. In this direction, several recent authors have derived exact entropy formulas to account for and measure disorder and order in atomic and molecular assemblies. One of the simpler entropy order/disorder formulas is that derived in 1984 by thermodynamic physicist Peter Landsberg, based on a combination of thermodynamics and information theory arguments. He argues that when constraints operate on a system, such that it is prevented from entering one or more of its possible or permitted states, as contrasted with its forbidden states, the measure of the total amount of "disorder" in the system is given by:
Similarly, the total amount of "order" in the system is given by:
In which "CD" is the "disorder" capacity of the system, which is the entropy of the parts contained in the permitted ensemble, "CI" is the "information" capacity of the system, an expression similar to Shannon's channel capacity, and "CO" is the "order" capacity of the system.
Energy dispersal.
The concept of entropy can be described qualitatively as a measure of energy dispersal at a specific temperature. Similar terms have been in use from early in the history of classical thermodynamics, and with the development of statistical thermodynamics and quantum theory, entropy changes have been described in terms of the mixing or "spreading" of the total energy of each constituent of a system over its particular quantized energy levels.
Ambiguities in the terms "disorder" and "chaos", which usually have meanings directly opposed to equilibrium, contribute to widespread confusion and hamper comprehension of entropy for most students. As the second law of thermodynamics shows, in an isolated system internal portions at different temperatures will tend to adjust to a single uniform temperature and thus produce equilibrium. A recently developed educational approach avoids ambiguous terms and describes such spreading out of energy as dispersal, which leads to loss of the differentials required for work even though the total energy remains constant in accordance with the first law of thermodynamics (compare discussion in next section). Physical chemist Peter Atkins, for example, who previously wrote of dispersal leading to a disordered state, now writes that "spontaneous changes are always accompanied by a dispersal of energy".
Relating entropy to energy "usefulness".
Following on from the above, it is possible (in a thermal context) to regard entropy as an indicator or measure of the "effectiveness" or "usefulness" of a particular quantity of energy. This is because energy supplied at a high temperature (i.e. with low entropy) tends to be more useful than the same amount of energy available at room temperature. Mixing a hot parcel of a fluid with a cold one produces a parcel of intermediate temperature, in which the overall increase in entropy represents a "loss" which can never be replaced.
Thus, the fact that the entropy of the universe is steadily increasing, means that its total energy is becoming less useful: eventually, this will lead to the "heat death of the Universe".
Entropy and adiabatic accessibility.
A definition of entropy based entirely on the relation of adiabatic accessibility between equilibrium states was given by E.H.Lieb and J. Yngvason in 1999. This approach has several predecessors, including the pioneering work of Constantin Carathéodory from 1909 and the monograph by R. Giles from 1964. In the setting of Lieb and Yngvason one starts by picking, for a unit amount of the substance under consideration, two reference states formula_62 and formula_63 such that the latter is adiabatically accessible from the former but not vice versa. Defining the entropies of the reference states to be 0 and 1 respectively the entropy of a state formula_64 is defined as the largest number formula_65 such that formula_64 is adiabatically accessible from a composite state consisting of an amount formula_65 in the state formula_63 and a complementary amount, formula_69, in the state formula_62. A simple but important result within this setting is that entropy is uniquely determined, apart from a choice of unit and an additive constant for each chemical element, by the following properties: It is monotonic with respect to the relation of adiabatic accessibility, additive on composite systems, and extensive under scaling.
Entropy in quantum mechanics.
In quantum statistical mechanics, the concept of entropy was developed by John von Neumann and is generally referred to as "von Neumann entropy",
where ρ is the density matrix and Tr is the trace operator.
This upholds the correspondence principle, because in the classical limit, when the phases between the basis states used for the classical probabilities are purely random, this expression is equivalent to the familiar classical definition of entropy,
i.e. in such a basis the density matrix is diagonal.
Von Neumann established a rigorous mathematical framework for quantum mechanics with his work "Mathematische Grundlagen der Quantenmechanik". He provided in this work a theory of measurement, where the usual notion of wave function collapse is described as an irreversible process (the so-called von Neumann or projective measurement). Using this concept, in conjunction with the density matrix he extended the classical concept of entropy into the quantum domain.
Information theory.
When viewed in terms of information theory, the entropy state function is simply the amount of information (in the Shannon sense) that would be needed to specify the full microstate of the system. This is left unspecified by the macroscopic description.
In information theory, "entropy" is the measure of the amount of information that is missing before reception and is sometimes referred to as "Shannon entropy". Shannon entropy is a broad and general concept which finds applications in information theory as well as thermodynamics. It was originally devised by Claude Shannon in 1948 to study the amount of information in a transmitted message. The definition of the information entropy is, however, quite general, and is expressed in terms of a discrete set of probabilities "pi so that
In the case of transmitted messages, these probabilities were the probabilities that a particular message was actually transmitted, and the entropy of the message system was a measure of the average amount of information in a message. For the case of equal probabilities (i.e. each message is equally probable), the Shannon entropy (in bits) is just the number of yes/no questions needed to determine the content of the message.
The question of the link between information entropy and thermodynamic entropy is a debated topic. While most authors argue that there is a link between the two, a few argue that they have nothing to do with each other.
The expressions for the two entropies are similar. If "W" is the number of microstates that can yield a given macrostate, and each microstate has the same "A priori" probability, then that probability is "p=1/W". The Shannon entropy (in nats) will be:
and if entropy is measured in units of "k" per nat, then the entropy is given by:
which is the famous Boltzmann entropy formula when "k" is Boltzmann's constant, which may be interpreted as the thermodynamic entropy per nat. There are many ways of demonstrating the equivalence of "information entropy" and "physics entropy", that is, the equivalence of "Shannon entropy" and "Boltzmann entropy". Nevertheless, some authors argue for dropping the word entropy for the "H" function of information theory and using Shannon's other term "uncertainty" instead.
Interdisciplinary applications of entropy.
Although the concept of entropy was originally a thermodynamic construct, it has been adapted in other fields of study, including information theory, psychodynamics, thermoeconomics/ecological economics, and evolution.
For instance, an entropic argument has been recently proposed for explaining the preference of cave spiders in choosing a suitable area for laying their eggs. 
The arrow of time.
Entropy is the only quantity in the physical sciences that seems to imply a particular direction of progress, sometimes called an arrow of time. As time progresses, the second law of thermodynamics states that the entropy of an isolated system never decreases. Hence, from this perspective, entropy measurement is thought of as a kind of clock.
Cosmology.
Since a finite universe is an isolated system, the Second Law of Thermodynamics states that its total entropy is constantly increasing. It has been speculated, since the 19th century, that the universe is fated to a heat death in which all the energy ends up as a homogeneous distribution of thermal energy, so that no more work can be extracted from any source.
If the universe can be considered to have generally increasing entropy, then – as Sir Roger Penrose has pointed out – gravity plays an important role in the increase because gravity causes dispersed matter to accumulate into stars, which collapse eventually into black holes. The entropy of a black hole is proportional to the surface area of the black hole's event horizon. Jacob Bekenstein and Stephen Hawking have shown that black holes have the maximum possible entropy of any object of equal size. This makes them likely end points of all entropy-increasing processes, if they are totally effective matter and energy traps. However, the escape of energy from black holes might be possible due to quantum activity, see Hawking radiation. Hawking has recently changed his stance on some details, in a paper which largely redefined the event horizons of black holes.
The role of entropy in cosmology remains a controversial subject since the time of Ludwig Boltzmann. Recent work has cast some doubt on the heat death hypothesis and the applicability of any simple thermodynamic model to the universe in general. Although entropy does increase in the model of an expanding universe, the maximum possible entropy rises much more rapidly, moving the universe further from the heat death with time, not closer. This results in an "entropy gap" pushing the system further away from the posited heat death equilibrium. Other complicating factors, such as the energy density of the vacuum and macroscopic quantum effects, are difficult to reconcile with thermodynamical models, making any predictions of large-scale thermodynamics extremely difficult.
The entropy gap is widely believed to have been originally opened up by the early rapid exponential expansion of the universe.

</doc>
<doc id="10296" url="https://en.wikipedia.org/wiki?curid=10296" title="EPR paradox">
EPR paradox

The EPR paradox
of 1935 is an influential thought experiment in quantum mechanics with which Albert Einstein and his colleagues Boris Podolsky and Nathan Rosen ("EPR")
claimed to demonstrate that the wave function does not provide a complete description of physical reality, and hence that the Copenhagen interpretation is unsatisfactory;
resolutions of the paradox have important implications for the interpretation of quantum mechanics.
The essence of the paradox is that particles can interact in such a way that it is possible to measure both their position and their momentum more accurately than Heisenberg's uncertainty principle allows,
unless measuring one particle instantaneously affects the other to prevent it, which would involve information being transmitted faster than light as forbidden by the theory of relativity ("spooky action at a distance").
This consequence had not previously been noticed and seemed unreasonable at the time; the phenomenon involved is now known as quantum entanglement.
While EPR felt that the paradox showed that quantum theory was incomplete and should be extended with hidden variables,
the usual modern resolution is to say that due to the common preparation of the two particles (for example the creation of an electron-positron pair from a photon) the property we want to measure has a well defined meaning only when analyzed for the whole system while the same property for the parts individually remains undefined. Therefore if similar measurements are being performed on the two entangled subsystems, there will always be a correlation between the outcomes resulting in a well defined global outcome i.e. for both subsystems together. However, the outcomes for each subsystem separately at each repetition of the experiment will not be well defined or predictable. This correlation does not imply any action of the measurement of one particle on the measurement of the other, therefore it doesn't imply any form of action at a distance. This modern resolution eliminates the need for hidden variables, action at a distance or other structures introduced over time in order to explain the phenomenon. 
A preference for the latter resolution is supported by experiments suggested by Bell's theorem of 1964, which exclude some classes of hidden variable theory.
According to quantum mechanics, under some conditions, a pair of quantum systems may be described by a single wave function, which encodes the probabilities of the outcomes of experiments that may be performed on the two systems, whether jointly or individually. At the time the EPR article discussed below was written, it was known from experiments that the outcome of an experiment sometimes cannot be uniquely predicted. An example of such indeterminacy can be seen when a beam of light is incident on a half-silvered mirror. One half of the beam will reflect, and the other will pass. If the intensity of the beam is reduced until only one photon is in transit at any time, whether that photon will reflect or transmit cannot be predicted quantum mechanically.
The routine explanation of this effect was, at that time, provided by Heisenberg's uncertainty principle. Physical quantities come in pairs called conjugate quantities. Examples of such conjugate pairs are position and momentum of a particle and components of spin measured around different axes. When one quantity was measured, and became determined, the conjugated quantity became indeterminate. Heisenberg explained this as a disturbance caused by measurement.
The EPR paper, written in 1935, was intended to illustrate that this explanation is inadequate. It considered two entangled particles, referred to as A and B, and pointed out that measuring a quantity of a particle A will cause the conjugated quantity of particle B to become undetermined, even if there was no contact, no classical disturbance. The basic idea was that the quantum states of two particles in a system cannot always be decomposed from the joint state of the two. An example (in bra–ket notation) is: formula_1
Heisenberg's principle was an attempt to provide a classical explanation of a quantum effect sometimes called non-locality. According to EPR there were two possible explanations. Either there was some interaction between the particles (even though they were separated) or the information about the outcome of all possible measurements was already present in both particles.
The EPR authors preferred the second explanation according to which that information was encoded in some 'hidden parameters'. The first explanation of an effect propagating instantly across a distance is in conflict with the theory of relativity. They then concluded that quantum mechanics was incomplete since its formalism does not permit hidden parameters.
Violations of the conclusions of Bell's theorem are generally understood to have demonstrated that the hypotheses of Bell's theorem, also assumed by Einstein, Podolsky and Rosen, do not apply in our world. Most physicists who have examined the issue concur that experiments, such as those of Alain Aspect and his group, have confirmed that physical probabilities, as predicted by quantum theory, do exhibit the phenomena of Bell-inequality violations that are considered to invalidate EPR's preferred "local hidden-variables" type of explanation for the correlations to which EPR first drew attention.
History of EPR developments.
The article that first brought forth these matters, "Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?" was published in 1935. The paper prompted a response by Bohr, which he published in the same journal, in the same year, using the same title. There followed a debate between Bohr and Einstein about the fundamental nature of reality. Einstein had been skeptical of the Heisenberg uncertainty principle and the role of chance in quantum theory. But the crux of this debate was not about chance, but something even deeper: Is there one objective physical reality, which every observer sees from his own vantage? (Einstein's view) Or does the observer co-create physical reality by the questions he poses with experiments? (Bohr's view)
Einstein struggled to the end of his life for a theory that could better comply with his idea of causality, protesting against the view that there exists no objective physical reality other than that which is revealed through measurement interpreted in terms of quantum mechanical formalism. However, since Einstein's death, experiments analogous to the one described in the EPR paper have been carried out, starting in 1976 by French scientists Lamehi-Rachti and Mittig at the Saclay Nuclear Research Centre. These experiments appear to show that the local realism idea is false, vindicating Bohr.
Quantum mechanics and its interpretation.
Since the early twentieth century, quantum theory has proved to be successful in describing accurately the physical reality of the mesoscopic and microscopic world, in multiple reproducible physics experiments.
Quantum mechanics was developed with the aim of describing atoms and explaining the observed spectral lines in a measurement apparatus. Although disputed especially in the early twentieth century, it has yet to be seriously challenged. Philosophical interpretations of quantum phenomena, however, are another matter: the question of how to "interpret" the mathematical formulation of quantum mechanics has given rise to a variety of different answers from people of different philosophical persuasions (see Interpretations of quantum mechanics).
Quantum theory and quantum mechanics do not provide single measurement outcomes in a deterministic way. According to the understanding of quantum mechanics known as the Copenhagen interpretation, measurement causes an instantaneous collapse of the wave function describing the quantum system into an eigenstate of the observable that was measured. Einstein characterized this imagined collapse in the 1927 Solvay Conference. He presented a thought experiment in which electrons are introduced through a small hole in a sphere whose inner surface serves as a detection screen. The electrons will contact the spherical detection screen in a widely dispersed manner. Those electrons, however, are all individually described by wave fronts that expand in all directions from the point of entry. A wave as it is understood in everyday life would paint a large area of the detection screen, but the electrons would be found to impact the screen at single points and would eventually form a pattern in keeping with the probabilities described by their identical wave functions. Einstein asks what makes each electron's wave front "collapse" at its respective location. Why do the electrons appear as single bright scintillations rather than as dim washes of energy across the surface? Why does any single electron appear at one point rather than some alternative point? The behavior of the electrons gives the impression of some signal having been sent to all possible points of contact that would have nullified all but one of them, or, in other words, would have preferentially selected a single point to the exclusion of all others.
Einstein's opposition.
Einstein was the most prominent opponent of the Copenhagen interpretation. In his view, quantum mechanics is incomplete. Commenting on this, other writers (such as John von Neumann and David Bohm) hypothesized that consequently there would have to be 'hidden' variables responsible for random measurement results, something which was not expressly claimed in the original paper.
The 1935 EPR paper condensed the philosophical discussion into a physical argument. The authors claim that given a specific experiment, in which the outcome of a measurement is known before the measurement takes place, there must exist something in the real world, an "element of reality", that determines the measurement outcome. They postulate that these elements of reality are local, in the sense that each belongs to a certain point in spacetime. Each element may only be influenced by events which are located in the backward light cone of its point in spacetime (i.e., the past). These claims are founded on assumptions about nature that constitute what is now known as local realism.
Though the EPR paper has often been taken as an exact expression of Einstein's views, it was primarily authored by Podolsky, based on discussions at the Institute for Advanced Study with Einstein and Rosen. Einstein later expressed to Erwin Schrödinger that, "it did not come out as well as I had originally wanted; rather, the essential thing was, so to speak, smothered by the formalism." In 1936, Einstein presented an individual account of his local realist ideas.
Description of the paradox.
The original EPR paradox challenges the prediction of quantum mechanics that it is impossible to know both the position and the momentum of a quantum particle. This challenge can be extended to other pairs of physical properties.
EPR paper.
The original paper purports to describe what must happen to "two systems I and II, which we permit to interact ...", and, after some time, "we suppose that there is no longer any interaction between the two parts." As explained by Manjit Kumar (2009), the EPR description involves "two particles, A and B, [which] interact briefly and then move off in opposite directions." According to Heisenberg's uncertainty principle, it is impossible to measure both the momentum and the position of particle B exactly. However, according to Kumar, it is possible to measure the exact position of particle A. By calculation, therefore, with the exact position of particle A known, the exact position of particle B can be known. Also, the exact momentum of particle A can be measured, so the exact momentum of particle B can be worked out. Kumar writes: "EPR argued that they had proved that ... [particle] B can have simultaneously exact values of position and momentum. ... Particle B has a position that is real and a momentum that is real." EPR appeared to have contrived a means to establish the exact values of "either" the momentum "or" the position of B due to measurements made on particle A, without the slightest possibility of particle B being physically disturbed.
EPR tried to set up a paradox to question the range of true application of Quantum Mechanics: Quantum theory predicts that both values cannot be known for a particle, and yet the EPR thought experiment purports to show that they must all have determinate values. The EPR paper says: "We are thus forced to conclude that the quantum-mechanical description of physical reality given by wave functions is not complete."
The EPR paper ends by saying: While we have thus shown that the wave function does not provide a complete description of the physical reality, we left open the question of whether or not such a description exists. We believe, however, that such a theory is possible.
Measurements on an entangled state.
We have a source that emits electron–positron pairs, with the electron sent to destination "A", where there is an observer named Alice, and the positron sent to destination "B", where there is an observer named Bob. According to quantum mechanics, we can arrange our source so that each emitted pair occupies a quantum state called a spin singlet. The particles are thus said to be entangled. This can be viewed as a quantum superposition of two states, which we call state I and state II. In state I, the electron has spin pointing upward along the "z"-axis ("+z") and the positron has spin pointing downward along the "z"-axis (−"z"). In state II, the electron has spin −"z" and the positron has spin +"z". Therefore, it is impossible (without measuring) to know the definite state of spin of either particle in the spin singlet.
Alice now measures the spin along the "z"-axis. She can obtain one of two possible outcomes: +"z" or −"z". Suppose she gets +"z". According to the Copenhagen interpretation of quantum mechanics, the quantum state of the system collapses into state I. The quantum state determines the probable outcomes of any measurement performed on the system. In this case, if Bob subsequently measures spin along the "z"-axis, there is 100% probability that he will obtain −"z". Similarly, if Alice gets −"z", Bob will get +"z".
There is, of course, nothing special about choosing the "z"-axis: according to quantum mechanics the spin singlet state may equally well be expressed as a superposition of spin states pointing in the "x" direction. Suppose that Alice and Bob had decided to measure spin along the "x"-axis. We'll call these states Ia and IIa. In state Ia, Alice's electron has spin +"x" and Bob's positron has spin −"x". In state IIa, Alice's electron has spin −"x" and Bob's positron has spin +"x". Therefore, if Alice measures +"x", the system 'collapses' into state Ia, and Bob will get −"x". If Alice measures −"x", the system collapses into state IIa, and Bob will get +"x".
Whatever axis their spins are measured along, they are always found to be opposite. This can only be explained if the particles are linked in some way. Either they were created with a definite (opposite) spin about every axis—a "hidden variable" argument—or they are linked so that one electron "feels" which axis the other is having its spin measured along, and becomes its opposite about that one axis—an "entanglement" argument. Moreover, if the two particles have their spins measured about "different" axes, once the electron's spin has been measured about the "x"-axis (and the positron's spin about the "x"-axis deduced), the positron's spin about the "z"-axis will no longer be certain, as if (a) it knows that the measurement has taken place, or (b) it has a definite spin already, about a "second" axis—a hidden variable. However, it turns out that the predictions of Quantum Mechanics, which have been confirmed by experiment, cannot be explained by any local hidden variable theory. This is demonstrated in Bell's theorem.
In quantum mechanics, the "x"-spin and "z"-spin are "incompatible observables", meaning the Heisenberg uncertainty principle applies to alternating measurements of them: a quantum state cannot possess a definite value for both of these variables. Suppose Alice measures the "z"-spin and obtains "+z", so that the quantum state collapses into state I. Now, instead of measuring the "z"-spin as well, Bob measures the "x"-spin. According to quantum mechanics, when the system is in state I, Bob's "x"-spin measurement will have a 50% probability of producing +"x" and a 50% probability of -"x". It is impossible to predict which outcome will appear until Bob actually "performs" the measurement.
Here is the crux of the matter. You might imagine that, when Bob measures the "x"-spin of his positron, he would get an answer with absolute certainty, since prior to this he hasn't disturbed his particle at all. Bob's positron has a 50% probability of producing +"x" and a 50% probability of −"x"—so the outcome is not certain. Bob's positron "knows" that Alice's electron has been measured, and its "z"-spin detected, and hence B's "z"-spin has been calculated, but the "x"-spin of Bob's positron remains uncertain.
Put another way, how does Bob's positron know which way to point if Alice decides (based on information unavailable to Bob) to measure x (i.e., to be the opposite of Alice's electron's spin about the "x"-axis) and "also" how to point if Alice measures z, since it is only supposed to know one thing at a time? The Copenhagen interpretation rules that say the wave function "collapses" at the time of measurement, so there must be action at a distance (entanglement) or the positron must know more than it's supposed to know (hidden variables). 
Here is the paradox summed up:
It is one thing to say that physical measurement of the first particle's momentum affects uncertainty in its "own" position, but to say that measuring the first particle's momentum affects the uncertainty in the position of the "other" is another thing altogether. Einstein, Podolsky and Rosen asked how can the second particle "know" to have precisely defined momentum but uncertain position? Since this implies that one particle is communicating with the other instantaneously across space, i.e., faster than light, this is the "paradox".
Incidentally, Bell used spin as his example, but many types of physical quantities—referred to as "observables" in quantum mechanics—can be used. The EPR paper used momentum for the observable. Experimental realisations of the EPR scenario often use photon polarization, because polarized photons are easy to prepare and measure.
Locality in the EPR experiment.
The principle of locality states that physical processes occurring at one place should have no immediate effect on the elements of reality at another location. At first sight, this appears to be a reasonable assumption to make, as it seems to be a consequence of special relativity, which states that information can never be transmitted faster than the speed of light without violating causality. It is generally believed that any theory which violates causality would also be internally inconsistent, and thus useless.
It turns out that the usual rules for combining quantum mechanical and classical descriptions violate the principle of locality without violating causality. Causality is preserved because there is no way for Alice to transmit messages (i.e., information) to Bob by manipulating her measurement axis. Whichever axis she uses, she has a 50% probability of obtaining "+" and 50% probability of obtaining "−", completely at random; according to quantum mechanics, it is fundamentally impossible for her to influence what result she gets. Furthermore, Bob is only able to perform his measurement "once": there is a fundamental property of quantum mechanics, known as the "no cloning theorem", which makes it impossible for him to make a million copies of the electron he receives, perform a spin measurement on each, and look at the statistical distribution of the results. Therefore, in the one measurement he is allowed to make, there is a 50% probability of getting "+" and 50% of getting "−", regardless of whether or not his axis is aligned with Alice's.
However, the principle of locality appeals powerfully to physical intuition, and Einstein, Podolsky and Rosen were unwilling to abandon it. Einstein derided the quantum mechanical predictions as "spooky action at a distance". The conclusion they drew was that quantum mechanics is not a complete theory.
In recent years, however, doubt has been cast on EPR's conclusion due to developments in understanding locality and especially quantum decoherence. The word locality has several different meanings in physics. For example, in quantum field theory "locality" means that quantum fields at different points of space do not interact with one another. However, quantum field theories that are "local" in this sense "appear" to violate the principle of locality as defined by EPR, but they nevertheless do not violate locality in a more general sense. Wavefunction collapse can be viewed as an epiphenomenon of quantum decoherence, which in turn is nothing more than an effect of the underlying local time evolution of the wavefunction of a system and "all" of its environment. Since the "underlying" behaviour doesn't violate local causality, it follows that neither does the additional effect of wavefunction collapse, whether real "or" apparent. Therefore, as outlined in the example above, neither the EPR experiment nor any quantum experiment demonstrates that faster-than-light signaling is possible.
Resolving the paradox.
Hidden variables.
There are several ways to resolve the EPR paradox. The one suggested by EPR is that quantum mechanics, despite its success in a wide variety of experimental scenarios, is actually an incomplete theory. In other words, there is some yet undiscovered theory of nature to which quantum mechanics acts as a kind of statistical approximation (albeit an exceedingly successful one). Unlike quantum mechanics, the more complete theory contains variables corresponding to all the "elements of reality". There must be some unknown mechanism acting on these variables to give rise to the observed effects of "non-commuting quantum observables", i.e. the Heisenberg uncertainty principle. Such a theory is called a hidden variable theory.
To illustrate this idea, we can formulate a very simple hidden variable theory for the above thought experiment. One supposes that the quantum spin-singlet states emitted by the source are actually approximate descriptions for "true" physical states possessing definite values for the "z"-spin and "x"-spin. In these "true" states, the positron going to Bob always has spin values opposite to the electron going to Alice, but the values are otherwise completely random. For example, the first pair emitted by the source might be "(+"z", −"x") to Alice and (−"z", +"x") to Bob", the next pair "(−"z", −"x") to Alice and (+"z", +"x") to Bob", and so forth. Therefore, if Bob's measurement axis is aligned with Alice's, he will necessarily get the opposite of whatever Alice gets; otherwise, he will get "+" and "−" with equal probability.
Assuming we restrict our measurements to the "z"- and "x"-axes, such a hidden variable theory is experimentally indistinguishable from quantum mechanics. In reality, there may be an infinite number of axes along which Alice and Bob can perform their measurements, so there would have to be an infinite number of independent hidden variables. However, this is not a serious problem; we have formulated a very simplistic hidden variable theory, and a more sophisticated theory might be able to patch it up. It turns out that there is a much more serious challenge to the idea of hidden variables.
Bell's inequality.
In 1964, John Bell showed that the predictions of quantum mechanics in the EPR thought experiment are significantly different from the predictions of a particular class of hidden variable theories (the "local" hidden variable theories). Roughly speaking, quantum mechanics has a much stronger statistical correlation with measurement results performed on different axes than do these hidden variable theories. These differences, expressed using inequality relations known as "Bell's inequalities", are in principle experimentally detectable. Later work by Eberhard showed that the key properties of local hidden variable theories which lead to Bell's inequalities are locality and counter-factual definiteness. Any theory in which these principles apply produces the inequalities. Arthur Fine subsequently showed that any theory satisfying the inequalities can be modeled by a local hidden variable theory.
After the publication of Bell's paper, a variety of experiments to test Bell's inequalities were devised. These generally relied on measurement of photon polarization. All experiments conducted to date have found behavior in line with the predictions of standard quantum mechanics theory.
However, Bell's theorem does not apply to all possible philosophically realist theories. It is a common misconception that quantum mechanics is inconsistent with all notions of philosophical realism. Realist interpretations of quantum mechanics are possible, although as discussed above, such interpretations must reject either locality or counter-factual definiteness. Mainstream physics prefers to keep locality, while striving also to maintain a notion of realism that nevertheless rejects counter-factual definiteness. Examples of such mainstream realist interpretations are the consistent histories interpretation and the transactional interpretation (first proposed by John G. Cramer in 1986). Fine's work showed that, taking locality as a given, there exist scenarios in which two statistical variables are correlated in a manner inconsistent with counter-factual definiteness, and that such scenarios are no more mysterious than any other, despite the fact that the inconsistency with counter-factual definiteness may seem 'counter-intuitive'.
Violation of locality is difficult to reconcile with special relativity, and is thought to be incompatible with the principle of causality. However, the Bohm interpretation of quantum mechanics keeps counter-factual definiteness, while introducing a conjectured non-local mechanism in the form of the 'quantum potential' that is defined as one of the terms of the Schrödinger equation. Some workers in the field have also attempted to formulate hidden variable theories that exploit loopholes in actual experiments, such as the assumptions made in interpreting experimental data, although no theory has been proposed that can reproduce all the results of quantum mechanics.
Alternatives are still possible. A recent review article based on the Wheeler–Feynman time-symmetric theory rewrites the entire theory in terms of retared Liénard–Wiechert potentials only, which becomes manifestly causal, and, establishes a conservation law for total generalized momenta held instantaneously for any closed system. The outcome results in correlation between particles from a "handshake principle" based on a variational principle applied to a system as a whole, an idea with a slightly non-local feature but the theory is nonetheless in agreement with the essential results of quantum electrodynamics and relativistic quantum chemistry.
There are also individual EPR-like experiments that have no local hidden variables explanation. Examples have been suggested by David Bohm and by Lucien Hardy.
Einstein's hope for a purely algebraic theory.
The Bohm interpretation of quantum mechanics hypothesizes that the state of the universe evolves smoothly through time with no collapsing of quantum wavefunctions. One problem for the Copenhagen interpretation is to precisely define wavefunction collapse. Einstein maintained that quantum mechanics is physically incomplete and logically unsatisfactory. In "The Meaning of Relativity", Einstein wrote, "One can give good reasons why reality cannot at all be represented by a continuous field. From the quantum phenomena it appears to follow with certainty that a finite system of finite energy can be completely described by a finite set of numbers (quantum numbers). This does not seem to be in accordance with a continuum theory and must lead to an attempt to find a purely algebraic theory for the representation of reality. But nobody knows how to find the basis for such a theory."
If time, space, and energy are secondary features derived from a substrate below the Planck scale, then Einstein's hypothetical algebraic system might resolve the EPR paradox (although Bell's theorem would still be valid). If physical reality is totally finite, then the Copenhagen interpretation might be an approximation to an information processing system below the Planck scale.
"Acceptable theories" and the experiment.
According to the present view of the situation, quantum mechanics flatly contradicts Einstein's philosophical postulate that any acceptable physical theory must fulfill "local realism".
In the EPR paper (1935), the authors realised that quantum mechanics was inconsistent with their assumptions, but Einstein nevertheless thought that quantum mechanics might simply be augmented by hidden variables (i.e., variables which were, at that point, still obscure to him), without any other change, to achieve an acceptable theory. He pursued these ideas for over twenty years until the end of his life, in 1955.
In contrast, John Bell, in his 1964 paper, showed that quantum mechanics and the class of hidden variable theories Einstein favored would lead to different experimental results: different by a factor of for certain correlations. So the issue of "acceptability", up to that time mainly concerning theory, finally became experimentally decidable.
There are many Bell test experiments, e.g., those of Alain Aspect and others. They support the predictions of quantum mechanics rather than the class of hidden variable theories supported by Einstein.
Implications for quantum mechanics.
Most physicists today believe that quantum mechanics is correct, and that the EPR paradox is a "paradox" only because classical intuitions do not correspond to physical reality. How EPR is interpreted regarding locality depends on the interpretation of quantum mechanics one uses. In the Copenhagen interpretation, it is usually understood that instantaneous wave function collapse does occur. However, the view that there is no "causal" instantaneous effect has also been proposed within the Copenhagen interpretation: in this alternate view, measurement affects our ability to define (and measure) quantities in the physical system, not the system itself. In the many-worlds interpretation, locality is strictly preserved, since the effects of operations such as measurement affect only the state of the particle that is measured. However, the results of the measurement are not unique—every possible result is obtained.
The EPR paradox has deepened our understanding of quantum mechanics by exposing the fundamentally non-classical characteristics of the measurement process. Before the publication of the EPR paper, a measurement was often visualized as a physical disturbance inflicted directly upon the measured system. For instance, when measuring the position of an electron, one imagines shining a light on it, thus disturbing the electron and producing the quantum mechanical uncertainties in its position. Such explanations, which are still encountered in popular expositions of quantum mechanics, are debunked by the EPR paradox, which shows that a "measurement" can be performed on a particle without disturbing it directly, by performing a measurement on a distant entangled particle. In fact, Yakir Aharonov and his collaborators have developed a whole theory of so-called Weak measurement.
Technologies relying on quantum entanglement are now being developed. In quantum cryptography, entangled particles are used to transmit signals that cannot be eavesdropped upon without leaving a trace. In quantum computation, entangled quantum states are used to perform computations in parallel, which may allow certain calculations to be performed much more quickly than they ever could be with classical computers.
Mathematical formulation.
The above discussion can be expressed mathematically using the quantum mechanical formulation of spin. The spin degree of freedom for an electron is associated with a two-dimensional complex vector space "V", with each quantum state corresponding to a vector in that space. The operators corresponding to the spin along the "x", "y", and "z" direction, denoted "Sx", "Sy", and "Sz" respectively, can be represented using the Pauli matrices:
where formula_3 is the reduced Planck constant (or the Planck constant divided by 2π).
The eigenstates of "Sz" are represented as
and the eigenstates of "Sx" are represented as
The vector space of the electron-positron pair is formula_6, the tensor product of the electron's and positron's vector spaces. The spin singlet state is
where the two terms on the right hand side are what we have referred to as state I and state II above.
From the above equations, it can be shown that the spin singlet can also be written as
where the terms on the right hand side are what we have referred to as state Ia and state IIa.
To illustrate how this leads to the violation of local realism, we need to show that after Alice's measurement of "Sz" (or "Sx"), Bob's value of "Sz" (or "Sx") is uniquely determined, and therefore corresponds to an "element of physical reality". This follows from the principles of measurement in quantum mechanics. When "S"z is measured, the system state ψ collapses into an eigenvector of "S"z. If the measurement result is "+z", this means that immediately after measurement the system state undergoes an orthogonal projection of ψ onto the
space of states of the form
For the spin singlet, the new state is
Similarly, if Alice's measurement result is −"z", the system undergoes an orthogonal projection onto
which means that the new state is
This implies that the measurement for "S"z for Bob's positron is now determined. It will be −"z" in the first case or +"z" in the second case.
It remains only to show that "Sx" and "Sz" cannot simultaneously possess definite values in quantum mechanics. One may show in a straightforward manner that no possible vector can be an eigenvector of both matrices. More generally, one may use the fact that the operators do not commute,
along with the Heisenberg uncertainty relation

</doc>
<doc id="65913" url="https://en.wikipedia.org/wiki?curid=65913" title="Equations of motion">
Equations of motion

In mathematical physics, equations of motion are equations that describe the behaviour of a physical system in terms of its motion as a function of time. More specifically, the equations of motion describe the behaviour of a physical system as a set of mathematical functions in terms of dynamic variables: normally spatial coordinates and time are used, but others are also possible, such as momentum components and time. The most general choice are generalized coordinates which can be any convenient variables characteristic of the physical system. The functions are defined in a Euclidean space in classical mechanics, but are replaced by curved spaces in relativity. If the dynamics of a system is known, the equations are the solutions to the differential equations describing the motion of the dynamics.
There are two main descriptions of motion: dynamics and kinematics. Dynamics is general, since momenta, forces and energy of the particles are taken into account. In this instance, sometimes the term refers to the differential equations that the system satisfies (e.g., Newton's second law or Euler–Lagrange equations), and sometimes to the solutions to those equations.
However, kinematics is simpler as it concerns only variables derived from the positions of objects, and time. In circumstances of constant acceleration, these simpler equations of motion are usually referred to as the "SUVAT" equations, arising from the definitions of kinematic quantities: displacement (S), initial velocity (U), final velocity (V), acceleration (A), and time (T). (see below).
Equations of motion can therefore be grouped under these main classifiers of motion. In all cases, the main types of motion are translations, rotations, oscillations, or any combinations of these.
A differential equation of motion, usually identified as some physical law and applying definitions of physical quantities, is used to set up an equation for the problem. Solving the differential equation will lead to a general solution with arbitrary constants, the arbitrariness corresponding to a family of solutions. A particular solution can be obtained by setting the initial values, which fixes the values of the constants.
To state this formally, in general an equation of motion "M" is a function of the position r of the object, its velocity (the first time derivative of r, v = "d"r/"dt"), and its acceleration (the second derivative of r, a = "d"2r/"dt"2), and time "t". Euclidean vectors in 3d are denoted throughout in bold. This is equivalent to saying an equation of motion in r is a second order ordinary differential equation (ODE) in r,
where "t" is time, and each overdot denotes one time derivative. The initial conditions are given by the "constant" values at "t" = 0,
The solution r("t") to the equation of motion, with specified initial values, describes the system for all times "t" after "t" = 0. Other dynamical variables like the momentum p of the object, or quantities derived from r and p like angular momentum, can be used in place of r as the quantity to solve for from some equation of motion, although the position of the object at time "t" is by far the most sought-after quantity.
Sometimes, the equation will be linear and is more likely to be exactly solvable. In general, the equation will be non-linear, and cannot be solved exactly so a variety of approximations must be used. The solutions to nonlinear equations may show chaotic behavior depending on how "sensitive" the system is to the initial conditions.
History.
Historically, equations of motion first appeared in classical mechanics to describe the motion of massive objects, a notable application was to celestial mechanics to predict the motion of the planets as if they orbit like clockwork (this was how Neptune was predicted before its discovery), and also investigate the stability of the solar system.
It is important to observe that the huge body of work involving kinematics, dynamics and the mathematical models of the universe developed in baby steps - faltering, getting up and correcting itself - over three millennia and included contributions of both known names and others who have since faded from the annals of history.
In antiquity, notwithstanding the success of priests, astrologers and astronomers in predicting solar and lunar eclipses, the solstices and the equinoxes of the Sun and the period of the moon, there was nothing other than a set of algorithms to help them. Despite the great strides made in the development of geometry in the Ancient Greece and surveys in Rome, we were to wait for another thousand years before the first equations of motion arrive.
The exposure of Europe to the collected works by the Muslims of the Greeks, the Indians and the Islamic scholars, such as Euclid’s Elements, the works of Archimedes, and Al-Khwārizmī’s treatises began in Spain, and scholars from all over Europe went to Spain, read, copied and translated the learning into Latin. The exposure of Europe to Indo-Arabic numerals and their ease in computations encouraged first the scholars to learn them and then the merchants and envigorated the spread of knowledge throughout Europe.
By the 13th century the universities of Oxford and Paris had come up, and the scholars were now studying mathematics and philosophy with lesser worries about mundane chores of life—the fields were not as clearly demarcated as they are in the modern times. Of these, compendia and redactions, such as those of Johannes Campanus, of Euclid and Aristotle, confronted scholars with ideas about infinity and the ratio theory of elements as a means of expressing relations between various quantities involved with moving bodies. These studies led to a new body of knowledge that is now known as physics.
Of these institutes Merton College sheltered a group of scholars devoted to natural science, mainly physics, astronomy and mathematics, of similar in stature to the intellectuals at the University of Paris. Thomas Bradwardine, one of those scholars, extended Aristotelian quantities such as distance and velocity, and assigned intensity and extension to them. Bradwardine suggested an exponential law involving force, resistance, distance, velocity and time. Nicholas Oresme further extended Bradwardine's arguments. The Merton school proved the that the quantity of motion of a body undergoing a uniformly accelerated motion is equal to the quantity of a uniform motion at the speed achieved halfway through the accelerated motion.
For writers on kinematics before Galileo, since small time intervals could not be measured, the affinity between time and motion was obscure. They used time as a function of distance, and in free fall, greater velocity as a result of greater elevation. Only Domingo de Soto, a Spanish Theologean, in his commentary on Aristotle's Physics published in 1545, after defining "uniform difform" motion (which is uniformly accelerated motion) - the word velocity wasn't used - as proportional to time, declared correctly that this kind of motion was identifiable with freely falling bodies and projectiles, without his proving these propositions or suggesting a formula relating time, velocity and distance. de Soto's comments are shockingly correct regarding the definitions of acceleration (acceleration was a rate of change of motion (velocity) in time) and the observation that during the violent motion of ascent acceleration would be negative.
Discourses such as these spread throughout the Europe and definitely influenced Galileo and others, and helped in laying the foundation of kinematics. Galileo deduced the equation formula_3 in his work geometrically, using Merton's rule, now known as a special case of one of the equations of Kinematics. He couldn't use the now-familiar mathematical reasoning. The relationships between speed, distance, time and acceleration was not known at the time.
Galileo was the first to show that the path of a projectile is a parabola. Galileo had an understanding of centrifugal force and gave a correct definition of momentum. This emphasis of momentum as a fundamental quantity in dynamics is of prime importance. He measured momentum by the product of velocity and weight; mass is a later concept, developed by Huygens and Newton. In the swinging of a simple pendulum, Galileo says in Discourses that "every momentum acquired in the descent along an arc is equal to that which causes the same moving body to ascend through the same arc." His analysis on projectiles indicates that Galileo had grasped the first law and the second law of motion. He did not generalize and make them applicable to bodies not subject to the earth's gravitation. That step was Newton's contribution.
The term "inertia" was used by Kepler who applied it to bodies at rest.The first law of motion is now often called the law of inertia.
Galileo did not fully grasp the third law of motion, the law of the equality of action and reaction, though he corrected some errors of Aristotle. With Stevin and others Galileo also wrote on statics. He formulated the principle of the parallelogram of forces, but he did not fully recognize its scope.
Galileo also was interested by the laws of the pendulum, his first observations was when he was a young man. In 1583, while he was praying in the cathedral at Pisa, his attention was arrested by the motion of the great lamp lighted and left swinging, referencing his own pulse for time keeping. To him the period appeared the same, even after the motion had greatly diminished, discovering the isochronism of the pendulum.
More careful experiments carried out by him later, and described in his Discourses, revealed the period of oscillation to be independent of the mass and material of the pendulum and as the square root of its length.
Thus we arrive at Rene Descartes, Isaac Newton, Leibniz, et al; and the evolved forms of the equations of motion that begin to be recognized as the modern ones.
Later the Equations of Motion also appeared in electrodynamics, when describing the motion of charged particles in electric and magnetic fields, the Lorentz force is the general equation which serves as the definition of what is meant by an electric field and magnetic field. With the advent of special relativity and general relativity, the theoretical modifications to spacetime meant the classical equations of motion were also modified to account for the finite speed of light, and curvature of spacetime. In all these cases the differential equations were in terms of a function describing the particle's trajectory in terms of space and time coordinates, as influenced by forces or energy transformations.
However, the equations of quantum mechanics can also be considered "equations of motion", since they are differential equations of the wavefunction, which describes how a quantum state behaves analogously using the space and time coordinates of the particles. There are analogs of equations of motion in other areas of physics, for collections of physical phenomena that can be considered waves, fluids, or fields.
Kinematic equations for one particle.
Kinematic quantities.
From the instantaneous position r = r("t"), instantaneous meaning at an instant value of time "t", the instantaneous velocity v = v("t") and acceleration a = a("t") have the general, coordinate-independent definitions;
Notice that velocity always points in the direction of motion, in other words for a curved path it is the tangent vector. Loosely speaking, first order derivatives are related to tangents of curves. Still for curved paths, the acceleration is directed towards the center of curvature of the path. Again, loosely speaking, second order derivatives are related to curvature.
The rotational analogues are the "angular vector" (angle the particle rotates about some axis) θ = θ("t"), angular velocity ω = ω("t"), and angular acceleration α = α("t"):
where n is a unit vector in the direction of the axis of rotation, and "θ" is the angle the object turns through about the axis.
The following relation holds for a point-like particle, orbiting about some axis with angular velocity ω:
where r is the position vector of the particle (radial from the rotation axis) and v the tangential velocity of the particle. For a rotating continuum rigid body, these relations hold for each point in the rigid body.
Uniform acceleration.
The differential equation of motion for a particle of constant or uniform acceleration in a straight line is simple: the acceleration is constant, so the second derivative of the position of the object is constant. The results of this case are summarized below.
Constant translational acceleration in a straight line.
These equations apply to a particle moving linearly, in three dimensions in a straight line with constant acceleration. Since the position, velocity, and acceleration are collinear (parallel, and lie on the same line) - only the magnitudes of these vectors are necessary, and because the motion is along a straight line, the problem effectively reduces from three dimensions to one.
Special cases of motion described be these equations are summarized qualitatively in the table below. Two have already been discussed above, in the cases that either the radial components or the angular components are zero, and the non-zero component of motion describes uniform acceleration.
General 3d motion.
In 3d space, the equations in spherical coordinates ("r", "θ", "ϕ") with corresponding unit vectors formula_9, the position, velocity, and acceleration generalize respectively to
In the case of a constant "ϕ" this reduces to the planar equations above.
Dynamic equations of motion.
Newtonian mechanics.
The first general equation of motion developed was Newton's second law of motion, in its most general form states the rate of change of momentum p = p("t") = "m"v("t") of an object equals the force F = F(x("t"), v("t"), "t") acting on it,
The force in the equation is "not" the force the object exerts. Replacing momentum by mass times velocity, the law is also written more famously as
since "m" is a constant in Newtonian mechanics.
Newton's second law applies to point-like particles, and to all points in a rigid body. They also apply to each point in a mass continua, like deformable solids or fluids, but the motion of the system must be accounted for, see material derivative. In the case the mass is not constant, it is not sufficient to use the product rule for the time derivative on the mass and velocity, and Newton's second law requires some modification consistent with conservation of momentum, see variable-mass system.
It may be simple to write down the equations of motion in vector form using Newton's laws of motion, but the components may vary in complicated ways with spatial coordinates and time, and solving them is not easy. Often there is an excess of variables to solve for the problem completely, so Newton's laws are not always the most efficient way to determine the motion of a system. In simple cases of rectangular geometry, Newton's laws work fine in Cartesian coordinates, but in other coordinate systems can become dramatically complex.
The momentum form is preferable since this is readily generalized to more complex systems, generalizes to special and general relativity (see four-momentum). It can also be used with the momentum conservation. However, Newton's laws are not more fundamental than momentum conservation, because Newton's laws are merely consistent with the fact that zero resultant force acting on an object implies constant momentum, while a resultant force implies the momentum is not constant. Momentum conservation is always true for an isolated system not subject to resultant forces.
For a number of particles (see many body problem), the equation of motion for one particle "i" influenced by other particles is
where p"i" is the momentum of particle "i", Fij is the force on particle "i" by particle "j", and FE is the resultant external force due to any agent not part of system. Particle "i" does not exert a force on itself.
Euler's laws of motion are similar to Newton's laws, but they are applied specifically to the motion of rigid bodies. The Newton–Euler equations combine the forces and torques acting on a rigid body into a single equation.
Newton's second law for rotation takes a similar form to the translational case,
by equating the torque acting on the body to the rate of change of its angular momentum L. Analogous to mass times acceleration, the moment of inertia tensor I depends on the distribution of mass about the axis of rotation, and the angular acceleration is the rate of change of angular velocity,
Again, these equations apply to point like particles, or at each point of a rigid body.
Likewise, for a number of particles, the equation of motion for one particle "i" is
where L"i" is the angular momentum of particle "i", τ"ij" the torque on particle "i" by particle "j", and τ"E" = resultant external torque (due to any agent not part of system). Particle "i" does not exert a torque on itself.
Applications.
Some examples of Newton's law include describing the motion of a simple pendulum,
and a damped, sinusoidally driven harmonic oscillator,
For describing the motion of masses due to gravity, Newton's law of gravity can be combined with Newton's second law. For two examples, a ball of mass "m" thrown in the air, in air currents (such as wind) described by a vector field of resistive forces R = R(r, "t"),
where "G" is the gravitational constant, "M" the mass of the Earth, and A = R/"m" is the acceleration of the projectile due to the air currents at position r and time "t".
The classical "N"-body problem for "N" particles each interacting with each other due to gravity is a set of "N" nonlinear coupled second order ODEs,
where "i" = 1, 2, ..., "N" labels the quantities (mass, position, etc.) associated with each particle.
Analytical mechanics.
Using all three coordinates of 3d space is unnecessary if there are constraints on the system. If the system has "N" degrees of freedom, then one can use a set of "N" generalized coordinates q("t") = ["q"1("t"), "q"2("t") ... "qN"("t")], to define the configuration of the system. They can be in the form of arc lengths or angles. They are a considerable simplification to describe motion, since they take advantage of the intrinsic constraints that limit the system's motion, and the number of coordinates is reduced to a minimum. The time derivatives of the generalized coordinates are the "generalized velocities"
The Euler–Lagrange equations are
where the "Lagrangian" is a function of the configuration q and its time rate of change dq/d"t" (and possibly time "t")
Setting up the Lagrangian of the system, then substituting into the equations and evaluating the partial derivatives and simplifying, a set of coupled "N" second order ODEs in the coordinates are obtained.
Hamilton's equations are
where the Hamiltonian
is a function of the configuration q and conjugate ""generalized" momenta"
in which is a shorthand notation for a vector of partial derivatives with respect to the indicated variables (see for example matrix calculus for this denominator notation), and possibly time "t",
Setting up the Hamiltonian of the system, then substituting into the equations and evaluating the partial derivatives and simplifying, a set of coupled 2"N" first order ODEs in the coordinates "qi" and momenta "pi" are obtained.
The Hamilton–Jacobi equation is
where
is "Hamilton's principal function", also called the "classical action" is a functional of "L". In this case, the momenta are given by
Although the equation has a simple general form, for a given Hamiltonian it is actually a single first order "non-linear" PDE, in "N" + 1 variables. The action "S" allows identification of conserved quantities for mechanical systems, even when the mechanical problem itself cannot be solved fully, because any differentiable symmetry of the action of a physical system has a corresponding conservation law, a theorem due to Emmy Noether.
All classical equations of motion can be derived from the variational principle known as Hamilton's principal of least action
stating the path the system takes through the configuration space is the one with the least action "S".
Electrodynamics.
In electrodynamics, the force on a charged particle of charge "q" is the Lorentz force:
Combining with Newton's second law gives a first order differential equation of motion, in terms of position of the particle:
or its momentum:
The same equation can be obtained using the Lagrangian (and applying Lagrange's equations above) for a charged particle of mass "m" and charge "q":
where A and "ϕ" are the electromagnetic scalar and vector potential fields. The Lagrangian indicates an additional detail: the canonical momentum in Lagrangian mechanics is given by:
instead of just "m"v, implying the motion of a charged particle is fundamentally determined by the mass and charge of the particle. The Lagrangian expression was first used to derive the force equation.
Alternatively the Hamiltonian (and substituting into the equations):
can derive the Lorentz force equation.
General relativity.
Geodesic equation of motion.
The above equations are valid in flat spacetime. In curved space spacetime, things become mathematically more complicated since there is no straight line; this is generalized and replaced by a "geodesic" of the curved spacetime (the shortest length of curve between two points). For curved manifolds with a metric tensor "g", the metric provides the notion of arc length (see line element for details), the differential arc length is given by:
and the geodesic equation is a second-order differential equation in the coordinates, the general solution is a family of geodesics:
where Γμαβ is a Christoffel symbol of the second kind, which contains the metric (with respect to the coordinate system).
Given the mass-energy distribution provided by the stress–energy tensor "T"αβ, the Einstein field equations are a set of non-linear second-order partial differential equations in the metric, and imply the curvature of space time is equivalent to a gravitational field (see principle of equivalence). Mass falling in curved spacetime is equivalent to a mass falling in a gravitational field - because gravity is a fictitious force. The "relative acceleration" of one geodesic to another in curved spacetime is given by the "geodesic deviation equation":
where ξα = ("x"2)α − ("x"1)α is the separation vector between two geodesics, D/d"s" ("not" just d/d"s") is the covariant derivative, and "R"αβγδ is the Riemann curvature tensor, containing the Christoffel symbols. In other words, the geodesic deviation equation is the equation of motion for masses in curved spacetime, analogous to the Lorentz force equation for charges in an electromagnetic field.
For flat spacetime, the metric is a constant tensor so the Christoffel symbols vanish, and the geodesic equation has the solutions of straight lines. This is also the limiting case when masses move according to Newton's law of gravity.
Spinning objects.
In general relativity, rotational motion is described by the relativistic angular momentum tensor, including the spin tensor, which enter the equations of motion under covariant derivatives with respect to proper time. The Mathisson–Papapetrou–Dixon equations describe the motion of spinning objects moving in a gravitational field.
Analogues for waves and fields.
Unlike the equations of motion for describing particle mechanics, which are systems of coupled ordinary differential equations, the analogous equations governing the dynamics of waves and fields are always partial differential equations, since the waves or fields are functions of space and time. For a particular solution, boundary conditions along with initial conditions need to be specified.
Sometimes in the following contexts, the wave or field equations are also called "equations of motion".
Field equations.
Equations that describe the spatial dependence and time evolution of fields are called "field equations". These include
This terminology is not universal: for example although the Navier–Stokes equations govern the velocity field of a fluid, they are not usually called "field equations", since in this context they represent the momentum of the fluid and are called the "momentum equations" instead.
Wave equations.
Equations of wave motion are called "wave equations". The solutions to a wave equation give the time-evolution and spatial dependence of the amplitude. Boundary conditions determine if the solutions describe traveling waves or standing waves.
From classical equations of motion and field equations; mechanical, gravitational wave, and electromagnetic wave equations can be derived. The general linear wave equation in 3d is:
where "X" = "X"(r, "t") is any mechanical or electromagnetic field amplitude, say:
and "v" is the phase velocity. Non-linear equations model the dependence of phase velocity on amplitude, replacing "v" by "v"("X"). There are other linear and non-linear wave equations for very specific applications, see for example the Korteweg–de Vries equation.
Quantum theory.
In quantum theory, the wave and field concepts both appear.
In quantum mechanics, in which particles also have wave-like properties according to wave–particle duality, the analogue of the classical equations of motion (Newton's law, Euler–Lagrange equation, Hamilton–Jacobi equation, etc.) is the Schrödinger equation in its most general form:
where "Ψ" is the wavefunction of the system, formula_42 is the quantum Hamiltonian operator, rather than a function as in classical mechanics, and "ħ" is the Planck constant divided by 2"π". Setting up the Hamiltonian and inserting it into the equation results in a wave equation, the solution is the wavefunction as a function of space and time. The Schrödinger equation itself reduces to the Hamilton–Jacobi equation in when one considers the correspondence principle, in the limit that "ħ" becomes zero.
Throughout all aspects of quantum theory, relativistic or non-relativistic, there are various formulations alternative to the Schrödinger equation that govern the time evolution and behavior of a quantum system, for instance: 

</doc>
<doc id="1441435" url="https://en.wikipedia.org/wiki?curid=1441435" title="Equilibrium point">
Equilibrium point

In mathematics, specifically in differential equations, an equilibrium point is a constant solution to a differential equation.
Formal definition.
The point formula_1 is an equilibrium point for the differential equation 
if formula_3 for all formula_4.
Similarly, the point formula_1 is an equilibrium point (or fixed point) for the difference equation 
if formula_7 for formula_8.
Classification.
Equilibria can be classified by looking at the signs of the eigenvalues of the linearization of the equations about the equilibria. That is to say, by evaluating the Jacobian matrix at each of the equilibrium points of the system, and then finding the resulting eigenvalues, the equilibria can be categorized. Then the behavior of the system in the neighborhood of each equilibrium point can be qualitatively determined, (or even quantitatively determined, in some instances), by finding the eigenvector(s) associated with each eigenvalue. 
An equilibrium point is "hyperbolic" if none of the eigenvalues have zero real part. If all eigenvalues have negative real part, the equilibrium is a stable equation. If at least one has a positive real part, the equilibrium is an unstable node. If at least one eigenvalue has negative real part and at least one has positive real part, the equilibrium is a saddle point.

</doc>
<doc id="857235" url="https://en.wikipedia.org/wiki?curid=857235" title="Equivalence principle">
Equivalence principle

In the physics of general relativity, the equivalence principle is any of several related concepts dealing with the equivalence of gravitational and inertial mass, and to Albert Einstein's observation that the gravitational "force" as experienced locally while standing on a massive body (such as the Earth) is actually the same as the "pseudo-force" experienced by an observer in a non-inertial (accelerated) frame of reference.
Development of gravitation theory.
Something like the equivalence principle emerged in the early 16th century, when Galileo expressed experimentally that the acceleration of a test mass due to gravitation is independent of the amount of mass being accelerated. 
Kepler, using Galileo's discoveries, showed knowledge of the equivalence principle by accurately describing what would occur if the moon were stopped in its orbit and dropped towards Earth. This can be deduced without knowing if or in what manner gravity decreases with distance, but requires assuming the equivalency between gravity and inertia. 
The 1/54 ratio is Kepler's estimate of the moon-Earth mass ratio, based on their diameters. The accuracy of his statement can be deduced by using Newton's inertia law F=ma and Galileo's gravitational observation that distance D = 1/2*a*t^2. Setting these accelerations equal for a mass is the equivalence principle. Noting the time to collision for each mass is the same gives Kepler's statement that Dmoon/DEarth=MEarth/Mmoon, without knowing the time to collision or how or if the acceleration force from gravity is a function of distance.
Newton's gravitational theory simplified and formalized Galileo's and Kepler's ideas by recognizing Kepler's "animal force or some other equivalent" beyond gravity and inertia were not needed, deducing from Kepler's planetary laws how gravity reduces with distance.
The equivalence principle was properly introduced by Albert Einstein in 1907, when he observed that the acceleration of bodies towards the center of the Earth at a rate of 1'g' ("g" = 9.81 m/s2 being a standard reference of gravitational acceleration at the Earth's surface) is equivalent to the acceleration of an inertially moving body that would be observed on a rocket in free space being accelerated at a rate of 1"g". Einstein stated it thus:
That is, being on the surface of the Earth is equivalent to being inside a spaceship (far from any sources of gravity) that is being accelerated by its engines. The direction or vector of acceleration equivalence on the surface of the earth is "up" or directly opposite the center of the planet while the vector of acceleration in a spaceship is directly opposite from the mass ejected by its thrusters. From this principle, Einstein deduced that free-fall is actually inertial motion. Objects in free-fall do not accelerate downward (e.g. toward the earth or other massive body) but rather experience weightlessness and no acceleration. In an inertial frame of reference bodies (and photons, or light) obey Newton's first law, moving at constant velocity in straight lines. Analogously, in a curved spacetime the world line of an inertial particle or pulse of light is "as straight as possible" (in space "and" time). Such a world line is called a geodesic and from the point of view of the inertial frame is a straight line. This is why an accelerometer in free-fall doesn't register any acceleration; there isn't any.
As an example: an inertial body moving along a geodesic through space can be trapped into an orbit around a large gravitational mass without ever experiencing acceleration. This is possible because space is radically curved in close vicinity to a large gravitational mass. In such a situation the geodesic lines bend inward around the center of the mass and a free-floating (weightless) inertial body will simply follow those curved geodesics into an elliptical orbit. An accelerometer on-board would never record any acceleration.
By contrast, in Newtonian mechanics, gravity is assumed to be a force. This force draws objects having mass towards the center of any massive body. At the Earth's surface, the force of gravity is counteracted by the mechanical (physical) resistance of the Earth's surface. So in Newtonian physics, a person at rest on the surface of a (non-rotating) massive object is in an inertial frame of reference. These considerations suggest the following corollary to the equivalence principle, which Einstein formulated precisely in 1911:
Einstein also referred to two reference frames, K and K'. K is a uniform gravitational field, whereas K' has no gravitational field but is uniformly accelerated such that objects in the two frames experience identical forces:
This observation was the start of a process that culminated in general relativity. Einstein suggested that it should be elevated to the status of a general principle, which he called the "principle of equivalence," when constructing his theory of relativity:
Einstein combined (postulated) the equivalence principle with special relativity to predict that clocks run at different rates in a gravitational potential, and light rays bend in a gravitational field, even before he developed the concept of curved spacetime.
So the original equivalence principle, as described by Einstein, concluded that free-fall and inertial motion were physically equivalent. This form of the equivalence principle can be stated as follows. An observer in a windowless room cannot distinguish between being on the surface of the Earth, and being in a spaceship in deep space accelerating at 1g. This is not strictly true, because massive bodies give rise to tidal effects (caused by variations in the strength and direction of the gravitational field) which are absent from an accelerating spaceship in deep space. The room, therefore, should be small enough that tidal effects can be neglected.
Although the equivalence principle guided the development of general relativity, it is not a founding principle of relativity but rather a simple consequence of the "geometrical" nature of the theory. In general relativity, objects in free-fall follow geodesics of spacetime, and what we perceive as the force of gravity is instead a result of our being unable to follow those geodesics of spacetime, because the mechanical resistance of matter prevents us from doing so.
Since Einstein developed general relativity, there was a need to develop a framework to test the theory against other possible theories of gravity compatible with special relativity. This was developed by Robert Dicke as part of his program to test general relativity. Two new principles were suggested, the so-called Einstein equivalence principle and the strong equivalence principle, each of which assumes the weak equivalence principle as a starting point. They only differ in whether or not they apply to gravitational experiments.
Modern usage.
Three forms of the equivalence principle are in current use: weak (Galilean), Einsteinian, and strong.
The weak equivalence principle.
The weak equivalence principle, also known as the universality of free fall or the Galilean equivalence principle can be stated in many ways. The strong EP includes (astronomic) bodies with gravitational binding energy (e.g., 1.74 solar-mass pulsar PSR J1903+0327, 15.3% of whose separated mass is absent as gravitational binding energy). The weak EP assumes falling bodies are bound by non-gravitational forces only. Either way:
Locality eliminates measurable tidal forces originating from a radial divergent gravitational field (e.g., the Earth) upon finite sized physical bodies. The "falling" equivalence principle embraces Galileo's, Newton's, and Einstein's conceptualization. The equivalence principle does not deny the existence of measurable effects caused by a "rotating" gravitating mass (frame dragging), or bear on the measurements of light deflection and gravitational time delay made by non-local observers.
Active, passive, and inertial masses.
By definition of active and passive gravitational mass, the force on formula_1 due to the gravitational field of formula_2 is:
Likewise the force on a second object of arbitrary mass2 due to the gravitational field of mass0 is:
By definition of inertial mass:
If formula_6 and formula_7 are the same distance formula_8 from formula_9 then, by the weak equivalence principle, they fall at the same rate (i.e. their accelerations are the same)
Hence:
Therefore:
In other words, passive gravitational mass must be proportional to inertial mass for all objects.
Furthermore by Newton's third law of motion:
must be equal and opposite to
It follows that:
In other words, passive gravitational mass must be proportional to active gravitational mass for all objects.
The dimensionless Eötvös-parameter formula_16 is the difference of the ratios of gravitational and inertial masses divided by their average for the two sets of test masses "A" and "B."
Tests of the weak equivalence principle.
Tests of the weak equivalence principle are those that verify the equivalence of gravitational mass and inertial mass. An obvious test is dropping different objects, ideally in a vacuum environment, e.g., inside Fallturm Bremen.
See:
Experiments are still being performed at the University of Washington which have placed limits on the differential acceleration of objects towards the Earth, the Sun and towards dark matter in the galactic center. Future satellite experiments – STEP (Satellite Test of the Equivalence Principle), Galileo Galilei, and MICROSCOPE (MICROSatellite pour l'Observation de Principe d'Equivalence) – will test the weak equivalence principle in space, to much higher accuracy.
With the first successful production of antimatter, in particular anti-hydrogen, a new approach to test the weak equivalence principle has been proposed. Experiments to compare the gravitational behavior of matter and antimatter are currently being developed. 
Proposals that may lead to a quantum theory of gravity such as string theory and loop quantum gravity predict violations of the weak equivalence principle because they contain many light scalar fields with long Compton wavelengths, which should generate fifth forces and variation of the fundamental constants. Heuristic arguments suggest that the magnitude of these equivalence principle violations could be in the 10−13 to 10−18 range. Currently envisioned tests of the weak equivalence principle are approaching a degree of sensitivity such that "non-discovery" of a violation would be just as profound a result as discovery of a violation. Non-discovery of equivalence principle violation in this range would suggest that gravity is so fundamentally different from other forces as to require a major reevaluation of current attempts to unify gravity with the other forces of nature. A positive detection, on the other hand, would provide a major guidepost towards unification.
The Einstein equivalence principle.
What is now called the "Einstein equivalence principle" states that the weak equivalence principle holds, and that:
Here "local" has a very special meaning: not only must the experiment not look outside the laboratory, but it must also be small compared to variations in the gravitational field, tidal forces, so that the entire laboratory is freely falling. It also implies the absence of interactions with "external" fields "other than the gravitational field".
The principle of relativity implies that the outcome of local experiments must be independent of the velocity of the apparatus, so the most important consequence of this principle is the Copernican idea that dimensionless physical values such as the fine-structure constant and electron-to-proton mass ratio must not depend on where in space or time we measure them. Many physicists believe that any Lorentz invariant theory that satisfies the weak equivalence principle also satisfies the Einstein equivalence principle.
"Schiff's conjecture" suggests that the weak equivalence principle actually implies the Einstein equivalence principle, but it has not been proven. Nonetheless, the two principles are tested with very different kinds of experiments. The Einstein equivalence principle has been criticized as imprecise, because there is no universally accepted way to distinguish gravitational from non-gravitational experiments (see for instance Hadley and Durand).
Tests of the Einstein equivalence principle.
In addition to the tests of the weak equivalence principle, the Einstein equivalence principle can be tested by searching for variation of dimensionless constants and mass ratios. The present best limits on the variation of the fundamental constants have mainly been set by studying the naturally occurring Oklo natural nuclear fission reactor, where nuclear reactions similar to ones we observe today have been shown to have occurred underground approximately two billion years ago. These reactions are extremely sensitive to the values of the fundamental constants.
There have been a number of controversial attempts to constrain the variation of the strong interaction constant. There have been several suggestions that "constants" do vary on cosmological scales. The best known is the reported detection of variation (at the 10−5 level) of the fine-structure constant from measurements of distant quasars, see Webb et al. Other researchers dispute these findings. Other tests of the Einstein equivalence principle are gravitational redshift experiments, such as the Pound–Rebka experiment which test the position independence of experiments.
The strong equivalence principle.
The strong equivalence principle suggests the laws of gravitation are independent of velocity and location. In particular,
and
The first part is a version of the weak equivalence principle that applies to objects that exert a gravitational force on themselves, such as stars, planets, black holes or Cavendish experiments. The second part is the Einstein equivalence principle (with the same definition of "local"), restated to allow gravitational experiments and self-gravitating bodies. The freely-falling object or laboratory, however, must still be small, so that tidal forces may be neglected (hence "local experiment").
This is the only form of the equivalence principle that applies to self-gravitating objects (such as stars), which have substantial internal gravitational interactions. It requires that the gravitational constant be the same everywhere in the universe and is incompatible with a fifth force. It is much more restrictive than the Einstein equivalence principle.
The strong equivalence principle suggests that gravity is entirely geometrical by nature (that is, the metric alone determines the effect of gravity) and does not have any extra fields associated with it. If an observer measures a patch of space to be flat, then the strong equivalence principle suggests that it is absolutely equivalent to any other patch of flat space elsewhere in the universe. Einstein's theory of general relativity (including the cosmological constant) is thought to be the only theory of gravity that satisfies the strong equivalence principle. A number of alternative theories, such as Brans–Dicke theory, satisfy only the Einstein equivalence principle.
Tests of the strong equivalence principle.
The strong equivalence principle can be tested by searching for a variation of Newton's gravitational constant "G" over the life of the universe, or equivalently, variation in the masses of the fundamental particles. A number of independent constraints, from orbits in the solar system and studies of big bang nucleosynthesis have shown that "G" cannot have varied by more than 10%.
Thus, the strong equivalence principle can be tested by searching for fifth forces (deviations from the gravitational force-law predicted by general relativity). These experiments typically look for failures of the inverse-square law (specifically Yukawa forces or failures of Birkhoff's theorem) behavior of gravity in the laboratory. The most accurate tests over short distances have been performed by the Eöt-Wash group. A future satellite experiment, SEE (Satellite Energy Exchange), will search for fifth forces in space and should be able to further constrain violations of the strong equivalence principle. Other limits, looking for much longer-range forces, have been placed by searching for the Nordtvedt effect, a "polarization" of solar system orbits that would be caused by gravitational self-energy accelerating at a different rate from normal matter. This effect has been sensitively tested by the Lunar Laser Ranging Experiment. Other tests include studying the deflection of radiation from distant radio sources by the sun, which can be accurately measured by very long baseline interferometry. Another sensitive test comes from measurements of the frequency shift of signals to and from the Cassini spacecraft. Together, these measurements have put tight limits on Brans–Dicke theory and other alternative theories of gravity.
In 2014, astronomers discovered a stellar triple system including a millisecond pulsar PSR J0337+1715 and two white dwarfs orbiting it. The system will provide them a chance to test the strong equivalence principle in a strong gravitational field.
Challenges to the equivalence principle.
One challenge to the equivalence principle is the Brans–Dicke theory. Self-creation cosmology is a modification of the Brans–Dicke theory. The Fredkin Finite Nature Hypothesis is an even more radical challenge to the equivalence principle and has even fewer supporters.
In August 2010, researchers from the University of New South Wales, Swinburne University of Technology, and Cambridge University published a paper titled "Evidence for spatial variation of the fine structure constant", whose tentative conclusion is that, "qualitatively, [the] results suggest a violation of the Einstein Equivalence Principle, and could infer a very large or infinite universe, within which our 'local' Hubble volume represents a tiny fraction."
Explanations of the equivalence principle.
Dutch physicist and string theorist Erik Verlinde has generated a self-contained, logical derivation of the equivalence principle based on the starting assumption of a holographic universe. Given this situation, gravity would not be a true fundamental force as is currently thought but instead an "emergent property" related to entropy. Verlinde's entropic gravity theory apparently leads naturally to the correct observed strength of dark energy; previous failures to explain its incredibly small magnitude have been called by such people as cosmologist Michael Turner (who is credited as having coined the term "dark energy") as "the greatest embarrassment in the history of theoretical physics". However, it should be noted that these ideas are far from settled and still very controversial.

</doc>
<doc id="41118" url="https://en.wikipedia.org/wiki?curid=41118" title="Error">
Error

An error (from the Latin "error", meaning "wandering") is an action which is inaccurate or incorrect. In some usages, an error is synonymous with a mistake (for instance, a cook who misses a step from a recipe might describe it as either an error or a mistake), though in technical contexts the two are often distinguished. For instance, in statistics "error" refers to the difference between the value which has been computed and the correct value.
Human behavior.
One reference differentiates between "error" and "mistake" as follows:
In human behavior the norms or expectations for behavior or its consequences can be derived from the intention of the actor or from the expectations of other individuals or from a social grouping or from social norms. (See deviance.) Gaffes and faux pas can be labels for certain instances of this kind of error. More serious departures from social norms carry labels such as misbehavior and labels from the legal system, such as misdemeanor and crime. Departures from norms connected to religion can have other labels, such as sin.
Oral and written language.
An individual language user's deviations from standard language norms in grammar, syntax, pronunciation and punctuation are sometimes referred to as errors. However, in light of the role of language usage in everyday social class distinctions, many feel that linguistics should be descriptive rather than prescriptive to avoid reinforcing dominant class value judgments about what linguistic forms should and should not be used. One may distinguish various kinds of linguistic errors – some, such as aphasia or speech disorders, where the user is unable to say what they intend to, are generally considered errors, while cases where natural, intended speech is non-standard (as in dialects), are considered correct speech in descriptive linguistics, but errors in prescriptive linguistics. See also Error analysis (linguistics).
Gaffe.
A gaffe is a verbal mistake, usually made in a social environment. The mistake may come from saying something that is true, but inappropriate. It may also be an erroneous attempt to reveal a truth. Finally, gaffes can be malapropisms, grammatical errors or other verbal and gestural weaknesses or revelations through body language.
Actually revealing factual or social truth through words or body language, however, can commonly result in embarrassment or, when the gaffe has negative connotations, friction between people involved.
Philosophers and psychologists interested in the nature of the gaffe include Freud and Gilles Deleuze. Deleuze, in his "Logic of Sense", places the gaffe in a developmental process that can culminate in stuttering.
As used by some journalists, particularly sportswriters, "gaffe" becomes an imagined synonym for any kind of mistake, e.g., a dropped ball by a player in a baseball game.
Medicine.
See medical error for a description of error in medicine.
Science and engineering.
In statistics, an error (or "residual") is not a "mistake" but rather a difference between a computed, estimated, or measured value and the accepted true, specified, or theoretically correct value.
In science and engineering in general an error is defined as a difference between the desired and actual performance or behavior of a system or object. This definition is the basis of operation for many types of control systems, in which error is defined as the difference between a set point and the process value. An example of this would be the thermostat in a home heating system—the operation of the heating equipment is controlled by the difference (the error) between the thermostat setting and the sensed air temperature. Another approach is related to considering a scientific hypothesis as true or false, giving birth to two types of errors: Type 1 and Type 2. The first one is when a true hypothesis is considered false, while the second is the reverse (a false one is considered true).
Engineers seek to design devices, machines and systems and in such a way as to mitigate or preferably avoid the effects of error, whether unintentional or not. Such errors in a system can be latent design errors that may go unnoticed for years, until the right set of circumstances arises that cause them to become active. Other errors in engineered systems can arise due to human error, which includes cognitive bias. Human factors engineering is often applied to designs in an attempt to minimize this type of error by making systems more forgiving or error-tolerant.
Numerical analysis.
Numerical analysis provides a variety of techniques to represent (store) and compute approximations to mathematical numerical values. Errors arise from a trade-off between efficiency (space and computation time) and precision, which is limited anyway, since (using common floating-point arithmetic) only a finite amount of values can be represented exactly. The discrepancy between the exact mathematical value and the stored/computed value is called the approximation error.
Cybernetics.
The word "cybernetics" stems from the Greek Κυβερνήτης ("kybernētēs", steersman, governor, pilot, or rudder — the same root as government). In applying corrections to the trajectory or course being steered cybernetics can be seen as the most general approach to error and its correction for the achievement of any goal. The term was suggested by Norbert Wiener to describe a new science of control and information in the animal and the machine. Wiener's early work was on noise.
The cybernetician Gordon Pask held that the error that drives a servomechanism can be seen as a difference between a pair of analogous concepts in a servomechanism: the current state and the goal state. Later he suggested error can also be seen as an innovation or a contradiction depending on the context and perspective of interacting (observer) participants. The founder of management cybernetics, Stafford Beer, applied these ideas most notably in his Viable System Model.
Biology.
In biology, an error is said to occur when perfect fidelity is lost in the copying of information. For example, in an asexually reproducing species, an error (or mutation) has occurred for each DNA nucleotide that differs between the child and the parent. Many of these mutations can be harmful, but unlike other types of errors, some are neutral or even beneficial. Mutations are an important force driving evolution. Mutations that make organisms more adapted to their environment increase in the population through natural selection as organisms with favorable mutations have more offspring.
Philately.
In philately, an error refers to a postage stamp or piece of postal stationery that exhibits a printing or production mistake that differentiates it from a normal specimen or from the intended result. Examples are stamps printed in the wrong color or missing one or more colors, printed with a vignette inverted in relation to its frame, produced without any perforations on one or more sides when the normal stamps are perforated, or printed on the wrong type of paper. Legitimate errors must always be produced and sold unintentionally. Such errors may or may not be scarce or rare. A design error may refer to a mistake in the design of the stamp, such as a mislabeled subject, even if there are no printing or production mistakes.
Law.
In appellate review, error typically refers to mistakes made by a trial court or some other court of first instance in applying the law in a particular legal case. This may involve such mistakes as improper admission of evidence, inappropriate instructions to the jury, or applying the wrong standard of proof.
Governmental policy.
Within United States government intelligence agencies, such as Central Intelligence Agency agencies, error refers to intelligence error, as previous assumptions that used to exist at a senior intelligence level within senior intelligence agencies, but has since been disproven, and is sometimes eventually listed as unclassified, and therefore more available to the American public and citizenry of the United States. The Freedom of information act provides American citizenry with a means to read intelligence reports that were mired in error. Per United States Central Intelligence Agency's website (as of August, 2008) intelligence error is described as:
"Intelligence errors are factual inaccuracies in analysis resulting from poor or missing data; intelligence failure is systemic organizational surprise resulting from incorrect, missing, discarded, or inadequate hypotheses."
Numismatics.
In numismatics, an error refers to a coin or medal that has a minting mistake, similar to errors found in philately. Because the U.S. Bureau of the Mint keeps a careful eye on all potential errors, errors on U.S. coins are very few and usually very scarce. Examples of numismatic errors: extra metal attached to a coin, a clipped coin caused by the coin stamp machine stamping a second coin too early, double stamping of a coin. A coin that has been overdated, e.g.: 1942/41, is also considered an error.

</doc>
<doc id="4963820" url="https://en.wikipedia.org/wiki?curid=4963820" title="Error bar">
Error bar

Error bars are a graphical representation of the variability of data and are used on graphs to indicate the error, or uncertainty in a reported measurement. They give a general idea of how precise a measurement is, or conversely, how far from the reported value the true (error free) value might be. Error bars often represent one standard deviation of uncertainty, one standard error, or a certain confidence interval (e.g., a 95% interval). These quantities are not the same and so the measure selected should be stated explicitly in the graph or supporting text.
Error bars can be used to compare visually two quantities if various other conditions hold. This can determine whether differences are statistically significant. Error bars can also suggest goodness of fit of a given function, i.e., how well the function describes the data. Scientific papers in the experimental sciences are expected to include error bars on all graphs, though the practice differs somewhat between sciences, and each journal will have its own house style. It has also been shown that error bars can be used as a direct manipulation interface for controlling probabilistic algorithms for approximate computation.

</doc>
<doc id="177111" url="https://en.wikipedia.org/wiki?curid=177111" title="Error function">
Error function

In mathematics, the error function (also called the Gauss error function) is a special function (non-elementary) of sigmoid shape that occurs in probability, statistics, and partial differential equations describing diffusion. It is defined as:
The complementary error function, denoted "erfc", is defined as
which also defines "erfcx", the scaled complementary error function (which can be used instead of erfc to avoid arithmetic underflow). Another form of formula_3 is known as Craig's formula:
The imaginary error function, denoted "erfi", is defined as
where "D"("x") is the Dawson function (which can be used instead of erfi to avoid arithmetic overflow).
Despite the name "imaginary error function", formula_6 is real when "x" is real.
When the error function is evaluated for arbitrary complex arguments "z", the resulting complex error function is usually discussed in scaled form as the Faddeeva function:
The name "error function".
The error function is used in measurement theory (using probability and statistics), and its use in other branches of mathematics is typically unrelated to the characterization of measurement errors.
The error function is related to the cumulative distribution formula_8, the integral of the standard normal distribution, by
The error function, evaluated at formula_10 for positive "x" values, gives the probability that a measurement, under the influence of normally distributed errors with standard deviation formula_11, has a distance less than x from the mean value. This function is used in statistics to predict behavior of any sample with respect to the population mean. This usage is similar to the Q-function, which in fact can be written in terms of the error function.
Properties.
The property formula_12 means that the error function is an odd function. This directly results from the fact that the integrand formula_13 is an even function.
For any complex number "z":
where formula_15 is the complex conjugate of "z".
The integrand "ƒ" = exp(−"z"2) and "ƒ" = erf("z") are shown in the complex "z"-plane in figures 2 and 3. Level of Im("ƒ") = 0 is shown with a thick green line. Negative integer values of Im("ƒ") are shown with thick red lines. Positive integer values of Im("f") are shown with thick blue lines. Intermediate levels of Im("ƒ") = constant are shown with thin green lines. Intermediate levels of Re("ƒ") = constant are shown with thin red lines for negative values and with thin blue lines for positive values.
The error function at +∞ is exactly 1 (see Gaussian integral). At the real axis, erf("z") approaches unity at "z" → +∞ and −1 at "z" → −∞. At the imaginary axis, it tends to ±"i"∞.
Taylor series.
The error function is an entire function; it has no singularities (except that at infinity) and its Taylor expansion always converges.
The defining integral cannot be evaluated in closed form in terms of elementary functions, but by expanding the integrand "e"−"z"2 into its Maclaurin series and integrating term by term, one obtains the error function's Maclaurin series as:
which holds for every complex number "z". The denominator terms are sequence in the OEIS.
For iterative calculation of the above series, the following alternative formulation may be useful:
because formula_18 expresses the multiplier to turn the "k"th term into the ("k" + 1)th term (considering "z" as the first term).
The imaginary error function has a very similar Maclaurin series, which is:
which holds for every complex number "z".
Derivative and integral.
The derivative of the error function follows immediately from its definition:
From this, the derivative of the imaginary error function is also immediate:
An antiderivative of the error function, obtainable by integration by parts, is
An antiderivative of the imaginary error function, also obtainable by integration by parts, is
Higher order derivatives are given by
where formula_25 are the probabilists' Hermite polynomials.
Bürmann series.
An expansion, which converges more rapidly for all real values of formula_26 than a Taylor expansion, is obtained by using Hans Heinrich Bürmann's theorem:
By keeping only the first two coefficients and choosing formula_28 and formula_29 , the resulting approximation shows its largest relative error at formula_30, where it is less than formula_31 :
Inverse functions.
Given complex number "z", there is not a "unique" complex number "w" satisfying formula_33, so a true inverse function would be multivalued. However, for -1<"x"<1, there is a unique "real" number denoted formula_34 satisfying formula_35.
The inverse error function is usually defined with domain (-1,1), and it is restricted to this domain in many computer algebra systems. However, it can be extended to the disk |"z"|<1 of the complex plane, using the Maclaurin series
where "c"0 = 1 and
So we have the series expansion (note that common factors have been canceled from numerators and denominators):
(After cancellation the numerator/denominator fractions are entries A092676/A132467 in the OEIS; without cancellation the numerator terms are given in entry A002067.) Note that the error function's value at ±∞ is equal to ±1.
For |"z"|<1, we have formula_39.
The inverse complementary error function is defined as
For "real" "x", there is a unique "real" number formula_41 satisfying formula_42. The inverse imaginary error function is defined as formula_41.
For any real "x", Newton's method can be used to compute formula_41, and for formula_45, the following Maclaurin series converges:
where "c""k" is defined as above.
Asymptotic expansion.
A useful asymptotic expansion of the complementary error function (and therefore also of the error function) for large real "x" is
where (2"n" – 1)!! is the double factorial: the product of all odd numbers up to (2"n" – 1). This series diverges for every finite "x", and its meaning as asymptotic expansion is that, for any formula_48 one has
where the remainder, in Landau notation, is
Indeed, the exact value of the remainder is
which follows easily by induction, writing formula_53 and integrating by parts.
For large enough values of x, only the first few terms of this asymptotic expansion are needed to obtain a good approximation of erfc("x") (while for not too large values of "x" note that the above Taylor expansion at 0 provides a very fast convergence).
Continued fraction expansion.
A continued fraction expansion of the complementary error function is:
Approximation with elementary functions.
Abramowitz and Stegun give several approximations of varying accuracy (equations 7.1.25–28). This allows one to choose the fastest approximation suitable for a given application. In order of increasing accuracy, they are:
where "a"1 = 0.278393, "a"2 = 0.230389, "a"3 = 0.000972, "a"4 = 0.078108
where "p" = 0.47047, "a"1 = 0.3480242, "a"2 = −0.0958798, "a"3 = 0.7478556
where "a"1 = 0.0705230784, "a"2 = 0.0422820123, "a"3 = 0.0092705272, "a"4 = 0.0001520143, "a"5 = 0.0002765672, "a"6 = 0.0000430638
where "p" = 0.3275911, "a"1 = 0.254829592, "a"2 = −0.284496736, "a"3 = 1.421413741, "a"4 = −1.453152027, "a"5 = 1.061405429
All of these approximations are valid for "x" ≥ 0. To use these approximations for negative "x", use the fact that erf(x) is an odd function, so erf("x") = −erf(−"x").
Another approximation is given by
where
This is designed to be very accurate in a neighborhood of 0 and a neighborhood of infinity, and the error is less than 0.00035 for all "x". Using the alternate value "a" ≈ 0.147 reduces the maximum error to about 0.00012.
This approximation can also be inverted to calculate the inverse error function:
Exponential bounds and a pure exponential approximation for the complementary error function are given by 
A single-term lower bound is
where the parameter "β" can be picked to minimize error on the desired interval of approximation.
Numerical approximation.
Over the complete range of values, there is an approximation with a maximal error of formula_66, as follows:
with
and
Applications.
When the results of a series of measurements are described by a normal distribution with standard deviation formula_70 and expected value 0, then formula_71 is the probability that the error of a single measurement lies between −"a" and +"a", for positive "a". This is useful, for example, in determining the bit error rate of a digital communication system.
The error and complementary error functions occur, for example, in solutions of the heat equation when boundary conditions are given by the Heaviside step function.
The error function and its approximations can be used to estimate results that hold with high probability. Given random variable formula_72 and constant formula_73:
where "A" and "B" are certain numeric constants. If "L" is sufficiently far from the mean, i.e. formula_75, then:
so the probability goes to 0 as formula_77.
Related functions.
The error function is essentially identical to the standard normal cumulative distribution function, denoted Φ, also named norm(x) by software languages, as they differ only by scaling and translation. Indeed,
or rearranged for erf and erfc:
Consequently, the error function is also closely related to the Q-function, which is the tail probability of the standard normal distribution. The Q-function can be expressed in terms of the error function as
The inverse of formula_81 is known as the normal quantile function, or probit function and may be expressed in terms of the inverse error function as
The standard normal cdf is used more often in probability and statistics, and the error function is used more often in other branches of mathematics.
The error function is a special case of the Mittag-Leffler function, and can also be expressed as a confluent hypergeometric function (Kummer's function):
It has a simple expression in terms of the Fresnel integral.
In terms of the regularized Gamma function P and the incomplete gamma function,
formula_85 is the sign function.
Generalized error functions.
Some authors discuss the more general functions:
Notable cases are:
After division by "n"!, all the "En" for odd "n" look similar (but not identical) to each other. Similarly, the "En" for even "n" look similar (but not identical) to each other after a simple division by "n"!. All generalised error functions for "n" > 0 look similar on the positive "x" side of the graph.
These generalised functions can equivalently be expressed for "x" > 0 using the Gamma function and incomplete Gamma function:
Therefore, we can define the error function in terms of the incomplete Gamma function:
Iterated integrals of the complementary error function.
The iterated integrals of the complementary error function are defined by
They have the power series
from which follow the symmetry properties
and

</doc>
<doc id="15686544" url="https://en.wikipedia.org/wiki?curid=15686544" title="Eta meson">
Eta meson

<br>
 :<br><br>
<br>
The eta () and eta prime meson () are mesons made of a mixture of up, down and strange quarks and their antiquarks. The charmed eta meson () and bottom eta meson () are forms of quarkonium; they have the same spin and parity as the light eta but are made of charm quarks and bottom quarks respectively. The top quark is too heavy to form a similar meson, due to its very fast decay.
General.
The eta was discovered in pion-nucleon collisions at the Bevatron in 1961 by A. Pevsner et al. at a time when the proposal of the Eightfold Way was leading to predictions and discoveries of new particles from symmetry considerations.
The difference between the mass of the "η" and that of the "η"' is larger than the quark model can naturally explain. This "η-η' puzzle" can be resolved by the 't Hooft instanton mechanism, whose 1/N realization is also known as Witten-Veneziano mechanism.
Quark composition.
The particles belong to the 
"pseudo-scalar" nonet of mesons which have spin "J" = 0 and 
negative parity, and and have zero total isospin, "I", and zero strangeness and hypercharge. Each quark which appears in an particle is accompanied by its antiquark (the particle overall is "flavourless") and all the main quantum numbers are zero.
The basic SU(3) symmetry theory of quarks for the three lightest quarks, which only takes into account the strong force, predicts corresponding particles 
The subscripts refer to the fact that "η"1 belongs to a singlet (which is fully antisymmetrical) and "η"8 is part of an octet. However in this case the weak and electromagnetic forces, which can transform one flavour of quark into another, cause a significant, though small, amount of "mixing" of the eigenstates (with mixing angle "θP" = −11.5 degrees), so that the actual quark composition is a linear combination of these formulae. That is:
The unsubscripted name refers to the real particle which is actually observed and which is close to the "η8". The is the observed particle close to "η1".
The and particles are closely related to the better-known neutral pion , where
In fact "π"0, "η"1 and "η"8 are three mutually orthogonal linear combinations of the quark pairs "u", "d" and "s"; they are at the centre of the pseudo-scalar nonet of mesons with all the main quantum numbers equal to zero.
Eta Prime Meson.
The Eta Prime Meson is essentially a superposition of the Eta Meson, the only significant differences being a higher mass, a different decay state, and a shorter lifetime. 

</doc>
<doc id="9417" url="https://en.wikipedia.org/wiki?curid=9417" title="Euclidean geometry">
Euclidean geometry

Euclidean geometry is a mathematical system attributed to the Alexandrian Greek mathematician Euclid, which he described in his textbook on geometry: the "Elements". Euclid's method consists in assuming a small set of intuitively appealing axioms, and deducing many other propositions (theorems) from these. Although many of Euclid's results had been stated by earlier mathematicians, Euclid was the first to show how these propositions could fit into a comprehensive deductive and logical system. The "Elements" begins with plane geometry, still taught in secondary school as the first axiomatic system and the first examples of formal proof. It goes on to the solid geometry of three dimensions. Much of the "Elements" states results of what are now called algebra and number theory, explained in geometrical language.
For more than two thousand years, the adjective "Euclidean" was unnecessary because no other sort of geometry had been conceived. Euclid's axioms seemed so intuitively obvious (with the possible exception of the parallel postulate) that any theorem proved from them was deemed true in an absolute, often metaphysical, sense. Today, however, many other self-consistent non-Euclidean geometries are known, the first ones having been discovered in the early 19th century. An implication of Albert Einstein's theory of general relativity is that physical space itself is not Euclidean, and Euclidean space is a good approximation for it only where the gravitational field is weak.
Euclidean geometry is an example of synthetic geometry, in that it proceeds logically from axioms to propositions without the use of coordinates. This is in contrast to analytic geometry, which uses coordinates.
"The Elements".
The "Elements" is mainly a systematization of earlier knowledge of geometry. Its improvement over earlier treatments was rapidly recognized, with the result that there was little interest in preserving the earlier ones, and they are now nearly all lost.
There are 13 total books in the "Elements":
Books I–IV and VI discuss plane geometry. Many results about plane figures are proved, e.g., "If a triangle has two equal angles, then the sides subtended by the angles are equal." The Pythagorean theorem is proved.
Books V and VII–X deal with number theory, with numbers treated geometrically via their representation as line segments with various lengths. Notions such as prime numbers and rational and irrational numbers are introduced. The infinitude of prime numbers is proved.
Books XI–XIII concern solid geometry. A typical result is the 1:3 ratio between the volume of a cone and a cylinder with the same height and base.
Axioms.
Euclidean geometry is an axiomatic system, in which all theorems ("true statements") are derived from a small number of axioms. Near the beginning of the first book of the "Elements", Euclid gives five postulates (axioms) for plane geometry, stated in terms of constructions (as translated by Thomas Heath):
"Let the following be postulated":
Although Euclid's statement of the postulates only explicitly asserts the existence of the constructions, they are also taken to be unique.
The "Elements" also include the following five "common notions":
Parallel postulate.
To the ancients, the parallel postulate seemed less obvious than the others. They were concerned with creating a system which was absolutely rigorous and to them it seemed as if the parallel line postulate should have been able to be proven rather than simply accepted as a fact. It is now known that such a proof is impossible. Euclid himself seems to have considered it as being qualitatively different from the others, as evidenced by the organization of the "Elements": the first 28 propositions he presents are those that can be proved without it.
Many alternative axioms can be formulated that have the same logical consequences as the parallel postulate. For example, Playfair's axiom states:
Methods of proof.
Euclidean Geometry is "constructive". Postulates 1, 2, 3, and 5 assert the existence and uniqueness of certain geometric figures, and these assertions are of a constructive nature: that is, we are not only told that certain things exist, but are also given methods for creating them with no more than a compass and an unmarked straightedge. In this sense, Euclidean geometry is more concrete than many modern axiomatic systems such as set theory, which often assert the existence of objects without saying how to construct them, or even assert the existence of objects that cannot be constructed within the theory. Strictly speaking, the lines on paper are "models" of the objects defined within the formal system, rather than instances of those objects. For example, a Euclidean straight line has no width, but any real drawn line will. Though nearly all modern mathematicians consider nonconstructive methods just as sound as constructive ones, Euclid's constructive proofs often supplanted fallacious nonconstructive ones—e.g., some of the Pythagoreans' proofs that involved irrational numbers, which usually required a statement such as "Find the greatest common measure of ..."
Euclid often used proof by contradiction. Euclidean geometry also allows the method of superposition, in which a figure is transferred to another point in space. For example, proposition I.4, side-angle-side congruence of triangles, is proved by moving one of the two triangles so that one of its sides coincides with the other triangle's equal side, and then proving that the other sides coincide as well. Some modern treatments add a sixth postulate, the rigidity of the triangle, which can be used as an alternative to superposition.
System of measurement and arithmetic.
Euclidean geometry has two fundamental types of measurements: angle and distance. The angle scale is absolute, and Euclid uses the right angle as his basic unit, so that, e.g., a 45-degree angle would be referred to as half of a right angle. The distance scale is relative; one arbitrarily picks a line segment with a certain nonzero length as the unit, and other distances are expressed in relation to it. Addition of distances is represented by a construction in which one line segment is copied onto the end of another line segment to extend its length, and similarly for subtraction.
Measurements of area and volume are derived from distances. For example, a rectangle with a width of 3 and a length of 4 has an area that represents the product, 12. Because this geometrical interpretation of multiplication was limited to three dimensions, there was no direct way of interpreting the product of four or more numbers, and Euclid avoided such products, although they are implied, e.g., in the proof of book IX, proposition 20.
Euclid refers to a pair of lines, or a pair of planar or solid figures, as "equal" (ἴσος) if their lengths, areas, or volumes are equal, and similarly for angles. The stronger term "congruent" refers to the idea that an entire figure is the same size and shape as another figure. Alternatively, two figures are congruent if one can be moved on top of the other so that it matches up with it exactly. (Flipping it over is allowed.) Thus, for example, a 2x6 rectangle and a 3x4 rectangle are equal but not congruent, and the letter R is congruent to its mirror image. Figures that would be congruent except for their differing sizes are referred to as similar. Corresponding angles in a pair of similar shapes are congruent and corresponding sides are in proportion to each other.
Notation and terminology.
Naming of points and figures.
Points are customarily named using capital letters of the alphabet. Other figures, such as lines, triangles, or circles, are named by listing a sufficient number of points to pick them out unambiguously from the relevant figure, e.g., triangle ABC would typically be a triangle with vertices at points A, B, and C.
Complementary and supplementary angles.
Angles whose sum is a right angle are called complementary. Complementary angles are formed when a ray shares the same vertex and is pointed in a direction that is in between the two original rays that form the right angle. The number of rays in between the two original rays is infinite.
Angles whose sum is a straight angle are supplementary. Supplementary angles are formed when a ray shares the same vertex and is pointed in a direction that is in between the two original rays that form the straight angle (180 degree angle). The number of rays in between the two original rays is infinite.
Modern versions of Euclid's notation.
In modern terminology, angles would normally be measured in degrees or radians.
Modern school textbooks often define separate figures called lines (infinite), rays (semi-infinite), and line segments (of finite length). Euclid, rather than discussing a ray as an object that extends to infinity in one direction, would normally use locutions such as "if the line is extended to a sufficient length," although he occasionally referred to "infinite lines." A "line" in Euclid could be either straight or curved, and he used the more specific term "straight line" when necessary.
Some important or well known results.
Pons Asinorum.
The Bridge of Asses ("Pons Asinorum") states that "in isosceles triangles the angles at the base equal one another, and, if the equal straight lines are produced further, then the angles under the base equal one another." Its name may be attributed to its frequent role as the first real test in the "Elements" of the intelligence of the reader and as a bridge to the harder propositions that followed. It might also be so named because of the geometrical figure's resemblance to a steep bridge that only a sure-footed donkey could cross.
Congruence of triangles.
Triangles are congruent if they have all three sides equal (SSS), two sides and the angle between them equal (SAS), or two angles and a side equal (ASA) (Book I, propositions 4, 8, and 26). Triangles with three equal angles (AAA) are similar, but not necessarily congruent. Also, triangles with two equal sides and an adjacent angle are not necessarily equal or congruent.
Triangle Angle Sum.
The sum of the angles of a triangle is equal to a straight angle (180 degrees). This causes an equilateral triangle to have 3 interior angles of 60 degrees. Also, it causes every triangle to have at least 2 acute angles and up to 1 obtuse or right angle.
Pythagorean theorem.
The celebrated Pythagorean theorem (book I, proposition 47) states that in any right triangle, the area of the square whose side is the hypotenuse (the side opposite the right angle) is equal to the sum of the areas of the squares whose sides are the two legs (the two sides that meet at a right angle).
Thales' theorem.
Thales' theorem, named after Thales of Miletus states that if A, B, and C are points on a circle where the line AC is a diameter of the circle, then the angle ABC is a right angle. Cantor supposed that Thales proved his theorem by means of Euclid Book I, Prop. 32 after the manner of Euclid Book III, Prop. 31. Tradition has it that Thales sacrificed an ox to celebrate this theorem.
Scaling of area and volume.
In modern terminology, the area of a plane figure is proportional to the square of any of its linear dimensions, formula_1, and the volume of a solid to the cube, formula_2. Euclid proved these results in various special cases such as the area of a circle and the volume of a parallelepipedal solid. Euclid determined some, but not all, of the relevant constants of proportionality. E.g., it was his successor Archimedes who proved that a sphere has 2/3 the volume of the circumscribing cylinder.
Applications.
Because of Euclidean geometry's fundamental status in mathematics, it would be impossible to give more than a representative sampling of applications here.
As suggested by the etymology of the word, one of the earliest reasons for interest in geometry was surveying, and certain practical results from Euclidean geometry, such as the right-angle property of the 3-4-5 triangle, were used long before they were proved formally. The fundamental types of measurements in Euclidean geometry are distances and angles, and both of these quantities can be measured directly by a surveyor. Historically, distances were often measured by chains such as Gunter's chain, and angles using graduated circles and, later, the theodolite.
An application of Euclidean solid geometry is the determination of packing arrangements, such as the problem of finding the most efficient packing of spheres in n dimensions. This problem has applications in error detection and correction.
Geometric optics uses Euclidean geometry to analyze the focusing of light by lenses and mirrors.
Geometry is used extensively in architecture.
Geometry can be used to design origami. Some classical construction problems of geometry are impossible using compass and straightedge, but can be solved using origami.
As a description of the structure of space.
Euclid believed that his axioms were self-evident statements about physical reality. Euclid's proofs depend upon assumptions perhaps not obvious in Euclid's fundamental axioms, in particular that certain movements of figures do not change their geometrical properties such as the lengths of sides and interior angles, the so-called "Euclidean motions", which include translations, reflections and rotations of figures. Taken as a physical description of space, postulate 2 (extending a line) asserts that space does not have holes or boundaries (in other words, space is homogeneous and unbounded); postulate 4 (equality of right angles) says that space is isotropic and figures may be moved to any location while maintaining congruence; and postulate 5 (the parallel postulate) that space is flat (has no intrinsic curvature).
As discussed in more detail below, Einstein's theory of relativity significantly modifies this view.
The ambiguous character of the axioms as originally formulated by Euclid makes it possible for different commentators to disagree about some of their other implications for the structure of space, such as whether or not it is infinite (see below) and what its topology is. Modern, more rigorous reformulations of the system typically aim for a cleaner separation of these issues. Interpreting Euclid's axioms in the spirit of this more modern approach, axioms 1-4 are consistent with either infinite or finite space (as in elliptic geometry), and all five axioms are consistent with a variety of topologies (e.g., a plane, a cylinder, or a torus for two-dimensional Euclidean geometry).
Later work.
Archimedes and Apollonius.
Archimedes (ca. 287 BCE – ca. 212 BCE), a colorful figure about whom many historical anecdotes are recorded, is remembered along with Euclid as one of the greatest of ancient mathematicians. Although the foundations of his work were put in place by Euclid, his work, unlike Euclid's, is believed to have been entirely original. He proved equations for the volumes and areas of various figures in two and three dimensions, and enunciated the Archimedean property of finite numbers.
Apollonius of Perga (ca. 262 BCE–ca. 190 BCE) is mainly known for his investigation of conic sections.
17th century: Descartes.
René Descartes (1596–1650) developed analytic geometry, an alternative method for formalizing geometry which focused on turning geometry into algebra.
In this approach, a point on a plane is represented by its Cartesian ("x", "y") coordinates, a line is represented by its equation, and so on.
In Euclid's original approach, the Pythagorean theorem follows from Euclid's axioms. In the Cartesian approach, the axioms are the axioms of algebra, and the equation expressing the Pythagorean theorem is then a definition of one of the terms in Euclid's axioms, which are now considered theorems.
The equation
defining the distance between two points "P" = ("px", "py") and "Q" = ("qx", "qy") is then known as the "Euclidean metric", and other metrics define non-Euclidean geometries.
In terms of analytic geometry, the restriction of classical geometry to compass and straightedge constructions means a restriction to first- and second-order equations, e.g., "y" = 2"x" + 1 (a line), or "x"2 + "y"2 = 7 (a circle).
Also in the 17th century, Girard Desargues, motivated by the theory of perspective, introduced the concept of idealized points, lines, and planes at infinity. The result can be considered as a type of generalized geometry, projective geometry, but it can also be used to produce proofs in ordinary Euclidean geometry in which the number of special cases is reduced.
18th century.
Geometers of the 18th century struggled to define the boundaries of the Euclidean system. Many tried in vain to prove the fifth postulate from the first four. By 1763 at least 28 different proofs had been published, but all were found incorrect.
Leading up to this period, geometers also tried to determine what constructions could be accomplished in Euclidean geometry. For example, the problem of trisecting an angle with a compass and straightedge is one that naturally occurs within the theory, since the axioms refer to constructive operations that can be carried out with those tools. However, centuries of efforts failed to find a solution to this problem, until Pierre Wantzel published a proof in 1837 that such a construction was impossible. Other constructions that were proved impossible include doubling the cube and squaring the circle. In the case of doubling the cube, the impossibility of the construction originates from the fact that the compass and straightedge method involve equations whose order is an integral power of two, while doubling a cube requires the solution of a third-order equation.
Euler discussed a generalization of Euclidean geometry called affine geometry, which retains the fifth postulate unmodified while weakening postulates three and four in a way that eliminates the notions of angle (whence right triangles become meaningless) and of equality of length of line segments in general (whence circles become meaningless) while retaining the notions of parallelism as an equivalence relation between lines, and equality of length of parallel line segments (so line segments continue to have a midpoint).
19th century and non-Euclidean geometry.
In the early 19th century, Carnot and Möbius systematically developed the use of signed angles and line segments as a way of simplifying and unifying results.
The century's most significant development in geometry occurred when, around 1830, János Bolyai and Nikolai Ivanovich Lobachevsky separately published work on non-Euclidean geometry, in which the parallel postulate is not valid. Since non-Euclidean geometry is provably relatively consistent with Euclidean geometry, the parallel postulate cannot be proved from the other postulates.
In the 19th century, it was also realized that Euclid's ten axioms and common notions do not suffice to prove all of the theorems stated in the "Elements". For example, Euclid assumed implicitly that any line contains at least two points, but this assumption cannot be proved from the other axioms, and therefore must be an axiom itself. The very first geometric proof in the "Elements," shown in the figure above, is that any line segment is part of a triangle; Euclid constructs this in the usual way, by drawing circles around both endpoints and taking their intersection as the third vertex. His axioms, however, do not guarantee that the circles actually intersect, because they do not assert the geometrical property of continuity, which in Cartesian terms is equivalent to the completeness property of the real numbers. Starting with Moritz Pasch in 1882, many improved axiomatic systems for geometry have been proposed, the best known being those of Hilbert, George Birkhoff, and Tarski.
20th century and general relativity.
Einstein's theory of general relativity shows that the true geometry of spacetime is not Euclidean geometry. For example, if a triangle is constructed out of three rays of light, then in general the interior angles do not add up to 180 degrees due to gravity. A relatively weak gravitational field, such as the Earth's or the sun's, is represented by a metric that is approximately, but not exactly, Euclidean. Until the 20th century, there was no technology capable of detecting the deviations from Euclidean geometry, but Einstein predicted that such deviations would exist. They were later verified by observations such as the slight bending of starlight by the Sun during a solar eclipse in 1919, and such considerations are now an integral part of the software that runs the GPS system. It is possible to object to this interpretation of general relativity on the grounds that light rays might be improper physical models of Euclid's lines, or that relativity could be rephrased so as to avoid the geometrical interpretations. However, one of the consequences of Einstein's theory is that there is no possible physical test that can distinguish between a beam of light as a model of a geometrical line and any other physical model. Thus, the only logical possibilities are to accept non-Euclidean geometry as physically real, or to reject the entire notion of physical tests of the axioms of geometry, which can then be imagined as a formal system without any intrinsic real-world meaning.
Treatment of infinity.
Infinite objects.
Euclid sometimes distinguished explicitly between "finite lines" (e.g., Postulate 2) and "infinite lines" (book I, proposition 12). However, he typically did not make such distinctions unless they were necessary. The postulates do not explicitly refer to infinite lines, although for example some commentators interpret postulate 3, existence of a circle with any radius, as implying that space is infinite.
The notion of infinitesimal quantities had previously been discussed extensively by the Eleatic School, but nobody had been able to put them on a firm logical basis, with paradoxes such as Zeno's paradox occurring that had not been resolved to universal satisfaction. Euclid used the method of exhaustion rather than infinitesimals.
Later ancient commentators such as Proclus (410–485 CE) treated many questions about infinity as issues demanding proof and, e.g., Proclus claimed to prove the infinite divisibility of a line, based on a proof by contradiction in which he considered the cases of even and odd numbers of points constituting it.
At the turn of the 20th century, Otto Stolz, Paul du Bois-Reymond, Giuseppe Veronese, and others produced controversial work on non-Archimedean models of Euclidean geometry, in which the distance between two points may be infinite or infinitesimal, in the Newton–Leibniz sense. Fifty years later, Abraham Robinson provided a rigorous logical foundation for Veronese's work.
Infinite processes.
One reason that the ancients treated the parallel postulate as less certain than the others is that verifying it physically would require us to inspect two lines to check that they never intersected, even at some very distant point, and this inspection could potentially take an infinite amount of time.
The modern formulation of proof by induction was not developed until the 17th century, but some later commentators consider it implicit in some of Euclid's proofs, e.g., the proof of the infinitude of primes.
Supposed paradoxes involving infinite series, such as Zeno's paradox, predated Euclid. Euclid avoided such discussions, giving, for example, the expression for the partial sums of the geometric series in IX.35 without commenting on the possibility of letting the number of terms become infinite.
Logical basis.
Classical logic.
Euclid frequently used the method of proof by contradiction, and therefore the traditional presentation of Euclidean geometry assumes classical logic, in which every proposition is either true or false, i.e., for any proposition P, the proposition "P or not P" is automatically true.
Modern standards of rigor.
Placing Euclidean geometry on a solid axiomatic basis was a preoccupation of mathematicians for centuries. The role of primitive notions, or undefined concepts, was clearly put forward by Alessandro Padoa of the Peano delegation at the 1900 Paris conference: 
That is, mathematics is context-independent knowledge within a hierarchical framework. As said by Bertrand Russell:
Such foundational approaches range between foundationalism and formalism.
Constructive approaches and pedagogy.
The process of abstract axiomatization as exemplified by Hilbert's axioms reduces geometry to theorem proving or predicate logic. In contrast, the Greeks used construction postulates, and emphasized problem solving. For the Greeks, constructions are more primitive than existence propositions, and can be used to prove existence propositions, but not "vice versa". To describe problem solving adequately requires a richer system of logical concepts. The contrast in approach may be summarized:
Andrei Nicholaevich Kolmogorov proposed a problem solving basis for geometry. This work was a precursor of a modern formulation in terms of constructive type theory. This development has implications for pedagogy as well.

</doc>
<doc id="9697" url="https://en.wikipedia.org/wiki?curid=9697" title="Euclidean space">
Euclidean space

In geometry, Euclidean space encompasses the two-dimensional Euclidean plane, the three-dimensional space of Euclidean geometry, and certain other spaces. It is named after the Ancient Greek mathematician Euclid of Alexandria. The term "Euclidean" distinguishes these spaces from other types of spaces considered in modern geometry. Euclidean spaces also generalize to higher dimensions.
Classical Greek geometry defined the Euclidean plane and Euclidean three-dimensional space using certain postulates, while the other properties of these spaces were deduced as theorems. Geometric constructions are also used to define rational numbers. When algebra and mathematical analysis became developed enough, this relation reversed and now it is more common to define Euclidean space using Cartesian coordinates and the ideas of analytic geometry. It means that points of the space are specified with collections of real numbers, and geometric shapes are defined as equations and inequalities. This approach brings the tools of algebra and calculus to bear on questions of geometry and has the advantage that it generalizes easily to Euclidean spaces of more than three dimensions.
From the modern viewpoint, there is essentially only one Euclidean space of each dimension. With Cartesian coordinates it is modelled by the real coordinate space () of the same dimension. In one dimension, this is the real line; in two dimensions, it is the Cartesian plane; and in higher dimensions it is a coordinate space with three or more real number coordinates. Mathematicians denote the -dimensional Euclidean space by if they wish to emphasize its Euclidean nature, but is used as well since the latter is assumed to have the standard Euclidean structure, and these two structures are not always distinguished. Euclidean spaces have finite dimension.
Intuitive overview.
One way to think of the Euclidean plane is as a set of points satisfying certain relationships, expressible in terms of distance and angle. For example, there are two fundamental operations (referred to as motions) on the plane. One is translation, which means a shifting of the plane so that every point is shifted in the same direction and by the same distance. The other is rotation about a fixed point in the plane, in which every point in the plane turns about that fixed point through the same angle. One of the basic tenets of Euclidean geometry is that two figures (usually considered as subsets) of the plane should be considered equivalent (congruent) if one can be transformed into the other by some sequence of translations, rotations and reflections (see below).
In order to make all of this mathematically , the theory must clearly define the notions of distance, angle, translation, and rotation for a mathematically described space. Even when used in physical theories, Euclidean space is an abstraction detached from actual physical locations, specific reference frames, measurement instruments, and so on. A purely mathematical definition of Euclidean space also ignores questions of units of length and other physical dimensions: the distance in a "mathematical" space is a number, not something expressed in inches or metres. The standard way to define such space, as carried out in the remainder of this article, is to define the Euclidean plane as a two-dimensional real vector space equipped with an inner product. The reason for working with arbitrary vector spaces instead of is that it is often preferable to work in a "coordinate-free" manner (that is, without choosing a preferred basis). For then:
Once the Euclidean plane has been described in this language, it is actually a simple matter to extend its concept to arbitrary dimensions. For the most part, the vocabulary, formulae, and calculations are not made any more difficult by the presence of more dimensions. (However, rotations are more subtle in high dimensions, and visualizing high-dimensional spaces remains difficult, even for experienced mathematicians.)
A Euclidean space is not technically a vector space but rather an affine space, on which a vector space acts by translations, or, conversely, a Euclidean vector is the difference (displacement) in an ordered pair of points, not a single point. Intuitively, the distinction says merely that there is no canonical choice of where the origin should go in the space, because it can be translated anywhere. When a certain point is chosen, it can be declared the origin and subsequent calculations may ignore the difference between a point and its coordinate vector, as said above. See point–vector distinction for details.
Euclidean structure.
These are distances between points and the angles between lines or vectors, which satisfy certain conditions (see below), which makes a set of points a Euclidean space. The natural way to obtain these quantities is by introducing and using the standard inner product (also known as the dot product) on . The inner product of any two real -vectors and is defined by
where and are th coordinates of vectors and respectively.
The result is always a real number.
Distance.
The inner product of with itself is always non-negative. This product allows us to define the "length" of a vector through square root:
This length function satisfies the required properties of a norm and is called the Euclidean norm on .
Finally, one can use the norm to define a metric (or distance function) on by
This distance function is called the Euclidean metric. This formula expresses a special case of the Pythagorean theorem.
This distance function (which makes a metric space) is sufficient to define all Euclidean geometry, including the dot product. Thus, a real coordinate space together with this Euclidean structure is called Euclidean space. Its vectors form an inner product space (in fact a Hilbert space), and a normed vector space.
The metric space structure is the main reason behind the use of real numbers , not some other ordered field, as the mathematical foundation of Euclidean (and many other) spaces. Euclidean space is a complete metric space, a property which is impossible to achieve operating over rational numbers, for example.
Angle.
The (non-reflex) angle () between vectors and is then given by
where is the arccosine function. It is useful only for , and the case is somewhat special. Namely, on an oriented Euclidean plane one can define an angle between two vectors as a number defined modulo 1 turn (usually denoted as either or 360°), such that . This oriented angle is equal either to the angle from the formula above or to . If one non-zero vector is fixed (such as the first basis vector), then each non-zero vector is uniquely defined by its magnitude and angle.
The angle does not change if vectors and are multiplied by positive numbers.
Unlike the aforementioned situation with distance, the scale of angles is the same in pure mathematics, physics, and computing. It does not depend on the scale of distances; all distances may be multiplied by some fixed factor, and all angles will be preserved. Usually, the angle is considered a dimensionless quantity, but there are different units of measurement, such as radian (preferred in pure mathematics and theoretical physics) and degree (°) (preferred in most applications).
Rotations and reflections.
Symmetries of a Euclidean space are transformations which preserve the Euclidean metric (called "isometries"). Although aforementioned translations are most obvious of them, they have the same structure for any affine space and do not show a distinctive character of Euclidean geometry. Another family of symmetries leave one point fixed, which may be seen as the origin without loss of generality. All transformations, which preserves the origin and the Euclidean metric, are linear maps. Such transformations must, for any and , satisfy:
Such transforms constitute a group called the "orthogonal group" . Its elements are exactly solutions of a matrix equation
where T is the transpose of and is the identity matrix.
But a Euclidean space is orientable. Each of these transformations either preserves or reverses orientation depending on whether its determinant is +1 or −1 respectively. Only transformations which preserve orientation, which form the "special orthogonal" group , are considered (proper) rotations. This group has, as a Lie group, the same dimension and is the identity component of .
Groups are well-studied for . There are no non-trivial rotations in 0- and 1-spaces. Rotations of a Euclidean plane () are parametrized by the angle (modulo 1 turn). Rotations of a 3-space are parametrized with axis and angle, whereas a rotation of a 4-space is a superposition of two 2-dimensional rotations around perpendicular planes.
Among linear transforms in which reverse the orientation are hyperplane reflections. This is the only possible case for , but starting from three dimensions, such isometry in the general position is a rotoreflection.
Euclidean group.
The Euclidean group , also referred to as the group of all isometries , treats translations, rotations, and reflections in a uniform way, considering them as group actions in the context of group theory, and especially in Lie group theory. These group actions preserve the Euclidean structure.
As the group of all isometries, , the Euclidean group is important because it makes Euclidean geometry a case of Klein geometry, a theoretical framework including many alternative geometries.
The structure of Euclidean spaces – distances, lines, vectors, angles (up to sign), and so on – is invariant under the transformations of their associated Euclidean group. For instance, translations form a commutative subgroup that acts freely and transitively on , while the stabilizer of any point there is the aforementioned.
Along with translations, rotations, reflections, as well as the identity transformation, Euclidean motions comprise also glide reflections (for ), screw operations and rotoreflections (for ), and even more complex combinations of primitive transformations for .
The group structure determines which conditions a metric space needs to satisfy to be a Euclidean space:
Non-Cartesian coordinates.
Cartesian coordinates are arguably the standard, but not the only possible option for a Euclidean space.
Skew coordinates are compatible with the affine structure of , but make formulae for angles and distances more complicated.
Another approach, which goes in line with ideas of differential geometry and conformal geometry, is orthogonal coordinates, where coordinate hypersurfaces of different coordinates are orthogonal, although curved. Examples include the polar coordinate system on Euclidean plane, the second important plane coordinate system.
See below about expression of the Euclidean structure in curvilinear coordinates.
Geometric shapes.
Lines, planes, and other subspaces.
The simplest (after points) objects in Euclidean space are flats, or Euclidean "subspaces" of lesser dimension. Points are 0-dimensional flats, 1-dimensional flats are called "(straight) lines", and 2-dimensional flats are "planes". -dimensional flats are called "hyperplanes".
Any two distinct points lie on exactly one line. Any line and a point outside it lie on exactly one plane. More generally, the properties of flats and their incidence of Euclidean space are shared with affine geometry, whereas the affine geometry is devoid of distances and angles.
Line segments and triangles.
This is not only a line which a pair of distinct points defines. Points on the line which lie between and , together with and themselves, constitute a line segment . Any line segment has the length, which equals to distance between and . If , then the segment is degenerate and its length equals to 0, otherwise the length is positive.
A (non-degenerate) triangle is defined by three points not lying on the same line. Any triangle lies on one plane. The concept of triangle is not specific to Euclidean spaces, but Euclidean triangles have numerous special properties and define many derived objects.
A triangle can be thought of as a 3-gon on a plane, a special (and the first meaningful in Euclidean geometry) case of a polygon.
Polytopes and root systems.
Polytope is a concept that generalizes polygons on a plane and polyhedra in 3-dimensional space (which are among the earliest studied geometrical objects). A simplex is a generalization of a line segment (1-simplex) and a triangle (2-simplex). A tetrahedron is a 3-simplex.
The concept of a polytope belongs to affine geometry, which is more general than Euclidean. But Euclidean geometry distinguish "regular polytopes". For example, affine geometry does not see the difference between an equilateral triangle and a right triangle, but in Euclidean space the former is regular and the latter is not.
Root systems are special sets of Euclidean vectors. A root system is often identical to the set of vertices of a regular polytope.
Topology.
Since Euclidean space is a metric space, it is also a topological space with the natural topology induced by the metric. The metric topology on is called the Euclidean topology, and it is identical to the standard topology on . A set is open if and only if it contains an open ball around each of its points; in other words, open balls form a base of the topology. The topological dimension of the Euclidean -space equals , which implies that spaces of different dimension are not homeomorphic. A finer result is the invariance of domain, which proves that any subset of -space, that is (with its subspace topology) homeomorphic to an open subset of -space, is itself open.
Applications.
Aside from countless uses in fundamental mathematics, a Euclidean model of the physical space can be used to solve many practical problems with sufficient precision. Two usual approaches are a fixed, or "stationary" reference frame (i.e. the description of a motion of objects as their positions that change continuously with time), and the use of Galilean space-time symmetry (such as in Newtonian mechanics). To both of them the modern Euclidean geometry provides a convenient formalism; for example, the space of Galilean velocities is itself a Euclidean space (see relative velocity for details).
Topographical maps and technical drawings are planar Euclidean. An idea behind them is the scale invariance of Euclidean geometry, that permits to represent large objects in a small sheet of paper, or a screen.
Alternatives and generalizations.
Although Euclidean spaces are no longer considered to be the only possible setting for a geometry, they act as prototypes for other geometric objects. Ideas and terminology from Euclidean geometry (both traditional and analytic) are pervasive in modern mathematics, where other geometric objects share many similarities with Euclidean spaces, share part of their structure, or embed Euclidean spaces.
Curved spaces.
A smooth manifold is a Hausdorff topological space that is locally diffeomorphic to Euclidean space. Diffeomorphism does not respect distance and angle, but if one additionally prescribes a smoothly varying inner product on the manifold's tangent spaces, then the result is what is called a Riemannian manifold. Put differently, a manifold is a space constructed by deforming and patching together Euclidean spaces. Such a space enjoys notions of distance and angle, but they behave in a curved, non-Euclidean manner. The simplest Riemannian manifold, consisting of with a constant inner product, is essentially identical to Euclidean -space itself. Less trivial examples are -sphere and hyperbolic spaces. Discovery of the latter in the 19th century was branded as the non-Euclidean geometry.
Also, the concept of a Riemannian manifold permits an expression of the Euclidean structure in any smooth coordinate system, via metric tensor. From this tensor one can compute the Riemann curvature tensor. Where the latter equals to zero, the metric structure is locally Euclidean (it means that at least some open set in the coordinate space is isometric to a piece of Euclidean space), no matter whether coordinates are affine or curvilinear.
Indefinite quadratic form.
If one replaces the inner product of a Euclidean space with an indefinite quadratic form, the result is a pseudo-Euclidean space. Smooth manifolds built from such spaces are called pseudo-Riemannian manifolds. Perhaps their most famous application is the theory of relativity, where flat spacetime is a pseudo-Euclidean space called Minkowski space, where rotations correspond to motions of hyperbolic spaces mentioned above. Further generalization to curved spacetimes form pseudo-Riemannian manifolds, such as in general relativity.
Other number fields.
Another line of generalization is to consider other number fields than one of real numbers. Over complex numbers, a Hilbert space can be seen as a generalization of Euclidean dot product structure, although the definition of the inner product becomes a sesquilinear form for compatibility with metric structure.

</doc>
<doc id="32533" url="https://en.wikipedia.org/wiki?curid=32533" title="Euclidean vector">
Euclidean vector

In mathematics, physics, and engineering, a Euclidean vector (sometimes called a geometric or spatial vector, or—as here—simply a vector) is a geometric object that has magnitude (or length) and direction and can be added to other vectors according to vector algebra. A Euclidean vector is frequently represented by a line segment with a definite direction, or graphically as an arrow, connecting an "initial point" "A" with a "terminal point" "B", and denoted by formula_1
A vector is what is needed to "carry" the point "A" to the point "B"; the Latin word "vector" means "carrier". It was first used by 18th century astronomers investigating planet rotation around the Sun. The magnitude of the vector is the distance between the two points and the direction refers to the direction of displacement from "A" to "B". Many algebraic operations on real numbers such as addition, subtraction, multiplication, and negation have close analogues for vectors, operations which obey the familiar algebraic laws of commutativity, associativity, and distributivity. These operations and associated laws qualify Euclidean vectors as an example of the more generalized concept of vectors defined simply as elements of a vector space.
Vectors play an important role in physics: velocity and acceleration of a moving object and forces acting on it are all described by vectors. Many other physical quantities can be usefully thought of as vectors. Although most of them do not represent distances (except, for example, position or displacement), their magnitude and direction can be still represented by the length and direction of an arrow. The mathematical representation of a physical vector depends on the coordinate system used to describe it. Other vector-like objects that describe physical quantities and transform in a similar way under changes of the coordinate system include pseudovectors and tensors.
History.
The concept of vector, as we know it today, evolved gradually over a period of more than 200 years. About a dozen people made significant contributions.
Giusto Bellavitis abstracted the basic idea in 1835 when he established the concept of equipollence. Working in a Euclidean plane, he made equipollent any pair of line segments of the same length and orientation. Essentially he realized an equivalence relation on the pairs of points in the plane and thus erected the first space of vectors in the plane.
The term vector was introduced by William Rowan Hamilton as part of his system of quaternions "q" = "s" + "v" where "scalar" s ∈ ℝ and "vector" "v" ∈ ℝ3. Thus Hamilton's vectors are 3-dimensional. Like Bellavitis, Hamilton viewed vectors as representative of classes of equipollent directed segments. As complex numbers use an imaginary unit to complement the real line, Hamilton considered vectors "v" to be the "imaginary part" of quaternions:
Several other mathematicians developed vector-like systems in the middle of the nineteenth century, including Augustin Cauchy, Hermann Grassmann, August Möbius, Comte de Saint-Venant, and Matthew O'Brien. Grassmann's 1840 work "Theorie der Ebbe und Flut" (Theory of the Ebb and Flow) was the first system of spatial analysis similar to today's system and had ideas corresponding to the cross product, scalar product and vector differentiation. Grassmann's work was largely neglected until the 1870s.
Peter Guthrie Tait carried the quaternion standard after Hamilton. His 1867 "Elementary Treatise of Quaternions" included extensive treatment of the nabla or del operator ∇.
In 1878 "Elements of Dynamic" was published by William Kingdon Clifford. Clifford simplified the quaternion study by isolating the dot product and cross product of two vectors from the complete quaternion product. This approach made vector calculations available to engineers and others working in three dimensions and skeptical of the fourth.
Josiah Willard Gibbs, who was exposed to quaternions through James Clerk Maxwell's "Treatise on Electricity and Magnetism", separated off their vector part for independent treatment. The first half of Gibbs's "Elements of Vector Analysis", published in 1881, presents what is essentially the modern system of vector analysis. In 1901 Edwin Bidwell Wilson published "Vector Analysis", adapted from Gibb's lectures, which banished any mention of quaternions in the development of vector calculus.
Overview.
In physics and engineering, a vector is typically regarded as a geometric entity characterized by a magnitude and a direction. It is formally defined as a directed line segment, or arrow, in a Euclidean space. In pure mathematics, a vector is defined more generally as any element of a vector space. In this context, vectors are abstract entities which may or may not be characterized by a magnitude and a direction. This generalized definition implies that the above-mentioned geometric entities are a special kind of vectors, as they are elements of a special kind of vector space called Euclidean space.
This article is about vectors strictly defined as arrows in Euclidean space. When it becomes necessary to distinguish these special vectors from vectors as defined in pure mathematics, they are sometimes referred to as geometric, spatial, or Euclidean vectors.
Being an arrow, a Euclidean vector possesses a definite "initial point" and "terminal point". A vector with fixed initial and terminal point is called a bound vector. When only the magnitude and direction of the vector matter, then the particular initial point is of no importance, and the vector is called a free vector. Thus two arrows formula_2 and formula_3 in space represent the same free vector if they have the same magnitude and direction: that is, they are equivalent if the quadrilateral "ABB′A′" is a parallelogram. If the Euclidean space is equipped with a choice of origin, then a free vector is equivalent to the bound vector of the same magnitude and direction whose initial point is the origin.
The term "vector" also has generalizations to higher dimensions and to more formal approaches with much wider applications.
Examples in one dimension.
Since the physicist's concept of force has a direction and a magnitude, it may be seen as a vector. As an example, consider a rightward force "F" of 15 newtons. If the positive axis is also directed rightward, then "F" is represented by the vector 15 N, and if positive points leftward, then the vector for "F" is −15 N. In either case, the magnitude of the vector is 15 N. Likewise, the vector representation of a displacement Δ"s" of 4 meters to the right would be 4 m or −4 m, and its magnitude would be 4 m regardless.
In physics and engineering.
Vectors are fundamental in the physical sciences. They can be used to represent any quantity that has magnitude, has direction, and which adheres to the rules of vector addition. An example is velocity, the magnitude of which is speed. For example, the velocity "5 meters per second upward" could be represented by the vector (0,5) (in 2 dimensions with the positive "y" axis as 'up'). Another quantity represented by a vector is force, since it has a magnitude and direction and follows the rules of vector addition. Vectors also describe many other physical quantities, such as linear displacement, displacement, linear acceleration, angular acceleration, linear momentum, and angular momentum. Other physical vectors, such as the electric and magnetic field, are represented as a system of vectors at each point of a physical space; that is, a vector field. Examples of quantities that have magnitude and direction but fail to follow the rules of vector addition: Angular displacement and electric current. Consequently, these are not vectors.
In Cartesian space.
In the Cartesian coordinate system, a vector can be represented by identifying the coordinates of its initial and terminal point. For instance, the points "A" = (1,0,0) and "B" = (0,1,0) in space determine the free vector formula_2 pointing from the point "x"=1 on the "x"-axis to the point "y"=1 on the "y"-axis.
Typically in Cartesian coordinates, one considers primarily bound vectors. A bound vector is determined by the coordinates of the terminal point, its initial point always having the coordinates of the origin "O" = (0,0,0). Thus the bound vector represented by (1,0,0) is a vector of unit length pointing from the origin along the positive "x"-axis.
The coordinate representation of vectors allows the algebraic features of vectors to be expressed in a convenient numerical fashion. For example, the sum of the vectors (1,2,3) and (−2,0,4) is the vector
Euclidean and affine vectors.
In the geometrical and physical settings, sometimes it is possible to associate, in a natural way, a "length" or magnitude and a direction to vectors. In turn, the notion of direction is strictly associated with the notion of an "angle" between two vectors. When the length of vectors is defined, it is possible to also define a dot product — a scalar-valued product of two vectors — which gives a convenient algebraic characterization of both length (the square root of the dot product of a vector by itself) and angle (a function of the dot product between any two non-zero vectors). In three dimensions, it is further possible to define a cross product which supplies an algebraic characterization of the area and orientation in space of the parallelogram defined by two vectors (used as sides of the parallelogram).
However, it is not always possible or desirable to define the length of a vector in a natural way. This more general type of spatial vector is the subject of vector spaces (for bound vectors) and affine spaces (for free vectors). An important example is Minkowski space that is important to our understanding of special relativity, where there is a generalization of length that permits non-zero vectors to have zero length. Other physical examples come from thermodynamics, where many of the quantities of interest can be considered vectors in a space with no notion of length or angle.
Generalizations.
In physics, as well as mathematics, a vector is often identified with a tuple of components, or list of numbers, that act as scalar coefficients for a set of basis vectors. When the basis is transformed, for example by rotation or stretching, then the components of any vector in terms of that basis also transform in an opposite sense. The vector itself has not changed, but the basis has, so the components of the vector must change to compensate. The vector is called "covariant" or "contravariant" depending on how the transformation of the vector's components is related to the transformation of the basis. In general, contravariant vectors are "regular vectors" with units of distance (such as a displacement) or distance times some other unit (such as velocity or acceleration); covariant vectors, on the other hand, have units of one-over-distance such as gradient. If you change units (a special case of a change of basis) from meters to millimeters, a scale factor of 1/1000, a displacement of 1 m becomes 1000 mm–a contravariant change in numerical value. In contrast, a gradient of 1 K/m becomes 0.001 K/mm–a covariant change in value. See covariance and contravariance of vectors. Tensors are another type of quantity that behave in this way; a vector is one type of tensor.
In pure mathematics, a vector is any element of a vector space over some field and is often represented as a coordinate vector. The vectors described in this article are a very special case of this general definition because they are contravariant with respect to the ambient space. Contravariance captures the physical intuition behind the idea that a vector has "magnitude and direction".
Representations.
Vectors are usually denoted in lowercase boldface, as a or lowercase italic boldface, as a. (Uppercase letters are typically used to represent matrices.) Other conventions include formula_5 or "a", especially in handwriting. Alternatively, some use a tilde (~) or a wavy underline drawn beneath the symbol, e.g. formula_6, which is a convention for indicating boldface type. If the vector represents a directed distance or displacement from a point "A" to a point "B" (see figure), it can also be denoted as formula_2 or "AB". Especially in literature in German it was common to represent vectors with small fraktur letters as formula_8.
Vectors are usually shown in graphs or other diagrams as arrows (directed line segments), as illustrated in the figure. Here the point "A" is called the "origin", "tail", "base", or "initial point"; point "B" is called the "head", "tip", "endpoint", "terminal point" or "final point". The length of the arrow is proportional to the vector's magnitude, while the direction in which the arrow points indicates the vector's direction.
On a two-dimensional diagram, sometimes a vector perpendicular to the plane of the diagram is desired. These vectors are commonly shown as small circles. A circle with a dot at its centre (Unicode U+2299 ⊙) indicates a vector pointing out of the front of the diagram, toward the viewer. A circle with a cross inscribed in it (Unicode U+2297 ⊗) indicates a vector pointing into and behind the diagram. These can be thought of as viewing the tip of an arrow head on and viewing the flights of an arrow from the back.
In order to calculate with vectors, the graphical representation may be too cumbersome. Vectors in an "n"-dimensional Euclidean space can be represented as coordinate vectors in a Cartesian coordinate system. The endpoint of a vector can be identified with an ordered list of "n" real numbers ("n"-tuple). These numbers are the coordinates of the endpoint of the vector, with respect to a given Cartesian coordinate system, and are typically called the scalar components (or scalar projections) of the vector on the axes of the coordinate system.
As an example in two dimensions (see figure), the vector from the origin "O" = (0,0) to the point "A" = (2,3) is simply written as
The notion that the tail of the vector coincides with the origin is implicit and easily understood. Thus, the more explicit notation formula_10 is usually not deemed necessary and very rarely used.
In "three dimensional" Euclidean space (or ), vectors are identified with triples of scalar components:
This can be generalised to "n-dimensional" Euclidean space (or ).
These numbers are often arranged into a column vector or row vector, particularly when dealing with matrices, as follows:
Another way to represent a vector in "n"-dimensions is to introduce the standard basis vectors. For instance, in three dimensions, there are three of them:
These have the intuitive interpretation as vectors of unit length pointing up the "x", "y", and "z" axis of a Cartesian coordinate system, respectively. In terms of these, any vector a in can be expressed in the form:
or
where a1, a2, a3 are called the vector components (or vector projections) of a on the basis vectors or, equivalently, on the corresponding Cartesian axes "x", "y", and "z" (see figure), while "a"1, "a"2, "a"3 are the respective scalar components (or scalar projections).
In introductory physics textbooks, the standard basis vectors are often instead denoted formula_19 (or formula_20, in which the hat symbol ^ typically denotes unit vectors). In this case, the scalar and vector components are denoted respectively "a"x, "a"y, "a"z, and ax, ay, az (note the difference in boldface). Thus,
The notation e"i" is compatible with the index notation and the summation convention commonly used in higher level mathematics, physics, and engineering.
Decomposition.
As explained above a vector is often described by a set of vector components that add up to form the given vector. Typically, these components are the projections of the vector on a set of mutually perpendicular reference axes (basis vectors). The vector is said to be "decomposed" or "resolved with respect to" that set.
However, the decomposition of a vector into components is not unique, because it depends on the choice of the axes on which the vector is projected.
Moreover, the use of Cartesian unit vectors such as formula_20 as a basis in which to represent a vector is not mandated. Vectors can also be expressed in terms of an arbitrary basis, including the unit vectors of a cylindrical coordinate system (formula_23) or spherical coordinate system (formula_24). The latter two choices are more convenient for solving problems which possess cylindrical or spherical symmetry respectively.
The choice of a basis doesn't affect the properties of a vector or its behaviour under transformations.
A vector can be also decomposed with respect to "non-fixed" basis vectors that change their orientation as a function of time or space. For example, a vector in three-dimensional space can be decomposed with respect to two axes, respectively "normal", and "tangent" to a surface (see figure). Moreover, the "radial" and "tangential components" of a vector relate to the "radius of rotation" of an object. The former is parallel to the radius and the latter is orthogonal to it.
In these cases, each of the components may be in turn decomposed with respect to a fixed coordinate system or basis set (e.g., a "global" coordinate system, or inertial reference frame).
Basic properties.
The following section uses the Cartesian coordinate system with basis vectors
and assumes that all vectors have the origin as a common base point. A vector a will be written as
Equality.
Two vectors are said to be equal if they have the same magnitude and direction. Equivalently they will be equal if their coordinates are equal. So two vectors
and
are equal if
Opposite, parallel, and antiparallel vectors.
Two vectors are opposite if they have the same magnitude but opposite direction. So two vectors
and
are opposite if
Two vectors are parallel if they have the same direction but not necessarily the same magnitude, or antiparallel if they have opposite direction but not necessarily the same magnitude.
Addition and subtraction.
Assume now that a and b are not necessarily equal vectors, but that they may have different magnitudes and directions. The sum of a and b is
The addition may be represented graphically by placing the tail of the arrow b at the head of the arrow a, and then drawing an arrow from the tail of a to the head of b. The new arrow drawn represents the vector a + b, as illustrated below:
This addition method is sometimes called the "parallelogram rule" because a and b form the sides of a parallelogram and a + b is one of the diagonals. If a and b are bound vectors that have the same base point, this point will also be the base point of a + b. One can check geometrically that a + b = b + a and (a + b) + c = a + (b + c).
The difference of a and b is
Subtraction of two vectors can be geometrically defined as follows: to subtract b from a, place the tails of a and b at the same point, and then draw an arrow from the head of b to the head of a. This new arrow represents the vector a − b, as illustrated below:
Subtraction of two vectors may also be performed by adding the opposite of the second vector to the first vector, that is, a − b = a + (−b).
Scalar multiplication.
A vector may also be multiplied, or re-"scaled", by a real number "r". In the context of conventional vector algebra, these real numbers are often called scalars (from "scale") to distinguish them from vectors. The operation of multiplying a vector by a scalar is called "scalar multiplication". The resulting vector is
Intuitively, multiplying by a scalar "r" stretches a vector out by a factor of "r". Geometrically, this can be visualized (at least in the case when "r" is an integer) as placing "r" copies of the vector in a line where the endpoint of one vector is the initial point of the next vector.
If "r" is negative, then the vector changes direction: it flips around by an angle of 180°. Two examples ("r" = −1 and "r" = 2) are given below:
Scalar multiplication is distributive over vector addition in the following sense: "r"(a + b) = "ra + "rb for all vectors a and b and all scalars "r". One can also show that a − b = a + (−1)b.
Length.
The "length" or "magnitude" or "norm" of the vector a is denoted by ‖a‖ or, less commonly, |a|, which is not to be confused with the absolute value (a scalar "norm").
The length of the vector a can be computed with the Euclidean norm
A "unit vector" is any vector with a length of one; normally unit vectors are used simply to indicate direction. A vector of arbitrary length can be divided by its length to create a unit vector. This is known as "normalizing" a vector. A unit vector is often indicated with a hat as in â.
To normalize a vector , scale the vector by the reciprocal of its length ‖a‖. That is:
The "zero vector" is the vector with length zero. Written out in coordinates, the vector is , and it is commonly denoted formula_38, 0, or simply 0. Unlike any other vector, it has an arbitrary or indeterminate direction, and cannot be normalized (that is, there is no unit vector that is a multiple of the zero vector). The sum of the zero vector with any vector a is a (that is, ).
Dot product.
The "dot product" of two vectors a and b (sometimes called the "inner product", or, since its result is a scalar, the "scalar product") is denoted by a ∙ b and is defined as:
where "θ" is the measure of the angle between a and b (see trigonometric function for an explanation of cosine). Geometrically, this means that a and b are drawn with a common start point and then the length of a is multiplied with the length of that component of b that points in the same direction as a.
The dot product can also be defined as the sum of the products of the components of each vector as
Cross product.
The "cross product" (also called the "vector product" or "outer product") is only meaningful in three or seven dimensions. The cross product differs from the dot product primarily in that the result of the cross product of two vectors is a vector. The cross product, denoted a × b, is a vector perpendicular to both a and b and is defined as
where "θ" is the measure of the angle between a and b, and n is a unit vector perpendicular to both a and b which completes a right-handed system. The right-handedness constraint is necessary because there exist "two" unit vectors that are perpendicular to both a and b, namely, n and (–n).
The cross product a × b is defined so that a, b, and a × b also becomes a right-handed system (but note that a and b are not necessarily orthogonal). This is the right-hand rule.
The length of a × b can be interpreted as the area of the parallelogram having a and b as sides.
The cross product can be written as
For arbitrary choices of spatial orientation (that is, allowing for left-handed as well as right-handed coordinate systems) the cross product of two vectors is a pseudovector instead of a vector (see below).
Scalar triple product.
The "scalar triple product" (also called the "box product" or "mixed triple product") is not really a new operator, but a way of applying the other two multiplication operators to three vectors. The scalar triple product is sometimes denoted by (a b c) and defined as:
It has three primary uses. First, the absolute value of the box product is the volume of the parallelepiped which has edges that are defined by the three vectors. Second, the scalar triple product is zero if and only if the three vectors are linearly dependent, which can be easily proved by considering that in order for the three vectors to not make a volume, they must all lie in the same plane. Third, the box product is positive if and only if the three vectors a, b and c are right-handed.
In components ("with respect to a right-handed orthonormal basis"), if the three vectors are thought of as rows (or columns, but in the same order), the scalar triple product is simply the determinant of the 3-by-3 matrix having the three vectors as rows
The scalar triple product is linear in all three entries and anti-symmetric in the following sense:
Multiple Cartesian bases.
All examples thus far have dealt with vectors expressed in terms of the same basis, namely, e1, e2, e3. However, a vector can be expressed in terms of any number of different bases that are not necessarily aligned with each other, and still remain the same vector. For example, using the vector a from above,
where n1, n2, n3 form another orthonormal basis not aligned with e1, e2, e3. The values of "u", "v", and "w" are such that the resulting vector sum is exactly a.
It is not uncommon to encounter vectors known in terms of different bases (for example, one basis fixed to the Earth and a second basis fixed to a moving vehicle). In order to perform many of the operations defined above, it is necessary to know the vectors in terms of the same basis. One simple way to express a vector known in one basis in terms of another uses column matrices that represent the vector in each basis along with a third matrix containing the information that relates the two bases. For example, in order to find the values of "u", "v", and "w" that define a in the n1, n2, n3 basis, a matrix multiplication may be employed in the form
where each matrix element "c""jk" is the direction cosine relating n"j" to e"k". The term "direction cosine" refers to the cosine of the angle between two unit vectors, which is also equal to their dot product.
By referring collectively to e1, e2, e3 as the "e" basis and to n1, n2, n3 as the "n" basis, the matrix containing all the "c""jk" is known as the "transformation matrix from "e" to "n"", or the "rotation matrix from "e" to "n"" (because it can be imagined as the "rotation" of a vector from one basis to another), or the "direction cosine matrix from "e" to "n"" (because it contains direction cosines).
The properties of a rotation matrix are such that its inverse is equal to its transpose. This means that the "rotation matrix from "e" to "n"" is the transpose of "rotation matrix from "n" to "e"".
By applying several matrix multiplications in succession, any vector can be expressed in any basis so long as the set of direction cosines is known relating the successive bases.
Other dimensions.
With the exception of the cross and triple products, the above formulae generalise to two dimensions and higher dimensions. For example, addition generalises to two dimensions as
and in four dimensions as
The cross product does not readily generalise to other dimensions, though the closely related exterior product does, whose result is a bivector. In two dimensions this is simply a pseudoscalar
A seven-dimensional cross product is similar to the cross product in that its result is a vector orthogonal to the two arguments; there is however no natural way of selecting one of the possible such products.
Physics.
Vectors have many uses in physics and other sciences.
Length and units.
In abstract vector spaces, the length of the arrow depends on a dimensionless scale. If it represents, for example, a force, the "scale" is of physical dimension length/force. Thus there is typically consistency in scale among quantities of the same dimension, but otherwise scale ratios may vary; for example, if "1 newton" and "5 m" are both represented with an arrow of 2 cm, the scales are 1:250 and 1 m:50 N respectively. Equal length of vectors of different dimension has no particular significance unless there is some proportionality constant inherent in the system that the diagram represents. Also length of a unit vector (of dimension length, not length/force, etc.) has no coordinate-system-invariant significance.
Vector-valued functions.
Often in areas of physics and mathematics, a vector evolves in time, meaning that it depends on a time parameter "t". For instance, if r represents the position vector of a particle, then r("t") gives a parametric representation of the trajectory of the particle. Vector-valued functions can be differentiated and integrated by differentiating or integrating the components of the vector, and many of the familiar rules from calculus continue to hold for the derivative and integral of vector-valued functions.
Position, velocity and acceleration.
The position of a point x = ("x"1, "x"2, "x"3) in three-dimensional space can be represented as a position vector whose base point is the origin
The position vector has dimensions of length.
Given two points x = ("x"1, "x"2, "x"3), y = ("y"1, "y"2, "y"3) their displacement is a vector
which specifies the position of "y" relative to "x". The length of this vector gives the straight-line distance from "x" to "y". Displacement has the dimensions of length.
The velocity v of a point or particle is a vector, its length gives the speed. For constant velocity the position at time "t" will be
where x0 is the position at time "t"=0. Velocity is the time derivative of position. Its dimensions are length/time.
Acceleration a of a point is vector which is the time derivative of velocity. Its dimensions are length/time2.
Force, energy, work.
Force is a vector with dimensions of mass×length/time2 and Newton's second law is the scalar multiplication
Work is the dot product of force and displacement
Vectors as directional derivatives.
A vector may also be defined as a "directional derivative": consider a function formula_56 and a curve formula_57. Then the directional derivative of formula_58 is a scalar defined as
where the index formula_60 is summed over the appropriate number of dimensions (for example, from 1 to 3 in 3-dimensional Euclidean space, from 0 to 3 in 4-dimensional spacetime, etc.). Then consider a vector tangent to formula_57:
The directional derivative can be rewritten in differential form (without a given function formula_58) as
Therefore, any directional derivative can be identified with a corresponding vector, and any vector can be identified with a corresponding directional derivative. A vector can therefore be defined precisely as
Vectors, pseudovectors, and transformations.
An alternative characterization of Euclidean vectors, especially in physics, describes them as lists of quantities which behave in a certain way under a coordinate transformation. A "contravariant vector" is required to have components that "transform opposite to the basis" under changes of basis. The vector itself does not change when the basis is transformed; instead, the components of the vector make a change that cancels the change in the basis. In other words, if the reference axes (and the basis derived from it) were rotated in one direction, the component representation of the vector would rotate in the opposite way to generate the same final vector. Similarly, if the reference axes were stretched in one direction, the components of the vector would reduce in an exactly compensating way. Mathematically, if the basis undergoes a transformation described by an invertible matrix "M", so that a coordinate vector x is transformed to , then a contravariant vector v must be similarly transformed via . This important requirement is what distinguishes a contravariant vector from any other triple of physically meaningful quantities. For example, if "v" consists of the "x", "y", and "z"-components of velocity, then "v" is a contravariant vector: if the coordinates of space are stretched, rotated, or twisted, then the components of the velocity transform in the same way. On the other hand, for instance, a triple consisting of the length, width, and height of a rectangular box could make up the three components of an abstract vector, but this vector would not be contravariant, since rotating the box does not change the box's length, width, and height. Examples of contravariant vectors include displacement, velocity, electric field, momentum, force, and acceleration.
In the language of differential geometry, the requirement that the components of a vector transform according to the same matrix of the coordinate transition is equivalent to defining a "contravariant vector" to be a tensor of contravariant rank one. Alternatively, a contravariant vector is defined to be a tangent vector, and the rules for transforming a contravariant vector follow from the chain rule.
Some vectors transform like contravariant vectors, except that when they are reflected through a mirror, they flip "and" gain a minus sign. A transformation that switches right-handedness to left-handedness and vice versa like a mirror does is said to change the "orientation" of space. A vector which gains a minus sign when the orientation of space changes is called a "pseudovector" or an "axial vector". Ordinary vectors are sometimes called "true vectors" or "polar vectors" to distinguish them from pseudovectors. Pseudovectors occur most frequently as the cross product of two ordinary vectors.
One example of a pseudovector is angular velocity. Driving in a car, and looking forward, each of the wheels has an angular velocity vector pointing to the left. If the world is reflected in a mirror which switches the left and right side of the car, the "reflection" of this angular velocity vector points to the right, but the "actual" angular velocity vector of the wheel still points to the left, corresponding to the minus sign. Other examples of pseudovectors include magnetic field, torque, or more generally any cross product of two (true) vectors.
This distinction between vectors and pseudovectors is often ignored, but it becomes important in studying symmetry properties. See parity (physics).
References.
Mathematical treatments
Physical treatments

</doc>
<doc id="411492" url="https://en.wikipedia.org/wiki?curid=411492" title="Euler angles">
Euler angles

The Euler angles are three angles introduced by Leonhard Euler to describe the orientation of a rigid body. To describe such an orientation in 3-dimensional Euclidean space three parameters are required. They can be given in several ways, Euler angles being one of them; see charts on SO(3) for others. Euler angles are also used to describe the orientation of a frame of reference (typically, a coordinate system or basis) relative to another. They are typically denoted as , , , or , , .
Euler angles represent a sequence of three "elemental rotations", i.e. rotations about the axes of a coordinate system. For instance, a first rotation about by an angle , a second rotation about by an angle , and a last rotation again about , by an angle . These rotations start from a known standard orientation. In physics, this standard initial orientation is typically represented by a motionless ("fixed", "global", or "world") coordinate system; in linear algebra, by a standard basis.
Any orientation can be achieved by composing three elemental rotations. The elemental rotations can either occur about the axes of the fixed coordinate system (extrinsic rotations) or about the axes of a rotating coordinate system, which is initially aligned with the fixed one, and modifies its orientation after each elemental rotation (intrinsic rotations). The rotating coordinate system may be imagined to be rigidly attached to a rigid body. In this case, it is sometimes called a "local" coordinate system. Without considering the possibility of using two different conventions for the definition of the rotation axes (intrinsic or extrinsic), there exist twelve possible sequences of rotation axes, divided in two groups: 
Tait–Bryan angles are also called Cardan angles; nautical angles; heading, elevation, and bank; or yaw, pitch, and roll. Sometimes, both kinds of sequences are called "Euler angles". In that case, the sequences of the first group are called "proper" or "classic" Euler angles.
Proper Euler angles.
Classic definition.
Euler angles are a means of representing the spatial orientation of any reference frame (coordinate system or basis) as a composition of three elemental rotations starting from a known standard orientation, represented by another frame (sometimes referred to as the "original" or "fixed" reference frame, or standard basis). The reference orientation can be imagined to be an initial orientation from which the frame virtually rotates to reach its actual orientation. In the following, the axes of the original frame are denoted as "x","y","z" and the axes of the rotated frame are denoted as "X","Y","Z". In geometry and physics, the rotated coordinate system is often imagined to be rigidly attached to a rigid body. In this case, it is called a "local" coordinate system, and it is meant to represent both the position and the orientation of the body.
The geometrical definition (referred sometimes as static) of the Euler angles is based on the axes of the above-mentioned (original and rotated) reference frames and an additional axis called the line of nodes. The line of nodes ("N") is defined as the intersection of the "xy" and the "XY" coordinate planes. In other words, it is a line passing through the common origin of both frames, and perpendicular to the "zZ" plane, on which both "z" and "Z" lie. The three Euler angles are defined as follows:
This definition implies that:
If "β" is zero, there is no rotation about "N". As a consequence, "Z" coincides with "z", "α" and "γ" represent rotations about the same axis ("z"), and the final orientation can be obtained with a single rotation about "z", by an angle equal to "α"+"γ".
Alternative definition.
The rotated frame "XYZ" may be imagined to be initially aligned with "xyz", before undergoing the three elemental rotations represented by Euler angles. Its successive orientations may be denoted as follows:
For the above-listed sequence of rotations, the line of nodes "N" can be simply defined as the orientation of "X" after the first elemental rotation. Hence, "N" can be simply denoted "x’". Moreover, since the third elemental rotation occurs about "Z", it does not change the orientation of "Z". Hence "Z" coincides with "z″". This allows us to simplify the definition of the Euler angles as follows:
Conventions.
Different authors may use different sets of rotation axes to define Euler angles, or different names for the same angles. Therefore any discussion employing Euler angles should always be preceded by their definition. Unless otherwise stated, this article will use the convention described above.
The three elemental rotations may occur either about the axes "xyz" of the original coordinate system, which is assumed to remain motionless (extrinsic rotations), or about the axes of the rotating coordinate system "XYZ", which changes its orientation after each elemental rotation (intrinsic rotations). The definition above uses intrinsic rotations.
There are six possibilities of choosing the rotation axes for proper Euler angles. In all of them, the first and third rotation axes are the same. The six possible sequences are:
Euler angles between two reference frames are defined only if both frames have the same handedness.
Signs and ranges.
Angles are commonly defined according to the right hand rule. Namely, they have positive values when they represent a rotation that appears clockwise when looking in the positive direction of the axis, and negative values when the rotation appears counter-clockwise. The opposite convention (left hand rule) is less frequently adopted.
About the ranges:
The angles "α", "β" and "γ" are uniquely determined except for the singular case that the "xy" and the "XY" planes are identical, the "z" axis and the "Z" axis having the same or opposite directions. Indeed, if the "z" axis and the "Z" axis are the same, "β" = 0 and only ("α" + "γ") is uniquely defined (not the individual values), and, similarly, if the "z" axis and the "Z" axis are opposite, "β" = π and only ("α" − "γ") is uniquely defined (not the individual values). These ambiguities are known as gimbal lock in applications.
Geometric derivation.
The fastest way to get the Euler Angles of a given frame is to write the three given vectors as columns of a matrix and compare it with the expression of the theoretical matrix (see later table of matrices). Hence the three Euler Angles can be calculated. Nevertheless, the same result can be reached avoiding matrix algebra, which is more geometrical. Assuming a frame with unit vectors ("X", "Y", "Z") as in the main diagram, it can be seen that:
And, since 
we have
As formula_10 is the double projection of a unitary vector,
There is a similar construction for formula_13, projecting it first over the plane defined by the axis "z" and the line of nodes. As the angle between the planes is formula_14 and formula_15, this leads to:
and finally, using the inverse cosine function,
It is interesting to note that the inverse cosine function yields two possible values for the argument. In this geometrical description only one of the solutions is valid. When Euler Angles are defined as a sequence of rotations, all the solutions can be valid, but there will be only one inside the angle ranges. This is because the sequence of rotations to reach the target frame is not unique if the ranges are not previously defined.
For computational purposes, it may be useful to represent the angles using atan2(y,x):
Tait–Bryan angles.
The second type of formalism is called Tait–Bryan angles, after Peter Guthrie Tait and George H. Bryan.
The definitions and notations used for Tait-Bryan angles are similar to those described above for proper Euler angles (Classic definition, Alternative definition). The only difference is that Tait–Bryan angles represent rotations about three distinct axes (e.g. "x-y-z", or "x-y’-z″"), while proper Euler angles use the same axis for both the first and third elemental rotations (e.g., "z-x-z", or "z-x’-z″").
This implies a different definition for the line of nodes. In the first case it was defined as the intersection between two homologous Cartesian planes (parallel when Euler angles are zero; e.g. "xy" and "XY"). In the second one, it is defined as the intersection of two non-homologous planes (perpendicular when Euler angles are zero; e.g. "xy" and "YZ").
Conventions.
The three elemental rotations may occur either about the axes of the original coordinate system, which remains motionless (extrinsic rotations), or about the axes of the rotating coordinate system, which changes its orientation after each elemental rotation (intrinsic rotations).
There are six possibilities of choosing the rotation axes for Tait–Bryan angles. The six possible sequences are:
Alternative names.
Tait-Bryan angles, following z-y’-x″ (intrinsic rotations) convention, are also known as nautical angles, because they can be used to describe the orientation of a ship or aircraft, or Cardan angles, after the Italian mathematician and physicist Gerolamo Cardano, who first described in detail the Cardan suspension and the Cardan joint. They are also called heading, elevation and bank, or yaw, pitch and roll. Notice that the second set of terms is also used for the three aircraft principal axes.
Relationship with physical motions.
Intrinsic rotations.
Intrinsic rotations are elemental rotations that occur about the axes of the rotating coordinate system "XYZ", which changes its orientation after each elemental rotation. The "XYZ" system rotates, while "xyz" is fixed. Starting with "XYZ" overlapping "xyz", a composition of three intrinsic rotations can be used to reach any target orientation for "XYZ". The Euler or Tait-Bryan angles ("α", "β", "γ") are the amplitudes of these elemental rotations. For instance, the target orientation can be reached as follows:
The above-mentioned notation allows us to summarize this as follows: the three elemental rotations of the XYZ-system occur about "z", "x"’ and "z"″. Indeed, this sequence is often denoted "z-x’-z″". Sets of rotation axes associated with both proper Euler angles and Tait-Bryan angles are commonly named using this notation (see above for details). Sometimes, the same sequence is simply called "z-x-z", "Z-X-Z", or "3-1-3", but this notation may be ambiguous as it may be identical to that used for extrinsic rotations. In this case, it becomes necessary to separately specify whether the rotations are intrinsic or extrinsic.
Rotation matrices can be used to represent a sequence of intrinsic rotations. For instance, 
represents a composition of intrinsic rotations about axes "x-y’-z″", if used to pre-multiply column vectors, while
represents exactly the same composition when used to post-multiply row vectors. See Ambiguities in the definition of rotation matrices for more details.
Extrinsic rotations.
Extrinsic rotations are elemental rotations that occur about the axes of the fixed coordinate system "xyz". The "XYZ" system rotates, while "xyz" is fixed. Starting with "XYZ" overlapping "xyz", a composition of three extrinsic rotations can be used to reach any target orientation for "XYZ". The Euler or Tait-Bryan angles ("α", "β", "γ") are the amplitudes of these elemental rotations. For instance, the target orientation can be reached as follows:
In sum, the three elemental rotations occur about "z", "x" and "z". Indeed, this sequence is often denoted "z-x-z" (or 3-1-3). Sets of rotation axes associated with both proper Euler angles and Tait–Bryan angles are commonly named using this notation (see above for details).
Rotation matrices can be used to represent a sequence of extrinsic rotations. For instance, 
represents a composition of extrinsic rotations about axes "x-y-z", if used to pre-multiply column vectors, while
represents exactly the same composition when used to post-multiply row vectors. See Ambiguities in the definition of rotation matrices for more details.
Conversion between intrinsic and extrinsic rotations.
Any extrinsic rotation is equivalent to an intrinsic rotation by the same angles but with inverted order of elemental rotations, and vice versa. For instance, the intrinsic rotations "x-y’-z″" by angles "α", "β", "γ" are equivalent to the extrinsic rotations "z-y-x" by angles "γ", "β", "α". Both are represented by a matrix
if "R" is used to pre-multiply column vectors, and by a matrix
if "R" is used to post-multiply row vectors. See Ambiguities in the definition of rotation matrices for more details.
Gimbal motion relationship.
Euler basic motions are defined as the movements obtained by changing one of the Euler angles while leaving the other two constant. Euler rotations are never expressed in terms of the external frame, or in terms of the co-moving rotated body frame, but in a mixture. They constitute a mixed axes of rotation system, where the first angle moves the line of nodes around the external axis "z", the second rotates around the line of nodes and the third one is an intrinsic rotation around an axis fixed in the body that moves.
These rotations are called precession, nutation, and intrinsic rotation (spin). As an example, consider a top. The top spins around its own axis of symmetry; this corresponds to its intrinsic rotation. It also rotates around its pivotal axis, with its center of mass orbiting the pivotal axis; this rotation is a precession. Finally, the top can wobble up and down; the inclination angle is the nutation angle. While all three are rotations when applied over individual frames, only precession is valid as a rotation operator, and only precession can be expressed in general as a matrix in the basis of the space.
Gimbal analogy.
If we suppose a set of frames, able to move each with respect to the former according to just one angle, like a gimbal, there will exist an external fixed frame, one final frame and two frames in the middle, which are called "intermediate frames". The two in the middle work as two gimbal rings that allow the last frame to reach any orientation in space.
In these conditions, each Euler rotation works on one of the rings, independently from the rest.
Intermediate frames.
The gimbal rings indicate some intermediate frames. They can be defined statically too. Taking some vectors i, j and k over the axes "x", "y" and "z", and vectors I, J, K over "X", "Y" and "Z", and a vector N over the line of nodes, some intermediate frames can be defined using the vector cross product, as following:
These intermediate frames are equivalent to those of the gimbal. They are such that they differ from the previous one in just a single elemental rotation. This proves that:
Relationship to other representations.
Euler angles are one way to represent orientations. There are others, and it is possible to change to and from other conventions.
Rotation matrix.
Any orientation can be achieved by composing three elemental rotations, starting from a known standard orientation. Equivalently, any rotation matrix "R" can be decomposed as a product of three elemental rotation matrices. For instance:
is a rotation matrix that may be used to represent a composition of intrinsic rotations about axes "x-y’-z″". However, both the definition of the elemental rotation matrices "X", "Y", "Z", and their multiplication order depend on the choices taken by the user about the definition of both rotation matrices and Euler angles (see, for instance, Ambiguities in the definition of rotation matrices). Unfortunately, different sets of conventions are adopted by users in different contexts. The following table was built according to this set of conventions:
For the sake of simplicity, the following table uses the following nomenclature:
To change the formulas for the opposite direction of rotation, change the signs of the sine functions. To change the formulas for passive rotations, transpose the matrices (then each matrix transforms the initial coordinates of a vector remaining fixed to the coordinates of the same vector measured in the rotated reference system; same rotation axis, same angles, but now the coordinate system rotates, rather than the vector).
Quaternions.
Unit quaternions, also known as Euler–Rodrigues parameters, provide another mechanism for representing 3D rotations. This is equivalent to the special unitary group description.
Expressing rotations in 3D as unit quaternions instead of matrices has some advantages:
Geometric algebra.
Other representation comes from the Geometric algebra(GA). GA is a higher level abstraction, in which the quaternions are an even subalgebra. The principal tool in GA is the rotor formula_32 where formula_33angle of rotation, formula_34rotation axis (unitary vector) and formula_35pseudoscalar (trivector in formula_36)
Properties.
The Euler angles form a chart on all of SO(3), the special orthogonal group of rotations in 3D space. The chart is smooth except for a polar coordinate style singularity along "β"=0. See charts on SO(3) for a more complete treatment.
The space of rotations is called in general "The Hypersphere of rotations", though this is a misnomer: the group Spin(3) is isometric to the hypersphere "S"3, but the rotation space SO(3) is instead isometric to the real projective space RP3 which is a 2-fold quotient space of the hypersphere. This 2-to-1 ambiguity is the mathematical origin of spin in physics.
A similar three angle decomposition applies to SU(2), the special unitary group of rotations in complex 2D space, with the difference that "β" ranges from 0 to 2π. These are also called Euler angles.
The Haar measure for Euler angles has the simple form sin("β")."dα"."dβ"."dγ", usually normalized by a factor of 1/8π².
For example, to generate uniformly randomized orientations, let "α" and γ be uniform from 0 to 2π, let "z" be uniform from −1 to 1, and let "β" = arccos("z").
Higher dimensions.
It is possible to define parameters analogous to the Euler angles in dimensions higher than three.
The number of degrees of freedom of a rotation matrix is always less than the dimension of the matrix squared. That is, the elements of a rotation matrix are not all completely independent. For example, the rotation matrix in dimension 2 has only one degree of freedom, since all four of its elements depend on a single angle of rotation. A rotation matrix in dimension 3 (which has nine elements) has three degrees of freedom, corresponding to each independent rotation, for example by its three Euler angles or a magnitude one (unit) quaternion.
In SO(4) the rotation matrix is defined by two quaternions, and is therefore 6-parametric (three degrees of freedom for every quaternion). The rotation matrices have therefore 6 out of 16 independent components.
Any set of 6 parameters that define the rotation matrix could be considered an extension of Euler angles to dimension 4.
In general, the number of euler angles in dimension D is quadratic in D; since any one rotation consists of choosing two dimensions to rotate between, the total number of rotations available in dimension formula_37 is formula_38, which for formula_39 yields formula_40.
Applications.
Vehicles and moving frames.
Their main advantage over other orientation descriptions is that they are directly measurable from a gimbal mounted in a vehicle. As gyroscopes keep their rotation axis constant, angles measured in a gyro frame are equivalent to angles measured in the lab frame. Therefore gyros are used to know the actual orientation of moving spacecraft, and Euler angles are directly measurable. Intrinsic rotation angle cannot be read from a single gimbal, so there has to be more than one gimbal in a spacecraft. Normally there are at least three for redundancy. There is also a relation to the well-known gimbal lock problem of mechanical engineering  .
The most popular application is to describe aircraft attitudes, normally using a Tait–Bryan convention so that zero degrees elevation represents the horizontal attitude. Tait–Bryan angles represent the orientation of the aircraft respect a reference axis system ("world frame") with three angles which in the context of an aircraft are normally called Heading, Elevation and Bank. When dealing with vehicles, different axes conventions are possible.
When studying rigid bodies in general, one calls the "xyz" system "space coordinates", and the "XYZ" system "body coordinates". The space coordinates are treated as unmoving, while the body coordinates are considered embedded in the moving body. Calculations involving acceleration, angular acceleration, angular velocity, angular momentum, and kinetic energy are often easiest in body coordinates, because then the moment of inertia tensor does not change in time. If one also diagonalizes the rigid body's moment of inertia tensor (with nine components, six of which are independent), then one has a set of coordinates (called the principal axes) in which the moment of inertia tensor has only three components.
The angular velocity of a rigid body takes a simple form using Euler angles in the moving frame. Also the Euler's rigid body equations are simpler because the inertia tensor is constant in that frame.
Crystallographic texture.
In materials science, crystallographic texture (or preferred orientation) can be described using Euler angles. In texture analysis, the Euler angles provide a mathematical depiction of the orientation of individual crystallites within a polycrystalline material, allowing for the quantitative description of the macroscopic material. 
The most common definition of the angles is due to Bunge and corresponds to the "ZXZ" convention. It is important to note, however, that the application generally involves axis transformations of tensor quantities, i.e. passive rotations. Thus the matrix that corresponds to the Bunge Euler angles is the transpose of that shown in the table above.
Others.
Euler angles, normally in the Tait–Bryan convention, are also used in robotics for speaking about the degrees of freedom of a wrist. They are also used in Electronic stability control in a similar way.
Gun fire control systems require corrections to gun-order angles (bearing and elevation) to compensate for deck tilt (pitch and roll). In traditional systems, a stabilizing gyroscope with a vertical spin axis corrects for deck tilt, and stabilizes the optical sights and radar antenna. However, gun barrels point in a direction different from the line of sight to the target, to anticipate target movement and fall of the projectile due to gravity, among other factors. Gun mounts roll and pitch with the deck plane, but also require stabilization. Gun orders include angles computed from the vertical gyro data, and those computations involve Euler angles.
Euler angles are also used extensively in the quantum mechanics of angular momentum. In quantum mechanics, explicit descriptions of the representations of "SO(3)" are very important for calculations, and almost all the work has been done using Euler angles. In the early history of quantum mechanics, when physicists and chemists had a sharply negative reaction towards abstract group theoretic methods (called the "Gruppenpest"), reliance on Euler angles was also essential for basic theoretical work.
Many mobile computing devices contain accelerometers which can determine these devices' Euler angles with respect to the earth's gravitational attraction. These are used in applications such as games, bubble level simulations, and kaleidoscopes.

</doc>
<doc id="396022" url="https://en.wikipedia.org/wiki?curid=396022" title="Euler equations (fluid dynamics)">
Euler equations (fluid dynamics)

In fluid dynamics, the Euler equations are a set of quasilinear hyperbolic equations governing adiabatic and inviscid flow. They are named after Leonhard Euler. The equations represent Cauchy equations of conservation of mass (continuity), and balance of momentum and energy, and can be seen as particular Navier–Stokes equations with zero viscosity and zero thermal conductivity. In fact, Euler equations can be obtained by linearization of some more precise continuity equations like Navier-Stokes equations in a local equilibrium state given by a Maxwellian. The Euler equations can be applied to incompressible and to compressible flow – assuming the flow velocity is a solenoidal field, or using another appropriate energy equation respectively (the simplest form for Euler equations being the conservation of the specific entropy). Historically, only the incompressible equations have been derived by Euler. However, fluid dynamics literature often refers to the full set – including the energy equation – of the more general compressible equations together as "the Euler equations".
From the mathematical point of view, Euler equations are notably hyperbolic conservation equations in the case without external field (i.e. in the limit of high Froude number). In fact, like any Cauchy equation, the Euler equations originally formulated in convective form (also called usually "Lagrangian form", but this name is not self-explanatory and historically wrong, so it will be avoided) can also be put in the "conservation form" (also called usually "Eulerian form", but also this name is not self-explanatory and is historically wrong, so it will be avoided here). The conservation form emphasizes the mathematical interpretation of the equations as conservation equations through a control volume fixed in space, and is the most important for these equations also from a numerical point of view. The convective form emphasizes changes to the state in a frame of reference moving with the fluid.
History.
The Euler equations first appeared in published form in Euler's article "Principes généraux du mouvement des fluides", published in "Mémoires de l'Academie des Sciences de Berlin" in 1757 (in this article Euler actually published only the "general" form of the continuity equation and the momentum equation; the energy balance equation would be obtained a century later). They were among the first partial differential equations to be written down. At the time Euler published his work, the system of equations consisted of the momentum and continuity equations, and thus was underdetermined except in the case of an incompressible fluid. An additional equation, which was later to be called the adiabatic condition, was supplied by Pierre-Simon Laplace in 1816.
During the second half of the 19th century, it was found that the equation related to the balance of energy must at all times be kept, while the adiabatic condition is a consequence of the fundamental laws in the case of smooth solutions. With the discovery of the special theory of relativity, the concepts of energy density, momentum density, and stress were unified into the concept of the stress–energy tensor, and energy and momentum were likewise unified into a single concept, the energy–momentum vector.
Incompressible Euler equations with constant and uniform density.
In convective form (i.e. the form with the convective operator made explicit in the momentum equation), the incompressible Euler equations in case of density constant in time and uniform in space are:
where:
The first equation is the Euler momentum equation with uniform density (for this equation it could also not be constant in time). By expanding the material derivative, the equations become:
In fact for a flow with uniform density formula_10 the following identity holds:
formula_11
where formula_12 is the mechanic pressure. The second equation is the incompressible constraint, stating the flow velocity is a solenoidal field (the order of the equations is not casual, but underlines the fact that the incompressible constraint is not a degenerate form of the continuity equation, but rather of the energy equation, as it will become clear in the following). Notably, the continuity equation would be required also in this incompressible case as an additional third equation in case of density varying in time "or" varying in space. For example, with density uniform but varying in time, the continuity equation to be added to the above set would correspond to:
formula_13
So the case of constant and uniform density is the only one not requiring the continuity equation as additional equation regardless of the presence or absence of the incompressible constraint. In fact, the case of incompressible Euler equations with constant and uniform density being analyzed is a toy model featuring only two simplified equations, so it is ideal for didactical purposes even if with limited physical relevancy.
The equations above thus represent respectively conservation of mass (1 scalar equation) and momentum (1 vector equation containing formula_14 scalar components, where formula_14 is the physical dimension of the space of interest). In 3D for example formula_16 and the formula_17 and formula_1 vectors are explicitly formula_19 and formula_20. Flow velocity and pressure are the so-called "physical variables".
These equations may be expressed in subscript notation:
where the formula_22 and formula_23 subscripts label the N-dimensional space components. These equations may be more succinctly expressed using Einstein notation:
where the formula_22 and formula_23 subscripts label the N-dimensional space components, and formula_27; is the Kroenecker delta. In 3D formula_16 and the formula_17 and formula_1 vectors are explicitly formula_19 and formula_20, and matched indices imply a sum over those indices and formula_33 and formula_34.
Properties.
Although Euler first presented these equations in 1755, many fundamental questions about them remain unanswered.
In three space dimensions it is not even known whether solutions of the equations are defined for all time or if they form singularities.
Smooth solutions of the free (in the sense of without source term: g=0) equations satisfy the conservation of specific kinetic energy:
formula_35
In the one dimensional case without the source term (both pressure gradient and external force), the momentum equation becomes the inviscid Burgers equation:
formula_36
This is a model equation giving many insights on Euler equations.
Nondimensionalisation.
In order to make the equations dimensionless, a characteristic length formula_37, and a characteristic velocity formula_38, need to be defined. These should be chosen such that the dimensionless variables are all of order one. The following dimensionless variables are thus obtained:
formula_39
formula_40
formula_41
formula_42
formula_43
and of the field unit vector:
formula_44
Substitution of these inversed relations in Euler equations, defining the Froude number, yields (omitting the * at apix):
 \hat{\mathbf g}\\
\nabla\cdot \bold u=0
\end{align} \right.</math>
Euler equations in the Froude limit (no external field) are named free equations and are conservative. The limit of high Froude numbers (low external field) is thus notable and can be studied with perturbation theory.
Conservation form.
The conservation form emphasizes the mathematical properties of Euler equations, and especially the contracted form is often the most convenient one for computational fluid dynamics simulations. Computationally, there are some advantages in using the conserved variables. This gives rise to a large class of numerical methods
called conservative methods.
The free Euler equations are conservative, in the sense they are equivalent to a conservation equation:
or simply in Einstein notation:
where the conservation quantity formula_47 in this case is a vector, and formula_48 is a flux matrix. This can be simply proved.
At last Euler equations can be recast into the particular equation:
Spatial dimensions.
For certain problems, especially when used to analyze compressible flow in a duct or in case the flow is cylindrically or spherically symmetric, the one-dimensional Euler equations are a useful first approximation. Generally, the Euler equations are solved by Riemann's method of characteristics. This involves finding curves in plane of independent variables (i.e., formula_49 and formula_50) along which partial differential equations (PDE's) degenerate into ordinary differential equations (ODE's). Numerical solutions of the Euler equations rely heavily on the method of characteristics.
Incompressible Euler equations.
In convective form the incompressible Euler equations in case of density variable in space are:
where the additional variables are:
The first equation, which is the new one, is the incompressible continuity equation. In fact the general continuity equation would be:
formula_54
but here the last term is identically zero for the incompressibility constraint.
Conservation form.
The incompressible Euler equations in the Froude limit are equivalent to a single conservation equation with conserved quantity and associated flux respectively:
Here formula_47 has length N+2 and formula_48 has size N(N+2). In general (not only in the Froude limit) Euler equations are expressible as:
Conservation variables.
The variables for the equations in conservation form are not yet optimised. In fact we could define:
where:
where:
Euler equations.
In differential convective form, the compressible (and most general) Euler equations can be written shorty with the material derivative notation:
where the additional variables here is:
The equations above thus represent conservation of mass, momentum, and energy: the energy equation expressed in the variable internal energy allows to understand the link with the incompressible case, but it is not in the simplest form.
Mass density, flow velocity and pressure are the so-called "convective variables" (or physical variables, or lagrangian variables), while mass density, momentum density and total energy density are the so-called "conserved variables" (also called eulerian, or mathematical variables).
If one explicitates the material derivative the equations above are:
formula_63
Incompressible constraint.
Coming back to the incompressible case, it now becomes apparent that the "incompressible constraint" typical of the former cases actually is a particular form valid for incompressible flows of the "energy equation", and not of the mass equation. In particular, the incompressible constraint corresponds to the following very simple energy equation:
formula_64
Thus for an incompressible inviscid fluid the specific internal energy is constant along the flow lines, also in a time-dependent flow. The pressure in an incompressible flow acts like a Lagrange multiplier, being the multiplier of the incompressible constraint in the energy equation, and consequently in incompressible flows it has no thermodynamic meaning. In fact, thermodynamics is typical of compressible flows and degenerates in incompressible flows.
Basing on the mass conservation equation, one can put this equation in the conservation form:
formula_65
meaning that for an incompressible inviscid nonconductive flow a continuity equation holds for the internal energy.
Enthalpy conservation.
Since by definition the specific enthalpy is:
formula_66
The material derivative of the specific internal energy can be expressed as:
formula_67
Then by substituting the momentum equation in this expression, one obtains:
formula_68
And by substituting the latter in the energy equation, one obtains that the enthalpy expression for the Euler energy equation:
formula_69
In a reference frame moving with an inviscid and nonconductive flow, the variation of enthalpy directly corresponds to a variation of pressure.
Thermodynamic systems.
In thermodynamics the independent variables are the specific volume, and the specific entropy, while the specific energy is a function of state of these two variables.
For a thermodynamic fluid, the compressible Euler equations are consequently best written as:
\end{align}\right.</math>
where:
Note that, in the general case and not only in the incompressible case, the energy equation means that for an inviscid thermodynamic fluid the specific entropy is constant along the flow lines, also in a time-dependent flow. Basing on the mass conservation equation, one can put this equation in the conservation form:
formula_73
meaning that for an inviscid nonconductive flow a continuity equation holds for the entropy.
On the other hand, the two second-order partial derivatives of the specific internal energy in the momentum equation require the specification of the fundamental equation of state of the material considered, i.e. of the specific internal energy as function of the two variables specific volume and specific entropy:
formula_74
Note that the "fundamental" equation of state contains all the thermodynamic information about the system (Callen, 1985), exactly like the couple of a "thermal" equation of state together with a "caloric" equation of state.
Conservation form.
The Euler equations in the Froude limit are equivalent to a single conservation equation with conserved quantity and associated flux respectively:
where:
Here formula_47 has length N+2 and formula_48 has size N(N+2). In general (not only in the Froude limit) Euler equations are expressible as:
where:
We remark that also the Euler equation even when conservative (no external field, Froude limit) have no Riemann invariants in general. Some further assumptions are required
However, we already mentioned that for a thermodynamic fluid the equation for the total energy density is equivalent to the conservation equation:
formula_81
Then the conservation equations in the case of a thermodynamic fluid are more simply expressed as:
where:
Another possible form for the energy equation, being particularly useful for isobarics, is:
where:
Quasilinear form and characteristic equations.
Expanding the fluxes can be an important part of constructing numerical solvers, for example by exploiting (approximate) solutions to the Riemann problem. In regions where the state vector y varies smoothly, the equations in conservative form can be put in quasilinear form :
where formula_86 are called the flux Jacobians defined as the matrices:
Obviously this jacobian does not exist in disconinuity regions (e.g. contact disconitnuities, shock waves in inviscid nonconductive flows). Note that if the flux Jacobians formula_86 are not functions of the state vector formula_47, the equations reveals "linear".
Characteristic equations.
The compressible Euler equations can be decoupled into a set of N+2 wave equations that describes sound in Eulerian continuum if they are expressed in characteristic variables instead of conserved variables.
In fact the tensor A is always diagonalizable. If the eigenvalues (the case of Euler equations) are all real the system is defined "hyperbolic", and physically eignevalues represent the speeds of propagation of information. If they are all distinguished, the system is defined "strictly hyperbolic" (it will be proved to be the case of one-dimensional Euler equations). Furthermore, note that diagonalisation of compressible Euler equation is easier when the energy equation is expressed in the variable entropy (i.e. with equations for thermodynamic fluids) than in other energy variables. This will become clear by considering the 1D case.
If formula_90 is the right eigenvector of the matrix formula_91 corresponding to the eigenvalue formula_92, by building the projection matrix:
formula_93
One can finally find the "characteristic variables" as:
Since A is constant, multiplying the original 1-D equation in flux-Jacobian form with P−1 yields the characteristic equations:
The original equations have been decoupled into N+2 characteristic equations each describing a simple wave, with the eigenvalues being the wave speeds. The variables "w"i are called the "characteristic variables"and are a subset of the conservative variables. The solution of the initial value problem in terms of characteristic variables is finally very simple. In one spatial dimension it is:
Then the solution in terms of the original conservative variables is obtained by transforming back:
this computation can be explicited as the linear combination of the eigenvectors:
Now it becomes apparent that the characteristic variables act as weights in the linear combination of the jacobian eigenvectors. The solution can be seen as superposition of waves, each of which is advected independently without change in shape. Each i-th wave has shape wipi and speed of propagation λi. In the following we show a very simple example of this solution procedure.
Waves in 1D inviscid, nonconductive thermodynamic fluid.
If one considers Euler equations for a thermodynamic fluid with the two further assumptions of one spatial dimension and free (no external field: g=0) :
If one defines the vector of variables:
recalling that formula_70 is the specific volume, formula_102 the flow speed, formula_72 the specific entropy, the corresponding jacobian matrix is:
formula_104
At first one must find the eigenvalues of this matrix by solving the characteristic equation:
formula_105
that is explicitly:
formula_106
This determinant is very simple: the fastest computation starts on the last row, since it has the highest number of zero elements.
formula_107
Now by computing the determinant 2x2:
formula_108
by defining the parameter:
formula_109
or equivalently in mechanical variables, as:
formula_110
This parameter is always real according to the second law of thermodynamics. In fact the second law of thermodynamics can be expressed by several postulates. The most elementary of them in mathematical terms is the statement of convexity of the fundamental equation of state, i.e. the hessian matrix of the specific energy expresseed as function of specific volume and specific entropy:
formula_111
is defined positive. This statement corresponds to the two conditions:
The first condition is the one ensuring the parameter "a" is defined real.
The characteristic equation finally results:
formula_113
That has three real solutions:
formula_114
Then the matrix has three real eigenvalues all distinguished: the 1D Euler equations are a strictly hyperbolic system.
At this point one should determine the three eigenvectors: each one is obtained by substituting one eigenvalue in the eigenvalue equation and then solving it. By substituting the first eigenvalue λ1 one obtains:
formula_115
Basing on the third equation that simply has solution s1=0, the system reduces to:
formula_116
The two equations are redundant as usual, then the eigenvector is defined with a multiplying constant. We choose as right eigenvector:
formula_117
The other two eigenvectors can be found with analogous procedure as:
formula_118
Then the projection matrix can be built:
formula_119
Finally it becomes apparent that the real parameter "a" previously defined is the speed of propagation of the information characteristic of the hyperbolic system made of Euler equations, i.e. it is the "wave speed". It remains to be shown that the sound speed corresponds to the particular case of an isoentropic transformation:
Compressibility and sound speed.
Sound speed is defined as the wavespeed of an isentropic transformation:
by the definition of the isoentropic compressibility:
the soundspeed results always the square root of ratio between the isoentropic compressibility and the density:
Ideal gas.
The sound speed in an ideal gas depends only on its temperature:
Since the specific enthalpy in an ideal gas is proportional to its temperature:
the sound speed in an ideal gas can also be made dependent only on its specific enthalpy:
Bernoulli's theorems for steady inviscid flow.
Incompressible case and Lamb's form.
The vector calculus identity of the cross product of a curl holds:
where the Feynman subscript notation formula_128 is used, which means the subscripted gradient operates only on the factor formula_48.
Lamb in his famous classical book Hydrodynamics (1895), still in print, used this identity to change the convective term of the flow velocity in rotational form:
the Euler momentum equation in Lamb's form becomes:
Now, basing on the other identity:
the Euler momentum equation assumes a form that is optimal to demonstrate Bernoulli's theorem for steady flows:
In fact, in case of an external conservative field, by defining its potential φ:
In case of a steady flow the time derivative of the flow velocity disappears, so the momentum equation becomes:
And by projecting the momentum equation on the flow direction, i.e. along a "streamline", the cross product disappears due to a vector calculus identity of the triple scalar product:
In the steady incompressible case the mass equation is simply:
, that is the mass conservation for a steady incompressible flow states that the density along a streamline is constant. Then the Euler momentum equation in the steady incompressible case becomes:
The convenience of defining the total head for an inviscid liquid flow is now apparent:
, in fact the above equation can be simply written as:
That is, the momentum balance for a steady inviscid and incompressible flow in an external conservative field states that the total head along a streamline is constant.
Compressible case.
In the most general steady (compressibile) case the mass equation in conservation form is:
and the steady energy equation in conservation form is:
Thanks to the mass conservation equation and to the definition of the momentum density, the first member becomes simply:
and for an external conservative field, the second member becomes:
Then by dividing for the density, the energy equation becomes:
formula_145
Since the external field potential is usually small compared to the other terms, it is convenient to group the latters in the total enthalpy:
and the Bernoulli invariant for an inviscid gas flow is:
, in fact the above equation can be always written as:
That is, the energy balance for a steady inviscid flow in an external conservative field states that the sum of the total enthalpy and the external potential is constant along a streamline.
In the usual case of small potential field, simply:
Friedman form and Crocco form.
By substituting the pressure gradient with the entropy and enthalpy gradient, according to the first law of thermodynamics in the enthalpy form:
in the convective form of Euler momentum equation, one arrives to:
Friedmann deduced this equation for the particular case of a perfect gas and published it in 1922. However, this equation is general for an inviscid nonconductive fluid and no equation of state is implicit in it.
On the other hand, by substituting the enthalpy form of the first law of thermodynamics in the rotational form of Euler momentum equation, one obtains:
and by defining the specific total enthalpy:
one arrives to the Crocco-Vazsonyi form (Crocco, 1937) of the Euler momentum equation:
In the steady case the two variables entropy and total enthalpy are particularly useful since Euler equations can be recast into the Crocco's form:
Finally if the flow is also isothermal:
formula_156
by defining the specific total Gibbs free energy:
formula_157
the Crocco's form can be reduced to:
From these relationships one deduces that the specific total free energy is uniform in a steady, irrotational, isothermal, isoentropic, inviscid flow.
Discontinuities.
The Euler equations are quasilinear hyperbolic equations and their general solutions are waves. Under certain assumptions they can be simplified leading to Burgers equation. Much like the familiar oceanic waves, waves described by the Euler Equations 'break' and so-called shock waves are formed; this is a nonlinear effect and represents the solution becoming multi-valued. Physically this represents a breakdown of the assumptions that led to the formulation of the differential equations, and to extract further information from the equations we must go back to the more fundamental integral form. Then, weak solutions are formulated by working in 'jumps' (discontinuities) into the flow quantities – density, velocity, pressure, entropy – using the Rankine–Hugoniot equations. Physical quantities are rarely discontinuous; in real flows, these discontinuities are smoothed out by viscosity and by heat transfer. (See Navier–Stokes equations)
Shock propagation is studied – among many other fields – in aerodynamics and rocket propulsion, where sufficiently fast flows occur.
To properly compute the continuum quantities in discontinuous zones (for example shock waves or boundary layers) from the "local" forms (all the above forms are local forms, since the variables being described are typical of one point in the space caonsidered, i.e. they are "local variables") of Euler equations through finite difference methods generally too many space points and time steps would be necessary for the memory of computers now and in the near future. In these cases it is mandatory to avoid the local forms of the conservation equations, passing some weak forms, like the finite volume one.
Rankine-Hugoniot equations.
Starting from the simplest case, one consider a steady free conservation equation in conservation form in the space domain:
where in general F is the flux matrix. By integrating this local equation over a fixed volume Vm, it becomes:
formula_160
then, basing on the divergence theorem, we can transform this integral in a boundary integral of the flux:
formula_161
this "global form" simply states that there is no net flux of a conserved quantity passing through a region in the case steady and without source. In 1D the volume reduces to an interval, its boundary being its extrema, then the divergence theorem reduces to the fundamental theorem of calculus:
formula_162
that is the simple finite difference equation, known as the "jump relation":
formula_163
That can be made explicit as:
formula_164
where the notation employed is:
formula_165
Or, if one performs an indefinite integral:
formula_166
On the other hand, a transient conservation equation:
brings to a jump relation:
formula_168
For one-dimensional Euler equations the conservation variables and the flux are the vectors:
where:
In the one dimensional case the correspondent jump relations, called the Rankine-Hugoniot equations, are:
formula_173
In the steady one dimensional case the become simply:
formula_174
thanks to the mass difference equation, the energy difference equation can be simplyfied without any restriction:
formula_175
where formula_176 is the specific total enthalpy.
these are the usually expressed in the convective variables:
formula_177
where:
Note that the energy equation is an integral form of the Bernoulli equation in the compressible case. 
The former mass and momentum equations by substitution lead to the Rayleigh equation:
formula_180
Since the second member is a constant, the Rayleigh equation always describes a simple line in the pressure volume plane not depending of any equation of state, i.e. the Rayleigh line.
By substituition in the Rankine-Hugoniot equations, that can be also made explicit as:
formula_181
one can also obtain the kinetic equation and to the Hugoniot equation. The analytical passages are not shown here for brevity.
These are respectively:
formula_182
The Hugoniot equation, coupled with the fundamental equation of state of the material:
formula_183
describes in general in the pressure volume plane a curve passing by the conditions (v0,p0), i.e. the Hugoniot curve, whose shape strongly depends on the type of material considered.
It is also customary to define a "Hugoniot function":
formula_184
allowing to quantify deviations from the Hugoniot equation, similarly to the previous definition of the "hydraulic head", useful for the deviations from the Bernoulli equation.
Finite volume form.
On the other hand, by integrating a generic conservation equation:
formula_185
on a fixed volume Vm, and then basing on the divergence theorem, it becomes:
formula_186
and by integrating this equation also over a time interval:
formula_187
Now by defining the node conserved quantity:
formula_188
we deduce the finite volume form:
formula_189
In particular, for Euler equations, once the conserved quantities have been determined, the convective variables are deduced by back substitution:
formula_190
Then the explicit finite volume expressions of the original convective variables are:
 \oint_{\partial V_m} \rho \mathbf u \cdot \hat n ds dt \\[1.2ex]
\mathbf u_{m,n+1}=\mathbf u_{m,n} - \frac 1 {\rho_{m,n} V_m} \int_{t_n}^{t_{n+1}} \oint_{\partial V_m} (\rho \mathbf u \otimes \mathbf u - p \mathbf I) \cdot \hat n ds dt \\[1.2ex]
\mathbf e_{m,n+1}=\mathbf e_{m,n} - \frac 1 2 (u^2_{m,n+1} - u^2_{m,n}) - \frac 1 {\rho_{m,n} V_m} \int_{t_n}^{t_{n+1}} \oint_{\partial V_m} (\rho e + \frac 1 2 \rho u^2 + p) \mathbf u \cdot \hat n ds dt \\[1.2ex]
\end{align}\right.</math>
Constraints.
It has been shown that Euler equations are not a complete set of equations, but they require some additional constraints to admit a unique solution: these are the equation of state of the material considered.
To be constistent with thermodynamics these equations of state should satisfy the two laws of thermodynamics. On the other hand, by definition non-equilibrium system are described by laws lying outside these laws.
In the following we list some very simple equations of state and the corresponding influence on Euler equations.
Ideal polytropic gas.
For an ideal polytropic gas the fundamental equation of state is:
where formula_62 is the specific energy, formula_70 is the specific volume, formula_72 is the specific entropy, formula_195 is the molecular mass, formula_196 here is considered a constant (polytropic process), and can be shown to correspond to the heat capacity ratio. 
This equation can be shown to be consistent with the usual equations of state employed by thermodynamics.
From this equation one can derive the equation for pressure by its thermodynamic definition:
By inverting it one arrives to the mechanical equation of state:
Then for an ideal gas the compressible Euler equations can be simply expressed in the "mechanical" or "primitive variables" specific volume, flow velocity and pressure, by taking the set of the equations for a thermodynamic system and modifying the energy equation into a pressure equation through this mechanical equation of state. At last, in convective form they result:
\end{align}\right.</math>
and in one-dimensional quasilinear form they results:
where the conservative vector variable is:
and the corresponding jacobian matrix is:
formula_201
Steady flow in material coordinates.
In the case of steady flow, it is convenient to choose the Frenet–Serret frame along a streamline as the coordinate system for describing the steady momentum Euler equation:
where formula_1, formula_12 and formula_51 denote the flow velocity, the pressure and the density, respectively.
Let formula_206 be a Frenet–Serret orthonormal basis which consists of a tangential unit vector, a normal unit vector, and a binormal unit vector to the streamline, respectively.
Since a streamline is a curve that is tangent to the velocity vector of the flow, the left-handed side of the above equation, the convective derivative of velocity, can be described as follows:
where formula_208 is the radius of curvature of the streamline.
Therefore, the momentum part of the Euler equations for a steady flow is found to have a simple form:
For barotropic flow formula_210, Bernoulli's equation is derived from the first equation:
The second equation expresses that, in the case the streamline is curved, there should exist a pressure gradient normal to the streamline because the centripetal acceleration of the fluid parcel is only generated by the normal pressure gradient.
The third equation expresses that pressure is constant along the binormal axis.
Streamline curvature theorem.
Let formula_212 be the distance from the center of curvature of the streamline, 
then the second equation is written as follows:
where formula_214
This equation states: 
"In a steady flow of an inviscid fluid without external forces, the center of curvature of the streamline lies in the direction of decreasing radial pressure." 
Although this relationship between the pressure field and flow curvature is very useful, it doesn't have a name in the English-language scientific literature.
Japanese fluid-dynamicists call the relationship the "Streamline curvature theorem".
This "theorem" explains clearly why there are such low pressures in the centre of vortices, which consist of concentric circles of streamlines.
This also is a way to intuitively explain why airfoils generate lift forces.

</doc>
<doc id="898792" url="https://en.wikipedia.org/wiki?curid=898792" title="Euler's equations (rigid body dynamics)">
Euler's equations (rigid body dynamics)

In classical mechanics, Euler's rotation equations are a vectorial quasilinear first-order ordinary differential equation describing the rotation of a rigid body, using a rotating reference frame with its axes fixed to the body and parallel to the body's principal axes of inertia. Their general form is:
where "M" is the applied torques, "I" is the inertia matrix, and ω is the angular velocity about the principal axes. 
In 3D principal orthogonal coordinates, they become:
where "Mk" are the components of the applied torques, "Ik" are the principal moments of inertia and ω"k" are the components of the angular velocity about the principal axes. 
Motivation and derivation.
Starting from Euler's second law, in an inertial frame of reference (subscripted "in"), the time derivative of the angular momentum L equals the applied torque
where Iin is the moment of inertia tensor calculated in the inertial frame. Although this law is universally true, it is not always helpful in solving for the motion of a general rotating rigid body, since both Iin and ω can change during the motion.
Therefore, we change to a coordinate frame fixed in the rotating body, and chosen so that its axes are aligned with the principal axes of the moment of inertia tensor. In this frame, at least the moment of inertia tensor is constant (and diagonal), which simplifies calculations. As described in the moment of inertia, the angular momentum L can be written
where "Mk", "Ik" and ω"k" are as above.
In a "rotating" reference frame, the time derivative must be replaced with (see time derivative in rotating reference frame)
where the subscript "rot" indicates that it is taken in the rotating reference frame. The expressions for the torque in the rotating and inertial frames are related by
where Q is the rotation tensor(not rotation matrix) , an orthogonal tensor related to the angular velocity vector by 
for any vector v.
In general, L = I·ω is substituted and the time derivatives are taken realizing that the inertia tensor, and so also the principal moments, do not depend on time. This leads to the general vector form of Euler's equations
If principal axis rotation 
is substituted, and then taking the cross product and using the fact that the principal moments do not change with time, we arrive at the Euler equations in components at the beginning of the article.
Torque-free solutions.
For the RHSs equal to zero there are non-trivial solutions: torque-free precession. Notice that if I is constant (because the inertia tensor is the 3×3 identity matrix, because we work in the intrinsic frame, or because the torque is driving the rotation around the same axis formula_10 so that I is not changing) then we may write 
where
However, if I is not constant in the external reference frame (i.e. the body is moving and its inertia tensor is not the identity) then we cannot take the I outside the derivative. In this case we will have torque-free precession, in such a way that I("t") and ω("t") change together so that their derivative is zero. This motion can be visualized by Poinsot's construction.
Generalizations.
It is also possible to use these equations if the axes in which 
is described are not connected to the body. Then ω should be replaced with the rotation of the axes instead of the rotation of the body. It is, however, still required that the chosen axes are still principal axes of inertia. This form of the Euler equations is useful for rotation-symmetric objects that allow some of the principal axes of rotation to be chosen freely.

</doc>
<doc id="9613" url="https://en.wikipedia.org/wiki?curid=9613" title="Euler's formula">
Euler's formula

Euler's formula, named after Leonhard Euler, is a mathematical formula in complex analysis that establishes the fundamental relationship between the trigonometric functions and the complex exponential function. Euler's formula states that, for any real number :
where is the base of the natural logarithm, is the imaginary unit, and and are the trigonometric functions cosine and sine respectively, with the argument "x" given in radians. This complex exponential function is sometimes denoted ("cosine plus i sine"). The formula is still valid if is a complex number, and so some authors refer to the more general complex version as Euler's formula.
Euler's formula is ubiquitous in mathematics, physics, and engineering. The physicist Richard Feynman called the equation "our jewel" and "the most remarkable formula in mathematics."
History.
Johann Bernoulli noted that
formula_2
And since
formula_3
the above equation tells us something about complex logarithms. Bernoulli, however, did not evaluate the integral.
Bernoulli's correspondence with Euler (who also knew the above equation) shows that Bernoulli did not fully understand complex logarithms. Euler also suggested that the complex logarithms can have infinitely many values.
Meanwhile, Roger Cotes, in 1714, discovered that
formula_4
(formula_5 is the natural logarithm).
Cotes missed the fact that a complex logarithm can have infinitely many values, differing by multiples of , due to the periodicity of the trigonometric functions.
Around 1740 Euler turned his attention to the exponential function instead of logarithms, and obtained the formula used today that is named after him. It was published in 1748, obtained by comparing the series expansions of the exponential and trigonometric expressions.
None of these mathematicians saw the geometrical interpretation of the formula; the view of complex numbers as points in the complex plane was described some 50 years later by Caspar Wessel.
Applications in complex number theory.
This formula can be interpreted as saying that the function is a unit complex number, i.e., traces out the unit circle in the complex plane as ranges through the real numbers. Here, is the angle that a line connecting the origin with a point on the unit circle makes with the positive real axis, measured counter clockwise and in radians.
The original proof is based on the Taylor series expansions of the exponential function (where is a complex number) and of and for real numbers (see below). In fact, the same proof shows that Euler's formula is even valid for all "complex" numbers .
A point in the complex plane can be represented by a complex number written in
cartesian coordinates. Euler's formula provides a means of conversion between cartesian coordinates and polar coordinates. The polar form simplifies the mathematics when used in multiplication or powers of complex numbers. Any complex number , and its complex conjugate, , can be written as
where
 is the "argument" of "z"—i.e., the angle between the "x" axis and the vector "z" measured counterclockwise and in radians—which is defined up to addition of . Many texts write θ = tan−1("y"/"x") instead of θ = atan2("y","x"), but the first equation needs adjustment when "x" ≤ 0. This is because, for any real x, y not both zero, the angles of the vectors (x,y) and (-x,-y) differ by radians, but have the identical value of tan(θ) = y/x.
Now, taking this derived formula, we can use Euler's formula to define the logarithm of a complex number. To do this, we also use the definition of the logarithm (as the inverse operator of exponentiation) that
and that
both valid for any complex numbers "a" and "b".
Therefore, one can write:
for any "z" ≠ 0. Taking the logarithm of both sides shows that:
and in fact this can be used as the definition for the complex logarithm. The logarithm of a complex number is thus a multi-valued function, because is multi-valued.
Finally, the other exponential law
which can be seen to hold for all integers "k", together with Euler's formula, implies several trigonometric identities as well as de Moivre's formula.
Relationship to trigonometry.
Euler's formula provides a powerful connection between analysis and trigonometry, and provides an interpretation of the sine and cosine functions as weighted sums of the exponential function:
The two equations above can be derived by adding or subtracting Euler's formulas:
and solving for either cosine or sine.
These formulas can even serve as the definition of the trigonometric functions for complex arguments "x". For example, letting "x" = "iy", we have:
Complex exponentials can simplify trigonometry, because they are easier to manipulate than their sinusoidal components. One technique is simply to convert sinusoids into equivalent expressions in terms of exponentials. After the manipulations, the simplified result is still real-valued. For example:
Another technique is to represent the sinusoids in terms of the real part of a complex expression, and perform the manipulations on the complex expression. For example:
This formula is used for recursive generation of cos("nx") for integer values of "n" and arbitrary "x" (in radians).
See also Phasor arithmetic.
Topological interpretation.
In the language of topology, Euler's formula states that the imaginary exponential function formula_21 is a (surjective) morphism of topological groups from the real line to the unit circle formula_22. In fact, this exhibits as a covering space of formula_22. Similarly, Euler's identity says that the kernel of this map is formula_24, where formula_25. These observations may be combined and summarized in the commutative diagram below:
Other applications.
In differential equations, the function is often used to simplify derivations, even if the final answer is a real function involving sine and cosine. The reason for this is that the complex exponential is the eigenfunction of differentiation. Euler's identity is an easy consequence of Euler's formula.
In electronic engineering and other fields, signals that vary periodically over time are often described as a combination of sine and cosine functions (see Fourier analysis), and these are more conveniently expressed as the real part of exponential functions with imaginary exponents, using Euler's formula. Also, phasor analysis of circuits can include Euler's formula to represent the impedance of a capacitor or an inductor.
The growth and decay of the speed of a vibrating object is related to the natural log/exponent function.
Definitions of complex exponentiation.
The exponential function for real values of may be defined in a few different equivalent ways (see Characterizations of the exponential function). Several of these methods may be directly extended to give definitions of for complex values of simply by substituting in place of and using the complex algebraic operations. In particular we may use either of the two following definitions which are equivalent. From a more advanced perspective, each of these definitions may be interpreted as giving the unique analytic continuation of to the complex plane.
Power series definition.
For complex 
Using the ratio test it is possible to show that this power series has an infinite radius of convergence, and so defines for all complex .
Limit definition.
For complex 
Polar to Rectangular Conversion definition.
If the original z is expressed in not ordinary rectilinear but rather packed polar coordinates, where the normal x is actually representing the radius, and the normal y is actually representing the angle theta, then complex exponentiation serves to unpack the polar coordinates into regular rectangular space, while simultaneously exponentiating the radius part.
Thus for complex = + j 
where the new radius R' = er
Then the new x is R′cos( θ ), and the new y is R′sin( θ ), as usual.
Proofs.
Various proofs of the formula are possible.
Using power series.
Here is a proof of Euler's formula using power series expansions
as well as basic facts about the powers of "i":
and so on. Using now the power series definition from above we see that for real values of "x"
In the last step we have simply recognized the Maclaurin series for "cos(x)" and "sin(x)". The rearrangement of terms is justified because each series is absolutely convergent.
Using calculus.
Another proof is based on the fact that all complex numbers can be expressed in polar coordinates. Therefore for some and depending on ,
Now from any of the definitions of the exponential function it can be shown that the derivative of is . Therefore, differentiating both sides gives
Substituting formula_33 for formula_34 and equating real and imaginary parts in this formula gives formula_35 and formula_36. Together with the initial values formula_37 and formula_38 which come from formula_39 this gives formula_40 and formula_41. This proves the formula formula_42.

</doc>
<doc id="22222481" url="https://en.wikipedia.org/wiki?curid=22222481" title="Euler's laws of motion">
Euler's laws of motion

In classical mechanics, Euler's laws of motion are equations of motion which extend Newton's laws of motion for point particle to rigid body motion. They were formulated by Leonhard Euler about 50 years after Isaac Newton formulated his laws.
Overview.
Euler's first law.
Euler's first law states that the linear momentum of a body, p (also denoted G) is equal to the product of the mass of the body "m" and the velocity of its center of mass vcm: 
Internal forces between the particles that make up a body do not contribute to changing the total momentum of the body. The law is also stated as:
where acm = "dvcm/"dt" is the acceleration of the centre of mass and F = "dp/"dt" is the total applied force on the body. This is just the time derivative of the previous equation ("m" is a constant).
Euler's second law.
Euler's second law states that the rate of change of angular momentum L (also denoted H) about a point that is fixed in an inertial reference frame or the mass center of the body, is equal to the sum of the external moments of force (torques) M (also denoted τ or Γ) about that point: 
Note that the above formula holds only if both M,L are computed with respect to a fixed inertial frame or a frame parallel to the inertial frame but fixed on the center of mass. 
For rigid bodies translating and rotating in only 2d, this can be expressed as:
where rcm is the position vector of the center of mass with respect to the point about which moments are summed, α is the angular acceleration of the body, and "I" is the moment of inertia.
See also Euler's equations (rigid body dynamics).
Explanation and derivation.
The density of internal forces at every point in a deformable body are not necessarily equal, "i.e." there is a distribution of stresses throughout the body. This variation of internal forces throughout the body is governed by Newton's second law of motion of conservation of linear momentum and angular momentum, which normally are applied to a mass particle but are extended in continuum mechanics to a body of continuously distributed mass. For continuous bodies these laws are called Euler’s laws of motion. If a body is represented as an assemblage of discrete particles, each governed by Newton’s laws of motion, then Euler’s equations can be derived from Newton’s laws. Euler’s equations can, however, be taken as axioms describing the laws of motion for extended bodies, independently of any particle structure.
The total body force applied to a continuous body with mass "m", mass density ρ, and volume "V", is the volume integral integrated over the volume of the body:
where b is the force acting on the body per unit mass (dimensions of acceleration, misleadingly called the "body force"), and "dm" = ρ"dV" is an infinitesimal mass element of the body.
Body forces and contact forces acting on the body lead to corresponding moments of force (torques) relative to a given point. Thus, the total applied torque M about the origin is given by
where M"B" and M"C" respectively indicate the moments caused by the body and contact forces.
Thus, the sum of all applied forces and torques (with respect to the origin of the coordinate system) in the body can be given as the sum of a volume and surface integral:
where t = t(n) is called the surface traction, integrated over the surface of the body, in turn n denotes a unit vector normal and directed outwards to the surface "S".
Let the coordinate system ("x"1, "x"2, "x"3) be an inertial frame of reference, r be the position vector of a point particle in the continuous body with respect to the origin of the coordinate system, and v = "d"r/"dt" be the velocity vector of that point.
Euler’s first axiom or law (law of balance of linear momentum or balance of forces) states that in an inertial frame the time rate of change of linear momentum p of an arbitrary portion of a continuous body is equal to the total applied force F acting on the considered portion, and it is expressed as
Euler’s second axiom or law (law of balance of angular momentum or balance of torques) states that in an inertial frame the time rate of change of angular momentum L of an arbitrary portion of a continuous body is equal to the total applied torque M acting on the considered portion, and it is expressed as
The derivatives of p and L are material derivatives.

</doc>
<doc id="294995" url="https://en.wikipedia.org/wiki?curid=294995" title="Euler–Lagrange equation">
Euler–Lagrange equation

In calculus of variations, the Euler–Lagrange equation, Euler's equation, or Lagrange's equation (although the latter name is ambiguous—see disambiguation page), is a second-order partial differential equation whose solutions are the functions for which a given functional is stationary. It was developed by Swiss-Russian mathematician Leonhard Euler and French-Italian mathematician Joseph-Louis Lagrange in the 1750s.
Because a differentiable functional is stationary at its local maxima and minima, the Euler–Lagrange equation is useful for solving optimization problems in which, given some functional, one seeks the function minimizing (or maximizing) it. This is analogous to Fermat's theorem in calculus, stating that at any point where a differentiable function attains a local extremum, its derivative is zero.
In Lagrangian mechanics, because of Hamilton's principle of stationary action, the evolution of a physical system is described by the solutions to the Euler–Lagrange equation for the action of the system. In classical mechanics, it is equivalent to Newton's laws of motion, but it has the advantage that it takes the same form in any system of generalized coordinates, and it is better suited to generalizations. In classical field theory there is an analogous equation to calculate the dynamics of a field.
History.
The Euler–Lagrange equation was developed in the 1750s by Euler and Lagrange in connection with their studies of the tautochrone problem. This is the problem of determining a curve on which a weighted particle will fall to a fixed point in a fixed amount of time, independent of the starting point.
Lagrange solved this problem in 1755 and sent the solution to Euler. Both further developed Lagrange's method and applied it to mechanics, which led to the formulation of Lagrangian mechanics. Their correspondence ultimately led to the calculus of variations, a term coined by Euler himself in 1766.
Statement.
The Euler–Lagrange equation is an equation satisfied by a function, "q",
of a real argument, "t", which is a stationary point of the functional
where:
The Euler–Lagrange equation, then, is given by
where "L""x" and "L""v" denote the partial derivatives of "L" with respect to the second and third arguments, respectively.
If the dimension of the space "X" is greater than 1, this is a system of differential equations, one for each component:
Examples.
A standard example is finding the real-valued function on the interval ["a", "b"], such that "f"("a") = "c" and "f"("b") = "d", the length of whose graph is as short as possible. The length of the graph of "f" is:
the integrand function being evaluated at .
The partial derivatives of "L" are:
By substituting these into the Euler–Lagrange equation, we obtain
that is, the function must have constant first derivative, and thus its graph is a straight line.
Classical mechanics.
Basic method.
To find the equations of motions for a given system (whose potential energy is time-independent), one only has to follow these steps:
Particle in a conservative force field.
The motion of a single particle in a conservative force field (for example, the gravitational force) can be determined by requiring the action to be stationary, by Hamilton's principle. The action for this system is
where x("t") is the position of the particle at time "t". The dot above is Newton's notation for the time derivative: thus ẋ("t") is the particle velocity, v("t"). In the equation above, "L" is the Lagrangian (the kinetic energy minus the potential energy):
where:
In this case, the Lagrangian does not vary with its first argument "t". (By Noether's theorem, such symmetries of the system correspond to conservation laws. In particular, the invariance of the Lagrangian with respect to time implies the conservation of energy.)
By partial differentiation of the above Lagrangian, we find:
where the force is F = −∇"U" (the negative gradient of the potential, by definition of conservative force), and p is the momentum.
By substituting these into the Euler–Lagrange equation, we obtain a system of second-order differential equations for the coordinates on the particle's trajectory,
which can be solved on the interval ["t"0, "t"1], given the boundary values "x""i"("t"0) and "x""i"("t"1).
In vector notation, this system reads
or, using the momentum,
which is Newton's second law.
Variations for several functions, several variables, and higher derivatives.
Single function of single variable with higher derivatives.
The stationary values of the functional
can be obtained from the Euler–Lagrange equation 
under fixed boundary conditions for the function itself as well as for the first formula_33 derivatives (i.e. for all formula_34). The endpoint values of the highest derivative formula_35 remain flexible.
Several functions of one variable.
If the problem involves finding several functions (formula_36) of a single independent variable (formula_37) that define an extremum of the functional
then the corresponding Euler–Lagrange equations are
Single function of several variables.
A multi-dimensional generalization comes from considering a function on "n" variables. If Ω is some surface, then
is extremized only if "f" satisfies the partial differential equation
When "n" = 2 and formula_42 is the energy functional, this leads to the soap-film minimal surface problem.
Several functions of several variables.
If there are several unknown functions to be determined and several variables such that
the system of Euler–Lagrange equations is
Single function of two variables with higher derivatives.
If there is a single unknown function "f" to be determined that is dependent on two variables "x"1 and "x"2 and if the functional depends on higher derivatives of "f" up to "n"-th order such that
then the Euler–Lagrange equation is
which can be represented shortly as:
where formula_48 are indices that span the number of variables, that is they go from 1 to 2. Here summation over the formula_48 indices is implied according to Einstein notation.
Several functions of several variables with higher derivatives.
If there is are "p" unknown functions "f"i to be determined that are dependent on "m" variables "x"1 ... "x"m and if the functional depends on higher derivatives of the "f"i up to "n"-th order such that
where formula_51 are indices that span the number of variables, that is they go from 1 to m. Then the Euler–Lagrange equation is
where summation over the formula_51 is implied according to Einstein notation. This can be expressed more compactly as
Generalization to Manifolds.
Let formula_55 be a smooth manifold, and let formula_56 denote the space of smooth functions formula_57. Then, for functionals formula_58 of the form
where formula_60 is the Lagrangian, the statement formula_61 is equivalent to the statement that, for all formula_62, each coordinate frame trivialization formula_63 of a neighborhood of formula_64 yields the following formula_65 equations:

</doc>
