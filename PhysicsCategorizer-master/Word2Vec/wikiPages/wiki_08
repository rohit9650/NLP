<doc id="25264" url="https://en.wikipedia.org/wiki?curid=25264" title="Quantum chromodynamics">
Quantum chromodynamics

In theoretical physics, quantum chromodynamics (QCD) is the theory of strong interactions, a fundamental force describing the interactions between quarks and gluons which make up hadrons such as the proton, neutron and pion. QCD is a type of quantum field theory called a non-abelian gauge theory with symmetry group SU(3). The QCD analog of electric charge is a property called "color". Gluons are the force carrier of the theory, like photons are for the electromagnetic force in quantum electrodynamics. The theory is an important part of the Standard Model of particle physics. A large body of experimental evidence for QCD has been gathered over the years.
QCD enjoys two peculiar properties:
The phase transition temperature between these two properties has been measured by the ALICE experiment to be well above 160 MeV. Below this temperature, confinement is dominant, while above it, asymptotic freedom becomes dominant.
Terminology.
The word "quark" was coined by American physicist Murray Gell-Mann (b. 1929) in its present sense. It originally comes from the phrase "Three quarks for Muster Mark" in "Finnegans Wake" by James Joyce. On June 27, 1978, Gell-Mann wrote a private letter to the editor of the "Oxford English Dictionary", in which he related that he had been influenced by Joyce's words: "The allusion to three quarks seemed perfect." (Originally, only three quarks had been discovered.) Gell-Mann, however, wanted to pronounce the word to rhyme with "fork" rather than with "park", as Joyce seemed to indicate by rhyming words in the vicinity such as "Mark". Gell-Mann got around that "by supposing that one ingredient of the line 'Three quarks for Muster Mark' was a cry of 'Three quarts for Mister ...' heard in H.C. Earwicker's pub", a plausible suggestion given the complex punning in Joyce's novel.
The three kinds of charge in QCD (as opposed to one in quantum electrodynamics or QED) are usually referred to as "color charge" by loose analogy to the three kinds of color (red, green and blue) perceived by humans. Other than this nomenclature, the quantum parameter "color" is completely unrelated to the everyday, familiar phenomenon of color.
Since the theory of electric charge is dubbed "electrodynamics", the Greek word "chroma" Χρώμα (meaning color) is applied to the theory of color charge, "chromodynamics".
History.
With the invention of bubble chambers and spark chambers in the 1950s, experimental particle physics discovered a large and ever-growing number of particles called hadrons. It seemed that such a large number of particles could not all be fundamental. First, the particles were classified by charge and isospin by Eugene Wigner and Werner Heisenberg; then, in 1953, according to strangeness by Murray Gell-Mann and Kazuhiko Nishijima. To gain greater insight, the hadrons were sorted into groups having similar properties and masses using the "eightfold way", invented in 1961 by Gell-Mann and Yuval Ne'eman. Gell-Mann and George Zweig, correcting an earlier approach of Shoichi Sakata, went on to propose in 1963 that the structure of the groups could be explained by the existence of three flavors of smaller particles inside the hadrons: the quarks.
Perhaps the first remark that quarks should possess an additional quantum number was made as a short footnote in the preprint of Boris Struminsky in connection with Ω− hyperon composed of three strange quarks with parallel spins (this situation was peculiar, because since quarks are fermions, such combination is forbidden by the Pauli exclusion principle): Boris Struminsky was a PhD student of Nikolay Bogolyubov. The problem considered in this preprint was suggested by Nikolay Bogolyubov, who advised Boris Struminsky in this research. In the beginning of 1965, Nikolay Bogolyubov, Boris Struminsky and Albert Tavkhelidze wrote a preprint with a more detailed discussion of the additional quark quantum degree of freedom. This work was also presented by Albert Tavchelidze without obtaining consent of his collaborators for doing so at an international conference in Trieste (Italy), in May 1965.
A similar mysterious situation was with the Δ++ baryon; in the quark model, it is composed of three up quarks with parallel spins. In 1965, Moo-Young Han with Yoichiro Nambu and Oscar W. Greenberg independently resolved the problem by proposing that quarks possess an additional SU(3) gauge degree of freedom, later called color charge. Han and Nambu noted that quarks might interact via an octet of vector gauge bosons: the gluons.
Since free quark searches consistently failed to turn up any evidence for the new particles, and because an elementary particle back then was "defined" as a particle which could be separated and isolated, Gell-Mann often said that quarks were merely convenient mathematical constructs, not real particles. The meaning of this statement was usually clear in context: He meant quarks are confined, but he also was implying that the strong interactions could probably not be fully described by quantum field theory.
Richard Feynman argued that high energy experiments showed quarks are real particles: he called them "partons" (since they were parts of hadrons). By particles, Feynman meant objects which travel along paths, elementary particles in a field theory.
The difference between Feynman's and Gell-Mann's approaches reflected a deep split in the theoretical physics community. Feynman thought the quarks have a distribution of position or momentum, like any other particle, and he (correctly) believed that the diffusion of parton momentum explained diffractive scattering. Although Gell-Mann believed that certain quark charges could be localized, he was open to the possibility that the quarks themselves could not be localized because space and time break down. This was the more radical approach of S-matrix theory.
James Bjorken proposed that pointlike partons would imply certain relations should hold in deep inelastic scattering of electrons and protons, which were spectacularly verified in experiments at SLAC in 1969. This led physicists to abandon the S-matrix approach for the strong interactions.
The discovery of asymptotic freedom in the strong interactions by David Gross, David Politzer and Frank Wilczek allowed physicists to make precise predictions of the results of many high energy experiments using the quantum field theory technique of perturbation theory. Evidence of gluons was discovered in three-jet events at PETRA in 1979. These experiments became more and more precise, culminating in the verification of perturbative QCD at the level of a few percent at the LEP in CERN.
The other side of asymptotic freedom is confinement. Since the force between color charges does not decrease with distance, it is believed that quarks and gluons can never be liberated from hadrons. This aspect of the theory is verified within lattice QCD computations, but is not mathematically proven. One of the Millennium Prize Problems announced by the Clay Mathematics Institute requires a claimant to produce such a proof. Other aspects of non-perturbative QCD are the exploration of phases of quark matter, including the quark–gluon plasma.
The relation between the short-distance particle limit and the confining long-distance limit is one of the topics recently explored using string theory, the modern form of S-matrix theory.
Theory.
Some definitions.
Every field theory of particle physics is based on certain symmetries of nature whose existence is deduced from observations. These can be
QCD is a gauge theory of the SU(3) gauge group obtained by taking the color charge to define a local symmetry.
Since the strong interaction does not discriminate between different flavors of quark, QCD has approximate flavor symmetry, which is broken by the differing masses of the quarks.
There are additional global symmetries whose definitions require the notion of chirality, discrimination between left and right-handed. If the spin of a particle has a positive projection on its direction of motion then it is called left-handed; otherwise, it is right-handed. Chirality and handedness are not the same, but become approximately equivalent at high energies.
Additional remarks: duality.
As mentioned, "asymptotic freedom" means that at large energy – this corresponds also to "short distances" – there is practically no interaction between the particles. This is in contrast – more precisely one would say "dual" – to what one is used to, since usually one connects the absence of interactions with "large" distances. However, as already mentioned in the original paper of Franz Wegner, a solid state theorist who introduced 1971 simple gauge invariant lattice models, the high-temperature behaviour of the "original model", e.g. the strong decay of correlations at large distances, corresponds to the low-temperature behaviour of the (usually ordered!) "dual model", namely the asymptotic decay of non-trivial correlations, e.g. short-range deviations from almost perfect arrangements, for short distances. Here, in contrast to Wegner, we have only the dual model, which is that one described in this article.
Symmetry groups.
The color group SU(3) corresponds to the local symmetry whose gauging gives rise to QCD. The electric charge labels a representation of the local symmetry group U(1) which is gauged to give QED: this is an abelian group. If one considers a version of QCD with "Nf" flavors of massless quarks, then there is a global (chiral) flavor symmetry group SUL("Nf") × SUR("Nf") × UB(1) × UA(1). The chiral symmetry is spontaneously broken by the QCD vacuum to the vector (L+R) SUV("Nf") with the formation of a chiral condensate. The vector symmetry, UB(1) corresponds to the baryon number of quarks and is an exact symmetry. The axial symmetry UA(1) is exact in the classical theory, but broken in the quantum theory, an occurrence called an anomaly. Gluon field configurations called instantons are closely related to this anomaly.
There are two different types of SU(3) symmetry: there is the symmetry that acts on the different colors of quarks, and this is an exact gauge symmetry mediated by the gluons, and there is also a flavor symmetry which rotates different flavors of quarks to each other, or "flavor SU(3)". Flavor SU(3) is an approximate symmetry of the vacuum of QCD, and is not a fundamental symmetry at all. It is an accidental consequence of the small mass of the three lightest quarks.
In the QCD vacuum there are vacuum condensates of all the quarks whose mass is less than the QCD scale. This includes the up and down quarks, and to a lesser extent the strange quark, but not any of the others. The vacuum is symmetric under SU(2) isospin rotations of up and down, and to a lesser extent under rotations of up, down and strange, or full flavor group SU(3), and the observed particles make isospin and SU(3) multiplets.
The approximate flavor symmetries do have associated gauge bosons, observed particles like the rho and the omega, but these particles are nothing like the gluons and they are not massless. They are emergent gauge bosons in an approximate string description of QCD.
Lagrangian.
The dynamics of the quarks and gluons are controlled by the quantum chromodynamics Lagrangian. The gauge invariant QCD Lagrangian is
where formula_1 is the quark field, a dynamical function of spacetime, in the fundamental representation of the SU(3) gauge group, indexed by formula_2; formula_3 are the gluon fields, also dynamical functions of spacetime, in the adjoint representation of the SU(3) gauge group, indexed by "a", "b"... The γμ are Dirac matrices connecting the spinor representation to the vector representation of the Lorentz group.
The symbol formula_4 represents the gauge invariant gluon field strength tensor, analogous to the electromagnetic field strength tensor, "F"μν, in quantum electrodynamics. It is given by:
where "fabc" are the structure constants of SU(3). Note that the rules to move-up or pull-down the "a", "b", or "c" indexes are "trivial", (+, ..., +), so that "fabc" = "fabc" = "f""a""bc" whereas for the "μ" or "ν" indexes one has the non-trivial "relativistic" rules, corresponding e.g. to the metric signature (+ − − −).
The constants "m" and "g" control the quark mass and coupling constants of the theory, subject to renormalization in the full quantum theory.
An important theoretical notion concerning the final term of the above Lagrangian is the "Wilson loop" variable. This loop variable plays a most important role in discretized forms of the QCD (see lattice QCD), and more generally, it distinguishes confined and deconfined states of a gauge theory. It was introduced by the Nobel prize winner Kenneth G. Wilson and is treated in a separate article.
Fields.
Quarks are massive spin-1/2 fermions which carry a color charge whose gauging is the content of QCD. Quarks are represented by Dirac fields in the fundamental representation 3 of the gauge group SU(3). They also carry electric charge (either −1/3 or 2/3) and participate in weak interactions as part of weak isospin doublets. They carry global quantum numbers including the baryon number, which is 1/3 for each quark, hypercharge and one of the flavor quantum numbers.
Gluons are spin-1 bosons which also carry color charges, since they lie in the adjoint representation 8 of SU(3). They have no electric charge, do not participate in the weak interactions, and have no flavor. They lie in the singlet representation 1 of all these symmetry groups.
Every quark has its own antiquark. The charge of each antiquark is exactly the opposite of the corresponding quark.
Dynamics.
According to the rules of quantum field theory, and the associated Feynman diagrams, the above theory gives rise to three basic interactions: a quark may emit (or absorb) a gluon, a gluon may emit (or absorb) a gluon, and two gluons may directly interact. This contrasts with QED, in which only the first kind of interaction occurs, since photons have no charge. Diagrams involving Faddeev–Popov ghosts must be considered too (except in the unitarity gauge).
Area law and confinement.
Detailed computations with the above-mentioned Lagrangian show that the effective potential between a quark and its anti-quark in a meson contains a term formula_6, which represents some kind of "stiffness" of the interaction between the particle and its anti-particle at large 
distances, similar to the entropic elasticity of a rubber band (see below). This leads to "confinement"  of the quarks to the interior of hadrons, i.e. mesons and nucleons, with typical radii Rc, corresponding to former "Bag models" of the hadrons . The order of magnitude of the "bag radius" is 1 fm (= 10−15 m). Moreover, the above-mentioned stiffness is quantitatively related to the so-called "area law" behaviour of the expectation value of the Wilson loop product "PW" of the ordered coupling constants around a closed loop "W"; i.e. formula_7 is proportional to the "area" enclosed by the loop. For this behaviour the non-abelian behaviour of the gauge group is essential.
Methods.
Further analysis of the content of the theory is complicated. Various techniques have been developed to work with QCD. Some of them are discussed briefly below.
Perturbative QCD.
This approach is based on asymptotic freedom, which allows perturbation theory to be used accurately in experiments performed at very high energies. Although limited in scope, this approach has resulted in the most precise tests of QCD to date.
Lattice QCD.
Among non-perturbative approaches to QCD, the most well established one is lattice QCD. This approach uses a discrete set of spacetime points (called the lattice) to reduce the analytically intractable path integrals of the continuum theory to a very difficult numerical computation which is then carried out on supercomputers like the QCDOC which was constructed for precisely this purpose. While it is a slow and resource-intensive approach, it has wide applicability, giving insight into parts of the theory inaccessible by other means, in particular into the explicit forces acting between quarks and antiquarks in a meson. However, the numerical sign problem makes it difficult to use lattice methods to study QCD at high density and low temperature (e.g. nuclear matter or the interior of neutron stars).
1/N expansion.
A well-known approximation scheme, the 1/N expansion, starts from the premise that the number of colors is infinite, and makes a series of corrections to account for the fact that it is not. Until now, it has been the source of qualitative insight rather than a method for quantitative predictions. Modern variants include the AdS/CFT approach.
Effective theories.
For specific problems effective theories may be written down which give qualitatively correct results in certain limits. In the best of cases, these may then be obtained as systematic expansions in some parameter of the QCD Lagrangian. One such effective field theory is chiral perturbation theory or ChiPT, which is the QCD effective theory at low energies. More precisely, it is a low energy expansion based on the spontaneous chiral symmetry breaking of QCD, which is an exact symmetry when quark masses are equal to zero, but for the u,d and s quark, which have small mass, it is still a good approximate symmetry. Depending on the number of quarks which are treated as light, one uses either SU(2) ChiPT or SU(3) ChiPT . Other effective theories are heavy quark effective theory (which expands around heavy quark mass near infinity), and soft-collinear effective theory (which expands around large ratios of energy scales). In addition to effective theories, models like the Nambu–Jona-Lasinio model and the chiral model are often used when discussing general features.
QCD sum rules.
Based on an Operator product expansion one can derive sets of relations that connect different observables with each other.
Nambu–Jona-Lasinio model.
In one of his recent works, Kei-Ichi Kondo derived as a low-energy limit of QCD, a theory linked to the Nambu–Jona-Lasinio model since it is basically a particular non-local version of the Polyakov–Nambu–Jona-Lasinio model. The later being in its local version, nothing but the Nambu–Jona-Lasinio model in which one has included the Polyakov loop effect, in order to describe a 'certain confinement'.
The Nambu–Jona-Lasinio model in itself is, among many other things, used because it is a 'relatively simple' model of chiral symmetry breaking, phenomenon present up to certain conditions (Chiral limit i.e. massless fermions) in QCD itself.
In this model, however, there is no confinement. In particular, the energy of an isolated quark in the physical vacuum turns out well defined and finite.
Experimental tests.
The notion of quark flavors was prompted by the necessity of explaining the properties of hadrons during the development of the quark model. The notion of color was necessitated by the puzzle of the . This has been dealt with in the section on the history of QCD.
The first evidence for quarks as real constituent elements of hadrons was obtained in deep inelastic scattering experiments at SLAC. The first evidence for gluons came in three jet events at PETRA.
Several good quantitative tests of perturbative QCD exist:
Quantitative tests of non-perturbative QCD are fewer, because the predictions are harder to make. The best is probably the running of the QCD coupling as probed through lattice computations of heavy-quarkonium spectra. There is a recent claim about the mass of the heavy meson Bc . Other non-perturbative tests are currently at the level of 5% at best. Continuing work on masses and form factors of hadrons and their weak matrix elements are promising candidates for future quantitative tests. The whole subject of quark matter and the quark–gluon plasma is a non-perturbative test bed for QCD which still remains to be properly exploited.
One qualitative prediction of QCD is that there exist composite particles made solely of gluons called glueballs that have not yet been definitively observed experimentally. A definitive observation of a glueball with the properties predicted by QCD would strongly confirm the theory. In principle, if glueballs could be definitively ruled out, this would be a serious experimental blow to QCD. But, as of 2013, scientists are unable to confirm or deny the existence of glueballs definitively, despite the fact that particle accelerators have sufficient energy to generate them.
Cross-relations to solid state physics.
There are unexpected cross-relations to solid state physics. For example, the notion of gauge invariance forms the basis of the well-known Mattis spin glasses, which are systems with the usual spin degrees of freedom formula_8 for "i" =1...,N, with the special fixed "random" couplings formula_9 Here the εi and εk quantities can independently and "randomly" take the values ±1, which corresponds to a most-simple gauge transformation formula_10 This means that thermodynamic expectation values of measurable quantities, e.g. of the energy formula_11 are invariant.
However, here the "coupling degrees of freedom" formula_12, which in the QCD correspond to the "gluons", are "frozen" to fixed values (quenching). In contrast, in the QCD they "fluctuate" (annealing), and through the large number of gauge degrees of freedom the entropy plays an important role (see below).
For positive "J"0 the thermodynamics of the Mattis spin glass corresponds in fact simply to a "ferromagnet in disguise", just because these systems have no "frustration" at all. This term is a basic measure in spin glass theory. Quantitatively it is identical with the loop product formula_13 along a closed loop "W". However, for a Mattis spin glass – in contrast to "genuine" spin glasses – the quantity "PW" never becomes negative.
The basic notion "frustration" of the spin-glass is actually similar to the Wilson loop quantity of the QCD. The only difference is again that in the QCD one is dealing with SU(3) matrices, and that one is dealing with a "fluctuating" quantity. Energetically, perfect absence of frustration should be non-favorable and atypical for a spin glass, which means that one should add the loop product to the Hamiltonian, by some kind of term representing a "punishment". In the QCD the Wilson loop is essential for the Lagrangian rightaway.
The relation between the QCD and "disordered magnetic systems" (the spin glasses belong to them) were additionally stressed in a paper by Fradkin, Huberman und Shenker, which also stresses the notion of duality.
A further analogy consists in the already mentioned similarity to polymer physics, where, analogously to Wilson Loops, so-called "entangled nets" appear, which are important for the formation of the entropy-elasticity (force proportional to the length) of a rubber band. The non-abelian character of the SU(3) corresponds thereby to the non-trivial "chemical links", which glue different loop segments together, and "asymptotic freedom" means in the polymer analogy simply the fact that in the short-wave limit, i.e. for formula_14 (where "Rc" is a characteristic correlation length for the glued loops, corresponding to the above-mentioned "bag radius", while λw is the wavelength of an excitation) any non-trivial correlation vanishes totally, as if the system had crystallized.
There is also a correspondence between confinement in QCD – the fact that the color field is only different from zero in the interior of hadrons – and the behaviour of the usual magnetic field in the theory of type-II superconductors: there the magnetism is confined to the interiour of the Abrikosov flux-line lattice,   i.e., the London penetration depth "λ" of that theory is analogous to the confinement radius "Rc" of quantum chromodynamics. Mathematically, this correspondendence is supported by the second term, formula_15 on the r.h.s. of the Lagrangian.

</doc>
<doc id="25268" url="https://en.wikipedia.org/wiki?curid=25268" title="Quantum electrodynamics">
Quantum electrodynamics

In particle physics, quantum electrodynamics (QED) is the relativistic quantum field theory of electrodynamics. In essence, it describes how light and matter interact and is the first theory where full agreement between quantum mechanics and special relativity is achieved. QED mathematically describes all phenomena involving electrically charged particles interacting by means of exchange of photons and represents the quantum counterpart of classical electromagnetism giving a complete account of matter and light interaction.
In technical terms, QED can be described as a perturbation theory of the electromagnetic quantum vacuum. Richard Feynman called it "the jewel of physics" for its extremely accurate predictions of quantities like the anomalous magnetic moment of the electron and the Lamb shift of the energy levels of hydrogen.
History.
The first formulation of a quantum theory describing radiation and matter interaction is attributed to British scientist Paul Dirac, who (during the 1920s) was able to compute the coefficient of spontaneous emission of an atom.
Dirac described the quantization of the electromagnetic field as an ensemble of harmonic oscillators with the introduction of the concept of creation and annihilation operators of particles. In the following years, with contributions from Wolfgang Pauli, Eugene Wigner, Pascual Jordan, Werner Heisenberg and an elegant formulation of quantum electrodynamics due to Enrico Fermi, physicists came to believe that, in principle, it would be possible to perform any computation for any physical process involving photons and charged particles. However, further studies by Felix Bloch with Arnold Nordsieck, and Victor Weisskopf, in 1937 and 1939, revealed that such computations were reliable only at a first order of perturbation theory, a problem already pointed out by Robert Oppenheimer. At higher orders in the series infinities emerged, making such computations meaningless and casting serious doubts on the internal consistency of the theory itself. With no solution for this problem known at the time, it appeared that a fundamental incompatibility existed between special relativity and quantum mechanics.
Difficulties with the theory increased through the end of 1940. Improvements in microwave technology made it possible to take more precise measurements of the shift of the levels of a hydrogen atom, now known as the Lamb shift and magnetic moment of the electron. These experiments unequivocally exposed discrepancies which the theory was unable to explain.
A first indication of a possible way out was given by Hans Bethe. In 1947, while he was traveling by train to reach Schenectady from New York, after giving a talk at the conference at Shelter Island on the subject, Bethe completed the first non-relativistic computation of the shift of the lines of the hydrogen atom as measured by Lamb and Retherford. Despite the limitations of the computation, agreement was excellent. The idea was simply to attach infinities to corrections of mass and charge that were actually fixed to a finite value by experiments. In this way, the infinities get absorbed in those constants and yield a finite result in good agreement with experiments. This procedure was named renormalization.
Based on Bethe's intuition and fundamental papers on the subject by Sin-Itiro Tomonaga, Julian Schwinger, Richard Feynman and Freeman Dyson, it was finally possible to get fully covariant formulations that were finite at any order in a perturbation series of quantum electrodynamics. Sin-Itiro Tomonaga, Julian Schwinger and Richard Feynman were jointly awarded with a Nobel prize in physics in 1965 for their work in this area. Their contributions, and those of Freeman Dyson, were about covariant and gauge invariant formulations of quantum electrodynamics that allow computations of observables at any order of perturbation theory. Feynman's mathematical technique, based on his diagrams, initially seemed very different from the field-theoretic, operator-based approach of Schwinger and Tomonaga, but Freeman Dyson later showed that the two approaches were equivalent. Renormalization, the need to attach a physical meaning at certain divergences appearing in the theory through integrals, has subsequently become one of the fundamental aspects of quantum field theory and has come to be seen as a criterion for a theory's general acceptability. Even though renormalization works very well in practice, Feynman was never entirely comfortable with its mathematical validity, even referring to renormalization as a "shell game" and "hocus pocus".
QED has served as the model and template for all subsequent quantum field theories. One such subsequent theory is quantum chromodynamics, which began in the early 1960s and attained its present form in the 1975 work by H. David Politzer, Sidney Coleman, David Gross and Frank Wilczek. Building on the pioneering work of Schwinger, Gerald Guralnik, Dick Hagen, and Tom Kibble, Peter Higgs, Jeffrey Goldstone, and others, Sheldon Glashow, Steven Weinberg and Abdus Salam independently showed how the weak nuclear force and quantum electrodynamics could be merged into a single electroweak force.
Feynman's view of quantum electrodynamics.
Introduction.
Near the end of his life, Richard P. Feynman gave a series of lectures on QED intended for the lay public. These lectures were transcribed and published as Feynman (1985), "QED: The strange theory of light and matter", a classic non-mathematical exposition of QED from the point of view articulated below.
The key components of Feynman's presentation of QED are three basic actions.
These actions are represented in a form of visual shorthand by the three basic elements of Feynman diagrams: a wavy line for the photon, a straight line for the electron and a junction of two straight lines and a wavy one for a vertex representing emission or absorption of a photon by an electron. These can all be seen in the adjacent diagram.
It is important not to over-interpret these diagrams. Nothing is implied about "how" a particle gets from one point to another. The diagrams do "not" imply that the particles are moving in straight or curved lines. They do "not" imply that the particles are moving with fixed speeds. The fact that the photon is often represented, by convention, by a wavy line and not a straight one does "not" imply that it is thought that it is more wavelike than is an electron. The images are just symbols to represent the actions above: photons and electrons do, somehow, move from point to point and electrons, somehow, emit and absorb photons. We do not know how these things happen, but the theory tells us about the probabilities of these things happening.
As well as the visual shorthand for the actions Feynman introduces another kind of shorthand for the numerical quantities called probability amplitudes. The probability is the square of the total probability amplitude. If a photon moves from one place and time—in shorthand, A—to another place and time—in shorthand, B—the associated quantity is written in Feynman's shorthand as P(A to B). The similar quantity for an electron moving from C to D is written E(C to D). The quantity which tells us about the probability amplitude for the emission or absorption of a photon he calls 'j'. This is related to, but not the same as, the measured electron charge 'e'.
QED is based on the assumption that complex interactions of many electrons and photons can be represented by fitting together a suitable collection of the above three building blocks, and then using the probability amplitudes to calculate the probability of any such complex interaction. It turns out that the basic idea of QED can be communicated while making the assumption that the square of the total of the probability amplitudes mentioned above (P(A to B), E(A to B) and 'j') acts just like our everyday probability. (A simplification made in Feynman's book.) Later on, this will be corrected to include specifically quantum-style mathematics, following Feynman.
The basic rules of probability amplitudes that will be used are that a) if an event can happen in a variety of different ways then its probability amplitude is the sum of the probability amplitudes of the possible ways and b) if a process involves a number of independent sub-processes then its probability amplitude is the product of the component probability amplitudes.
Basic constructions.
Suppose we start with one electron at a certain place and time (this place and time being given the arbitrary label A) and a photon at another place and time (given the label B). A typical question from a physical standpoint is: 'What is the probability of finding an electron at C (another place and a later time) and a photon at D (yet another place and time)?'. The simplest process to achieve this end is for the electron to move from A to C (an elementary action) and for the photon to move from B to D (another elementary action). From a knowledge of the probability amplitudes of each of these sub-processes – E(A to C) and P(B to D) – then we would expect to calculate the probability amplitude of both happening together by multiplying them, using rule b) above. This gives a simple estimated overall probability amplitude, which is squared to give an estimated probability. But there are other ways in which the end result could come about. The electron might move to a place and time E where it absorbs the photon; then move on before emitting another photon at F; then move on to C where it is detected, while the new photon moves on to D. The probability of this complex process can again be calculated by knowing the probability amplitudes of each of the individual actions: three electron actions, two photon actions and two vertexes – one emission and one absorption. We would expect to find the total probability amplitude by multiplying the probability amplitudes of each of the actions, for any chosen positions of E and F. We then, using rule a) above, have to add up all these probability amplitudes for all the alternatives for E and F. (This is not elementary in practice, and involves integration.) But there is another possibility, which is that the electron first moves to G where it emits a photon which goes on to D, while the electron moves on to H, where it absorbs the first photon, before moving on to C. Again we can calculate the probability amplitude of these possibilities (for all points G and H). We then have a better estimation for the total probability amplitude by adding the probability amplitudes of these two possibilities to our original simple estimate. Incidentally the name given to this process of a photon interacting with an electron in this way is Compton scattering.
There are an "infinite number" of other intermediate processes in which more and more photons are absorbed and/or emitted. For each of these possibilities there is a Feynman diagram describing it. This implies a complex computation for the resulting probability amplitudes, but provided it is the case that the more complicated the diagram the less it contributes to the result, it is only a matter of time and effort to find as accurate an answer as one wants to the original question. This is the basic approach of QED. To calculate the probability of "any" interactive process between electrons and photons it is a matter of first noting, with Feynman diagrams, all the possible ways in which the process can be constructed from the three basic elements. Each diagram involves some calculation involving definite rules to find the associated probability amplitude.
That basic scaffolding remains when one moves to a quantum description but some conceptual changes are needed. One is that whereas we might expect in our everyday life that there would be some constraints on the points to which a particle can move, that is "not" true in full quantum electrodynamics. There is a possibility of an electron at A, or a photon at B, moving as a basic action to "any other place and time in the universe". That includes places that could only be reached at speeds greater than that of light and also "earlier times". (An electron moving backwards in time can be viewed as a positron moving forward in time.)
Probability amplitudes.
Quantum mechanics introduces an important change in the way probabilities are computed. Probabilities are still represented by the usual real numbers we use for probabilities in our everyday world, but probabilities are computed as the square of probability amplitudes. Probability amplitudes are complex numbers.
Feynman avoids exposing the reader to the mathematics of complex numbers by using a simple but accurate representation of them as arrows on a piece of paper or screen. (These must not be confused with the arrows of Feynman diagrams which are actually simplified representations in two dimensions of a relationship between points in three dimensions of space and one of time.) The amplitude arrows are fundamental to the description of the world given by quantum theory. No satisfactory reason has been given for "why" they are needed. But pragmatically we have to accept that they are an essential part of our description of all quantum phenomena. They are related to our everyday ideas of probability by the simple rule that the probability of an event is the square of the length of the corresponding amplitude arrow. So, for a given process, if two probability amplitudes, v and w, are involved, the probability of the process will be given either by
or
The rules as regards adding or multiplying, however, are the same as above. But where you would expect to add or multiply probabilities, instead you add or multiply probability amplitudes that now are complex numbers.
Addition and multiplication are familiar operations in the theory of complex numbers and are given in the figures. The sum is found as follows. Let the start of the second arrow be at the end of the first. The sum is then a third arrow that goes directly from the start of the first to the end of the second. The product of two arrows is an arrow whose length is the product of the two lengths. The direction of the product is found by adding the angles that each of the two have been turned through relative to a reference direction: that gives the angle that the product is turned relative to the reference direction.
That change, from probabilities to probability amplitudes, complicates the mathematics without changing the basic approach. But that change is still not quite enough because it fails to take into account the fact that both photons and electrons can be polarized, which is to say that their orientations in space and time have to be taken into account. Therefore, P(A to B) actually consists of 16 complex numbers, or probability amplitude arrows. There are also some minor changes to do with the quantity "j", which may have to be rotated by a multiple of 90° for some polarizations, which is only of interest for the detailed bookkeeping.
Associated with the fact that the electron can be polarized is another small necessary detail which is connected with the fact that an electron is a fermion and obeys Fermi–Dirac statistics. The basic rule is that if we have the probability amplitude for a given complex process involving more than one electron, then when we include (as we always must) the complementary Feynman diagram in which we just exchange two electron events, the resulting amplitude is the reverse – the negative – of the first. The simplest case would be two electrons starting at A and B ending at C and D. The amplitude would be calculated as the "difference", , where we would expect, from our everyday idea of probabilities, that it would be a sum.
Propagators.
Finally, one has to compute P (A to B) and E (C to D) corresponding to the probability amplitudes for the photon and the electron respectively. These are essentially the solutions of the Dirac Equation which describes the behavior of the electron's probability amplitude and the Klein–Gordon equation which describes the behavior of the photon's probability amplitude. These are called Feynman propagators. The translation to a notation commonly used in the standard literature is as follows:
where a shorthand symbol such as formula_4 stands for the four real numbers which give the time and position in three dimensions of the point labeled A.
Mass renormalization.
A problem arose historically which held up progress for twenty years: although we start with the assumption of three basic "simple" actions, the rules of the game say that if we want to calculate the probability amplitude for an electron to get from A to B we must take into account all the possible ways: all possible Feynman diagrams with those end points. Thus there will be a way in which the electron travels to C, emits a photon there and then absorbs it again at D before moving on to B. Or it could do this kind of thing twice, or more. In short we have a fractal-like situation in which if we look closely at a line it breaks up into a collection of "simple" lines, each of which, if looked at closely, are in turn composed of "simple" lines, and so on "ad infinitum". This is a very difficult situation to handle. If adding that detail only altered things slightly then it would not have been too bad, but disaster struck when it was found that the simple correction mentioned above led to "infinite" probability amplitudes. In time this problem was "fixed" by the technique of renormalization. However, Feynman himself remained unhappy about it, calling it a "dippy process".
Conclusions.
Within the above framework physicists were then able to calculate to a high degree of accuracy some of the properties of electrons, such as the anomalous magnetic dipole moment. However, as Feynman points out, it fails totally to explain why particles such as the electron have the masses they do. "There is no theory that adequately explains these numbers. We use the numbers in all our theories, but we don't understand them – what they are, or where they come from. I believe that from a fundamental point of view, this is a very interesting and serious problem."
Mathematics.
Mathematically, QED is an abelian gauge theory with the symmetry group U(1). The gauge field, which mediates the interaction between the charged spin-1/2 fields, is the electromagnetic field.
The QED Lagrangian for a spin-1/2 field interacting with the electromagnetic field is given by the real part of
where
Equations of motion.
To begin, substituting the definition of "D" into the Lagrangian gives us
Next, we can substitute this Lagrangian into the Euler–Lagrange equation of motion for a field:
to find the field equations for QED.
The two terms from this Lagrangian are then
Substituting these two back into the Euler–Lagrange equation () results in
with complex conjugate
Bringing the middle term to the right-hand side transforms this second equation into
The left-hand side is like the original Dirac equation and the right-hand side is the interaction with the electromagnetic field.
One further important equation can be found by substituting the above Lagrangian into another Euler–Lagrange equation, this time for the field formula_17:
The two terms this time are
and these two terms, when substituted back into () give us
Now, if we impose the Lorenz gauge condition, that the divergence of the four potential vanishes
then we get
which is a wave equation for the four potential, the QED version of the classical Maxwell equations in the Lorenz gauge. (In the above equation, the square represents the D'Alembert operator.)
Interaction picture.
This theory can be straightforwardly quantized by treating bosonic and fermionic sectors as free. This permits us to build a set of asymptotic states which can be used to start a computation of the probability amplitudes for different processes. In order to do so, we have to compute an evolution operator that, for a given initial state formula_22, will give a final state formula_23 in such a way to have
This technique is also known as the S-matrix. The evolution operator is obtained in the interaction picture where time evolution is given by the interaction Hamiltonian, which is the integral over space of the second term in the Lagrangian density given above:
and so, one has
where "T" is the time ordering operator. This evolution operator only has meaning as a series, and what we get here is a perturbation series with the fine structure constant as the development parameter. This series is called the Dyson series.
Feynman diagrams.
Despite the conceptual clarity of this Feynman approach to QED, almost no early textbooks follow him in their presentation. When performing calculations it is much easier to work with the Fourier transforms of the propagators. Quantum physics considers particles' momenta rather than their positions, and it is convenient to think of particles as being created or annihilated when they interact. Feynman diagrams then "look" the same, but the lines have different interpretations. The electron line represents an electron with a given energy and momentum, with a similar interpretation of the photon line. A vertex diagram represents the annihilation of one electron and the creation of another together with the absorption or creation of a photon, each having specified energies and momenta.
Using Wick theorem on the terms of the Dyson series, all the terms of the S-matrix for quantum electrodynamics can be computed through the technique of Feynman diagrams. In this case rules for drawing are the following
To these rules we must add a further one for closed loops that implies an integration on momenta formula_27, since these internal ("virtual") particles are not constrained to any specific energy–momentum – even that usually required by special relativity (see this article for details).
From them, computations of probability amplitudes are straightforwardly given. An example is Compton scattering, with an electron and a photon undergoing elastic scattering. Feynman diagrams are in this case
and so we are able to get the corresponding amplitude at the first order of a perturbation series for the S-matrix:
from which we are able to compute the cross section for this scattering.
Renormalizability.
Higher order terms can be straightforwardly computed for the evolution operator but these terms display diagrams containing the following simpler ones
that, being closed loops, imply the presence of diverging integrals having no mathematical meaning. To overcome this difficulty, a technique called renormalization has been devised, producing finite results in very close agreement with experiments. It is important to note that a criterion for theory being meaningful after renormalization is that the number of diverging diagrams is finite. In this case the theory is said to be renormalizable. The reason for this is that to get observables renormalized one needs a finite number of constants to maintain the predictive value of the theory untouched. This is exactly the case of quantum electrodynamics displaying just three diverging diagrams. This procedure gives observables in very close agreement with experiment as seen e.g. for electron gyromagnetic ratio.
Renormalizability has become an essential criterion for a quantum field theory to be considered as a viable one. All the theories describing fundamental interactions, except gravitation whose quantum counterpart is presently under very active research, are renormalizable theories.
Nonconvergence of series.
An argument by Freeman Dyson shows that the radius of convergence of the perturbation series in QED is zero. The basic argument goes as follows: if the coupling constant were negative, this would be equivalent to the Coulomb force constant being negative. This would "reverse" the electromagnetic interaction so that "like" charges would "attract" and "unlike" charges would "repel". This would render the vacuum unstable against decay into a cluster of electrons on one side of the universe and a cluster of positrons on the other side of the universe. Because the theory is 'sick' for any negative value of the coupling constant, the series do not converge, but are an asymptotic series.
From a modern perspective, we say that QED is not well defined as a quantum field theory to arbitrarily high energy. The coupling constant runs to infinity at finite energy, signalling a Landau pole. The problem is essentially that QED appears to suffer from quantum triviality issues. This is one of the motivations for embedding QED within a Grand Unified Theory.

</doc>
<doc id="25336" url="https://en.wikipedia.org/wiki?curid=25336" title="Quantum entanglement">
Quantum entanglement

Quantum entanglement is a physical phenomenon that occurs when pairs or groups of particles are generated or interact in ways such that the quantum state of each particle cannot be described independently—instead, a quantum state may be given for the system as a whole.
Measurements of physical properties such as position, momentum, spin, polarization, etc. performed on entangled particles are found to be appropriately correlated. For example, if a pair of particles is generated in such a way that their total spin is known to be zero, and one particle is found to have clockwise spin on a certain axis, then the spin of the other particle, measured on the same axis, will be found to be counterclockwise; because of the nature of quantum measurement. However, this behavior gives rise to paradoxical effects: any measurement of a property of a particle can be seen as acting on that particle (e.g. by collapsing a number of superposed states); and in the case of entangled particles, such action must be on the entangled system as a whole. It thus appears that one particle of an entangled pair "knows" what measurement has been performed on the other, and with what outcome, even though there is no known means for such information to be communicated between the particles, which at the time of measurement may be separated by arbitrarily large distances.
Such phenomena were the subject of a 1935 paper by Albert Einstein, Boris Podolsky, and Nathan Rosen, and several papers by Erwin Schrödinger shortly thereafter, describing what came to be known as the EPR paradox. Einstein and others considered such behavior to be impossible, as it violated the local realist view of causality (Einstein referring to it as "spooky action at a distance") and argued that the accepted formulation of quantum mechanics must therefore be incomplete. Later, however, the counterintuitive predictions of quantum mechanics were verified experimentally. Experiments have been performed involving measuring the polarization or spin of entangled particles in different directions, which – by producing violations of Bell's inequality – demonstrate statistically that the local realist view cannot be correct. This has been shown to occur even when the measurements are performed more quickly than light could travel between the sites of measurement: there is no lightspeed or slower influence that can pass between the entangled particles. Recent experiments have measured entangled particles within less than one one-hundredth of a percent of the travel time of light between them. According to the formalism of quantum theory, the effect of measurement happens instantly. It is not possible, however, to use this effect to transmit classical information at faster-than-light speeds (see Faster-than-light → Quantum mechanics).
Quantum entanglement is an area of extremely active research by the physics community, and its effects have been demonstrated experimentally with photons, electrons, molecules the size of buckyballs, and even small diamonds.
 Research is also focused on the utilization of entanglement effects in communication and computation.
History.
The counterintuitive predictions of quantum mechanics about strongly correlated systems were first discussed by Albert Einstein in 1935, in a joint paper with Boris Podolsky and Nathan Rosen. In this study, they formulated the EPR paradox (Einstein, Podolsky, Rosen paradox), a thought experiment that attempted to show that quantum mechanical theory was incomplete. They wrote: "We are thus forced to conclude that the quantum-mechanical description of physical reality given by wave functions is not complete."
However, they did not coin the word "entanglement", nor did they generalize the special properties of the state they considered. Following the EPR paper, Erwin Schrödinger wrote a letter (in German) to Einstein in which he used the word "Verschränkung" (translated by himself as "entanglement") "to describe the correlations between two particles that interact and then separate, as in the EPR experiment." He shortly thereafter published a seminal paper defining and discussing the notion, and terming it "entanglement." In the paper he recognized the importance of the concept, and stated: "I would not call [entanglement] "one" but rather "the" characteristic trait of quantum mechanics, the one that enforces its entire departure from classical lines of thought."
Like Einstein, Schrödinger was dissatisfied with the concept of entanglement, because it seemed to violate the speed limit on the transmission of information implicit in the theory of relativity. Einstein later famously derided entanglement as "spukhafte Fernwirkung" or "spooky action at a distance."
The EPR paper generated significant interest among physicists and inspired much discussion about the foundations of quantum mechanics (perhaps most famously Bohm's interpretation of quantum mechanics), but produced relatively little other published work. So, despite the interest, the weak point in EPR's argument was not discovered until 1964, when John Stewart Bell proved that one of their key assumptions, the principle of locality, which underlies the kind of hidden variables interpretation hoped for by EPR, was mathematically inconsistent with the predictions of quantum theory. Specifically, he demonstrated an upper limit, seen in Bell's inequality, regarding the strength of correlations that can be produced in any theory obeying local realism, and he showed that quantum theory predicts violations of this limit for certain entangled systems. His inequality is experimentally testable, and there have been numerous relevant experiments, starting with the pioneering work of Freedman and Clauser in 1972 and Aspect's experiments in 1982. They have all shown agreement with quantum mechanics rather than the principle of local realism. However, the issue is not finally settled, as each of these experimental tests has left open at least one loophole by which it is possible to question the validity of the results.
The work of Bell raised the possibility of using these super-strong correlations as a resource for communication. It led to the discovery of quantum key distribution protocols, most famously BB84 by Bennet and Brassard and E91 by Artur Ekert. Although BB84 does not use entanglement, Ekert's protocol uses the violation of a Bell's inequality as a proof of security.
Concept.
Meaning of entanglement.
An entangled system is defined to be one whose quantum state cannot be factored as a product of states of its local constituents (e.g. individual particles). If entangled, one constituent cannot be fully described without considering the other(s). Note that the state of a composite system is always expressible as a "sum", or superposition, of products of states of local constituents; it is entangled if this sum necessarily has more than one term.
Quantum systems can become entangled through various types of interactions. For some ways in which entanglement may be achieved for experimental purposes, see the section below on methods. Entanglement is broken when the entangled particles decohere through interaction with the environment; for example, when a measurement is made.
As an example of entanglement: a subatomic particle decays into an entangled pair of other particles. The decay events obey the various conservation laws, and as a result, the measurement outcomes of one daughter particle must be highly correlated with the measurement outcomes of the other daughter particle (so that the total momenta, angular momenta, energy, and so forth remains roughly the same before and after this process). For instance, a spin-zero particle could decay into a pair of spin-½ particles. Since the total spin before and after this decay must be zero (conservation of angular momentum), whenever the first particle is measured to be spin up on some axis, the other (when measured on the same axis) is always found to be spin down. (This is called the "spin anti-correlated" case; and if the prior probabilities for measuring each spin are equal, the pair is said to be in the singlet state.)
Paradox.
The paradox here is that a measurement made on either of the particles apparently collapses the state of the entire entangled system – and does so instantaneously, before any information about the measurement could have reached the other particle (assuming that information cannot travel faster than light). In the quantum formalism, the result of a spin measurement on one of the particles is a collapse into a state in which each particle has a definite spin (either up or down) along the axis of measurement. The outcome is taken to be random, with each possibility having a probability of 50%. However, if both spins are measured along the same axis, they are found to be anti-correlated. This means that the random outcome of the measurement made on one particle seems to have been transmitted to the other, so that it can make the "right choice" when it is measured. The distance and timing of the measurements can be chosen so as to make the interval between the two measurements spacelike, i.e. from any of the two measuring events to the other a message would have to travel faster than light. Then, according to the principles of special relativity, it is not in fact possible for any information to travel between two such measuring events—it is not even possible to say which of the measurements came first, as this would depend on the inertial system of the observer. Therefore the correlation between the two measurements cannot appropriately be explained as one measurement determining the other: different observers would disagree about the role of cause and effect.
Hidden variables theory.
A possible resolution to the paradox might be to assume that the state of the particles contains some hidden variables, whose values effectively determine, right from the moment of separation, what the outcomes of the spin measurements are going to be. This would mean that each particle carries all the required information with it, and nothing needs to be transmitted from one particle to the other at the time of measurement. It was originally believed by Einstein and others (see the previous section) that this was the only way out, and therefore that the accepted quantum mechanical description (with a random measurement outcome) must be incomplete. (In fact similar paradoxes can arise even without entanglement: the position of a single particle is spread out over space, and two widely separated detectors attempting to detect the particle in two different places must instantaneously attain appropriate correlation, so that they do not "both" detect the particle.)
Violations of Bell's inequality.
The hidden variables theory fails, however, when we consider measurements of the spin of entangled particles along different axes (for example, along any of three axes which make angles of 120 degrees). If a large number of pairs of such measurements are made (on a large number of pairs of entangled particles), then statistically, if the local realist or hidden variables view were correct, the results would always satisfy Bell's inequality. A number of experiments have shown in practice that Bell's inequality is not satisfied. However, all experiments have loopholes problem. When measurements of the entangled particles are made in moving relativistic reference frames, in which each measurement (in its own relativistic time frame) occurs before the other, the measurement results remain correlated.
The fundamental issue about measuring spin along different axes is that these measurements cannot have definite values at the same time―they are incompatible in the sense that these measurements' maximum simultaneous precision is constrained by the uncertainty principle. This is contrary to what is found in classical physics, where any number of properties can be measured simultaneously with arbitrary accuracy. It has been proven mathematically that compatible measurements cannot show Bell-inequality-violating correlations, and thus entanglement is a fundamentally non-classical phenomenon.
Other types of experiments.
In a 2012 experiment, "delayed-choice entanglement swapping" was used to decide whether two particles were entangled or not after they had already been measured.
In a 2013 experiment, entanglement swapping has been used to create entanglement between photons that never coexisted in time, thus demonstrating that "the nonlocality of quantum mechanics, as manifested by entanglement, does not apply only to particles with spacelike separation, but also to particles with timelike [i.e., temporal] separation".
In three independent experiments it was shown that classically-communicated separable quantum states can be used to carry entangled states.
In August 2014, researcher Gabriela Barreto Lemos and team were able to "take pictures" of objects using photons that have not interacted with the subjects, but were entangled with photons that did interact with such objects. Lemos, from the University of Vienna, is confident that this new quantum imaging technique could find application where low light imaging is imperative, in fields like biological or medical imaging.
Special Theory of Relativity.
Another theory explains quantum entanglement using special relativity. According to this theory, faster-than-light communication between entangled systems can be achieved because the time dilation of special relativity causes time to stand still in light's point of view. For example, in the case of two entangled photons, a measurement made on one photon at present time would determine the state of the photon for both the present and past at the same moment. This leads to the instantaneous determination of the state of the other photon. Corresponding logic is applied to explain entangled systems, i.e. electron and positron, that travel below the speed of light.
Mystery of time.
There exist physicists who say that time is an emergent phenomenon that is a side effect of quantum entanglement. The
Wheeler–DeWitt equation that combines general relativity and quantum mechanics – by leaving out time altogether – was introduced in the 1960s and it was taken up again in 1983, when the theorists Don Page and William Wootters made a solution based on the quantum phenomenon of entanglement. Page and Wootters argued that entanglement can be used to measure time.
In 2013, at the Istituto Nazionale di Ricerca Metrologica (INRIM) in Turin, Italy, Ekaterina Moreva, together with Giorgio Brida, Marco Gramegna, Vittorio Giovannetti, Lorenzo Maccone, and Marco Genovese performed the first experimental test of Page and Wootters' ideas. Their result has been interpreted to confirm that time is an emergent phenomenon for internal observers but absent for external observers of the universe.
Source for the arrow of time.
Physicist Seth Lloyd says that quantum uncertainty gives rise to "entanglement", the putative source of the arrow of time. According to Lloyd; "The arrow of time is an arrow of increasing correlations."
Non-locality and hidden variables.
There is much confusion about the meaning of entanglement, non-locality and hidden variables and how they relate to each other. As described above, entanglement is an experimentally verified and accepted property of nature, which has critical implications for the interpretations of quantum mechanics. The question becomes, "How can one account for something that was at one point indefinite with regard to its spin (or whatever is in this case the subject of investigation) suddenly becoming definite in that regard even though no physical interaction with the second object occurred, and, if the two objects are sufficiently far separated, could not even have had the time needed for such an interaction to proceed from the first to the second object?" The latter question involves the issue of locality, i.e., whether for a change to occur in something the agent of change has to be in physical contact (at least via some intermediary such as a field force) with the thing that changes. Study of entanglement brings into sharp focus the dilemma between locality and the completeness or lack of completeness of quantum mechanics.
Bell's theorem and related results rule out a local realistic explanation for quantum mechanics (one which obeys the principle of locality while also ascribing definite values to quantum observables). However, in other interpretations, the experiments that demonstrate the apparent non-locality can also be described in local terms: If each distant observer regards the other as a quantum system, communication between the two must then be treated as a measurement process, and this communication is strictly local. In particular, in the Many-worlds interpretation, the underlying description is fully local. More generally, the question of locality in quantum physics is extraordinarily subtle and sometimes hinges on precisely how it is defined.
In the media and popular science, quantum non-locality is often portrayed as being equivalent to entanglement. While it is true that a bipartite quantum state must be entangled in order for it to produce non-local correlations, there exist entangled states that do not produce such correlations. A well-known example of this is the Werner state that is entangled for certain values of formula_1, but can always be described using local hidden variables. In short, entanglement of a two-party state is necessary but not sufficient for that state to be non-local. Moreover, it was shown that, for arbitrary number of party, there exist states that are genuinely entangled but admits a fully local strategy. It is important to recognize that entanglement is more commonly viewed as an algebraic concept, noted for being a precedent to non-locality as well as to quantum teleportation and to superdense coding, whereas non-locality is defined according to experimental statistics and is much more involved with the foundations and interpretations of quantum mechanics.
Quantum mechanical framework.
The following subsections are for those with a good working knowledge of the formal, mathematical description of quantum mechanics, including familiarity with the formalism and theoretical framework developed in the articles: bra–ket notation and mathematical formulation of quantum mechanics.
Pure states.
Consider two noninteracting systems and , with respective Hilbert spaces and . The Hilbert space of the composite system is the tensor product
If the first system is in state formula_3 and the second in state formula_4, the state of the composite system is
States of the composite system which can be represented in this form are called "separable states", or (in the simplest case) "product states".
Not all states are separable states (and thus product states). Fix a basis formula_6 for and a basis formula_7 for . The most general state in is of the form
This state is separable if there exist vectors formula_9 so that formula_10 yielding formula_11 and formula_12 It is inseparable if for any vectors formula_9 at least for one pair of coordinates formula_14 we have formula_15 If a state is inseparable, it is called an "entangled state".
For example, given two basis vectors formula_16 of and two basis vectors formula_17 of , the following is an entangled state:
If the composite system is in this state, it is impossible to attribute to either system or system a definite pure state. Another way to say this is that while the von Neumann entropy of the whole state is zero (as it is for any pure state), the entropy of the subsystems is greater than zero. In this sense, the systems are "entangled". This has specific empirical ramifications for interferometry. It is worthwhile to note that the above example is one of four Bell states, which are (maximally) entangled pure states (pure states of the space, but which cannot be separated into pure states of each and ).
Now suppose Alice is an observer for system , and Bob is an observer for system . If in the entangled state given above Alice makes a measurement in the formula_19 eigenbasis of , there are two possible outcomes, occurring with equal probability:
If the former occurs, then any subsequent measurement performed by Bob, in the same basis, will always return 1. If the latter occurs, (Alice measures 1) then Bob's measurement will return 0 with certainty. Thus, system has been altered by Alice performing a local measurement on system . This remains true even if the systems and are spatially separated. This is the foundation of the EPR paradox.
The outcome of Alice's measurement is random. Alice cannot decide which state to collapse the composite system into, and therefore cannot transmit information to Bob by acting on her system. Causality is thus preserved, in this particular scheme. For the general argument, see no-communication theorem.
Ensembles.
As mentioned above, a state of a quantum system is given by a unit vector in a Hilbert space. More generally, if one has a large number of copies of the same system, then the state of this "ensemble" is described by a density matrix, which is a positive-semidefinite matrix, or a trace class when the state space is infinite-dimensional, and has trace 1. Again, by the spectral theorem, such a matrix takes the general form:
where the "w"i are positive-valued probabilities (they sum up to 1), the vectors are unit vectors, and in the infinite-dimensional case, we would take the closure of such states in the trace norm. We can interpret as representing an ensemble where is the proportion of the ensemble whose states are formula_23. When a mixed state has rank 1, it therefore describes a "pure ensemble". When there is less than total information about the state of a quantum system we need density matrices to represent the state.
Experimentally, a mixed ensemble might be realized as follows. Consider a "black box" apparatus that spits electrons towards an observer. The electrons' Hilbert spaces are identical. The apparatus might produce electrons that are all in the same state; in this case, the electrons received by the observer are then a pure ensemble. However, the apparatus could produce electrons in different states. For example, it could produce two populations of electrons: one with state formula_24 with spins aligned in the positive direction, and the other with state formula_25 with spins aligned in the negative direction. Generally, this is a mixed ensemble, as there can be any number of populations, each corresponding to a different state.
Following the definition above, for a bipartite composite system, mixed states are just density matrices on . That is, it has the general form
where the "w"i are positively valued probabilities, formula_27, and the vectors are unit vectors. This is self-adjoint and positive and has trace 1.
Extending the definition of separability from the pure case, we say that a mixed state is separable if it can be written as
where the are positively valued probabilities and the formula_29's and formula_30's are themselves mixed states (density operators) on the subsystems and respectively. In other words, a state is separable if it is a probability distribution over uncorrelated states, or product states. By writing the density matrices as sums of pure ensembles and expanding, we may assume without loss of generality that formula_29 and formula_30 are themselves pure ensembles. A state is then said to be "entangled" if it is not separable.
In general, finding out whether or not a mixed state is entangled is considered difficult. The general bipartite case has been shown to be NP-hard. For the and cases, a necessary and sufficient criterion for separability is given by the famous Positive Partial Transpose (PPT) condition.
Reduced density matrices.
The idea of a reduced density matrix was introduced by Paul Dirac in 1930. Consider as above systems and each with a Hilbert space . Let the state of the composite system be
As indicated above, in general there is no way to associate a pure state to the component system . However, it still is possible to associate a density matrix. Let
which is the projection operator onto this state. The state of is the partial trace of over the basis of system :
 is sometimes called the reduced density matrix of on subsystem . Colloquially, we "trace out" system to obtain the reduced density matrix on .
For example, the reduced density matrix of for the entangled state
discussed above is
This demonstrates that, as expected, the reduced density matrix for an entangled pure ensemble is a mixed ensemble. Also not surprisingly, the density matrix of for the pure product state formula_38 discussed above is
In general, a bipartite pure state ρ is entangled if and only if its reduced states are mixed rather than pure.
Two applications that use them.
Reduced density matrices were explicitly calculated in different spin chains with unique ground state. An example is the one-dimensional AKLT spin chain: the ground state can be divided into a block and an environment. The reduced density matrix of the block is proportional to a projector to a degenerate ground state of another Hamiltonian.
The reduced density matrix also was evaluated for XY spin chains, where it has full rank. It was proved that in the thermodynamic limit, the spectrum of the reduced density matrix of a large block of spins is an exact geometric sequence in this case.
Entropy.
In this section, the entropy of a mixed state is discussed as well as how it can be viewed as a measure of quantum entanglement.
Definition.
In classical information theory, the Shannon entropy, is associated to a probability distribution,formula_40, in the following way:
Since a mixed state is a probability distribution over an ensemble, this leads naturally to the definition of the von Neumann entropy:
In general, one uses the Borel functional calculus to calculate a non-polynomial function such as . If the nonnegative operator acts on a finite-dimensional Hilbert space and has eigenvalues formula_43, turns out to be nothing more than the operator with the same eigenvectors, but the eigenvalues formula_44. The Shannon entropy is then:
Since an event of probability 0 should not contribute to the entropy, and given that
the convention is adopted. This extends to the infinite-dimensional case as well: if has spectral resolution
assume the same convention when calculating
As in statistical mechanics, the more uncertainty (number of microstates) the system should possess, the larger the entropy. For example, the entropy of any pure state is zero, which is unsurprising since there is no uncertainty about a system in a pure state. The entropy of any of the two subsystems of the entangled state discussed above is (which can be shown to be the maximum entropy for mixed states).
As a measure of entanglement.
Entropy provides one tool which can be used to quantify entanglement, although other entanglement measures exist. If the overall system is pure, the entropy of one subsystem can be used to measure its degree of entanglement with the other subsystems.
For bipartite pure states, the von Neumann entropy of reduced states is the unique measure of entanglement in the sense that it is the only function on the family of states that satisfies certain axioms required of an entanglement measure.
It is a classical result that the Shannon entropy achieves its maximum at, and only at, the uniform probability distribution {1/"n"...,1/"n"}. Therefore, a bipartite pure state is said to be a maximally entangled state if the reduced state of is the diagonal matrix
For mixed states, the reduced von Neumann entropy is not the only reasonable entanglement measure.
As an aside, the information-theoretic definition is closely related to entropy in the sense of statistical mechanics (comparing the two definitions, we note that, in the present context, it is customary to set the Boltzmann constant ). For example, by properties of the Borel functional calculus, we see that for any unitary operator ,
Indeed, without this property, the von Neumann entropy would not be well-defined.
In particular, could be the time evolution operator of the system, i.e.
where is the Hamiltonian of the system. Here the entropy is unchanged.
The reversibility of a process is associated with the resulting entropy change, i.e., a process is reversible if, and only if, it leaves the entropy of the system invariant. Therefore, the march of the arrow of time towards thermodynamic equilibrium is simply the growing spread of quantum entanglement.
This provides a connection between quantum information theory and thermodynamics.
Rényi entropy also can be used as a measure of entanglement.
Entanglement measures.
Entanglement measures quantify the amount of entanglement in a (often viewed as a bipartite) quantum state. As aforementioned, entanglement entropy is the standard measure of entanglement for pure states (but no longer a measure of entanglement for mixed states). For mixed states, there are some entanglement measures in the literature and no single one is standard.
Most (but not all) of these entanglement measures reduce for pure states to entanglement entropy, and are difficult (NP-hard) to compute.
Quantum field theory.
The Reeh-Schlieder theorem of quantum field theory is sometimes seen as an analogue of quantum entanglement.
Applications.
Entanglement has many applications in quantum information theory. With the aid of entanglement, otherwise impossible tasks may be achieved.
Among the best-known applications of entanglement are superdense coding and quantum teleportation.
Most researchers believe that entanglement is necessary to realize quantum computing (although this is disputed by some).
Entanglement is used in some protocols of quantum cryptography. This is because the "shared noise" of entanglement makes for an excellent one-time pad. Moreover, since measurement of either member of an entangled pair destroys the entanglement they share, entanglement-based quantum cryptography allows the sender and receiver to more easily detect the presence of an interceptor.
In interferometry, entanglement is necessary for surpassing the standard quantum limit and achieving the Heisenberg limit.
Entangled states.
There are several canonical entangled states that appear often in theory and experiments.
For two qubits, the Bell states are
These four pure states are all maximally entangled (according to the entropy of entanglement) and form an orthonormal basis (linear algebra) of the Hilbert space of the two qubits. They play a fundamental role in Bell's theorem.
For M>2 qubits, the GHZ state is
which reduces to the Bell state formula_55 for formula_56. The traditional GHZ state was defined for formula_57. GHZ states are occasionally extended to "qudits", i.e. systems of "d" rather than 2 dimensions.
Also for M>2 qubits, there are spin squeezed states. Spin squeezed states are a class of squeezed coherent states satisfying certain restrictions on the uncertainty of spin measurements, and are necessarily entangled. Spin squeezed states are good candidates for enhancing precision measurements using quantum entanglement. 
For two bosonic modes, a NOON state is
This is like a Bell state formula_55 except the basis kets 0 and 1 have been replaced with "the "N" photons are in one mode" and "the "N" photons are in the other mode".
Finally, there also exist twin Fock states for bosonic modes, which can be created by feeding a Fock state into two arms leading to a beam splitter. They are the sum of multiple of NOON states, and can used to achieve the Heisenberg limit.
For the appropriately chosen measure of entanglement, Bell, GHZ, and NOON states are maximally entangled while spin squeezed and twin Fock states are only partially entangled. The partially entangled states are generally easier to prepare experimentally.
Methods of creating entanglement.
Entanglement is usually created by direct interactions between subatomic particles. These interactions can take numerous forms. One of the most commonly used methods is spontaneous parametric down-conversion to generate a pair of photons entangled in polarisation. Other methods include the use of a fiber coupler to confine and mix photons, the use of quantum dots to trap electrons until decay occurs, the use of the Hong-Ou-Mandel effect, etc. In the earliest tests of Bell's theorem, the entangled particles were generated using atomic cascades.
It is also possible to create entanglement between quantum systems that never directly interacted, through the use of entanglement swapping.
Testing a system for entanglement.
Systems which contain no entanglement are said to be separable. For 2-Qubit and Qubit-Qutrit systems (2 x 2 and 2 x 3 respectively) the simple Peres-Horodecki criterion provides both a necessary and a sufficient criterion for separability, and thus for detecting entanglement. However, for the general case, the criterion is merely a sufficient one for separability, as the problem becomes NP-hard. A numerical approach to the problem is suggested by Jon Magne Leinaas, Jan Myrheim and Eirik Ovrum in their paper "Geometrical aspects of entanglement". Leinaas et al. offer a numerical approach, iteratively refining an estimated separable state towards the target state to be tested, and checking if the target state can indeed be reached. An implementation of the algorithm (including a built in Peres-Horodecki criterion testing) is brought in the "StateSeparator" web-app.

</doc>
<doc id="25267" url="https://en.wikipedia.org/wiki?curid=25267" title="Quantum field theory">
Quantum field theory

In theoretical physics, quantum field theory (QFT) is a theoretical framework for constructing quantum mechanical models of subatomic particles in particle physics and quasiparticles in condensed matter physics. A QFT treats particles as excited states of an underlying physical field, so these are called field quanta.
In quantum field theory, quantum mechanical interactions between particles are described by interaction terms between the corresponding underlying fields.
Definition.
Quantum electrodynamics (QED) has one electron field and one photon field; quantum chromodynamics (QCD) has one field for each type of quark; and, in condensed matter, there is an atomic displacement field that gives rise to phonon particles. Edward Witten describes QFT as "by far" the most difficult theory in modern physics.
Dynamics.
Ordinary quantum mechanical systems have a fixed number of particles, with each particle having a finite number of degrees of freedom. In contrast, the excited states of a QFT can represent any number of particles. This makes quantum field theories especially useful for describing systems where the particle count/number may change over time, a crucial feature of relativistic dynamics.
States.
QFT interaction terms are similar in spirit to those between charges with electric and magnetic fields in Maxwell's equations. However, unlike the classical fields of Maxwell's theory, fields in QFT generally exist in quantum superpositions of states and are subject to the laws of quantum mechanics.
Because the fields are continuous quantities over space, there exist excited states with arbitrarily large numbers of particles in them, providing QFT systems with an effectively infinite number of degrees of freedom. Infinite degrees of freedom can easily lead to divergences of calculated quantities (i.e., the quantities become infinite). Techniques such as renormalization of QFT parameters or discretization of spacetime, as in lattice QCD, are often used to avoid such infinities so as to yield physically meaningful results.
Fields and radiation.
The gravitational field and the electromagnetic field are the only two fundamental fields in nature that have infinite range and a corresponding classical low-energy limit, which greatly diminishes and hides their "particle-like" excitations. Albert Einstein in 1905, attributed "particle-like" and discrete exchanges of momenta and energy, characteristic of "field quanta", to the electromagnetic field. Originally, his principal motivation was to explain the thermodynamics of radiation. Although the photoelectric effect and Compton scattering strongly suggest the existence of the photon, it might alternately be explained by a mere quantization of emission; more definitive evidence of the quantum nature of radiation is now taken up into modern quantum optics as in the antibunching effect.
Theories.
There is currently no complete quantum theory of the remaining fundamental force, gravity. Many of the proposed theories to describe gravity as a QFT postulate the existence of a graviton particle that mediates the gravitational force. Presumably, the as yet unknown correct quantum field-theoretic treatment of the gravitational field will behave like Einstein's general theory of relativity in the low-energy limit. Quantum field theory of the fundamental forces itself has been postulated to be the low-energy effective field theory limit of a more fundamental theory such as superstring theory.
Most theories in standard particle physics are formulated as relativistic quantum field theories, such as QED, QCD, and the Standard Model. QED, the quantum field-theoretic description of the electromagnetic field, approximately reproduces Maxwell's theory of electrodynamics in the low-energy limit, with small non-linear corrections to the Maxwell equations required due to virtual electron–positron pairs.
In the perturbative approach to quantum field theory, the full field interaction terms are approximated as a perturbative expansion in the number of particles involved. Each term in the expansion can be thought of as forces between particles being mediated by other particles. In QED, the electromagnetic force between two electrons is caused by an exchange of photons. Similarly, intermediate vector bosons mediate the weak force and gluons mediate the strong force in QCD. The notion of a force-mediating particle comes from perturbation theory, and does not make sense in the context of non-perturbative approaches to QFT, such as with bound states.
History.
Foundations.
The early development of the field involved Dirac, Fock, Pauli, Heisenberg and Bogolyubov. This phase of development culminated with the construction of the theory of quantum electrodynamics in the 1950s.
Gauge theory.
Gauge theory was formulated and quantized, leading to the unification of forces embodied in the standard model of particle physics. This effort started in the 1950s with the work of Yang and Mills, was carried on by Martinus Veltman and a host of others during the 1960s and completed by the 1970s through the work of Gerard 't Hooft, Frank Wilczek, David Gross and David Politzer.
Grand synthesis.
Parallel developments in the understanding of phase transitions in condensed matter physics led to the study of the renormalization group. This in turn led to the grand synthesis of theoretical physics, which unified theories of particle and condensed matter physics through quantum field theory. This involved the work of Michael Fisher and Leo Kadanoff in the 1970s, which led to the seminal reformulation of quantum field theory by Kenneth G. Wilson in 1975.
Principles.
Classical and quantum fields.
A classical field is a function defined over some region of space and time. Two physical phenomena which are described by classical fields are Newtonian gravitation, described by Newtonian gravitational field g(x, "t"), and classical electromagnetism, described by the electric and magnetic fields E(x, "t") and B(x, "t"). Because such fields can in principle take on distinct values at each point in space, they are said to have infinite degrees of freedom.
Classical field theory does not, however, account for the quantum-mechanical aspects of such physical phenomena. For instance, it is known from quantum mechanics that certain aspects of electromagnetism involve discrete particles—photons—rather than continuous fields. The business of "quantum" field theory is to write down a field that is, like a classical field, a function defined over space and time, but which also accommodates the observations of quantum mechanics. This is a "quantum field".
It is not immediately clear "how" to write down such a quantum field, since quantum mechanics has a structure very unlike a field theory. In its most general formulation, quantum mechanics is a theory of abstract operators (observables) acting on an abstract state space (Hilbert space), where the observables represent physically observable quantities and the state space represents the possible states of the system under study. For instance, the fundamental observables associated with the motion of a single quantum mechanical particle are the position and momentum operators formula_1 and formula_2. Field theory, in contrast, treats "x" as a way to index the field rather than as an operator.
There are two common ways of developing a quantum field: the path integral formalism and canonical quantization. The latter of these is pursued in this article.
Lagrangian formalism.
Quantum field theory frequently makes use of the Lagrangian formalism from classical field theory. This formalism is analogous to the Lagrangian formalism used in classical mechanics to solve for the motion of a particle under the influence of a field. In classical field theory, one writes down a Lagrangian density, formula_3, involving a field, φ(x,"t"), and possibly its first derivatives (∂φ/∂"t" and ∇φ), and then applies a field-theoretic form of the Euler–Lagrange equation. Writing coordinates ("t", x) = ("x"0, "x"1, "x"2, "x"3) = "x"μ, this form of the Euler–Lagrange equation is
where a sum over μ is performed according to the rules of Einstein notation.
By solving this equation, one arrives at the "equations of motion" of the field. For example, if one begins with the Lagrangian density
and then applies the Euler–Lagrange equation, one obtains the equation of motion
This equation is Newton's law of universal gravitation, expressed in differential form in terms of the gravitational potential φ("t", x) and the mass density ρ("t", x). Despite the nomenclature, the "field" under study is the gravitational potential, φ, rather than the gravitational field, g. Similarly, when classical field theory is used to study electromagnetism, the "field" of interest is the electromagnetic four-potential ("V"/"c", A), rather than the electric and magnetic fields E and B.
Quantum field theory uses this same Lagrangian procedure to determine the equations of motion for quantum fields. These equations of motion are then supplemented by commutation relations derived from the canonical quantization procedure described below, thereby incorporating quantum mechanical effects into the behavior of the field.
Single- and many-particle quantum mechanics.
In quantum mechanics, a particle (such as an electron or proton) is described by a complex wavefunction, , whose time-evolution is governed by the Schrödinger equation:
There are several shortcomings to the above description of quantum mechanics, which are addressed by quantum field theory. First, it is unclear how to extend quantum mechanics to include the effects of special relativity. Attempted replacements for the Schrödinger equation, such as the Klein–Gordon equation or the Dirac equation, have many unsatisfactory qualities; for instance, they possess energy eigenvalues that extend to –∞, so that there seems to be no easy definition of a ground state. It turns out that such inconsistencies arise from relativistic wavefunctions not having a well-defined probabilistic interpretation in position space, as probability conservation is not a relativistically covariant concept. The second shortcoming, related to the first, is that in quantum mechanics there is no mechanism to describe particle creation and annihilation; this is crucial for describing phenomena such as pair production, which result from the conversion between mass and energy according to the relativistic relation "E" = "mc"2.
Second quantization.
In this section, we will describe a method for constructing a quantum field theory called second quantization. This basically involves choosing a way to index the quantum mechanical degrees of freedom in the space of multiple identical-particle states. It is based on the Hamiltonian formulation of quantum mechanics.
Several other approaches exist, such as the Feynman path integral, which uses a Lagrangian formulation. For an overview of some of these approaches, see the article on quantization.
Bosons.
For simplicity, we will first discuss second quantization for bosons, which form perfectly symmetric quantum states. Let us denote the mutually orthogonal single-particle states which are possible in the system by formula_8 and so on. For example, the 3-particle state with one particle in state formula_9 and two in state formula_10 is
The first step in second quantization is to express such quantum states in terms of occupation numbers, by listing the number of particles occupying each of the single-particle states formula_12 etc. This is simply another way of labelling the states. For instance, the above 3-particle state is denoted as
An "N"-particle state belongs to a space of states describing systems of "N" particles. The next step is to combine the individual "N"-particle state spaces into an extended state space, known as Fock space, which can describe systems of any number of particles. This is composed of the state space of a system with no particles (the so-called vacuum state, written as formula_14), plus the state space of a 1-particle system, plus the state space of a 2-particle system, and so forth. States describing a definite number of particles are known as Fock states: a general element of Fock space will be a linear combination of Fock states. There is a one-to-one correspondence between the occupation number representation and valid boson states in the Fock space.
At this point, the quantum mechanical system has become a quantum field in the sense we described above. The field's elementary degrees of freedom are the occupation numbers, and each occupation number is indexed by a number formula_15 indicating which of the single-particle states formula_16 it refers to:
The properties of this quantum field can be explored by defining creation and annihilation operators, which add and subtract particles. They are analogous to ladder operators in the quantum harmonic oscillator problem, which added and subtracted energy quanta. However, these operators literally create and annihilate particles of a given quantum state. The bosonic annihilation operator formula_18 and creation operator formula_19 are easily defined in the occupation number representation as having the following effects:
It can be shown that these are operators in the usual quantum mechanical sense, i.e. linear operators acting on the Fock space. Furthermore, they are indeed Hermitian conjugates, which justifies the way we have written them. They can be shown to obey the commutation relation
where formula_23 stands for the Kronecker delta. These are precisely the relations obeyed by the ladder operators for an infinite set of independent quantum harmonic oscillators, one for each single-particle state. Adding or removing bosons from each state is therefore analogous to exciting or de-exciting a quantum of energy in a harmonic oscillator.
Applying an annihilation operator formula_24 followed by its corresponding creation operator formula_25 returns the number formula_26 of particles in the "k"th single-particle eigenstate:
The combination of operators formula_28 is known as the number operator for the "k"th eigenstate.
The Hamiltonian operator of the quantum field (which, through the Schrödinger equation, determines its dynamics) can be written in terms of creation and annihilation operators. For instance, for a field of free (non-interacting) bosons, the total energy of the field is found by summing the energies of the bosons in each energy eigenstate. If the "k"th single-particle energy eigenstate has energy formula_29 and there are formula_26 bosons in this state, then the total energy of these bosons is formula_31. The energy in the "entire" field is then a sum over formula_32:
This can be turned into the Hamiltonian operator of the field by replacing formula_26 with the corresponding number operator, formula_28. This yields
Fermions.
It turns out that a different definition of creation and annihilation must be used for describing fermions. According to the Pauli exclusion principle, fermions cannot share quantum states, so their occupation numbers "Ni" can only take on the value 0 or 1. The fermionic annihilation operators "c" and creation operators formula_37 are defined by their actions on a Fock state thus
These obey an anticommutation relation:
One may notice from this that applying a fermionic creation operator twice gives zero, so it is impossible for the particles to share single-particle states, in accordance with the exclusion principle.
Field operators.
We have previously mentioned that there can be more than one way of indexing the degrees of freedom in a quantum field. Second quantization indexes the field by enumerating the single-particle quantum states. However, as we have discussed, it is more natural to think about a "field", such as the electromagnetic field, as a set of degrees of freedom indexed by position.
To this end, we can define "field operators" that create or destroy a particle at a particular point in space. In particle physics, these operators turn out to be more convenient to work with, because they make it easier to formulate theories that satisfy the demands of relativity.
Single-particle states are usually enumerated in terms of their momenta (as in the particle in a box problem.) We can construct field operators by applying the Fourier transform to the creation and annihilation operators for these states. For example, the bosonic field annihilation operator formula_43 is
The bosonic field operators obey the commutation relation
where formula_46 stands for the Dirac delta function. As before, the fermionic relations are the same, with the commutators replaced by anticommutators.
The field operator is not the same thing as a single-particle wavefunction. The former is an operator acting on the Fock space, and the latter is a quantum-mechanical amplitude for finding a particle in some position. However, they are closely related, and are indeed commonly denoted with the same symbol. If we have a Hamiltonian with a space representation, say
where the indices "i" and "j" run over all particles, then the field theory Hamiltonian (in the non-relativistic limit and for negligible self-interactions) is
This looks remarkably like an expression for the expectation value of the energy, with formula_49 playing the role of the wavefunction. This relationship between the field operators and wavefunctions makes it very easy to formulate field theories starting from space-projected Hamiltonians.
Dynamics.
Once the Hamiltonian operator is obtained as part of the canonical quantization process, the time dependence of the state is described with the Schrödinger equation, just as with other quantum theories. Alternatively, the Heisenberg picture can be used where the time dependence is in the operators rather than in the states.
Implications.
Unification of fields and particles.
The "second quantization" procedure that we have outlined in the previous section takes a set of single-particle quantum states as a starting point. Sometimes, it is impossible to define such single-particle states, and one must proceed directly to quantum field theory. For example, a quantum theory of the electromagnetic field "must" be a quantum field theory, because it is impossible (for various reasons) to define a wavefunction for a single photon. In such situations, the quantum field theory can be constructed by examining the mechanical properties of the classical field and guessing the corresponding quantum theory. For free (non-interacting) quantum fields, the quantum field theories obtained in this way have the same properties as those obtained using second quantization, such as well-defined creation and annihilation operators obeying commutation or anticommutation relations.
Quantum field theory thus provides a unified framework for describing "field-like" objects (such as the electromagnetic field, whose excitations are photons) and "particle-like" objects (such as electrons, which are treated as excitations of an underlying electron field), so long as one can treat interactions as "perturbations" of free fields. There are still unsolved problems relating to the more general case of interacting fields that may or may not be adequately described by perturbation theory. For more on this topic, see Haag's theorem.
Physical meaning of particle indistinguishability.
The second quantization procedure relies crucially on the particles being identical. We would not have been able to construct a quantum field theory from a distinguishable many-particle system, because there would have been no way of separating and indexing the degrees of freedom.
Many physicists prefer to take the converse interpretation, which is that "quantum field theory explains what identical particles are". In ordinary quantum mechanics, there is not much theoretical motivation for using symmetric (bosonic) or antisymmetric (fermionic) states, and the need for such states is simply regarded as an empirical fact. From the point of view of quantum field theory, particles are identical if and only if they are excitations of the same underlying quantum field. Thus, the question "why are all electrons identical?" arises from mistakenly regarding individual electrons as fundamental objects, when in fact it is only the electron field that is fundamental.
Particle conservation and non-conservation.
During second quantization, we started with a Hamiltonian and state space describing a fixed number of particles ("N"), and ended with a Hamiltonian and state space for an arbitrary number of particles. Of course, in many common situations "N" is an important and perfectly well-defined quantity, e.g. if we are describing a gas of atoms sealed in a box. From the point of view of quantum field theory, such situations are described by quantum states that are eigenstates of the number operator formula_50, which measures the total number of particles present. As with any quantum mechanical observable, formula_50 is conserved if it commutes with the Hamiltonian. In that case, the quantum state is trapped in the "N"-particle subspace of the total Fock space, and the situation could equally well be described by ordinary "N"-particle quantum mechanics. (Strictly speaking, this is only true in the noninteracting case or in the low energy density limit of renormalized quantum field theories)
For example, we can see that the free-boson Hamiltonian described above conserves particle number. Whenever the Hamiltonian operates on a state, each particle destroyed by an annihilation operator formula_24 is immediately put back by the creation operator formula_25.
On the other hand, it is possible, and indeed common, to encounter quantum states that are "not" eigenstates of formula_50, which do not have well-defined particle numbers. Such states are difficult or impossible to handle using ordinary quantum mechanics, but they can be easily described in quantum field theory as quantum superpositions of states having different values of "N". For example, suppose we have a bosonic field whose particles can be created or destroyed by interactions with a fermionic field. The Hamiltonian of the combined system would be given by the Hamiltonians of the free boson and free fermion fields, plus a "potential energy" term such as
where formula_25 and formula_24 denotes the bosonic creation and annihilation operators, formula_58 and formula_59 denotes the fermionic creation and annihilation operators, and formula_60 is a parameter that describes the strength of the interaction. This "interaction term" describes processes in which a fermion in state "k" either absorbs or emits a boson, thereby being kicked into a different eigenstate formula_61. (In fact, this type of Hamiltonian is used to describe interaction between conduction electrons and phonons in metals. The interaction between electrons and photons is treated in a similar way, but is a little more complicated because the role of spin must be taken into account.) One thing to notice here is that even if we start out with a fixed number of bosons, we will typically end up with a superposition of states with different numbers of bosons at later times. The number of fermions, however, is conserved in this case.
In condensed matter physics, states with ill-defined particle numbers are particularly important for describing the various superfluids. Many of the defining characteristics of a superfluid arise from the notion that its quantum state is a superposition of states with different particle numbers. In addition, the concept of a coherent state (used to model the laser and the BCS ground state) refers to a state with an ill-defined particle number but a well-defined phase.
Axiomatic approaches.
The preceding description of quantum field theory follows the spirit in which most physicists approach the subject. However, it is not mathematically rigorous. Over the past several decades, there have been many attempts to put quantum field theory on a firm mathematical footing by formulating a set of axioms for it. These attempts fall into two broad classes.
The first class of axioms, first proposed during the 1950s, include the Wightman, Osterwalder–Schrader, and Haag–Kastler systems. They attempted to formalize the physicists' notion of an "operator-valued field" within the context of functional analysis, and enjoyed limited success. It was possible to prove that any quantum field theory satisfying these axioms satisfied certain general theorems, such as the spin-statistics theorem and the CPT theorem. Unfortunately, it proved extraordinarily difficult to show that any realistic field theory, including the Standard Model, satisfied these axioms. Most of the theories that could be treated with these analytic axioms were physically trivial, being restricted to low-dimensions and lacking interesting dynamics. The construction of theories satisfying one of these sets of axioms falls in the field of constructive quantum field theory. Important work was done in this area in the 1970s by Segal, Glimm, Jaffe and others.
During the 1980s, a second set of axioms based on geometric ideas was proposed. This line of investigation, which restricts its attention to a particular class of quantum field theories known as topological quantum field theories, is associated most closely with Michael Atiyah and Graeme Segal, and was notably expanded upon by Edward Witten, Richard Borcherds, and Maxim Kontsevich. However, most of the physically relevant quantum field theories, such as the Standard Model, are not topological quantum field theories; the quantum field theory of the fractional quantum Hall effect is a notable exception. The main impact of axiomatic topological quantum field theory has been on mathematics, with important applications in representation theory, algebraic topology, and differential geometry.
Finding the proper axioms for quantum field theory is still an open and difficult problem in mathematics. One of the Millennium Prize Problems—proving the existence of a mass gap in Yang–Mills theory—is linked to this issue.
Associated phenomena.
In the previous part of the article, we described the most general features of quantum field theories. Some of the quantum field theories studied in various fields of theoretical physics involve additional special ideas, such as renormalizability, gauge symmetry, and supersymmetry. These are described in the following sections.
Renormalization.
Early in the history of quantum field theory, it was found that many seemingly innocuous calculations, such as the perturbative shift in the energy of an electron due to the presence of the electromagnetic field, give infinite results. The reason is that the perturbation theory for the shift in an energy involves a sum over all other energy levels, and there are infinitely many levels at short distances that each give a finite contribution which results in a divergent series.
Many of these problems are related to failures in classical electrodynamics that were identified but unsolved in the 19th century, and they basically stem from the fact that many of the supposedly "intrinsic" properties of an electron are tied to the electromagnetic field that it carries around with it. The energy carried by a single electron—its self energy—is not simply the bare value, but also includes the energy contained in its electromagnetic field, its attendant cloud of photons. The energy in a field of a spherical source diverges in both classical and quantum mechanics, but as discovered by Weisskopf with help from Furry, in quantum mechanics the divergence is much milder, going only as the logarithm of the radius of the sphere.
The solution to the problem, presciently suggested by Stueckelberg, independently by Bethe after the crucial experiment by Lamb, implemented at one loop by Schwinger, and systematically extended to all loops by Feynman and Dyson, with converging work by Tomonaga in isolated postwar Japan, comes from recognizing that all the infinities in the interactions of photons and electrons can be isolated into redefining a finite number of quantities in the equations by replacing them with the observed values: specifically the electron's mass and charge: this is called renormalization. The technique of renormalization recognizes that the problem is essentially purely mathematical, that extremely short distances are at fault. In order to define a theory on a continuum, first place a cutoff on the fields, by postulating that quanta cannot have energies above some extremely high value. This has the effect of replacing continuous space by a structure where very short wavelengths do not exist, as on a lattice. Lattices break rotational symmetry, and one of the crucial contributions made by Feynman, Pauli and Villars, and modernized by 't Hooft and Veltman, is a symmetry-preserving cutoff for perturbation theory (this process is called regularization). There is no known symmetrical cutoff outside of perturbation theory, so for rigorous or numerical work people often use an actual lattice.
On a lattice, every quantity is finite but depends on the spacing. When taking the limit of zero spacing, we make sure that the physically observable quantities like the observed electron mass stay fixed, which means that the constants in the Lagrangian defining the theory depend on the spacing. Hopefully, by allowing the constants to vary with the lattice spacing, all the results at long distances become insensitive to the lattice, defining a continuum limit.
The renormalization procedure only works for a certain class of quantum field theories, called renormalizable quantum field theories. A theory is perturbatively renormalizable when the constants in the Lagrangian only diverge at worst as logarithms of the lattice spacing for very short spacings. The continuum limit is then well defined in perturbation theory, and even if it is not fully well defined non-perturbatively, the problems only show up at distance scales that are exponentially small in the inverse coupling for weak couplings. The Standard Model of particle physics is perturbatively renormalizable, and so are its component theories (quantum electrodynamics/electroweak theory and quantum chromodynamics). Of the three components, quantum electrodynamics is believed to not have a continuum limit, while the asymptotically free SU(2) and SU(3) weak hypercharge and strong color interactions are nonperturbatively well defined.
The renormalization group describes how renormalizable theories emerge as the long distance low-energy effective field theory for any given high-energy theory. Because of this, renormalizable theories are insensitive to the precise nature of the underlying high-energy short-distance phenomena. This is a blessing because it allows physicists to formulate low energy theories without knowing the details of high energy phenomenon. It is also a curse, because once a renormalizable theory like the standard model is found to work, it gives very few clues to higher energy processes. The only way high energy processes can be seen in the standard model is when they allow otherwise forbidden events, or if they predict quantitative relations between the coupling constants.
Haag's theorem.
From a mathematically rigorous perspective, there exists no interaction picture in a Lorentz-covariant quantum field theory. This implies that the perturbative approach of Feynman diagrams in QFT is not strictly justified, despite producing vastly precise predictions validated by experiment. This is called Haag's theorem, but most particle physicists relying on QFT largely shrug it off.
Gauge freedom.
A gauge theory is a theory that admits a symmetry with a local parameter. For example, in every quantum theory the global phase of the wave function is arbitrary and does not represent something physical. Consequently, the theory is invariant under a global change of phases (adding a constant to the phase of all wave functions, everywhere); this is a global symmetry. In quantum electrodynamics, the theory is also invariant under a "local" change of phase, that is – one may shift the phase of all wave functions so that the shift may be different at every point in space-time. This is a "local" symmetry. However, in order for a well-defined derivative operator to exist, one must introduce a new field, the gauge field, which also transforms in order for the local change of variables (the phase in our example) not to affect the derivative. In quantum electrodynamics this gauge field is the electromagnetic field. The change of local gauge of variables is termed gauge transformation. It is worth noting that by Noether's theorem, for every such symmetry there exists an associated conserved current. The aforementioned symmetry of the wavefunction under global phase changes implies the conservation of electric charge. 
In quantum field theory the excitations of fields represent particles. The particle associated with excitations of the gauge field is the gauge boson, which is the photon in the case of quantum electrodynamics.
The degrees of freedom in quantum field theory are local fluctuations of the fields. The existence of a gauge symmetry reduces the number of degrees of freedom, simply because some fluctuations of the fields can be transformed to zero by gauge transformations, so they are equivalent to having no fluctuations at all, and they therefore have no physical meaning. Such fluctuations are usually called "non-physical degrees of freedom" or "gauge artifacts"; usually some of them have a negative norm, making them inadequate for a consistent theory. Therefore, if a classical field theory has a gauge symmetry, then its quantized version (i.e. the corresponding quantum field theory) will have this symmetry as well. In other words, a gauge symmetry cannot have a quantum anomaly. If a gauge symmetry is anomalous (i.e. not kept in the quantum theory) then the theory is non-consistent: for example, in quantum electrodynamics, had there been a gauge anomaly, this would require the appearance of photons with longitudinal polarization and polarization in the time direction, the latter having a negative norm, rendering the theory inconsistent; another possibility would be for these photons to appear only in intermediate processes but not in the final products of any interaction, making the theory non-unitary and again inconsistent (see optical theorem).
In general, the gauge transformations of a theory consist of several different transformations, which may not be commutative. These transformations are together described by a mathematical object known as a gauge group. Infinitesimal gauge transformations are the gauge group generators. Therefore, the number of gauge bosons is the group dimension (i.e. number of generators forming a basis).
All the fundamental interactions in nature are described by gauge theories. These are:
Multivalued gauge transformations.
The gauge transformations which leave the theory invariant involve, by definition, only single-valued gauge functions formula_62 which satisfy the Schwarz integrability criterion
An interesting extension of gauge transformations arises if the gauge functions formula_62 are allowed to be multivalued functions which violate the integrability criterion. These are capable of changing the physical field strengths
and are therefore not proper symmetry transformations. Nevertheless, the transformed field equations describe correctly the physical laws in the presence of the newly generated field strengths. See the textbook by H. Kleinert cited below
for the applications to phenomena in physics.
Supersymmetry.
Supersymmetry assumes that every fundamental fermion has a superpartner that is a boson and vice versa. It was introduced in order to solve the so-called Hierarchy Problem, that is, to explain why particles not protected by any symmetry (like the Higgs boson) do not receive radiative corrections to its mass driving it to the larger scales (GUT, Planck...). It was soon realized that supersymmetry has other interesting properties: its gauged version is an extension of general relativity (Supergravity), and it is a key ingredient for the consistency of string theory.
The way supersymmetry protects the hierarchies is the following: since for every particle there is a superpartner with the same mass, any loop in a radiative correction is cancelled by the loop corresponding to its superpartner, rendering the theory UV finite.
Since no superpartners have yet been observed, if supersymmetry exists it must be broken (through a so-called soft term, which breaks supersymmetry without ruining its helpful features). The simplest models of this breaking require that the energy of the superpartners not be too high; in these cases, supersymmetry is expected to be observed by experiments at the Large Hadron Collider. The Higgs particle has been detected at the LHC, and no such superparticles have been discovered.
Further reading.
Articles:

</doc>
<doc id="403226" url="https://en.wikipedia.org/wiki?curid=403226" title="Quantum gauge theory">
Quantum gauge theory

In quantum physics, in order to quantize a gauge theory, like for example Yang-Mills theory, Chern-Simons or BF model, one method is to perform a gauge fixing. This is done in the BRST and Batalin-Vilkovisky formulation. Another is to factor out the symmetry by dispensing with vector potentials altogether (they're not physically observable anyway) and work directly with Wilson loops, Wilson lines contracted with other charged fields at its endpoints and spin networks.
Older approaches to quantization for Abelian models use the Gupta-Bleuler formalism with a "semi-Hilbert space" with an indefinite sesquilinear form. However, it is much more elegant to just work with the quotient space of vector field configurations by gauge transformations.
An alternative approach using lattice approximations is covered in (Wick rotated) lattice gauge theory.
To establish the existence of the Yang-Mills theory and a mass gap is one of the seven Millennium Prize Problems of the Clay Mathematics Institute.

</doc>
<doc id="25312" url="https://en.wikipedia.org/wiki?curid=25312" title="Quantum gravity">
Quantum gravity

Quantum gravity (QG) is a field of theoretical physics that seeks to describe the force of gravity according to the principles of quantum mechanics.
The current understanding of gravity is based on Albert Einstein's general theory of relativity, which is formulated within the framework of classical physics. On the other hand, the nongravitational forces are described within the framework of quantum mechanics, a radically different formalism for describing physical phenomena based on probability. The necessity of a quantum mechanical description of gravity follows from the fact that one cannot consistently couple a classical system to a quantum one.
Although a quantum theory of gravity is needed in order to reconcile general relativity with the principles of quantum mechanics, difficulties arise when one attempts to apply the usual prescriptions of quantum field theory to the force of gravity. From a technical point of view, the problem is that the theory one gets in this way is not renormalizable and therefore cannot be used to make meaningful physical predictions. As a result, theorists have taken up more radical approaches to the problem of quantum gravity, the most popular approaches being string theory and loop quantum gravity. A recent development is the theory of causal fermion systems which gives quantum mechanics, general relativity, and quantum field theory as limiting cases.
Strictly speaking, the aim of quantum gravity is only to describe the quantum behavior of the gravitational field and should not be confused with the objective of unifying all fundamental interactions into a single mathematical framework. While any substantial improvement into the present understanding of gravity would aid further work towards unification, study of quantum gravity is a field in its own right with various branches having different approaches to unification. Although some quantum gravity theories, such as string theory, try to unify gravity with the other fundamental forces, others, such as loop quantum gravity, make no such attempt; instead, they make an effort to quantize the gravitational field while it is kept separate from the other forces. A theory of quantum gravity that is also a grand unification of all known interactions is sometimes referred to as a theory of everything (TOE).
One of the difficulties of quantum gravity is that quantum gravitational effects are only expected to become apparent near the Planck scale, a scale far smaller in distance (equivalently, far larger in energy) than what is currently accessible at high energy particle accelerators. As a result, quantum gravity is a mainly theoretical enterprise, although there are speculations about how quantum gravity effects might be observed in existing experiments.
Overview.
Much of the difficulty in meshing these theories at all energy scales comes from the different assumptions that these theories make on how the universe works. Quantum field theory depends on particle fields embedded in the flat space-time of special relativity. General relativity models gravity as a curvature within space-time that changes as a gravitational mass moves. Historically, the most obvious way of combining the two (such as treating gravity as simply another particle field) ran quickly into what is known as the renormalization problem. In the old-fashioned understanding of renormalization, gravity particles would attract each other and adding together all of the interactions results in many infinite values which cannot easily be cancelled out mathematically to yield sensible, finite results. This is in contrast with quantum electrodynamics where, given that the series still do not converge, the interactions sometimes evaluate to infinite results, but those are few enough in number to be removable via renormalization.
Effective field theories.
Quantum gravity can be treated as an effective field theory. Effective quantum field theories come with some high-energy cutoff, beyond which we do not expect that the theory provides a good description of nature. The "infinities" then become large but finite quantities depending on this finite cutoff scale, and correspond to processes that involve very high energies near the fundamental cutoff. These quantities can then be absorbed into an infinite collection of coupling constants, and at energies well below the fundamental cutoff of the theory, to any desired precision; only a finite number of these coupling constants need to be measured in order to make legitimate quantum-mechanical predictions. This same logic works just as well for the highly successful theory of low-energy pions as for quantum gravity. Indeed, the first quantum-mechanical corrections to graviton-scattering and Newton's law of gravitation have been explicitly computed (although they are so infinitesimally small that we may never be able to measure them). In fact, gravity is in many ways a much better quantum field theory than the Standard Model, since it appears to be valid all the way up to its cutoff at the Planck scale.
While confirming that quantum mechanics and gravity are indeed consistent at reasonable energies, it is clear that near or above the fundamental cutoff of our effective quantum theory of gravity (the cutoff is generally assumed to be of the order of the Planck scale), a new model of nature will be needed. Specifically, the problem of combining quantum mechanics and gravity becomes an issue only at very high energies, and may well require a totally new kind of model.
Quantum gravity theory for the highest energy scales.
The general approach to deriving a quantum gravity theory that is valid at even the highest energy scales is to assume that such a theory will be simple and elegant and, accordingly, to study symmetries and other clues offered by current theories that might suggest ways to combine them into a comprehensive, unified theory. One problem with this approach is that it is unknown whether quantum gravity will actually conform to a simple and elegant theory, as it should resolve the dual conundrums of special relativity with regard to the uniformity of acceleration and gravity, and general relativity with regard to spacetime curvature.
Such a theory is required in order to understand problems involving the combination of very high energy and very small dimensions of space, such as the behavior of black holes, and the origin of the universe.
Quantum mechanics and general relativity.
The graviton.
At present, one of the deepest problems in theoretical physics is harmonizing the theory of general relativity, which describes gravitation, and applications to large-scale structures (stars, planets, galaxies), with quantum mechanics, which describes the other three fundamental forces acting on the atomic scale. This problem must be put in the proper context, however. In particular, contrary to the popular claim that quantum mechanics and general relativity are fundamentally incompatible, one can demonstrate that the structure of general relativity essentially follows inevitably from the quantum mechanics of interacting theoretical spin-2 massless particles 
(called gravitons).
While there is no concrete proof of the existence of gravitons, quantized theories of matter may necessitate their existence. Supporting this theory is the observation that all fundamental forces except gravity have one or more known messenger particles, leading researchers to believe that at least one most likely does exist; they have dubbed this hypothetical particle the "graviton". The predicted find would result in the classification of the graviton as a "force particle" similar to the photon of the electromagnetic field. Many of the accepted notions of a unified theory of physics since the 1970s assume, and to some degree depend upon, the existence of the graviton. These include string theory, superstring theory, M-theory, and loop quantum gravity. Detection of gravitons is thus vital to the validation of various lines of research to unify quantum mechanics and relativity theory. 
The dilaton.
The dilaton made its first appearance in Kaluza–Klein theory, a five-dimensional theory that combined gravitation and electromagnetism. Generally, it appears in string theory. More recently, however, it's become central to the lower-dimensional many-bodied gravity problem based on the field theoretic approach of Roman Jackiw. The impetus arose from the fact that complete analytical solutions for the metric of a covariant "N"-body system have proven elusive in general relativity. To simplify the problem, the number of dimensions was lowered to "(1+1)", i.e., one spatial dimension and one temporal dimension. This model problem, known as "R=T" theory (as opposed to the general "G=T" theory) was amenable to exact solutions in terms of a generalization of the Lambert W function. It was also found that the field equation governing the dilaton (derived from differential geometry) was the Schrödinger equation and consequently amenable to quantization.
Thus, one had a theory which combined gravity, quantization, and even the electromagnetic interaction, promising ingredients of a fundamental physical theory. It is worth noting that this outcome revealed a previously unknown and already existing "natural link" between general relativity and quantum mechanics. However, this theory lacks generalization to the "(2+1)" or "(3+1)" dimensions. In principle, the field equations are amenable to such generalization (as shown with the inclusion of a one-graviton process) and yield the correct Newtonian limit in "d" dimensions but only if a dilaton is included. Furthermore, it is not yet clear what the fully generalized field equation governing the dilaton in (3+1) dimensions should be. The fact that gravitons can propagate in "(3+1)" dimensions implies that gravitons and dilatons do exist in the real world. Nonetheless, detection of the dilaton is expected to be even more elusive than the graviton. But since this simplified approach combines gravitational, electromagnetic and quantum effects, their coupling could potentially lead to a means of vindicating the theory, through cosmology and even, perhaps, experimentally.
Nonrenormalizability of gravity.
General relativity, like electromagnetism, is a classical field theory. One might expect that, as with electromagnetism, the gravitational force should also have a corresponding quantum field theory.
However, gravity is perturbatively nonrenormalizable. For a quantum field theory to be well-defined according to this understanding of the subject, it must be asymptotically free or asymptotically safe. The theory must be characterized by a choice of "finitely many" parameters, which could, in principle, be set by experiment. For example, in quantum electrodynamics these parameters are the charge and mass of the electron, as measured at a particular energy scale.
On the other hand, in quantizing gravity there are "infinitely many independent parameters" (counterterm coefficients) needed to define the theory. For a given choice of those parameters, one could make sense of the theory, but since it's impossible to conduct infinite experiments to fix the values of every parameter, we do not have a meaningful physical theory:
If we treat QG as an effective field theory, there is a way around this problem.
That is, the meaningful theory of quantum gravity (that makes sense and is predictive at all energy levels) inherently implies some deep principle that reduces the infinitely many unknown parameters to a finite number that can then be measured:
QG as an effective field theory.
In an effective field theory, all but the first few of the infinite set of parameters in a non-renormalizable theory are suppressed by huge energy scales and hence can be neglected when computing low-energy effects. Thus, at least in the low-energy regime, the model is indeed a predictive quantum field theory. (A very similar situation occurs for the very similar effective field theory of low-energy pions.) Furthermore, many theorists agree that even the Standard Model should really be regarded as an effective field theory as well, with "nonrenormalizable" interactions suppressed by large energy scales and whose effects have consequently not been observed experimentally.
Recent work has shown that by treating general relativity as an effective field theory, one can actually make legitimate predictions for quantum gravity, at least for low-energy phenomena. An example is the well-known calculation of the tiny first-order quantum-mechanical correction to the classical Newtonian gravitational potential between two masses.
Spacetime background dependence.
A fundamental lesson of general relativity is that there is no fixed spacetime background, as found in Newtonian mechanics and special relativity; the spacetime geometry is dynamic. While easy to grasp in principle, this is the hardest idea to understand about general relativity, and its consequences are profound and not fully explored, even at the classical level. To a certain extent, general relativity can be seen to be a relational theory, in which the only physically relevant information is the relationship between different events in space-time.
On the other hand, quantum mechanics has depended since its inception on a fixed background (non-dynamic) structure. In the case of quantum mechanics, it is time that is given and not dynamic, just as in Newtonian classical mechanics. In relativistic quantum field theory, just as in classical field theory, Minkowski spacetime is the fixed background of the theory.
String theory.
String theory can be seen as a generalization of quantum field theory where instead of point particles, string-like objects propagate in a fixed spacetime background, although the interactions among closed strings give rise to space-time in a dynamical way.
Although string theory had its origins in the study of quark confinement and not of quantum gravity, it was soon discovered that the string spectrum contains the graviton, and that "condensation" of certain vibration modes of strings is equivalent to a modification of the original background. In this sense, string perturbation theory exhibits exactly the features one would expect of a perturbation theory that may exhibit a strong dependence on asymptotics (as seen, for example, in the AdS/CFT correspondence) which is a weak form of background dependence.
Background independent theories.
Loop quantum gravity is the fruit of an effort to formulate a background-independent quantum theory.
Topological quantum field theory provided an example of background-independent quantum theory, but with no local degrees of freedom, and only finitely many degrees of freedom globally. This is inadequate to describe gravity in 3+1 dimensions, which has local degrees of freedom according to general relativity. In 2+1 dimensions, however, gravity is a topological field theory, and it has been successfully quantized in several different ways, including spin networks.
Semi-classical quantum gravity.
Quantum field theory on curved (non-Minkowskian) backgrounds, while not a full quantum theory of gravity, has shown many promising early results. In an analogous way to the development of quantum electrodynamics in the early part of the 20th century (when physicists considered quantum mechanics in classical electromagnetic fields), the consideration of quantum field theory on a curved background has led to predictions such as black hole radiation.
Phenomena such as the Unruh effect, in which particles exist in certain accelerating frames but not in stationary ones, do not pose any difficulty when considered on a curved background (the Unruh effect occurs even in flat Minkowskian backgrounds). The vacuum state is the state with the least energy (and may or may not contain particles).
See Quantum field theory in curved spacetime for a more complete discussion.
Points of tension.
There are other points of tension between quantum mechanics and general relativity.
Candidate theories.
There are a number of proposed quantum gravity theories. Currently, there is still no complete and consistent quantum theory of gravity, and the candidate models still need to overcome major formal and conceptual problems. They also face the common problem that, as yet, there is no way to put quantum gravity predictions to experimental tests, although there is hope for this to change as future data from cosmological observations and particle physics experiments becomes available.
String theory.
One suggested starting point is ordinary quantum field theories which, after all, are successful in describing the other three basic fundamental forces in the context of the standard model of elementary particle physics. However, while this leads to an acceptable effective (quantum) field theory of gravity at low energies, gravity turns out to be much more problematic at higher energies. For ordinary field theories such as quantum electrodynamics, a technique known as renormalization is an integral part of deriving predictions which take into account higher-energy contributions, but gravity turns out to be nonrenormalizable: at high energies, applying the recipes of ordinary quantum field theory yields models that are devoid of all predictive power.
One attempt to overcome these limitations is to replace ordinary quantum field theory, which is based on the classical concept of a point particle, with a quantum theory of one-dimensional extended objects: string theory. At the energies reached in current experiments, these strings are indistinguishable from point-like particles, but, crucially, different modes of oscillation of one and the same type of fundamental string appear as particles with different (electric and other) charges. In this way, string theory promises to be a unified description of all particles and interactions. The theory is successful in that one mode will always correspond to a graviton, the messenger particle of gravity; however, the price of this success are unusual features such as six extra dimensions of space in addition to the usual three for space and one for time.
In what is called the , it was conjectured that both string theory and a unification of general relativity and supersymmetry known as supergravity form part of a hypothesized eleven-dimensional model known as M-theory, which would constitute a uniquely defined and consistent theory of quantum gravity. As presently understood, however, string theory admits a very large number (10500 by some estimates) of consistent vacua, comprising the so-called "string landscape". Sorting through this large family of solutions remains a major challenge.
Loop quantum gravity.
Loop quantum gravity seriously considers general relativity's insight that spacetime is a dynamical field and is therefore a quantum object. Its second idea is that the quantum discreteness that determines the particle-like behavior of other field theories (for instance, the photons of the electromagnetic field) also affects the structure of space.
The main result of loop quantum gravity is the derivation of a granular structure of space at the Planck length. This is derived from following considerations: In the case of electromagnetism, the quantum operator representing the energy of each frequency of the field has a discrete spectrum. Thus the energy of each frequency is quantized, and the quanta are the photons. In the case of gravity, the operators representing the area and the volume of each surface or space region likewise have discrete spectrum. Thus area and volume of any portion of space are also quantized, where the quanta are elementary quanta of space. It follows, then, that spacetime has an elementary quantum granular structure at the Planck scale, which cuts off the ultraviolet infinities of quantum field theory.
The quantum state of spacetime is described in the theory by means of a mathematical structure called spin networks. Spin networks were initially introduced by Roger Penrose in abstract form, and later shown by Carlo Rovelli and Lee Smolin to derive naturally from a non-perturbative quantization of general relativity. Spin networks do not represent quantum states of a field in spacetime: they represent directly quantum states of spacetime.
The theory is based on the reformulation of general relativity known as Ashtekar variables, which represent geometric gravity using mathematical analogues of electric and magnetic fields. 
In the quantum theory, space is represented by a network structure called a spin network, evolving over time in discrete steps.
The dynamics of the theory is today constructed in several versions. One version starts with the canonical quantization of general relativity. The analogue of the Schrödinger equation is a Wheeler–DeWitt equation, which can be defined within the theory. 
In the covariant, or spinfoam formulation of the theory, the quantum dynamics is obtained via a sum over discrete versions of spacetime, called spinfoams. These represent histories of spin networks.
Other approaches.
There are a number of other approaches to quantum gravity. The approaches differ depending on which features of general relativity and quantum theory are accepted unchanged, and which features are modified. Examples include:
Weinberg–Witten theorem.
In quantum field theory, the Weinberg–Witten theorem places some constraints on theories of composite gravity/emergent gravity. However, recent developments attempt to show that if locality is only approximate and the holographic principle is correct, the Weinberg–Witten theorem would not be valid.
Experimental tests.
As was emphasized above, quantum gravitational effects are extremely weak and therefore difficult to test. For this reason, the possibility of experimentally testing quantum gravity had not received much attention prior to the late 1990s. However, in the past decade, physicists have realized that evidence for quantum gravitational effects can guide the development of the theory. Since theoretical development has been slow, the field of phenomenological quantum gravity, which studies the possibility of experimental tests, has obtained increased attention.
The most widely pursued possibilities for quantum gravity phenomenology include violations of Lorentz invariance, imprints of quantum gravitational effects in the cosmic microwave background (in particular its polarization), and decoherence induced by fluctuations in the space-time foam.
The BICEP2 experiment detected what was initially thought to be primordial B-mode polarization caused by gravitational waves in the early universe. If truly primordial, these waves were born as quantum fluctuations in gravity itself. Cosmologist Ken Olum (Tufts University) stated: "I think this is the only observational evidence that we have that actually shows that gravity is quantized...It's probably the only evidence of this that we will ever have."

</doc>
<doc id="50719" url="https://en.wikipedia.org/wiki?curid=50719" title="Quantum harmonic oscillator">
Quantum harmonic oscillator

The quantum harmonic oscillator is the quantum-mechanical analog of the classical harmonic oscillator. Because an arbitrary potential can usually be approximated as a harmonic potential at the vicinity of a stable equilibrium point, it is one of the most important model systems in quantum mechanics. Furthermore, it is one of the few quantum-mechanical systems for which an exact, analytical solution is known.
One-dimensional harmonic oscillator.
Hamiltonian and energy eigenstates.
The Hamiltonian of the particle is:
The functions "Hn" are the physicists' Hermite polynomials,
The corresponding energy levels are
This energy spectrum is noteworthy for three reasons. First, the energies are quantized, meaning that only discrete energy values (integer-plus-half multiples of ) are possible; this is a general feature of quantum-mechanical systems when a particle is confined. Second, these discrete energy levels are equally spaced, unlike in the Bohr model of the atom, or the particle in a box. Third, the lowest achievable energy (the energy of the state, called the ground state) is not equal to the minimum of the potential well, but above it; this is called zero-point energy. Because of the zero-point energy, the position and momentum of the oscillator in the ground state are not fixed (as they would be in a classical oscillator), but have a small range of variance, in accordance with the Heisenberg uncertainty principle. This zero-point energy further has important implications in quantum field theory and quantum gravity.
Note that the ground state probability density is concentrated at the origin. This means the particle spends most of its time at the bottom of the potential well, as we would expect for a state with little energy. As the energy increases, the probability density becomes concentrated at the classical "turning points", where the state's energy coincides with the potential energy. This is consistent with the classical harmonic oscillator, in which the particle spends most of its time (and is therefore most likely to be found) at the turning points, where it is the slowest. The correspondence principle is thus satisfied. Moreover, special nondispersive wave packets, with minimum uncertainty, called coherent states in fact oscillate very much like classical objects, as illustrated in the figure; they are "not" eigenstates of the Hamiltonian.
Ladder operator method.
The spectral method solution, though straightforward, is rather tedious. The "ladder operator" method, developed by Paul Dirac, allows us to extract the energy eigenvalues without directly solving the differential equation. Furthermore, it is readily generalizable to more complicated problems, notably in quantum field theory. Following this approach, we define the operators and its adjoint ,
This leads to the useful representation of and , 
The operator is not Hermitian, since itself and its adjoint are not equal. Yet the energy eigenstates |"n">, when operated on by these ladder operators, give
It is then evident that , in essence, appends a single quantum of energy to the oscillator, while removes a quantum. For this reason, they are sometimes referred to as "creation" and "annihilation" operators.
From the relations above, we can also define a number operator , which has the following property:
The following commutators can be easily obtained by substituting the canonical commutation relation,
And the Hamilton operator can be expressed as
so the eigenstate of is also the eigenstate of energy.
The commutation property yields
and similarly,
This means that acts on to produce, up to a multiplicative constant, , and acts on to produce . For this reason, is called a "lowering operator", and a "raising operator". The two operators together are called ladder operators. In quantum field theory, and are alternatively called "annihilation" and "creation" operators because they destroy and create particles, which correspond to our quanta of energy.
Given any energy eigenstate, we can act on it with the lowering operator, , to produce another eigenstate with less energy. By repeated application of the lowering operator, it seems that we can produce energy eigenstates down to . However, since
the smallest eigen-number is 0, and
In this case, subsequent applications of the lowering operator will just produce zero kets, instead of additional energy eigenstates. Furthermore, we have shown above that
Finally, by acting on |0⟩ with the raising operator and multiplying by suitable normalization factors, we can produce an infinite set of energy eigenstates 
such that
which matches the energy spectrum given in the preceding section.
Arbitrary eigenstates can be expressed in terms of |0⟩, 
The ground state |0⟩ in the position representation is determined by "a" |0⟩ = 0, 
and hence
and so on, as in the previous section.
Natural length and energy scales.
The quantum harmonic oscillator possesses natural scales for length and energy, which can be used to simplify the problem. These can be found by nondimensionalization.
The result is that, if we measure "energy" in units of and "distance" in units of , then the Hamiltonian simplifies to
while the energy eigenfunctions and eigenvalues simplify to
where are the Hermite polynomials.
To avoid confusion, we will not adopt these "natural units" in this article. However, they frequently come in handy when performing calculations, by bypassing clutter.
For example, the fundamental solution () of , the time-dependent Schrödinger operator for this oscillator, simply boils down to the Mehler kernel,
where . The most general solution for a given initial configuration then is simply
Phase space solutions.
In the phase space formulation of quantum mechanics, solutions to the quantum harmonic oscillator in several different representations of the quasiprobability distribution can be written in closed form. The most widely used of these is for the Wigner quasiprobability distribution, which has the solution
where
and "Ln" are the Laguerre polynomials.
This example illustrates how the Hermite and Laguerre polynomials are linked through the Wigner map.
"N"-dimensional harmonic oscillator.
The one-dimensional harmonic oscillator is readily generalizable to "N" dimensions, where "N" = 1, 2, 3, ... . In one dimension, the position of the particle was specified by a single coordinate, "x". In "N" dimensions, this is replaced by "N" position coordinates, which we label "x"1, ..., "x""N". Corresponding to each position coordinate is a momentum; we label these "p"1, ..., "p""N". The canonical commutation relations between these operators are
The Hamiltonian for this system is
As the form of this Hamiltonian makes clear, the "N"-dimensional harmonic oscillator is exactly analogous to "N" independent one-dimensional harmonic oscillators with the same mass and spring constant. In this case, the quantities "x"1, ..., "x""N" would refer to the positions of each of the "N" particles. This is a convenient property of the formula_33 potential, which allows the potential energy to be separated into terms depending on one coordinate each.
This observation makes the solution straightforward. For a particular set of quantum numbers {"n"} the energy eigenfunctions for the "N"-dimensional oscillator are expressed in terms of the 1-dimensional eigenfunctions as:
In the ladder operator method, we define "N" sets of ladder operators,
By a procedure analogous to the one-dimensional case, we can then show that each of the "a""i" and "a"†"i" operators lower and raise the energy by ℏω respectively. The Hamiltonian is
This Hamiltonian is invariant under the dynamic symmetry group "U"("N") (the unitary group in "N" dimensions), defined by
where formula_38 is an element in the defining matrix representation of "U"("N").
The energy levels of the system are
As in the one-dimensional case, the energy is quantized. The ground state energy is "N" times the one-dimensional energy, as we would expect using the analogy to "N" independent one-dimensional oscillators. There is one further difference: in the one-dimensional case, each energy level corresponds to a unique quantum state. In "N"-dimensions, except for the ground state, the energy levels are "degenerate", meaning there are several states with the same energy.
The degeneracy can be calculated relatively easily. As an example, consider the 3-dimensional case: Define "n" = "n"1 + "n"2 + "n"3. All states with the same "n" will have the same energy. For a given "n", we choose a particular "n"1. Then "n"2 + "n"3 = "n" − "n"1. There are "n" − "n"1 + 1 possible pairs {"n"2, "n"3}. "n"2 can take on the values 0 to "n" − "n"1, and for each "n"2 the value of "n"3 is fixed. The degree of degeneracy therefore is:
Formula for general "N" and "n" ["g""n" being the dimension of the symmetric irreducible "n"th power representation of the unitary group "U"("N")]:
The special case "N" = 3, given above, follows directly from this general equation. This is however, only true for distinguishable particles, or one particle in N dimensions (as dimensions are distinguishable). For the case of "N" bosons in a one-dimension harmonic trap, the degeneracy scales as the number of ways to partition an integer "n" using integers less than or equal to "N".
This arises due to the constraint of putting "N" quanta into a state ket where formula_44 and formula_45, which are the same constraints as in integer partition.
Example: 3D isotropic harmonic oscillator.
The Schrödinger equation of a spherically-symmetric three-dimensional harmonic oscillator can be solved explicitly by separation of variables; see this article for the present case. This procedure is analogous to the separation performed in the hydrogen-like atom problem, but with the spherically symmetric potential
where is the mass of the problem. (Because will be used below for the magnetic quantum number, mass is indicated by , instead of , as earlier in this article.)
The solution reads
where
are generalized Laguerre polynomials; The order of the polynomial is a non-negative integer;
The energy eigenvalue is
The energy is usually described by the single quantum number
Because is a non-negative integer, for every even we have and for every odd we have . The magnetic quantum number is an integer satisfying , so for every and ℓ there are 2"ℓ" + 1 different quantum states, labeled by . Thus, the degeneracy at level is
where the sum starts from 0 or 1, according to whether is even or odd.
This result is in accordance with the dimension formula above, and amounts to the dimensionality of a symmetric representation of , the relevant degeneracy group.
Harmonic oscillators lattice: phonons.
We can extend the notion of a harmonic oscillator to a one lattice of many particles. Consider a one-dimensional quantum mechanical "harmonic chain" of "N" identical atoms. This is the simplest quantum mechanical model of a lattice, and we will see how phonons arise from it. The formalism that we will develop for this model is readily generalizable to two and three dimensions.
As in the previous section, we denote the positions of the masses by , as measured from their equilibrium positions (i.e. = 0 if the particle is at its equilibrium position.) In two or more dimensions, the are vector quantities. The Hamiltonian for this system is
where is the (assumed uniform) mass of each atom, and and are the position and momentum operators for the "i" th atom and the sum is made over the nearest neighbors (nn). However, it is customary to rewrite the Hamiltonian in terms of the normal modes of the wavevector rather than in terms of the particle coordinates so that one can work in the more convenient Fourier space.
We introduce, then, a set of "normal coordinates" , defined as the discrete Fourier transforms of the s, and "conjugate momenta" defined as the Fourier transforms of the s,
The quantity will turn out to be the wave number of the phonon, i.e. 2"π" divided by the wavelength. It takes on quantized values, because the number of atoms is finite.
This preserves the desired commutation relations in either real space or wave vector space
From the general result
it is easy to show, through elementary trigonometry, that the potential energy term is
where
The Hamiltonian may be written in wave vector space as
Note that the couplings between the position variables have been transformed away; if the s and s were hermitian(which they are not), the transformed Hamiltonian would describe "uncoupled" harmonic oscillators.
The form of the quantization depends on the choice of boundary conditions; for simplicity, we impose "periodic" boundary conditions, defining the th atom as equivalent to the first atom. Physically, this corresponds to joining the chain at its ends. The resulting quantization is
The upper bound to comes from the minimum wavelength, which is twice the lattice spacing , as discussed above.
The harmonic oscillator eigenvalues or energy levels for the mode are 
If we ignore the zero-point energy then the levels are evenly spaced at 
So an exact amount of energy , must be supplied to the harmonic oscillator lattice to push it to the next energy level. In comparison to the photon case when the electromagnetic field is quantised, the quantum of vibrational energy is called a phonon.
All quantum systems show wave-like and particle-like properties. The particle-like properties of the phonon are best understood using the methods of second quantization and operator techniques described later.

</doc>
<doc id="25202" url="https://en.wikipedia.org/wiki?curid=25202" title="Quantum mechanics">
Quantum mechanics

Quantum mechanics (QM; also known as quantum physics or quantum theory) including quantum field theory, is a fundamental branch of physics concerned with processes involving, for example, atoms and photons. In such processes, said to be quantized, the action has been observed to be only in integer multiples of the Planck constant, a physical quantity that is exceedingly, indeed perhaps ultimately, small. This is utterly inexplicable in classical physics.
Quantum mechanics gradually arose from Max Planck's solution in 1900 to the black-body radiation problem (reported 1859) and Albert Einstein's 1905 paper which offered a quantum-based theory to explain the photoelectric effect (reported 1887). Early quantum theory was significantly reformulated in the mid-1920s.
The mathematical formulations of quantum mechanics are abstract. A mathematical function, the wave function, provides information about the probability amplitude of position, momentum, and other physical properties of a particle.
Important applications of quantum mechanical theory include superconducting magnets, light-emitting diodes and the laser, the transistor and semiconductors such as the microprocessor, medical and research imaging such as magnetic resonance imaging and electron microscopy, and explanations for many biological and physical phenomena.
History.
Scientific inquiry into the wave nature of light began in the 17th and 18th centuries, when scientists such as Robert Hooke, Christiaan Huygens and Leonhard Euler proposed a wave theory of light based on experimental observations. In 1803, Thomas Young, an English polymath, performed the famous double-slit experiment that he later described in a paper entitled "On the nature of light and colours". This experiment played a major role in the general acceptance of the wave theory of light.
In 1838, Michael Faraday discovered cathode rays. These studies were followed by the 1859 statement of the black-body radiation problem by Gustav Kirchhoff, the 1877 suggestion by Ludwig Boltzmann that the energy states of a physical system can be discrete, and the 1900 quantum hypothesis of Max Planck. Planck's hypothesis that energy is radiated and absorbed in discrete "quanta" (or energy elements) precisely matched the observed patterns of black-body radiation.
In 1896, Wilhelm Wien empirically determined a distribution law of black-body radiation, known as Wien's law in his honor. Ludwig Boltzmann independently arrived at this result by considerations of Maxwell's equations. However, it was valid only at high frequencies and underestimated the radiance at low frequencies. Later, Planck corrected this model using Boltzmann's statistical interpretation of thermodynamics and proposed what is now called Planck's law, which led to the development of quantum mechanics.
Following Max Planck's solution in 1900 to the black-body radiation problem (reported 1859), Albert Einstein offered a quantum-based theory to explain the photoelectric effect (1905, reported 1887). Around 1900-1910, the atomic theory and the corpuscular theory of light first came to be widely accepted as scientific fact; these latter theories can be viewed as quantum theories of matter and electromagnetic radiation, respectively.
Among the first to study quantum phenomena in nature were Arthur Compton, C. V. Raman, and Pieter Zeeman, each of whom has a quantum effect named after him. Robert Andrews Millikan studied the photoelectric effect experimentally, and Albert Einstein developed a theory for it. At the same time, Niels Bohr developed his theory of the atomic structure, which was later confirmed by the experiments of Henry Moseley. In 1913, Peter Debye extended Niels Bohr's theory of atomic structure, introducing elliptical orbits, a concept also introduced by Arnold Sommerfeld. This phase is known as old quantum theory.
According to Planck, each energy element ("E")" "is proportional to its frequency ("ν"):
where "h" is Planck's constant.
Planck cautiously insisted that this was simply an aspect of the "processes" of absorption and emission of radiation and had nothing to do with the "physical reality" of the radiation itself. In fact, he considered his quantum hypothesis a mathematical trick to get the right answer rather than a sizable discovery. However, in 1905 Albert Einstein interpreted Planck's quantum hypothesis realistically and used it to explain the photoelectric effect, in which shining light on certain materials can eject electrons from the material. He won the 1921 Nobel Prize in Physics for this work.
Einstein further developed this idea to show that an electromagnetic wave such as light could also be described as a particle (later called the photon), with a discrete quantum of energy that was dependent on its frequency.
The foundations of quantum mechanics were established during the first half of the 20th century by Max Planck, Niels Bohr, Werner Heisenberg, Louis de Broglie, Arthur Compton, Albert Einstein, Erwin Schrödinger, Max Born, John von Neumann, Paul Dirac, Enrico Fermi, Wolfgang Pauli, Max von Laue, Freeman Dyson, David Hilbert, Wilhelm Wien, Satyendra Nath Bose, Arnold Sommerfeld, and . The Copenhagen interpretation of Niels Bohr became widely accepted.
In the mid-1920s, developments in quantum mechanics led to its becoming the standard formulation for atomic physics. In the summer of 1925, Bohr and Heisenberg published results that closed the old quantum theory. Out of deference to their particle-like behavior in certain processes and measurements, light quanta came to be called photons (1926). From Einstein's simple postulation was born a flurry of debating, theorizing, and testing. Thus, the entire field of quantum physics emerged, leading to its wider acceptance at the Fifth Solvay Conference in 1927.
It was found that subatomic particles and electromagnetic waves are neither simply particle nor wave but have certain properties of each. This originated the concept of wave–particle duality.
By 1930, quantum mechanics had been further unified and formalized by the work of David Hilbert, Paul Dirac and John von Neumann with greater emphasis on measurement, the statistical nature of our knowledge of reality, and philosophical speculation about the 'observer'. It has since permeated many disciplines including quantum chemistry, quantum electronics, quantum optics, and quantum information science. Its speculative modern developments include string theory and quantum gravity theories. It also provides a useful framework for many features of the modern periodic table of elements, and describes the behaviors of atoms during chemical bonding and the flow of electrons in computer semiconductors, and therefore plays a crucial role in many modern technologies.
While quantum mechanics was constructed to describe the world of the very small, it is also needed to explain some macroscopic phenomena such as superconductors, and [[superfluid]]s.
The word "quantum" derives from the [[Latin language|Latin]], meaning "how great" or "how much". In quantum mechanics, it refers to a discrete unit assigned to certain [[Physical quantity|physical quantities]] such as the [[energy]] of an [[atom]] at rest (see Figure 1). The discovery that particles are discrete packets of energy with wave-like properties led to the branch of physics dealing with atomic and subatomic systems which is today called quantum mechanics. It underlies the [[mathematical]] framework of many fields of [[physics]] and [[chemistry]], including [[condensed matter physics]], [[solid-state physics]], [[atomic physics]], [[molecular physics]], [[computational physics]], [[computational chemistry]], [[quantum chemistry]], [[particle physics]], [[nuclear chemistry]], and [[nuclear physics]]. Some fundamental aspects of the theory are still actively studied.
Quantum mechanics is essential to understanding the behavior of systems at [[atom]]ic length scales and smaller. If the physical nature of an atom was solely described by [[classical mechanics]], electrons would not "orbit" the nucleus, since orbiting electrons emit radiation (due to [[circular motion]]) and would eventually collide with the nucleus due to this loss of energy. This framework was unable to explain the stability of atoms. Instead, electrons remain in an uncertain, non-deterministic, "smeared", [[probability|probabilistic]] wave–particle [[Atomic orbital|orbital]] about the nucleus, defying the traditional assumptions of classical mechanics and [[electromagnetism]].
Quantum mechanics was initially developed to provide a better explanation and description of the atom, especially the differences in the [[spectrum|spectra]] of light emitted by different [[isotope]]s of the same [[chemical element]], as well as subatomic particles. In short, the quantum-mechanical atomic model has succeeded spectacularly in the realm where classical mechanics and electromagnetism falter.
Broadly speaking, quantum mechanics incorporates four classes of phenomena for which classical physics cannot account:
Mathematical formulations.
In the mathematically rigorous formulation of quantum mechanics developed by [[Paul Dirac]], [[David Hilbert]], [[John von Neumann]], and [[Hermann Weyl]], the possible states of a quantum mechanical system are represented by [[unit vector]]s (called "state vectors"). Formally, these reside in a [[complex number|complex]] [[Separable space|separable]] [[Hilbert space]]—variously called the [[State space (physics)|"state space"]] or the "associated Hilbert space" of the system—that is well defined up to a complex number of norm 1 (the phase factor). In other words, the possible states are points in the [[projective space]] of a Hilbert space, usually called the [[complex projective space]]. The exact nature of this Hilbert space is dependent on the system—for example, the state space for position and momentum states is the space of [[square-integrable]] functions, while the state space for the spin of a single proton is just the product of two complex planes. Each observable is represented by a maximally [[Hermitian adjoint|Hermitian]] (precisely: by a [[self-adjoint operator|self-adjoint]]) linear [[Operator (physics)|operator]] acting on the state space. Each [[eigenstate]] of an observable corresponds to an [[eigenvector]] of the operator, and the associated [[eigenvalue]] corresponds to the value of the observable in that eigenstate. If the operator's spectrum is discrete, the observable can attain only those discrete eigenvalues.
In the formalism of quantum mechanics, the state of a system at a given time is described by a [[complex number|complex]] [[wave function]], also referred to as state vector in a complex [[vector space]]. This abstract mathematical object allows for the calculation of [[probability|probabilities]] of outcomes of concrete experiments. For example, it allows one to compute the probability of finding an electron in a particular region around the nucleus at a particular time. Contrary to classical mechanics, one can never make simultaneous predictions of [[conjugate variables]], such as position and momentum, with accuracy. For instance, electrons may be considered (to a certain probability) to be located somewhere within a given region of space, but with their exact positions unknown. Contours of constant probability, often referred to as "clouds", may be drawn around the nucleus of an atom to conceptualize where the electron might be located with the most probability. Heisenberg's [[uncertainty principle]] quantifies the inability to precisely locate the particle given its conjugate momentum.
According to one interpretation, as the result of a measurement the wave function containing the probability information for a system collapses from a given initial state to a particular eigenstate. The possible results of a measurement are the eigenvalues of the operator representing the observable—which explains the choice of "Hermitian" operators, for which all the eigenvalues are real. The probability distribution of an observable in a given state can be found by computing the [[spectral theorem|spectral decomposition]] of the corresponding operator. Heisenberg's [[uncertainty principle]] is represented by the statement that the operators corresponding to certain observables do not [[Commutator|commute]].
The [[probability|probabilistic]] nature of quantum mechanics thus stems from the act of measurement. This is one of the most difficult aspects of quantum systems to understand. It was the central topic in the famous [[Bohr–Einstein debates]], in which the two scientists attempted to clarify these fundamental principles by way of [[thought experiment]]s. In the decades after the formulation of quantum mechanics, the question of what constitutes a "measurement" has been extensively studied. Newer [[Interpretation of quantum mechanics|interpretations of quantum mechanics]] have been formulated that do away with the concept of "wave function collapse" (see, for example, the [[relative state interpretation]]). The basic idea is that when a quantum system interacts with a measuring apparatus, their respective wave functions become [[Quantum Entanglement|entangled]], so that the original quantum system ceases to exist as an independent entity. For details, see the article on [[measurement in quantum mechanics]].
Generally, quantum mechanics does not assign definite values. Instead, it makes a prediction using a [[probability distribution]]; that is, it describes the probability of obtaining the possible outcomes from measuring an observable. Often these results are skewed by many causes, such as dense probability clouds. Probability clouds are approximate (but better than [[Bohr model|the Bohr model]]) whereby electron location is given by a [[probability function]], the [[wave function]] [[eigenvalue]], such that the probability is the squared modulus of the [[probability amplitude|complex amplitude]], or quantum state nuclear attraction. Naturally, these probabilities will depend on the quantum state at the "instant" of the measurement. Hence, uncertainty is involved in the value. There are, however, certain states that are associated with a definite value of a particular observable. These are known as [[eigenstate]]s of the observable ("eigen" can be translated from [[German language|German]] as meaning "inherent" or "characteristic").
In the everyday world, it is natural and intuitive to think of everything (every observable) as being in an eigenstate. Everything appears to have a definite position, a definite momentum, a definite energy, and a definite time of occurrence. However, quantum mechanics does not pinpoint the exact values of a particle's position and momentum (since they are [[Conjugate variables|conjugate pairs]]) or its energy and time (since they too are conjugate pairs); rather, it provides only a range of probabilities in which that particle might be given its momentum and momentum probability. Therefore, it is helpful to use different words to describe states having "[[Uncertainty principle|uncertain]]" values and states having "definite" values (eigenstates). Usually, a system will not be in an [[eigenstate]] of the observable (particle) we are interested in. However, if one measures the observable, the wave function will instantaneously be an eigenstate (or "generalized" eigenstate) of that observable. This process is known as [[wave function collapse]], a controversial and much-debated process that involves expanding the system under study to include the measurement device. If one knows the corresponding wave function at the instant before the measurement, one will be able to compute the probability of the wave function collapsing into each of the possible eigenstates. For example, the free particle in the previous example will usually have a wave function that is a [[wave packet]] centered around some mean position "x"0 (neither an eigenstate of position nor of momentum). When one measures the position of the particle, it is impossible to predict with certainty the result. It is probable, but not certain, that it will be near "x"0, where the amplitude of the wave function is large. After the measurement is performed, having obtained some result "x", the wave function collapses into a position eigenstate centered at "x".
The time evolution of a quantum state is described by the [[Schrödinger equation]], in which the [[Hamiltonian (quantum mechanics)|Hamiltonian]] (the [[Operator (physics)|operator]] corresponding to the [[total energy]] of the system) generates the time evolution. The [[time evolution]] of wave functions is [[determinism|deterministic]] in the sense that - given a wave function at an "initial" time - it makes a definite prediction of what the wave function will be at any "later" time.
During a [[quantum measurement|measurement]], on the other hand, the change of the initial wave function into another, later wave function is not deterministic, it is unpredictable (i.e., [[random]]). A time-evolution simulation can be seen here.
Wave functions change as time progresses. The [[Schrödinger equation]] describes how wave functions change in time, playing a role similar to [[Newton's second law]] in [[classical mechanics]]. The Schrödinger equation, applied to the aforementioned example of the free particle, predicts that the center of a wave packet will move through space at a constant velocity (like a classical particle with no forces acting on it). However, the wave packet will also spread out as time progresses, which means that the position becomes more uncertain with time. This also has the effect of turning a position eigenstate (which can be thought of as an infinitely sharp wave packet) into a broadened wave packet that no longer represents a (definite, certain) position eigenstate.
[[Image:HAtomOrbitals.png|thumb|275px|Fig. 1: [[Probability density function|Probability densities]] corresponding to the wave functions of an electron in a hydrogen atom possessing definite energy levels (increasing from the top of the image to the bottom: "n" = 1, 2, 3, ...) and angular momenta (increasing across from left to right: "s", "p", "d", ...). Brighter areas correspond to higher probability density in a position measurement. Such wave functions are directly comparable to [[Chladni's figures]] of [[acoustics|acoustic]] modes of vibration in [[classical physics]], and are modes of oscillation as well, possessing a sharp [[energy]] and, thus, a definite [[frequency]]. The [[angular momentum]] and energy are [[quantization (physics)|quantized]], and take only discrete values like those shown (as is the case for [[Resonant frequency|resonant frequencies]] in acoustics)]]
Some wave functions produce probability distributions that are constant, or independent of time—such as when in a [[Eigenstate#Schrödinger equation|stationary state]] of constant energy, time vanishes in the absolute square of the wave function. Many systems that are treated dynamically in classical mechanics are described by such "static" wave functions. For example, a single [[electron]] in an unexcited [[atom]] is pictured classically as a particle moving in a circular trajectory around the [[atomic nucleus]], whereas in quantum mechanics it is described by a static, [[spherical coordinate system|spherically symmetric]] wave function surrounding the nucleus ([[:Image:HAtomOrbitals.png|Fig. 1]]) (note, however, that only the lowest angular momentum states, labeled "s", are spherically symmetric).
The Schrödinger equation acts on the "entire" probability amplitude, not merely its absolute value. Whereas the absolute value of the probability amplitude encodes information about probabilities, its [[phase (waves)|phase]] encodes information about the [[Interference (wave propagation)|interference]] between quantum states. This gives rise to the "wave-like" behavior of quantum states. As it turns out, analytic solutions of the Schrödinger equation are available for only [[List of quantum-mechanical systems with analytical solutions|a very small number of relatively simple model Hamiltonians]], of which the [[quantum harmonic oscillator]], the [[particle in a box]], the [[dihydrogen cation]], and the [[hydrogen atom]] are the most important representatives. Even the [[helium]] atom—which contains just one more electron than does the hydrogen atom—has defied all attempts at a fully analytic treatment.
There exist several techniques for generating approximate solutions, however. In the important method known as [[perturbation theory (quantum mechanics)|perturbation theory]], one uses the analytic result for a simple quantum mechanical model to generate a result for a more complicated model that is related to the simpler model by (for one example) the addition of a weak [[potential energy]]. Another method is the "semi-classical equation of motion" approach, which applies to systems for which quantum mechanics produces only weak (small) deviations from classical behavior. These deviations can then be computed based on the classical motion. This approach is particularly important in the field of [[quantum chaos]].
Mathematically equivalent formulations of quantum mechanics.
There are numerous mathematically equivalent formulations of quantum mechanics. One of the oldest and most commonly used formulations is the "[[transformation theory (quantum mechanics)|transformation theory]]" proposed by [[Paul Dirac]], which unifies and generalizes the two earliest formulations of quantum mechanics - [[matrix mechanics]] (invented by [[Werner Heisenberg]]) and [[Schrödinger equation|wave mechanics]] (invented by [[Erwin Schrödinger]]).
Especially since [[Werner Heisenberg]] was awarded the [[Nobel Prize in Physics]] in 1932 for the creation of quantum mechanics, the role of [[Max Born]] in the development of QM was overlooked until the 1954 Nobel award. The role is noted in a 2005 biography of Born, which recounts his role in the matrix formulation of quantum mechanics, and the use of probability amplitudes. Heisenberg himself acknowledges having learned matrices from Born, as published in a 1940 "[[festschrift]]" honoring [[Max Planck]]. In the matrix formulation, the [[quantum state|instantaneous state of a quantum system]] encodes the probabilities of its measurable properties, or "[[observable]]s". Examples of observables include [[energy]], [[position operator|position]], [[momentum operator|momentum]], and [[angular momentum]]. Observables can be either [[Continuous function|continuous]] (e.g., the position of a particle) or [[Discrete mathematics|discrete]] (e.g., the energy of an electron bound to a hydrogen atom). An alternative formulation of quantum mechanics is [[Feynman]]'s [[path integral formulation]], in which a quantum-mechanical amplitude is considered as a sum over all possible classical and non-classical paths between the initial and final states. This is the quantum-mechanical counterpart of the [[action principle]] in classical mechanics.
Interactions with other scientific theories.
The rules of quantum mechanics are fundamental. They assert that the state space of a system is a [[Hilbert space]] and that observables of that system are [[Hermitian operators]] acting on that space—although they do not tell us which Hilbert space or which operators. These can be chosen appropriately in order to obtain a quantitative description of a quantum system. An important guide for making these choices is the [[correspondence principle]], which states that the predictions of quantum mechanics reduce to those of classical mechanics when a system moves to higher energies or, equivalently, larger quantum numbers, i.e. whereas a single particle exhibits a degree of randomness, in systems incorporating millions of particles averaging takes over and, at the high energy limit, the statistical probability of random behaviour approaches zero. In other words, classical mechanics is simply a quantum mechanics of large systems. This "high energy" limit is known as the "classical" or "correspondence limit". One can even start from an established classical model of a particular system, then attempt to guess the underlying quantum model that would give rise to the classical model in the correspondence limit.
When quantum mechanics was originally formulated, it was applied to models whose
correspondence limit was [[theory of relativity|non-relativistic]] [[classical mechanics]]. For instance, the well-known model of the [[quantum harmonic oscillator]] uses an explicitly non-relativistic expression for the [[kinetic energy]] of the oscillator, and is thus a quantum version of the [[harmonic oscillator|classical harmonic oscillator]].
Early attempts to merge quantum mechanics with [[special relativity]] involved the replacement of the Schrödinger equation with a covariant equation such as the [[Klein–Gordon equation]] or the [[Dirac equation]]. While these theories were successful in explaining many experimental results, they had certain unsatisfactory qualities stemming from their neglect of the relativistic creation and annihilation of particles. A fully relativistic quantum theory required the development of [[quantum field theory]], which applies quantization to a field (rather than a fixed set of particles). The first complete quantum field theory, [[quantum electrodynamics]], provides a fully quantum description of the [[electromagnetism|electromagnetic interaction]]. The full apparatus of quantum field theory is often unnecessary for describing electrodynamic systems. A simpler approach, one that has been employed since the inception of quantum mechanics, is to treat [[electric charge|charged]] particles as quantum mechanical objects being acted on by a classical [[electromagnetic field]]. For example, the elementary quantum model of the [[hydrogen atom]] describes the [[electric field]] of the hydrogen atom using a classical formula_2 [[Electric potential|Coulomb potential]]. This "semi-classical" approach fails if quantum fluctuations in the electromagnetic field play an important role, such as in the emission of [[photon]]s by [[charged particle]]s.
[[Field (physics)|Quantum field]] theories for the [[strong nuclear force]] and the [[weak nuclear force]] have also been developed. The quantum field theory of the strong nuclear force is called [[quantum chromodynamics]], and describes the interactions of subnuclear particles such as [[quark]]s and [[gluon]]s. The weak nuclear force and the [[electromagnetic force]] were unified, in their quantized forms, into a single quantum field theory (known as [[electroweak theory]]), by the physicists [[Abdus Salam]], [[Sheldon Glashow]] and [[Steven Weinberg]]. These three men shared the Nobel Prize in Physics in 1979 for this work.
It has proven difficult to construct quantum models of [[gravity]], the remaining [[fundamental force]]. Semi-classical approximations are workable, and have led to predictions such as [[Hawking radiation]]. However, the formulation of a complete theory of [[quantum gravity]] is hindered by apparent incompatibilities between [[general relativity]] (the most accurate theory of gravity currently known) and some of the fundamental assumptions of quantum theory. The resolution of these incompatibilities is an area of active research, and theories such as [[string theory]] are among the possible candidates for a future theory of quantum gravity.
Classical mechanics has also been extended into the [[complex domain]], with complex classical mechanics exhibiting behaviors similar to quantum mechanics.
Quantum mechanics and classical physics.
Predictions of quantum mechanics have been verified experimentally to an extremely high degree of accuracy. According to the [[correspondence principle]] between classical and quantum mechanics, all objects obey the laws of quantum mechanics, and classical mechanics is just an approximation for large systems of objects (or a statistical quantum mechanics of a large collection of particles). The laws of classical mechanics thus follow from the laws of quantum mechanics as a statistical average at the limit of large systems or large [[quantum number]]s. However, [[Chaos theory|chaotic systems]] do not have good quantum numbers, and [[quantum chaos]] studies the relationship between classical and quantum descriptions in these systems.
[[Quantum coherence]] is an essential difference between classical and quantum theories as illustrated by the [[EPR paradox|Einstein–Podolsky–Rosen (EPR) paradox]] — an attack on a certain philosophical interpretation of quantum mechanics by an appeal to [[local realism]]. [[Quantum interference#Quantum interference|Quantum interference]] involves adding together "[[probability amplitude]]s", whereas classical "waves" infer that there is an adding together of "intensities". For microscopic bodies, the extension of the system is much smaller than the [[coherence length]], which gives rise to long-range entanglement and other nonlocal phenomena characteristic of quantum systems. Quantum coherence is not typically evident at macroscopic scales, though an exception to this rule may occur at extremely low temperatures (i.e. approaching [[absolute zero]]) at which quantum behavior may manifest itself macroscopically. This is in accordance with the following observations:
Copenhagen interpretation of quantum versus classical kinematics.
A big difference between classical and quantum mechanics is that they use very different kinematic descriptions.
In [[Niels Bohr]]'s mature view, quantum mechanical phenomena are required to be experiments, with complete descriptions of all the devices for the system, preparative, intermediary, and finally measuring. The descriptions are in macroscopic terms, expressed in ordinary language, supplemented with the concepts of classical mechanics. The initial condition and the final condition of the system are respectively described by values in a configuration space, for example a position space, or some equivalent space such as a momentum space. Quantum mechanics does not admit a completely precise description, in terms of both position and momentum, of an initial condition or "state" (in the classical sense of the word) that would support a precisely deterministic and causal prediction of a final condition. In this sense, advocated by Bohr in his mature writings, a quantum phenomenon is a process, a passage from initial to final condition, not an instantaneous "state" in the classical sense of that word. Thus there are two kinds of processes in quantum mechanics: stationary and transitional. For a stationary process, the initial and final condition are the same. For a transition, they are different. Obviously by definition, if only the initial condition is given, the process is not determined. Given its initial condition, prediction of its final condition is possible, causally but only probabilistically, because the Schrödinger equation is deterministic for wave function evolution, but the wave function describes the system only probabilistically.
For many experiments, it is possible to think of the initial and final conditions of the system as being a particle. In some cases it appears that there are potentially several spatially distinct pathways or trajectories by which a particle might pass from initial to final condition. It is an important feature of the quantum kinematic description that it does not permit a unique definite statement of which of those pathways is actually followed. Only the initial and final conditions are definite, and, as stated in the foregoing paragraph, they are defined only as precisely as allowed by the configuration space description or its equivalent. In every case for which a quantum kinematic description is needed, there is always a compelling reason for this restriction of kinematic precision. An example of such a reason is that for a particle to be experimentally found in a definite position, it must be held motionless; for it to be experimentally found to have a definite momentum, it must have free motion; these two are logically incompatible.
Classical kinematics does not primarily demand experimental description of its phenomena. It allows completely precise description of an instantaneous state by a value in phase space, the Cartesian product of configuration and momentum spaces. This description simply assumes or imagines a state as a physically existing entity without concern about its experimental measurability. Such a description of an initial condition, together with Newton's laws of motion, allows a precise deterministic and causal prediction of a final condition, with a definite trajectory of passage. [[Hamiltonian mechanics|Hamiltonian]] dynamics can be used for this. Classical kinematics also allows the description of a process analogous to the initial and final condition description used by quantum mechanics. [[Lagrangian mechanics]] applies to this. For processes that need account to be taken of actions of a small number of [[Planck's constant|Planck constants]], classical kinematics is not adequate; quantum mechanics is needed.
Relativity and quantum mechanics.
Even with the defining postulates of both Einstein's theory of general relativity and quantum theory being indisputably supported by rigorous and repeated [[empirical evidence]], and while they do not directly contradict each other theoretically (at least with regard to their primary claims), they have proven extremely difficult to incorporate into one consistent, cohesive model.
Einstein himself is well known for rejecting some of the claims of quantum mechanics. While clearly contributing to the field, he did not accept many of the more "philosophical consequences and interpretations" of quantum mechanics, such as the lack of deterministic [[causality]]. He is famously quoted as saying, in response to this aspect, "My God does not play with dice". He also had difficulty with the assertion that a single [[subatomic particle]] can occupy numerous areas of space at one time. However, he was also the first to notice some of the apparently exotic consequences of [[Quantum entanglement|entanglement]], and used them to formulate the [[Einstein–Podolsky–Rosen paradox]] in the hope of showing that quantum mechanics had unacceptable implications if taken as a complete description of physical reality. This was 1935, but in 1964 it was shown by John Bell (see [[Bell inequality]]) that - although Einstein was correct in identifying seemingly paradoxical implications of [[Quantum nonlocality|quantum mechanical nonlocality]] - these implications could be experimentally tested. Alain Aspect's initial experiments in 1982, and many subsequent experiments since, have definitively verified quantum entanglement.
According to the paper of J. Bell and the [[Copenhagen interpretation]]—the common interpretation of quantum mechanics by physicists since 1927 - and contrary to Einstein's ideas, quantum mechanics was "not", at the same time a "realistic" theory and a "[[Principle of locality|local]]" theory.
The [[Einstein–Podolsky–Rosen paradox]] shows in any case that there exist experiments by which one can measure the state of one particle and instantaneously change the state of its entangled partner - although the two particles can be an arbitrary distance apart. However, this effect does not violate [[causality]], since no transfer of information happens. Quantum entanglement forms the basis of [[quantum cryptography]], which is used in high-security commercial applications in banking and government.
Gravity is negligible in many areas of particle physics, so that unification between general relativity and quantum mechanics is not an urgent issue in those particular applications. However, the lack of a correct theory of [[quantum gravity]] is an important issue in [[cosmology]] and the search by physicists for an elegant "[[theory of everything|Theory of Everything]]" (TOE). Consequently, resolving the inconsistencies between both theories has been a major goal of 20th and 21st century physics. Many prominent physicists, including [[Stephen Hawking]], have labored for many years in the attempt to discover a theory underlying "everything". This TOE would combine not only the different models of subatomic physics, but also derive the four [[Fundamental interaction|fundamental forces of nature]] - the [[strong interaction|strong force]], [[electromagnetism]], the [[weak interaction|weak force]], and [[gravity]] - from a single force or phenomenon. While Stephen Hawking was initially a believer in the Theory of Everything, after considering [[Gödel's Incompleteness Theorem]], he has concluded that one is not obtainable, and has stated so publicly in his lecture "Gödel and the End of Physics" (2002).
Attempts at a unified field theory.
The quest to unify the [[fundamental force]]s through quantum mechanics is still ongoing. [[Quantum electrodynamics]] (or "quantum electromagnetism"), which is currently (in the perturbative regime at least) the most accurately tested physical theory in competition with general relativity, (blog) has been successfully merged with the weak nuclear force into the [[electroweak force]] and work is currently being done to merge the electroweak and strong force into the [[electrostrong force]]. Current predictions state that at around 1014 GeV the three aforementioned forces are fused into a single unified field. Beyond this "grand unification", it is speculated that it may be possible to merge gravity with the other three gauge symmetries, expected to occur at roughly 1019 GeV. However — and while special relativity is parsimoniously incorporated into quantum electrodynamics — the expanded [[general relativity]], currently the best theory describing the gravitation force, has not been fully incorporated into quantum theory. One of those searching for a coherent TOE is [[Edward Witten]], a theoretical physicist who formulated the [[M-theory]], which is an attempt at describing the supersymmetrical based [[string theory]]. M-theory posits that our apparent 4-dimensional [[spacetime]] is, in reality, actually an 11-dimensional spacetime containing 10 spatial dimensions and 1 time dimension, although 7 of the spatial dimensions are - at lower energies - completely "compactified" (or infinitely curved) and not readily amenable to measurement or probing.
Another popular theory is [[Loop quantum gravity]] (LQG), a theory first proposed by [[Carlo Rovelli]] that describes the quantum properties of gravity. It is also a theory of [[quantum space]] and [[Chronon|quantum time]], because in general relativity the geometry of spacetime is a manifestation of [[gravity]]. LQG is an attempt to merge and adapt standard quantum mechanics and standard [[general relativity]]. The main output of the theory is a physical picture of space where space is granular. The granularity is a direct consequence of the quantization. It has the same nature of the granularity of the photons in the quantum theory of electromagnetism or the discrete levels of the energy of the atoms. But here it is space itself which is discrete.
More precisely, space can be viewed as an extremely fine fabric or network "woven" of finite loops. These networks of loops are called [[spin networks]]. The evolution of a spin network over time is called a spin foam. The predicted size of this structure is the [[Planck length]], which is approximately 1.616×10−35 m. According to theory, there is no meaning to length shorter than this (cf. [[Planck scale]] energy). Therefore, LQG predicts that not just matter, but also space itself, has an atomic structure.
Philosophical implications.
Since its inception, the many [[counter-intuitive]] aspects and results of quantum mechanics have provoked strong [[philosophy|philosophical]] debates and many [[interpretations of quantum mechanics|interpretations]]. Even fundamental issues, such as [[Max Born]]'s basic [[Born rule|rules]] concerning [[probability amplitude]]s and [[probability distribution]]s, took decades to be appreciated by society and many leading scientists. [[Richard Feynman]] once said, "I think I can safely say that nobody understands quantum mechanics." According to [[Steven Weinberg]], "There is now in my opinion no entirely satisfactory interpretation of quantum mechanics."
The [[Copenhagen interpretation]] - due largely to the Danish theoretical physicist [[Niels Bohr]] - remains the quantum mechanical formalism that is currently most widely accepted amongst physicists, some 75 years after its enunciation. According to this interpretation, the probabilistic nature of quantum mechanics is not a "temporary" feature which will eventually be replaced by a deterministic theory, but instead must be considered a "final" renunciation of the classical idea of "causality." It is also believed therein that any well-defined application of the quantum mechanical formalism must always make reference to the experimental arrangement, due to the [[complementarity (physics)|conjugate]] nature of evidence obtained under different experimental situations.
[[Albert Einstein]], himself one of the founders of quantum theory, [[Bohr-Einstein debates|rejected]] the quantum theoretical doctrine that the state of a system depends on the experimental arrangement for its measurement. He held that underlying quantum mechanics there should be a theory that thoroughly and directly expresses the rule against [[action at a distance]]; in other words, he insisted on the [[principle of locality]]. He inferred that the present theory was incomplete, contrary to the Copenhagen doctrine that it is complete. He therefore produced a series of objections, the most famous of which has become known as the [[Einstein–Podolsky–Rosen paradox]].
[[John Stewart Bell|John Bell]] showed that this "EPR" paradox led to [[Bell's theorem|experimentally testable differences]] between quantum mechanics and theories that rely on added hidden variables. [[Bell test experiments|Experiments]] have been performed confirming the accuracy of quantum mechanics, thereby demonstrating that quantum mechanics cannot be improved upon by addition of hidden variables. The "[[Bohr-Einstein debates]]" provide a vibrant critique of the Copenhagen Interpretation from an [[epistemological]] point of view.
The [[Everett many-worlds interpretation]], formulated in 1956, holds that "all" the possibilities described by quantum theory "simultaneously" occur in a [[multiverse]] composed of mostly independent parallel universes. This is not accomplished by introducing some "new axiom" to quantum mechanics, but on the contrary, by "removing" the axiom of the collapse of the wave packet. "All" of the possible consistent states of the measured system and the measuring apparatus (including the observer) are present in a "real" physical - not just formally mathematical, as in other interpretations - [[quantum superposition]]. Such a superposition of consistent state combinations of different systems is called an [[Quantum entanglement|entangled state]]. While the multiverse is deterministic, we perceive non-deterministic behavior governed by probabilities, because we can only observe the universe (i.e., the consistent state contribution to the aforementioned superposition) that we, as observers, inhabit. Everett's interpretation is perfectly consistent with [[John Stewart Bell|John Bell]]'s experiments and makes them intuitively understandable. However, according to the theory of [[quantum decoherence]], these "parallel universes" will never be accessible to us. The inaccessibility can be understood as follows: once a measurement is done, the measured system becomes [[Quantum entanglement|entangled]] with "both" the physicist who measured it "and" a huge number of other particles, some of which are [[photon]]s flying away at the [[speed of light]] towards the other end of the universe. In order to prove that the wave function did not collapse, one would have to bring "all" these particles back and measure them again, together with the system that was originally measured. Not only is this completely impractical, but even if one "could" theoretically do this, it would have to destroy any evidence that the original measurement took place (including the physicist's memory). In light of these [[Bell test]]s, Cramer (1986) formulated his [[transactional interpretation]]. [[Relational quantum mechanics]] appeared in the late 1990s as the modern derivative of the [[Copenhagen Interpretation]].
Applications.
Quantum mechanics has had enormous success in explaining many of the features of our universe. Quantum mechanics is often the only tool available that can reveal the individual behaviors of the [[subatomic particle]]s that make up all forms of matter ([[electron]]s, [[proton]]s, [[neutron]]s, [[photon]]s, and others). Quantum mechanics has strongly influenced [[string theory|string theories]], candidates for a [[theory of everything|Theory of Everything]] (see [[reductionism]]).
Quantum mechanics is also critically important for understanding how individual atoms combine covalently to form [[molecules]]. The application of quantum mechanics to [[chemistry]] is known as [[quantum chemistry]]. Relativistic quantum mechanics can, in principle, mathematically describe most of chemistry. Quantum mechanics can also provide quantitative insight into [[Ionic bond|ionic]] and [[covalent bonding]] processes by explicitly showing which molecules are energetically favorable to which others and the magnitudes of the energies involved. Furthermore, most of the calculations performed in modern [[computational chemistry]] rely on quantum mechanics.
[[File:Rtd seq v3.gif|thumb|350px|right| A working mechanism of a resonant tunneling diode device, based on the phenomenon of quantum tunneling through potential barriers]]
A great deal of modern technological inventions operate at a scale where quantum effects are significant. Examples include the [[laser]], the [[transistor]] (and thus the [[Integrated circuit|microchip]]), the [[electron microscope]], and [[Magnetic Resonance Imaging|magnetic resonance imaging]] (MRI). The study of [[semiconductor]]s led to the invention of the [[diode]] and the [[transistor]], which are indispensable parts of modern [[electronics]] systems and devices.
Researchers are currently seeking robust methods of directly manipulating quantum states. Efforts are being made to more fully develop [[quantum cryptography]], which will theoretically allow guaranteed secure transmission of [[information]]. A more distant goal is the development of [[quantum computer]]s, which are expected to perform certain computational tasks exponentially faster than classical [[computer]]s. Instead of using classical bits, quantum computers use [[qubits]], which can be in [[quantum superposition|superpositions]] of states. Another active research topic is [[quantum teleportation]], which deals with techniques to transmit quantum information over arbitrary distances.
[[Quantum tunneling]] is vital to the operation of many devices. Even in the simple [[light switch]], the electrons in the [[electric current]] could not penetrate the potential barrier made up of a layer of oxide without quantum tunneling. [[Flash memory]] chips found in [[USB flash drive|USB drive]]s use quantum tunneling to erase their memory cells.
While quantum mechanics primarily applies to the smaller atomic regimes of matter and energy, some systems exhibit [[Mechanics#Classical versus quantum|quantum mechanical effects]] on a large scale. [[Superfluidity]], the frictionless flow of a liquid at temperatures near [[absolute zero]], is one well-known example. So is the closely related phenomenon of [[superconductivity]], the frictionless flow of an electron gas in a conducting material (an [[electric current]]) at sufficiently low temperatures.
Quantum theory also provides accurate descriptions for many previously unexplained phenomena, such as [[black-body radiation]] and the stability of the [[Atomic orbital|orbitals]] of electrons in atoms. It has also given insight into the workings of many different [[biological systems]], including [[smell receptors]] and [[protein structure]]s. Recent work on [[photosynthesis]] has provided evidence that quantum correlations play an essential role in this fundamental process of plants and many other organisms. Even so, [[classical physics]] can often provide good approximations to results otherwise obtained by quantum physics, typically in circumstances with large numbers of particles or large [[quantum number]]s. Since classical formulas are much simpler and easier to compute than quantum formulas, classical approximations are used and preferred when the system is large enough to render the effects of quantum mechanics insignificant.
Examples.
Free particle.
For example, consider a [[free particle]]. In quantum mechanics, there is [[wave–particle duality]], so the properties of the particle can be described as the properties of a wave. Therefore, its [[quantum state]] can be represented as a [[wave]] of arbitrary shape and extending over space as a [[wave function]]. The position and momentum of the particle are [[observables]]. The [[Uncertainty Principle]] states that both the position and the momentum cannot simultaneously be measured with complete precision. However, one "can" measure the position (alone) of a moving free particle, creating an eigenstate of position with a wave function that is very large (a [[Dirac delta]]) at a particular position "x", and zero everywhere else. If one performs a position measurement on such a wave function, the resultant "x" will be obtained with 100% probability (i.e., with full certainty, or complete precision). This is called an eigenstate of position—or, stated in mathematical terms, a "generalized position eigenstate ([[Distribution (mathematics)|eigendistribution]])". If the particle is in an eigenstate of position, then its momentum is completely unknown. On the other hand, if the particle is in an eigenstate of momentum, then its position is completely unknown.
In an eigenstate of momentum having a [[plane wave]] form, it can be shown that the [[wavelength]] is equal to "h/p", where "h" is [[Planck's constant]] and "p" is the momentum of the [[eigenstate]].
[[File:QuantumDot wf.gif|thumb|600px|center|3D confined electron wave functions for each eigenstate in a Quantum Dot. Here, rectangular and triangular-shaped quantum dots are shown. Energy states in rectangular dots are more ‘s-type’ and ‘p-type’. However, in a triangular dot, the wave functions are mixed due to confinement symmetry.]]
Step potential.
[[File:Qm step pot temp.png|thumb|right|400px|Scattering at a finite potential step of height "V"0, shown in green. The amplitudes and direction of left- and right-moving waves are indicated. Yellow is the incident wave, blue are reflected and transmitted waves, red does not occur. "E" > "V"0 for this figure.]]
The potential in this case is given by:
The solutions are superpositions of left- and right-moving waves:
where the [[wave vector]]s are related to the energy via
with coefficients A and B determined from the [[boundary condition]]s and by imposing a continuous [[derivative]] on the solution.
Each term of the solution can be interpreted as an incident, reflected, or transmitted component of the wave, allowing the calculation of transmission and reflection coefficients. Notably, in contrast to classical mechanics, incident particles with energies greater than the potential step are partially reflected.
Rectangular potential barrier.
This is a model for the [[quantum tunneling]] effect which plays an important role in the performance of modern technologies such as [[flash memory]] and [[scanning tunneling microscope|scanning tunneling microscopy]]. Quantum tunneling is central to physical phenomena involved in [[superlattice]]s.
Particle in a box.
[[File:Infinite potential well.svg|thumb|1-dimensional potential energy box (or infinite potential well)]]
The particle in a one-dimensional potential energy box is the most mathematically simple example where restraints lead to the quantization of energy levels. The box is defined as having zero potential energy everywhere "inside" a certain region, and infinite potential energy everywhere "outside" that region. For the one-dimensional case in the formula_8 direction, the time-independent Schrödinger equation may be written
With the differential operator defined by
the previous equation is evocative of the [[Kinetic energy#Kinetic energy of rigid bodies|classic kinetic energy analogue]],
with state formula_12 in this case having energy formula_13 coincident with the kinetic energy of the particle.
The general solutions of the Schrödinger equation for the particle in a box are
or, from [[Euler's formula]],
The infinite potential walls of the box determine the values of "C", "D", and "k" at and where must be zero. Thus, at ,
and . At ,
in which "C" cannot be zero as this would conflict with the Born interpretation. Therefore, since , "kL" must be an integer multiple of π,
The quantization of energy levels follows from this constraint on "k", since
Finite potential well.
A finite potential well is the generalization of the infinite potential well problem to potential wells having finite depth.
The finite potential well problem is mathematically more complicated than the infinite particle-in-a-box problem as the wave function is not pinned to zero at the walls of the well. Instead, the wave function must satisfy more complicated mathematical boundary conditions as it is nonzero in regions outside the well.
Harmonic oscillator.
[[File:QuantumHarmonicOscillatorAnimation.gif|thumb|300px|right|Some trajectories of a [[harmonic oscillator]] (i.e. a ball attached to a [[Hooke's law|spring]]) in [[classical mechanics]] (A-B) and quantum mechanics (C-H). In quantum mechanics, the position of the ball is represented by a [[wave]] (called the [[wave function]]), with the [[real part]] shown in blue and the [[imaginary part]] shown in red. Some of the trajectories (such as C,D,E,and F) are [[standing wave]]s (or "[[stationary state]]s"). Each standing-wave frequency is proportional to a possible [[energy level]] of the oscillator. This "energy quantization" does not occur in classical physics, where the oscillator can have "any" energy.]]
As in the classical case, the potential for the quantum harmonic oscillator is given by
This problem can either be treated by directly solving the Schrödinger equation, which is not trivial, or by using the more elegant "ladder method" first proposed by Paul Dirac. The [[eigenstate]]s are given by
where "Hn" are the [[Hermite polynomials]],
and the corresponding energy levels are
This is another example illustrating the quantization of energy for bound states.
References.
The following titles, all by working physicists, attempt to communicate quantum theory to lay people, using a minimum of technical apparatus.
More technical:
External links.
[[Category:Concepts in physics]]
[[Category:Quantum mechanics| ]]

</doc>
<doc id="532405" url="https://en.wikipedia.org/wiki?curid=532405" title="Quantum number">
Quantum number

Quantum numbers describe values of conserved quantities in the dynamics of a quantum system. In the case of quantum numbers of electrons, they can be defined as "the sets of numerical values which give acceptable solutions to the Schrödinger wave equation for the hydrogen atom". Perhaps the most important aspect of quantum mechanics is the quantization of observable quantities, since quantum numbers are discrete sets of integers or half-integers, although they could approach infinity in some cases. This is distinguished from classical mechanics where the values can range continuously. Quantum numbers often describe specifically the energy levels of electrons in atoms, but other possibilities include angular momentum, spin, etc. Any quantum system can have one or more quantum numbers; it is thus difficult to list all possible quantum numbers.
How many quantum numbers?
The question of "how many quantum numbers are needed to describe any given system" has no universal answer. Hence for each system one must find the answer for a full analysis of the system. A quantized system requires at least one quantum number. The dynamics of any quantum system are described by a quantum Hamiltonian, "H". There is one quantum number of the system corresponding to the energy, i.e., the eigenvalue of the Hamiltonian. There is also one quantum number for each operator "O" that commutes with the Hamiltonian. These are all the quantum numbers that the system can have. Note that the operators "O" defining the quantum numbers should be independent of each other. Often, there is more than one way to choose a set of independent operators. Consequently, in different situations different sets of quantum numbers may be used for the description of the same system.
Spatial and angular momentum numbers.
There are four quantum numbers which can describe an electron in an atom completely.
Traditional nomenclatures.
Many different models have been proposed throughout the history of quantum mechanics, but the most prominent system of nomenclature spawned from the Hund-Mulliken molecular orbital theory of Friedrich Hund, Robert S. Mulliken, and contributions from Schrödinger, Slater and John Lennard-Jones. This system of nomenclature incorporated Bohr energy levels, Hund-Mulliken orbital theory, and observations on electron spin based on spectroscopy and Hund's rules.
This model describes electrons using four quantum numbers, "n", "", "m", "ms", given below. It is also the common nomenclature in the classical description of nuclear particle states (e.g. protons and neutrons). Molecular orbitals require different quantum numbers, because the Hamiltonian and its symmetries are quite different.
Note that there is no universal fixed value for "m" and "ms" values. Rather, the "m" and "ms" values are random. The only requirement is that the naming schematic used within a particular set of calculations or descriptions must be consistent (e.g. the orbital occupied by the first electron in a p orbital could be described as "m" = −1 or "m" = 0, or "m" = 1, but the "m" value of the other electron in that orbital must be different; yet, the "m" assigned to electrons in other orbitals again can be "m" = −1 or "m" = 0, or "m" = 1 ).
These rules are summarized as follows:
Example: The quantum numbers used to refer to the outermost valence electrons of the Carbon (C) atom, which are located in the 2p atomic orbital, are; "n" = 2 (2nd electron shell), "" = 1 (p orbital subshell), "m" = 1, 0 or −1, "ms" = ½ (parallel spins).
Results from spectroscopy indicated that up to two electrons can occupy a single orbital. However two electrons can never have the same exact quantum state nor the same set of quantum numbers according to Hund's rules, which addresses the Pauli exclusion principle. A fourth quantum number with two possible values was added as an "ad hoc" assumption to resolve the conflict; this supposition could later be explained in detail by relativistic quantum mechanics and from the results of the renowned Stern–Gerlach experiment.
Total angular momenta numbers.
Total momentum of a particle.
When one takes the spin-orbit interaction into consideration, the "L" and "S" operators no longer commute with the Hamiltonian, and their eigenvalues therefore change over time. Thus another set of quantum numbers should be used. This set includes
For example, consider the following eight states, defined by their quantum numbers:
The quantum states in the system can be described as linear combination of these eight states. However, in the presence of spin-orbit interaction, if one wants to describe the same system by eight states which are eigenvectors of the Hamiltonian (i.e. each represents a state which does not mix with others over time), we should consider the following eight states:
Nuclear angular momentum quantum numbers.
In nuclei, the entire assembly of protons and neutrons (nucleons) has a resultant angular momentum due to the angular momenta of each nucleon, usually denoted I. If the total angular momentum of a neutron is "jn" = + "s" and for a proton is "jp" = + "s" (where "s" for protons and neutrons happens to be ½ again) then the nuclear angular momentum quantum numbers "I" are given by:
Parity with the number "I" is used to label nuclear angular momentum states, examples for some isotopes of Hydrogen (H), Carbon (C), and Sodium (Na) are;
The reason for the unusual fluctuations in "I", even by differences of just one nucleon, are due to the odd/even numbers of protons and neutrons - pairs of nucleons have a total angular momentum of zero (just like electrons in orbitals), leaving an odd/even numbers of unpaired nucleons. The property of nuclear spin is an important factor for the operation of NMR spectroscopy in organic chemistry, and MRI in nuclear medicine, due to the nuclear magnetic moment interacting with an external magnetic field.
Elementary particles.
Elementary particles contain many quantum numbers which are usually said to be intrinsic to them. However, it should be understood that the elementary particles are quantum states of the standard model of particle physics, and hence the quantum numbers of these particles bear the same relation to the Hamiltonian of this model as the quantum numbers of the Bohr atom does to its Hamiltonian. In other words, each quantum number denotes a symmetry of the problem. It is more useful in quantum field theory to distinguish between spacetime and internal symmetries.
Typical quantum numbers related to spacetime symmetries are spin (related to rotational symmetry), the parity, C-parity and T-parity (related to the Poincaré symmetry of spacetime). Typical internal symmetries are lepton number and baryon number or the electric charge. (For a full list of quantum numbers of this kind see the article on flavour.)
A minor but often confusing point is as follows: most conserved quantum numbers are additive, so in an elementary particle reaction, the "sum" of the quantum numbers should be the same before and after the reaction. However, some, usually called a "parity", are multiplicative; i.e., their "product" is conserved. All multiplicative quantum numbers belong to a symmetry (like parity) in which applying the symmetry transformation twice is equivalent to doing nothing (involution). These are all examples of an abstract group called Z2.

</doc>
<doc id="30876419" url="https://en.wikipedia.org/wiki?curid=30876419" title="Quantum state">
Quantum state

In quantum physics, quantum state refers to the state of a quantum system. 
A quantum state can be either pure or mixed. A pure quantum state is represented by a vector, called a state vector, in a Hilbert space. For example, when dealing with the energy spectrum of the electron in a hydrogen atom, the relevant state vectors are identified by the principal quantum number, written formula_1. For a more complicated case, consider Bohm's formulation of the EPR experiment, where the state vector 
involves superposition of joint spin states for two particles.
Mathematically, a pure quantum state is represented by a state vector in a Hilbert space over complex numbers, which is a generalization of our more usual three-dimensional space. If this Hilbert space is represented as a function space, then its elements are called wave functions.
A mixed quantum state corresponds to a probabilistic mixture of pure states; however, different distributions of pure states can generate equivalent (i.e., physically indistinguishable) mixed states. Mixed states are described by so-called density matrices. A pure state can also be recast as a density matrix; in this way, pure states can be represented as a subset of the more general mixed states.
For example, if the spin of an electron is measured in any direction, e.g. with a Stern–Gerlach experiment, there are two possible results: up or down. The Hilbert space for the electron's spin is therefore two-dimensional. A pure state here is represented by a two-dimensional complex vector formula_3, with a length of one; that is, with
where formula_5 and formula_6 are the absolute values of formula_7 and formula_8. A mixed state, in this case, is a formula_9 matrix that is Hermitian, positive-definite, and has trace 1.
Before a particular measurement is performed on a quantum system, the theory usually gives only a probability distribution for the outcome, and the form that this distribution takes is completely determined by the quantum state and the observable describing the measurement. These probability distributions arise for both mixed states and pure states: it is impossible in quantum mechanics (unlike classical mechanics) to prepare a state in which all properties of the system are fixed and certain. This is exemplified by the uncertainty principle, and reflects a core difference between classical and quantum physics. Even in quantum theory, however, for every observable there are some states that have an exact and determined value for that observable.
Conceptual description.
Pure states.
In the mathematical formulation of quantum mechanics, pure quantum states correspond to vectors in a Hilbert space, while each observable quantity (such as the energy or momentum of a particle) is associated with a mathematical operator. The operator serves as a linear function which acts on the states of the system. The eigenvalues of the operator correspond to the possible values of the observable, i.e. it is possible to observe a particle with a momentum of 1 kg⋅m/s if and only if one of the eigenvalues of the momentum operator is 1 kg⋅m/s. The corresponding eigenvector (which physicists call an eigenstate) with eigenvalue 1 kg⋅m/s would be a quantum state with a definite, well-defined value of momentum of 1 kg⋅m/s, with no quantum uncertainty. If its momentum were measured, the result is guaranteed to be 1 kg⋅m/s.
On the other hand, a system in a linear combination of multiple different eigenstates "does" in general have quantum uncertainty for the given observable. We can represent this linear combination of eigenstates as:
The coefficient which corresponds to a particular state in the linear combination is complex thus allowing interference effects between states. The coefficients are time dependent. How a quantum system changes in time is governed by the time evolution operator. The symbols formula_11 and formula_12 surrounding the formula_13 are part of bra–ket notation.
Statistical mixtures of states are different from a linear combination. A statistical mixture of states is a statistical ensemble of independent systems. Statistical mixtures represent the degree of knowledge whilst the uncertainty within quantum mechanics is fundamental. Mathematically, a statistical mixture is not a combination using complex coefficients, but rather a combination using real-valued, positive probabilities of different states formula_14. A number formula_15 represents the probability of a randomly selected system being in the state formula_14. Unlike the linear combination case each system is in a definite eigenstate.
The expectation value formula_17 of an observable "A" is a statistical mean of measured values of the observable. It is this mean, and the distribution of probabilities, that is predicted by physical theories.
There is no state which is simultaneously an eigenstate for "all" observables. For example, we cannot prepare a state such that both the position measurement "Q"("t") and the momentum measurement "P"("t") (at the same time "t") are known exactly; at least one of them will have a range of possible values. This is the content of the Heisenberg uncertainty relation.
Moreover, in contrast to classical mechanics, it is unavoidable that "performing a measurement on the system generally changes its state"
More precisely: After measuring an observable "A", the system will be in an eigenstate of "A"; thus the state has changed, unless the system was already in that eigenstate. This expresses a kind of logical consistency: If we measure "A" twice in the same run of the experiment, the measurements being directly consecutive in time, then they will produce the same results. This has some strange consequences, however, as follows.
Consider two observables, "A" and "B", where "A" corresponds to a measurement earlier in time than "B". 
Suppose that the system is in an eigenstate of "B" at the experiment's begin. If we measure only "B", we will not notice statistical behaviour.
If we measure first "A" and then "B" in the same run of the experiment, the system will transfer to an eigenstate of "A" after the first measurement, and we will generally notice that the results of "B" are statistical. Thus: "Quantum mechanical measurements influence one another", and it is important in which order they are performed.
Another feature of quantum states becomes relevant if we consider a physical system that consists of multiple subsystems; for example, an experiment with two particles rather than one. Quantum physics allows for certain states, called "entangled states", that show certain statistical correlations between measurements on the two particles which cannot be explained by classical theory. For details, see entanglement. These entangled states lead to experimentally testable properties (Bell's theorem)
that allow us to distinguish between quantum theory and alternative classical (non-quantum) models.
Schrödinger picture vs. Heisenberg picture.
One can take the observables to be dependent on time, while the state "σ" was fixed once at the beginning of the experiment. This approach is called the Heisenberg picture. (This approach was taken in the later part of the discussion above, with time-varying observables "P"("t"), "Q"("t").) One can, equivalently, treat the observables as fixed, while the state of the system depends on time; that is known as the Schrödinger picture. (This approach was taken in the earlier part of the discussion above, with a time-varying state formula_10.) Conceptually (and mathematically), the two approaches are equivalent; choosing one of them is a matter of convention.
Both viewpoints are used in quantum theory. While non-relativistic quantum mechanics is usually formulated in terms of the Schrödinger picture, the Heisenberg picture is often preferred in a relativistic context, that is, for quantum field theory. Compare with Dirac picture.
Formalism in quantum physics.
Pure states as rays in a Hilbert space.
Quantum physics is most commonly formulated in terms of linear algebra, as follows. Any given system is identified with some finite- or infinite-dimensional Hilbert space. The pure states correspond to vectors of norm 1. Thus the set of all pure states corresponds to the unit sphere in the Hilbert space.
Multiplying a pure state by a scalar is physically inconsequential (as long as the state is considered by itself). If one vector is obtained from the other by multiplying by a scalar of unit magnitude, the two vectors are said to correspond to the same "ray" in Hilbert space and also to the same point in the projective Hilbert space.
Bra–ket notation.
Calculations in quantum mechanics make frequent use of linear operators, inner products, dual spaces and Hermitian conjugation. In order to make such calculations flow smoothly, and to obviate the need (in some contexts) to fully understand the underlying linear algebra, Paul Dirac invented a notation to describe quantum states, known as "bra-ket notation". Although the details of this are beyond the scope of this article (see the article bra–ket notation), some consequences of this are:
Spin.
The angular momentum has the same dimension as the Planck constant and, at quantum scale, behaves as a "discrete" degree of freedom. Most particles possess a kind of intrinsic angular momentum that does not appear at all in classical mechanics and arises from Dirac's relativistic generalization of the theory. Mathematically it is described with spinors. In non-relativistic quantum mechanics the group representations of the Lie group SU(2) are used to describe this additional freedom. For a given particle, the choice of representation (and hence the range of possible values of the spin observable) is specified by a non-negative number "S" that, in units of Planck's reduced constant "ħ", is either an integer (0, 1, 2 ...) or a half-integer (1/2, 3/2, 5/2 ...). For a massive particle with spin "S", its spin quantum number "m" always assumes one of the 2"S" + 1 possible values in the set 
As a consequence, the quantum state of a particle with spin is described by a vector-valued wave function with values in C2"S"+1. Equivalently, it is represented by a complex-valued function of four variables: one discrete quantum number variable (for the spin) is added to the usual three continuous variables (for the position in space).
Many-body states and particle statistics.
The quantum state of a system of "N" particles, each potentially with spin, is described by a complex-valued function with four variables per particle, e.g.
Here, the spin variables "mν" assume values from the set 
where formula_30 is the spin of "ν"th particle. formula_31 for a particle that does not exhibit spin.
The treatment of identical particles is very different for bosons (particles with integer spin) versus fermions (particles with half-integer spin). The above "N"-particle function must either be symmetrized (in the bosonic case) or anti-symmetrized (in the fermionic case) with respect to the particle numbers. If not all "N" particles are identical, but some of them are, then the function must be (anti)symmetrized separately over the variables corresponding to each group of identical variables, according to its statistics (bosonic or fermionic).
Electrons are fermions with "S" = 1/2, photons (quanta of light) are bosons with "S" = 1 (although in the vacuum they are massless and can't be described with Schrödingerian mechanics).
When symmetrization or anti-symmetrization is unnecessary, "N"-particle spaces of states can be obtained simply by tensor products of one-particle spaces, to which we will return later.
Basis states of one-particle systems.
As with any Hilbert space, if a basis is chosen for the Hilbert space of a system, then any ket can be expanded as a linear combination of those basis elements. Symbolically, given basis kets formula_32, any ket formula_33 can be written
where "ci" are complex numbers. In physical terms, this is described by saying that formula_33 has been expressed as a "quantum superposition" of the states formula_32. If the basis kets are chosen to be orthonormal (as is often the case), then formula_37.
One property worth noting is that the "normalized" states formula_33 are characterized by
Expansions of this sort play an important role in measurement in quantum mechanics. In particular, if the formula_32 are eigenstates (with eigenvalues "ki") of an observable, and that observable is measured on the normalized state formula_33, then the probability that the result of the measurement is "ki" is |"ci"|2. (The normalization condition above mandates that the total sum of probabilities is equal to one.)
A particularly important example is the "position basis", which is the basis consisting of eigenstates of the observable which corresponds to measuring position. If these eigenstates are nondegenerate (for example, if the system is a single, spinless particle), then any ket formula_33 is associated with a complex-valued function of three-dimensional space:
This function is called the wavefunction corresponding to formula_33.
Superposition of pure states.
One aspect of quantum states, mentioned above, is that superpositions of them can be formed. If formula_45 and formula_46 are two kets corresponding to quantum states, the ket
is a different quantum state (possibly not normalized). Note that "which" quantum state it is depends on both the amplitudes and phases (arguments) of formula_48 and formula_49. In other words, for example, even though formula_33 and formula_51 (for real "θ") correspond to the same physical quantum state, they are "not interchangeable", since for example formula_52 and formula_53 do "not" (in general) correspond to the same physical state. However, formula_52 and formula_55 "do" correspond to the same physical state. This is sometimes described by saying that "global" phase factors are unphysical, but "relative" phase factors are physical and important.
One example of a quantum interference phenomenon that arises from superposition is the double-slit experiment. The photon state is a superposition of two different states, one of which corresponds to the photon having passed through the left slit, and the other corresponding to passage through the right slit. The relative phase of those two states has a value which depends on the distance from each of the two slits. Depending on what that phase is, the interference is constructive at some locations and destructive in others, creating the interference pattern. By the analogy with coherence in other wave phenomena, a superposed state can be referred to as a "coherent superposition".
Another example of the importance of relative phase in quantum superposition is Rabi oscillations, where the relative phase of two states varies in time due to the Schrödinger equation. The resulting superposition ends up oscillating back and forth between two different states.
Mixed states.
A "pure quantum state" is a state which can be described by a single ket vector, as described above. A "mixed quantum state" is a statistical ensemble of pure states (see quantum statistical mechanics). Mixed states inevitably arise from pure states when, for a composite quantum system formula_56 with an entangled state on it, the part formula_57 is inaccessible to the observer. The state of the part formula_58 is expressed then as the partial trace over formula_57.
A mixed state "cannot" be described as a ket vector. Instead, it is described by its associated "density matrix" (or "density operator"), usually denoted "ρ". Note that density matrices can describe both mixed "and" pure states, treating them on the same footing. Moreover, a mixed quantum state on a given quantum system described by a Hilbert space formula_60 can be always represented as the partial trace of a pure quantum state (called a purification) on a larger bipartite system formula_61 for a sufficiently large Hilbert space formula_62.
The density matrix describing a mixed state is defined to be an operator of the form
where formula_64 is the fraction of the ensemble in each pure state formula_65 The density matrix can be thought of as a way of using the one-particle formalism to describe the behavior of many similar particles by giving a probability distribution (or ensemble) of states that these particles can be found in.
A simple criterion for checking whether a density matrix is describing a pure or mixed state is that the trace of "ρ"2 is equal to 1 if the state is pure, and less than 1 if the state is mixed. Another, equivalent, criterion is that the von Neumann entropy is 0 for a pure state, and strictly positive for a mixed state.
The rules for measurement in quantum mechanics are particularly simple to state in terms of density matrices. For example, the ensemble average (expectation value) of a measurement corresponding to an observable "A" is given by
where formula_67 are eigenkets and eigenvalues, respectively, for the operator "A", and "tr" denotes trace. It is important to note that two types of averaging are occurring, one being a weighted quantum superposition over the basis kets formula_68 of the pure states, and the other being a statistical (said "incoherent") average with the probabilities "ps" of those states.
According to Wigner, the concept of mixture was put forward by Landau.
Interpretation.
Although theoretically, for a given quantum system, a state vector provides the full information about its evolution, it is not easy to understand what information about the "real world" it carries. Due to the uncertainty principle, a state, even if it has the value of one observable exactly defined (i.e. the observable has this state as an eigenstate), cannot exactly define values of "all" observables.
For state vectors (pure states), probability amplitudes offer a probabilistic interpretation. It can be generalized for all states (including mixed), for instance, as expectation values mentioned above.
Mathematical generalizations.
States can be formulated in terms of observables, rather than as vectors in a vector space. These are positive normalized linear functionals on a C*-algebra, or sometimes other classes of algebras of observables.
See State on a C*-algebra and Gelfand–Naimark–Segal construction for more details.
Further reading.
The concept of quantum states, in particular the content of the section Formalism in quantum physics above, is covered in most standard textbooks on quantum mechanics.
For a discussion of conceptual aspects and a comparison with classical states, see:
For a more detailed coverage of mathematical aspects, see:
For a discussion of purifications of mixed quantum states, see Chapter 2 of John Preskill's lecture notes for Physics 219 at Caltech.

</doc>
<doc id="643769" url="https://en.wikipedia.org/wiki?curid=643769" title="Quantum tunnelling">
Quantum tunnelling

Quantum tunnelling or tunneling (see spelling differences) refers to the quantum mechanical phenomenon where a particle tunnels through a barrier that it classically could not surmount. This plays an essential role in several physical phenomena, such as the nuclear fusion that occurs in main sequence stars like the Sun. It has important applications to modern devices such as the tunnel diode, quantum computing, and the scanning tunnelling microscope. The effect was predicted in the early 20th century and its acceptance as a general physical phenomenon came mid-century.
Tunnelling is often explained using the Heisenberg uncertainty principle and the wave–particle duality of matter. Pure quantum mechanical concepts are central to the phenomenon, so quantum tunnelling is one of the novel implications of quantum mechanics.
History.
Quantum tunnelling was developed from the study of radioactivity, which was discovered in 1896 by Henri Becquerel. Radioactivity was examined further by Marie Curie and Pierre Curie, for which they earned the Nobel Prize in Physics in 1903. Ernest Rutherford and Egon Schweidler studied its nature, which was later verified empirically by Friedrich Kohlrausch. The idea of the half-life and the impossibility of predicting decay was created from their work.
Friedrich Hund was the first to take notice of tunnelling in 1927 when he was calculating the ground state of the double-well potential. Its first application was a mathematical explanation for alpha decay, which was done in 1928 by George Gamow and independently by Ronald Gurney and Edward Condon. The two researchers simultaneously solved the [[Schrödinger equation]] for a model nuclear potential and derived a relationship between the [[half-life]] of the particle and the energy of emission that depended directly on the mathematical probability of tunnelling.
After attending a seminar by Gamow, [[Max Born]] recognised the generality of tunnelling. He realised that it was not restricted to [[nuclear physics]], but was a general result of [[quantum mechanics]] that applies to many different systems. Shortly thereafter, both groups considered the case of particles tunnelling into the nucleus. The study of [[semiconductor]]s and the development of [[transistor]]s and [[diode]]s led to the acceptance of electron tunnelling in solids by 1957. The work of [[Leo Esaki]], [[Ivar Giaever]] and [[Brian Josephson]] predicted the tunnelling of [[superconducting]] [[Cooper pair]]s, for which they received the [[Nobel Prize in Physics]] in 1973.
Introduction to the concept.
[[File:Quantum tunnel effect and its application to the scanning tunneling microscope.ogv|thumb|right|upright=1.5|Animation showing the tunnel effect and its application to an [[STM]]]]
[[File:TunnelEffektKling1.png|300px|right|thumb|Quantum tunnelling through a barrier. The energy of the tunnelled particle is the same but the amplitude is decreased.]]
[[File:Quantum Tunnelling animation.gif|300px|right|thumb|Quantum tunnelling through a barrier. At the origin (x=0), there is a very high, but narrow potential barrier. A significant tunnelling effect can be seen.]]
Quantum tunnelling falls under the domain of [[quantum mechanics]]: the study of what happens at the [[quantum realm|quantum scale]]. This process cannot be directly perceived, but much of its understanding is shaped by the microscopic world, which [[classical mechanics]] cannot adequately explain. To understand the [[phenomenon]], particles attempting to travel between [[potential barrier]]s can be compared to a ball trying to roll over a hill; [[quantum mechanics]] and [[classical mechanics]] differ in their treatment of this scenario. Classical mechanics predicts that particles that do not have enough energy to classically surmount a barrier will not be able to reach the other side. Thus, a ball without sufficient energy to surmount the hill would roll back down. Or, lacking the energy to penetrate a wall, it would bounce back (reflection) or in the extreme case, bury itself inside the wall (absorption). In quantum mechanics, these particles can, with a very small probability, "tunnel" to the other side, thus crossing the barrier. Here, the "ball" could, in a sense, "borrow" energy from its surroundings to tunnel through the wall or "roll over the hill", paying it back by making the reflected electrons more energetic than they otherwise would have been.
The reason for this difference comes from the treatment of matter in quantum mechanics as [[wave–particle duality|having properties of waves and particles]]. One interpretation of this duality involves the [[Heisenberg uncertainty principle]], which defines a limit on how precisely the position and the [[momentum]] of a particle can be known at the same time. This implies that there are no solutions with a probability of exactly zero (or one), though a solution may approach infinity if, for example, the calculation for its position was taken as a probability of 1, the other, i.e. its speed, would have to be infinity. Hence, the probability of a given particle's existence on the opposite side of an intervening barrier is non-zero, and such particles will appear on the 'other' (a semantically difficult word in this instance) side with a relative frequency proportional to this probability.
[[File:EffetTunnel.gif|250px|right|thumb|An electron [[wavepacket]] directed at a potential barrier. Note the dim spot on the right that represents tunnelling electrons.]]
[[File:Wigner function for tunnelling.ogv|250px|thumb|Quantum tunnelling in the [[phase space formulation]] of quantum mechanics. [[Wigner quasiprobability distribution|Wigner function]] for tunnelling through the potential barrier formula_1 in atomic units (a.u.). The solid lines represent the [[level set]] of the [[Hamiltonian mechanics|Hamiltonian]] formula_2.]]
The tunnelling problem.
The [[wave function]] of a particle summarises everything that can be known about a [[physical system]]. Therefore, problems in quantum mechanics center around the analysis of the wave function for a system. Using mathematical formulations of quantum mechanics, such as the [[Schrödinger equation]], the wave function can be solved. This is directly related to the probability density of the particle's position, which describes the probability that the particle is at any given place. In the limit of large barriers, the probability of tunnelling decreases for taller and wider barriers.
For simple tunnelling-barrier models, such as the [[rectangular potential barrier|rectangular barrier]], an analytic solution exists. Problems in real life often do not have one, so "semiclassical" or "quasiclassical" methods have been developed to give approximate solutions to these problems, like the [[WKB approximation]]. Probabilities may be derived with arbitrary precision, constrained by computational resources, via [[Feynman]]'s [[Path integral formulation|path integral]] method; such precision is seldom required in engineering practice.
Related phenomena.
There are several phenomena that have the same behaviour as quantum tunnelling, and thus can be accurately described by tunnelling. Examples include the tunnelling of a classical wave-particle association, [[evanescent wave coupling]] (the application of [[Maxwell's equations|Maxwell's wave-equation]] to [[light]]) and the application of the [[wave equation|non-dispersive wave-equation]] from [[acoustics]] applied to [[Wave#Waves on strings|"waves on strings"]]. Evanescent wave coupling, until recently, was only called "tunnelling" in quantum mechanics; now it is used in other contexts.
These effects are modelled similarly to the [[rectangular potential barrier]]. In these cases, there is one [[transmission medium]] through which the [[wave propagation|wave propagates]] that is the same or nearly the same throughout, and a second medium through which the wave travels differently. This can be described as a thin region of medium B between two regions of medium A. The analysis of a [[Rectangular potential barrier|rectangular barrier]] by means of the Schrödinger equation can be adapted to these other effects provided that the wave equation has [[travelling wave]] solutions in medium A but real [[exponential function|exponential]] solutions in medium B.
In [[optics]], medium A is a vacuum while medium B is glass. In acoustics, medium A may be a liquid or gas and medium B a solid. For both cases, medium A is a region of space where the particle's [[total energy]] is greater than its [[potential energy]] and medium B is the potential barrier. These have an incoming wave and resultant waves in both directions. There can be more mediums and barriers, and the barriers need not be discrete; approximations are useful in this case.
Applications.
Tunnelling occurs with barriers of thickness around 1-3 nm and smaller, but is the cause of some important macroscopic physical phenomena. For instance, tunnelling is a source of current leakage in [[very-large-scale integration]] (VLSI) electronics and results in the substantial power drain and heating effects that plague high-speed and mobile technology; it is considered the lower limit on how small computer chips can be made.
Radioactive decay.
Radioactive decay is the process of emission of particles and energy from the unstable nucleus of an atom to form a stable product. This is done via the tunnelling of a particle out of the nucleus (an electron tunnelling into the nucleus is [[electron capture]]). This was the first application of quantum tunnelling and led to the first approximations.
Spontaneous DNA mutation.
Spontaneous mutation of DNA occurs when normal DNA replication takes place after a particularly significant proton has defied the odds in quantum tunnelling in what is called "proton tunnelling" (quantum biology). A hydrogen bond joins normal base pairs of DNA. There exists a double well potential along a hydrogen bond separated by a potential energy barrier. It is believed that the double well potential is asymmetric with one well deeper than the other so the proton normally rests in the deeper well. For a mutation to occur, the proton must have tunnelled into the shallower of the two potential wells. The movement of the proton from its regular position is called a [[Tautomer|tautomeric transition]]. If DNA replication takes place in this state, the base pairing rule for DNA may be jeopardised causing a mutation. [[Per-Olov Lowdin]] was the first to develop this theory of spontaneous mutation within the double helix (quantum bio). Other instances of quantum tunnelling-induced mutations in biology are believed to be a cause of ageing and cancer.
Cold emission.
[[field electron emission|Cold emission]] of [[electrons]] is relevant to [[semiconductor]]s and [[superconductor]] physics. It is similar to [[thermionic emission]], where electrons randomly jump from the surface of a metal to follow a voltage bias because they statistically end up with more energy than the barrier, through random collisions with other particles. When the electric field is very large, the barrier becomes thin enough for electrons to tunnel out of the atomic state, leading to a current that varies approximately exponentially with the electric field. These materials are important for [[flash memory]], vacuum tubes, as well as some electron microscopes.
Tunnel junction.
A simple barrier can be created by separating two conductors with a very thin insulator. These are tunnel junctions, the study of which requires quantum tunnelling. [[Josephson junction]]s take advantage of quantum tunnelling and the superconductivity of some [[semiconductors]] to create the [[Josephson effect]]. This has applications in precision measurements of voltages and [[magnetic fields]], as well as the [[multijunction solar cell]].
[[File:Rtd seq v3.gif|thumb|400px|right|A working mechanism of a [[resonant tunnelling diode]] device, based on the phenomenon of quantum tunnelling through the potential barriers.]]
Tunnel diode.
[[Diode]]s are electrical [[semiconductor device]]s that allow [[electric current]] flow in one direction more than the other. The device depends on a [[depletion layer]] between [[N-type semiconductor|N-type]] and [[P-type semiconductor]]s to serve its purpose; when these are very heavily doped the depletion layer can be thin enough for tunnelling. Then, when a small forward bias is applied the current due to tunnelling is significant. This has a maximum at the point where the [[voltage bias]] is such that the energy level of the p and n [[conduction band]]s are the same. As the voltage bias is increased, the two conduction bands no longer line up and the diode acts typically.
Because the tunnelling current drops off rapidly, tunnel diodes can be created that have a range of voltages for which current decreases as voltage is increased. This peculiar property is used in some applications, like high speed devices where the characteristic tunnelling probability changes as rapidly as the bias voltage.
The [[resonant tunnelling diode]] makes use of quantum tunnelling in a very different manner to achieve a similar result. This diode has a resonant voltage for which there is a lot of current that favors a particular voltage, achieved by placing two very thin layers with a high energy conductance band very near each other. This creates a quantum [[potential well]] that have a discrete lowest [[energy level]]. When this energy level is higher than that of the electrons, no tunnelling will occur, and the diode is in reverse bias. Once the two voltage energies align, the electrons flow like an open wire. As the voltage is increased further tunnelling becomes improbable and the diode acts like a normal diode again before a second energy level becomes noticeable.
Tunnel field-effect transistors.
A European research project has demonstrated field effect transistors in which the gate (channel) is controlled via quantum tunnelling rather than by thermal injection, reducing gate voltage from ~1 volt to 0.2 volts and reducing power consumption by up to 100×. If these transistors can be scaled up into [[Vlsi|VLSI chips]], they will significantly improve the performance per power of [[integrated circuit]]s.
Quantum conductivity.
While the [[Drude model]] of [[electrical conductivity]] makes excellent predictions about the nature of electrons conducting in metals, it can be furthered by using quantum tunnelling to explain the nature of the electron's collisions. When a free electron wave packet encounters a long array of uniformly spaced [[potential barrier|barriers]] the reflected part of the wave packet interferes uniformly with the transmitted one between all barriers so that there are cases of 100% transmission. The theory predicts that if positively charged nuclei form a perfectly rectangular array, electrons will tunnel through the metal as free electrons, leading to an extremely high [[Electrical conductance|conductance]], and that impurities in the metal will disrupt it significantly.
Scanning tunnelling microscope.
The scanning tunnelling microscope (STM), invented by [[Gerd Binnig]] and [[Heinrich Rohrer]], allows imaging of individual atoms on the surface of a metal. It operates by taking advantage of the relationship between quantum tunnelling with distance. When the tip of the STM's needle is brought very close to a conduction surface that has a voltage bias, by measuring the current of electrons that are tunnelling between the needle and the surface, the distance between the needle and the surface can be measured. By using [[Piezoelectric sensor|piezoelectric rods]] that change in size when voltage is applied over them the height of the tip can be adjusted to keep the tunnelling current constant. The time-varying voltages that are applied to these rods can be recorded and used to image the surface of the conductor. STMs are accurate to 0.001 nm, or about 1% of atomic diameter.
Faster than light.
It is possible for spin zero particles to travel faster than the [[speed of light]] when tunnelling. This apparently violates the principle of [[causality (physics)|causality]], since there will be a frame of reference in which it arrives before it has left. However, careful analysis of the transmission of the wave packet shows that there is actually no violation of relativity theory. In 1998, [[Francis E. Low]] reviewed briefly the phenomenon of zero time tunnelling. More recently experimental tunnelling time data of [[phonons]], [[photons]], and [[electrons]] have been published by [[Günter Nimtz]].
Mathematical discussions of quantum tunnelling.
The following subsections discuss the mathematical formulations of quantum tunnelling.
The Schrödinger equation.
The [[Schrödinger equation#Time independent equation 2|time-independent Schrödinger equation]] for one particle in one [[dimension]] can be written as
where formula_5 is the reduced [[Planck's constant]], m is the particle mass, x represents distance measured in the direction of motion of the particle, Ψ is the Schrödinger wave function, V is the [[potential energy]] of the particle (measured relative to any convenient reference level), "E" is the energy of the particle that is associated with motion in the x-axis (measured relative to V), and M(x) is a quantity defined by V(x) – E which has no accepted name in physics.
The solutions of the Schrödinger equation take different forms for different values of x, depending on whether M(x) is positive or negative. When M(x) is constant and negative, then the Schrödinger equation can be written in the form
The solutions of this equation represent traveling waves, with phase-constant +"k" or -"k".
Alternatively, if M(x) is constant and positive, then the Schrödinger equation can be written in the form
The solutions of this equation are rising and falling exponentials in the form of [[evanescent wave]]s.
When M(x) varies with position, the same difference in behaviour occurs, depending on whether M(x) is negative or positive. It follows that the sign of M(x) determines the nature of the medium, with positive M(x) corresponding to medium A as described above and negative M(x) corresponding to medium B. It thus follows that evanescent wave coupling can occur if a region of positive M(x) is sandwiched between two regions of negative M(x), hence creating a potential barrier.
The mathematics of dealing with the situation where M(x) varies with x is difficult, except in special cases that usually do not correspond to physical reality. A discussion of the semi-classical approximate method, as found in physics textbooks, is given in the next section. A full and complicated mathematical treatment appears in the 1965 monograph by Fröman and Fröman noted below. Their ideas have not been incorporated into physics textbooks, but their corrections have little quantitative effect.
The WKB approximation.
The wave function is expressed as the exponential of a function:
formula_10 is then separated into real and imaginary parts:
Substituting the second equation into the first and using the fact that the imaginary part needs to be 0 results in:
To solve this equation using the semiclassical approximation, each function must be expanded as a [[power series]] in formula_13. From the equations, the power series must start with at least an order of formula_14 to satisfy the real part of the equation; for a good classical limit starting with the highest power of [[Planck's constant]] possible is preferable, which leads to
and
with the following constraints on the lowest order terms,
and
At this point two extreme cases can be considered.
Case 1
If the amplitude varies slowly as compared to the phase formula_19 and
Case 2
In both cases it is apparent from the denominator that both these approximate solutions are bad near the classical turning points formula_25. Away from the potential hill, the particle acts similar to a free and oscillating wave; beneath the potential hill, the particle undergoes exponential changes in amplitude. By considering the behaviour at these limits and classical turning points a global solution can be made.
To start, choose a classical turning point, formula_26 and expand formula_27 in a power series about formula_26:
Keeping only the first order term ensures linearity:
Using this approximation, the equation near formula_26 becomes a [[differential equation]]:
This can be solved using [[Airy function]]s as solutions.
Taking these solutions for all classical turning points, a global solution can be formed that links the limiting solutions. Given the 2 coefficients on one side of a classical turning point, the 2 coefficients on the other side of a classical turning point can be determined by using this local solution to connect them.
Hence, the Airy function solutions will asymptote into sine, cosine and exponential functions in the proper limits. The relationships between formula_34 and formula_35 are
and
With the coefficients found, the global solution can be found. Therefore, the [[transmission coefficient (physics)|transmission coefficient]] for a particle tunnelling through a single potential barrier is
where formula_39 are the 2 classical turning points for the potential barrier.
For a rectangular barrier, this expression is simplified to:
External links.
[[Category:Particle physics]]
[[Category:Quantum mechanics]]
[[Category:Solid state engineering]]
[[Category:Articles containing video clips]]

</doc>
<doc id="25179" url="https://en.wikipedia.org/wiki?curid=25179" title="Quark">
Quark

A quark ( or ) is an elementary particle and a fundamental constituent of matter. Quarks combine to form composite particles called hadrons, the most stable of which are protons and neutrons, the components of atomic nuclei. Due to a phenomenon known as "color confinement", quarks are never directly observed or found in isolation; they can be found only within hadrons, such as baryons (of which protons and neutrons are examples), and mesons. For this reason, much of what is known about quarks has been drawn from observations of the hadrons themselves.
Quarks have various intrinsic properties, including electric charge, mass, color charge and spin. Quarks are the only elementary particles in the Standard Model of particle physics to experience all four fundamental interactions, also known as "fundamental forces" (electromagnetism, gravitation, strong interaction, and weak interaction), as well as the only known particles whose electric charges are not integer multiples of the elementary charge.
There are six types of quarks, known as "flavors": up, down, strange, charm, top, and bottom. Up and down quarks have the lowest masses of all quarks. The heavier quarks rapidly change into up and down quarks through a process of particle decay: the transformation from a higher mass state to a lower mass state. Because of this, up and down quarks are generally stable and the most common in the universe, whereas strange, charm, bottom, and top quarks can only be produced in high energy collisions (such as those involving cosmic rays and in particle accelerators). For every quark flavor there is a corresponding type of antiparticle, known as an "antiquark", that differs from the quark only in that some of its properties have equal magnitude but opposite sign.
The quark model was independently proposed by physicists Murray Gell-Mann and George Zweig in 1964. Quarks were introduced as parts of an ordering scheme for hadrons, and there was little evidence for their physical existence until deep inelastic scattering experiments at the Stanford Linear Accelerator Center in 1968. Accelerator experiments have provided evidence for all six flavors. The top quark was the last to be discovered at Fermilab in 1995.
Classification.
The Standard Model is the theoretical framework describing all the currently known elementary particles. This model contains six flavors of quarks (), named up (), down (), strange (), charm (), bottom (), and top (). Antiparticles of quarks are called "antiquarks", and are denoted by a bar over the symbol for the corresponding quark, such as for an up antiquark. As with antimatter in general, antiquarks have the same mass, mean lifetime, and spin as their respective quarks, but the electric charge and other charges have the opposite sign.
Quarks are spin- particles, implying that they are fermions according to the spin-statistics theorem. They are subject to the Pauli exclusion principle, which states that no two identical fermions can simultaneously occupy the same quantum state. This is in contrast to bosons (particles with integer spin), any number of which can be in the same state. Unlike leptons, quarks possess color charge, which causes them to engage in the strong interaction. The resulting attraction between different quarks causes the formation of composite particles known as "hadrons" (see "Strong interaction and color charge" below).
The quarks which determine the quantum numbers of hadrons are called "valence quarks"; apart from these, any hadron may contain an indefinite number of virtual (or "sea") quarks, antiquarks, and gluons which do not influence its quantum numbers. There are two families of hadrons: baryons, with three valence quarks, and mesons, with a valence quark and an antiquark. The most common baryons are the proton and the neutron, the building blocks of the atomic nucleus. A great number of hadrons are known (see list of baryons and list of mesons), most of them differentiated by their quark content and the properties these constituent quarks confer. The existence of "exotic" hadrons with more valence quarks, such as tetraquarks () and pentaquarks (), has been conjectured but not proven. However, on 13 July 2015, the LHCb collaboration at CERN reported results consistent with pentaquark states.
Elementary fermions are grouped into three generations, each comprising two leptons and two quarks. The first generation includes up and down quarks, the second strange and charm quarks, and the third bottom and top quarks. All searches for a fourth generation of quarks and other elementary fermions have failed, and there is strong indirect evidence that no more than three generations exist. Particles in higher generations generally have greater mass and less stability, causing them to decay into lower-generation particles by means of weak interactions. Only first-generation (up and down) quarks occur commonly in nature. Heavier quarks can only be created in high-energy collisions (such as in those involving cosmic rays), and decay quickly; however, they are thought to have been present during the first fractions of a second after the Big Bang, when the universe was in an extremely hot and dense phase (the quark epoch). Studies of heavier quarks are conducted in artificially created conditions, such as in particle accelerators.
Having electric charge, mass, color charge, and flavor, quarks are the only known elementary particles that engage in all four fundamental interactions of contemporary physics: electromagnetism, gravitation, strong interaction, and weak interaction. Gravitation is too weak to be relevant to individual particle interactions except at extremes of energy (Planck energy) and distance scales (Planck distance). However, since no successful quantum theory of gravity exists, gravitation is not described by the Standard Model.
See the table of properties below for a more complete overview of the six quark flavors' properties.
History.
The quark model was independently proposed by physicists Murray Gell-Mann
(pictured) and George Zweig in 1964. The proposal came shortly after Gell-Mann's 1961 formulation of a particle classification system known as the "Eightfold Way"—or, in more technical terms, SU(3) flavor symmetry. Physicist Yuval Ne'eman had independently developed a scheme similar to the Eightfold Way in the same year.
At the time of the quark theory's inception, the "particle zoo" included, amongst other particles, a multitude of hadrons. Gell-Mann and Zweig posited that they were not elementary particles, but were instead composed of combinations of quarks and antiquarks. Their model involved three flavors of quarks, up, down, and strange, to which they ascribed properties such as spin and electric charge. The initial reaction of the physics community to the proposal was mixed. There was particular contention about whether the quark was a physical entity or a mere abstraction used to explain concepts that were not fully understood at the time.
In less than a year, extensions to the Gell-Mann–Zweig model were proposed. Sheldon Lee Glashow and James Bjorken predicted the existence of a fourth flavor of quark, which they called "charm". The addition was proposed because it allowed for a better description of the weak interaction (the mechanism that allows quarks to decay), equalized the number of known quarks with the number of known leptons, and implied a mass formula that correctly reproduced the masses of the known mesons.
In 1968, deep inelastic scattering experiments at the Stanford Linear Accelerator Center (SLAC) showed that the proton contained much smaller, point-like objects and was therefore not an elementary particle. Physicists were reluctant to firmly identify these objects with quarks at the time, instead calling them "partons"—a term coined by Richard Feynman. The objects that were observed at SLAC would later be identified as up and down quarks as the other flavors were discovered. Nevertheless, "parton" remains in use as a collective term for the constituents of hadrons (quarks, antiquarks, and gluons).
The strange quark's existence was indirectly validated by SLAC's scattering experiments: not only was it a necessary component of Gell-Mann and Zweig's three-quark model, but it provided an explanation for the kaon () and pion () hadrons discovered in cosmic rays in 1947.
In a 1970 paper, Glashow, John Iliopoulos and Luciano Maiani presented the so-called GIM mechanism to explain the experimental non-observation of flavor-changing neutral currents. This theoretical model required the existence of the as-yet undiscovered charm quark. The number of supposed quark flavors grew to the current six in 1973, when Makoto Kobayashi and Toshihide Maskawa noted that the experimental observation of CP violation could be explained if there were another pair of quarks.
Charm quarks were produced almost simultaneously by two teams in November 1974 (see November Revolution)—one at SLAC under Burton Richter, and one at Brookhaven National Laboratory under Samuel Ting. The charm quarks were observed bound with charm antiquarks in mesons. The two parties had assigned the discovered meson two different symbols, and ; thus, it became formally known as the meson. The discovery finally convinced the physics community of the quark model's validity.
In the following years a number of suggestions appeared for extending the quark model to six quarks. Of these, the 1975 paper by Haim Harari was the first to coin the terms "top" and "bottom" for the additional quarks.
In 1977, the bottom quark was observed by a team at Fermilab led by Leon Lederman. This was a strong indicator of the top quark's existence: without the top quark, the bottom quark would have been without a partner. However, it was not until 1995 that the top quark was finally observed, also by the CDF and DØ teams at Fermilab. It had a mass much larger than had been previously expected, almost as large as that of a gold atom.
Etymology.
For some time, Gell-Mann was undecided on an actual spelling for the term he intended to coin, until he found the word "quark" in James Joyce's book "Finnegans Wake":
Gell-Mann went into further detail regarding the name of the quark in his book "The Quark and the Jaguar":
Zweig preferred the name "ace" for the particle he had theorized, but Gell-Mann's terminology came to prominence once the quark model had been commonly accepted.
The quark flavors were given their names for several reasons. The up and down quarks are named after the up and down components of isospin, which they carry. Strange quarks were given their name because they were discovered to be components of the strange particles discovered in cosmic rays years before the quark model was proposed; these particles were deemed "strange" because they had unusually long lifetimes. Glashow, who coproposed charm quark with Bjorken, is quoted as saying, "We called our construct the 'charmed quark', for we were fascinated and pleased by the symmetry it brought to the subnuclear world." The names "bottom" and "top", coined by Harari, were chosen because they are "logical partners for up and down quarks". In the past, bottom and top quarks were sometimes referred to as "beauty" and "truth" respectively, but these names have somewhat fallen out of use. While "truth" never did catch on, accelerator complexes devoted to massive production of bottom quarks are sometimes called "beauty factories".
Properties.
Electric charge.
Quarks have fractional electric charge values – either or times the elementary charge (e), depending on flavor. Up, charm, and top quarks (collectively referred to as "up-type quarks") have a charge of + e, while down, strange, and bottom quarks ("down-type quarks") have − e. Antiquarks have the opposite charge to their corresponding quarks; up-type antiquarks have charges of − e and down-type antiquarks have charges of + e. Since the electric charge of a hadron is the sum of the charges of the constituent quarks, all hadrons have integer charges: the combination of three quarks (baryons), three antiquarks (antibaryons), or a quark and an antiquark (mesons) always results in integer charges. For example, the hadron constituents of atomic nuclei, neutrons and protons, have charges of 0 e and +1 e respectively; the neutron is composed of two down quarks and one up quark, and the proton of two up quarks and one down quark.
Spin.
Spin is an intrinsic property of elementary particles, and its direction is an important degree of freedom. It is sometimes visualized as the rotation of an object around its own axis (hence the name ""), though this notion is somewhat misguided at subatomic scales because elementary particles are believed to be point-like.
Spin can be represented by a vector whose length is measured in units of the reduced Planck constant "ħ" (pronounced "h bar"). For quarks, a measurement of the spin vector component along any axis can only yield the values +"ħ"/2 or −"ħ"/2; for this reason quarks are classified as spin- particles. The component of spin along a given axis – by convention the "z" axis – is often denoted by an up arrow ↑ for the value + and down arrow ↓ for the value −, placed after the symbol for flavor. For example, an up quark with a spin of + along the "z" axis is denoted by u↑.
Weak interaction.
A quark of one flavor can transform into a quark of another flavor only through the weak interaction, one of the four fundamental interactions in particle physics. By absorbing or emitting a W boson, any up-type quark (up, charm, and top quarks) can change into any down-type quark (down, strange, and bottom quarks) and vice versa. This flavor transformation mechanism causes the radioactive process of beta decay, in which a neutron () "splits" into a proton (), an electron () and an electron antineutrino () (see picture). This occurs when one of the down quarks in the neutron () decays into an up quark by emitting a virtual boson, transforming the neutron into a proton (). The boson then decays into an electron and an electron antineutrino.
Both beta decay and the inverse process of "inverse beta decay" are routinely used in medical applications such as positron emission tomography (PET) and in experiments involving neutrino detection.
While the process of flavor transformation is the same for all quarks, each quark has a preference to transform into the quark of its own generation. The relative tendencies of all flavor transformations are described by a mathematical table, called the Cabibbo–Kobayashi–Maskawa matrix (CKM matrix). Enforcing unitarity, the approximate magnitudes of the entries of the CKM matrix are:
where "V""ij" represents the tendency of a quark of flavor "i" to change into a quark of flavor "j" (or vice versa).
There exists an equivalent weak interaction matrix for leptons (right side of the W boson on the above beta decay diagram), called the Pontecorvo–Maki–Nakagawa–Sakata matrix (PMNS matrix). Together, the CKM and PMNS matrices describe all flavor transformations, but the links between the two are not yet clear.
Strong interaction and color charge.
According to quantum chromodynamics (QCD), quarks possess a property called "color charge". There are three types of color charge, arbitrarily labeled "blue", "green", and "red". Each of them is complemented by an anticolor – "antiblue", "antigreen", and "antired". Every quark carries a color, while every antiquark carries an anticolor.
The system of attraction and repulsion between quarks charged with different combinations of the three colors is called strong interaction, which is mediated by force carrying particles known as "gluons"; this is discussed at length below. The theory that describes strong interactions is called quantum chromodynamics (QCD). A quark, which will have a single color value, can form a bound system with an antiquark carrying the corresponding anticolor. The result of two attracting quarks will be color neutrality: a quark with color charge "ξ" plus an antiquark with color charge −"ξ" will result in a color charge of 0 (or "white" color) and the formation of a meson. This is analogous to the additive color model in basic optics. Similarly, the combination of three quarks, each with different color charges, or three antiquarks, each with anticolor charges, will result in the same "white" color charge and the formation of a baryon or antibaryon.
In modern particle physics, gauge symmetries – a kind of symmetry group – relate interactions between particles (see gauge theories). Color SU(3) (commonly abbreviated to SU(3)c) is the gauge symmetry that relates the color charge in quarks and is the defining symmetry for quantum chromodynamics. Just as the laws of physics are independent of which directions in space are designated "x", "y", and "z", and remain unchanged if the coordinate axes are rotated to a new orientation, the physics of quantum chromodynamics is independent of which directions in three-dimensional color space are identified as blue, red, and green. SU(3)c color transformations correspond to "rotations" in color space (which, mathematically speaking, is a complex space). Every quark flavor "f", each with subtypes "f"B, "f"G, "f"R corresponding to the quark colors, forms a triplet: a three-component quantum field which transforms under the fundamental representation of SU(3)c. The requirement that SU(3)c should be local – that is, that its transformations be allowed to vary with space and time – determines the properties of the strong interaction, in particular the existence of eight gluon types to act as its force carriers.
Mass.
Two terms are used in referring to a quark's mass: "current quark mass" refers to the mass of a quark by itself, while "constituent quark mass" refers to the current quark mass plus the mass of the gluon particle field surrounding the quark. These masses typically have very different values. Most of a hadron's mass comes from the gluons that bind the constituent quarks together, rather than from the quarks themselves. While gluons are inherently massless, they possess energy – more specifically, quantum chromodynamics binding energy (QCBE) – and it is this that contributes so greatly to the overall mass of the hadron (see mass in special relativity). For example, a proton has a mass of approximately 938 MeV/c2, of which the rest mass of its three valence quarks only contributes about 11 MeV/c2; much of the remainder can be attributed to the gluons' QCBE.
The Standard Model posits that elementary particles derive their masses from the Higgs mechanism, which is related to the Higgs boson. Physicists hope that further research into the reasons for the top quark's large mass of ~173 GeV/c2, almost the mass of a gold atom, might reveal more about the origin of the mass of quarks and other elementary particles.
Table of properties.
The following table summarizes the key properties of the six quarks. Flavor quantum numbers (isospin ("I"3), charm ("C"), strangeness ("S", not to be confused with spin), topness ("T"), and bottomness ("B"′)) are assigned to certain quark flavors, and denote qualities of quark-based systems and hadrons. The baryon number ("B") is + for all quarks, as baryons are made of three quarks. For antiquarks, the electric charge ("Q") and all flavor quantum numbers ("B", "I"3, "C", "S", "T", and "B"′) are of opposite sign. Mass and total angular momentum ("J"; equal to spin for point particles) do not change sign for the antiquarks.
Interacting quarks.
As described by quantum chromodynamics, the strong interaction between quarks is mediated by gluons, massless vector gauge bosons. Each gluon carries one color charge and one anticolor charge. In the standard framework of particle interactions (part of a more general formulation known as perturbation theory), gluons are constantly exchanged between quarks through a virtual emission and absorption process. When a gluon is transferred between quarks, a color change occurs in both; for example, if a red quark emits a red–antigreen gluon, it becomes green, and if a green quark absorbs a red–antigreen gluon, it becomes red. Therefore, while each quark's color constantly changes, their strong interaction is preserved.
Since gluons carry color charge, they themselves are able to emit and absorb other gluons. This causes "asymptotic freedom": as quarks come closer to each other, the chromodynamic binding force between them weakens. Conversely, as the distance between quarks increases, the binding force strengthens. The color field becomes stressed, much as an elastic band is stressed when stretched, and more gluons of appropriate color are spontaneously created to strengthen the field. Above a certain energy threshold, pairs of quarks and antiquarks are created. These pairs bind with the quarks being separated, causing new hadrons to form. This phenomenon is known as "color confinement": quarks never appear in isolation. This process of hadronization occurs before quarks, formed in a high energy collision, are able to interact in any other way. The only exception is the top quark, which may decay before it hadronizes.
Sea quarks.
Hadrons, along with the "valence quarks" () that contribute to their quantum numbers, contain virtual quark–antiquark () pairs known as "sea quarks" (). Sea quarks form when a gluon of the hadron's color field splits; this process also works in reverse in that the annihilation of two sea quarks produces a gluon. The result is a constant flux of gluon splits and creations colloquially known as "the sea". Sea quarks are much less stable than their valence counterparts, and they typically annihilate each other within the interior of the hadron. Despite this, sea quarks can hadronize into baryonic or mesonic particles under certain circumstances.
Other phases of quark matter.
Under sufficiently extreme conditions, quarks may become deconfined and exist as free particles. In the course of asymptotic freedom, the strong interaction becomes weaker at higher temperatures. Eventually, color confinement would be lost and an extremely hot plasma of freely moving quarks and gluons would be formed. This theoretical phase of matter is called quark–gluon plasma. The exact conditions needed to give rise to this state are unknown and have been the subject of a great deal of speculation and experimentation. A recent estimate puts the needed temperature at kelvin. While a state of entirely free quarks and gluons has never been achieved (despite numerous attempts by CERN in the 1980s and 1990s), recent experiments at the Relativistic Heavy Ion Collider have yielded evidence for liquid-like quark matter exhibiting "nearly perfect" fluid motion.
The quark–gluon plasma would be characterized by a great increase in the number of heavier quark pairs in relation to the number of up and down quark pairs. It is believed that in the period prior to 10−6 seconds after the Big Bang (the quark epoch), the universe was filled with quark–gluon plasma, as the temperature was too high for hadrons to be stable.
Given sufficiently high baryon densities and relatively low temperatures – possibly comparable to those found in neutron stars – quark matter is expected to degenerate into a Fermi liquid of weakly interacting quarks. This liquid would be characterized by a condensation of colored quark Cooper pairs, thereby breaking the local SU(3)c symmetry. Because quark Cooper pairs harbor color charge, such a phase of quark matter would be color superconductive; that is, color charge would be able to pass through it with no resistance.

</doc>
<doc id="2008215" url="https://en.wikipedia.org/wiki?curid=2008215" title="Quark model">
Quark model

In particle physics, the quark model is a classification scheme for hadrons in terms of their valence quarks—the quarks and antiquarks which give rise to the quantum numbers of the hadrons. The quark model underlies "flavor SU(3)", or the "Eightfold Way", the successful classification scheme organizing the large number of lighter hadrons that were being discovered starting in the 1950s and continuing through the 1960s. It received experimental verification beginning in the late 1960s and is a valid effective classification of them to date. The quark model was independently proposed by physicists Murray Gell-Mann, and
George Zweig (also see ) in 1964. Today, the model has essentially been absorbed as a component of the established quantum field theory of strong and electroweak particle interactions, dubbed the Standard Model.
Hadrons are not really "elementary", and can be regarded as bound states of their "valence quarks" and antiquarks, which give rise to the quantum numbers of the hadrons. These quantum numbers are labels identifying the hadrons, and are of two kinds. One set comes from the Poincaré symmetry—"J""PC", where "J", "P" and "C" stand for the total angular momentum, P-symmetry, and C-symmetry, respectively.
The remaining are flavor quantum numbers such as the isospin, strangeness, charm, and so on. The strong interactions binding the quarks together are insensitive to these quantum numbers, so variation of them leads to systematic mass and coupling relationships among the hadrons in the same flavor multiplet.
All quarks are assigned a baryon number of ⅓. Up, charm and top quarks have an electric charge of +⅔, while the down, strange, and bottom quarks have an electric charge of −⅓. Antiquarks have the opposite quantum numbers. Quarks are spin-½ particles, and thus fermions. Each quark or antiquark obeys the Gell-Mann−Nishijima formula individually, so any additive assembly of them will as well.
Mesons are made of a valence quark−antiquark pair (thus have a baryon number of 0), while baryons are made of three quarks (thus have a baryon number of 1). This article discusses the quark model for the up, down, and strange flavors of quark (which form an approximate flavor SU(3) symmetry). There are generalizations to larger number of flavors.
History.
Developing classification schemes for hadrons became a timely question after new experimental techniques uncovered so many of them, that it became clear that they could not all be elementary. These discoveries led Wolfgang Pauli to exclaim "Had I foreseen that, I would have gone into botany," and Enrico Fermi to advise his student Leon Lederman: "Young man, if I could remember the names of these particles, I would have been a botanist." These new schemes earned Nobel prizes for experimental particle physicists, including Luis Alvarez, who was at the forefront of many of these developments. Constructing hadrons as bound states of fewer constituents would thus organize the "zoo" at hand. Several early proposals, such as the ones by
Enrico Fermi and Chen-Ning Yang (1949), and by Shoichi Sakata (1956), ended up satisfactorily covering the mesons, but failed with baryons, and so were unable to explain all the data.
The Gell-Mann–Nishijima formula, developed by Murray Gell-Mann and Kazuhiko Nishijima, led to the Eightfold way classification, invented by Gell-Mann, with important independent contributions from Yuval Ne'eman, in 1961. The hadrons were organized into SU(3) representation multiplets, octets and decuplets, of roughly the same mass, due to the strong interactions; and smaller mass differences linked to the flavor quantum numbers, invisible to the strong interactions. The Gell-Mann–Okubo mass formula systematized the quantification of these small mass differences among members of a hadronic multiplet, controlled by the explicit symmetry breaking of SU(3).
The spin- baryon, a member of the ground-state decuplet, was a crucial prediction of that classification. After it was discovered in an experiment at Brookhaven National Laboratory, Gell-Mann received a Nobel prize in physics for his work on the Eightfold Way, in 1969.
Finally, in 1964, Gell-Mann, and, independently, George Zweig, discerned what the Eightfold Way picture encodes. They posited elementary fermionic constituents, unobserved, and possibly unobservable in a free form, underlying and elegantly encoding the Eightfold Way classification, in an economical, tight structure, resulting in further simplicity. Hadronic mass differences were now linked to the different masses of the constituent quarks.
It would take about a decade for the unexpected nature—and physical reality—of these quarks to be appreciated more fully (See Quarks). Counter-intuitively, they cannot ever be observed in isolation (color confinement), but instead always combine with other quarks to form full hadrons, which then furnish ample indirect information on the trapped quarks themselves. Conversely, the quarks serve in the definition of Quantum chromodynamics, the fundamental theory fully describing the strong interactions; and the Eightfold Way is now understood to be a consequence of the flavor symmetry structure of the lightest three of them. To date, no Nobel prize has been awarded to Gell-Mann and Zweig for this discovery.
Mesons.
The Eightfold Way classification is named after the following fact. If we take three flavors of quarks, then the quarks lie in the fundamental representation, 3 (called the triplet) of flavor SU(3). The antiquarks lie in the complex conjugate representation . The nine states (nonet) made out of a pair can be decomposed into the trivial representation, 1 (called the singlet), and the adjoint representation, 8 (called the octet). The notation for this decomposition is
Figure 1 shows the application of this decomposition to the mesons. If the flavor symmetry were exact (as in the limit that only the strong interactions operate, but the electroweak interactions are notionally switched off), then all nine mesons would have the same mass. However, the physical content of the full theory includes consideration of the symmetry breaking induced by the quark mass differences, and considerations of mixing between various multiplets (such as the octet and the singlet).
N.B. Nevertheless, the mass splitting between the and the is larger than the quark model can accommodate, and this "– puzzle" has its origin in topological peculiarities of the strong interaction vacuum, such as instanton configurations.
Mesons are hadrons with zero baryon number. If the quark–antiquark pair are in an orbital angular momentum state, and have spin , then
If "P" = (−1)"J", then it follows that "S" = 1, thus "PC"= 1. States with these quantum numbers are called "natural parity states"; while all other quantum numbers are thus called "exotic" (for example the state ).
Baryons.
 
Since quarks are fermions, the spin-statistics theorem implies that the wavefunction of a baryon must be antisymmetric under exchange of any two quarks. This antisymmetric wavefunction is obtained by making it fully antisymmetric in color, discussed below, and symmetric in flavor, spin and space put together. With three flavors, the decomposition in flavor is
The decuplet is symmetric in flavor, the singlet antisymmetric and the two octets have mixed symmetry. The space and spin parts of the states are thereby fixed once the orbital angular momentum is given.
It is sometimes useful to think of the basis states of quarks as the six states of three flavors and two spins per flavor. This approximate symmetry is called spin-flavor SU(6). In terms of this, the decomposition is
The 56 states with symmetric combination of spin and flavour decompose under flavor SU(3) into
where the superscript denotes the spin, "S", of the baryon. Since these states are symmetric in spin and flavor, they should also be symmetric in space—a condition that is easily satisfied by making the orbital angular momentum "L" = 0. These are the ground state baryons.
The "S" =  octet baryons are the two nucleons (, ), the three Sigmas (, , ), the two Xis (, ), and the Lambda (). The "S" =  decuplet baryons are the four Deltas (, , , ), three Sigmas (, , ), two Xis (, ), and the Omega ().
Mixing of baryons, mass splittings within and between multiplets, and magnetic moments are some of the other questions that the model predicts successfully.
The discovery of color.
Color quantum numbers are the characteristic charges of the strong force, and are completely uninvolved in electroweak interactions. They were discovered as a consequence of the quark model classification, when it was appreciated that the spin "S" =  baryon, the , required three up quarks with parallel spins and vanishing orbital angular momentum. Therefore, it could not have an antisymmetric wave function, (due to the Pauli exclusion principle), "unless there were a hidden quantum number". Oscar Greenberg noted this problem in 1964, suggesting that quarks should be para-fermions.
Instead, six months later, Moo-Young Han and Yoichiro Nambu suggested the existence of three triplets of quarks to solve this problem, but flavor and color intertwined in that model--- they did not commute.
The modern concept of color completely commuting with all other charges and providing the strong force charge was articulated in 1973, by William Bardeen, ,
and Murray Gell-Mann.
States outside the quark model.
While the quark model is derivable from the theory of quantum chromodynamics, the structure of hadrons is more complicated than this model allows. The full quantum mechanical wave function of any hadron must include virtual quark pairs as well as virtual gluons, and allows for a variety of mixings. There may be hadrons which lie outside the quark model. Among these are the "glueballs" (which contain only valence gluons), "hybrids" (which contain valence quarks as well as gluons) and "exotic hadrons" (such as tetraquarks or pentaquarks).

</doc>
<doc id="1685182" url="https://en.wikipedia.org/wiki?curid=1685182" title="Quarkonium">
Quarkonium

In particle physics, quarkonium (from quark + onium, pl. quarkonia) designates a flavorless meson whose constituents are a quark and its own antiquark. Examples of quarkonia are the J/ψ meson (an example of charmonium, ) and the meson (bottomonium, ). Because of the high mass of the top quark, toponium does not exist, since the top quark decays through the electroweak interaction before a bound state can form. Usually quarkonium refers only to charmonium and bottomonium, and not to any of the lighter quark–antiquark states. This usage is because the lighter quarks (up, down, and strange) are much less massive than the heavier quarks, and so the physical states actually seen in experiments (η, η′, and π0 mesons) are quantum mechanical mixtures of the light quark states. The much larger mass differences between the charm and bottom quarks and the lighter quarks results in states that are well defined in terms of a quark–antiquark pair of a given flavor.
Charmonium states.
In the following table, the same particle can be named with the spectroscopic notation or with its mass. In some cases excitation series are used: Ψ' is the first excitation of Ψ (for historical reasons, this one is called "J/ψ" particle); Ψ" is a second excitation, and so on. That is, names in the same cell are synonymous.
Some of the states are predicted, but have not been identified; others are unconfirmed. The quantum numbers of the X(3872) particle have been measured recently by the LHCb experiment at CERN
. This measurement shed some light on its identity, excluding the third option among the three envised, which are :
In 2005, the BaBar experiment announced the discovery of a new state: Y(4260). CLEO and Belle have since corroborated these observations. At first, Y(4260) was thought to be a charmonium state, but the evidence suggests more exotic explanations, such as a D "molecule", a 4-quark construct, or a hybrid meson.
Bottomonium states.
In the following table, the same particle can be named with the spectroscopic notation or with its mass.
Some of the states are predicted, but have not been identified; others are unconfirmed.
The χb (3P) state was the first particle discovered in the Large Hadron Collider. The article about this discovery was first submitted to arXiv on 21 December 2011. On April 2012, Tevatron's DØ experiment confirms the result in a paper published in "Phys. Rev. D".
QCD and quarkonia.
The computation of the properties of mesons in Quantum chromodynamics (QCD) is a fully non-perturbative one. As a result, the only general method available is a direct computation using lattice QCD (LQCD) techniques. However, other techniques are effective for heavy quarkonia as well.
The light quarks in a meson move at relativistic speeds, since the mass of the bound state is much larger than the mass of the quark. However, the speed of the charm and the bottom quarks in their respective quarkonia is sufficiently smaller, so that relativistic effects affect these states much less. It is estimated that the speed, v, is roughly 0.3 times the speed of light for charmonia and roughly 0.1 times the speed of light for bottomonia. The computation can then be approximated by an expansion in powers of v/c and v2/c2. This technique is called non-relativistic QCD (NRQCD).
NRQCD has also been quantized as a lattice gauge theory, which provides another technique for LQCD calculations to use. Good agreement with the bottomonium masses has been found, and this provides one of the best non-perturbative tests of LQCD. For charmonium masses the agreement is not as good, but the LQCD community is actively working on improving their techniques. Work is also being done on calculations of such properties as widths of quarkonia states and transition rates between the states.
An early, but still effective, technique uses models of the "effective" potential to calculate masses of quarkonia states. In this technique, one uses the fact that the motion of the quarks that comprise the quarkonium state is non-relativistic to assume that they move in a static potential, much like non-relativistic models of the hydrogen atom. One of the most popular potential models is the so-called "Cornell potential"
where formula_3 is the effective radius of the quarkonium state, formula_4 and formula_5 are parameters. This potential has two parts. The first part, formula_6 corresponds to the potential induced by one-gluon exchange between the quark and its anti-quark, and is known as the "Coulombic" part of the potential, since its formula_7 form is identical to the well-known Coulombic potential induced by the electromagnetic force. The second part, formula_8, is known as the "confinement" part of the potential, and parameterizes the poorly understood non-perturbative effects of QCD. Generally, when using this approach, a convenient form for the wave function of the quarks is taken, and then formula_4 and formula_5 are determined by fitting the results of the calculations to the masses of well-measured quarkonium states. Relativistic and other effects can be incorporated into this approach by adding extra terms to the potential, much in the same way that they are for the hydrogen atom in non-relativistic quantum mechanics. This form has been derived from QCD up to formula_11 by Y. Sumino in 2003. It is popular because it allows for accurate predictions of quarkonia parameters without a lengthy lattice computation, and provides a separation between the short-distance "Coulombic" effects and the long-distance "confinement" effects that can be useful in understanding the quark/anti-quark force generated by QCD.
Quarkonia have been suggested as a diagnostic tool of the formation of the quark–gluon plasma: both disappearance and enhancement of their formation depending on the yield of heavy quarks in plasma can occur.

</doc>
<doc id="153130" url="https://en.wikipedia.org/wiki?curid=153130" title="Quaternion group">
Quaternion group

In group theory, the quaternion group is a non-abelian group of order eight, isomorphic to a certain eight-element subset of the quaternions under multiplication. It is often denoted by Q or Q8, and is given by the group presentation
where 1 is the identity element and −1 commutes with the other elements of the group.
Compared to dihedral group.
The Q8 group has the same order as the dihedral group D4, but a different structure, as shown by their Cayley and cycle graphs:
The dihedral group D4 arises in the split-quaternions in the same way that Q8 lies in the quaternions.
Cayley table.
The Cayley table (multiplication table) for Q is given by:
The multiplication of the six imaginary units {±"i", ±"j", ±"k"} works like the cross product of unit vectors in three-dimensional Euclidean space.
Properties.
The quaternion group has the unusual property of being Hamiltonian: every subgroup of Q is a normal subgroup, but the group is non-abelian. Every Hamiltonian group contains a copy of Q.
In abstract algebra, one can construct a real four-dimensional vector space with basis {1, "i", "j", "k"} and turn it into an associative algebra by using the above multiplication table and distributivity. The result is a skew field called the quaternions. Note that this is not quite the same as the group algebra on Q (which would be eight-dimensional). Conversely, one can start with the quaternions and "define" the quaternion group as the multiplicative subgroup consisting of the eight elements {1, −1, "i", −"i", "j", −"j", "k", −"k"}. The complex four-dimensional vector space on the same basis is called the algebra of biquaternions.
Note that "i", "j", and "k" all have order four in Q and any two of them generate the entire group. Another presentation of Q demonstrating this is:
One may take, for instance, "i" = "x", "j" = "y" and "k" = "x y".
The center and the commutator subgroup of Q is the subgroup {±1}. The factor group Q/{±1} is isomorphic to the Klein four-group V. The inner automorphism group of Q is isomorphic to Q modulo its center, and is therefore also isomorphic to the Klein four-group. The full automorphism group of Q is isomorphic to S4, the symmetric group on four letters. The outer automorphism group of Q is then S4/V which is isomorphic to S3.
Matrix representations.
The quaternion group can be represented as a subgroup of the general linear group GL2(C). A representation
is given by
Since all of the above matrices have unit determinant, this is a representation of Q in the special linear group SL2(C). The standard identities for quaternion multiplication can be verified using the usual laws of matrix multiplication in GL2(C).
There is also an important action of Q on the eight nonzero elements of the 2-dimensional vector space over the finite field F3. A representation
is given by
where {−1,0,1} are the three elements of F3. Since all of the above matrices have unit determinant over F3, this is a representation of Q in the special linear group SL(2, 3). Indeed, the group SL(2, 3) has order 24, and Q is a normal subgroup of SL(2, 3) of index 3.
Galois group.
As Richard Dean showed in 1981, the quaternion group can be presented as the Galois group Gal("T"/Q) where Q is the field of rational numbers and "T" is the splitting field, over Q, of the polynomial
The development uses the fundamental theorem of Galois theory in specifying four intermediate fields between Q and "T" and their Galois groups, as well as two theorems on cyclic extension of degree four over a field.
Generalized quaternion group.
A group is called a generalized quaternion group when its order is a power of 2 and it is dicyclic group. 
It is a part of more general class of dicyclic groups.
Some authors define generalized quaternion group to be the same as dicyclic group.
for some integer . This group is denoted Q4"n" and has order 4"n". Coxeter labels these "dicyclic groups" <2,2,"n">, being a special case of the binary polyhedral group <"l","m","n"> and related to the polyhedral groups (p,q,r), and dihedral group (2,2,n). The usual quaternion group corresponds to the case . The generalized quaternion group can be realized as the subgroup of GL2(C) generated by
where ω"n" = eiπ/"n". It can also be realized as the subgroup of unit quaternions generated by and .
The generalized quaternion groups have the property that every abelian subgroup is cyclic. It can be shown that a finite "p"-group with this property (every abelian subgroup is cyclic) is either cyclic or a generalized quaternion group as defined above. Another characterization is that a finite "p"-group in which there is a unique subgroup of order "p" is either cyclic or a 2-group isomorphic to generalized quaternion group. In particular, for a finite field "F" with odd characteristic, the 2-Sylow subgroup of SL2("F") is non-abelian and has only one subgroup of order 2, so this 2-Sylow subgroup must be a generalized quaternion group, . Letting "pr" be the size of "F", where "p" is prime, the size of the 2-Sylow subgroup of SL2("F") is 2"n", where .
The Brauer–Suzuki theorem shows that groups whose Sylow 2-subgroups are generalized quaternion cannot be simple.

</doc>
<doc id="26003" url="https://en.wikipedia.org/wiki?curid=26003" title="Radian">
Radian

The radian is the standard unit of angular measure, used in many areas of mathematics. An angle's measurement in radians is numerically equal to the length of a corresponding arc of a unit circle; one radian is just under 57.3 degrees (when the arc length is equal to the radius). The unit was formerly an SI supplementary unit, but this category was abolished in 1995 and the radian is now considered an SI derived unit.
Separately, the SI unit of solid angle measurement is the steradian.
The radian is represented by the symbol rad (Unicode-encoded as ). An alternative symbol is c, the superscript letter c, for "circular measure", or the letter r, but both of those symbols are infrequently used as it can be easily mistaken for a degree symbol (°) or a radius (r). So for example, a value of 1.2 radians could be written as 1.2 rad, 1.2 r, 1.2, or 1.2.
Definition.
Radian describes the plane angle subtended by a circular arc as the length of the arc divided by the radius of the arc. One radian is the angle subtended at the center of a circle by an arc that is equal in length to the radius of the circle. More generally, the magnitude in radians of such a subtended angle is equal to the ratio of the arc length to the radius of the circle; that is, "θ" = "s" /"r", where "θ" is the subtended angle in radians, "s" is arc length, and "r" is radius. Conversely, the length of the enclosed arc is equal to the radius multiplied by the magnitude of the angle in radians; that is, "s" = "rθ".
As the ratio of two lengths, the radian is a "pure number" that needs no unit symbol, and in mathematical writing the symbol "rad" is almost always omitted. When quantifying an angle in the absence of any symbol, radians are assumed, and when degrees are meant the symbol ° is used.
It follows that the magnitude in radians of one complete revolution (360 degrees) is the length of the entire circumference divided by the radius, or 2π"r" /"r", or 2π. Thus 2π radians is equal to 360 degrees, meaning that one radian is equal to 180/π degrees.
History.
The concept of radian measure, as opposed to the degree of an angle, is normally credited to Roger Cotes in 1714. He described the radian in everything but name, and he recognized its naturalness as a unit of angular measure. The idea of measuring angles by the length of the arc was already in use by other mathematicians. For example, al-Kashi (c. 1400) used so-called "diameter parts" as units where one diameter part was radian and they also used sexagesimal subunits of the diameter part.
The term "radian" first appeared in print on 5 June 1873, in examination questions set by James Thomson (brother of Lord Kelvin) at Queen's College, Belfast. He had used the term as early as 1871, while in 1869, Thomas Muir, then of the University of St Andrews, vacillated between the terms "rad", "radial", and "radian". In 1874, after a consultation with James Thomson, Muir adopted "radian".
Conversions.
Conversion between radians and degrees.
As stated, one radian is equal to 180/π degrees. Thus, to convert from radians to degrees, multiply by 180/π.
For example:
Conversely, to convert from degrees to radians, multiply by π/180.
For example:
formula_7
Radians can be converted to turns (complete revolutions) by dividing the number of radians by 2π.
Radian to degree conversion derivation.
The length of circumference of a circle is given by formula_8, where formula_9 is the radius of the circle.
So the following equivalent relation is true:
formula_10[Since a formula_11 sweep is needed to draw a full circle]
By the definition of radian, a full circle represents:
Combining both the above relations:
Conversion between radians and gradians.
formula_17 radians are equal to one turn, which is by definition 400 gradians (400 gons or 400g). So, to convert from radians to gradians multiply by formula_18, and to convert from gradians to radians multiply by formula_19. For example,
Advantages of measuring in radians.
In calculus and most other branches of mathematics beyond practical geometry, angles are universally measured in radians. This is because radians have a mathematical "naturalness" that leads to a more elegant formulation of a number of important results.
Most notably, results in analysis involving trigonometric functions are simple and elegant when the functions' arguments are expressed in radians. For example, the use of radians leads to the simple limit formula
which is the basis of many other identities in mathematics, including
Because of these and other properties, the trigonometric functions appear in solutions to mathematical problems that are not obviously related to the functions' geometrical meanings (for example, the solutions to the differential equation formula_25, the evaluation of the integral formula_26, and so on). In all such cases it is found that the arguments to the functions are most naturally written in the form that corresponds, in geometrical contexts, to the radian measurement of angles.
The trigonometric functions also have simple and elegant series expansions when radians are used; for example, the following Taylor series for sin "x" :
If "x" were expressed in degrees then the series would contain messy factors involving powers of π/180: if "x" is the number of degrees, the number of radians is "y" = π"x" /180, so
Mathematically important relationships between the sine and cosine functions and the exponential function (see, for example, Euler's formula) are, again, elegant when the functions' arguments are in radians and messy otherwise.
Dimensional analysis.
Although the radian is a unit of measure, it is a dimensionless quantity. This can be seen from the definition given earlier: the angle subtended at the centre of a circle, measured in radians, is equal to the ratio of the length of the enclosed arc to the length of the circle's radius. Since the units of measurement cancel, this ratio is dimensionless.
Although polar and spherical coordinates use radians to describe coordinates in two and three dimensions, the unit is derived from the radius coordinate, so the angle measure is still dimensionless.
Use in physics.
The radian is widely used in physics when angular measurements are required. For example, angular velocity is typically measured in radians per second (rad/s). One revolution per second is equal to 2π radians per second.
For the purpose of dimensional analysis, the units of angular velocity are s−1 and s−2 respectively.
Similarly, angular acceleration is often measured in radians per second per second (rad/s2).
Likewise, the phase difference of two waves can also be measured in radians. For example, if the phase difference of two waves is (k·2π) radians, where k is an integer, they are considered in phase, whilst if the phase difference of two waves is (k·2π + π), where k is an integer, they are considered in antiphase.
Multiples of radian units.
Metric prefixes have limited use with radians, and none in mathematics.
A milliradian (mrad) is a thousandth of a radian and a microradian (μrad) is a millionth of a radian, i.e. 103 mrad = 106 μrad = 1 rad.
There are 2π × 1000 milliradians (≈ 6283.185 mrad) in a circle. So a trigonometric milliradian is just under of a circle. This “real” trigonometric unit of angular measurement of a circle is in use by telescopic sight manufacturers using (stadiametric) rangefinding in reticles.
The divergence of laser beams is also usually measured in milliradians.
An approximation of the trigonometric milliradian (0.001 rad), known as the (angular) mil, is used by NATO and other military organizations in gunnery and targeting. Each angular mil represents of a circle and is % smaller than the trigonometric milliradian. For the small angles typically found in targeting work, the convenience of using the number 6400 in calculation outweighs the small mathematical errors it introduces. In the past, other gunnery systems have used different approximations to ; for example Sweden used the "streck" and the USSR used .
Being based on the milliradian, the NATO mil subtends roughly 1 m at a range of 1000 m (at such small angles, the curvature is negligible).
Smaller units like microradians (μrad) and nanoradians (nrad) are used in astronomy, and can also be used to measure the beam quality of lasers with ultra-low divergence. More common is arc second, which is radians (around 4.8481 microradians). Similarly, the prefixes smaller than milli- are potentially useful in measuring extremely small angles.

</doc>
<doc id="3909371" url="https://en.wikipedia.org/wiki?curid=3909371" title="Radiation flux">
Radiation flux

Radiation flux is a measure of the amount of radiation received by an object from a given source. This can be any type of radiation, including electromagnetic, sound, and particles from a radioactive source.
Φ = is the radiation flux, L is the luminosity, or total power output of the source, and r is the distance from the radiation source. The units of radiation flux are W·m−2, or kg·s−3.
Radiation flux density is a related measure that takes into account the area the radiation flux passes through, and is defined as the flux divided by the area it passes through. The Radiation flux density is also known as Intensity where I = 

</doc>
<doc id="197767" url="https://en.wikipedia.org/wiki?curid=197767" title="Radioactive decay">
Radioactive decay

Radioactive decay, also known as nuclear decay or radioactivity, is the process by which a nucleus of an unstable atom loses energy by emitting radiation. A material that spontaneously emits such radiation — which includes alpha particles, beta particles, gamma rays and conversion electrons — is considered radioactive.
Radioactive decay is a stochastic (i.e. random) process at the level of single atoms, in that, according to quantum theory, it is impossible to predict when a particular atom will decay. The chance that a given atom will decay never changes, that is, it does not matter how long the atom has existed. For a large collection of atoms however, the decay rate for that collection can be calculated from their measured decay constants or half-lives. This is the basis of radiometric dating. The half-lives of radioactive atoms have no known limits for shortness or length of duration, and range over 55 orders of magnitude in time.
There are many types of radioactive decay (see table below). A decay, or loss of energy from the nucleus, results when an atom with one type of nucleus, called the "parent radionuclide" (or "parent radioisotope"), transforms into an atom with a nucleus in a different state, or with a nucleus containing a different number of protons and neutrons. The product is called the "daughter nuclide". In some decays, the parent and the daughter nuclides are different chemical elements, and thus the decay process results in the creation of an atom of a different element. This is known as a nuclear transmutation.
The first decay processes to be discovered were alpha decay, beta decay, and gamma decay. Alpha decay occurs when the nucleus ejects an alpha particle (helium nucleus). This is the most common process of emitting nucleons, but in rarer types of decays, nuclei can eject protons, or in the case of cluster decay specific nuclei of other elements. Beta decay occurs when the nucleus emits an electron or positron and a neutrino, in a process that changes a proton to a neutron or the other way about. The nucleus may capture an orbiting electron, causing a proton to convert into a neutron in a process called electron capture. All of these processes result in a well-defined nuclear transmutation.
By contrast, there are radioactive decay processes that do not result in a nuclear transmutation. The energy of an excited nucleus may be emitted as a gamma ray in a process called gamma decay, or be used to eject an orbital electron by its interaction with the excited nucleus, in a process called internal conversion. Highly excited neutron-rich nuclei, formed as the product of other types of decay, occasionally lose energy by way of neutron emission, resulting in a change of an element from one isotope to another. 
Another type of radioactive decay results in products that are not defined, but appear in a range of "pieces" of the original nucleus. This decay, called spontaneous fission, happens when a large unstable nucleus spontaneously splits into two (and occasionally three) smaller daughter nuclei, and generally leads to the emission of gamma rays, neutrons, or other particles from those products.
For a summary table showing the number of stable and radioactive nuclides in each category, see radionuclide. There exist twenty-nine chemical elements on Earth that are radioactive. They are those that contain thirty-four radionuclides that date before the time of formation of the solar system, and are known as primordial nuclides. Well-known examples are uranium and thorium, but also included are naturally occurring long-lived radioisotopes such as potassium-40. Another fifty or so shorter-lived radionuclides, such as radium and radon, found on Earth, are the products of decay chains that began with the primordial nuclides, and ongoing cosmogenic processes, such as the production of carbon-14 from nitrogen-14 by cosmic rays. Radionuclides may also be produced artificially in particle accelerators or nuclear reactors, resulting in 650 of these with half-lives of over an hour, and several thousand more with even shorter half-lives. See this list of nuclides for a list of these, sorted by half life.
History of discovery.
Radioactivity was discovered in 1896 by the French scientist Henri Becquerel, while working with phosphorescent materials. These materials glow in the dark after exposure to light, and he suspected that the glow produced in cathode ray tubes by X-rays might be associated with phosphorescence. He wrapped a photographic plate in black paper and placed various phosphorescent salts on it. All results were negative until he used uranium salts. The uranium salts caused a blackening of the plate in spite of the plate being wrapped in black paper. These radiations were given the name "Becquerel Rays".
It soon became clear that the blackening of the plate had nothing to do with phosphorescence, as the blackening was also produced by non-phosphorescent salts of uranium and metallic uranium. It became clear from these experiments that there was a form of invisible radiation that could pass through paper and was causing the plate to react as if exposed to light.
At first, it seemed as though the new radiation was similar to the then recently discovered X-rays. Further research by Becquerel, Ernest Rutherford, Paul Villard, Pierre Curie, Marie Curie, and others showed that this form of radioactivity was significantly more complicated. Rutherford was the first to realize that all such elements decay in accordance with the same mathematical exponential formula. Rutherford and his student Frederick Soddy were the first to realize that many decay processes resulted in the transmutation of one element to another. Subsequently, the radioactive displacement law of Fajans and Soddy was formulated to describe the products of alpha and beta decay.
The early researchers also discovered that many other chemical elements, besides uranium, have radioactive isotopes. A systematic search for the total radioactivity in uranium ores also guided Pierre and Marie Curie to isolate two new elements: polonium and radium. Except for the radioactivity of radium, the chemical similarity of radium to barium made these two elements difficult to distinguish.
Early health dangers.
The dangers of ionizing radiation due to radioactivity and X-rays were not immediately recognized.
X-rays.
The discovery of x‑rays by Wilhelm Röntgen in 1895 led to widespread experimentation by scientists, physicians, and inventors. Many people began recounting stories of burns, hair loss and worse in technical journals as early as 1896. In February of that year, Professor Daniel and Dr. Dudley of Vanderbilt University performed an experiment involving X-raying Dudley's head that resulted in his hair loss. A report by Dr. H.D. Hawks, of his suffering severe hand and chest burns in an X-ray demonstration, was the first of many other reports in "Electrical Review".
Other experimenters including Elihu Thomson, and Nikola Tesla also reported burns. Thomson deliberately exposed a finger to an X-ray tube over a period of time and suffered pain, swelling, and blistering. Other effects, including ultraviolet rays and ozone were sometimes blamed for the damage, 
and many physicians still claimed that there were no effects from X-ray exposure at all.
Despite this, there were some early systematic hazard investigations, and as early as 1902 William Herbert Rollins wrote almost despairingly that his warnings about the dangers involved in careless use of X-rays was not being heeded, either by industry or by his colleagues. By this time Rollins had proved that X-rays could kill experimental animals, could cause a pregnant guinea pig to abort, and that they could kill a fetus. He also stressed that "animals vary in susceptibility to the external action of X-light" and warned that these differences be considered when patients were treated by means of X-rays.
Radioactive substances.
However, the biological effects of radiation due to radioactive substances were less easy to gauge. This gave the opportunity for many physicians and corporations to market radioactive substances as patent medicines. Examples were radium enema treatments, and radium-containing waters to be drunk as tonics. Marie Curie protested against this sort of treatment, warning that the effects of radiation on the human body were not well understood. Curie later died from aplastic anaemia, likely caused by exposure to ionizing radiation. By the 1930s, after a number of cases of bone necrosis and death of radium treatment enthusiasts, radium-containing medicinal products had been largely removed from the market (radioactive quackery).
Radiation protection.
Only a year after Röntgen's discovery of X rays, the American engineer Wolfram Fuchs (1896) gave what is probably the first protection advice, but it was not until 1925 that the first International Congress of Radiology (ICR) was held and considered establishing international protection standards. The effects of radiation on genes, including the effect of cancer risk, were recognized much later. In 1927, Hermann Joseph Muller published research showing genetic effects and, in 1946, was awarded the Nobel prize for his findings.
The second ICR was held in Stockholm in 1928 and proposed the adoption of the rontgen unit, and the 'International X-ray and Radium Protection Committee' (IXRPC) was formed. Rolf Sievert was named Chairman, but a driving force was George Kaye of the British National Physical Laboratory. The committee met in 1931, 1934 and 1937.
After World War II the increased range and quantity of radioactive substances being handled as a result of military and civil nuclear programmes led to large groups of occupational workers and the public being potentially exposed to harmful levels of ionising radiation. This was considered at the first post-war ICR convened in London in 1950, when the present International Commission on Radiological Protection (ICRP) was born.
Since then the ICRP has developed the present international system of radiation protection, covering all aspects of radiation hazard.
Units of radioactivity.
The International System of Units (SI) unit of radioactive activity is the becquerel (Bq), named in honour of the scientist Henri Becquerel. One Bq is defined as one transformation (or decay or disintegration) per second.
An older unit of radioactivity is the curie, Ci, which was originally defined as "the quantity or mass of radium emanation in equilibrium with one gram of radium (element)". Today, the curie is defined as disintegrations per second, so that 1 curie (Ci) = .
For radiological protection purposes, although the United States Nuclear Regulatory Commission permits the use of the unit curie alongside SI units, the European Union European units of measurement directives required that its use for "public health ... purposes" be phased out by 31 December 1985.
Types of decay.
Early researchers found that an electric or magnetic field could split radioactive emissions into three types of beams. The rays were given the names alpha, beta, and gamma, in order of their ability to penetrate matter. While alpha decay was observed only in heavier elements of atomic number 52 (tellurium) and greater, the other two types of decay were produced by all of the elements. Lead, atomic number 82, is the heaviest element to have any isotopes stable (to the limit of measurement) to radioactive decay. Radioactive decay is seen in all isotopes of all elements of atomic number 83 (bismuth) or greater. Bismuth, however, is only very slightly radioactive.
In analysing the nature of the decay products, it was obvious from the direction of the electromagnetic forces applied to the radiations by external magnetic and electric fields that alpha particles carried a positive charge, beta particles carried a negative charge, and gamma rays were neutral. From the magnitude of deflection, it was clear that alpha particles were much more massive than beta particles. Passing alpha particles through a very thin glass window and trapping them in a discharge tube allowed researchers to study the emission spectrum of the captured particles, and ultimately proved that alpha particles are helium nuclei. Other experiments showed beta radiation, resulting from decay and cathode rays, were high-speed electrons. Likewise, gamma radiation and X-rays were found to be high-energy electromagnetic radiation.
The relationship between the types of decays also began to be examined: For example, gamma decay was almost always found to be associated with other types of decay, and occurred at about the same time, or afterwards. Gamma decay as a separate phenomenon, with its own half-life (now termed isomeric transition), was found in natural radioactivity to be a result of the gamma decay of excited metastable nuclear isomers, which were in turn created from other types of decay.
Although alpha, beta, and gamma radiations were most commonly found, other types of emission were eventually discovered. Shortly after the discovery of the positron in cosmic ray products, it was realized that the same process that operates in classical beta decay can also produce positrons (positron emission), along with neutrinos (classical beta decay produces antineutrinos). In a more common analogous process, called electron capture, some proton-rich nuclides were found to capture their own atomic electrons instead of emitting positrons, and subsequently these nuclides emit only a neutrino and a gamma ray from the excited nucleus (and often also Auger electrons and characteristic X-rays, as a result of the re-ordering of electrons to fill the place of the missing captured electron). These types of decay involve the nuclear capture of electrons or emission of electrons or positrons, and thus acts to move a nucleus toward the ratio of neutrons to protons that has the least energy for a given total number of nucleons. This consequently produces a more stable (lower energy) nucleus.
Shortly after the discovery of the neutron in 1932, Enrico Fermi realized that certain rare beta-decay reactions immediately yield neutrons as a decay particle (neutron emission). Isolated proton emission was eventually observed in some elements. It was also found that some heavy elements may undergo spontaneous fission into products that vary in composition. In a phenomenon called cluster decay, specific combinations of neutrons and protons other than alpha particles (helium nuclei) were found to be spontaneously emitted from atoms.
Other types of radioactive decay were found to emit previously-seen particles, but via different mechanisms. An example is internal conversion, which results in an initial electron emission, and then often further characteristic X-rays and Auger electrons emissions, although the internal conversion process involves neither beta nor gamma decay. A neutrino is not emitted, and none of the electron(s) and photon(s) emitted originate in the nucleus, even though the energy to emit all of them does originate there. Internal conversion decay, like isomeric transition gamma decay and neutron emission, involves the release of energy by an excited nuclide, without the transmutation of one element into another.
Rare events that involve a combination of two beta-decay type events happening simultaneously are known (see below). Any decay process that does not violate the conservation of energy or momentum laws (and perhaps other particle conservation laws) is permitted to happen, although not all have been detected. An interesting example discussed in a final section, is bound state beta decay of rhenium-187. In this process, beta electron-decay of the parent nuclide is not accompanied by beta electron emission, because the beta particle has been captured into the K-shell of the emitting atom. An antineutrino is emitted, as in all negative beta decays.
Radionuclides can undergo a number of different reactions. These are summarized in the following table. A nucleus with mass number "A" and atomic number "Z" is represented as ("A", "Z"). The column "Daughter nucleus" indicates the difference between the new nucleus and the original nucleus. Thus, ("A" − 1, "Z") means that the mass number is one less than before, but the atomic number is the same as before.
If energy circumstances are favorable, a given radionuclide may undergo many competing types of decay, with some atoms decaying by one route, and others decaying by another. An example is copper-64, which has 29 protons, and 35 neutrons, which decays with a half-life of about 12.7 hours. This isotope has one unpaired proton and one unpaired neutron, so either the proton or the neutron can decay to the opposite particle. This particular nuclide (though not all nuclides in this situation) is almost equally likely to decay through proton decay, producing a positron emission (18%), or through electron capture (43%), as it does through neutron decay by electron emission (39%). The excited energy states resulting from these decays which fail to end in a ground energy state, also produce later internal conversion and gamma decay in almost 0.5% of the time.
Radioactive decay results in a reduction of summed rest mass, once the released energy (the "disintegration energy") has escaped in some way. Although decay energy is sometimes defined as associated with the difference between the mass of the parent nuclide products and the mass of the decay products, this is true only of rest mass measurements, where some energy has been removed from the product system. This is true because the decay energy must always carry mass with it, wherever it appears (see mass in special relativity) according to the formula "E" = "mc"2. The decay energy is initially released as the energy of emitted photons plus the kinetic energy of massive emitted particles (that is, particles that have rest mass). If these particles come to thermal equilibrium with their surroundings and photons are absorbed, then the decay energy is transformed to thermal energy, which retains its mass.
Decay energy therefore remains associated with a certain measure of mass of the decay system, called invariant mass, which does not change during the decay, even though the energy of decay is distributed among decay particles. The energy of photons, the kinetic energy of emitted particles, and, later, the thermal energy of the surrounding matter, all contribute to the invariant mass of the system. Thus, while the sum of the rest masses of the particles is not conserved in radioactive decay, the "system" mass and system invariant mass (and also the system total energy) is conserved throughout any decay process. This is a restatement of the equivalent laws of conservation of energy and conservation of mass.
Radioactive decay rates.
The "decay rate", or "activity", of a radioactive substance is characterized by:
Constant quantities:
Although these are constants, they are associated with the statistical behavior of populations of atoms. In consequence, predictions using these constants are less accurate for minuscule samples of atoms.
In principle a half-life, a third-life, or even a (1/)-life, can be used in exactly the same way as half-life; but the mean life and half-life have been adopted as standard times associated with exponential decay.
Time-variable quantities:
These are related as follows:
where "N"0 is the initial amount of active substance — substance that has the same percentage of unstable particles as when the substance was formed.
Mathematics of radioactive decay.
Universal law of radioactive decay.
Radioactivity is one very frequently given example of exponential decay. The law describes the statistical behaviour of a large number of nuclides, rather than individual atoms. In the following formalism, the number of nuclides or the nuclide population "N", is of course a discrete variable (a natural number)—but for any physical sample "N" is so large that it can be treated as a continuous variable. Differential calculus is needed to set up differential equations for the modelling the behaviour of the nuclear decay.
The mathematics of radioactive decay depend on a key assumption that a nucleus of a radionuclide has no "memory" or way of translating its history into its present behavior. A nucleus does not "age" with the passage of time. Thus, the probability of its breaking down does not increase with time, but stays constant no matter how long the nucleus has existed. This constant probability may vary greatly between different types of nuclei, leading to the many different observed decay rates. However, whatever the probability is, it does not change. This is in marked contrast to complex objects which do show aging, such as automobiles and humans. These systems do have a chance of breakdown per unit of time, that increases from the moment they begin their existence.
One-decay process.
Consider the case of a nuclide that decays into another by some process (emission of other particles, like electron neutrinos and electrons e− as in beta decay, are irrelevant in what follows). The decay of an unstable nucleus is entirely random and it is impossible to predict when a particular atom will decay. However, it is equally likely to decay at any instant in time. Therefore, given a sample of a particular radioisotope, the number of decay events expected to occur in a small interval of time is proportional to the number of atoms present , that is
Particular radionuclides decay at different rates, so each has its own decay constant . The expected decay is proportional to an increment of time, :
The negative sign indicates that decreases as time increases, as the decay events follow one after another. The solution to this first-order differential equation is the function:
where is the value of at time = 0.
We have for all time :
where is the constant number of particles throughout the decay process, which is equal to the initial number of nuclides since this is the initial substance.
If the number of non-decayed nuclei is:
then the number of nuclei of , i.e. the number of decayed nuclei, is
The number of decays observed over a given interval obeys Poisson statistics. If the average number of decays is , the probability of a given number of decays is
Chain-decay processes.
Chain of two decays
Now consider the case of a chain of two decays: one nuclide decaying into another by one process, then decaying into another by a second process, i.e. "". The previous equation cannot be applied to the decay chain, but can be generalized as follows. Since decays into , "then" decays into , the activity of adds to the total number of nuclides in the present sample, "before" those nuclides decay and reduce the number of nuclides leading to the later sample. In other words, the number of second generation nuclei increases as a result of the first generation nuclei decay of , and decreases as a result of its own decay into the third generation nuclei . The sum of these two terms gives the law for a decay chain for two nuclides:
The rate of change of , that is , is related to the changes in the amounts of and , can increase as is produced from and decrease as produces .
Re-writing using the previous results:
The subscripts simply refer to the respective nuclides, i.e. is the number of nuclides of type , is the initial number of nuclides of type , is the decay constant for - and similarly for nuclide . Solving this equation for gives:
In the case where is a stable nuclide ( = 0), this equation reduces to the previous solution:
as shown above for one decay. The solution can be found by the integration factor method, where the integrating factor is . This case is perhaps the most useful, since it can derive both the one-decay equation (above) and the equation for multi-decay chains (below) more directly.
Chain of any number of decays
For the general case of any number of consecutive decays in a decay chain, i.e. , where is the number of decays and is a dummy index (), each nuclide population can be found in terms of the previous population. In this case , ..., . Using the above result in a recursive form:
The general solution to the recursive problem is given by Bateman's equations:
Alternative decay modes.
In all of the above examples, the initial nuclide decays into only one product. Consider the case of one initial nuclide that can decay into either of two products, that is ' and ' in parallel. For example, in a sample of potassium-40, 89.3% of the nuclei decay to calcium-40 and 10.7% to argon-40. We have for all time :
which is constant, since the total number of nuclides remains constant. Differentiating with respect to time:
defining the "total decay constant" in terms of the sum of "partial decay constants" and :
Notice that 
Solving this equation for :
where is the initial number of nuclide A. When measuring the production of one nuclide, one can only observe the total decay constant . The decay constants and determine the probability for the decay to result in products or as follows:
because the fraction of nuclei decay into while the fraction of nuclei decay into .
Corollaries of the decay laws.
The above equations can also be written using quantities related to the number of nuclide particles in a sample;
where "L" = is Avogadro's constant, " is the relative atomic mass number, and the amount of the substance is in moles.
Decay timing: definitions and relations.
Time constant and mean-life.
For the one-decay solution ":
the equation indicates that the decay constant has units of "", and can thus also be represented as 1/, where is a characteristic time of the process called the "time constant".
In a radioactive decay process, this time constant is also the mean lifetime for decaying atoms. Each atom "lives" for a finite amount of time before it decays, and it may be shown that this mean lifetime is the arithmetic mean of all the atoms' lifetimes, and that it is , which again is related to the decay constant as follows:
This form is also true for two-decay processes simultaneously ", inserting the equivalent values of decay constants (as given above)
into the decay solution leads to:
Half-life.
A more commonly used parameter is the half-life. Given a sample of a particular radionuclide, the half-life is the time taken for half the radionuclide's atoms to decay. For the case of one-decay nuclear reactions:
the half-life is related to the decay constant as follows: set " and = to obtain
This relationship between the half-life and the decay constant shows that highly radioactive substances are quickly spent, while those that radiate weakly endure longer. Half-lives of known radionuclides vary widely, from more than 1019 years, such as for the very nearly stable nuclide 209Bi, to 10−23 seconds for highly unstable ones.
The factor of in the above relations results from the fact that concept of "half-life" is merely a way of selecting a different base other than the natural base for the lifetime expression. The time constant is the -life, the time until only 1/"e" remains, about 36.8%, rather than the 50% in the half-life of a radionuclide. Thus, is longer than . The following equation can be shown to be valid:
Since radioactive decay is exponential with a constant probability, each process could as easily be described with a different constant time period that (for example) gave its "(1/3)-life" (how long until only 1/3 is left) or "(1/10)-life" (a time period until only 10% is left), and so on. Thus, the choice of and "" for marker-times, are only for convenience, and from convention. They reflect a fundamental principle only in so much as they show that the "same proportion" of a given radioactive substance will decay, during any time-period that one chooses.
Mathematically, the life for the above situation would be found in the same way as aboveby setting "", and substituting into the decay solution to obtain
Example.
A sample of 14C has a half-life of 5,730 years and a decay rate of 14 disintegration per minute (dpm) per gram of natural carbon.
If an artifact is found to have radioactivity of 4 dpm per gram of its present C, we can find the approximate age of the object using the above equation:
where: formula_30
Changing decay rates.
The radioactive decay modes of electron capture and internal conversion are known to be slightly sensitive to chemical and environmental effects that change the electronic structure of the atom, which in turn affects the presence of 1s and 2s electrons that participate in the decay process. A small number of mostly light nuclides are affected. For example, chemical bonds can affect the rate of electron capture to a small degree (in general, less than 1%) depending on the proximity of electrons to the nucleus. In 7Be, a difference of 0.9% has been observed between half-lives in metallic and insulating environments. This relatively large effect is because beryllium is a small atom whose valence electrons are in 2s atomic orbitals, which are subject to electron capture in 7Be because (like all s atomic orbitals in all atoms) they naturally penetrate into the nucleus.
In 1992, Jung et al. of the Darmstadt Heavy-Ion Research group observed an accelerated β decay of 163Dy66+. Although neutral 163Dy is a stable isotope, the fully ionized 163Dy66+ undergoes β decay into the K and L shells with a half-life of 47 days.
Rhenium-187 is another spectacular example. 187Re normally beta decays to 187Os with a half-life of 41.6 × 109 years, but studies using fully ionised 187Re atoms (bare nuclei) have found that this can decrease to only 33 years. This is attributed to "bound-state β− decay" of the fully ionised atom – the electron is emitted into the "K-shell" (1s atomic orbital), which cannot occur for neutral atoms in which all low-lying bound states are occupied.
A number of experiments have found that decay rates of other modes of artificial and naturally occurring radioisotopes are, to a high degree of precision, unaffected by external conditions such as temperature, pressure, the chemical environment, and electric, magnetic, or gravitational fields. Comparison of laboratory experiments over the last century, studies of the Oklo natural nuclear reactor (which exemplified the effects of thermal neutrons on nuclear decay), and astrophysical observations of the luminosity decays of distant supernovae (which occurred far away so the light has taken a great deal of time to reach us), for example, strongly indicate that unperturbed decay rates have been constant (at least to within the limitations of small experimental errors) as a function of time as well.
Recent results suggest the possibility that decay rates might have a weak dependence on environmental factors. It has been suggested that measurements of decay rates of silicon-32, manganese-54, and radium-226 exhibit small seasonal variations (of the order of 0.1%), while the decay of Radon-222 exhibit large 4% peak-to-peak seasonal variations, proposed to be related to either solar flare activity or distance from the Sun. However, such measurements are highly susceptible to systematic errors, and a subsequent paper has found no evidence for such correlations in seven other isotopes (22Na, 44Ti, 108Ag, 121Sn, 133Ba, 241Am, 238Pu), and sets upper limits on the size of any such effects.
Theoretical basis of decay phenomena.
The neutrons and protons that constitute nuclei, as well as other particles that approach close enough to them, are governed by several interactions. The strong nuclear force, not observed at the familiar macroscopic scale, is the most powerful force over subatomic distances. The electrostatic force is almost always significant, and, in the case of beta decay, the weak nuclear force is also involved.
The interplay of these forces produces a number of different phenomena in which energy may be released by rearrangement of particles in the nucleus, or else the change of one type of particle into others. These rearrangements and transformations may be hindered energetically, so that they do not occur immediately. In certain cases, random quantum vacuum fluctuations are theorized to promote relaxation to a lower energy state (the "decay") in a phenomenon known as quantum tunneling. Radioactive decay half-life of nuclides has been measured over timescales of 55 orders of magnitude, from 2.3 x 10−23 seconds (for hydrogen-7) to 6.9 x 1031 seconds (for tellurium-128). The limits of these timescales are set by the sensitivity of instrumentation only, and there are no known natural limits to how brief or long a decay half life for radioactive decay of a radionuclide may be.
The decay process, like all hindered energy transformations, may be analogized by a snowfield on a mountain. While friction between the ice crystals may be supporting the snow's weight, the system is inherently unstable with regard to a state of lower potential energy. A disturbance would thus facilitate the path to a state of greater entropy: The system will move towards the ground state, producing heat, and the total energy will be distributable over a larger number of quantum states. Thus, an avalanche results. The "total" energy does not change in this process, but, because of the second law of thermodynamics, avalanches have only been observed in one direction and that is toward the "ground state" — the state with the largest number of ways in which the available energy could be distributed.
Such a collapse (a "decay event") requires a specific activation energy. For a snow avalanche, this energy comes as a disturbance from outside the system, although such disturbances can be arbitrarily small. In the case of an excited atomic nucleus, the arbitrarily small disturbance comes from quantum vacuum fluctuations. A radioactive nucleus (or any excited system in quantum mechanics) is unstable, and can, thus, "spontaneously" stabilize to a less-excited system. The resulting transformation alters the structure of the nucleus and results in the emission of either a photon or a high-velocity particle that has mass (such as an electron, alpha particle, or other type). 
Occurrence and applications.
According to the Big Bang theory, stable isotopes of the lightest five elements (H, He, and traces of Li, Be, and B) were produced very shortly after the emergence of the universe, in a process called Big Bang nucleosynthesis. These lightest stable nuclides (including deuterium) survive to today, but any radioactive isotopes of the light elements produced in the Big Bang (such as tritium) have long since decayed. Isotopes of elements heavier than boron were not produced at all in the Big Bang, and these first five elements do not have any long-lived radioisotopes. Thus, all radioactive nuclei are, therefore, relatively young with respect to the birth of the universe, having formed later in various other types of nucleosynthesis in stars (in particular, supernovae), and also during ongoing interactions between stable isotopes and energetic particles. For example, carbon-14, a radioactive nuclide with a half-life of only 5,730 years, is constantly produced in Earth's upper atmosphere due to interactions between cosmic rays and nitrogen.
Nuclides that are produced by radioactive decay are called radiogenic nuclides, whether they themselves are stable or not. There exist stable radiogenic nuclides that were formed from short-lived extinct radionuclides in the early solar system. The extra presence of these stable radiogenic nuclides (such as Xe-129 from primordial I-129) against the background of primordial stable nuclides can be inferred by various means.
Radioactive decay has been put to use in the technique of radioisotopic labeling, which is used to track the passage of a chemical substance through a complex system (such as a living organism). A sample of the substance is synthesized with a high concentration of unstable atoms. The presence of the substance in one or another part of the system is determined by detecting the locations of decay events.
On the premise that radioactive decay is truly random (rather than merely chaotic), it has been used in hardware random-number generators. Because the process is not thought to vary significantly in mechanism over time, it is also a valuable tool in estimating the absolute ages of certain materials. For geological materials, the radioisotopes and some of their decay products become trapped when a rock solidifies, and can then later be used (subject to many well-known qualifications) to estimate the date of the solidification. These include checking the results of several simultaneous processes and their products against each other, within the same sample. In a similar fashion, and also subject to qualification, the rate of formation of carbon-14 in various eras, the date of formation of organic matter within a certain period related to the isotope's half-life may be estimated, because the carbon-14 becomes trapped when the organic matter grows and incorporates the new carbon-14 from the air. Thereafter, the amount of carbon-14 in organic matter decreases according to decay processes that may also be independently cross-checked by other means (such as checking the carbon-14 in individual tree rings, for example).
Origins of radioactive nuclides.
Radioactive primordial nuclides found in the Earth are residues from ancient supernova explosions which occurred before the formation of the solar system. They are the long-lived fraction of radionuclides surviving in the primordial solar nebula through planet accretion until the present. The naturally occurring short-lived radiogenic radionuclides found in rocks are the daughters of these radioactive primordial nuclides. Another minor source of naturally occurring radioactive nuclides are cosmogenic nuclides, formed by cosmic ray bombardment of material in the Earth's atmosphere or crust. The radioactive decay of these radionuclides in rocks within Earth's mantle and crust contribute significantly to Earth's internal heat budget.
Decay chains and multiple modes.
The daughter nuclide of a decay event may also be unstable (radioactive). In this case, it will also decay, producing radiation. The resulting second daughter nuclide may also be radioactive. This can lead to a sequence of several decay events. Eventually, a stable nuclide is produced. This is called a "decay chain" (see this article for specific details of important natural decay chains).
An example is the natural decay chain of 238U, which is as follows:
Some radionuclides may have several different paths of decay. For example, approximately 36% of bismuth-212 decays, through alpha-emission, to thallium-208 while approximately 64% of bismuth-212 decays, through beta-emission, to polonium-212. Both thallium-208 and polonium-212 are radioactive daughter products of bismuth-212, and both decay directly to stable lead-208.

</doc>
<doc id="26561" url="https://en.wikipedia.org/wiki?curid=26561" title="Rank (linear algebra)">
Rank (linear algebra)

In linear algebra, the rank of a matrix "A" is the dimension of the vector space generated (or spanned) by its columns. This is the same as the dimension of the space spanned by its rows. It is a measure of the "nondegenerateness" of the system of linear equations and linear transformation encoded by "A". There are multiple equivalent definitions of rank. A matrix's rank is one of its most fundamental characteristics.
The rank is commonly denoted rank("A") or rk("A"); sometimes the parentheses are unwritten, as in rank "A".
Main definitions.
In this section we give some definitions of the rank of a matrix. Many definitions are possible; see below for several of these.
The column rank of "A" is the dimension of the column space of "A", while the row rank of "A" is the dimension of the row space of "A".
A fundamental result in linear algebra is that the column rank and the row rank are always equal. (Two proofs of this result are given in below.) This number (i.e., the number of linearly independent rows or columns) is simply called the rank of "A".
A matrix is said to have full rank if its rank equals the largest possible for a matrix of the same dimensions, which is the lesser of the number of rows and columns. A matrix is said to be rank deficient if it does not have full rank.
The rank is also the dimension of the image of the linear transformation that is given by multiplication by "A". More generally, if a linear operator on a vector space (possibly infinite-dimensional) has finite-dimensional image (e.g., a finite-rank operator), then the rank of the operator is defined as the dimension of the image.
Examples.
The matrix
has rank 2: the first two rows are linearly independent, so the rank is at least 2, but all three rows are linearly dependent (the first is equal to the sum of the second and third) so the rank must be less than 3.
The matrix
has rank 1: there are nonzero columns, so the rank is positive, but any pair of columns is linearly dependent. Similarly, the transpose
of "A" has rank 1. Indeed, since the column vectors of "A" are the row vectors of the transpose of "A", the statement that the column rank of a matrix equals its row rank is equivalent to the statement that the rank of a matrix is equal to the rank of its transpose, i.e., rk("A") = rk("A"T).
Computing the rank of a matrix.
Rank from row echelon forms.
A common approach to finding the rank of a matrix is to reduce it to a simpler form, generally row echelon form, by elementary row operations. Row operations do not change the row space (hence do not change the row rank), and, being invertible, map the column space to an isomorphic space (hence do not change the column rank). Once in row echelon form, the rank is clearly the same for both row rank and column rank, and equals the number of pivots (or basic columns) and also the number of non-zero rows.
For example, the matrix "A" given by
can be put in reduced row-echelon form by using the following elementary row operations:
The final matrix (in reduced row echelon form) has two non-zero rows and thus the rank of matrix "A" is 2.
Computation.
When applied to floating point computations on computers, basic Gaussian elimination (LU decomposition) can be unreliable, and a rank-revealing decomposition should be used instead. An effective alternative is the singular value decomposition (SVD), but there are other less expensive choices, such as QR decomposition with pivoting (so-called rank-revealing QR factorization), which are still more numerically robust than Gaussian elimination. Numerical determination of rank requires a criterion for deciding when a value, such as a singular value from the SVD, should be treated as zero, a practical choice which depends on both the matrix and the application.
Proofs that column rank = row rank.
The fact that the column and row ranks of any matrix are equal forms an important part of the fundamental theorem of linear algebra. We present two proofs of this result. The first is short, uses only basic properties of linear combinations of vectors, and is valid over any field. The proof is based upon Wardlaw (2005). The second is an elegant argument using orthogonality and is valid for matrices over the real numbers; it is based upon Mackiw (1995). Both proofs can be found in the book by Banerjee and Roy (2014) 
First proof.
Let "A" be a matrix of size "m × n" (with "m" rows and "n" columns). Let the column rank of "A" be "r" and let 
"c1"...,"cr" be any basis for the column space of "A". Place these as the columns of an "m × r" matrix "C". Every column of "A" can be expressed as a linear combination of the "r" columns in "C". This means that there is an "r × n" matrix "R" such that "A = CR". "R" is the matrix whose "i"-th column is formed from the coefficients giving the "i"-th column of "A" as a linear combination of the "r" columns of "C". Now, each row of "A" is given by a linear combination of the "r" rows of "R". Therefore, the rows of "R" form a spanning set of the row space of "A" and, by the Steinitz exchange lemma, the row rank of "A" cannot exceed "r". This proves that the row rank of "A" is less than or equal to the column rank of "A". This result can be applied to any matrix, so apply the result to the transpose of "A". Since the row rank of the transpose of "A" is the column rank of "A" and the column rank of the transpose of "A" is the row rank of "A", this establishes the reverse inequality and we obtain the equality of the row rank and the column rank of "A". (Also see rank factorization.)
Second proof.
Let "A" be an "m" × "n" matrix with entries in the real numbers whose row rank is "r". Therefore, the dimension of the row space of "A" is "r". Let formula_6 be a basis of the row space of "A". We claim that the vectors formula_7 are linearly independent. To see why, consider a linear homogeneous relation involving these vectors with scalar coefficients formula_8:
where formula_10. We make two observations: (a) "v" is a linear combination of vectors in the row space of "A", which implies that "v" belongs to the row space of "A", and (b) since "A" "v" = 0, the vector "v" is orthogonal to every row vector of "A" and, hence, is orthogonal to every vector in the row space of "A". The facts (a) and (b) together imply that "v" is orthogonal to itself, which proves that "v" = 0 or, by the definition of "v",
But recall that the formula_12 were chosen as a basis of the row space of "A" and so are linearly independent. This implies that formula_13. It follows that formula_7 are linearly independent.
Now, each formula_15 is obviously a vector in the column space of "A". So, formula_7 is a set of "r" linearly independent vectors in the column space of "A" and, hence, the dimension of the column space of "A" (i.e., the column rank of "A") must be at least as big as "r". This proves that row rank of "A" is no larger than the column rank of "A". Now apply this result to the transpose of "A" to get the reverse inequality and conclude as in the previous proof.
Alternative definitions.
In all the definitions in this section, the matrix "A" is taken to be an "m" × "n" matrix over an arbitrary field "F".
Given the matrix "A", there is an associated linear mapping 
defined by
The rank of "A" is the dimension of the image of "f". This definition has the advantage that it can be applied to any linear map without need for a specific matrix.
Given the same linear mapping "f" as above, the rank is "n" minus the dimension of the kernel of "f". The rank–nullity theorem states that this definition is equivalent to the preceding one.
The rank of "A" is the maximal number of linearly independent columns formula_17 of "A"; this is the dimension of the column space of "A" (the column space being the subspace of "F""m" generated by the columns of "A", which is in fact just the image of the linear map "f" associated to "A").
The rank of "A" is the maximal number of linearly independent rows of "A"; this is the dimension of the row space of "A".
The rank of "A" is the smallest integer "k" such that "A" can be factored as formula_18, where "C" is an "m" × "k" matrix and "R" is a "k" × "n" matrix. In fact, for all integers "k", the following are equivalent:
Indeed, the following equivalences are obvious: formula_26.
For example, to prove (3) from (2), take "C" to be the matrix whose columns are formula_19 from (2).
To prove (2) from (3), take formula_19 to be the columns of "C".
It follows from the equivalence formula_29 that the row rank is equal to the column rank.
As in the case of the "dimension of image" characterization, this can be generalized to a definition of the rank of any linear map: the rank of a linear map "f" : "V" → "W" is the minimal dimension "k" of an intermediate space "X" such that "f" can be written as the composition of a map "V" → "X" and a map "X" → "W". Unfortunately, this definition does not suggest an efficient manner to compute the rank (for which it is better to use one of the alternative definitions). See rank factorization for details.
The rank of "A" is the largest order of any non-zero minor in "A". (The order of a minor is the side-length of the square sub-matrix of which it is the determinant.) Like the decomposition rank characterization, this does not give an efficient way of computing the rank, but it is useful theoretically: a single non-zero minor witnesses a lower bound (namely its order) for the rank of the matrix, which can be useful (for example) to prove that certain operations do not lower the rank of a matrix.
A non-vanishing "p"-minor ("p" × "p" submatrix with non-zero determinant) shows that the rows and columns of that submatrix are linearly independent, and thus those rows and columns of the full matrix are linearly independent (in the full matrix), so the row and column rank are at least as large as the determinantal rank; however, the converse is less straightforward. The equivalence of determinantal rank and column rank is a strengthening of the statement that if the span of "n" vectors has dimension "p," then "p" of those vectors span the space (equivalently, that one can choose a spanning set that is a "subset" of the vectors): the equivalence implies that a subset of the rows and a subset of the columns simultaneously define an invertible submatrix (equivalently, if the span of "n" vectors has dimension "p," then "p" of these vectors span the space "and" there is a set of "p" coordinates on which they are linearly independent).
The rank of "A" is the smallest number "k" such that "A" can be written as a sum of "k" rank 1 matrices, where a matrix is defined to have rank 1 if and only if it can be written as a nonzero product formula_30 of a column vector "c" and a row vector "r". This notion of rank is called tensor rank; it can be generalized in the separable models interpretation of the singular value decomposition.
Properties.
We assume that "A" is an matrix, and we define the linear map "f" by as above.
Applications.
One useful application of calculating the rank of a matrix is the computation of the number of solutions of a system of linear equations. According to the Rouché–Capelli theorem, the system is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, then the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has "k" free parameters where "k" is the difference between the number of variables and the rank. In this case (and assuming the system of equations is in the real or complex numbers) the system of equations has infinitely many solutions.
In control theory, the rank of a matrix can be used to determine whether a linear system is controllable, or observable.
In the field of communication complexity, the rank of the communication matrix of a function gives bounds on the amount of communication needed for two parties to compute the function.
Generalization.
There are different generalisations of the concept of rank to matrices over arbitrary rings. In those generalisations, column rank, row rank, dimension of column space and dimension of row space of a matrix may be different from the others or may not exist.
Thinking of matrices as tensors, the tensor rank generalizes to arbitrary tensors; note that for tensors of order greater than 2 (matrices are order 2 tensors), rank is very hard to compute, unlike for matrices.
There is a notion of rank for smooth maps between smooth manifolds. It is equal to the linear rank of the derivative.
Matrices as tensors.
Matrix rank should not be confused with tensor order, which is called tensor rank. Tensor order is the number of indices required to write a tensor, and thus matrices all have tensor order 2. More precisely, matrices are tensors of type (1,1), having one row index and one column index, also called covariant order 1 and contravariant order 1; see Tensor (intrinsic definition) for details.
Note that the tensor rank of a matrix can also mean the minimum number of simple tensors necessary to express the matrix as a linear combination, and that this definition does agree with matrix rank as here discussed.

</doc>
<doc id="20646438" url="https://en.wikipedia.org/wiki?curid=20646438" title="Real number">
Real number

In mathematics, a real number is a value that represents a quantity along a continuous line. The adjective "real" in this context was introduced in the 17th century by Descartes, who distinguished between real and imaginary roots of polynomials.
The real numbers include all the rational numbers, such as the integer −5 and the fraction 4/3, all the irrational numbers, such as (1.41421356…, the square root of two, an irrational algebraic number) and all transcendental numbers, such as (3.14159265…, a transcendental number). Real numbers can be thought of as points on an infinitely long line called the number line or real line, where the points corresponding to integers are equally spaced. Any real number can be determined by a possibly infinite decimal representation, such as that of 8.632, where each consecutive digit is measured in units one tenth the size of the previous one. The real line can be thought of as a part of the complex plane, and complex numbers include real numbers.
These descriptions of the real numbers are not sufficiently rigorous by the modern standards of pure mathematics. The discovery of a suitably rigorous definition of the real numbers – indeed, the realization that a better definition was needed – was one of the most important developments of 19th century mathematics. The currently standard axiomatic definition is that real numbers form the unique Archimedean complete totally ordered field up to an isomorphism, whereas popular constructive definitions of real numbers include declaring them as equivalence classes of Cauchy sequences of rational numbers, Dedekind cuts, or certain infinite "decimal representations", together with precise interpretations for the arithmetic operations and the order relation. These definitions are equivalent in the realm of classical mathematics.
The reals are uncountable; that is: while both the set of all natural numbers and the set of all real numbers are infinite sets, there can be no one-to-one function from the real numbers to the natural numbers: the cardinality of the set of all real numbers (denoted formula_1 and called cardinality of the continuum) is strictly greater than the cardinality of the set of all natural numbers (denoted formula_2). The statement that there is no subset of the reals with cardinality strictly greater than formula_2 and strictly smaller than formula_1 is known as the continuum hypothesis (CH). It is known to be neither provable nor refutable using the axioms of Zermelo–Fraenkel set theory (ZFC), the standard foundation of modern mathematics, in the sense that some models of ZFC satisfy CH, while others violate it.
History.
Simple fractions have been used by the Egyptians around 1000 BC; the Vedic "Sulba Sutras" ("The rules of chords") in, , include what may be the first "use" of irrational numbers. The concept of irrationality was implicitly accepted by early Indian mathematicians since Manava , who were aware that the square roots of certain numbers such as 2 and 61 could not be exactly determined. Around 500 BC, the Greek mathematicians led by Pythagoras realized the need for irrational numbers, in particular the irrationality of the square root of 2.
The Middle Ages brought the acceptance of zero, negative, integral, and fractional numbers, first by Indian and Chinese mathematicians, and then by Arabic mathematicians, who were also the first to treat irrational numbers as algebraic objects, which was made possible by the development of algebra. Arabic mathematicians merged the concepts of "number" and "magnitude" into a more general idea of real numbers. The Egyptian mathematician Abū Kāmil Shujā ibn Aslam was the first to accept irrational numbers as solutions to quadratic equations or as coefficients in an equation, often in the form of square roots, cube roots and fourth roots.
In the 16th century, Simon Stevin created the basis for modern decimal notation, and insisted that there is no difference between rational and irrational numbers in this regard.
In the 17th century, Descartes introduced the term "real" to describe roots of a polynomial, distinguishing them from "imaginary" ones.
In the 18th and 19th centuries, there was much work on irrational and transcendental numbers. Johann Heinrich Lambert (1761) gave the first flawed proof that cannot be rational; Adrien-Marie Legendre (1794) completed the proof, and showed that is not the square root of a rational number. Paolo Ruffini (1799) and Niels Henrik Abel (1842) both constructed proofs of the Abel–Ruffini theorem: that the general quintic or higher equations cannot be solved by a general formula involving only arithmetical operations and roots.
Évariste Galois (1832) developed techniques for determining whether a given equation could be solved by radicals, which gave rise to the field of Galois theory. Joseph Liouville (1840) showed that neither "e" nor "e"2 can be a root of an integer quadratic equation, and then established the existence of transcendental numbers; Georg Cantor (1873) extended and greatly simplified this proof. Charles Hermite (1873) first proved that "e" is transcendental, and Ferdinand von Lindemann (1882), showed that is transcendental. Lindemann's proof was much simplified by Weierstrass (1885), still further by David Hilbert (1893), and has finally been made elementary by Adolf Hurwitz and Paul Gordan.
The development of calculus in the 18th century used the entire set of real numbers without having defined them cleanly. The first rigorous definition was given by Georg Cantor in 1871. In 1874, he showed that the set of all real numbers is uncountably infinite but the set of all algebraic numbers is countably infinite. Contrary to widely held beliefs, his first method was not his famous diagonal argument, which he published in 1891. See Cantor's first uncountability proof.
Definition.
The real number system formula_5 can be defined axiomatically up to an isomorphism, which is described hereafter. There are also many ways to construct "the" real number system, for example, starting from natural numbers, then defining rational numbers algebraically, and finally defining real numbers as equivalence classes of their Cauchy sequences or as Dedekind cuts, which are certain subsets of rational numbers. Another possibility is to start from some rigorous axiomatization of Euclidean geometry (Hilbert, Tarski, etc.) and then define the real number system geometrically. From the structuralist point of view all these constructions are on equal footing.
Axiomatic approach.
Let ℝ denote the set of all real numbers. Then:
The last property is what differentiates the reals from the rationals. For example, the set of rationals with square less than 2 has a rational upper bound (e.g., 1.5) but no rational least upper bound, because the square root of 2 is not rational.
The real numbers are uniquely specified by the above properties. More precisely, given any two Dedekind-complete ordered fields ℝ1 and ℝ2, there exists a unique field isomorphism from ℝ1 to ℝ2, allowing us to think of them as essentially the same mathematical object.
For another axiomatization of ℝ, see Tarski's axiomatization of the reals.
Construction from the rational numbers.
The real numbers can be constructed as a completion of the rational numbers in such a way that a sequence defined by a decimal or binary expansion like (3; 3.1; 3.14; 3.141; 3.1415; …) converges to a unique real number, in this case . For details and other constructions of real numbers, see construction of the real numbers.
Properties.
Basic properties.
A real number may be either rational or irrational; either algebraic or transcendental; and either positive, negative, or zero. Real numbers are used to measure continuous quantities. They may be expressed by decimal representations that have an infinite sequence of digits to the right of the decimal point; these are often represented in the same form as 324.823122147… The ellipsis (three dots) indicates that there would still be more digits to come.
More formally, real numbers have the two basic properties of being an ordered field, and having the least upper bound property. The first says that real numbers comprise a field, with addition and multiplication as well as division by non-zero numbers, which can be totally ordered on a number line in a way compatible with addition and multiplication. The second says that, if a non-empty set of real numbers has an upper bound, then it has a real least upper bound. The second condition distinguishes the real numbers from the rational numbers: for example, the set of rational numbers whose square is less than 2 is a set with an upper bound (e.g. 1.5) but no (rational) least upper bound: hence the rational numbers do not satisfy the least upper bound property.
Completeness.
A main reason for using real numbers is that the reals contain all limits. More precisely, every sequence of real numbers having the property that consecutive terms of the sequence become arbitrarily close to each other necessarily has the property that after some term in the sequence the remaining terms are arbitrarily close to some specific real number. In mathematical terminology, this means that the reals are complete (in the sense of metric spaces or uniform spaces, which is a different sense than the Dedekind completeness of the order in the previous section). This is formally defined in the following way:
A sequence ("x""n") of real numbers is called a "Cauchy sequence" if for any there exists an integer "N" (possibly depending on ε) such that the distance is less than ε for all "n" and "m" that are both greater than "N". In other words, a sequence is a Cauchy sequence if its elements "x""n" eventually come and remain arbitrarily close to each other.
A sequence ("x""n") "converges to the limit" "x" if for any there exists an integer "N" (possibly depending on ε) such that the distance is less than ε provided that "n" is greater than "N". In other words, a sequence has limit "x" if its elements eventually come and remain arbitrarily close to "x".
Notice that every convergent sequence is a Cauchy sequence. The converse is also true: 
That is: the reals are complete.
Note that the rationals are not complete. For example, the sequence (1; 1.4; 1.41; 1.414; 1.4142; 1.41421…), where each term adds a digit of the decimal expansion of the positive square root of 2, is Cauchy but it does not converge to a rational number. (In the real numbers, in contrast, it converges to the positive square root of 2.)
The existence of limits of Cauchy sequences is what makes calculus work and is of great practical use. The standard numerical test to determine if a sequence has a limit is to test if it is a Cauchy sequence, as the limit is typically not known in advance.
For example, the standard series of the exponential function
converges to a real number because for every "x" the sums
can be made arbitrarily small by choosing "N" sufficiently large. This proves that the sequence is Cauchy, so we know that the sequence converges even if the limit is not known in advance.
"The complete ordered field".
The real numbers are often described as "the complete ordered field", a phrase that can be interpreted in several ways.
First, an order can be lattice-complete. It is easy to see that no ordered field can be lattice-complete, because it can have no largest element (given any element "z", is larger), so this is not the sense that is meant.
Additionally, an order can be Dedekind-complete, as defined in the section Axioms. The uniqueness result at the end of that section justifies using the word "the" in the phrase "complete ordered field" when this is the sense of "complete" that is meant. This sense of completeness is most closely related to the construction of the reals from Dedekind cuts, since that construction starts from an ordered field (the rationals) and then forms the Dedekind-completion of it in a standard way.
These two notions of completeness ignore the field structure. However, an ordered group (in this case, the additive group of the field) defines a uniform structure, and uniform structures have a notion of completeness (topology); the description in the previous section Completeness is a special case. (We refer to the notion of completeness in uniform spaces rather than the related and better known notion for metric spaces, since the definition of metric space relies on already having a characterization of the real numbers.) It is not true that R is the "only" uniformly complete ordered field, but it is the only uniformly complete "Archimedean field", and indeed one often hears the phrase "complete Archimedean field" instead of "complete ordered field". Every uniformly complete Archimedean field must also be Dedekind-complete (and vice versa, of course), justifying using "the" in the phrase "the complete Archimedean field". This sense of completeness is most closely related to the construction of the reals from Cauchy sequences (the construction carried out in full in this article), since it starts with an Archimedean field (the rationals) and forms the uniform completion of it in a standard way.
But the original use of the phrase "complete Archimedean field" was by David Hilbert, who meant still something else by it. He meant that the real numbers form the "largest" Archimedean field in the sense that every other Archimedean field is a subfield of R. Thus R is "complete" in the sense that nothing further can be added to it without making it no longer an Archimedean field. This sense of completeness is most closely related to the construction of the reals from surreal numbers, since that construction starts with a proper class that contains every ordered field (the surreals) and then selects from it the largest Archimedean subfield.
Advanced properties.
The reals are uncountable; that is: there are strictly more real numbers than natural numbers, even though both sets are infinite. In fact, the cardinality of the reals equals that of the set of subsets (i.e. the power set) of the natural numbers, and Cantor's diagonal argument states that the latter set's cardinality is strictly greater than the cardinality of N. Since the set of algebraic numbers is countable, almost all real numbers are transcendental. The non-existence of a subset of the reals with cardinality strictly between that of the integers and the reals is known as the continuum hypothesis. The continuum hypothesis can neither be proved nor be disproved; it is independent from the axioms of set theory.
As a topological space, the real numbers are separable. This is because the set of rationals, which is countable, is dense in the real numbers. The irrational numbers are also dense in the real numbers, however they are uncountable and have the same cardinality as the reals.
The real numbers form a metric space: the distance between "x" and "y" is defined as the absolute value . By virtue of being a totally ordered set, they also carry an order topology; the topology arising from the metric and the one arising from the order are identical, but yield different presentations for the topology – in the order topology as ordered intervals, in the metric topology as epsilon-balls. The Dedekind cuts construction uses the order topology presentation, while the Cauchy sequences construction uses the metric topology presentation. The reals are a contractible (hence connected and simply connected), separable and complete metric space of Hausdorff dimension 1. The real numbers are locally compact but not compact. There are various properties that uniquely specify them; for instance, all unbounded, connected, and separable order topologies are necessarily homeomorphic to the reals.
Every nonnegative real number has a square root in R, although no negative number does. This shows that the order on R is determined by its algebraic structure. Also, every polynomial of odd degree admits at least one real root: these two properties make R the premier example of a real closed field. Proving this is the first half of one proof of the fundamental theorem of algebra.
The reals carry a canonical measure, the Lebesgue measure, which is the Haar measure on their structure as a topological group normalized such that the unit interval [0;1] has measure 1. There exist sets of real numbers that are not Lebesgue measurable, e.g. Vitali sets.
The supremum axiom of the reals refers to subsets of the reals and is therefore a second-order logical statement. It is not possible to characterize the reals with first-order logic alone: the Löwenheim–Skolem theorem implies that there exists a countable dense subset of the real numbers satisfying exactly the same sentences in first-order logic as the real numbers themselves. The set of hyperreal numbers satisfies the same first order sentences as R. Ordered fields that satisfy the same first-order sentences as R are called nonstandard models of R. This is what makes nonstandard analysis work; by proving a first-order statement in some nonstandard model (which may be easier than proving it in R), we know that the same statement must also be true of R.
The field R of real numbers is an extension field of the field Q of rational numbers, and R can therefore be seen as a vector space over Q. Zermelo–Fraenkel set theory with the axiom of choice guarantees the existence of a basis of this vector space: there exists a set "B" of real numbers such that every real number can be written uniquely as a finite linear combination of elements of this set, using rational coefficients only, and such that no element of "B" is a rational linear combination of the others. However, this existence theorem is purely theoretical, as such a base has never been explicitly described.
The well-ordering theorem implies that the real numbers can be well-ordered if the axiom of choice is assumed: there exists a total order on R with the property that every non-empty subset of R has a least element in this ordering. (The standard ordering ≤ of the real numbers is not a well-ordering since e.g. an open interval does not contain a least element in this ordering.) Again, the existence of such a well-ordering is purely theoretical, as it has not been explicitly described. If V=L is assumed in addition to the axioms of ZF, a well ordering of the real numbers can be shown to be explicitly definable by a formula.
Applications and connections to other areas.
Real numbers and logic.
The real numbers are most often formalized using the Zermelo–Fraenkel axiomatization of set theory, but some mathematicians study the real numbers with other logical foundations of mathematics. In particular, the real numbers are also studied in reverse mathematics and in constructive mathematics.
The hyperreal numbers as developed by Edwin Hewitt, Abraham Robinson and others extend the set of the real numbers by introducing infinitesimal and infinite numbers, allowing for building infinitesimal calculus in a way closer to the original intuitions of Leibniz, Euler, Cauchy and others.
Edward Nelson's internal set theory enriches the Zermelo–Fraenkel set theory syntactically by introducing a unary predicate "standard". In this approach, infinitesimals are (non-"standard") elements of the set of the real numbers (rather than being elements of an extension thereof, as in Robinson's theory).
The continuum hypothesis posits that the cardinality of the set of the real numbers is formula_8; i.e. the smallest infinite cardinal number after formula_2, the cardinality of the integers. Paul Cohen proved in 1963 that it is an axiom independent of the other axioms of set theory; that is: one may choose either the continuum hypothesis or its negation as an axiom of set theory, without contradiction.
In physics.
In the physical sciences, most physical constants such as the universal gravitational constant, and physical variables, such as position, mass, speed, and electric charge, are modeled using real numbers. In fact, the fundamental physical theories such as classical mechanics, electromagnetism, quantum mechanics, general relativity and the standard model are described using mathematical structures, typically smooth manifolds or Hilbert spaces, that are based on the real numbers, although actual measurements of physical quantities are of finite accuracy and precision.
In some recent developments of theoretical physics stemming from the holographic principle, the Universe is seen fundamentally as an information store, essentially zeroes and ones, organized in much less geometrical fashion and manifesting itself as space-time and particle fields only on a more superficial level. This approach removes the real number system from its foundational role in physics and even prohibits the existence of infinite precision real numbers in the physical universe by considerations based on the Bekenstein bound.
In computation.
With some exceptions, most calculators do not operate on real numbers. Instead, they work with finite-precision approximations called floating-point numbers. In fact, most scientific computation uses floating-point arithmetic. Real numbers satisfy the usual rules of arithmetic, but floating-point numbers do not.
Computers cannot directly store arbitrary real numbers with infinitely many digits.
The precision is limited by the number of bits allocated to store a number, whether as floating-point numbers or arbitrary precision numbers. However, computer algebra systems can operate on irrational quantities exactly by manipulating formulas for them (such as formula_10, formula_11, orformula_12) rather than their rational or decimal approximation; however, it is not in general possible to determine whether two such expressions are equal (the constant problem).
A real number is called "computable" if there exists an algorithm that yields its digits. Because there are only countably many algorithms, but an uncountable number of reals, almost all real numbers fail to be computable. Moreover, the equality of two computable numbers is an undecidable problem. Some constructivists accept the existence of only those reals that are computable. The set of definable numbers is broader, but still only countable.
"Reals" in set theory.
In set theory, specifically descriptive set theory, the Baire space is used as a surrogate for the real numbers since the latter have some topological properties (connectedness) that are a technical inconvenience. Elements of Baire space are referred to as "reals".
Vocabulary and notation.
Mathematicians use the symbol R, or, alternatively, ℝ, the letter "R" in blackboard bold (encoded in Unicode as ), to represent the set of all real numbers. As this set is naturally endowed with the structure of a field, the expression "field of real numbers" is frequently used when its algebraic properties are under consideration.
The sets of positive real numbers and negative real numbers are often noted R+ and R−, respectively; R+ and R− are also used. The non-negative real numbers can be noted R≥0 but one often sees this set noted R+ ∪ {0}. In French mathematics, the "positive real numbers" and "negative real numbers" commonly include zero, and these sets are noted respectively ℝ+ and ℝ−. In this understanding, the respective sets without zero are called strictly positive real numbers and strictly negative real numbers, and are noted ℝ+* and ℝ−*.
The notation R"n" refers to the cartesian product of "n" copies of R, which is an "n"-dimensional vector space over the field of the real numbers; this vector space may be identified to the "n"-dimensional space of Euclidean geometry as soon as a coordinate system has been chosen in the latter. For example, a value from R3 consists of three real numbers and specifies the coordinates of a point in 3‑dimensional space.
In mathematics, "real" is used as an adjective, meaning that the underlying field is the field of the real numbers (or "the real field"). For example, "real matrix", "real polynomial" and "real Lie algebra". The word is also used as a noun, meaning a real number (as in "the set of all reals").
Generalizations and extensions.
The real numbers can be generalized and extended in several different directions:

</doc>
<doc id="70048" url="https://en.wikipedia.org/wiki?curid=70048" title="Rectangle">
Rectangle

In Euclidean plane geometry, a rectangle is any quadrilateral with four right angles. It can also be defined as an equiangular quadrilateral, since equiangular means that all of its angles are equal (360°/4 = 90°). It can also be defined as a parallelogram containing a right angle. A rectangle with four sides of equal length is a square. The term oblong is occasionally used to refer to a non-square rectangle.
A rectangle with vertices "ABCD" would be denoted as .
The word rectangle comes from the Latin "rectangulus", which is a combination of "rectus" (right) and "angulus" (angle).
A so-called crossed rectangle is a crossed (self-intersecting) quadrilateral which consists of two opposite sides of a rectangle along with the two diagonals.
It is a special case of an antiparallelogram, and its angles are not right angles. Other geometries, such as spherical, elliptic, and hyperbolic, have so-called rectangles with opposite sides equal in length and equal angles that are not right angles.
Rectangles are involved in many tiling problems, such as tiling the plane by rectangles or tiling a rectangle by polygons.
Characterizations.
A convex quadrilateral is a rectangle if and only if it is any one of the following:
Classification.
Traditional hierarchy.
A rectangle is a special case of a parallelogram in which each pair of adjacent sides is perpendicular.
A parallelogram is a special case of a trapezium (known as a trapezoid in North America) in which "both" pairs of opposite sides are parallel and equal in length.
A trapezium is a convex quadrilateral which has at least one pair of parallel opposite sides.
A convex quadrilateral is
Alternative hierarchy.
De Villiers defines a rectangle more generally as any quadrilateral with axes of symmetry through each pair of opposite sides.
This definition includes both right-angled rectangles and crossed rectangles. Each has an axis of symmetry parallel to and equidistant from a pair of opposite sides, and another which is the perpendicular bisector of those sides, but, in the case of the crossed rectangle, the first axis is not an axis of symmetry for either side that it bisects.
Quadrilaterals with two axes of symmetry, each through a pair of opposite sides, belong to the larger class of quadrilaterals with at least one axis of symmetry through a pair of opposite sides. These quadrilaterals comprise isosceles trapezia and crossed isosceles trapezia (crossed quadrilaterals with the same vertex arrangement as isosceles trapezia).
Properties.
Symmetry.
A rectangle is cyclic: all corners lie on a single circle.
It is equiangular: all its corner angles are equal (each of 90 degrees).
It is isogonal or vertex-transitive: all corners lie within the same symmetry orbit.
It has two lines of reflectional symmetry and rotational symmetry of order 2 (through 180°).
Rectangle-rhombus duality.
The dual polygon of a rectangle is a rhombus, as shown in the table below.
Miscellaneous.
The two diagonals are equal in length and bisect each other. Every quadrilateral with both these properties is a rectangle.
A rectangle is rectilinear: its sides meet at right angles.
A rectangle in the plane can be defined by five independent degrees of freedom consisting, for example, of three for position (comprising two of translation and one of rotation), one for shape (aspect ratio), and one for overall size (area).
Two rectangles, neither of which will fit inside the other, are said to be incomparable.
Formulae.
If a rectangle has length formula_3 and width formula_4
Theorems.
The isoperimetric theorem for rectangles states that among all rectangles of a given perimeter, the square has the largest area.
The midpoints of the sides of any quadrilateral with perpendicular diagonals form a rectangle.
A parallelogram with equal diagonals is a rectangle.
The Japanese theorem for cyclic quadrilaterals
states that the incentres of the four triangles determined by the vertices of a cyclic quadrilateral taken three at a time form a rectangle.
The British flag theorem states that with vertices denoted "A", "B", "C", and "D", for any point "P" on the same plane of a rectangle:
For every convex body "C" in the plane, we can inscribe a rectangle "r" in "C" such that a homothetic copy "R" of "r" is circumscribed about "C" and the positive homothety ratio is at most 2 and formula_10.
Crossed rectangles.
A crossed (self-intersecting) quadrilateral consists of two opposite sides of a non-self-intersecting quadrilateral along with the two diagonals. Similarly, a crossed rectangle is a crossed quadrilateral which consists of two opposite sides of a rectangle along with the two diagonals. It has the same vertex arrangement as the rectangle. It appears as two identical triangles with a common vertex, but the geometric intersection is not considered a vertex.
A crossed quadrilateral is sometimes likened to a bow tie or butterfly. A three-dimensional rectangular wire frame that is twisted can take the shape of a bow tie. A crossed rectangle is sometimes called an "angular eight".
The interior of a crossed rectangle can have a polygon density of ±1 in each triangle, dependent upon the winding orientation as clockwise or counterclockwise.
A crossed rectangle is not equiangular. The sum of its interior angles (two acute and two reflex), as with any crossed quadrilateral, is 720°.
A rectangle and a crossed rectangle are quadrilaterals with the following properties in common:
Other rectangles.
In spherical geometry, a spherical rectangle is a figure whose four edges are great circle arcs which meet at equal angles greater than 90°. Opposite arcs are equal in length. The surface of a sphere in Euclidean solid geometry is a non-Euclidean surface in the sense of elliptic geometry. Spherical geometry is the simplest form of elliptic geometry.
In elliptic geometry, an elliptic rectangle is a figure in the elliptic plane whose four edges are elliptic arcs which meet at equal angles greater than 90°. Opposite arcs are equal in length.
In hyperbolic geometry, a hyperbolic rectangle is a figure in the hyperbolic plane whose four edges are hyperbolic arcs which meet at equal angles less than 90°. Opposite arcs are equal in length.
Tessellations.
The rectangle is used in many periodic tessellation patterns, in brickwork, for example, these tilings:
Squared, perfect, and other tiled rectangles.
A rectangle tiled by squares, rectangles, or triangles is said to be a "squared", "rectangled", or "triangulated" (or "triangled") rectangle respectively. The tiled rectangle is
"perfect"
if the tiles are similar and finite in number and no two tiles are the same size. If two such tiles are the same size, the tiling is "imperfect". In a perfect (or imperfect) triangled rectangle the triangles must be right triangles.
A rectangle has commensurable sides if and only if it is tileable by a finite number of unequal squares.
The same is true if the tiles are unequal isosceles right triangles.
The tilings of rectangles by other tiles which have attracted the most attention are those by congruent non-rectangular polyominoes, allowing all rotations and reflections. There are also tilings by congruent polyaboloes.

</doc>
<doc id="26262" url="https://en.wikipedia.org/wiki?curid=26262" title="Redshift">
Redshift

In physics, redshift happens when light or other electromagnetic radiation from an object is increased in wavelength, or shifted to the red end of the spectrum. In general, whether or not the radiation is within the visible spectrum, "redder" means an increase in wavelength – equivalent to a lower frequency and a lower photon energy, in accordance with, respectively, the wave and quantum theories of light.
Some redshifts are an example of the Doppler effect, familiar in the change of apparent pitches of sirens and frequency of the sound waves emitted by speeding vehicles. A redshift occurs whenever a light source moves away from an observer. Another kind of redshift is cosmological redshift, which is due to the expansion of the universe, and sufficiently distant light sources (generally more than a few million light years away) show redshift corresponding to the rate of increase in their distance from Earth. Finally, gravitational redshift is a relativistic effect observed in electromagnetic radiation moving out of gravitational fields. Conversely, a "decrease" in wavelength is called blueshift and is generally seen when a light-emitting object moves toward an observer or when electromagnetic radiation moves into a gravitational field. However, redshift is a more common term and sometimes blueshift is referred to as negative redshift.
Knowledge of redshifts and blueshifts has been applied to develop several terrestrial technologies such as Doppler radar and radar guns. Redshifts are also seen in the spectroscopic observations of astronomical objects. Its value is represented by the letter "z."
A special relativistic redshift formula (and its classical approximation) can be used to calculate the redshift of a nearby object when spacetime is flat. However, in many contexts, such as black holes and Big Bang cosmology, redshifts must be calculated using general relativity. Special relativistic, gravitational, and cosmological redshifts can be understood under the umbrella of frame transformation laws. There exist other physical processes that can lead to a shift in the frequency of electromagnetic radiation, including scattering and optical effects; however, the resulting changes are distinguishable from true redshift and are not generally referred to as such (see section on physical optics and radiative transfer).
History.
The history of the subject began with the development in the 19th century of wave mechanics and the exploration of phenomena associated with the Doppler effect. The effect is named after Christian Doppler, who offered the first known physical explanation for the phenomenon in 1842. The hypothesis was tested and confirmed for sound waves by the Dutch scientist Christophorus Buys Ballot in 1845. Doppler correctly predicted that the phenomenon should apply to all waves, and in particular suggested that the varying colors of stars could be attributed to their motion with respect to the Earth. Before this was verified, however, it was found that stellar colors were primarily due to a star's temperature, not motion. Only later was Doppler vindicated by verified redshift observations.
The first Doppler redshift was described by French physicist Hippolyte Fizeau in 1848, who pointed to the shift in spectral lines seen in stars as being due to the Doppler effect. The effect is sometimes called the "Doppler–Fizeau effect". In 1868, British astronomer William Huggins was the first to determine the velocity of a star moving away from the Earth by this method. In 1871, optical redshift was confirmed when the phenomenon was observed in Fraunhofer lines using solar rotation, about 0.1 Å in the red.
In 1887, Vogel and Scheiner discovered the "annual Doppler effect", the yearly change in the Doppler shift of stars located near the ecliptic due to the orbital velocity of the Earth. In 1901, Aristarkh Belopolsky verified optical redshift in the laboratory using a system of rotating mirrors.
The earliest occurrence of the term "red-shift" in print (in this hyphenated form) appears to be by American astronomer Walter S. Adams in 1908, in which he mentions "Two methods of investigating that nature of the nebular red-shift". The word does not appear unhyphenated until about 1934 by Willem de Sitter, perhaps indicating that up to that point its German equivalent, "Rotverschiebung", was more commonly used.
Beginning with observations in 1912, Vesto Slipher discovered that most spiral galaxies, then mostly thought to be spiral nebulae, had considerable redshifts. Slipher first reports on his measurement in the inaugural volume of the "Lowell Observatory Bulletin". Three years later, he wrote a review in the journal "Popular Astronomy". In it he states, "[...] the early discovery that the great Andromeda spiral had the quite exceptional velocity of –300 km(/s) showed the means then available, capable of investigating not only the spectra of the spirals but their velocities as well." Slipher reported the velocities for 15 spiral nebulae spread across the entire celestial sphere, all but three having observable "positive" (that is recessional) velocities. Subsequently, Edwin Hubble discovered an approximate relationship between the redshifts of such "nebulae" and the distances to them with the formulation of his eponymous Hubble's law. These observations corroborated Alexander Friedmann's 1922 work, in which he derived the famous Friedmann equations. They are today considered strong evidence for an expanding universe and the Big Bang theory.
Measurement, characterization, and interpretation.
The spectrum of light that comes from a single source (see idealized spectrum illustration top-right) can be measured. To determine the redshift, one searches for features in the spectrum such as absorption lines, emission lines, or other variations in light intensity. If found, these features can be compared with known features in the spectrum of various chemical compounds found in experiments where that compound is located on Earth. A very common atomic element in space is hydrogen. The spectrum of originally featureless light shone through hydrogen will show a signature spectrum specific to hydrogen that has features at regular intervals. If restricted to absorption lines it would look similar to the illustration (top right). If the same pattern of intervals is seen in an observed spectrum from a distant source but occurring at shifted wavelengths, it can be identified as hydrogen too. If the same spectral line is identified in both spectra—but at different wavelengths—then the redshift can be calculated using the table below. Determining the redshift of an object in this way requires a frequency- or wavelength-range. In order to calculate the redshift one has to know the wavelength of the emitted light in the rest frame of the source, in other words, the wavelength that would be measured by an observer located adjacent to and comoving with the source. Since in astronomical applications this measurement cannot be done directly, because that would require travelling to the distant star of interest, the method using spectral lines described here is used instead. Redshifts cannot be calculated by looking at unidentified features whose rest-frame frequency is unknown, or with a spectrum that is featureless or white noise (random fluctuations in a spectrum).
Redshift (and blueshift) may be characterized by the relative difference between the observed and emitted wavelengths (or frequency) of an object. In astronomy, it is customary to refer to this change using a dimensionless quantity called . If represents wavelength and represents frequency (note, where is the speed of light), then is defined by the equations:
After is measured, the distinction between redshift and blueshift is simply a matter of whether is positive or negative. See the formula section below for some basic interpretations that follow when either a redshift or blueshift is observed. For example, Doppler effect blueshifts () are associated with objects approaching (moving closer to) the observer with the light shifting to greater energies. Conversely, Doppler effect redshifts () are associated with objects receding (moving away) from the observer with the light shifting to lower energies. Likewise, gravitational blueshifts are associated with light emitted from a source residing within a weaker gravitational field as observed from within a stronger gravitational field, while gravitational redshifting implies the opposite conditions.
Redshift formulae.
In general relativity one can derive several important special-case formulae for redshift in certain special spacetime geometries, as summarized in the following table. In all cases the magnitude of the shift (the value of ) is independent of the wavelength.
Doppler effect.
If a source of the light is moving away from an observer, then redshift () occurs; if the source moves towards the observer, then blueshift () occurs. This is true for all electromagnetic waves and is explained by the Doppler effect. Consequently, this type of redshift is called the "Doppler redshift". If the source moves away from the observer with velocity , which is much less than the speed of light (), the redshift is given by
where is the speed of light. In the classical Doppler effect, the frequency of the source is not modified, but the recessional motion causes the illusion of a lower frequency.
A more complete treatment of the Doppler redshift requires considering relativistic effects associated with motion of sources close to the speed of light. A complete derivation of the effect can be found in the article on the relativistic Doppler effect. In brief, objects moving close to the speed of light will experience deviations from the above formula due to the time dilation of special relativity which can be corrected for by introducing the Lorentz factor into the classical Doppler formula as follows (for motion solely in the line of sight):
This phenomenon was first observed in a 1938 experiment performed by Herbert E. Ives and G.R. Stilwell, called the Ives–Stilwell experiment.
Since the Lorentz factor is dependent only on the magnitude of the velocity, this causes the redshift associated with the relativistic correction to be independent of the orientation of the source movement. In contrast, the classical part of the formula is dependent on the projection of the movement of the source into the line-of-sight which yields different results for different orientations. If is the angle between the direction of relative motion and the direction of emission in the observer's frame (zero angle is directly away from the observer), the full form for the relativistic Doppler effect becomes:
and for motion solely in the line of sight (), this equation reduces to:
For the special case that the light is approaching at right angles () to the direction of relative motion in the observer's frame, the relativistic redshift is known as the transverse redshift, and a redshift:
is measured, even though the object is not moving away from the observer. Even when the source is moving towards the observer, if there is a transverse component to the motion then there is some speed at which the dilation just cancels the expected blueshift and at higher speed the approaching source will be redshifted.
Expansion of space.
In the early part of the twentieth century, Slipher, Hubble and others made the first measurements of the redshifts and blueshifts of galaxies beyond the Milky Way. They initially interpreted these redshifts and blueshifts as due solely to the Doppler effect, but later Hubble discovered a rough correlation between the increasing redshifts and the increasing distance of galaxies. Theorists almost immediately realized that these observations could be explained by a different mechanism for producing redshifts. Hubble's law of the correlation between redshifts and distances is required by models of cosmology derived from general relativity that have a metric expansion of space. As a result, photons propagating through the expanding space are stretched, creating the cosmological redshift.
There is a distinction between a redshift in cosmological context as compared to that witnessed when nearby objects exhibit a local Doppler-effect redshift. Rather than cosmological redshifts being a consequence of relative velocities, the photons instead increase in wavelength and redshift because of a feature of the spacetime through which they are traveling that causes space to expand. Due to the expansion increasing as distances increase, the distance between two remote galaxies can increase at more than 3 m/s, but this does not imply that the galaxies move faster than the speed of light at their present location (which is forbidden by Lorentz covariance).
Mathematical derivation.
The observational consequences of this effect can be derived using the equations from general relativity that describe a homogeneous and isotropic universe.
To derive the redshift effect, use the geodesic equation for a light wave, which is
where
For an observer observing the crest of a light wave at a position and time , the crest of the light wave was emitted at a time in the past and a distant position . Integrating over the path in both space and time that the light wave travels yields:
In general, the wavelength of light is not the same for the two positions and times considered due to the changing properties of the metric. When the wave was emitted, it had a wavelength . The next crest of the light wave was emitted at a time
The observer sees the next crest of the observed light wave with a wavelength to arrive at a time
Since the subsequent crest is again emitted from and is observed at , the following equation can be written:
The right-hand side of the two integral equations above are identical which means
Using the following manipulation:
we find that:
For very small variations in time (over the period of one cycle of a light wave) the scale factor is essentially a constant ( today and previously). This yields
which can be rewritten as
Using the definition of redshift provided above, the equation
is obtained. In an expanding universe such as the one we inhabit, the scale factor is monotonically increasing as time passes, thus, is positive and distant galaxies appear redshifted.
Using a model of the expansion of the Universe, redshift can be related to the age of an observed object, the so-called "cosmic time–redshift relation". Denote a density ratio as :
with the critical density demarcating a universe that eventually crunches from one that simply expands. This density is about three hydrogen atoms per thousand liters of space. At large redshifts one finds:
where is the present-day Hubble constant, and is the redshift.
Distinguishing between cosmological and local effects.
For cosmological redshifts of additional Doppler redshifts and blueshifts due to the peculiar motions of the galaxies relative to one another cause a wide scatter from the standard Hubble Law. The resulting situation can be illustrated by the Expanding Rubber Sheet Universe, a common cosmological analogy used to describe the expansion of space. If two objects are represented by ball bearings and spacetime by a stretching rubber sheet, the Doppler effect is caused by rolling the balls across the sheet to create peculiar motion. The cosmological redshift occurs when the ball bearings are stuck to the sheet and the sheet is stretched.
The redshifts of galaxies include both a component related to recessional velocity from expansion of the Universe, and a component related to peculiar motion (Doppler shift). The redshift due to expansion of the Universe depends upon the recessional velocity in a fashion determined by the cosmological model chosen to describe the expansion of the Universe, which is very different from how Doppler redshift depends upon local velocity. Describing the cosmological expansion origin of redshift, cosmologist Edward Robert Harrison said, "Light leaves a galaxy, which is stationary in its local region of space, and is eventually received by observers who are stationary in their own local region of space. Between the galaxy and the observer, light travels through vast regions of expanding space. As a result, all wavelengths of the light are stretched by the expansion of space. It is as simple as that..." Steven Weinberg clarified, "The increase of wavelength from emission to absorption of light does not depend on the rate of change of [here is the Robertson-Walker scale factor] at the times of emission or absorption, but on the increase of in the whole period from emission to absorption."
Popular literature often uses the expression "Doppler redshift" instead of "cosmological redshift" to describe the redshift of galaxies dominated by the expansion of spacetime, but the cosmological redshift is not found using the relativistic Doppler equation which is instead characterized by special relativity; thus is impossible while, in contrast, is possible for cosmological redshifts because the space which separates the objects (for example, a quasar from the Earth) can expand faster than the speed of light. More mathematically, the viewpoint that "distant galaxies are receding" and the viewpoint that "the space between galaxies is expanding" are related by changing coordinate systems. Expressing this precisely requires working with the mathematics of the Friedmann-Robertson-Walker metric.
If the Universe were contracting instead of expanding, we would see distant galaxies blueshifted by an amount proportional to their distance instead of redshifted.
Gravitational redshift.
In the theory of general relativity, there is time dilation within a gravitational well. This is known as the gravitational redshift or "Einstein Shift". The theoretical derivation of this effect follows from the Schwarzschild solution of the Einstein equations which yields the following formula for redshift associated with a photon traveling in the gravitational field of an uncharged, nonrotating, spherically symmetric mass:
where
This gravitational redshift result can be derived from the assumptions of special relativity and the equivalence principle; the full theory of general relativity is not required.
The effect is very small but measurable on Earth using the Mössbauer effect and was first observed in the Pound–Rebka experiment. However, it is significant near a black hole, and as an object approaches the event horizon the red shift becomes infinite. It is also the dominant cause of large angular-scale temperature fluctuations in the cosmic microwave background radiation (see Sachs-Wolfe effect).
Observations in astronomy.
The redshift observed in astronomy can be measured because the emission and absorption spectra for atoms are distinctive and well known, calibrated from spectroscopic experiments in laboratories on Earth. When the redshift of various absorption and emission lines from a single astronomical object is measured, is found to be remarkably constant. Although distant objects may be slightly blurred and lines broadened, it is by no more than can be explained by thermal or mechanical motion of the source. For these reasons and others, the consensus among astronomers is that the redshifts they observe are due to some combination of the three established forms of Doppler-like redshifts. Alternative hypotheses and explanations for redshift such as tired light are not generally considered plausible.
Spectroscopy, as a measurement, is considerably more difficult than simple photometry, which measures the brightness of astronomical objects through certain filters. When photometric data is all that is available (for example, the Hubble Deep Field and the Hubble Ultra Deep Field), astronomers rely on a technique for measuring photometric redshifts. Due to the broad wavelength ranges in photometric filters and the necessary assumptions about the nature of the spectrum at the light-source, errors for these sorts of measurements can range up to , and are much less reliable than spectroscopic determinations. However, photometry does at least allow a qualitative characterization of a redshift. For example, if a Sun-like spectrum had a redshift of , it would be brightest in the infrared rather than at the yellow-green color associated with the peak of its blackbody spectrum, and the light intensity will be reduced in the filter by a factor of four, . Both the photon count rate and the photon energy are redshifted. (See K correction for more details on the photometric consequences of redshift.)
Local observations.
In nearby objects (within our Milky Way galaxy) observed redshifts are almost always related to the line-of-sight velocities associated with the objects being observed. Observations of such redshifts and blueshifts have enabled astronomers to measure velocities and parametrize the masses of the orbiting stars in spectroscopic binaries, a method first employed in 1868 by British astronomer William Huggins. Similarly, small redshifts and blueshifts detected in the spectroscopic measurements of individual stars are one way astronomers have been able to diagnose and measure the presence and characteristics of planetary systems around other stars and have even made very detailed differential measurements of redshifts during planetary transits to determine precise orbital parameters. Finely detailed measurements of redshifts are used in helioseismology to determine the precise movements of the photosphere of the Sun. Redshifts have also been used to make the first measurements of the rotation rates of planets, velocities of interstellar clouds, the rotation of galaxies, and the dynamics of accretion onto neutron stars and black holes which exhibit both Doppler and gravitational redshifts. Additionally, the temperatures of various emitting and absorbing objects can be obtained by measuring Doppler broadening – effectively redshifts and blueshifts over a single emission or absorption line. By measuring the broadening and shifts of the 21-centimeter hydrogen line in different directions, astronomers have been able to measure the recessional velocities of interstellar gas, which in turn reveals the rotation curve of our Milky Way. Similar measurements have been performed on other galaxies, such as Andromeda. As a diagnostic tool, redshift measurements are one of the most important spectroscopic measurements made in astronomy.
Extragalactic observations.
The most distant objects exhibit larger redshifts corresponding to the Hubble flow of the Universe. The largest observed redshift, corresponding to the greatest distance and furthest back in time, is that of the cosmic microwave background radiation; the numerical value of its redshift is about ( corresponds to present time), and it shows the state of the Universe about 13.8 billion years ago, and 379,000 years after the initial moments of the Big Bang.
The luminous point-like cores of quasars were the first "high-redshift" () objects discovered before the improvement of telescopes allowed for the discovery of other high-redshift galaxies.
For galaxies more distant than the Local Group and the nearby Virgo Cluster, but within a thousand megaparsecs or so, the redshift is approximately proportional to the galaxy's distance. This correlation was first observed by Edwin Hubble and has come to be known as Hubble's law. Vesto Slipher was the first to discover galactic redshifts, in about the year 1912, while Hubble correlated Slipher's measurements with distances he measured by other means to formulate his Law. In the widely accepted cosmological model based on general relativity, redshift is mainly a result of the expansion of space: this means that the farther away a galaxy is from us, the more the space has expanded in the time since the light left that galaxy, so the more the light has been stretched, the more redshifted the light is, and so the faster it appears to be moving away from us. Hubble's law follows in part from the Copernican principle. Because it is usually not known how luminous objects are, measuring the redshift is easier than more direct distance measurements, so redshift is sometimes in practice converted to a crude distance measurement using Hubble's law.
Gravitational interactions of galaxies with each other and clusters cause a significant scatter in the normal plot of the Hubble diagram. The peculiar velocities associated with galaxies superimpose a rough trace of the mass of virialized objects in the Universe. This effect leads to such phenomena as nearby galaxies (such as the Andromeda Galaxy) exhibiting blueshifts as we fall towards a common barycenter, and redshift maps of clusters showing a Fingers of God effect due to the scatter of peculiar velocities in a roughly spherical distribution. This added component gives cosmologists a chance to measure the masses of objects independent of the "mass to light ratio" (the ratio of a galaxy's mass in solar masses to its brightness in solar luminosities), an important tool for measuring dark matter.
The Hubble law's linear relationship between distance and redshift assumes that the rate of expansion of the Universe is constant. However, when the Universe was much younger, the expansion rate, and thus the Hubble "constant", was larger than it is today. For more distant galaxies, then, whose light has been travelling to us for much longer times, the approximation of constant expansion rate fails, and the Hubble law becomes a non-linear integral relationship and dependent on the history of the expansion rate since the emission of the light from the galaxy in question. Observations of the redshift-distance relationship can be used, then, to determine the expansion history of the Universe and thus the matter and energy content.
While it was long believed that the expansion rate has been continuously decreasing since the Big Bang, recent observations of the redshift-distance relationship using Type Ia supernovae have suggested that in comparatively recent times the expansion rate of the Universe has begun to accelerate.
Highest redshifts.
Currently, the objects with the highest known redshifts are galaxies and the objects producing gamma ray bursts. The most reliable redshifts are from spectroscopic data, and the highest confirmed spectroscopic redshift of a galaxy is that of
UDFy-38135539
at a redshift of , corresponding to just 600 million years after the Big Bang.
The previous record was held by
IOK-1, at a redshift , corresponding to just 750 million years after the Big Bang. Slightly less reliable are Lyman-break redshifts, the highest of which is the lensed galaxy A1689-zD1 at a redshift 
Effects due to physical optics or radiative transfer.
The interactions and phenomena summarized in the subjects of [[radiative transfer]] and [[physical optics]] can result in shifts in the wavelength and frequency of electromagnetic radiation. In such cases the shifts correspond to a physical energy transfer to matter or other photons rather than being due to a transformation between reference frames. These shifts can be due to such physical phenomena as [[Wolf effect|coherence effects]] or the [[scattering]] of [[electromagnetic radiation]] whether from [[electric charge|charged]] [[elementary particle]]s, from particulates, or from fluctuations of the [[index of refraction]] in a [[dielectric medium]] as occurs in the radio phenomenon of [[Whistler (radio)|radio whistlers]]. While such phenomena are sometimes referred to as "redshifts" and "blueshifts", in astrophysics light-matter interactions that result in energy shifts in the radiation field are generally referred to as "reddening" rather than "redshifting" which, as a term, is normally reserved for the [[#Redshift formulae|effects discussed above]].
In many circumstances scattering causes radiation to redden because [[entropy]] results in the predominance of many low-[[energy]] photons over few high-energy ones (while [[conservation of energy|conserving total energy]]). Except possibly under carefully controlled conditions, scattering does not produce the same relative change in wavelength across the whole spectrum; that is, any calculated is generally a [[function (mathematics)|function]] of wavelength. Furthermore, scattering from [[randomness|random]] [[matter|media]] generally occurs at many [[angle]]s, and is a function of the scattering angle. If multiple scattering occurs, or the scattering particles have relative motion, then there is generally distortion of [[spectral line]]s as well.
In [[interstellar medium|interstellar astronomy]], [[visible spectrum|visible spectra]] can appear redder due to scattering processes in a phenomenon referred to as [[interstellar reddening]] – similarly [[Rayleigh scattering]] causes the [[Earth's atmosphere|atmospheric]] reddening of the Sun seen in the sunrise or sunset and causes the rest of the sky to have a blue color. This phenomenon is distinct from red"shift"ing because the [[atomic spectral line|spectroscopic lines]] are not shifted to other wavelengths in reddened objects and there is an additional [[extinction (astronomy)|dimming]] and distortion associated with the phenomenon due to photons being scattered in and out of the [[Line-of-sight propagation|line-of-sight]].
"For a list of scattering processes, see [[Scattering]]."
External links.
[[Category:Astronomical spectroscopy]]
[[Category:Doppler effects]]
[[Category:Physical cosmology]]
[[Category:Physical quantities]]

</doc>
<doc id="85747" url="https://en.wikipedia.org/wiki?curid=85747" title="Reduced mass">
Reduced mass

In physics, the reduced mass is the "effective" inertial mass appearing in the two-body problem of Newtonian mechanics. It is a quantity which allows the two-body problem to be solved as if it were a one-body problem. Note, however, that the mass determining the gravitational force is "not" reduced. In the computation one mass "can" be replaced by the reduced mass, if this is compensated by replacing the other mass by the sum of both masses. The reduced mass is frequently denoted by formula_1 (Greek lower case mu), although the standard gravitational parameter is also denoted by formula_2 (and so are a number of other physical quantities as well). It has the dimensions of mass, and SI unit kg.
Equation.
Given two bodies, one with mass "m"1 and the other with mass "m"2, the equivalent one-body problem, with the position of one body with respect to the other as the unknown, is that of a single body of mass 
where the force on this mass is given by the force between the two bodies.
Properties.
The reduced mass is always less than or equal to the mass of each body:
and has the reciprocal additive property:
which by re-arrangement is equivalent to half of the harmonic mean.
In the special case that formula_6:
Derivation.
The equation can be derived as follows.
Newtonian mechanics.
Using Newton's second law, the force exerted by body 2 on body 1 is
The force exerted by body 1 on body 2 is
According to Newton's third law, the force that body 2 exerts on body 1 is equal and opposite to the force that body 1 exerts on body 2:
Therefore,
and
The relative acceleration arel between the two bodies is given by
So we conclude that body 1 moves with respect to the position of body 2 as a body of mass equal to the reduced mass.
Lagrangian mechanics.
Alternatively, a Lagrangian description of the two-body problem gives a Lagrangian of
where formula_15 is the position vector of mass formula_16 (of particle "formula_17"). The potential energy "V" is a function as it is only dependent on the absolute distance between the particles. If we define 
and let the centre of mass coincide with our origin in this reference frame, i.e. 
then
Then substituting above gives a new Lagrangian
where 
is the reduced mass. Thus we have reduced the two-body problem to that of one body.
Applications.
Reduced mass can be used in a multitude of two-body problems, where classical mechanics is applicable.
Collisions of particles.
In a collision with a coefficient of restitution "e", the change in kinetic energy can be written as 
where vrel is the relative velocity of the bodies before collision.
For typical applications in nuclear physics, where one particle's mass is much larger than the other the reduced mass can be approximated as the smaller mass of the system. The limit of the reduced mass formula as one mass goes to infinity is the smaller mass, thus this approximation is used to ease calculations, especially when the larger particles exact mass is not known.
Motions of masses in gravitational fields.
In the case of the gravitational potential energy 
we find that the position of the first body with respect to the second is governed by the same differential equation as the position of a body with the reduced mass orbiting a body with a mass equal to the sum of the two masses, because 
Non-relativistic quantum mechanics.
Consider the electron (mass "me") and proton (mass "mp") in the hydrogen atom. They orbit each other about a common centre of mass, a two body problem. To analyze the motion of the electron, a one-body problem, the reduced mass replaces the electron mass
and the proton mass becomes the sum of the two masses
This idea is used to set up the Schrödinger equation for the hydrogen atom.
Other uses.
"Reduced mass" may also refer more generally to an algebraic term of the form 
that simplifies an equation of the form
The reduced mass is typically used as a relationship between two system elements in parallel, such as resistors; whether these be in the electrical, thermal, hydraulic, or mechanical domains. This relationship is determined by the physical properties of the elements as well as the continuity equation linking them.

</doc>
<doc id="360011" url="https://en.wikipedia.org/wiki?curid=360011" title="Regular representation">
Regular representation

In mathematics, and in particular the theory of group representations, the regular representation of a group "G" is the linear representation afforded by the group action of "G" on itself by translation.
One distinguishes the left regular representation λ given by left translation and the right regular representation ρ given by the inverse of right translation.
Finite groups.
For a finite group "G", the left regular representation λ (over a field "K") is a linear representation on the "K"-vector space "V" freely generated by the elements of "G", i. e. they can be identified with a basis of "V". Given "g" ∈ "G", λ("g") is the linear map determined by its action on the basis by left translation by "g", i.e.
For the right regular representation ρ, an inversion must occur in order to satisfy the axioms of a representation. Specifically, given "g" ∈ "G", ρ("g") is the linear map on "V" determined by its action on the basis by right translation by "g"−1, i.e.
Alternatively, these representations can be defined on the "K"-vector space "W" of all functions . It is in this form that the regular representation is generalized to topological groups such as Lie groups.
The specific definition in terms of "W" is as follows. Given a function and an element "g" ∈ "G",
and
Significance of the regular representation of a group.
To say that "G" acts on itself by multiplication is tautological. If we consider this action as a permutation representation it is characterised as having a single orbit and stabilizer the identity subgroup {"e"} of "G". The regular representation of "G", for a given field "K", is the linear representation made by taking this permutation representation as a set of basis vectors of a vector space over "K". The significance is that while the permutation representation doesn't decompose - it is transitive - the regular representation in general breaks up into smaller representations. For example if "G" is a finite group and "K" is the complex number field, the regular representation decomposes as a direct sum of irreducible representations, with each irreducible representation appearing in the decomposition with multiplicity its dimension. The number of these irreducibles is equal to the number of conjugacy classes of "G".
The article on group rings articulates the regular representation for finite groups, as well as showing how the regular representation can be taken to be a module.
Module theory point of view.
To put the construction more abstractly, the group ring "K"["G"] is considered as a module over itself. (There is a choice here of left-action or right-action, but that is not of importance except for notation.) If "G" is finite and the characteristic of K doesn't divide |"G"|, this is a semisimple ring and we are looking at its left (right) ring ideals. This theory has been studied in great depth. It is known in particular that the direct sum decomposition of the regular representation contains a representative of every isomorphism class of irreducible linear representations of "G" over "K". You can say that the regular representation is "comprehensive" for representation theory, in this case. The modular case, when the characteristic of "K" does divide |"G"|, is harder mainly because with "K"["G"] not semisimple, and a representation can fail to be irreducible without splitting as a direct sum.
Structure for finite cyclic groups.
For a cyclic group "C" generated by "g" of order "n", the matrix form of an element of "K"["C"] acting on "K"["C"] by multiplication takes a distinctive form known as a "circulant matrix", in which each row is a shift to the right of the one above (in cyclic order, i.e. with the right-most element appearing on the left), when referred to the natural basis
When the field "K" contains a primitive n-th root of unity, one can diagonalise the representation of "C" by writing down "n" linearly independent simultaneous eigenvectors for all the "n"×"n" circulants. In fact if ζ is any "n"-th root of unity, the element
is an eigenvector for the action of "g" by multiplication, with eigenvalue
and so also an eigenvector of all powers of "g", and their linear combinations.
This is the explicit form in this case of the abstract result that over an algebraically closed field "K" (such as the complex numbers) the regular representation of "G" is completely reducible, provided that the characteristic of "K" (if it is a prime number "p") doesn't divide the order of "G". That is called "Maschke's theorem". In this case the condition on the characteristic is implied by the existence of a "primitive" "n"-th root of unity, which cannot happen in the case of prime characteristic "p" dividing "n".
Circulant determinants were first encountered in nineteenth century mathematics, and the consequence of their diagonalisation drawn. Namely, the determinant of a circulant is the product of the "n" eigenvalues for the "n" eigenvectors described above. The basic work of Frobenius on group representations started with the motivation of finding analogous factorisations of the group determinants for any finite "G"; that is, the determinants of arbitrary matrices representing elements of "K"["G"] acting by multiplication on the basis elements given by "g" in "G". Unless "G" is abelian, the factorisation must contain non-linear factors corresponding to irreducible representations of "G" of degree > 1.
Topological group case.
For a topological group "G", the regular representation in the above sense should be replaced by a suitable space of functions on "G", with "G" acting by translation. See Peter–Weyl theorem for the compact case. If "G" is a Lie group but not compact nor abelian, this is a difficult matter of harmonic analysis. The locally compact abelian case is part of the Pontryagin duality theory.
Normal bases in Galois theory.
In Galois theory it is shown that for a field "L", and a finite group "G" of automorphisms of "L", the fixed field "K" of "G" has ["L":"K"] = |"G"|. In fact we can say more: "L" viewed as a "K"["G"]-module is the regular representation. This is the content of the normal basis theorem, a normal basis being an element "x" of "L" such that the "g"("x") for "g" in "G" are a vector space basis for "L" over "K". Such "x" exist, and each one gives a "K"["G"]-isomorphism from "L" to "K"["G"]. From the point of view of algebraic number theory it is of interest to study "normal integral bases", where we try to replace "L" and "K" by the rings of algebraic integers they contain. One can see already in the case of the Gaussian integers that such bases may not exist: "a" + "bi" and "a" − "bi" can never form a Z-module basis of Z["i"] because 1 cannot be an integer combination. The reasons are studied in depth in Galois module theory.
More general algebras.
The regular representation of a group ring is such that the left-hand and right-hand regular representations give isomorphic modules (and we often need not distinguish the cases). Given an algebra over a field "A", it doesn't immediately make sense to ask about the relation between "A" as left-module over itself, and as right-module. In the group case, the mapping on basis elements "g" of "K"["G"] defined by taking the inverse element gives an isomorphism of "K"["G"] to its "opposite" ring. For "A" general, such a structure is called a Frobenius algebra. As the name implies, these were introduced by Frobenius in the nineteenth century. They have been shown to be related to topological quantum field theory in 1 + 1 dimensions.

</doc>
<doc id="4946686" url="https://en.wikipedia.org/wiki?curid=4946686" title="Relative velocity">
Relative velocity

The relative velocity formula_1 (also formula_2 or formula_3) is the velocity of an object or observer B in the rest frame of another object or observer A.
Classical mechanics.
In one dimension (non-relativistic).
We begin with relative motion in the classical, (or non-relativistic, or the Newtonian approximation) that all speeds are much less than the speed of light. This limit is associated with the Galilean transformation. The figure shows a man on top of a train, at the back edge. At 1:00 pm he begins to walk forward at a walking speed of 10 km/hr (kilometers per hour). The train is moving at 40 km/hr. The figure depicts the man and train at two different times: first, when the journey began, and also one hour later at 2:00 pm. The figure suggests that the man is 50 km from the starting point after having traveled (by walking and by train) for one hour. This, by definition, is 50 km/hour, which suggests that the prescription for calculating relative velocity in this fashion is to add the two velocities.
The figure displays clocks and rulers to remind the reader that while the logic behind this calculation seem flawless, it makes false assumptions about how clocks and rulers behave. (See The train-and-platform thought experiment.) To recognize that this classical model of relative motion violates special relativity, we generalize the example into an equation:
Fully legitimate expressions for ""the velocity of A relative to B" include "the velocity of A with respect to B" and "the velocity of A in the coordinate system where B is always at rest"". The violation of special relativity occurs because this equation for relative velocity falsely predicts that different observers will measure different speeds when observing the motion of light. 
In two dimensions (non-relativistic).
The figure shows two objects (A and B) moving at constant velocity. The equations of motion are:
where the subscript "i" refers to the initial displacement (at time "t" equal to zero). The difference between the two displacement vectors, formula_10, represents the location of B as seen from A.
Hence:
After making the substitutions,
formula_13 and
formula_14,
we have:
Though easy to learn, this notation is unfortunately a bit cumbersome for lengthy calculations.
Galilean transformation (non-relativistic).
To construct a theory of relative motion consistent with the theory of special relativity, we must adopt a different convention. Continuing to work in the (non-relativistic) Newtonian limit we begin with a Galilean transformation in one dimension:
where x' is the position as seen by a reference frame that is moving at speed, v, in the "unprimed" (x) reference frame. Taking the differential of the first of the two equations above, we have, formula_19, and what may seem like the obvious statement that formula_20, we have:
To recover the previous expressions for relative velocity, we assume that particle "A" is following the path defined by dx/dt in the unprimed reference (and hence dx'/dt' in the primed frame). Thus formula_22 and formula_23, where formula_24 and formula_25 refer to motion of "A" as seen by an observer in the unprimed and primed frame, respectively. Recall that v is the motion of a stationary object in the primed frame, as seen from the unprimed frame. Thus we have formula_26, and:
where the latter form has the desired (easily learned) symmetry.
Special Relativity.
As in classical mechanics, in Special Relativity the relative velocity formula_1 is the velocity of an object or observer B in the rest frame of another object or observer A. However, unlike the case of classical mechanics, in Special Relativity, it is generally not the case that
This peculiar lack of symmetry is related to Thomas precession and the fact that two successive Lorentz transformations rotate the coordinate system. This rotation has no effect on the magnitude of a vector, and hence relative speed is symmetrical.
Parallel Velocities.
In the case where two objects are traveling in parallel directions, the relativistic formula for relative velocity is similar in form to the formula for addition of relativistic velocities.
The relative speed is given by the formula:
Perpendicular Velocities.
In the case where two objects are traveling in perpendicular directions, the relativistic relative velocity formula_1 is given by the formula:
where
The relative speed is given by the formula
General Case.
The general formula for the relative velocity formula_1 of an object or observer B in the rest frame of another object or observer A is given by the formula:
where
The relative speed is given by the formula
See also:

</doc>
<doc id="577162" url="https://en.wikipedia.org/wiki?curid=577162" title="Relativistic wave equations">
Relativistic wave equations

In physics, specifically relativistic quantum mechanics (RQM) and its applications to particle physics, relativistic wave equations predict the behavior of particles at high energies and velocities comparable to the speed of light. In the context of quantum field theory (QFT), the equations determine the dynamics of quantum fields.
The solutions to the equations, universally denoted as or (Greek psi), are referred to as "wavefunctions" in the context of RQM, and "fields" in the context of QFT. The equations themselves are called "wave equations" or "field equations", because they have the mathematical form of a wave equation or are generated from a Lagrangian density and the field-theoretic Euler–Lagrange equations (see classical field theory for background).
In the Schrödinger picture, the wavefunction or field is the solution to the Schrödinger equation;
one of the postulates of quantum mechanics. All relativistic wave equations can be constructed by specifying various forms of the Hamiltonian operator "Ĥ" describing the quantum system. Alternatively, Feynman's path integral formulation uses a Lagrangian rather than a Hamiltonian operator.
More generally - the modern formalism behind relativistic wave equations is Lorentz group theory, wherein the spin of the particle has a correspondence with the representations of the Lorentz group.
History.
Early 1920s: Classical and quantum mechanics.
The failure of classical mechanics applied to molecular, atomic, and nuclear systems and smaller induced the need for a new mechanics: "quantum mechanics". The mathematical formulation was led by De Broglie, Bohr, Schrödinger, Pauli, and Heisenberg, and others, around the mid-1920s, and at that time was analogous to that of classical mechanics. The Schrödinger equation and the Heisenberg picture resemble the classical equations of motion in the limit of large quantum numbers and as the reduced Planck constant , the quantum of action, tends to zero. This is the correspondence principle. At this point, special relativity was not fully combined with quantum mechanics, so the Schrödinger and Heisenberg formulations, as originally proposed, could not be used in situations where the particles travel near the speed of light, or when the number of each type of particle changes (this happens in real particle interactions; the numerous forms of particle decays, annihilation, matter creation, pair production, and so on).
Late 1920s: Relativistic quantum mechanics of spin-0 and spin- particles.
A description of quantum mechanical systems which could account for "relativistic" effects was sought for by many theoretical physicists; from the late 1920s to the mid-1940s. The first basis for relativistic quantum mechanics, i.e. special relativity applied with quantum mechanics together, was found by all those who discovered what is frequently called the Klein–Gordon equation:
by inserting the energy operator and momentum operator into the relativistic energy–momentum relation:
The solutions to () are scalar fields. The KG equation is undesirable due to its prediction of "negative" energies and probabilities, as a result of the quadratic nature of () - inevitable in a relativistic theory. This equation was initially proposed by Schrödinger, and he discarded it for such reasons, only to realize a few months later that its non-relativistic limit (what is now called the Schrödinger equation) was still of importance. Nevertheless - () is applicable to spin-0 bosons.
Neither the non-relativistic nor relativistic equations found by Schrödinger could predict the hyperfine structure in the Hydrogen spectral series. The mysterious underlying property was "spin". The first two-dimensional "spin matrices" (better known as the Pauli matrices) were introduced by Pauli in the Pauli equation; the Schrödinger equation with a non-relativistic Hamiltonian including an extra term for particles in magnetic fields, but this was "phenomenological". Weyl found a relativistic equation in terms of the Pauli matrices; the Weyl equation, for "massless" spin- fermions. The problem was resolved by Dirac in the late 1920s, when he furthered the application of equation () to the electron – by various manipulations he factorized the equation into the form:
and one of these factors is the Dirac equation (see below), upon inserting the energy and momentum operators. For the first time, this introduced new four-dimensional spin matrices and in a relativistic wave equation, and explained the hyperfine structure of hydrogen. The solutions to () are multi-component spinor fields, and each component satisfies (). A remarkable result of spinor solutions is that half of the components describe a particle, while the other half describe an antiparticle; in this case the electron and positron. The Dirac equation is now known to apply for all massive spin- fermions. In the non-relativistic limit, the Pauli equation is recovered, while the massless case results in the Weyl equation.
Although a "landmark" in quantum theory, the Dirac equation is only true for spin- fermions, and still predicts negative energy solutions, which caused controversy at the time (in particular - not all physicists were comfortable with the "Dirac sea" of negative energy states).
1930s–1960s: Relativistic quantum mechanics of higher-spin particles.
The natural problem became clear: to generalize the Dirac equation to particles with "any spin"; both fermions and bosons, and in the same equations their antiparticles (possible because of the spinor formalism introduced by Dirac in his equation, and then-recent developments in spinor calculus by van der Waerden in 1929), and ideally with positive energy solutions.
This was introduced and solved by Majorana in 1932, by a deviated approach to Dirac. Majorana considered one "root" of ():
where is a spinor field now with infinitely many components, irreducible to a finite number of tensors or spinors, to remove the indeterminacy in sign. The matrices and are infinite-dimensional matrices, related to infinitesimal Lorentz transformations. He did not demand that each component of to satisfy equation (), instead he regenerated the equation using a Lorentz-invariant action, via the principle of least action, and application of Lorentz group theory.
Majorana produced other important contributions that were unpublished, including wave equations of various dimensions (5, 6, and 16). They were anticipated later (in a more involved way) by de Broglie (1934), and Duffin, Kemmer, and Petiau (around 1938–1939), see Duffin–Kemmer–Petiau algebra. The Dirac–Fierz–Pauli formalism was more sophisticated than Majorana’s, as spinors were new mathematical tools in the early twentieth century, although Majorana’s paper of 1932 was difficult to fully understand; it took Pauli and Wigner some time to understand it, around 1940.
Dirac in 1936, and Fierz and Pauli in 1939, built equations from irreducible spinors and , symmetric in all indices, for a massive particle of spin for integer (see Van der Waerden notation for the meaning of the dotted indices):
B_{\gamma\epsilon_1\epsilon_2\cdots\epsilon_n}^{\dot{\beta}_1\dot{\beta}_2\cdots\dot{\beta}_n} = mcA_{\epsilon_1\epsilon_2\cdots\epsilon_n}^{\dot{\alpha}\dot{\beta}_1\dot{\beta}_2\cdots\dot{\beta}_n} 
where is the momentum as a covariant spinor operator. For , the equations reduce to the coupled Dirac equations and and together transform as the original Dirac spinor. Eliminating either or shows that and each fulfill ().
In 1941, Rarita and Schwinger focussed on spin- particles and derived the Rarita–Schwinger equation, including a Lagrangian to generate it, and later generalized the equations analogous to spin for integer . In 1945, Pauli suggested Majorana's 1932 paper to Bhabha, who returned to the general ideas introduced by Majorana in 1932. Bhabha and Lubanski proposed a completely general set of equations by replacing the mass terms in () and () by an arbitrary constant, subject to a set of conditions which the wavefunctions must obey.
Finally, in the year 1948 (the same year as Feynman's path integral formulation was cast), Bargmann and Wigner formulated the general equation for massive particles which could have any spin, by considering the Dirac equation with a totally symmetric finite-component spinor, and using Lorentz group theory (as Majorana did): the Bargmann–Wigner equations. In the early 1960s, a reformulation of the Bargmann–Wigner equations was made by H. Joos and Steven Weinberg. Various theorists at this time did further research in relativistic Hamiltonians for higher spin particles.
1960s–Present.
The relativistic description of spin particles has been a difficult problem in quantum theory. It is still an area of the present-day research, because the problem is only partially solved; including interactions in the equations is problematic, and paradoxical predictions (even from the Dirac equation) are still present.
Linear equations.
The following equations have solutions which satisfy the superposition principle, that is, the wavefunctions are additive.
Throughout, the standard conventions of tensor index notation and Feynman slash notation are used, including Greek indices which take the values 1, 2, 3 for the spatial components and 0 for the timelike component of the indexed quantities. The wavefunctions are denoted ", and are the components of the four-gradient operator.
In matrix equations, the Pauli matrices are denoted by " in which , where is the identity matrix:
and the other matrices have their usual representations. The expression
is a matrix operator which acts on 2-component spinor fields.
The gamma matrices are denoted by "", in which again , and there are a number of representations to select from. The matrix is "not" necessarily the identity matrix. The expression
is a matrix operator which acts on 4-component spinor fields.
Note that terms such as "" scalar multiply an identity matrix of the relevant dimension, the common sizes are or , and are "conventionally" not written for simplicity.
Gauge fields.
The Duffin–Kemmer–Petiau equation is an alternative equation for spin-0 and spin-1 particles:
Non-linear equations.
There are equations which have solutions that do not satisfy the superposition principle.

</doc>
<doc id="291453" url="https://en.wikipedia.org/wiki?curid=291453" title="Renormalization">
Renormalization

In quantum field theory, the statistical mechanics of fields, and the theory of self-similar geometric structures, renormalization is any of a collection of techniques used to treat infinities arising in calculated quantities.
Renormalization specifies relationships between parameters in the theory when the parameters describing large distance scales differ from the parameters describing small distances. Physically, the pileup of contributions from an infinity of scales involved in a problem may then result in infinities. When describing space and time as a continuum, certain statistical and quantum mechanical constructions are ill defined. To define them, this continuum limit, the removal of the "construction scaffolding" of lattices at various scales, has to be taken carefully, as detailed below.
Renormalization was first developed in quantum electrodynamics (QED) to make sense of infinite integrals in perturbation theory. Initially viewed as a suspect provisional procedure even by some of its originators, renormalization eventually was embraced as an important and self-consistent actual mechanism of scale physics in several fields of physics and mathematics. Today, the point of view has shifted: on the basis of the breakthrough renormalization group insights of Kenneth Wilson, the focus is on variation of physical quantities across contiguous scales, while distant scales are related to each other through "effective" descriptions. "All scales" are linked in a broadly systematic way, and the actual physics pertinent to each is extracted with the suitable specific computational techniques appropriate for each.
Self-interactions in classical physics.
The problem of infinities first arose in the classical electrodynamics of point particles in the 19th and early 20th century.
The mass of a charged particle should include the mass-energy in its electrostatic field (Electromagnetic mass). Assume that the particle is a charged spherical shell of radius . The mass-energy in the field is
which becomes infinite as . This implies that the point particle would have infinite inertia, making it unable to be accelerated. Incidentally, the value of that makes formula_2 equal to the electron mass is called the classical electron radius, which (setting formula_3 and restoring factors of and formula_4) turns out to be
where formula_6 is the fine structure constant, and formula_7 is the Compton wavelength of the electron.
The total effective mass of a spherical charged particle includes the actual bare mass of the spherical shell (in addition to the aforementioned mass associated with its electric field). If the shell's bare mass is allowed to be negative, it might be possible to take a consistent point limit. This was called "renormalization", and Lorentz and Abraham attempted to develop a classical theory of the electron this way. This early work was the inspiration for later attempts at regularization and renormalization in quantum field theory.
When calculating the electromagnetic interactions of charged particles, it is tempting to ignore the "back-reaction" of a particle's own field on itself. But this back reaction is necessary to explain the friction on charged particles when they emit radiation. If the electron is assumed to be a point, the value of the back-reaction diverges, for the same reason that the mass diverges, because the field is inverse-square.
The Abraham–Lorentz theory had a noncausal "pre-acceleration". Sometimes an electron would start moving "before" the force is applied. This is a sign that the point limit is inconsistent. 
The trouble was worse in classical field theory than in quantum field theory, because in quantum field theory a charged particle experiences Zitterbewegung due to interference with virtual particle-antiparticle pairs, thus effectively smearing out the charge over a region comparable to the Compton wavelength. In quantum electrodynamics at small coupling the electromagnetic mass only diverges as the logarithm of the radius of the particle.
Divergences in quantum electrodynamics.
When developing quantum electrodynamics in the 1930s, Max Born, Werner Heisenberg, Pascual Jordan, and Paul Dirac discovered that in perturbative calculations many integrals were divergent.
One way of describing the divergences was discovered in the 1930s by Ernst Stueckelberg, in the 1940s by Julian Schwinger, Richard Feynman, and Shin'ichiro Tomonaga, and systematized by Freeman Dyson. The divergences appear in calculations involving Feynman diagrams with closed "loops" of virtual particles in them.
While virtual particles obey conservation of energy and momentum, they can have any energy and momentum, even one that is not allowed by the relativistic energy-momentum relation for the observed mass of that particle. (That is, formula_8 is not necessarily the mass of the particle in that process (e.g. for a photon it could be nonzero).) Such a particle is called off-shell. When there is a loop, the momentum of the particles involved in the loop is not uniquely determined by the energies and momenta of incoming and outgoing particles. A variation in the energy of one particle in the loop must be balanced by an equal and opposite variation in the energy of another particle in the loop. So to find the amplitude for the loop process one must integrate over "all" possible combinations of energy and momentum that could travel around the loop.
These integrals are often "divergent", that is, they give infinite answers. The divergences which are significant are the "ultraviolet" (UV) ones. An ultraviolet divergence can be described as one which comes from
So these divergences are short-distance, short-time phenomena.
There are exactly three one-loop divergent loop diagrams in quantum electrodynamics:
The three divergences correspond to the three parameters in the theory:
A second class of divergence, called an infrared divergence, is due to massless particles, like the photon. Every process involving charged particles emits infinitely many coherent photons of infinite wavelength, and the amplitude for emitting any finite number of photons is zero. For photons, these divergences are well understood. For example, at the 1-loop order, the vertex function has both ultraviolet and "infrared" divergences. In contrast to the ultraviolet divergence, the infrared divergence does not require the renormalization of a parameter in the theory. The infrared divergence of the vertex diagram is removed by including a diagram similar to the vertex diagram with the following important difference: the photon connecting the two legs of the electron is cut and replaced by two on shell (i.e. real) photons whose wavelengths tend to infinity; this diagram is equivalent to the bremsstrahlung process. This additional diagram must be included because there is no physical way to distinguish a zero-energy photon flowing through a loop as in the vertex diagram and zero-energy photons emitted through bremsstrahlung. From a mathematical point of view the IR divergences can be regularized by assuming fractional differentiation with respect to a parameter, for example
is well defined at but is UV divergent, if we take the 3/2-th fractional derivative with respect to we obtain the IR divergence
so we can cure IR divergences by turning them into UV divergences.
A loop divergence.
The diagram in Figure 2 shows one of the several one-loop contributions to electron-electron scattering in QED. The electron on the left side of the diagram, represented by the solid line, starts out with four-momentum and ends up with four-momentum . It emits a virtual photon carrying to transfer energy and momentum to the other electron. But in this diagram, before that happens, it emits another virtual photon carrying four-momentum , and it reabsorbs this one after emitting the other virtual photon. Energy and momentum conservation do not determine the four-momentum uniquely, so all possibilities contribute equally and we must integrate.
This diagram's amplitude ends up with, among other things, a factor from the loop of
The various factors in this expression are gamma matrices as in the covariant formulation of the Dirac equation; they have to do with the spin of the electron. The factors of are the electric coupling constant, while the formula_12 provide a heuristic definition of the contour of integration around the poles in the space of momenta. The important part for our purposes is the dependency on of the three big factors in the integrand, which are from the propagators of the two electron lines and the photon line in the loop.
This has a piece with two powers of on top that dominates at large values of (Pokorski 1987, p. 122):
This integral is divergent, and infinite unless we cut it off at finite energy and momentum in some way.
Similar loop divergences occur in other quantum field theories.
Renormalized and bare quantities.
The solution was to realize that the quantities initially appearing in the theory's formulae (such as the formula for the Lagrangian), representing such things as the electron's electric charge and mass, as well as the normalizations of the quantum fields themselves, did "not" actually correspond to the physical constants measured in the laboratory. As written, they were "bare" quantities that did not take into account the contribution of virtual-particle loop effects to "the physical constants themselves". Among other things, these effects would include the quantum counterpart of the electromagnetic back-reaction that so vexed classical theorists of electromagnetism. In general, these effects would be just as divergent as the amplitudes under study in the first place; so finite measured quantities would in general imply divergent bare quantities.
In order to make contact with reality, then, the formulae would have to be rewritten in terms of measurable, "renormalized" quantities. The charge of the electron, say, would be defined in terms of a quantity measured at a specific kinematic "renormalization point" or "subtraction point" (which will generally have a characteristic energy, called the "renormalization scale" or simply the energy scale). The parts of the Lagrangian left over, involving the remaining portions of the bare quantities, could then be reinterpreted as "counterterms", involved in divergent diagrams exactly "canceling out" the troublesome divergences for other diagrams.
Renormalization in QED.
For example, in the Lagrangian of QED
the fields and coupling constant are really "bare" quantities, hence the subscript above. Conventionally the bare quantities are written so that the corresponding Lagrangian terms are multiples of the renormalized ones:
Gauge invariance, via a Ward–Takahashi identity, turns out to imply that we can renormalize the two terms of the covariant derivative piece
together (Pokorski 1987, p. 115), which is what happened to ; it is the same as .
A term in this Lagrangian, for example, the electron-photon interaction pictured in Figure 1, can then be written
The physical constant , the electron's charge, can then be defined in terms of some specific experiment; we set the renormalization scale equal to the energy characteristic of this experiment, and the first term gives the interaction we see in the laboratory (up to small, finite corrections from loop diagrams, providing such exotica as the high-order corrections to the magnetic moment). The rest is the counterterm. If the theory is "renormalizable" (see below for more on this), as it is in QED, the "divergent" parts of loop diagrams can all be decomposed into pieces with three or fewer legs, with an algebraic form that can be canceled out by the second term (or by the similar counterterms that come from and ).
The diagram with the counterterm's interaction vertex placed as in Figure 3 cancels out the divergence from the loop in Figure 2.
Historically, the splitting of the "bare terms" into the original terms and counterterms came before the renormalization group insights due to Kenneth Wilson. According to such renormalization group insights, detailed in the next section, this splitting is unnatural and actually unphysical, as all scales of the problem enter in systematic continuous ways.
Running couplings.
To minimize the contribution of loop diagrams to a given calculation (and therefore make it easier to extract results), one chooses a renormalization point close to the energies and momenta actually exchanged in the interaction. However, the renormalization point is not itself a physical quantity: the physical predictions of the theory, calculated to all orders, should in principle be "independent" of the choice of renormalization point, as long as it is within the domain of application of the theory. Changes in renormalization scale will simply affect how much of a result comes from Feynman diagrams without loops, and how much comes from the leftover finite parts of loop diagrams. One can exploit this fact to calculate the effective variation of physical constants with changes in scale. This variation is encoded by beta-functions, and the general theory of this kind of scale-dependence is known as the renormalization group.
Colloquially, particle physicists often speak of certain physical "constants" as varying with the energy of an interaction, though in fact it is the renormalization scale that is the independent quantity. This "running" does, however, provide a convenient means of describing changes in the behavior of a field theory under changes in the energies involved in an interaction. For example, since the coupling in quantum chromodynamics becomes small at large energy scales, the theory behaves more like a free theory as the energy exchanged in an interaction becomes large, a phenomenon known as asymptotic freedom. Choosing an increasing energy scale and using the renormalization group makes this clear from simple Feynman diagrams; were this not done, the prediction would be the same, but would arise from complicated high-order cancellations.
For example,
is ill defined.
To eliminate the divergence, simply change lower limit of integral into and :
Making sure , then 
Regularization.
Since the quantity is ill-defined, in order to make this notion of canceling divergences precise, the divergences first have to be tamed mathematically using the theory of limits, in a process known as regularization (Weinberg, 1995).
An essentially arbitrary modification to the loop integrands, or "regulator", can make them drop off faster at high energies and momenta, in such a manner that the integrals converge. A regulator has a characteristic energy scale known as the cutoff; taking this cutoff to infinity (or, equivalently, the corresponding length/time scale to zero) recovers the original integrals.
With the regulator in place, and a finite value for the cutoff, divergent terms in the integrals then turn into finite but cutoff-dependent terms. After canceling out these terms with the contributions from cutoff-dependent counterterms, the cutoff is taken to infinity and finite physical results recovered. If physics on scales we can measure is independent of what happens at the very shortest distance and time scales, then it should be possible to get cutoff-independent results for calculations.
Many different types of regulator are used in quantum field theory calculations, each with its advantages and disadvantages. One of the most popular in modern use is "dimensional regularization", invented by Gerardus 't Hooft and Martinus J. G. Veltman, which tames the integrals by carrying them into a space with a fictitious fractional number of dimensions. Another is "Pauli–Villars regularization", which adds fictitious particles to the theory with very large masses, such that loop integrands involving the massive particles cancel out the existing loops at large momenta.
Yet another regularization scheme is the "Lattice regularization", introduced by Kenneth Wilson, which pretends that our space-time is constructed by hyper-cubical lattice with fixed grid size. This size is a natural cutoff for the maximal momentum that a particle could possess when propagating on the lattice. And after doing calculation on several lattices with different grid size, the physical result is extrapolated to grid size 0, or our natural universe. This presupposes the existence of a scaling limit.
A rigorous mathematical approach to renormalization theory is the so-called causal perturbation theory, where ultraviolet divergences are avoided from the start in calculations by performing well-defined mathematical operations only within the framework of distribution theory. The disadvantage of the method is the fact that the approach is quite technical and requires a high level of mathematical knowledge.
Zeta function regularization.
Julian Schwinger discovered a relationship between zeta function regularization and renormalization, using the asymptotic relation:
as the regulator . Based on this, he considered using the values of to get finite results. Although he reached inconsistent results, an improved formula studied by Hartle, J. Garcia, and based on the works by E. Elizalde includes the technique of the zeta regularization algorithm
where the "B"'s are the Bernoulli numbers and
So every can be written as a linear combination of .
Or simply using Abel–Plana formula we have for every divergent integral:
valid when , Here the zeta function is Hurwitz zeta function and Beta is a positive real number.
The "geometric" analogy is given by, (if we use rectangle method) to evaluate the integral so:
Using Hurwitz zeta regularization plus the rectangle method with step h (not to be confused with Planck's constant).
The logarithmic divergent integral has the regularization
For multi-loop integrals that will depend on several variables formula_28 we can make a change of variables to polar coordinates and then replace the integral over the angles formula_29 by a sum so we have only a divergent integral, that will depend on the modulus formula_30 and then we can apply the zeta regularization algorithm, the main idea for multi-loop integrals is to replace the factor formula_31 after a change to hyperspherical coordinates so the UV overlapping divergences are encoded in variable . In order to regularize these integrals one needs a regulator, for the case of multi-loop integrals, these regulator can be taken as
so the multi-loop integral will converge for big enough using the Zeta regularization we can analytic continue the variable to the physical limit where and then regularize any UV integral, by replacing a divergent integral by a linear combination of divergent series, which can be regularized in terms of the negative values of the Riemann zeta function .
Attitudes and interpretation.
The early formulators of QED and other quantum field theories were, as a rule, dissatisfied with this state of affairs. It seemed illegitimate to do something tantamount to subtracting infinities from infinities to get finite answers.
Freeman Dyson argued that these infinities are of a basic nature and cannot be eliminated by any formal mathematical procedures, such as the renormalization method.
Dirac's criticism was the most persistent. As late as 1975, he was saying:
Another important critic was Feynman. Despite his crucial role in the development of quantum electrodynamics, he wrote the following in 1985:
While Dirac's criticism was based on the procedure of renormalization itself, Feynman's criticism was very different. Feynman was concerned that all field theories known in the 1960s had the property that the interactions become infinitely strong at short enough distance scales. This property, called a Landau pole, made it plausible that quantum field theories were all inconsistent. In 1974, Gross, Politzer and Wilczek showed that another quantum field theory, quantum chromodynamics, does not have a Landau pole. Feynman, along with most others, accepted that QCD was a fully consistent theory.
The general unease was almost universal in texts up to the 1970s and 1980s. Beginning in the 1970s, however, inspired by work on the renormalization group and effective field theory, and despite the fact that Dirac and various others—all of whom belonged to the older generation—never withdrew their criticisms, attitudes began to change, especially among younger theorists. Kenneth G. Wilson and others demonstrated that the renormalization group is useful in statistical field theory applied to condensed matter physics, where it provides important insights into the behavior of phase transitions. In condensed matter physics, a "physical" short-distance regulator exists: matter ceases to be continuous on the scale of atoms. Short-distance divergences in condensed matter physics do not present a philosophical problem, since the field theory is only an effective, smoothed-out representation of the behavior of matter anyway; there are no infinities since the cutoff is actually always finite, and it makes perfect sense that the bare quantities are cutoff-dependent.
If QFT holds all the way down past the Planck length (where it might yield to string theory, causal set theory or something different), then there may be no real problem with short-distance divergences in particle physics either; "all" field theories could simply be effective field theories. In a sense, this approach echoes the older attitude that the divergences in QFT speak of human ignorance about the workings of nature, but also acknowledges that this ignorance can be quantified and that the resulting effective theories remain useful.
Be that as it may, Salam's remark in 1972 seems still relevant
In QFT, the value of a physical constant, in general, depends on the scale that one chooses as the renormalization point, and it becomes very interesting to examine the renormalization group running of physical constants under changes in the energy scale. The coupling constants in the Standard Model of particle physics vary in different ways with increasing energy scale: the coupling of quantum chromodynamics and the weak isospin coupling of the electroweak force tend to decrease, and the weak hypercharge coupling of the electroweak force tends to increase. At the colossal energy scale of 1015 GeV (far beyond the reach of our current particle accelerators), they all become approximately the same size (Grotz and Klapdor 1990, p. 254), a major motivation for speculations about grand unified theory. Instead of being only a worrisome problem, renormalization has become an important theoretical tool for studying the behavior of field theories in different regimes.
If a theory featuring renormalization (e.g. QED) can only be sensibly interpreted as an effective field theory, i.e. as an approximation reflecting human ignorance about the workings of nature, then the problem remains of discovering a more accurate theory that does not have these renormalization problems. As Lewis Ryder has put it, "In the Quantum Theory, these [classical] divergences do not disappear; on the contrary, they appear to get worse. And despite the comparative success of renormalisation theory the feeling remains that there ought to be a more satisfactory way of doing things."
Renormalizability.
From this philosophical reassessment a new concept follows naturally: the notion of renormalizability. Not all theories lend themselves to renormalization in the manner described above, with a finite supply of counterterms and all quantities becoming cutoff-independent at the end of the calculation. If the Lagrangian contains combinations of field operators of high enough dimension in energy units, the counterterms required to cancel all divergences proliferate to infinite number, and, at first glance, the theory would seem to gain an infinite number of free parameters and therefore lose all predictive power, becoming scientifically worthless. Such theories are called "nonrenormalizable".
The Standard Model of particle physics contains only renormalizable operators, but the interactions of general relativity become nonrenormalizable operators if one attempts to construct a field theory of quantum gravity in the most straightforward manner (treating the metric in the Einstein-Hilbert Lagrangian as a perturbation about the Minkowski metric), suggesting that perturbation theory is useless in application to quantum gravity.
However, in an effective field theory, "renormalizability" is, strictly speaking, a misnomer. In a nonrenormalizable effective field theory, terms in the Lagrangian do multiply to infinity, but have coefficients suppressed by ever-more-extreme inverse powers of the energy cutoff. If the cutoff is a real, physical quantity—if, that is, the theory is only an effective description of physics up to some maximum energy or minimum distance scale—then these extra terms could represent real physical interactions. Assuming that the dimensionless constants in the theory do not get too large, one can group calculations by inverse powers of the cutoff, and extract approximate predictions to finite order in the cutoff that still have a finite number of free parameters. It can even be useful to renormalize these "nonrenormalizable" interactions.
Nonrenormalizable interactions in effective field theories rapidly become weaker as the energy scale becomes much smaller than the cutoff. The classic example is the Fermi theory of the weak nuclear force, a nonrenormalizable effective theory whose cutoff is comparable to the mass of the W particle. This fact may also provide a possible explanation for "why" almost all of the particle interactions we see are describable by renormalizable theories. It may be that any others that may exist at the GUT or Planck scale simply become too weak to detect in the realm we can observe, with one exception: gravity, whose exceedingly weak interaction is magnified by the presence of the enormous masses of stars and planets.
Renormalization schemes.
In actual calculations, the counterterms introduced to cancel the divergences in Feynman diagram calculations beyond tree level must be "fixed" using a set of "renormalization conditions". The common renormalization schemes in use include:
Application in statistical physics.
As mentioned in the introduction, the methods of renormalization have been applied to Statistical Physics, namely to the problems of the critical behaviour near second-order phase transitions, in particular at fictitious spatial dimensions just below the number of 4, where the above-mentioned methods could even be sharpened (i.e., instead of "renormalizability" one gets "super-renormalizability"), which allowed extrapolation to the real spatial dimensionality for phase transitions, 3. Details can be found in the book of Zinn-Justin, mentioned below.
For the discovery of these unexpected applications, and working out the details, in 1982 the physics Nobel prize was awarded to Kenneth G. Wilson.

</doc>
<doc id="292786" url="https://en.wikipedia.org/wiki?curid=292786" title="Representation of a Lie group">
Representation of a Lie group

In mathematics and theoretical physics, the idea of a representation of a Lie group plays an important role in the study of continuous symmetry. A great deal is known about such representations, a basic tool in their study being the use of the corresponding 'infinitesimal' representations of Lie algebras. The physics literature sometimes passes over the distinction between Lie groups and Lie algebras.
Representations on a complex finite-dimensional vector space.
Let us first discuss representations acting on finite-dimensional complex vector spaces. A representation of a Lie group "G" on a finite-dimensional complex vector space "V" is a smooth group homomorphism Ψ:"G"→Aut("V") from "G" to the automorphism group of "V".
For "n"-dimensional "V", the automorphism group of "V" is identified with a subset of the complex square matrices of order "n". The automorphism group of "V" is given the structure of a smooth manifold using this identification. The condition that Ψ is smooth, in the definition above, means that Ψ is a smooth map from the smooth manifold "G" to the smooth manifold Aut("V").
If a basis for the complex vector space "V" is chosen, the representation can be expressed as a homomorphism into general linear group GL("n",C). This is known as a "matrix representation".
Representations on a finite-dimensional vector space over an arbitrary field.
A representation of a Lie group "G" on a vector space "V" (over a field "K") is a smooth (i.e. respecting the differential structure) group homomorphism "G"→Aut("V") from "G" to the automorphism group of "V". If a basis for the vector space "V" is chosen, the representation can be expressed as a homomorphism into general linear group GL("n","K"). This is known as a "matrix representation".
Two representations of "G" on vector spaces "V", "W" are "equivalent" if they have the 
same matrix representations with respect to some choices of bases 
for "V" and "W".
On the Lie algebra level, there is a corresponding linear mapping from the Lie algebra of G to End("V") preserving the Lie bracket [ , ]. See representation of Lie algebras for the Lie algebra theory.
If the homomorphism is in fact a monomorphism, the representation is said to be "faithful".
A unitary representation is defined in the same way, except that G maps to unitary matrices; the Lie algebra will then map to skew-hermitian matrices.
If "G" is a compact Lie group, every finite-dimensional representation is equivalent to
a unitary one.
Representations on Hilbert spaces.
A representation of a Lie group "G" on a complex Hilbert space "V" is a group homomorphism Ψ:"G" → B("V") from "G" to B("V"), the group of bounded linear operators of "V" which have a bounded inverse, such that the map "G"×"V" → "V" given by ("g","v") → Ψ("g")"v" is continuous.
This definition can handle representations on infinite-dimensional Hilbert spaces. Such representations can be found in e.g. quantum mechanics, but also in Fourier analysis as shown in the following example.
Let "G"=R, and let the complex Hilbert space "V" be "L"2(R). We define the representation Ψ:R → B("L"2(R)) by Ψ("r"){"f"("x")} → "f"("r"−1"x").
See also Wigner's classification for representations of the Poincaré group.
Classification.
If G is a semisimple group, its finite-dimensional representations can be decomposed as direct sums of irreducible representations. The irreducibles are indexed by highest weight; the allowable ("dominant") highest weights satisfy a suitable positivity condition. In particular, there exists a set of "fundamental weights", indexed by the vertices of the Dynkin diagram of G, such that dominant weights are simply non-negative integer linear combinations of the fundamental weights. The characters of the irreducible representations are given by the Weyl character formula.
If G is a commutative Lie group, then its irreducible representations are simply the continuous characters of G: see Pontryagin duality for this case.
A quotient representation is a quotient module of the group ring.
Formulaic examples.
Let F"q" be a finite field of order "q" and characteristic "p". Let "G" be a finite group of Lie type, that is, "G" is the F"q"-rational points of a connected reductive group "G" defined over F"q". For example, if "n" is a positive integer GL("n", F"q") and SL(n, F"q") are finite groups of Lie type. Let formula_1, where "I"n is the "n"×"n" identity matrix. Let
Then Sp(2,F"q") is a symplectic group of rank "n" and is a finite group of Lie type. For "G" = GL("n", F"q") or SL("n", F"q") (and some other examples), the "standard Borel subgroup" "B" of "G" is the subgroup of "G" consisting of the upper triangular elements in "G". A "standard parabolic subgroup" of "G" is a subgroup of "G" which contains the standard Borel subgroup "B". If "P" is a standard parabolic subgroup of GL("n", F"q"), then there exists a partition ("n"1, …, "n"r) of "n" (a set of positive integers formula_3 such that formula_4) such that formula_5, where formula_6 has the form
and
where formula_9 denotes arbitrary entries in formula_10.

</doc>
<doc id="19378200" url="https://en.wikipedia.org/wiki?curid=19378200" title="Representation theory">
Representation theory

Representation theory is a branch of mathematics that studies abstract algebraic structures by "representing" their elements as linear transformations of vector spaces, and studies 
modules over these abstract algebraic structures. In essence, a representation makes an abstract algebraic object more concrete by describing its elements by matrices and the algebraic operations in terms of matrix addition and matrix multiplication. The algebraic objects amenable to such a description include groups, associative algebras and Lie algebras. The most prominent of these (and historically the first) is the representation theory of groups, in which elements of a group are represented by invertible matrices in such a way that the group operation is matrix multiplication.
Representation theory is a useful method because it reduces problems in abstract algebra to problems in linear algebra, a subject that is well understood. Furthermore, the vector space on which a group (for example) is represented can be infinite-dimensional, and by allowing it to be, for instance, a Hilbert space, methods of analysis can be applied to the theory of groups. Representation theory is also important in physics because, for example, it describes how the symmetry group of a physical system affects the solutions of equations describing that system.
A feature of representation theory is its pervasiveness in mathematics. There are two sides to this. First, the applications of representation theory are diverse: in addition to its impact on algebra, representation theory:
The second aspect is the diversity of approaches to representation theory. The same objects can be studied using methods from algebraic geometry, module theory, analytic number theory, differential geometry, operator theory, algebraic combinatorics and topology.
The success of representation theory has led to numerous generalizations. One of the most general is in category theory. The algebraic objects to which representation theory applies can be viewed as particular kinds of categories, and the representations as functors from the object category to the category of vector spaces. This description points to two obvious generalizations: first, the algebraic objects can be replaced by more general categories; second, the target category of vector spaces can be replaced by other well-understood categories.
A "representation" should not be confused with a "presentation".
Definitions and concepts.
Let "V" be a vector space over a field F. For instance, suppose "V" is R"n" or C"n", the standard "n"-dimensional space of column vectors over the real or complex numbers respectively. In this case, the idea of representation theory is to do abstract algebra concretely by using "n" × "n" matrices of real or complex numbers.
There are three main sorts of algebraic objects for which this can be done: groups, associative algebras and Lie algebras.
This generalizes to any field F and any vector space "V" over F, with linear maps replacing matrices and composition replacing matrix multiplication: there is a group GL("V",F) of automorphisms of "V", an associative algebra EndF("V") of all endomorphisms of "V", and a corresponding Lie algebra gl("V",F).
Definition.
There are two ways to say what a representation is. The first uses the idea of an action, generalizing the way that matrices act on column vectors by matrix multiplication. A representation of a group "G" or (associative or Lie) algebra "A" on a vector space "V" is a map
with two properties. First, for any "g" in "G" (or "a" in "A"), the map
is linear (over F). Second, if we introduce the notation "g" · "v" for Φ ("g", "v"), then for any "g"1, "g"2 in "G" and "v" in "V":
where "e" is the identity element of "G" and "g"1"g"2 is product in "G". The requirement for associative algebras is analogous, except that associative algebras do not always have an identity element, in which case equation (1) is ignored. Equation (2) is an abstract expression of the associativity of matrix multiplication. This doesn't hold for the matrix commutator and also there is no identity element for the commutator. Hence for Lie algebras, the only requirement is that for any "x"1, "x"2 in "A" and "v" in "V":
where ["x"1, "x"2] is the Lie bracket, which generalizes the matrix commutator "MN" − "NM".
The second way to define a representation focuses on the map "φ" sending "g" in "G" to a linear map "φ"("g"): "V" → "V", which satisfies
and similarly in the other cases. This approach is both more concise and more abstract.
From this point of view:
Terminology.
The vector space "V" is called the representation space of "φ" and its dimension (if finite) is called the dimension of the representation (sometimes "degree", as in ). It is also common practice to refer to "V" itself as the representation when the homomorphism "φ" is clear from the context; otherwise the notation ("V","φ") can be used to denote a representation.
When "V" is of finite dimension "n", one can choose a basis for "V" to identify "V" with F"n" and hence recover a matrix representation with entries in the field F.
An effective or faithful representation is a representation ("V","φ") for which the homomorphism "φ" is injective.
Equivariant maps and isomorphisms.
If "V" and "W" are vector spaces over F, equipped with representations "φ" and "ψ" of a group "G", then an equivariant map from "V" to "W" is a linear map "α": "V" → "W" such that
for all "g" in "G" and "v" in "V". In terms of "φ": "G" → GL("V") and "ψ": "G" → GL("W"), this means
for all "g" in "G".
Equivariant maps for representations of an associative or Lie algebra are defined similarly. If "α" is invertible, then it is said to be an isomorphism, in which case "V" and "W" (or, more precisely, "φ" and "ψ") are "isomorphic representations".
Isomorphic representations are, for all practical purposes, "the same": they provide the same information about the group or algebra being represented. Representation theory therefore seeks to classify representations "up to isomorphism".
Subrepresentations, quotients, and irreducible representations.
If ("W","ψ") is a representation of (say) a group "G", and "V" is a linear subspace of "W" that is preserved by the action of "G" in the sense that "g" · "v" ∈ "V" for all "v" ∈ "V" (Serre calls these "V" "stable under G"), then "V" is called a "subrepresentation": by defining "φ"("g") to be the restriction of "ψ"("g") to "V", ("V", "φ") is a representation of "G" and the inclusion of "V" into "W" is an equivariant map. The quotient space "W"/"V" can also be made into a representation of "G".
If "W" has exactly two subrepresentations, namely the trivial subspace {0} and "W" itself, then the representation is said to be "irreducible"; if "W" has a proper nontrivial subrepresentation, the representation is said to be "reducible".
The definition of an irreducible representation implies Schur's lemma: an equivariant map "α": "V" → "W" between irreducible representations is either the zero map or an isomorphism, since its kernel and image are subrepresentations. In particular, when "V" = "W", this shows that the equivariant endomorphisms of "V" form an associative division algebra over the underlying field F. If F is algebraically closed, the only equivariant endomorphisms of an irreducible representation are the scalar multiples of the identity.
Irreducible representations are the building blocks of representation theory: if a representation "W" is not irreducible then it is built from a subrepresentation and a quotient that are both "simpler" in some sense; for instance, if "W" is finite-dimensional, then both the subrepresentation and the quotient have smaller dimension.
Direct sums and indecomposable representations.
If ("V","φ") and ("W","ψ") are representations of (say) a group "G", then the direct sum of "V" and "W" is a representation, in a canonical way, via the equation
The direct sum of two representations carries no more information about the group "G" than the two representations do individually. If a representation is the direct sum of two proper nontrivial subrepresentations, it is said to be decomposable. Otherwise, it is said to be indecomposable.
In favourable circumstances, every representation is a direct sum of irreducible representations: such representations are said to be semisimple. In this case, it suffices to understand only the irreducible representations. In other cases, one must understand how indecomposable representations can be built from irreducible representations as extensions of a quotient by a subrepresentation.
Branches and topics.
Representation theory is notable for the number of branches it has, and the diversity of the approaches to studying representations of groups and algebras. Although, all the theories have in common the basic concepts discussed already, they differ considerably in detail. The differences are at least 3-fold:
Finite groups.
Group representations are a very important tool in the study of finite groups. They also arise in the applications of finite group theory to geometry and crystallography. Representations of finite groups exhibit many of the features of the general theory and point the way to other branches and topics in representation theory.
Over a field of characteristic zero, the representation theory of a finite group "G" has a number of convenient properties. First, the representations of "G" are semisimple (completely reducible). This is a consequence of Maschke's theorem, which states that any subrepresentation "V" of a "G"-representation "W" has a "G"-invariant complement. One proof is to choose any projection "π" from "W" to "V" and replace it by its average "π""G" defined by
"π""G" is equivariant, and its kernel is the required complement.
The finite-dimensional "G"-representations can be understood using character theory: the character of a representation "φ": "G" → GL("V") is the class function "χ""φ": "G" → F defined by
where formula_12 is the trace. An irreducible representation of "G" is completely determined by its character.
Maschke's theorem holds more generally for fields of positive characteristic "p", such as the finite fields, as long as the prime "p" is coprime to the order of "G". When "p" and |"G"| have a common factor, there are "G"-representations that are not semisimple, which are studied in a subbranch called modular representation theory.
Averaging techniques also show that if F is the real or complex numbers, then any "G"-representation preserves an inner product formula_13 on "V" in the sense that
for all "g" in "G" and "v", "w" in "W". Hence any "G"-representation is unitary.
Unitary representations are automatically semisimple, since Maschke's result can be proven by taking the orthogonal complement of a subrepresentation. When studying representations of groups that are not finite, the unitary representations provide a good generalization of the real and complex representations of a finite group.
Results such as Maschke's theorem and the unitary property that rely on averaging can be generalized to more general groups by replacing the average with an integral, provided that a suitable notion of integral can be defined. This can be done for compact groups or locally compact groups, using Haar measure, and the resulting theory is known as abstract harmonic analysis.
Over arbitrary fields, another class of finite groups that have a good representation theory are the finite groups of Lie type. Important examples are linear algebraic groups over finite fields. The representation theory of linear algebraic groups and Lie groups extends these examples to infinite-dimensional groups, the latter being intimately related to Lie algebra representations. The importance of character theory for finite groups has an analogue in the theory of weights for representations of Lie groups and Lie algebras.
Representations of a finite group "G" are also linked directly to algebra representations via the group algebra F["G"], which is a vector space over F with the elements of "G" as a basis, equipped with the multiplication operation defined by the group operation, linearity, and the requirement that the group operation and scalar multiplication commute.
Modular representations.
Modular representations of a finite group "G" are representations over a field whose characteristic is not coprime to |"G"|, so that Maschke's theorem no longer holds (because |"G"| is not invertible in F and so one cannot divide by it). Nevertheless, Richard Brauer extended much of character theory to modular representations, and this theory played an important role in early progress towards the classification of finite simple groups, especially for simple groups whose characterization was not amenable to purely group-theoretic methods because their Sylow 2-subgroups were "too small".
As well as having applications to group theory, modular representations arise naturally in other branches of mathematics, such as algebraic geometry, coding theory, combinatorics and number theory.
Unitary representations.
A unitary representation of a group "G" is a linear representation "φ" of "G" on a real or (usually) complex Hilbert space "V" such that "φ"("g") is a unitary operator for every "g" ∈ "G". Such representations have been widely applied in quantum mechanics since the 1920s, thanks in particular to the influence of Hermann Weyl, and this has inspired the development of the theory, most notably through the analysis of representations of the Poincaré group by Eugene Wigner. One of the pioneers in constructing a general theory of unitary representations (for any group "G" rather than just for particular groups useful in applications) was George Mackey, and an extensive theory was developed by Harish-Chandra and others in the 1950s and 1960s.
A major goal is to describe the "unitary dual", the space of irreducible unitary representations of "G". The theory is most well-developed in the case that "G" is a locally compact (Hausdorff) topological group and the representations are strongly continuous. For "G" abelian, the unitary dual is just the space of characters, while for "G" compact, the Peter–Weyl theorem shows that the irreducible unitary representations are finite-dimensional and the unitary dual is discrete. For example, if "G" is the circle group "S"1, then the characters are given by integers, and the unitary dual is Z.
For non-compact "G", the question of which representations are unitary is a subtle one. Although irreducible unitary representations must be "admissible" (as Harish-Chandra modules) and it is easy to detect which admissible representations have a nondegenerate invariant sesquilinear form, it is hard to determine when this form is positive definite. An effective description of the unitary dual, even for relatively well-behaved groups such as real reductive Lie groups (discussed below), remains an important open problem in representation theory. It has been solved for many particular groups, such as SL(2,R) and the Lorentz group.
Harmonic analysis.
The duality between the circle group "S"1 and the integers Z, or more generally, between a torus "T""n" and Z"n" is well known in analysis as the theory of Fourier series, and the Fourier transform similarly expresses the fact that the space of characters on a real vector space is the dual vector space. Thus unitary representation theory and harmonic analysis are intimately related, and abstract harmonic analysis exploits this relationship, by developing the analysis of functions on locally compact topological groups and related spaces.
A major goal is to provide a general form of the Fourier transform and the Plancherel theorem. This is done by constructing a measure on the unitary dual and an isomorphism between the regular representation of "G" on the space L2("G") of square integrable functions on "G" and its representation on the space of L2 functions on the unitary dual. Pontrjagin duality and the Peter–Weyl theorem achieve this for abelian and compact "G" respectively.
Another approach involves considering all unitary representations, not just the irreducible ones. These form a category, and Tannaka–Krein duality provides a way to recover a compact group from its category of unitary representations.
If the group is neither abelian nor compact, no general theory is known with an analogue of the Plancherel theorem or Fourier inversion, although Alexander Grothendieck extended Tannaka–Krein duality to a relationship between linear algebraic groups and tannakian categories.
Harmonic analysis has also been extended from the analysis of functions on a group "G" to functions on homogeneous spaces for "G". The theory is particularly well developed for symmetric spaces and provides a theory of automorphic forms (discussed below).
Lie groups.
A Lie group is a group that is also a smooth manifold. Many classical groups of matrices over the real or complex numbers are Lie groups. Many of the groups important in physics and chemistry are Lie groups, and their representation theory is crucial to the application of group theory in those fields.
The representation theory of Lie groups can be developed first by considering the compact groups, to which results of compact representation theory apply. This theory can be extended to finite-dimensional representations of semisimple Lie groups using Weyl's unitary trick: each semisimple real Lie group "G" has a complexification, which is a complex Lie group "G"c, and this complex Lie group has a maximal compact subgroup "K". The finite-dimensional representations of "G" closely correspond to those of "K".
A general Lie group is a semidirect product of a solvable Lie group and a semisimple Lie group (the Levi decomposition). The classification of representations of solvable Lie groups is intractable in general, but often easy in practical cases. Representations of semidirect products can then be analysed by means of general results called "Mackey theory", which is a generalization of the methods used in Wigner's classification of representations of the Poincaré group.
Lie algebras.
A Lie algebra over a field F is a vector space over F equipped with a skew-symmetric bilinear operation called the Lie bracket, which satisfies the Jacobi identity. Lie algebras arise in particular as tangent spaces to Lie groups at the identity element, leading to their interpretation as "infinitesimal symmetries". An important approach to the representation theory of Lie groups is to study the corresponding representation theory of Lie algebras, but representations of Lie algebras also have an intrinsic interest.
Lie algebras, like Lie groups, have a Levi decomposition into semisimple and solvable parts, with the representation theory of solvable Lie algebras being intractable in general. In contrast, the finite-dimensional representations of semisimple Lie algebras are completely understood, after work of Élie Cartan. A representation of a semisimple Lie algebra g is analysed by choosing a Cartan subalgebra, which is essentially a generic maximal subalgebra h of g on which the Lie bracket is zero ("abelian"). The representation of g can be decomposed into weight spaces that are eigenspaces for the action of h and the infinitesimal analogue of characters. The structure of semisimple Lie algebras then reduces the analysis of representations to easily understood combinatorics of the possible weights that can occur.
Infinite-dimensional Lie algebras.
There are many classes of infinite-dimensional Lie algebras whose representations have been studied. Among these, an important class are the Kac–Moody algebras. They are named after Victor Kac and Robert Moody, who independently discovered them. These algebras form a generalization of finite-dimensional semisimple Lie algebras, and share many of their combinatorial properties. This means that they have a class of representations that can be understood in the same way as representations of semisimple Lie algebras.
Affine Lie algebras are a special case of Kac–Moody algebras, which have particular importance in mathematics and theoretical physics, especially conformal field theory and the theory of exactly solvable models. Kac discovered an elegant proof of certain combinatorial identities, Macdonald identities, which is based on the representation theory of affine Kac–Moody algebras.
Lie superalgebras.
Lie superalgebras are generalizations of Lie algebras in which the underlying vector space has a Z2-grading, and skew-symmetry and Jacobi identity properties of the Lie bracket are modified by signs. Their representation theory is similar to the representation theory of Lie algebras.
Linear algebraic groups.
Linear algebraic groups (or more generally, affine group schemes) are analogues in algebraic geometry of Lie groups, but over more general fields than just R or C. In particular, over finite fields, they give rise to finite groups of Lie type. Although linear algebraic groups have a classification that is very similar to that of Lie groups, their representation theory is rather different (and much less well understood) and requires different techniques, since the Zariski topology is relatively weak, and techniques from analysis are no longer available.
Invariant theory.
Invariant theory studies actions on algebraic varieties from the point of view of their effect on functions, which form representations of the group. Classically, the theory dealt with the question of explicit description of polynomial functions that do not change, or are "invariant", under the transformations from a given linear group. The modern approach analyses the decomposition of these representations into irreducibles.
Invariant theory of infinite groups is inextricably linked with the development of linear algebra, especially, the theories of quadratic forms and determinants. Another subject with strong mutual influence is projective geometry, where invariant theory can be used to organize the subject, and during the 1960s, new life was breathed into the subject by David Mumford in the form of his geometric invariant theory.
The representation theory of semisimple Lie groups has its roots in invariant theory and the strong links between representation theory and algebraic geometry have many parallels in differential geometry, beginning with Felix Klein's Erlangen program and Élie Cartan's connections, which place groups and symmetry at the heart of geometry. Modern developments link representation theory and invariant theory to areas as diverse as holonomy, differential operators and the theory of several complex variables.
Automorphic forms and number theory.
Automorphic forms are a generalization of modular forms to more general analytic functions, perhaps of several complex variables, with similar transformation properties. The generalization involves replacing the modular group PSL2 (R) and a chosen congruence subgroup by a semisimple Lie group "G" and a discrete subgroup "Γ". Just as modular forms can be viewed as differential forms on a quotient of the upper half space H = PSL2 (R)/SO(2), automorphic forms can be viewed as differential forms (or similar objects) on "Γ"\"G"/"K", where "K" is (typically) a maximal compact subgroup of "G". Some care is required, however, as the quotient typically has singularities. The quotient of a semisimple Lie group by a compact subgroup is a symmetric space and so the theory of automorphic forms is intimately related to harmonic analysis on symmetric spaces.
Before the development of the general theory, many important special cases were worked out in detail, including the Hilbert modular forms and Siegel modular forms. Important results in the theory include the Selberg trace formula and the realization by Robert Langlands that the Riemann-Roch theorem could be applied to calculate the dimension of the space of automorphic forms. The subsequent notion of "automorphic representation" has proved of great technical value for dealing with the case that "G" is an algebraic group, treated as an adelic algebraic group. As a result, an entire philosophy, the Langlands program has developed around the relation between representation and number theoretic properties of automorphic forms.
Associative algebras.
In one sense, associative algebra representations generalize both representations of groups and Lie algebras. A representation of a group induces a representation of a corresponding group ring or group algebra, while representations of a Lie algebra correspond bijectively to representations of its universal enveloping algebra. However, the representation theory of general associative algebras does not have all of the nice properties of the representation theory of groups and Lie algebras.
Module theory.
When considering representations of an associative algebra, one can forget the underlying field, and simply regard the associative algebra as a ring, and its representations as modules. This approach is surprisingly fruitful: many results in representation theory can be interpreted as special cases of results about modules over a ring.
Hopf algebras and quantum groups.
Hopf algebras provide a way to improve the representation theory of associative algebras, while retaining the representation theory of groups and Lie algebras as special cases. In particular, the tensor product of two representations is a representation, as is the dual vector space.
The Hopf algebras associated to groups have a commutative algebra structure, and so general Hopf algebras are known as quantum groups, although this term is often restricted to certain Hopf algebras arising as deformations of groups or their universal enveloping algebras. The representation theory of quantum groups has added surprising insights to the representation theory of Lie groups and Lie algebras, for instance through the crystal basis of Kashiwara.
Generalizations.
Set-theoretic representations.
A "set-theoretic representation" (also known as a group action or "permutation representation") of a group "G" on a set "X" is given by a function ρ from "G" to "X""X", the set of functions from "X" to "X", such that for all "g"1, "g"2 in "G" and all "x" in "X":
This condition and the axioms for a group imply that ρ("g") is a bijection (or permutation) for all "g" in "G". Thus we may equivalently define a permutation representation to be a group homomorphism from G to the symmetric group S"X" of "X".
Representations in other categories.
Every group "G" can be viewed as a category with a single object; morphisms in this category are just the elements of "G". Given an arbitrary category "C", a "representation" of "G" in "C" is a functor from "G" to "C". Such a functor selects an object "X" in "C" and a group homomorphism from "G" to Aut("X"), the automorphism group of "X".
In the case where "C" is VectF, the category of vector spaces over a field F, this definition is equivalent to a linear representation. Likewise, a set-theoretic representation is just a representation of "G" in the category of sets.
For another example consider the category of topological spaces, Top. Representations in Top are homomorphisms from "G" to the homeomorphism group of a topological space "X".
Two types of representations closely related to linear representations are:
Representations of categories.
Since groups are categories, one can also consider representation of other categories. The simplest generalization is to monoids, which are categories with one object. Groups are monoids for which every morphism is invertible. General monoids have representations in any category. In the category of sets, these are monoid actions, but monoid representations on vector spaces and other objects can be studied.
More generally, one can relax the assumption that the category being represented has only one object. In full generality, this is simply the theory of functors between categories, and little can be said.
One special case has had a significant impact on representation theory, namely the representation theory of quivers. A quiver is simply a directed graph (with loops and multiple arrows allowed), but it can be made into a category (and also an algebra) by considering paths in the graph. Representations of such categories/algebras have illuminated several aspects of representation theory, for instance by allowing non-semisimple representation theory questions about a group to be reduced in some cases to semisimple representation theory questions about a quiver.

</doc>
<doc id="23017591" url="https://en.wikipedia.org/wiki?curid=23017591" title="Resonance (particle physics)">
Resonance (particle physics)

In particle physics, a resonance is the peak located around a certain energy found in differential cross sections of scattering experiments. These peaks are associated with subatomic particles (such as nucleons, delta baryons, upsilon mesons) and their excitations. The width of the resonance ("Γ") is related to the lifetime ("τ") of the particle (or its excited state) by the relation
where "h" is the Planck constant.

</doc>
<doc id="3031501" url="https://en.wikipedia.org/wiki?curid=3031501" title="Rho meson">
Rho meson

In particle physics, a rho meson is a short-lived hadronic particle that is an isospin triplet whose three states are denoted as , and . After the pions and kaons, the rho mesons are the lightest strongly interacting particle with a mass of roughly for all three states. There should be a small mass difference between the and the that can be attributed to the electromagnetic self-energy of the particle as well as a small effect due to isospin breaking arising from the light quark masses; however, the current experimental limit is that this mass difference is less than .
The rho mesons have a very short lifetime and their decay width is about with the peculiar feature that the decay widths are not described by a Breit-Wigner form. The principal decay route of the rho mesons is to a pair of pions with a branching rate of 99.9%. Neutral rho mesons can decay to a pair of electrons or muons which occurs with a branching ratio of . This decay of the neutral rho to leptons can be interpreted as a mixing between the photon and rho. In principle the charged rho mesons mix with the weak vector bosons and can lead to decay to an electron or muon plus a neutrino; however, this has never been observed.
In the De Rujula–Georgi–Glashow description of hadrons, the rho mesons can be interpreted as a bound state of a quark and an anti-quark and is an excited version of the pion. Unlike the pion, the rho meson has spin "j" = 1 (a vector meson) and a much higher value of the mass. This mass difference between the pions and rho mesons is attributed to a large hyperfine interaction between the quark and anti-quark. The main objection with the De Rujula–Georgi–Glashow description is that it attributes the lightness of the pions as an accident rather than a result of chiral symmetry breaking.
The rho mesons can be thought of as the gauge bosons of a spontaneously broken gauge symmetry whose local character is emergent (arising from QCD); Note that this broken gauge symmetry (sometimes called hidden local symmetry) is distinct from the global chiral symmetry acting on the flavors. This was described by Howard Georgi in a paper titled "The Vector Limit of Chiral Symmetry" where he ascribed much of the literature of hidden local symmetry to a non-linear sigma model.
[a] PDG reports the resonance width (Γ). Here the conversion τ =  is given instead. <br>
[b] The exact value depends on the method used. See the given reference for detail.

</doc>
<doc id="130526" url="https://en.wikipedia.org/wiki?curid=130526" title="Riemann curvature tensor">
Riemann curvature tensor

In the mathematical field of differential geometry, the Riemann curvature tensor or Riemann–Christoffel tensor (after Bernhard Riemann and Elwin Bruno Christoffel) is the most common method used to express the curvature of Riemannian manifolds. It associates a tensor to each point of a Riemannian manifold (i.e., it is a tensor field), that measures the extent to which the metric tensor is not locally isometric to that of Euclidean space. The curvature tensor can also be defined for any pseudo-Riemannian manifold, or indeed any manifold equipped with an affine connection. It is a central mathematical tool in the theory of general relativity, the modern theory of gravity, and the curvature of spacetime is in principle observable via the geodesic deviation equation. The curvature tensor represents the tidal force experienced by a rigid body moving along a geodesic in a sense made precise by the Jacobi equation.
The curvature tensor is given in terms of the Levi-Civita connection formula_1 by the following formula:
where ["u","v"] is the Lie bracket of vector fields. For each pair of tangent vectors "u", "v", "R"("u","v") is a linear transformation of the tangent space of the manifold. It is linear in "u" and "v", and so defines a tensor. Occasionally, the curvature tensor is defined with the opposite sign.
If formula_3 and formula_4 are coordinate vector fields then formula_5 and therefore the formula simplifies to 
The curvature tensor measures "noncommutativity of the covariant derivative", and as such is the integrability obstruction for the existence of an isometry with Euclidean space (called, in this context, "flat" space). The linear transformation formula_7 is also called the curvature transformation or endomorphism.
The curvature formula can also be expressed in terms of the second covariant derivative defined as:
which is linear in "u" and "v". Then:
Thus in the general case of non-coordinate vectors "u" and "v", the curvature tensor measures the noncommutativity of the second covariant derivative.
Geometrical meaning.
Informally.
Imagine walking around the bounding white line of a tennis court with a stick held out in front of you. When you reach the first corner of the court, you turn to follow the white line, but you keep the stick held out in the same direction, which means you are now holding the stick out to your side. You do the same when you reach each corner of the court. When you get back to where you started, you are holding the stick out in exactly the same direction as you were when you started (no surprise there).
Now imagine you are standing on the equator of the earth, facing north with the stick held out in front of you. You walk north up along a line of longitude until you get to the north pole. At that point you turn right, ninety degrees, but you keep the stick held out in the same direction, which means you are now holding the stick out to your left. You keep walking until you get to the equator. There, you turn right again (and so now you have to hold the stick pointing out behind you) and walk along the equator until you get back to where you started from. But here is the thing: the stick is pointing back along the equator from where you just came, not north up to the pole how it was when you started!
The reason for the difference is that the surface of the earth is curved, whereas the surface of a tennis court is flat, but it is not quite that simple. Imagine that the tennis court is slightly humped along its centre-line so that it is like part of the surface of a cylinder. If you walk around the court again, the stick still points in the same direction as it did when you started. This is a consequence of that the tennis court still has zero Gaussian curvature (such as for the surface of a sheet of paper that is bent but not stretched) and the Gauss–Bonnet theorem.
The Riemann curvature tensor is a way to capture a measure of the intrinsic curvature. When you write it down in terms of its components (like writing down the components of a vector), it consists of a multi-dimensional array of sums and products of partial derivatives (some of those partial derivatives can be thought of as akin to capturing the curvature imposed upon someone walking in straight lines on a curved surface).
Formally.
When a vector in a Euclidean space is parallel transported around a loop, it will again point in the initial direction after returning to its original position. However, this property does not hold in the general case. The Riemann curvature tensor directly measures the failure of this in a general Riemannian manifold. This failure is known as the non-holonomy of the manifold. 
Let "x""t" be a curve in a Riemannian manifold "M". Denote by τ"x""t" : Tx0"M" → Txt"M" the parallel transport map along "x"t. The parallel transport maps are related to the covariant derivative by
for each vector field "Y" defined along the curve.
Suppose that "X" and "Y" are a pair of commuting vector fields. Each of these fields generates a one-parameter group of diffeomorphisms in a neighborhood of "x"0. Denote by τtX and τtY, respectively, the parallel transports along the flows of "X" and "Y" for time "t". Parallel transport of a vector "Z" ∈ Tx0"M" around the quadrilateral with sides "tY", "sX", −"tY", −"sX" is given by
This measures the failure of parallel transport to return "Z" to its original position in the tangent space Tx0"M". Shrinking the loop by sending "s", "t" → 0 gives the infinitesimal description of this deviation:
where "R" is the Riemann curvature tensor.
Coordinate expression.
Converting to the tensor index notation, the Riemann curvature tensor is given by
where formula_14 are the coordinate vector fields. The above expression can be written using Christoffel symbols:
(see also the list of formulas in Riemannian geometry).
The Riemann curvature tensor is also the commutator of the covariant derivative of an arbitrary covector formula_16
with itself:
since the connection formula_18 is torsionless, which means that the torsion tensor formula_19 vanishes.
This formula is often called the "Ricci identity". This is the classical method used by Ricci and Levi-Civita to obtain an expression for the Riemann curvature tensor. In this way, the tensor character of the set of quantities formula_20 is proved.
This identity can be generalized to get the commutators for two covariant derivatives of arbitrary tensors as follows
This formula also applies to tensor densities without alteration, because for the Levi-Civita ("not generic") connection one gets:
It is sometimes convenient to also define the purely covariant version by
Symmetries and identities.
The Riemann curvature tensor has the following symmetries:
Here the bracket formula_27 refers to the inner product on the tangent space induced by the metric tensor. The last identity was discovered by Ricci, but is often called the first Bianchi identity or algebraic Bianchi identity, because it looks similar to the Bianchi identity below. (Also, if there is nonzero torsion, the first Bianchi identity becomes a differential identity of the torsion tensor.)
These three identities form a complete list of symmetries of the curvature tensor, i.e. given any tensor which satisfies the identities above, one can find a Riemannian manifold with such a curvature tensor at some point. Simple calculations show that such a tensor has formula_28 independent components.
Yet another useful identity follows from these three:
On a Riemannian manifold one has the covariant derivative formula_30 and the Bianchi identity (often called the second Bianchi identity or differential Bianchi identity) takes the form: 
Given any coordinate chart about some point on the manifold, the above identities may be written in terms of the components of the Riemann tensor at this point as:
The algebraic symmetries are also equivalent to saying that "R" belongs to the image of the Young symmetrizer corresponding to the partition 2+2.
Special cases.
For a two-dimensional surface, the Bianchi identities imply that the Riemann tensor can be expressed as
where formula_39 is the metric tensor and formula_40 is a function called the Gaussian curvature and "a", "b", "c" and "d" take values either 1 or 2. The Riemann tensor has only one functionally independent component. The Gaussian curvature coincides with the sectional curvature of the surface. It is also exactly half the scalar curvature of the 2-manifold, while the Ricci curvature tensor of the surface is simply given by 
A Riemannian manifold is a space form if its sectional curvature is equal to a constant "K". The Riemann tensor of a space form is given by
Conversely, except in dimension 2, if the curvature of a Riemannian manifold has this form for some function "K", then the Bianchi identities imply that "K" is constant and thus that the manifold is (locally) a space form.

</doc>
<doc id="173181" url="https://en.wikipedia.org/wiki?curid=173181" title="Riemann surface">
Riemann surface

In mathematics, particularly in complex analysis, a Riemann surface, first studied by and named after Bernhard Riemann, is a one-dimensional complex manifold. Riemann surfaces can be thought of as deformed versions of the complex plane: locally near every point they look like patches of the complex plane, but the global topology can be quite different. For example, they can look like a sphere or a torus or several sheets glued together.
The main point of Riemann surfaces is that holomorphic functions may be defined between them. Riemann surfaces are nowadays considered the natural setting for studying the global behavior of these functions, especially multi-valued functions such as the square root and other algebraic functions, or the logarithm.
Every Riemann surface is a two-dimensional real analytic manifold (i.e., a surface), but it contains more structure (specifically a complex structure) which is needed for the unambiguous definition of holomorphic functions. A two-dimensional real manifold can be turned into a Riemann surface (usually in several inequivalent ways) if and only if it is orientable and metrizable. So the sphere and torus admit complex structures, but the Möbius strip, Klein bottle and projective plane do not.
Geometrical facts about Riemann surfaces are as "nice" as possible, and they often provide the intuition and motivation for generalizations to other curves, manifolds or varieties. The Riemann–Roch theorem is a prime example of this influence.
Definitions.
There are several equivalent definitions of a Riemann surface.
A complex structure gives rise to a conformal structure by choosing the standard Euclidean metric given on the complex plane and transporting it to "X" by means of the charts. Showing that a conformal structure determines a complex structure is more difficult.
Further definitions and properties.
As with any map between complex manifolds, a function "f": "M" → "N" between two Riemann surfaces "M" and "N" is called "holomorphic" if for every chart "g" in the atlas of "M" and every chart "h" in the atlas of "N", the map "h" o "f" o "g"−1 is holomorphic (as a function from C to C) wherever it is defined. The composition of two holomorphic maps is holomorphic. The two Riemann surfaces "M" and "N" are called "biholomorphic" (or "conformally equivalent" to emphasize the conformal point of view) if there exists a bijective holomorphic function from "M" to "N" whose inverse is also holomorphic (it turns out that the latter condition is automatic and can therefore be omitted). Two conformally equivalent Riemann surfaces are for all practical purposes identical.
Orientability.
We noted in the preamble that all Riemann surfaces, like all complex manifolds, are orientable as a real manifold. The reason is that for complex charts "f" and "g" with transition function "h" = "f"("g"−1("z")) we can consider "h" as a map from an open set of R2 to R2 whose Jacobian in a point "z" is just the real linear map given by multiplication by the complex number "h"'("z"). However, the real determinant of multiplication by a complex number "α" equals |"α"|2, so the Jacobian of "h" has positive determinant. Consequently the complex atlas is an oriented atlas.
Functions.
Every non-compact Riemann surface admits non-constant holomorphic functions (with values in C). In fact, every non-compact Riemann surface is a Stein manifold.
In contrast, on a compact Riemann surface "X" every holomorphic function with value in C is constant due to the maximum principle. However, there always exists non-constant meromorphic functions (holomorphic functions with values in the Riemann sphere C ∪ {∞}). More precisely, the function field of "X" is a finite extension of C("t"), the function field in one variable, i.e. any two meromorphic functions are algebraically dependent. This statement generalizes to higher dimensions, see .
Analytic vs. algebraic.
The above fact about existence of nonconstant meromorphic functions can be used to show that any compact Riemann surface is a projective variety, i.e. can be given by polynomial equations inside a projective space. Actually, it can be shown that every compact Riemann surface can be embedded into complex projective 3-space. This is a surprising theorem: Riemann surfaces are given by locally patching charts. If one global condition, namely compactness, is added, the surface is necessarily algebraic. This feature of Riemann surfaces allows one to study them with either the means of analytic or algebraic geometry. The corresponding statement for higher-dimensional objects is false, i.e. there are compact complex 2-manifolds which are not algebraic. On the other hand, every projective complex manifold is necessarily algebraic, see Chow's theorem.
As an example, consider the torus "T" := C/(Z + "τ" Z). The Weierstrass function formula_1 belonging to the lattice Z + "τ" Z is a meromorphic function on "T". This function and its derivative formula_2 generate the function field of "T". There is an equation
where the coefficients "g"2 and "g"3 depend on τ, thus giving an elliptic curve "E"τ in the sense of algebraic geometry. Reversing this is accomplished by the j-invariant "j"("E"), which can be used to determine "τ" and hence a torus.
Classification of Riemann surfaces.
The realm of Riemann surfaces can be divided into three regimes: hyperbolic, parabolic and elliptic Riemann surfaces, with the distinction given by the uniformization theorem. Geometrically, these correspond to negative curvature, zero curvature/flat, and positive curvature: stating the uniformization theorem in terms of conformal geometry, every connected Riemann surface "X" admits a complete 2-dimensional real Riemann metric with constant curvature −1, 0 or 1 inducing the same conformal structure – every metric is conformally equivalent to a constant curvature metric. The surface "X" is called hyperbolic, parabolic, and elliptic, respectively.
For simply connected Riemann surfaces, the uniformization theorem states that every simply connected Riemann surface is conformally equivalent to one of the following:
The existence of these three types parallels the several non-Euclidean geometries.
The general technique of associating to a manifold "X" its universal cover "Y", and expressing the original "X" as the quotient of "Y" by the group of deck transformations gives a first overview over Riemann surfaces.
Elliptic Riemann surfaces.
By definition, these are the surfaces "X" with constant curvature +1. The Riemann sphere C ∪ {∞} is the only example. (Elliptic functions are examples of parabolic Riemann surfaces. The naming comes from the history: elliptic functions are associated to elliptic integrals, which in turn show up in calculating the circumference of ellipses).
Parabolic Riemann surfaces.
By definition, these are the surfaces "X" with constant curvature 0. Equivalently, by the uniformization theorem, the universal cover of "X" has to be the complex plane.
There are then three possibilities for "X". It can be the plane itself, the punctured plane (or cylinder), or a torus
The set of representatives of the cosets are called fundamental domains.
Care must be taken insofar as two tori are always homeomorphic, but in general not biholomorphic to each other. This is the first appearance of the problem of moduli. The modulus of a torus can be captured by a single complex number τ with positive imaginary part. In fact, the marked moduli space (Teichmüller space) of the torus is biholomorphic to the upper half-plane or equivalently the open unit disk.
Hyperbolic Riemann surfaces.
The Riemann surfaces with curvature −1 are called "hyperbolic". This group is the "biggest" one.
The celebrated Riemann mapping theorem states that any simply connected open strict subset of the complex plane is biholomorphic to the unit disk. Therefore the open disk with the Poincaré-metric of constant curvature −1 is the local model of any hyperbolic Riemann surface. According to the uniformization theorem above, all hyperbolic surfaces are quotients of the unit disk.
Examples include all surfaces with genus "g" > 1 such as hyper-elliptic curves.
For every hyperbolic Riemann surface, the fundamental group is isomorphic to a Fuchsian group, and thus the surface can be modelled by a Fuchsian model H/Γ where H is the upper half-plane and Γ is the Fuchsian group. The set of representatives of the cosets of H/Γ are free regular sets and can be fashioned into metric fundamental polygons. Quotient structures as H/Γ are generalized to Shimura varieties.
Unlike elliptic and parabolic surfaces, no classification of the hyperbolic surfaces is possible. Any connected open strict subset of the plane gives a hyperbolic surface; consider the plane minus a Cantor set. A classification "is" possible for surfaces of finite type: those isomorphic to a compact surface with a finite number of points removed. Any one of these has a finite number of moduli and so a finite-dimensional Teichmüller space. The problem of moduli (solved by Lars Ahlfors and extended by Lipman Bers) was to justify Riemann's claim that for a closed surface of genus g, 3"g" − 3 complex parameters suffice.
When a hyperbolic surface is compact, then the total area of the surface is 4π("g" − 1), where "g" is the genus of the surface; the area is obtained by applying the Gauss–Bonnet theorem to the area of the fundamental polygon.
Maps between Riemann surfaces.
The geometric classification is reflected in maps between Riemann surfaces,
as detailed in Liouville's theorem and the Little Picard theorem: maps from hyperbolic to parabolic to elliptic are easy, but maps from elliptic to parabolic or parabolic to hyperbolic are very constrained (indeed, generally constant!). There are inclusions of the disc in the plane in the sphere: formula_5 but any meromorphic map from the sphere to the plane is constant, any holomorphic map from the plane into the unit disk is constant (Liouville's theorem), and in fact any holomorphic map from the plane into the plane minus two points is constant (Little Picard theorem)!
Punctured spheres.
These statements are clarified by considering the type of a Riemann sphere formula_6 with a number of punctures. With no punctures, it is the Riemann sphere, which is elliptic. With one puncture, which can be placed at infinity, it is the complex plane, which is parabolic. With two punctures, it is the punctured plane or alternatively annulus or cylinder, which is parabolic. With three or more punctures, it is hyperbolic – compare pair of pants. One can map from one puncture to two, via the exponential map (which is entire and has an essential singularity at infinity, so not defined at infinity, and misses zero and infinity), but all maps from zero punctures to one or more, or one or two punctures to three or more are constant.
Ramified covering spaces.
Continuing in this vein, compact Riemann surfaces can map to surfaces of "lower" genus, but not to "higher" genus, except as constant maps. This is because holomorphic and meromorphic maps behave locally like formula_7 so non-constant maps are ramified covering maps, and for compact Riemann surfaces these are constrained by the Riemann–Hurwitz formula in algebraic topology, which relates the Euler characteristic of a space and a ramified cover.
For example, hyperbolic Riemann surfaces are ramified covering spaces of the sphere (they have non-constant meromorphic functions), but the sphere does not cover or otherwise map to higher genus surfaces, except as a constant.
Isometries of Riemann surfaces.
The isometry group of a uniformized Riemann surface (equivalently, the conformal automorphism group) reflects its geometry:
Function-theoretic classification.
The classification scheme above is typically used by geometers. There is a different classification for Riemann surfaces which is typically used by complex analysts. It employs a different definition for "parabolic" and "hyperbolic". In this alternative classification scheme, a Riemann surface is called "parabolic" if there are no nonconstant negative subharmonic functions on the surface and is otherwise called "hyperbolic". This class of hyperbolic surfaces is further subdivided into subclasses according to whether function spaces other than the negative subharmonic functions are degenerate, e.g. Riemann surfaces on which all bounded holomorphic functions are constant, or on which all bounded harmonic functions are constant, or on which all positive harmonic functions are constant, etc.
To avoid confusion, call the classification based on metrics of constant curvature the "geometric classification", and the one based on degeneracy of function spaces "the function-theoretic classification". For example, the Riemann surface consisting of "all complex numbers but 0 and 1" is parabolic in the function-theoretic classification but it is hyperbolic in the geometric classification.

</doc>
<doc id="195243" url="https://en.wikipedia.org/wiki?curid=195243" title="Riemannian geometry">
Riemannian geometry

Riemannian geometry is the branch of differential geometry that studies Riemannian manifolds, smooth manifolds with a "Riemannian metric", i.e. with an inner product on the tangent space at each point that varies smoothly from point to point. This gives, in particular, local notions of angle, length of curves, surface area, and volume. From those some other global quantities can be derived by integrating local contributions.
Riemannian geometry originated with the vision of Bernhard Riemann expressed in his inaugurational lecture "Ueber die Hypothesen, welche der Geometrie zu Grunde liegen" ("On the Hypotheses which lie at the Bases of Geometry"). It is a very broad and abstract generalization of the differential geometry of surfaces in R3. Development of Riemannian geometry resulted in synthesis of diverse results concerning the geometry of surfaces and the behavior of geodesics on them, with techniques that can be applied to the study of differentiable manifolds of higher dimensions. It enabled Einstein's general relativity theory, made profound impact on group theory and representation theory, as well as analysis, and spurred the development of algebraic and differential topology.
Introduction.
Riemannian geometry was first put forward in generality by Bernhard Riemann in the 19th century. It deals with a broad range of geometries whose metric properties vary from point to point, including the standard types of Non-Euclidean geometry.
Any smooth manifold admits a Riemannian metric, which often helps to solve problems of differential topology. It also serves as an entry level for the more complicated structure of pseudo-Riemannian manifolds, which (in four dimensions) are the main objects of the theory of general relativity. Other generalizations of Riemannian geometry include Finsler geometry.
There exists a close analogy of differential geometry with the mathematical structure of defects in regular crystals. Dislocations and Disclinations produce torsions and curvature.
The following articles provide some useful introductory material:
Classical theorems in Riemannian geometry.
What follows is an incomplete list of the most classical theorems in Riemannian geometry. The choice is made depending on its importance, beauty, and simplicity of formulation. Most of the results can be found in the classic monograph by Jeff Cheeger and D. Ebin (see below).
The formulations given are far from being very exact or the most general. This list is oriented to those who already know the basic definitions and want to know what these definitions are about.
Geometry in large.
In all of the following theorems we assume some local behavior of the space (usually formulated using curvature assumption) to derive some information about the global structure of the space, including either some information on the topological type of the manifold or on the behavior of points at "sufficiently large" distances.

</doc>
<doc id="144652" url="https://en.wikipedia.org/wiki?curid=144652" title="Riemannian manifold">
Riemannian manifold

In differential geometry, a (smooth) Riemannian manifold or (smooth) Riemannian space ("M","g") is a real smooth manifold "M" equipped with an inner product formula_1 on the tangent space formula_2 at each point formula_3
that varies smoothly from point to point in the sense that if "X" and "Y" are vector fields on "M", then 
formula_4 is a smooth function. 
The family formula_1 of inner products is called a Riemannian metric (tensor). 
These terms are named after the German mathematician Bernhard Riemann.
The study of Riemannian manifolds constitutes the subject called Riemannian geometry.
A Riemannian metric (tensor) makes it possible to define various geometric notions on a Riemannian manifold, such as angles, lengths of curves, areas (or volumes), curvature, gradients of functions and divergence of vector fields.
Introduction.
In 1828, Carl Friedrich Gauss proved his Theorema Egregium ("remarkable theorem" in Latin), establishing an important property of surfaces. Informally, the theorem says that the curvature of a surface can be determined entirely by measuring distances along paths on the surface. That is, curvature does not depend on how the surface might be embedded in 3-dimensional space. "See" differential geometry of surfaces. Bernhard Riemann extended Gauss's theory to higher-dimensional spaces called manifolds in a way that also allows distances and angles to be measured and the notion of curvature to be defined, again in a way that was intrinsic to the manifold and not dependent upon its embedding in higher-dimensional spaces. Albert Einstein used the theory of Riemannian manifolds to develop his general theory of relativity. In particular, his equations for gravitation are constraints on the curvature of space.
Overview.
The tangent bundle of a smooth manifold "M" assigns to each fixed point of "M" a vector space called the tangent space, and each tangent space can be equipped with an inner product. If such a collection of inner products on the tangent bundle of a manifold varies smoothly as one traverses the manifold, then concepts that were defined only pointwise at each tangent space can be extended to yield analogous notions over finite regions of the manifold. For example, a smooth curve α("t"): [0, 1] → "M" has tangent vector α′("t"0) in the tangent space T"M"(α("t"0)) at any point "t"0 ∈ (0, 1), and each such vector has length ‖α′("t"0)‖, where ‖·‖ denotes the norm induced by the inner product on T"M"(α("t"0)). The integral of these lengths gives the length of the curve α:
Smoothness of α("t") for "t" in [0, 1] guarantees that the integral "L"(α) exists and the length of this curve is defined.
In many instances, in order to pass from a linear-algebraic concept to a differential-geometric one, the smoothness requirement is very important.
Every smooth submanifold of R"n" has an induced Riemannian metric "g": the inner product on each tangent space is the restriction of the inner product on R"n". In fact, as follows from the Nash embedding theorem, all Riemannian manifolds can be realized this way.
In particular one could "define" Riemannian manifold as a metric space which is isometric to a smooth submanifold of R"n" with the induced intrinsic metric, where isometry here is meant in the sense of preserving the length of curves. This definition might theoretically not be flexible enough, but it is quite useful to build the first geometric intuitions in Riemannian geometry.
Riemannian manifolds as metric spaces.
Usually a Riemannian manifold is defined as a smooth manifold with a smooth section of the positive-definite quadratic forms on the tangent bundle. Then one has to work to show that it can be turned to a metric space:
If γ: ["a", "b"] → "M" is a continuously differentiable curve in the Riemannian manifold "M", then we define its length "L"(γ) in analogy with the example above by
With this definition of length, every connected Riemannian manifold "M" becomes a metric space (and even a length metric space) in a natural fashion: the distance "d"("x", "y") between the points "x" and "y" of "M" is defined as
Even though Riemannian manifolds are usually "curved," there is still a notion of "straight line" on them: the geodesics. These are curves which locally join their points along shortest paths.
Assuming the manifold is compact, any two points "x" and "y" can be connected with a geodesic whose length is "d"("x","y"). Without compactness, this need not be true. For example, in the punctured plane R2 \ {0}, the distance between the points (−1, 0) and (1, 0) is 2, but there is no geodesic realizing this distance.
Properties.
In Riemannian manifolds, the notions of geodesic completeness, topological completeness and metric completeness are the same: that each implies the other is the content of the Hopf–Rinow theorem.
Riemannian metrics.
Let "M" be a differentiable manifold of dimension "n". A Riemannian metric on "M" is a family of (positive definite) inner products
such that, for all differentiable vector fields "X","Y" on "M",
defines a smooth function "M" → R.
In other words, a Riemannian metric "g" is a symmetric (0,2)-tensor that is positive definite (i.e. "g"("X", "X") > 0 for all tangent vectors "X" ≠ 0).
In a system of local coordinates on the manifold "M" given by "n" real-valued functions "x"1,"x"2, …, "x""n", the vector fields
give a basis of tangent vectors at each point of "M". Relative to this coordinate system, the components of the metric tensor are, at each point "p",
Equivalently, the metric tensor can be written in terms of the dual basis {d"x"1, …, d"x""n"} of the cotangent bundle as
Endowed with this metric, the differentiable manifold ("M", "g") is a Riemannian manifold.
The pullback metric.
If "f":"M"→"N" is a differentiable map and ("N","gN") a Riemannian manifold, then the pullback of "g"N along "f" is a quadratic form on the tangent space of "M". The pullback is the quadratic form "f"*"gN" on "TM" defined for "v", "w" ∈ "T"p"M" by
where "df(v)" is the pushforward of "v" by "f".
The quadratic form "f"*"gN" is in general only a semi definite form because "df" can have a kernel. If "f" is a diffeomorphism, or more generally an immersion, then it defines a Riemannian metric on "M", the pullback metric. In particular, every embedded smooth submanifold inherits a metric from being embedded in a Riemannian manifold, and every covering space inherits a metric from covering a Riemannian manifold.
Existence of a metric.
Every paracompact differentiable manifold admits a Riemannian metric. To prove this result, let "M" be a manifold and {("U"α, φ("U"α))|α ∈ "I"} a locally finite atlas of open subsets "U" of "M" and diffeomorphisms onto open subsets of R"n"
Let τα be a differentiable partition of unity subordinate to the given atlas. Then define the metric "g" on "M" by
where "g"can is the Euclidean metric. This is readily seen to be a metric on "M".
Isometries.
Let ("M", "gM") and ("N", "gN") be two Riemannian manifolds, and "f": "M" → "N" be a diffeomorphism. Then, "f" is called an isometry, if
or pointwise
Moreover, a differentiable mapping "f": "M" → "N" is called a local isometry at "p" ∈ "M" if there is a neighbourhood "U" ⊂ "M", "p" ∈ "U", such that "f": "U" → "f(U)" is a diffeomorphism satisfying the previous relation.
Riemannian manifolds as metric spaces.
A connected Riemannian manifold carries the structure of a metric space whose distance function is the arclength of a minimizing geodesic.
Specifically, let ("M","g") be a connected Riemannian manifold. Let "c": ["a,b"] → "M" be a parametrized curve in "M", which is differentiable with velocity vector "c"′. The length of "c" is defined as
By change of variables, the arclength is independent of the chosen parametrization. In particular, a curve ["a,b"] → "M" can be parametrized by its arc length. A curve is parametrized by arclength if and only if formula_30 for all formula_31.
The distance function "d" : "M"×"M" → [0,∞) is defined by
where the infimum extends over all differentiable curves γ beginning at "p" ∈ "M" and ending at "q" ∈ "M".
This function "d" satisfies the properties of a distance function for a metric space. The only property which is not completely straightforward is to show that "d"("p","q") = 0 implies that "p" = "q". For this property, one can use a normal coordinate system, which also allows one to show that the topology induced by "d" is the same as the original topology on "M".
Diameter.
The diameter of a Riemannian manifold "M" is defined by
The diameter is invariant under global isometries. Furthermore, the Heine–Borel property holds for (finite-dimensional) Riemannian manifolds: "M" is compact if and only if it is complete and has finite diameter.
Geodesic completeness.
A Riemannian manifold "M" is geodesically complete if for all "p" ∈ "M", the exponential map formula_34 is defined for all formula_35, i.e. if any geodesic formula_36 starting from "p" is defined for all values of the parameter "t" ∈ R. The Hopf-Rinow theorem asserts that "M" is geodesically complete if and only if it is complete as a metric space.
If "M" is complete, then "M" is non-extendable in the sense that it is not isometric to an open proper submanifold of any other Riemannian manifold. The converse is not true, however: there exist non-extendable manifolds which are not complete.

</doc>
<doc id="221536" url="https://en.wikipedia.org/wiki?curid=221536" title="Right-hand rule">
Right-hand rule

In mathematics and physics, the right-hand rule is a common mnemonic for understanding notation conventions for vectors in three dimensions.
Ampère's right hand screw rules.
Ampère's right hand screw rule (also called "right-hand grip rule", "coffee-mug rule" or the "corkscrew-rule") is used either when a vector (such as the Euler vector) must be defined to represent the rotation of a body, a magnetic field, or a fluid, or vice versa, when it is necessary to define a rotation vector to understand how rotation occurs. It reveals a connection between the current and the magnetic field lines in the magnetic field that the current created.
André-Marie Ampère, a French physicist and mathematician, for whom the rule was named, was inspired by Hans Christian Oersted, another physicist who experimented with magnet needles. Oersted observed that the needles swirled when in the proximity of an electric current-carrying wire, and concluded that electricity could create magnetic fields.
Application.
This version of the rule is used in two complementary applications of Ampère's circuital law:
The rule is also used to determine the direction of the torque vector. When gripping the imaginary axis of rotation of the rotational force so that your fingers point in the direction of the force, the extended thumb points in the direction of the torque vector.
Cross products.
The cross product of two vectors is often taken in physics and engineering. For example, in statics and dynamics, torque is the cross product of lever length and force and angular momentum is the cross product of linear momentum and distance. In electricity and magnetism, the force exerted on a moving charged particle when moving in a magnetic field B is given by:
The direction of the cross product may be found by application of the right hand rule as follows: 
For example, for a positively charged particle moving to the North, in a region where the magnetic field points West, the resultant force points up
Applications.
The right hand rule is in widespread use in physics. A list of physical quantities whose directions are related by the right-hand rule is given below. (Some of these are related only indirectly to cross products, and use the second form.)

</doc>
<doc id="1209823" url="https://en.wikipedia.org/wiki?curid=1209823" title="Rotating reference frame">
Rotating reference frame

A rotating frame of reference is a special case of a non-inertial reference frame that is rotating relative to an inertial reference frame. An everyday example of a rotating reference frame is the surface of the Earth. (This article considers only frames rotating about a fixed axis. For more general rotations, see Euler angles.)
Fictitious forces.
All non-inertial reference frames exhibit fictitious forces. Rotating reference frames are characterized by three fictitious forces:
and, for non-uniformly rotating reference frames,
Scientists living in a rotating box can measure the speed and direction of their rotation by measuring these fictitious forces. For example, Léon Foucault was able to show the Coriolis force that results from the Earth's rotation using the Foucault pendulum. If the Earth were to rotate many times faster, these fictitious forces could be felt by humans, as they are when on a spinning carousel.
Relating rotating frames to stationary frames.
The following is a derivation of the formulas for accelerations as well as fictitious forces in a rotating frame. It begins with the relation between a particle's coordinates in a rotating frame and its coordinates in an inertial (stationary) frame. Then, by taking time derivatives, formulas are derived that relate the velocity of the particle as seen in the two frames, and the acceleration relative to each frame. Using these accelerations, the fictitious forces are identified by comparing Newton's second law as formulated in the two different frames.
Relation between positions in the two frames.
To derive these fictitious forces, it's helpful to be able to convert between the coordinates formula_1 of the rotating reference frame and the coordinates formula_2 of an inertial reference frame with the same origin. If the rotation is about the formula_3 axis with an angular velocity formula_4 and the two reference frames coincide at time formula_5, the transformation from rotating coordinates to inertial coordinates can be written
whereas the reverse transformation is
This result can be obtained from a rotation matrix.
Introduce the unit vectors formula_10 representing standard unit basis vectors in the rotating frame. The time-derivatives of these unit vectors are found next. Suppose the frames are aligned at "t = "0 and the "z"-axis is the axis of rotation. Then for a counterclockwise rotation through angle "Ωt":
where the ("x", "y") components are expressed in the stationary frame. Likewise,
Thus the time derivative of these vectors, which rotate without changing magnitude, is
where formula_15.
This result is the same as found using a vector cross product with the rotation vector formula_16 pointed along the z-axis of rotation formula_17, namely,
where formula_19 is either formula_20 or formula_21.
Time derivatives in the two frames.
Introduce the unit vectors formula_10 representing standard unit basis vectors in the rotating frame. As they rotate they will remain normalized. If we let them rotate at the speed of formula_23 about an axis formula_24 then each unit vector formula_25 of the rotating coordinate system abides by the following equation:
Then if we have a vector function formula_27, 
and we want to examine its first dervative we have (using the product rule of differentiation):
where formula_32 is the rate of change of formula_27 as observed in the rotating coordinate system. As a shorthand the differentiation is expressed as:
This result is also known as the Transport Theorem in analytical dynamics and is also sometimes referred to as the Basic Kinematic Equation.
Relation between velocities in the two frames.
A velocity of an object is the time-derivative of the object's position, or
The time derivative of a position formula_36 in a rotating reference frame has two components, one from the explicit time dependence due to motion of the particle itself, and another from the frame's own rotation. Applying the result of the previous subsection to the displacement formula_36, the velocities in the two reference frames are related by the equation
where subscript "i" means the inertial frame of reference, and "r" means the rotating frame of reference.
Relation between accelerations in the two frames.
Acceleration is the second time derivative of position, or the first time derivative of velocity
where subscript "i" means the inertial frame of reference.
Carrying out the differentiations and re-arranging some terms yields the acceleration in the "rotating" reference frame
where formula_41 is the apparent acceleration in the rotating reference frame, the term formula_42 represents centrifugal acceleration, and the term formula_43 is the coriolis acceleration.
Newton's second law in the two frames.
When the expression for acceleration is multiplied by the mass of the particle, the three extra terms on the right-hand side result in fictitious forces in the rotating reference frame, that is, apparent forces that result from being in a non-inertial reference frame, rather than from any physical interaction between bodies.
Using Newton's second law of motion formula_44, we obtain:
where formula_48 is the mass of the object being acted upon by these fictitious forces. Notice that all three forces vanish when the frame is not rotating, that is, when formula_49
For completeness, the inertial acceleration formula_50 due to impressed external forces formula_51 can be determined from the total physical force in the inertial (non-rotating) frame (for example, force from physical interactions such as electromagnetic forces) using Newton's second law in the inertial frame:
Newton's law in the rotating frame then becomes
In other words, to handle the laws of motion in a rotating reference frame:
Centrifugal force.
In classical mechanics, centrifugal force is an outward force associated with rotation. Centrifugal force is one of several so-called pseudo-forces (also known as inertial forces), so named because, unlike real forces, they do not originate in interactions with other bodies situated in the environment of the particle upon which they act. Instead, centrifugal force originates in the rotation of the frame of reference within which observations are made.
Coriolis effect.
The mathematical expression for the Coriolis force appeared in an 1835 paper by a French scientist Gaspard-Gustave Coriolis in connection with hydrodynamics, and also in the tidal equations of Pierre-Simon Laplace in 1778. Early in the 20th century, the term Coriolis force began to be used in connection with meteorology.
Perhaps the most commonly encountered rotating reference frame is the Earth. Moving objects on the surface of the Earth experience a Coriolis force, and appear to veer to the right in the northern hemisphere, and to the left in the southern. Movements of air in the atmosphere and water in the ocean are notable examples of this behavior: rather than flowing directly from areas of high pressure to low pressure, as they would on a non-rotating planet, winds and currents tend to flow to the right of this direction north of the equator, and to the left of this direction south of the equator. This effect is responsible for the rotation of large cyclones (see Coriolis effects in meteorology).
Euler force.
In classical mechanics, the Euler acceleration (named for Leonhard Euler), also known as azimuthal acceleration or transverse acceleration is an acceleration that appears when a non-uniformly rotating reference frame is used for analysis of motion and there is variation in the angular velocity of the reference frame's axis. This article is restricted to a frame of reference that rotates about a fixed axis.
The Euler force is a fictitious force on a body that is related to the Euler acceleration by "F"  = "ma", where "a" is the Euler acceleration and "m" is the mass of the body.
Use in magnetic resonance.
It is convenient to consider magnetic resonance in a frame that rotates at the Larmor frequency of the spins. This is illustrated in the animation below. The rotating wave approximation may also be used.

</doc>
<doc id="410009" url="https://en.wikipedia.org/wiki?curid=410009" title="Rotation (mathematics)">
Rotation (mathematics)

Rotation in mathematics is a concept originating in geometry. Any rotation is a motion of a certain space that preserves at least one point. It can describe, for example, the motion of a rigid body around a fixed point. A rotation is different from other types of motions: translations, which have no fixed points, and (hyperplane) reflections, each of them having an entire -dimensional flat of fixed points in a -dimensional space.
Mathematically, a rotation is a map. All rotations about a fixed point form a group under composition called the rotation group (of a particular space). But in mechanics and, more generally, in physics, this concept is frequently understood as a coordinate transformation (importantly, a transformation of an orthonormal basis), because for any motion of a body there is an inverse transformation which if applied to the frame of reference results in the body being at the same coordinates. For example, in two dimensions rotating a body clockwise about a point keeping the axes fixed is equivalent to rotating the axes counterclockwise about the same point while the body is kept fixed. These two types of rotation are called active and passive transformations.
Related definitions and terminology.
The "rotation group" is a Lie group of rotations about a fixed point. This (common) fixed point is called the "center of rotation" and is usually identified with the origin. The rotation group is a "point stabilizer" in a broader group of (orientation-preserving) motions.
For a particular rotation:
A "representation" of rotations is a particular formalism, either algebraic or geometric, used to parametrize a rotation map. This meaning is somehow inverse to the meaning in the group theory.
Rotations of (affine) spaces of points and of respective vector spaces are not always clearly distinguished. The former are sometimes referred to as "affine rotations" (although the term is misleading), whereas the latter are "vector rotations". See the article below for details.
Definitions and representations.
In Euclidean geometry.
A motion of a Euclidean space is the same as its isometry: it leaves the distance between any two points unchanged after the transformation. But a (proper) rotation also has to preserve the orientation structure. The "improper rotation" term refers to isometries that reverse (flip) the orientation. In the language of group theory the distinction is expressed as "direct" vs "indirect" isometries in the Euclidean group, where the former comprise the identity component. Any direct Euclidean motion can be represented as a composition of a rotation about the fixed point and a translation.
There are no non-trivial rotations in one dimension. In two dimensions, only a single angle is needed to specify a rotation about the origin – the "angle of rotation" that specifies an element of the circle group (also known as ). The rotation is acting to rotate an object counterclockwise through an angle about the origin; see below for details. Composition of rotations sums their angles modulo 1 turn, which implies that all two-dimensional rotations about "the same" point commute. Rotations about "different" points, in general, do not commute. Any two-dimensional direct motion is either a translation or a rotation; see Euclidean plane isometry for details.
Rotations in three-dimensional space differ from those in two dimensions in a number of important ways. Rotations in three dimensions are generally not commutative, so the order in which rotations are applied is important even about the same point. Also, unlike two-dimensional case, a three-dimensional direct motion, in general position, is not a rotation but a screw operation. Rotations about the origin have three degrees of freedom (see rotation formalisms in three dimensions for details), the same as the number of dimensions.
A three-dimensional rotation can be specified in a number of ways. The most usual methods are:
A general rotation in four dimensions has only one fixed point, the centre of rotation, and no axis of rotation; see rotations in 4-dimensional Euclidean space for details. Instead the rotation has two mutually orthogonal planes of rotation, each of which is fixed in the sense that points in each plane stay within the planes. The rotation has two angles of rotation, one for each plane of rotation, through which points in the planes rotate. If these are and then all points not in the planes rotate through an angle between and . Rotations in four dimensions about a fixed point have six degrees of freedom. A four-dimensional direct motion in general position "is" a rotation about certain point (as in all even Euclidean dimensions), but screw operations exist also.
Linear and multilinear algebra formalism.
When one considers motions of the Euclidean space that preserve the origin, the distinction between points and vectors, important in pure mathematics, can be erased because there is a canonical one-to-one correspondence between points and position vectors. The same is true for geometries other than Euclidean, but whose space is an affine space with a supplementary structure; see an example below. Alternatively, the vector description of rotations can be understood as a parametrization of geometric rotations up to their composition with translations. In other words, one vector rotation presents many equivalent rotations about "all" points in the space.
A motion that preserves the origin is the same as a linear operator on vectors that preserves the same geometric structure but expressed in terms of vectors. For Euclidean vectors, this expression is their "magnitude" (Euclidean norm). In components, such operator is expressed with orthogonal matrix that is multiplied to column vectors.
As it was already stated, a (proper) rotation is different from an arbitrary fixed-point motion in its preservation of the orientation of the vector space. Thus, the determinant of a rotation orthogonal matrix must be 1. The only other possibility for the determinant of an orthogonal matrix is , and this result means the transformation is a hyperplane reflection, a point reflection (for odd ), or another kind of improper rotation. Matrices of all proper rotations form the special orthogonal group.
Two dimensions.
In two dimensions, to carry out a rotation using matrices the point to be rotated (orientation from positive to ) is written as a vector, then multiplied by a matrix calculated from the angle, :
where are the coordinates of the point that after rotation, and the formulae for and can be seen to be
The vectors formula_3 and formula_4 have the same magnitude and are separated by an angle as expected.
Points on the plane can be also presented as complex numbers: the point in the plane is represented by the complex number
This can be rotated through an angle by multiplying it by , then expanding the product using Euler's formula as follows:
and equating real and imaginary parts gives the same result as a two-dimensional matrix:
Since complex numbers form a commutative ring, vector rotations in two dimensions are commutative, unlike in higher dimensions. They have only one degree of freedom, as such rotations are entirely determined by the angle of rotation.
Three dimensions.
As in two dimensions, a matrix can be used to rotate a point to a point . The matrix used is a matrix,
This is multiplied by a vector representing the point to give the result
The set of all appropriate matrices together with the operation of matrix multiplication is the rotation group SO(3). The matrix is a member of the three-dimensional special orthogonal group, , that is it is an orthogonal matrix with determinant 1. That it is an orthogonal matrix means that its rows are a set of orthogonal unit vectors (so they are an orthonormal basis) as are its columns, making it simple to spot and check if a matrix is a valid rotation matrix.
Above-mentioned Euler angles and axis–angle representations can be easily converted to a rotation matrix.
Another possibility to represent a rotation of three-dimensional Euclidean vectors are quaternions described below.
Quaternions.
Unit quaternions, or "versors", are in some ways the least intuitive representation of three-dimensional rotations. They are not the three-dimensional instance of a general approach. They are more compact than matrices and easier to work with than all other methods, so are often preferred in real-world applications.
A versor (also called a "rotation quaternion") consists of four real numbers, constrained so the norm of the quaternion is 1. This constraint limits the degrees of freedom of the quaternion to three, as required. Unlike matrices and complex numbers two multiplications are needed:
where is the versor, is its inverse, and is the vector treated as a quaternion with zero scalar part. The quaternion can be related to the rotation vector form of the axis angle rotation by the exponential map over the quaternions,
where is the rotation vector treated as a quaternion.
A single multiplication by a versor, either left or right, is itself a rotation, but in four dimensions. Any four-dimensional rotation about the origin can be represented with two quaternion multiplications: one left and one right, by two "different" unit quaternions.
Further notes.
More generally, coordinate rotations in any dimension are represented by orthogonal matrices. The set of all orthogonal matrices in dimensions which describe proper rotations (determinant = +1), together with the operation of matrix multiplication, forms the special orthogonal group .
Matrices are often used for doing transformations, especially when a large number of points are being transformed, as they are a direct representation of the linear operator. Rotations represented in other ways are often converted to matrices before being used. They can be extended to represent rotations and transformations at the same time using homogeneous coordinates. Projective transformations are represented by matrices. They are not rotation matrices, but a transformation that represents a Euclidean rotation has a rotation matrix in the upper left corner.
The main disadvantage of matrices is that they are more expensive to calculate and do calculations with. Also in calculations where numerical instability is a concern matrices can be more prone to it, so calculations to restore orthonormality, which are expensive to do for matrices, need to be done more often.
More alternatives to the matrix formalism.
As was demonstrated above, there exist three multilinear algebra rotation formalisms: one of U(1), or complex numbers, for two dimensions, and yet two of versors, or quaternions, for three and four dimensions.
In general (and not necessarily for Euclidean vectors) the rotation of a vector space equipped with a quadratic form can be expressed as a bivector. This formalism is used in geometric algebra and, more generally, in the Clifford algebra representation of Lie groups.
The doubly covering group of is known as the Spin group, . It can be conveniently described in terms of Clifford algebra. Unit quaternions present the group .
In non-Euclidean geometries.
In spherical geometry, a direct motion of the -sphere (an example of the elliptic geometry) is the same as a rotation of -dimensional Euclidean space about the origin (). For odd , most of these motions do not have fixed points on the -sphere and, strictly speaking, are not rotations "of the sphere"; such motions are sometimes referred to as "Clifford translations". Rotations about a fixed point in elliptic and hyperbolic geometries are not different from Euclidean ones.
Affine geometry and projective geometry have not a distinct notion of rotation.
In relativity.
One application of this is special relativity, as it can be considered to operate in a four-dimensional space, spacetime, spanned by three space dimensions and one of time. In special relativity this space is linear and the four-dimensional rotations, called Lorentz transformations, have practical physical interpretations. The Minkowski space is not a metric space, and the term "isometry" is inapplicable to Lorentz transformation.
If a rotation is only in the three space dimensions, i.e. in a plane that is entirely in space, then this rotation is the same as a spatial rotation in three dimensions. But a rotation in a plane spanned by a space dimension and a time dimension is a hyperbolic rotation, a transformation between two different reference frames, which is sometimes called a "Lorentz boost". These transformations demonstrate the pseudo-Euclidean nature of the Minkowski space. They are sometimes described as "squeeze mappings" and frequently appear on Minkowski diagrams which visualize (1 + 1)-dimensional pseudo-Euclidean geometry on planar drawings. The study of relativity is concerned with the Lorentz group generated by the space rotations and hyperbolic rotations.
Whereas rotations, in physics and astronomy, correspond to rotations of celestial sphere as a 2-sphere in the Euclidean 3-space, Lorentz transformations from induce conformal transformations of the celestial sphere. It is a broader class of the sphere transformations known as Möbius transformations.
Importance.
Rotations define important classes of symmetry: rotational symmetry is an invariance with respect to a "particular rotation". The circular symmetry is an invariance with respect to all rotation about the fixed axis.
As was stated above, Euclidean rotations are applied to rigid body dynamics. Moreover, most of mathematical formalism in physics (such as the vector calculus) is rotation-invariant; see rotation for more physical aspects. Euclidean rotations and, more generally, Lorentz symmetry described above are thought to be symmetry laws of nature. In contrast, the reflectional symmetry is not a precise symmetry law of nature.
Generalizations.
The complex-valued matrices analogous to real orthogonal matrices are the unitary matrices. The set of all unitary matrices in a given dimension forms a unitary group of degree ; and its subgroup representing proper rotations is the special unitary group of degree . These complex rotations are important in the context of spinors. The elements of are used to parametrize "three"-dimensional Euclidean rotations (see above), as well as respective transformations of the spin (see representation theory of SU(2)).

</doc>
<doc id="3096395" url="https://en.wikipedia.org/wiki?curid=3096395" title="Rotation around a fixed axis">
Rotation around a fixed axis

Rotation around a fixed axis is a special case of rotational motion. The fixed axis hypothesis excludes the possibility of an axis changing its orientation, and cannot describe such phenomena as wobbling or precession. According to Euler's rotation theorem, simultaneous rotation along a number of stationary axes at the same time is impossible. If two rotations are forced at the same time, a new axis of rotation will appear.
This article assumes that the rotation is also stable, such that no torque is required to keep it going. The kinematics and dynamics of rotation around a fixed axis of a rigid body are mathematically much simpler than those for free rotation of a rigid body; they are entirely analogous to those of linear motion along a single fixed direction, which is not true for "free rotation of a rigid body". The expressions for the kinetic energy of the object, and for the forces on the parts of the object, are also simpler for rotation around a fixed axis, than for general rotational motion. For these reasons, rotation around a fixed axis is typically taught in introductory physics courses after students have mastered linear motion; the full generality of rotational motion is not usually taught in introductory physics classes.
Translation and rotation.
A "rigid body" is an object of finite extent in which all the distances between the component particles are constant. No truly rigid body exists; external forces can deform any solid. For our purposes, then, a rigid body is a solid which requires large forces to deform it appreciably.
A change in the position of a particle in three-dimensional space can be completely specified by three coordinates. A change in the position of a rigid body is more complicated to describe. It can be regarded as a combination of two distinct types of motion: translational motion and rotational motion.
Purely "translational motion" occurs when every particle of the body has the same instantaneous velocity as every other particle; then the path traced out by any particle is exactly parallel to the path traced out by every other particle in the body. Under translational motion, the change in the position of a rigid body is specified completely by three coordinates such as "x", "y", and "z" giving the displacement of any point, such as the center of mass, fixed to the rigid body.
Purely "rotational motion" occurs if every particle in the body moves in a circle about a single line. This line is called the axis of rotation. Then the radius vectors from the axis to all particles undergo the same angular displacement in the same time. The axis of rotation need not go through the body. In general, any rotation can be specified completely by the three angular displacements with respect to the rectangular-coordinate axes "x", "y", and "z". Any change in the position of the rigid body is thus completely described by three translational and three rotational coordinates.
Any displacement of a rigid body may be arrived at by first subjecting the body to a displacement followed by a rotation, or conversely, to a rotation followed by a displacement. We already know that for any collection of particles—whether at rest with respect to one another, as in a rigid body, or in relative motion, like the exploding fragments of a shell, the acceleration of the center of mass is given by
where "M" is the total mass of the system and "a"cm is the acceleration of the center of mass. There remains the matter of describing the rotation of the body about the center of mass and relating it to the external forces acting on the body. The kinematics and dynamics of "rotational motion around a single axis" resemble the kinematics and dynamics of translational motion; rotational motion around a single axis even has a work-energy theorem analogous to that of particle dynamics.
Kinematics.
Angular displacement.
A particle moves in a circle of radius formula_2. Having moved an arc length formula_3, its angular position is formula_4 relative to its original position, where formula_5.
In mathematics and physics it is usual to use the natural unit radians rather than degrees or revolutions. Units are converted as follows:
An angular displacement is a change in angular position:
where formula_9 is the angular displacement, formula_10 is the initial angular position and formula_11 is the final angular position.
Angular speed and angular velocity.
Angular velocity is the change in angular displacement per unit time. The symbol for angular velocity is formula_12 and the units are typically rad s−1. Angular speed is the magnitude of angular velocity.
The instantaneous angular velocity is given by
Using the formula for angular position and letting formula_15, we have also
where formula_17 is the translational speed of the particle.
Angular velocity and frequency are related by
Angular acceleration.
A changing angular velocity indicates the presence of an angular acceleration in rigid body, typically measured in rad s−2. The average angular acceleration formula_19 over a time interval Δ"t" is given by 
The instantaneous acceleration "α"("t") is given by
Thus, the angular acceleration is the rate of change of the angular velocity, just as acceleration is the rate of change of velocity.
The translational acceleration of a point on the object rotating is given by
where "r" is the radius or distance from the axis of rotation. This is also the tangential component of acceleration: it is tangential to the direction of motion of the point. If this component is 0, the motion is uniform circular motion, and the velocity changes in direction only.
The radial acceleration (perpendicular to direction of motion) is given by
It is directed towards the center of the rotational motion, and is often called the "centripetal acceleration".
The angular acceleration is caused by the torque, which can have a positive or negative value in accordance with the convention of positive and negative angular frequency. The ratio of torque and angular acceleration (how difficult it is to start, stop, or otherwise change rotation) is given by the moment of inertia: formula_24.
Equations of kinematics.
When the angular acceleration is constant, the five quantities angular displacement formula_4, initial angular velocity formula_26, final angular velocity formula_27, angular acceleration formula_28, and time formula_29 can be related by four equations of kinematics:
Dynamics.
Moment of inertia.
The moment of inertia of an object, symbolized by I, is a measure of the object's resistance to changes to its rotation. The moment of inertia is measured in kilogram metre² (kg m²). It depends on the object's mass: increasing the mass of an object increases the moment of inertia. It also depends on the distribution of the mass: distributing the mass further from the centre of rotation increases the moment of inertia by a greater degree. For a single particle of mass formula_34 a distance formula_2 from the axis of rotation, the moment of inertia is given by
Torque.
Torque formula_37 is the twisting effect of a force F applied to a rotating object which is at position r from its axis of rotation. Mathematically,
where × denotes the cross product. A net torque acting upon an object will produce an angular acceleration of the object according to
just as F = "m"a in linear dynamics.
The work done by a torque acting on an object equals the magnitude of the torque times the angle through which the torque is applied:
The power of a torque is equal to the work done by the torque per unit time, hence:
Angular Momentum.
The angular momentum L is a measure of the difficulty of bringing a rotating object to rest. It is given by
Angular momentum is related to angular velocity by
just as p = "m"v in linear dynamics.
The equivalent of linear momentum in rotational motion is angular momentum. The greater the angular momentum of the spinning object such as a top, the greater its tendency to continue to spin.
The Angular Momentum of a rotating body is proportional to its mass and to how rapidly it is turning. In addition the angular momentum depends on how the mass is distributed relative to the axis of rotation: the further away the mass is located from the axis of rotation, the greater the angular momentum . A flat disk such as a record turntable has less angular momentum than a hollow cylinder of the same mass and velocity of rotation.
Like linear momentum, angular momentum is vector quantity, and its conservation implies that the direction of the spin axis tends to remain unchanged. For this reason the spinning top remains upright whereas a stationary one falls over immediately.
The angular momentum equation can be used to relate the moment of the resultant force on a body about an axis (sometimes called torque), and the rate of rotation about that axis.
Torque and angular momentum are related according to
just as F = "d"p/"dt" in linear dynamics. In the absence of an external torque, the angular momentum of a body remains constant. The conservation of angular momentum is notably demonstrated in figure skating: when pulling the arms closer to the body during a spin, the moment of inertia is decreased, and so the angular velocity is increased.
Kinetic energy.
The kinetic energy "K"rot due to the rotation of the body is given by
just as "K"trans = "mv"2 in linear dynamics.
Vector expression.
The above development is a special case of general rotational motion. In the general case, angular displacement, angular velocity, angular acceleration and torque are considered to be vectors.
An angular displacement is considered to be a vector, pointing along the axis, of magnitude equal to that of formula_9. A right-hand rule is used to find which way it points along the axis; if the fingers of the right hand are curled to point in the way that the object has rotated, then the thumb of the right hand points in the direction of the vector.
The angular velocity vector also points along the axis of rotation in the same way as the angular displacements it causes. If a disk spins counterclockwise as seen from above, its angular velocity vector points upwards. Similarly, the angular acceleration vector points along the axis of rotation in the same direction that the angular velocity would point if the angular acceleration were maintained for a long time.
The torque vector points along the axis around which the torque tends to cause rotation. To maintain rotation around a fixed axis, the total torque vector has to be along the axis, so that it only changes the magnitude and not the direction of the angular velocity vector. In the case of a hinge, only the component of the torque vector along the axis has effect on the rotation, other forces and torques are compensated by the structure.
Examples and applications.
Constant angular speed.
The simplest case of rotation around a fixed axis is that of constant angular speed. Then the total torque is zero. For the example of the Earth rotating around its axis, there is very little friction. For a fan, the motor applies a torque to compensate for friction. The angle of rotation is a linear function of time, which modulo 360° is a periodic function.
An example of this is the two-body problem with circular orbits.
Centripetal force.
Internal tensile stress provides the centripetal force that keeps a spinning object together. A rigid body model neglects the accompanying strain. If the body is not rigid this strain will cause it to change shape. This is expressed as the object changing shape due to the "centrifugal force".
Celestial bodies rotating about each other often have elliptic orbits. The special case of circular orbits is an example of a rotation around a fixed axis: this axis is the line through the center of mass perpendicular to the plane of motion. The centripetal force is provided by gravity, see also two-body problem. This usually also applies for a spinning celestial body, so it need not be solid to keep together, unless the angular speed is too high in relation to its density. (It will, however, tend to become oblate.) For example, a spinning celestial body of water must take at least 3 hours and 18 minutes to rotate, regardless of size, or the water will separate. If the density of the fluid is higher the time can be less. See orbital period.

</doc>
<doc id="173965" url="https://en.wikipedia.org/wiki?curid=173965" title="Rotation group SO(3)">
Rotation group SO(3)

In mechanics and geometry, the 3D rotation group, often denoted SO(3), is the group of all rotations about the origin of three-dimensional Euclidean space R3 under the operation of composition. By definition, a rotation about the origin is a transformation that preserves the origin, Euclidean distance (so it is an isometry), and orientation (i.e. "handedness" of space). Every non-trivial rotation is determined by its axis of rotation (a line through the origin) and its angle of rotation. 
Composing two rotations results in another rotation; every rotation has a unique inverse rotation; and the identity map satisfies the definition of a rotation. Owing to the above properties (along with the associative property, which rotations obey), the set of all rotations is a group under composition. Moreover, the rotation group has a natural structure as a manifold for which the group operations are smooth; so it is in fact a Lie group. It is compact and has dimension 3. 
Rotations are linear transformations of R3 and can therefore be represented by matrices once a basis of R3 has been chosen. Specifically, if we choose an orthonormal basis of R3, every rotation is described by an orthogonal 3x3 matrix (i.e. a 3x3 matrix with real entries which, when multiplied by its transpose, results in the identity matrix) with determinant 1. The group SO(3) can therefore be identified with the group of these matrices under matrix multiplication. These matrices are known as "special orthogonal matrices", explaining the notation SO(3).
The group SO(3) is used to describe the possible rotational symmetries of an object, as well as the possible orientations of an object in space. Its representations are important in physics, where they give rise to the elementary particles of integer spin.
Length and angle.
Besides just preserving length, rotations also preserve the angles between vectors. This follows from the fact that the standard dot product between two vectors u and v can be written purely in terms of length:
It follows that any length-preserving transformation in R3 preserves the dot product, and thus the angle between vectors. Rotations are often defined as linear transformations that preserve the inner product on R3, which is equivalent to requiring them to preserve length. See classical group for a treatment of this more general approach, where appears as a special case.
Orthogonal and rotation matrices.
Every rotation maps an orthonormal basis of to another orthonormal basis. Like any linear transformation of finite-dimensional vector spaces, a rotation can always be represented by a matrix. Let be a given rotation. With respect to the standard basis of the columns of are given by . Since the standard basis is orthonormal, and since preserves angles and length, the columns of form another orthonormal basis. This orthonormality condition can be expressed in the form
where denotes the transpose of and is the identity matrix. Matrices for which this property holds are called orthogonal matrices. The group of all orthogonal matrices is denoted , and consists of all proper and improper rotations.
In addition to preserving length, proper rotations must also preserve orientation. A matrix will preserve or reverse orientation according to whether the determinant of the matrix is positive or negative. For an orthogonal matrix , note that implies , so that . The subgroup of orthogonal matrices with determinant is called the "special orthogonal group", denoted .
Thus every rotation can be represented uniquely by an orthogonal matrix with unit determinant. Moreover, since composition of rotations corresponds to matrix multiplication, the rotation group is isomorphic to the special orthogonal group .
Improper rotations correspond to orthogonal matrices with determinant , and they do not form a group because the product of two improper rotations is a proper rotation.
Group structure.
The rotation group is a group under function composition (or equivalently the product of linear transformations). It is a subgroup of the general linear group consisting of all invertible linear transformations of the real 3-space R3.
Furthermore, the rotation group is nonabelian. That is, the order in which rotations are composed makes a difference. For example, a quarter turn around the positive "x"-axis followed by a quarter turn around the positive "y"-axis is a different rotation than the one obtained by first rotating around "y" and then "x".
The orthogonal group, consisting of all proper and improper rotations, is generated by reflections. Every proper rotation is the composition of two reflections, a special case of the Cartan–Dieudonné theorem.
Axis of rotation.
Every nontrivial proper rotation in 3 dimensions fixes a unique 1-dimensional linear subspace of R3 which is called the "axis of rotation" (this is Euler's rotation theorem). Each such rotation acts as an ordinary 2-dimensional rotation in the plane orthogonal to this axis. Since every 2-dimensional rotation can be represented by an angle φ, an arbitrary 3-dimensional rotation can be specified by an axis of rotation together with an angle of rotation about this axis. (Technically, one needs to specify an orientation for the axis and whether the rotation is taken to be clockwise or counterclockwise with respect to this orientation).
For example, counterclockwise rotation about the positive "z"-axis by angle φ is given by
Given a unit vector n in R3 and an angle φ, let "R"(φ, n) represent a counterclockwise rotation about the axis through n (with orientation determined by n). Then
Using these properties one can show that any rotation can be represented by a unique angle φ in the range 0 ≤ φ ≤ π and a unit vector n such that
Topology.
The Lie group SO(3) is diffeomorphic to the real projective space RP3.
Consider the solid ball in R3 of radius π (that is, all points of R3 of distance π or less from the origin). Given the above, for every point in this ball there is a rotation, with axis through the point and the origin, and rotation angle equal to the distance of the point from the origin. The identity rotation corresponds to the point at the center of the ball. Rotation through angles between 0 and −π correspond to the point on the same axis and distance from the origin but on the opposite side of the origin. The one remaining issue is that the two rotations through π and through −π are the same. So we identify (or "glue together") antipodal points on the surface of the ball. After this identification, we arrive at a topological space homeomorphic to the rotation group.
Indeed, the ball with antipodal surface points identified is a smooth manifold, and this manifold is diffeomorphic to the rotation group. It is also diffeomorphic to the real 3-dimensional projective space RP3, so the latter can also serve as a topological model for the rotation group.
These identifications illustrate that SO(3) is connected but not simply connected. As to the latter, in the ball with antipodal surface points identified, consider the path running from the "north pole" straight through the interior down to the south pole. This is a closed loop, since the north pole and the south pole are identified. This loop cannot be shrunk to a point, since no matter how you deform the loop, the start and end point have to remain antipodal, or else the loop will "break open". In terms of rotations, this loop represents a continuous sequence of rotations about the "z"-axis starting and ending at the identity rotation (i.e. a series of rotation through an angle φ where φ runs from 0 to 2π).
Surprisingly, if you run through the path twice, i.e., run from north pole down to south pole, jump back to the north pole (using the fact that north and south poles are identified), and then again run from north pole down to south pole, so that φ runs from 0 to 4π, you get a closed loop which "can" be shrunk to a single point: first move the paths continuously to the ball's surface, still connecting north pole to south pole twice. The second half of the path can then be mirrored over to the antipodal side without changing the path at all. Now we have an ordinary closed loop on the surface of the ball, connecting the north pole to itself along a great circle. This circle can be shrunk to the north pole without problems. The Balinese plate trick and similar tricks demonstrate this practically.
The same argument can be performed in general, and it shows that the fundamental group of SO(3) is cyclic group of order 2. In physics applications, the non-triviality of the fundamental group allows for the existence of objects known as spinors, and is an important tool in the development of the spin-statistics theorem.
The universal cover of SO(3) is a Lie group called Spin(3). The group Spin(3) is isomorphic to the special unitary group SU(2); it is also diffeomorphic to the unit 3-sphere "S"3 and can be understood as the group of versors (quaternions with absolute value 1). The connection between quaternions and rotations, commonly exploited in computer graphics, is explained in quaternions and spatial rotations. The map from "S"3 onto SO(3) that identifies antipodal points of "S"3 is a surjective homomorphism of Lie groups, with kernel {±1}. Topologically, this map is a two-to-one covering map.
Connection between SO(3) and SU(2).
The general reference for this section is . The points on the sphere can, barring the north pole , be put into one-to-one bijection with points on the plane defined by , see figure. The map is called stereographic projection.
Let the coordinates on be . The line passing through and can be written
Demanding that the equals , one finds , hence
where, for later convenience, the plane is identified with the complex plane .
For the inverse, write as
and demand to find and thus
If is a rotation, then it will take points on to points on by its standard action on the embedding space . By composing this action with one obtains a transformation of , . Thus is a transformation of associated to the transformation of .
It turns out that represented in this way by can be expressed as a matrix (where the notation is recycled to use the same name for the matrix as for the transformation of it represents). To identify this matrix, consider first a rotation about the through an angle ,
Hence
which, unsurprisingly, is a rotation in the complex plane. In an analogous way, if is a rotation about the through and angle , then
which, after a little algebra, becomes
These two rotations, , thus correspond to bilinear transforms of , namely, they are examples of Möbius transformations.
A general Möbius transformation is given by
The rotations, generate all of and the composition rules of the Möbius transformations show that any composition of translates to the corresponding composition of Möbius transformations. The Möbius transformations can be represented by matrices
since a common factor of cancels.
For the same reason, the matrix is "not" uniquely defined since multiplication by has no effect on either the determinant or the Möbius transformation. The composition law of Möbius transformations follow that of the corresponding matrices. The conclusion is that each Möbius transformation corresponds to two matrices .
Using this correspondence one may write
These matrices are unitary and thus . In terms of Euler angles one finds for a general rotation 
one has
 & 0\\
\end{matrix}\right)
\cos\frac{\theta}{2} & i\sin\frac{\theta}{2}\\
\end{matrix}\right)
e^{i\frac{\psi}{2}} & 0\\
\end{matrix}\right)\\
&= \pm
\cos\frac{\theta}{2}e^{i\frac{\varphi + \psi}{2}} & i\sin\frac{\theta}{2}e^{i\frac{\varphi - \psi}{2}}\\
For the converse, consider a general matrix
Make the substitutions
With the substitutions, assumes the form of the right hand side (RHS) of , which corresponds under to a matrix on the form of the RHS of with the same . In terms of the complex parameters ,
To verify this, substitute for the elements of the matrix on the RHS of . After some manipulation, the matrix assumes the form of the RHS of .
It is clear from the explicit form in terms of Euler angles that the map just described is a smooth, and onto group homomorphism. It is hence an explicit description of the universal covering map of from the universal covering group .
Quaternions of unit norm.
 is isomorphic to the quaternions of unit norm via a map given by
This means that there is a homomorphism from quaternions of unit norm to . Concretely, a unit quaternion, , with
is mapped to the rotation matrix
This is a rotation around the vector by an angle , where and . The proper sign for is implied, once the signs of the axis components are fixed. The is apparent since both and map to the same .
Lie algebra.
Associated with every Lie group is its Lie algebra, a linear space of the same dimension as the Lie group, closed under a bilinear alternating product called the Lie bracket. The Lie algebra of is denoted by 
and consists of all skew-symmetric matrices. This may be seen by differentiating the orthogonality condition, . The Lie bracket of two elements of is, as for the Lie algebra of every matrix group, given by the matrix commutator, , which is again a skew-symmetric matrix. The Lie algebra bracket captures the essence of the Lie group product in a sense made precise by the Baker–Campbell–Hausdorff formula.
The elements of are the "infinitesimal generators" of rotations, i.e. they are the elements of the tangent space of the manifold SO(3) at the identity element. If "R"(φ, n) denotes a counter-clockwise rotation with angle φ about the axis specified by the unit vector n, then
for every vector x in R3. 
This can be used to show that the Lie algebra (with commutator) is isomorphic to the Lie algebra R3 (with cross product). Under this isomorphism, an Euler vector formula_22 corresponds to the linear map formula_23 defined by formula_24. 
In more detail, a most often suitable basis for as a vector space is
The commutation relations of these basis elements are,
which agree with the relations of the three standard unit vectors of R3 under the cross product.
As announced above, one can identify any matrix in this Lie algebra with an Euler vector in ℝ3,
This identification is sometimes called the hat-map. Under this identification, the so(3) bracket corresponds in to the cross product,
The matrix identified with a vector has the property that
where ordinary matrix multiplication is implied on the left hand side. This implies that is in the null space of the skew-symmetric matrix with which it is identified, because .
Isomorphism with su(2).
The Lie algebras and are isomorphic. One basis for is given by
These are related to the Pauli matrices by . The Pauli matrices abide the physicist convention for Lie algebras. In that convention, Lie algebra elements are multiplied by , the exponential map (below) is defined with an extra factor of in the exponent and the structure constants remain the same, but the "definition" of them acquires a factor of . Likewise, commutation relations acquire a factor of . The commutation relations for the are
where is the totally anti-symmetric symbol with . The isomorphism between and can be set up in several ways. For later convenience, and are identified by mapping
and extending by linearity.
Exponential map.
The exponential map for , is, since is a matrix Lie group, defined using the standard matrix exponential series,
For any skew-symmetric matrix , is always in . The level of difficulty of proof depends on how a matrix group Lie algebra is defined. defines the Lie algebra as the set of matrices , in which case it is trivial. uses for a definition derivatives of smooth curve segments in through the identity taken at the identity, in which case it is harder.
For a fixed , is a one-parameter subgroup along a geodesic in . That this gives a one-parameter subgroup follows directly from properties of the exponential map.
The exponential map provides a diffeomorphism between a neighborhood of the origin in the and a neighborhood of the identity in the . For a proof, see Closed subgroup theorem.
The exponential map is surjective. This follows from the fact that that every , since every rotation leaves an axis fixed (Euler's rotation theorem), and is conjugate to a block diagonal matrix of the form
such that , and that 
together with the fact that is closed under the adjoint action of , meaning that .
Thus, e.g., it is easy to check the popular identity 
As shown above, every element is associated with a vector , where is a unit magnitude vector. Since is in the null space of , if one now rotates to a new basis, through some other orthogonal matrix , with as the axis, the final column and row of the rotation matrix in the new basis will be zero.
Thus, we know in advance from the formula for the exponential that must leave fixed. It is mathematically impossible to supply a straightforward formula for such a basis as a function of , because its existence would violate the hairy ball theorem; but direct exponentiation is possible, and yields
where . This is recognized as a matrix for a rotation around axis by the angle : cf. Rodrigues' rotation formula.
Logarithm map.
Given , let
denote the antisymmetric part. 
Then, the logarithm of is given by
This is manifest by inspection of the mixed symmetry form of Rodrigues' formula,
where the first and last term on the right-hand side are symmetric.
Baker–Campbell–Hausdorff formula.
Suppose and in the Lie algebra are given. Their exponentials, and , are rotation matrices, which can be multiplied. Since the exponential map is a surjection, for some in the Lie algebra, , and one may tentatively write
for some expression in and . When and commute, then , mimicking the behavior of complex exponentiation.
The general case is given by the more elaborate BCH formula, a series expansion of nested Lie brackets. For matrices, the Lie bracket is the same operation as the commutator, which monitors lack of commutativity in multiplication. This general expansion unfolds as follows,
The infinite expansion in the BCH formula for reduces to a compact form, 
for suitable trigonometric function coefficients .
The are given by
where
for
The inner product is the Hilbert–Schmidt inner product and the norm is the associated norm. Under the hat-isomorphism,
which explains the factors for and . This drops out in the expression for the angle.
It is worthwhile to write this composite rotation generator as
to emphasize that this is a "Lie algebra identity".
The above identity holds for all faithful representations of . The kernel of a Lie algebra homomorphism is an ideal, but , being simple, has no nontrivial ideals and all nontrivial representations are hence faithful. It holds in particular in the doublet or spinor representation. The same explicit formula thus follows in a simpler way through Pauli matrices, cf. the 2×2 derivation for SU(2).
The Pauli vector version of the same BCH formula is the somewhat simpler group composition law of SU(2),
where 
the spherical law of cosines. (Note are angles, not the above.)
This is manifestly of the same format as above, 
with
so that 
For uniform normalization of the generators in the Lie algebra involved, express the Pauli matrices in terms of -matrices, , so that 
To verify then these are the same coefficients as above, compute the ratios of the coefficients, 
Finally, given the identity . 
For the general case, one might use Ref.
Infinitesimal rotations.
The matrices in the Lie algebra are not themselves rotations; the skew-symmetric matrices are derivatives. An actual "differential rotation", or "infinitesimal rotation matrix" has the form
where is vanishingly small and .
These matrices do not satisfy all the same properties as ordinary finite rotation matrices under the usual treatment of infinitesimals . To understand what this means, one considers
First, test the orthogonality condition, . The product is
differing from an identity matrix by second order infinitesimals, discarded here. So, to first order, an infinitesimal rotation matrix is an orthogonal matrix.
Next, examine the square of the matrix,
Again discarding second order effects, note that the angle simply doubles. This hints at the most essential difference in behavior, which we can exhibit with the assistance of a second infinitesimal rotation,
Compare the products to ,
Since is second order, we discard it: thus, to first order, multiplication of infinitesimal rotation matrices is "commutative". In fact,
again to first order. In other words, the order in which infinitesimal rotations are applied is irrelevant.
This useful fact makes, for example, derivation of rigid body rotation relatively simple. But one must always be careful to distinguish (the first order treatment of) these infinitesimal rotation matrices from both finite rotation matrices and from Lie algebra elements. When contrasting the behavior of finite rotation matrices in the BCH formula above with that of infinitesimal rotation matrices, where all the commutator terms will be second order infinitesimals one finds a bona fide vector space. Technically, this dismissal of any second order terms amounts to Group contraction.
Realizations of rotations.
We have seen that there are a variety of ways to represent rotations:
A note on representations.
The Lie group SO(3) is compact and simple of rank 1, and so it has a single independent Casimir element, a quadratic invariant function of the three generators which commutes with all of them. The Killing form for the rotation group is just the Kronecker delta, and so this Casimir invariant is simply the sum of the squares of the generators, formula_63, of the algebra
That is, the Casimir invariant is given by
For unitary irreducible representations , the eigenvalues of this invariant are real and discrete, and characterize each representation, which is finite dimensional, of dimensionality 2+1. That is, the eigenvalues of this Casimir operator are 
where is integer or half-integer, and referred to as the spin or angular momentum.
So, above, the 3×3 generators L displayed act on the triplet (spin 1) representation, while the 2×2 ones (t) act on the doublet (spin-½) representation. By taking Kronecker products of with itself repeatedly, one may construct all higher irreducible representations . That is, the resulting generators for higher spin systems in three spatial dimensions, for arbitrarily large , can be calculated using these spin operators and ladder operators.
For every unitary irreducible representations there is an equivalent one, . All 
infinite-dimensional irreducible representations must be non-unitary, since the group is compact.
In quantum mechanics, the Casimir invariant is the "angular-momentum-squared" operator; integer values of spin characterize bosonic representations, while half-integer values fermionic representations, respectively. The antihermitean matrices used above are utilized as spin operators, after they are multiplied by , so they are now hermitean (like the Pauli matrices). Thus, in this language,
and hence 
Explicit expressions for these are,
for arbitrary .
For example, the resulting spin matrices for spin 1, spin , and are:
For formula_70
For formula_72:
For formula_74:
and so on.
Spherical harmonics.
The subgroup of three-dimensional Euclidean rotations has an infinite-dimensional representation on the Hilbert space , where the are spherical harmonics. Its elements are square integrable complex-valued functions on the sphere. The inner product on this space is given by
If is an arbitrary square integrable function defined on the unit sphere , then it can be expressed as
where the expansion coefficients are given by
The Lorentz group action restricts to that of and is expressed as
This action is unitary, meaning that
The can be obtained from the of above using Clebsch–Gordan decomposition, but they are more easily directly expressed as an exponential of an odd-dimensional -representation (the 3-dimensional one is exactly ). In this case the space decomposes neatly into an infinite direct sum of irreducible odd finite-dimensional representations according to
This is characteristic of infinite-dimensional unitary representations of . If is an infinite-dimensional unitary representation on a separable Hilbert space, then it decomposes as a direct sum of finite-dimensional unitary representations. Such a representation is thus never irreducible. All irreducible finite-dimensional representations can be made unitary by an appropriate choice of inner product,
where the integral is the unique invariant integral over normalized to , here expressed using the Euler angles parametrization. The inner product inside the integral is any inner product on .
Generalizations.
The rotation group generalizes quite naturally to "n"-dimensional Euclidean space, R"n" with its standard Euclidean structure. The group of all proper and improper rotations in "n" dimensions is called the orthogonal group O("n"), and the subgroup of proper rotations is called the special orthogonal group SO("n"), which is a Lie group of dimension .
In special relativity, one works in a 4-dimensional vector space, known as Minkowski space rather than 3-dimensional Euclidean space. Unlike Euclidean space, Minkowski space has an inner product with an indefinite signature. However, one can still define "generalized rotations" which preserve this inner product. Such generalized rotations are known as Lorentz transformations and the group of all such transformations is called the Lorentz group.
The rotation group SO(3) can be described as a subgroup of E+(3), the Euclidean group of direct isometries of Euclidean R3. This larger group is the group of all motions of a rigid body: each of these is a combination of a rotation about an arbitrary axis and a translation along the axis, or put differently, a combination of an element of SO(3) and an arbitrary translation.
In general, the rotation group of an object is the symmetry group within the group of direct isometries; in other words, the intersection of the full symmetry group and the group of direct isometries. For chiral objects it is the same as the full symmetry group.

</doc>
<doc id="692463" url="https://en.wikipedia.org/wiki?curid=692463" title="Rotation operator (quantum mechanics)">
Rotation operator (quantum mechanics)

This article concerns the rotation operator, as it appears in quantum mechanics.
Quantum mechanical rotations.
With every physical rotation R, we postulate a quantum mechanical rotation operator D(R) which rotates quantum mechanical states.
In terms of the generators of rotation,
formula_3 is rotation axis, and formula_4 is angular momentum.
The translation operator.
The rotation operator formula_5, with the first argument formula_6 indicating the rotation axis and the second formula_7 the rotation angle, can operate through the translation operator formula_8 for infinitesimal rotations as explained below. This is why, it is first shown how the translation operator is acting on a particle at position x (the particle is then in the state formula_9 according to Quantum Mechanics).
Translation of the particle at position x to position x+a: formula_10 
Because a translation of 0 does not change the position of the particle, we have (with 1 meaning the identity operator, which does nothing):
Taylor development gives:
with
From that follows:
This is a differential equation with the solution formula_18.
Additionally, suppose a Hamiltonian formula_19 is independent of the formula_20 position. Because the translation operator can be written in terms of formula_21, and formula_22, we know that formula_23. This result means that linear momentum for the system is conserved.
In relation to the orbital angular momentum.
Classically we have for the angular momentum formula_24. This is the same in quantum mechanics considering formula_25 and formula_26 as operators. Classically, an infinitesimal rotation formula_27 of the vector r=(x,y,z) about the z-axis to r'=(x',y',z) leaving z unchanged can be expressed by the following infinitesimal translations (using Taylor approximation):
From that follows for states:
And consequently:
Using formula_36 from above with formula_37 and Taylor development we get:
with lz = x py - y px the z-component of the angular momentum according to the classical cross product.
To get a rotation for the angle formula_40, we construct the following differential equation using the condition formula_41:
Similar to the translation operator, if we are given a Hamiltonian formula_19 which rotationally symmetric about the z axis, formula_48 implies formula_49. This result means that angular momentum is conserved.
For the spin angular momentum about the y-axis we just replace formula_50 with formula_51 and we get the spin rotation operator formula_52.
Effect on the spin operator and quantum states.
Operators can be represented by matrices. From linear algebra one knows that a certain matrix formula_53 can be represented in another basis through the transformation
where formula_55 is the basis transformation matrix. If the vectors formula_56 respectively formula_57 are the z-axis in one basis respectively another, they are perpendicular to the y-axis with a certain angle formula_40 between them. The spin operator formula_59 in the first basis can then be transformed into the spin operator formula_60 of the other basis through the following transformation:
From standard quantum mechanics we have the known results formula_62 and formula_63 where formula_64 and formula_65 are the top spins in their corresponding bases. So we have:
Comparison with formula_62 yields formula_69.
This means that if the state formula_65 is rotated about the y-axis by an angle formula_40, it becomes the state formula_64, a result that can be generalized to arbitrary axes. It is important, for instance, in Sakurai's Bell inequality.

</doc>
<doc id="813086" url="https://en.wikipedia.org/wiki?curid=813086" title="Rotational speed">
Rotational speed

Rotational speed (or speed of revolution) of an object rotating around an axis is the number of turns of the object divided by time, specified as revolutions per minute (rpm), revolutions per second (rev/s), or radians per second (rad/s). Rotational speed is equal to the angular velocity ω (or Ω) divided by 2π.
The symbol for rotational speed is formula_1(the Greek lowercase letter "omega").
When proper units are used for tangential speed "v", rotational speed formula_1, and radial distance "r", the direct proportion of "v" to both "r" and ω becomes the exact equation:
An algebraic rearrangement of this equation allows us to solve for rotational speed:
Thus, the tangential speed will be directly proportional to "r" when all parts of a system simultaneously have the same ω, as for a wheel, disk, or rigid wand. It is important to note that the direct proportionality of "v" to "r" is not valid for the planets, because the planets have different rotational speeds (ω).
Rotational speed can measure, for example, how fast a motor is running. Rotational speed and angular speed are sometimes used as synonyms, but typically they are measured with a different unit. Angular speed, however, tells the change in angle per time unit, which is measured in radians per second in the SI system. Since there are 2π radians per cycle, or 360 degrees per cycle, we can convert angular speed to rotational speed by:
and
where
For example, a stepper motor might turn exactly one complete revolution each second.
Its angular speed is 360 degrees per second (360°/s), or 2π radians per second (2π rad/s), while the rotational speed is 60 rpm.
Rotational speed is not to be confused with tangential speed, despite some relation between the two concepts. Imagine a rotating merry-go-round. No matter how close or far you stand from the axis of rotation, your rotational speed will remain constant. However, your tangential speed does not remain constant. If you stand two meters from the axis of rotation, your tangential speed will be double the amount if you were standing only one meter from the axis of rotation.

</doc>
<doc id="701096" url="https://en.wikipedia.org/wiki?curid=701096" title="Rotational symmetry">
Rotational symmetry

 
Generally, an object with 'rotational symmetry' also known in biological contexts as 'radial symmetry', is an object that looks the same after a certain amount of rotation. An object may have more than one rotational symmetry; for instance, if reflections or turning it over are not counted. The degree of rotational symmetry is how many degrees the shape has to be turned to look the same on a different side or vertex. It cannot be the same side or vertex.
Formal treatment.
Formally the rotational symmetry is symmetry with respect to some or all rotations in "m"-dimensional Euclidean space. Rotations are direct isometries, i.e., isometries preserving orientation. Therefore a symmetry group of rotational symmetry is a subgroup of "E"+("m") (see Euclidean group).
Symmetry with respect to all rotations about all points implies translational symmetry with respect to all translations, so space is homogeneous, and the symmetry group is the whole "E"("m"). With the modified notion of symmetry for vector fields the symmetry group can also be "E"+("m").
For symmetry with respect to rotations about a point we can take that point as origin. These rotations form the special orthogonal group SO("m"), the group of "m"×"m" orthogonal matrices with determinant 1. For this is the rotation group SO(3).
In another meaning of the word, the rotation group "of an object" is the symmetry group within "E"+("n"), the group of direct isometries; in other words, the intersection of the full symmetry group and the group of direct isometries. For chiral objects it is the same as the full symmetry group.
Laws of physics are SO(3)-invariant if they do not distinguish different directions in space. Because of Noether's theorem, rotational symmetry of a physical system is equivalent to the angular momentum conservation law.
Discrete rotational symmetry.
Rotational symmetry of order "n, also called n"-fold rotational symmetry, or discrete rotational symmetry of the "n"th order, with respect to a particular point (in 2D) or axis (in 3D) means that rotation by an angle of 360°/n (180°, 120°, 90°, 72°, 60°, 51 °, etc.) does not change the object. Note that "1-fold" symmetry is no symmetry, and "2-fold" is the simplest symmetry, so it does not mean "more than basic".
The notation for "n"-fold symmetry is Cn or simply "n". The actual symmetry group is specified by the point or axis of symmetry, together with the "n". For each point or axis of symmetry the abstract group type is cyclic group Z"n" of order "n". Although for the latter also the notation "C""n" is used, the geometric and abstract "C""n" should be distinguished: there are other symmetry groups of the same abstract group type which are geometrically different, see cyclic symmetry groups in 3D.
The fundamental domain is a sector of 360°/n.
Examples without additional reflection symmetry:
"C""n" is the rotation group of a regular "n"-sided polygon in 2D and of a regular "n"-sided pyramid in 3D.
If there is e.g. rotational symmetry with respect to an angle of 100°, then also with respect to one of 20°, the greatest common divisor of 100° and 360°.
A typical 3D object with rotational symmetry (possibly also with perpendicular axes) but no mirror symmetry is a propeller.
Multiple symmetry axes through the same point.
For discrete symmetry with multiple symmetry axes through the same point, there are the following possibilities:
In the case of the Platonic solids, the 2-fold axes are through the midpoints of opposite edges, the number of them is half the number of edges. The other axes are through opposite vertices and through centers of opposite faces, except in the case of the tetrahedron, where the 3-fold axes are each through one vertex and the center of one face.
Rotational symmetry with respect to any angle.
Rotational symmetry with respect to any angle is, in two dimensions, circular symmetry. The fundamental domain is a half-line.
In three dimensions we can distinguish cylindrical symmetry and spherical symmetry (no change when rotating about one axis, or for any rotation). That is, no dependence on the angle using cylindrical coordinates and no dependence on either angle using spherical coordinates. The fundamental domain is a half-plane through the axis, and a radial half-line, respectively. Axisymmetric or axisymmetrical are adjectives which refer to an object having cylindrical symmetry, or axisymmetry. An example of approximate spherical symmetry is the Earth (with respect to density and other physical and chemical properties).
In 4D, continuous or discrete rotational symmetry about a plane corresponds to corresponding 2D rotational symmetry in every perpendicular plane, about the point of intersection. An object can also have rotational symmetry about two perpendicular planes, e.g. if it is the Cartesian product of two rotationally symmetry 2D figures, as in the case of e.g. the duocylinder and various regular duoprisms.
Rotational symmetry with translational symmetry.
2-fold rotational symmetry together with single translational symmetry is one of the Frieze groups. There are two rotocenters per primitive cell.
Together with double translational symmetry the rotation groups are the following wallpaper groups, with axes per primitive cell:
Scaling of a lattice divides the number of points per unit area by the square of the scale factor. Therefore the number of 2-, 3-, 4-, and 6-fold rotocenters per primitive cell is 4, 3, 2, and 1, respectively, again including 4-fold as a special case of 2-fold, etc.
3-fold rotational symmetry at one point and 2-fold at another one (or ditto in 3D with respect to parallel axes) implies rotation group p6, i.e. double translational symmetry and 6-fold rotational symmetry at some point (or, in 3D, parallel axis). The translation distance for the symmetry generated by one such pair of rotocenters is 2√3 times their distance.

</doc>
<doc id="622053" url="https://en.wikipedia.org/wiki?curid=622053" title="S-matrix">
S-matrix

In physics, the S-matrix or scattering matrix relates the initial state and the final state of a physical system undergoing a scattering process. It is used in quantum mechanics, scattering theory and quantum field theory.
More formally, the S-matrix is defined as the unitary matrix connecting asymptotic particle states in the Hilbert space of physical states (scattering channels). While the S-matrix may be defined for any background (spacetime) that is asymptotically solvable and has no event horizons, it has a simple form in the case of the Minkowski space. In this special case, the Hilbert space is a space of irreducible unitary representations of the inhomogeneous Lorentz group (the Poincaré group); the S-matrix is the evolution operator between time equal to minus infinity (the distant past), and time equal to plus infinity (the distant future). It is defined only in the limit of zero energy density (or infinite particle separation distance).
It can be shown that if a quantum field theory in Minkowski space has a mass gap, the state in the asymptotic past and in the asymptotic future are both described by Fock spaces.
History.
The S-matrix was first introduced by John Archibald Wheeler in the 1937 paper "'On the Mathematical Description of Light Nuclei by the Method of Resonating Group Structure'". In this paper Wheeler introduced a "scattering matrix" – a unitary matrix of coefficients connecting "the asymptotic behaviour of an arbitrary particular solution [of the integral equations] with that of solutions of a standard form",
but did not develop it fully.
In the 1940s, Werner Heisenberg developed, independently, and substantiated the idea of the S-matrix. Because of the problematic divergences present in quantum field theory at that time, Heisenberg was motivated to isolate the "essential features of the theory" that would not be affected by future changes as the theory developed. In doing so, he was led to introduce a unitary "characteristic" S-matrix.
After World War II, the clout of Heisenberg and his attachment to the S-matrix approach may well have slowed down development of alternative approaches, such as quantum field theory and the closer study of sub-hadronic physics for a decade or more, at least in Europe: "Pretty much like medieval Scholastic Magisters were extremely inventive in defending the Church Dogmas and blocking the way to experimental science, some great minds in the sixties developed the S-Matrix dogma with great perfection and skill before it was buried down in the seventies after discovery of quarks and asymptotic freedom" 
Today, however, exact S-matrix results are a crowning achievement of Conformal field theory, Integrable systems, and several further areas of quantum field theory and string theory. S-matrices are not substitutes for a field-theoretic treatment, but rather, complements and the end results of such.
Motivation.
In high-energy particle physics we are interested in computing the probability for different outcomes in scattering experiments. These experiments can be broken down into three stages:
1. Collide together a collection of incoming particles (usually "two" particles with high energies).
2. Allowing the incoming particles to interact. These interactions may change the types of particles present (e.g. if an electron and a positron annihilate they may produce two photons).
3. Measuring the resulting outgoing particles.
The process by which the incoming particles are transformed (through their interaction) into the outgoing particles is called scattering. For particle physics, a physical theory of these processes must be able to compute the probability for different outgoing particles when different incoming particles collide with different energies.
The S-matrix in quantum field theory achieves exactly this. It is assumed that the small-energy-density approximation is valid in these cases.
Use of S-matrices.
The S-matrix is closely related to the transition probability amplitude in quantum mechanics and to cross sections of various interactions; the elements (individual numerical entries) in the S-matrix are known as scattering amplitudes. Poles of the S-matrix in the complex-energy plane are identified with bound states, virtual states or resonances. Branch cuts of the S-matrix in the complex-energy plane are associated to the opening of a scattering channel.
In the Hamiltonian approach to quantum field theory, the S-matrix may be calculated as a time-ordered exponential of the integrated Hamiltonian in the interaction picture; it may also be expressed using Feynman's path integrals. In both cases, the perturbative calculation of the S-matrix leads to Feynman diagrams.
In scattering theory, the S-matrix is an operator mapping free particle "in-states" to free particle "out-states" (scattering channels) in the Heisenberg picture. This is very useful because often we cannot describe the interaction (at least, not the most interesting ones) exactly.
S-matrix in one-dimensional quantum mechanics.
A simple prototype in which the S-matrix is 2-dimensional is considered first, for the purposes of illustration. In it, particles with sharp energy scatter from a localized potential according to the rules of 1-dimensional quantum mechanics. Already this simple model displays some features of more general cases, but is easier to handle.
Each energy yields an S-matrix that depends on . Thus, the total S-matrix could, figuratively speaking, be visualized, in a suitable basis, as a "continuous matrix" with every element zero except for -blocks along the diagonal for a given .
Definition.
Consider a localized one dimensional potential barrier , subjected to a beam of quantum particles with energy . These particles incident on the potential barrier from left and right.
The solution of Schrödinger's equation outside the potential barrier are plane waves given by
for the region to the left of the potential barrier, and
for the region to the right to the potential barrier, where
is the wave vector. The time dependence is not needed and is hence omitted. The term with coefficient represents the incoming wave, whereas term with coefficient represents the outgoing wave. stands for the reflecting wave, and could be omitted if we set the incoming wave as the positive direction.
The "scattering amplitude", i.e., the transition overlap of the outgoing waves with the incoming waves is a linear relation defining the S-matrix,
The above relation can be written as
where
The elements of completely characterize the scattering properties of the potential barrier .
Unitary property of S-matrix.
The unitary property of S-matrix is directly related to the conservation of the probability current in quantum mechanics.
The probability current of the wave function is defined as
The current density to the left of the barrier is
while the current density to the right of the barrier is
For conservation of the probability current density, . This implies the S-matrix is a unitary matrix.
Time-reversal symmetry.
If the potential is real,then the system possesses time-reversal symmetry. Under this condition, if is a solution of Schrödinger's equation, then is also a solution.
The time-reversed solution is given by
for the region to the left to the potential barrier, and
for the region to the right to the potential barrier,
where the terms with coefficient , represent incoming wave, and terms with coefficient , represent outgoing wave.
They are again related by the S-matrix,
that is,
Now, the relations
together yield a condition
This condition, in conjunction with the unitarity relation, implies that the S-matrix is symmetric, as a result of time reversal symmetry,
Transmission coefficient and Reflection coefficient.
The transmission coefficient from the left of the potential barrier is, when ,
so the in and out states sough after are eigenstates of the full Hamiltonian that are necessarily non-interacting due to the absence of mixed particle energy terms. The discussion in the section above suggests that the in states and the out states should be such that
for large positive and negative has the appearance of the corresponding package, represented by , of free-particle states, assumed smooth and suitably localized in momentum. Wave packages are necessary, else the time evolution will yield only a phase factor indicating free particles, which cannot be the case. The right hand side follows from that the in and out states are eigenstates of the Hamiltonian per above. To formalize this requirement, assume that the full Hamiltonian can be divided into two terms, a free-particle Hamiltonian and an interaction , such that the eigenstates of have the same appearance as the in- and out-states with respect to normalization and Lorentz transformation properties,
The in and out states are defined as eigenstates of the full Hamiltonian,
satisfying
for or respectively. Define
then
This last expression will work only using wave packages.From these definitions follow that the in and out states are normalized in the same way as the free-particle states,
and the three sets are unitarily equivalent. Now rewrite the eigenvalue equation,
where the terms has been added to make the operator on the LHS invertible. Since the in and out states reduce to the free-particle states for , put
on the RHS to obtain
Then use the completeness of the free-particle states,
to finally obtain
Here has been replaced by its eigenvalue on the free-particle states. This is the Lippmann–Schwinger equation.
In states expressed as out states.
The initial states can be expanded in a basis of final states (or vice versa). Using the completeness relation,
where is the probability that the interaction transforms
into
By the ordinary rules of quantum mechanics,
and one may write
The expansion coefficients are precisely the S-matrix elements to be defined below.
The S-matrix.
The S-matrix is now defined by
Here and are shorthands that represent the particle content but suppresses the individual labels. Associated to the S-matrix there is the S-operator defined by
where the are free particle states. This definition conforms with the direct approach used in the interaction picture. Also, due to unitary equivalence,
As a physical requirement, must be a unitary operator. This is a statement of conservation of probability in quantum field theory. But
By completeness then,
so S is the unitary transformation from in-states to out states.
Lorentz invariance is another crucial requirement on the S-matrix. The S-operator represents the quantum canonical transformation of the initial "in" states to the final "out" states. Moreover, leaves the vacuum state invariant and transforms "in"-space fields to "out"-space fields,
In terms of creation and annihilation operators, this becomes
hence
A similar expression holds when operates to the left on an out state. This means that the S-matrix can be expressed as
If describes an interaction correctly, these properties must be also true:
S-matrix and evolution operator "U".
Define a time-dependent creation and annihilation operator as follows,
so, for the fields,
where
We allow for a phase difference, given by
because for ,
Substituting the explicit expression for , one has
where formula_54 is the interaction part of the hamiltonian and formula_55 is the time ordering.
By inspection, it can be seen that this formula is not explicitly covariant.
Dyson series.
The most widely used expression for the S-matrix is the Dyson series. This expresses the S-matrix operator as the series:
where:

</doc>
<doc id="640249" url="https://en.wikipedia.org/wiki?curid=640249" title="Saddle point">
Saddle point

In mathematics, a saddle point is a point in the domain of a function that is a stationary point but not a local extremum. The name derives from the fact that the prototypical example in two dimensions is a surface that "curves up" in one direction, and "curves down" in a different direction, resembling a saddle or a mountain pass. In terms of contour lines, a saddle point in two dimensions gives rise to a contour that appears to intersect itself.
Mathematical discussion.
A simple criterion for checking if a given stationary point of a real-valued function "F"("x","y") of two real variables is a saddle point is to compute the function's Hessian matrix at that point: if the Hessian is indefinite, then that point is a saddle point. For example, the Hessian matrix of the function formula_1 at the stationary point formula_2 is the matrix
which is indefinite. Therefore, this point is a saddle point. This criterion gives only a sufficient condition. For example, the point formula_2 is a saddle point for the function formula_5 but the Hessian matrix of this function at the origin is the null matrix, which is not indefinite.
In the most general terms, a saddle point for a smooth function (whose graph is a curve, surface or hypersurface) is a stationary point such that the curve/surface/etc. in the neighborhood of that point is not entirely on any side of the tangent space at that point.
In one dimension, a saddle point is a point which is both a stationary point and a point of inflection. Since it is a point of inflection, it is not a local extremum.
Other uses.
In dynamical systems, if the dynamic is given by a differentiable map "f" then a point is hyperbolic if and only if the differential of "ƒ" "n" (where "n" is the period of the point) has no eigenvalue on the (complex) unit circle when computed at the point. Then
a "saddle point" is a hyperbolic periodic point whose stable and unstable manifolds have a dimension that is not zero. 
In a two-player zero sum game defined on a continuous space, the equilibrium point is a saddle point.
A saddle point of a matrix is an element which is both the largest element in its column and the smallest element in its row.
For a second-order linear autonomous system, a critical point is a saddle point if the characteristic equation has one positive and one negative real eigenvalue.

</doc>
<doc id="3588331" url="https://en.wikipedia.org/wiki?curid=3588331" title="Scalar (mathematics)">
Scalar (mathematics)

In linear algebra, real numbers are called scalars and relate to vectors in a vector space through the operation of scalar multiplication, in which a vector can be multiplied by a number to produce another vector. More generally, a vector space may be defined by using any field instead of real numbers, such as complex numbers. Then the scalars of that vector space will be the elements of the associated field.
A scalar product operation – not to be confused with scalar multiplication – may be defined on a vector space, allowing two vectors to be multiplied to produce a scalar. A vector space equipped with a scalar product is called an inner product space.
The real component of a quaternion is also called its scalar part.
The term is also sometimes used informally to mean a vector, matrix, tensor, or other usually "compound" value that is actually reduced to a single component. Thus, for example, the product of a 1×"n" matrix and an "n"×1 matrix, which is formally a 1×1 matrix, is often said to be a scalar.
The term scalar matrix is used to denote a matrix of the form "kI" where "k" is a scalar and "I" is the identity matrix.
Etymology.
The word "scalar" derives from the Latin word "scalaris", an adjectival form of "scala" (Latin for "ladder"). The English word "scale" also comes from "scala". The first recorded usage of the word "scalar" in mathematics occurs in François Viète's "Analytic Art" ("In artem analyticem isagoge") (1591):
According to a citation in the "Oxford English Dictionary" the first recorded usage of the term "scalar" in English came with W. R. Hamilton in 1846, referring to the real part of a quaternion:
Definitions and properties.
Scalars of vector spaces.
A vector space is defined as a set of vectors, a set of scalars, and a scalar multiplication operation that takes a scalar "k" and a vector v to another vector "k"v. For example, in a coordinate space, the scalar multiplication formula_1 yields formula_2. In a (linear) function space, "kƒ" is the function "x" "k"("ƒ"("x")).
The scalars can be taken from any field, including the rational, algebraic, real, and complex numbers, as well as finite fields. a number by the elements inside the brackets.
Scalars as vector components.
According to a fundamental theorem of linear algebra, every vector space has a basis. It follows that every vector space over a scalar field "K" is isomorphic to a coordinate vector space where the coordinates are elements of "K". For example, every real vector space of dimension "n" is isomorphic to "n"-dimensional real space R"n".
Scalars in normed vector spaces.
Alternatively, a vector space "V" can be equipped with a norm function that assigns to every vector v in "V" a scalar ||v||. By definition, multiplying v by a scalar "k" also multiplies its norm by |"k"|. If ||v|| is interpreted as the "length" of v, this operation can be described as scaling the length of v by "k". A vector space equipped with a norm is called a normed vector space (or "normed linear space").
The norm is usually defined to be an element of "V"'s scalar field "K", which restricts the latter to fields that support the notion of sign. Moreover, if "V" has dimension 2 or more, "K" must be closed under square root, as well as the four arithmetic operations; thus the rational numbers Q are excluded, but the surd field is acceptable. For this reason, not every scalar product space is a normed vector space.
Scalars in modules.
When the requirement that the set of scalars form a field is relaxed so that it need only form a ring (so that, for example, the division of scalars need not be defined, or the scalars need not be commutative), the resulting more general algebraic structure is called a module.
In this case the "scalars" may be complicated objects. For instance, if "R" is a ring, the vectors of the product space "R""n" can be made into a module with the "n"×"n" matrices with entries from "R" as the scalars. Another example comes from manifold theory, where the space of sections of the tangent bundle forms a module over the algebra of real functions on the manifold.
Scaling transformation.
The scalar multiplication of vector spaces and modules is a special case of scaling, a kind of linear transformation.
Scalar operations (computer science).
Operations that apply to a single value at a time.

</doc>
<doc id="3588425" url="https://en.wikipedia.org/wiki?curid=3588425" title="Scalar (physics)">
Scalar (physics)

In physics, a scalar is a one-dimensional physical quantity, i.e. one that can be described by a single real number (sometimes signed, often with units), in other words a scalar is a physical quantity that only has magnitude but no direction, unlike (or as a special case of) vectors, tensors, etc. which are described by several numbers which characterize magnitude and direction and a vector can be defined as a physical quantity that has magnitude and direction. Formally, a scalar is unchanged by coordinate system rotations or reflections (in Newtonian mechanics), or by Lorentz transformations or space-time translations (in relativity). A related concept is a pseudoscalar, which is invariant under proper rotations but (like a pseudovector) flips sign under improper rotations. The concept of a scalar in physics is essentially the same as in mathematics. A physical scalar field is one type of more general fields, like vector fields, spinor fields, and tensor fields. 
An example of a scalar quantity is temperature: the temperature at a given point is a single number. Velocity, on the other hand, is a vector quantity: velocity in three-dimensional space is specified by three values; in a Cartesian coordinate system the values are the speeds relative to each coordinate axis. The associated fields describe the temperature and velocity in each point of some space. Considering the norms of the velocity vectors results in a scalar field of the speeds in each point of the space.
Physical quantity.
A physical quantity is expressed as the product of a numerical value and a physical unit, not merely a number. The quantity does not depend on the unit (e.g. for distance, 1 km is the same as 1000 m), although the number depends on the unit. Thus, following the example of distance, the quantity does not depend on the length of the base vectors of the coordinate system. Also, other changes of the coordinate system may affect the formula for computing the scalar (for example, the Euclidean formula for distance in terms of coordinates relies on the basis being orthonormal), but not the scalar itself. In this sense, physical distance deviates from the definition of metric in not being just a real number; however it satisfies all other properties. The same applies for other physical quantities which are not dimensionless. Direction does not apply to scalars; they are specified by magnitude or quantity alone.
Examples in classical physics.
Some examples of scalars include the mass, charge, volume, time, speed, temperature, or electric potential at a point inside a medium. The distance between two points in three-dimensional space is a scalar, but the direction from one of those points to the other is not, since describing a direction requires two physical quantities such as the angle on the horizontal plane and the angle away from that plane. Force cannot be described using a scalar, since force is composed of direction and magnitude, however, the magnitude of a force alone can be described with a scalar, for instance the gravitational force acting on a particle is not a scalar, but its magnitude is. The speed of an object is a scalar (e.g. 180 km/h), while its velocity is not (i.e. 180 km/h "north"). 
Other examples of scalar quantities in Newtonian mechanics include electric charge and charge density.
An example of a pseudoscalar is the scalar triple product (see vector), and thus the signed volume. Another example is magnetic charge (as it is mathematically defined, regardless of whether it actually exists physically).
Scalars in relativity theory.
In the theory of relativity, one considers changes of coordinate systems that trade space for time. As a consequence, several physical quantities that are scalars in "classical" (non-relativistic) physics need to be combined with other quantities and treated as four-vectors or tensors. For example, the charge density at a point in a medium, which is a scalar in classical physics, must be combined with the local current density (a 3-vector) to comprise a relativistic 4-vector. Similarly, energy density must be combined with momentum density and pressure into the stress–energy tensor.
Examples of scalar quantities in relativity include electric charge, spacetime interval (e.g., proper time and proper length), and invariant mass.

</doc>
<doc id="2945390" url="https://en.wikipedia.org/wiki?curid=2945390" title="Scalar boson">
Scalar boson

A scalar boson is a boson whose spin equals zero. "Boson" means that it has an integer-valued spin; the "scalar" fixes this value to 0.
The name "scalar boson" arises from quantum field theory. It refers to the particular transformation properties under Lorentz transformation.

</doc>
<doc id="5173456" url="https://en.wikipedia.org/wiki?curid=5173456" title="Scalar field theory">
Scalar field theory

In theoretical physics, scalar field theory can refer to a classical or quantum theory of scalar fields. A scalar field is invariant under any Lorentz transformation.
The only fundamental scalar quantum field that has been observed in nature is the Higgs field. However, scalar quantum fields feature in the effective field theory descriptions of many physical phenomena. An example is the pion, which is actually a pseudoscalar.
Since they do not involve polarization complications, scalar fields are often the easiest to appreciate second quantization through. For this reason, scalar field theories are often used for purposes of introduction of novel concepts and techniques.
The signature of the metric employed below is .
Classical scalar field theory.
A general reference for this section is Ramond, Pierre (2001-12-21). Field Theory: A Modern Primer (Second Edition). USA: Westview Press. ISBN 0-201-30450-3, Ch 1.
Linear (free) theory.
The most basic scalar field theory is the linear theory. Through the Fourier decomposition of the fields, it represents the normal modes of an infinity of coupled oscillators (see phonons). The action for the free relativistic scalar field theory is then
where formula_3 is known as a Lagrangian density; for the three spatial coordinates; is the Kronecker delta function; and for the -th coordinate .
This is an example of a quadratic action, since each of the terms is quadratic in the field, . The term proportional to is sometimes known as a mass term, due to its subsequent interpretation, in the quantized version of this theory, in terms of particle mass.
The equation of motion for this theory is obtained by extremizing the action above. It takes the following form, linear in , 
where "∇" ² is the Laplace operator. This is the Klein–Gordon equation, with the interpretation as a classical field equation, rather than as a quantum-mechanical wave equation.
Nonlinear (interacting) theory.
The most common generalization of the linear theory above is to add a scalar potential to the Lagrangian, where typically, in addition to a mass term, "V" is a polynomial in . Such a theory is sometimes said to be interacting, because the Euler-Lagrange equation is now nonlinear, implying a self-interaction. The action for the most general such theory is
The "n"! factors in the expansion are introduced because they are useful in the Feynman diagram expansion of the quantum theory, as described below.
The corresponding Euler-Lagrange equation of motion is now
Dimensional analysis and scaling.
Physical quantities in these scalar field theories may have dimensions of length, time or mass, or some combination of the three.
However, in a relativistic theory, any quantity , with dimensions of time, can be readily converted into a "length", , by using the velocity of light, . Similarly, any length is equivalent to an inverse mass, /"mc", using Planck's constant, . In natural units, one thinks of a time as a length, or either time or length as an inverse mass.
In short, one can think of the dimensions of any physical quantity as defined in terms of "just one" independent dimension, rather than in terms of all three. This is most often termed the mass dimension of the quantity. Knowing the dimensions of each quantity, allows one to "uniquely restore" conventional dimensions from a natural units expression in terms of this mass dimension, by simply reinserting the requisite powers of and required for dimensional consistency.
One conceivable objection is that this theory is classical, and therefore it is not obvious how Planck's constant should be a part of the theory at all. If desired, one could indeed recast the theory without mass dimensions at all: However, this would be at the expense of slightly obscuring the connection with the quantum scalar field. Given that one has dimensions of mass, Planck's constant is thought of here as an essentially "arbitrary fixed reference quantity of action" (not necessarily connected to quantization), hence with dimensions appropriate to convert between mass and inverse length.
Scaling Dimension.
The classical scaling dimension, or mass dimension, , of describes the transformation of the field under a rescaling of coordinates:
The units of action are the same as the units of , and so the action itself has zero mass dimension. This fixes the scaling dimension of the field to be
Scale invariance.
There is a specific sense in which some scalar field theories are scale-invariant. While the actions above are all constructed to have zero mass dimension, not all actions are invariant under the scaling transformation
The reason that not all actions are invariant is that one usually thinks of the parameters "m" and as fixed quantities, which are not rescaled under the transformation above. The condition for a scalar field theory to be scale invariant is then quite obvious: all of the parameters appearing in the action should be dimensionless quantities. In other words, a scale invariant theory is one without any fixed length scale (or equivalently, mass scale) in the theory.
For a scalar field theory with D spacetime dimensions, the only dimensionless parameter satisfies = . For example, in "D"=4, only is classically dimensionless, and so the only classically scale-invariant scalar field theory in "D"=4 is the massless 4 theory.
Classical scale invariance, however, normally does not imply quantum scale invariance, because of the renormalization group involved−−see the discussion of the beta function below.
Conformal invariance.
A transformation
is said to be conformal if the transformation satisfies
for some function formula_15.
The conformal group contains as subgroups the isometries of the metric formula_16 (the Poincaré group) and also the scaling transformations (or dilatations) considered above. In fact, the scale-invariant theories in the previous section are also conformally-invariant.
4 theory.
Massive 4 theory illustrates a number of interesting phenomena in scalar field theory.
The Lagrangian density is
Spontaneous symmetry breaking.
This Lagrangian has a ℤ₂ symmetry under the transformation .
This is an example of an internal symmetry, in contrast to a space-time symmetry.
If is positive, the potential 
has a single minimum, at the origin. The solution "φ"=0 is clearly invariant under the ℤ₂ symmetry.
Conversely, if is negative, then one can readily see that the potential 
has two minima. This is known as a "double well potential", and the lowest energy states (known as the vacua, in quantum field theoretical language) in such a theory are not invariant under the ℤ₂ symmetry of the action (in fact it maps each of the two vacua into the other). In this case, the ℤ₂ symmetry is said to be "spontaneously broken".
Kink solutions.
The 4 theory with a negative 2 also has a kink solution, which is a canonical example of a soliton. Such a solution is of the form
where is one of the spatial variables ( is taken to be independent of , and the remaining spatial variables). The solution interpolates between the two different vacua of the double well potential. It is not possible to deform the kink into a constant solution without passing through a solution of infinite energy, and for this reason the kink is said to be stable. For "D">2, i.e. theories with more than one spatial dimension, this solution is called a domain wall.
Another well-known example of a scalar field theory with kink solutions is the sine-Gordon theory.
Complex scalar field theory.
In a complex scalar field theory, the scalar field takes values in the complex numbers,
rather than the real numbers. The action considered normally takes the form
This has a U(1), equivalently O(2) symmetry, whose action on the space of fields rotates formula_22, for some real phase angle .
As for the real scalar field, spontaneous symmetry breaking is found if "m"2 is negative. This gives rise to Goldstone's Mexican hat potential which is a rotation of the double-well potential of a real scalar
field by 2π radians about the "V"formula_23 axis. The symmetry breaking takes place in one higher dimension, i.e. the choice of vacuum breaks a continuous "U"(1) symmetry instead of a discrete one.
The two components of the scalar field are reconfigured as a massive mode and a massless Goldstone boson.
"O"("N") theory.
One can express the complex scalar field theory in terms of two real fields, "φ" ¹=Re "φ" and "φ" ² =Im "φ", which transform in the vector representation of the "U"(1)="O"(2) internal symmetry. Although such fields transform as a vector under the "internal symmetry", they are still Lorentz scalars.
This can be generalised to a theory of N scalar fields transforming in the vector representation of the "O"("N") symmetry. The Lagrangian for an "O"("N")-invariant scalar field theory is typically of the form
using an appropriate "O"("N")-invariant inner product.
Quantum scalar field theory.
A general reference for this section is Ramond, Pierre (2001-12-21). Field Theory: A Modern Primer (Second Edition). USA: Westview Press. ISBN 0-201-30450-3, Ch.4
In quantum field theory, the fields, and all observables constructed from them, are replaced by quantum operators on a Hilbert space. This Hilbert space is built on a vacuum state, and dynamics are governed by a quantum Hamiltonian, a positive-definite operator which annihilates the vacuum. A construction of a quantum scalar field theory is detailed in the canonical quantization article, which relies on canonical commutation relations among the fields. Essentially, the infinity of classical oscillators repackaged in the scalar field as its (decoupled) normal modes, above, are now quantized in the standard manner, so the respective quantum operator field describes an infinity of quantum harmonic oscillators acting on a respective Fock space.
In brief, the basic variables are the quantum field and its canonical momentum . Both these operator-valued fields are Hermitian. At spatial points , , at equal times, their canonical commutation relations are given by
while the free Hamiltonian is, similarly to above, 
A spatial Fourier transform leads to momentum space fields
which resolve to annihilation and creation operators
where formula_32 .
These operators satisfy the commutation relations
The state formula_35 annihilated by all of the operators "a" is identified as the "bare vacuum", and a particle with momentum is created by applying formula_36 to the vacuum.
Applying all possible combinations of creation operators to the vacuum constructs the relevant Hilbert space: This construction is called Fock space. The vacuum is annihilated by the Hamiltonian
where the zero-point energy has been removed by Wick ordering. (See canonical quantization.)
Interactions can be included by adding an interaction Hamiltonian. For a "φ"4 theory, this corresponds to adding a Wick ordered term "g":"φ"4:/4! to the Hamiltonian, and integrating over "x". Scattering amplitudes may be calculated from this Hamiltonian in the interaction picture. These are constructed in perturbation theory by means of the Dyson series, which gives the time-ordered products, or "n"-particle Green's functions formula_38
All of these Green's functions may be obtained by expanding the exponential in "J"("x")φ("x") in the generating function
A Wick rotation may be applied to make time imaginary. Changing the signature to (++++) then turns the Feynman integral into a statistical mechanics partition function in Euclidean space,
Normally, this is applied to the scattering of particles with fixed momenta, in which case, a Fourier transform is useful, giving instead
The standard trick to evaluate this functional integral is to write it as a product of exponential factors, schematically,
The second two exponential factors can be expanded as power series, and the combinatorics of this expansion can be represented graphically through Feynman diagrams.
The integral with {{mvar|λ}} = 0 can be treated as a product of infinitely many elementary Gaussian integrals: the result may be expressed as a sum of Feynman diagrams, calculated using the following Feynman rules:
The last rule takes into account the effect of dividing by {{overset|~|"Z"}}[0]. The Minkowski-space Feynman rules are similar, except that each vertex is represented by "−ig", while each internal line is represented by a propagator "i"/("q"2−"m"2+"iε"), where the {{mvar|ε}} term represents the small Wick rotation needed to make the Minkowski-space Gaussian integral converge.
Renormalization.
The integrals over unconstrained momenta, called "loop integrals", in the Feynman graphs typically diverge. This is normally handled by renormalization, which is a procedure of adding divergent counter-terms to the Lagrangian in such a way that the diagrams constructed from the original Lagrangian and counter-terms is finite. A renormalization scale must be introduced in the process, and the coupling constant and mass become dependent upon it.
The dependence of a coupling constant {{mvar|g}} on the scale {{mvar|λ}} is encoded by a beta function, {{math|"β"("g")}}, defined by
This dependence on the energy scale is known as "the running of the coupling parameter", and theory of this systematic scale-dependence in quantum field theory is described by the renormalization group.
Beta-functions are usually computed in an approximation scheme, most commonly perturbation theory, where one assumes that the coupling constant is small. One can then make an expansion in powers of the coupling parameters and truncate the higher-order terms (also known as higher loop contributions, due to the number of loops in the corresponding Feynman graphs).
The {{math|"β"}}-function at one loop (the first perturbative contribution) for the {{mvar|φ}}4 theory is
The fact that the sign in front of the lowest-order term is positive suggests that the coupling constant increases with energy. If this behavior persisted at large couplings, this would indicate the presence of a Landau pole at finite energy, arising from quantum triviality. However, the question can only be answered non-perturbatively, since it involves strong coupling.
A quantum field theory is said to be "trivial" when the renormalized coupling, computed through its beta function, goes to zero when the ultraviolet cutoff is removed. Consequently, the propagator becomes that of a free particle and the field is no longer interacting.
For a {{mvar|φ}}4 interaction, Michael Aizenman proved that the theory is indeed trivial, for space-time dimension {{mvar|D}} ≥ 5.
For {{mvar|D}} = 4, the triviality has yet to be proven rigorously, but lattice computations have provided strong evidence for this. This fact is important as Quantum triviality can be used to bound or even "predict" parameters such as the Higgs boson mass. This can also lead to a predictable Higgs mass in asymptotic safety scenarios.

</doc>
<doc id="244373" url="https://en.wikipedia.org/wiki?curid=244373" title="Scalar multiplication">
Scalar multiplication

In mathematics, scalar multiplication is one of the basic operations defining a vector space in linear algebra (or more generally, a module in abstract algebra). In an intuitive geometrical context, scalar multiplication of a real Euclidean vector by a positive real number multiplies the magnitude of the vector without changing its direction. The term "scalar" itself derives from this usage: a scalar is that which scales vectors. Scalar multiplication is the multiplication of a vector by a scalar (where the product is a vector), and must be distinguished from inner product of two vectors (where the product is a scalar).
Definition.
In general, if "K" is a field and "V" is a vector space over "K", then scalar multiplication is a function from "K" × "V" to "V".
The result of applying this function to "c" in "K" and v in "V" is denoted "cv".
Properties.
Scalar multiplication obeys the following rules "(vector in boldface)":
Here + is addition either in the field or in the vector space, as appropriate; and 0 is the additive identity in either.
Juxtaposition indicates either scalar multiplication or the multiplication operation in the field.
Interpretation.
Scalar multiplication may be viewed as an external binary operation or as an action of the field on the vector space. A geometric interpretation of scalar multiplication is that it stretches, or contracts, vectors by a constant factor.
As a special case, "V" may be taken to be "K" itself and scalar multiplication may then be taken to be simply the multiplication in the field.
When "V" is "K""n", scalar multiplication is equivalent to multiplication of each component with the scalar, and may be defined as such.
The same idea applies if "K" is a commutative ring and "V" is a module over "K".
"K" can even be a rig, but then there is no additive inverse.
If "K" is not commutative, the distinct operations "left scalar multiplication" "cv and "right scalar multiplication" vc" may be defined.

</doc>
<doc id="164483" url="https://en.wikipedia.org/wiki?curid=164483" title="Scattering">
Scattering

Scattering is a general physical process where some forms of radiation, such as light, sound, or moving particles, are forced to deviate from a straight trajectory by one or more paths due to localized non-uniformities in the medium through which they pass. In conventional use, this also includes deviation of reflected radiation from the angle predicted by the law of reflection. Reflections that undergo scattering are often called "diffuse reflections" and unscattered reflections are called "specular" (mirror-like) reflections. 
Scattering may also refer to particle-particle collisions between molecules, atoms, electrons, photons and other particles. Examples are: cosmic rays scattering by the Earth's upper atmosphere; particle collisions inside particle accelerators; electron scattering by gas atoms in fluorescent lamps; and neutron scattering inside nuclear reactors. 
The types of non-uniformities which can cause scattering, sometimes known as "scatterers" or "scattering centers", are too numerous to list, but a small sample includes particles, bubbles, droplets, density fluctuations in fluids, crystallites in polycrystalline solids, defects in monocrystalline solids, surface roughness, cells in organisms, and textile fibers in clothing. The effects of such features on the path of almost any type of propagating wave or moving particle can be described in the framework of scattering theory.
Some areas where scattering and scattering theory are significant include radar sensing, medical ultrasound, semiconductor wafer inspection, polymerization process monitoring, acoustic tiling, free-space communications and computer-generated imagery. Particle-particle scattering theory is important in areas such as particle physics, atomic, molecular, and optical physics, nuclear physics and astrophysics.
Single and multiple scattering.
When radiation is only scattered by one localized scattering center, this is called "single scattering", It is very common that scattering centers are grouped together, and in those cases the radiation may scatter many times, which is known as "multiple scattering". The main difference between the effects of single and multiple scattering is that single scattering can usually be treated as a random phenomenon and multiple scattering is usually more stochastic. Because the location of a single scattering center is not usually well known relative to the path of the radiation, the outcome, which tends to depend strongly on the exact incoming trajectory, appears random to an observer. This type of scattering would be exemplified by an electron being fired at an atomic nucleus. In that case, the atom's exact position relative to the path of the electron is unknown and would be immeasurable, so the exact direction of the electron after the collision is unknown, plus the quantum-mechanical nature of this particular interaction also makes the interaction random. Single scattering is therefore often described by probability distributions.
With multiple scattering, the randomness of the interaction tends to be averaged out by the large number of scattering events, so that the final path of the radiation appears to be a deterministic distribution of intensity. This is exemplified by a light beam passing through thick fog. Multiple scattering is highly analogous to diffusion, and the terms "multiple scattering" and "diffusion" are interchangeable in many contexts. Optical elements designed to produce multiple scattering are thus known as "diffusers". Coherent backscattering, an enhancement of backscattering that occurs when coherent radiation is multiply scattered by a random medium, is usually attributed to weak localization.
Not all single scattering is random, however. A well-controlled laser beam can be exactly positioned to scatter off a microscopic particle with a deterministic outcome, for instance. Such situations are encountered in radar scattering as well, where the targets tend to be macroscopic objects such as people or aircraft.
Similarly, multiple scattering can sometimes have somewhat random outcomes, particularly with coherent radiation. The random fluctuations in the multiply scattered intensity of coherent radiation are called speckles. Speckle also occurs if multiple parts of a coherent wave scatter from different centers. In certain rare circumstances, multiple scattering may only involve a small number of interactions such that the randomness is not completely averaged out. These systems are considered to be some of the most difficult to model accurately.
The description of scattering and the distinction between single and multiple scattering are often highly involved with wave–particle duality.
Scattering theory.
"Scattering theory" is a framework for studying and understanding the scattering of waves and particles. Prosaically, wave scattering corresponds to the collision and scattering of a wave with some material object, for instance sunlight scattered by rain drops to form a rainbow. Scattering also includes the interaction of billiard balls on a table, the Rutherford scattering (or angle change) of alpha particles by gold nuclei, the Bragg scattering (or diffraction) of electrons and X-rays by a cluster of atoms, and the inelastic scattering of a fission fragment as it traverses a thin foil. More precisely, scattering consists of the study of how solutions of partial differential equations, propagating freely "in the distant past", come together and interact with one another or with a boundary condition, and then propagate away "to the distant future".
Electromagnetic scattering.
Electromagnetic waves are one of the best known and most commonly encountered forms of radiation that undergo scattering. Scattering of light and radio waves (especially in radar) is particularly important. Several different aspects of electromagnetic scattering are distinct enough to have conventional names. Major forms of elastic light scattering (involving negligible energy transfer) are Rayleigh scattering and Mie scattering. Inelastic scattering includes Brillouin scattering, Raman scattering, inelastic X-ray scattering and Compton scattering.
Light scattering is one of the two major physical processes that contribute to the visible appearance of most objects, the other being absorption. Surfaces described as "white" owe their appearance to multiple scattering of light by internal or surface inhomogeneities in the object, for example by the boundaries of transparent microscopic crystals that make up a stone or by the microscopic fibers in a sheet of paper. More generally, the gloss (or lustre or sheen) of the surface is determined by scattering. Highly scattering surfaces are described as being dull or having a matte finish, while the absence of surface scattering leads to a glossy appearance, as with polished metal or stone. 
Spectral absorption, the selective absorption of certain colors, determines the color of most objects with some modification by elastic scattering. The apparent blue color of veins in skin is a common example where both spectral absorption and scattering play important and complex roles in the coloration. Light scattering can also create color without absorption, often shades of blue, as with the sky (Rayleigh scattering), the human blue iris, and the feathers of some birds (Prum et al. 1998). However, resonant light scattering in nanoparticles can produce many different highly saturated and vibrant hues, especially when surface plasmon resonance is involved (Roqué et al. 2006).
Models of light scattering can be divided into three domains based on a dimensionless size parameter, "α" which is defined as:
where π"D"p is the circumference of a particle and "λ" is the wavelength of incident radiation. Based on the value of "α", these domains are:
Rayleigh scattering is a process in which electromagnetic radiation (including light) is scattered by a small spherical volume of variant refractive index, such as a particle, bubble, droplet, or even a density fluctuation. This effect was first modeled successfully by Lord Rayleigh, from whom it gets its name. In order for Rayleigh's model to apply, the sphere must be much smaller in diameter than the wavelength ("λ") of the scattered wave; typically the upper limit is taken to be about 1/10 the wavelength. In this size regime, the exact shape of the scattering center is usually not very significant and can often be treated as a sphere of equivalent volume. The inherent scattering that radiation undergoes passing through a pure gas is due to microscopic density fluctuations as the gas molecules move around, which are normally small enough in scale for Rayleigh's model to apply. This scattering mechanism is the primary cause of the blue color of the Earth's sky on a clear day, as the shorter blue wavelengths of sunlight passing overhead are more strongly scattered than the longer red wavelengths according to Rayleigh's famous 1/"λ"4 relation. Along with absorption, such scattering is a major cause of the attenuation of radiation by the atmosphere. The degree of scattering varies as a function of the ratio of the particle diameter to the wavelength of the radiation, along with many other factors including polarization, angle, and coherence.
For larger diameters, the problem of electromagnetic scattering by spheres was first solved by Gustav Mie, and scattering by spheres larger than the Rayleigh range is therefore usually known as Mie scattering. In the Mie regime, the shape of the scattering center becomes much more significant and the theory only applies well to spheres and, with some modification, spheroids and ellipsoids. Closed-form solutions for scattering by certain other simple shapes exist, but no general closed-form solution is known for arbitrary shapes.
Both Mie and Rayleigh scattering are considered elastic scattering processes, in which the energy (and thus wavelength and frequency) of the light is not substantially changed. However, electromagnetic radiation scattered by moving scattering centers does undergo a Doppler shift, which can be detected and used to measure the velocity of the scattering center/s in forms of techniques such as lidar and radar. This shift involves a slight change in energy.
At values of the ratio of particle diameter to wavelength more than about 10, the laws of geometric optics are mostly sufficient to describe the interaction of light with the particle, and at this point the interaction is not usually described as scattering.
For modeling of scattering in cases where the Rayleigh and Mie models do not apply such as irregularly shaped particles, there are many numerical methods that can be used. The most common are finite-element methods which solve Maxwell's equations to find the distribution of the scattered electromagnetic field. Sophisticated software packages exist which allow the user to specify the refractive index or indices of the scattering feature in space, creating a 2- or sometimes 3-dimensional model of the structure. For relatively large and complex structures, these models usually require substantial execution times on a computer.

</doc>
<doc id="59874" url="https://en.wikipedia.org/wiki?curid=59874" title="Schrödinger equation">
Schrödinger equation

In quantum mechanics, the Schrödinger equation is a partial differential equation that describes how the quantum state of a quantum system changes with time. It was formulated in late 1925, and published in 1926, by the Austrian physicist Erwin Schrödinger.
In classical mechanics Newton's second law, (), is used to mathematically predict what a given system will do at any time after a known initial condition. In quantum mechanics, the analogue of Newton's law is Schrödinger's equation for a quantum system (usually atoms, molecules, and subatomic particles whether free, bound, or localized). It is not a simple algebraic equation, but in general a linear partial differential equation, describing the time-evolution of the system's wave function (also called a "state function").
The concept of a wavefunction is a fundamental postulate of quantum mechanics. Although Schrödinger's equation is often presented as a separate postulate, some authors show that some properties resulting from Schrödinger's equation may be deduced just from symmetry principles alone, for example the commutation relations. Generally, “derivations” of the Schrödinger equation demonstrate its mathematical plausibility for describing wave-particle duality, but to date there are no universally accepted derivations of Schrödinger's equation from appropriate axioms.
In the Copenhagen interpretation of quantum mechanics, the wave function is the most complete description that can be given of a physical system. Solutions to Schrödinger's equation describe not only molecular, atomic, and subatomic systems, but also macroscopic systems, possibly even the whole universe. The Schrödinger equation, in its most general form, is consistent with both classical mechanics and special relativity, but the original formulation by Schrödinger himself was non-relativistic.
The Schrödinger equation is not the only way to make predictions in quantum mechanics—other formulations can be used, such as Werner Heisenberg's matrix mechanics, and Richard Feynman's path integral formulation.
Equation.
Time-dependent equation.
The form of the Schrödinger equation depends on the physical situation (see below for special cases). The most general form is the time-dependent Schrödinger equation, which gives a description of a system evolving with time:
where is the imaginary unit, is the Planck constant divided by , the symbol ∂/∂"t" indicates a partial derivative with respect to time , (the Greek letter Psi) is the wave function of the quantum system, and is the Hamiltonian operator (which characterizes the total energy of any given wave function and takes different forms depending on the situation).
The most famous example is the non-relativistic Schrödinger equation for a single particle moving in an electric field (but not a magnetic field; see the Pauli equation):
where is the particle's "reduced mass", is its potential energy, is the Laplacian (a differential operator), and is the wave function (more precisely, in this context, it is called the "position-space wave function"). In plain language, it means "total energy equals kinetic energy plus potential energy", but the terms take unfamiliar forms for reasons explained below.
Given the particular differential operators involved, this is a linear partial differential equation. It is also a diffusion equation, but unlike the heat equation, this one is also a wave equation given the imaginary unit present in the transient term.
The term "Schrödinger equation" can refer to both the general equation (first box above), or the specific nonrelativistic version (second box above and variations thereof). The general equation is indeed quite general, used throughout quantum mechanics, for everything from the Dirac equation to quantum field theory, by plugging in various complicated expressions for the Hamiltonian. The specific nonrelativistic version is a simplified approximation to reality, which is quite accurate in many situations, but very inaccurate in others (see relativistic quantum mechanics and relativistic quantum field theory).
To apply the Schrödinger equation, the Hamiltonian operator is set up for the system, accounting for the kinetic and potential energy of the particles constituting the system, then inserted into the Schrödinger equation. The resulting partial differential equation is solved for the wave function, which contains information about the system.
Time-independent equation.
The time-independent Schrödinger equation predicts that wave functions can form standing waves, called stationary states (also called "orbitals", as in atomic orbitals or molecular orbitals). These states are important in their own right, and if the stationary states are classified and understood, then it becomes easier to solve the time-dependent Schrödinger equation for "any" state. The "time-independent Schrödinger equation" is the equation describing stationary states. (It is only used when the Hamiltonian itself is not dependent on time. However, even in this case the total wave function still has a time dependency.)
In words, the equation states:
The time-independent Schrödinger equation is discussed further below. In linear algebra terminology, this equation is an eigenvalue equation.
As before, the most famous manifestation is the non-relativistic Schrödinger equation for a single particle moving in an electric field (but not a magnetic field):
with definitions as above.
Implications.
The Schrödinger equation and its solutions introduced a breakthrough in thinking about physics. Schrödinger's equation was the first of its type, and solutions led to consequences that were very unusual and unexpected for the time.
Total, kinetic, and potential energy.
The "overall" form of the equation is "not" unusual or unexpected as it uses the principle of the conservation of energy. The terms of the nonrelativistic Schrödinger equation can be interpreted as total energy of the system, equal to the system kinetic energy plus the system potential energy. In this respect, it is just the same as in classical physics.
Quantization.
The Schrödinger equation predicts that if certain properties of a system are measured, the result may be "quantized", meaning that only specific discrete values can occur. One example is "energy quantization": the energy of an electron in an atom is always one of the quantized energy levels, a fact discovered via atomic spectroscopy. (Energy quantization is discussed below.) Another example is quantization of angular momentum. This was an "assumption" in the earlier Bohr model of the atom, but it is a "prediction" of the Schrödinger equation.
Another result of the Schrödinger equation is that not every measurement gives a quantized result in quantum mechanics. For example, position, momentum, time, and (in some situations) energy can have any value across a continuous range.
Measurement and uncertainty.
In classical mechanics, a particle has, at every moment, an exact position and an exact momentum. These values change deterministically as the particle moves according to Newton's laws. Under the Copenhagen interpretation of quantum mechanics, particles do not have exactly determined properties, and when they are measured, the result is randomly drawn from a probability distribution. The Schrödinger equation predicts what the probability distributions are, but fundamentally cannot predict the exact result of each measurement.
The Heisenberg uncertainty principle is the statement of the inherent measurement uncertainty in quantum mechanics. It states that the more precisely a particle's position is known, the less precisely its momentum is known, and vice versa.
The Schrödinger equation describes the (deterministic) evolution of the wave function of a particle. However, even if the wave function is known exactly, the result of a specific measurement on the wave function is uncertain.
Quantum tunneling.
In classical physics, when a ball is rolled slowly up a large hill, it will come to a stop and roll back, because it doesn't have enough energy to get over the top of the hill to the other side. However, the Schrödinger equation predicts that there is a small probability that the ball will get to the other side of the hill, even if it has too little energy to reach the top. This is called quantum tunneling. It is related to the distribution of energy: although the ball's assumed position seems to be on one side of the hill, there is a chance of finding it on the other side.
Particles as waves.
The nonrelativistic Schrödinger equation is a type of partial differential equation called a wave equation. Therefore, it is often said particles can exhibit behavior usually attributed to waves. In some modern interpretations this description is reversed – the quantum state, i.e. wave, is the only genuine physical reality, and under the appropriate conditions it can show features of particle-like behavior. However, Ballentine shows that such an interpretation has problems. Ballentine points out that whilst it is arguable to associate a physical wave with a single particle, there is still only "one" Schrödinger wave equation for many particles. He points out:
Two-slit diffraction is a famous example of the strange behaviors that waves regularly display, that are not intuitively associated with particles. The overlapping waves from the two slits cancel each other out in some locations, and reinforce each other in other locations, causing a complex pattern to emerge. Intuitively, one would not expect this pattern from firing a single particle at the slits, because the particle should pass through one slit or the other, not a complex overlap of both.
However, since the Schrödinger equation is a wave equation, a single particle fired through a double-slit "does" show this same pattern (figure on right). Note: The experiment must be repeated many times for the complex pattern to emerge. Although this is counterintuitive, the prediction is correct; in particular, electron diffraction and neutron diffraction are well understood and widely used in science and engineering.
Related to diffraction, particles also display superposition and interference.
The superposition property allows the particle to be in a quantum superposition of two or more quantum states at the same time. However, it is noted that a "quantum state" in QM means the "probability" that a system will be, for example at a position x, not that the system will actually be at position x. It does not infer that the particle itself may be in two classical states at once. Indeed, QM is generally unable to assign values for properties prior to measurement at all.
Interpretation of the wave function.
The Schrödinger equation provides a way to calculate the wave function of a system and how it changes dynamically in time. However, the Schrödinger equation does not directly say "what", exactly, the wave function is. Interpretations of quantum mechanics address questions such as what the relation is between the wave function, the underlying reality, and the results of experimental measurements.
An important aspect is the relationship between the Schrödinger equation and wavefunction collapse. In the oldest Copenhagen interpretation, particles follow the Schrödinger equation "except" during wavefunction collapse, during which they behave entirely differently. The advent of quantum decoherence theory allowed alternative approaches (such as the Everett many-worlds interpretation and consistent histories), wherein the Schrödinger equation is "always" satisfied, and wavefunction collapse should be explained as a consequence of the Schrödinger equation.
Historical background and development.
Following Max Planck's quantization of light (see black body radiation), Albert Einstein interpreted Planck's quanta to be photons, particles of light, and proposed that the energy of a photon is proportional to its frequency, one of the first signs of wave–particle duality. Since energy and momentum are related in the same way as frequency and wavenumber in special relativity, it followed that the momentum of a photon is inversely proportional to its wavelength , or proportional to its wavenumber .
where is Planck's constant. Louis de Broglie hypothesized that this is true for all particles, even particles which have mass such as electrons. He showed that, assuming that the matter waves propagate along with their particle counterparts, electrons form standing waves, meaning that only certain discrete rotational frequencies about the nucleus of an atom are allowed.
These quantized orbits correspond to discrete energy levels, and de Broglie reproduced the Bohr model formula for the energy levels. The Bohr model was based on the assumed quantization of angular momentum according to: 
According to de Broglie the electron is described by a wave and a whole number of wavelengths must fit along the circumference of the electron's orbit:
This approach essentially confined the electron wave in one dimension, along a circular orbit of radius .
In 1921, prior to de Broglie, Arthur C. Lunn at the University of Chicago had used the same argument based on the completion of the relativistic energy–momentum 4-vector to derive what we now call the de Broglie relation Unlike de Broglie, Lunn went on to formulate the differential equation now known as the Schrödinger equation, and solve for its energy eigenvalues for the hydrogen atom. Unfortunately the paper was rejected by the Physical Review, as recounted by Kamen.
Following up on de Broglie's ideas, physicist Peter Debye made an offhand comment that if particles behaved as waves, they should satisfy some sort of wave equation. Inspired by Debye's remark, Schrödinger decided to find a proper 3-dimensional wave equation for the electron. He was guided by William R. Hamilton's analogy between mechanics and optics, encoded in the observation that the zero-wavelength limit of optics resembles a mechanical system — the trajectories of light rays become sharp tracks that obey Fermat's principle, an analog of the principle of least action. A modern version of his reasoning is reproduced below. The equation he found is:
However, by that time, Arnold Sommerfeld had refined the Bohr model with relativistic corrections. Schrödinger used the relativistic energy momentum relation to find what is now known as the Klein–Gordon equation in a Coulomb potential (in natural units):
He found the standing waves of this relativistic equation, but the relativistic corrections disagreed with Sommerfeld's formula. Discouraged, he put away his calculations and secluded himself in an isolated mountain cabin in December 1925.
While at the cabin, Schrödinger decided that his earlier non-relativistic calculations were novel enough to publish, and decided to leave off the problem of relativistic corrections for the future. Despite the difficulties in solving the differential equation for hydrogen (he had sought help from his friend the mathematician Hermann Weyl) Schrödinger showed that his non-relativistic version of the wave equation produced the correct spectral energies of hydrogen in a paper published in 1926. In the equation, Schrödinger computed the hydrogen spectral series by treating a hydrogen atom's electron as a wave , moving in a potential well , created by the proton. This computation accurately reproduced the energy levels of the Bohr model. In a paper, Schrödinger himself explained this equation as follows:
This 1926 paper was enthusiastically endorsed by Einstein, who saw the matter-waves as an intuitive depiction of nature, as opposed to Heisenberg's matrix mechanics, which he considered overly formal.
The Schrödinger equation details the behavior of but says nothing of its "nature". Schrödinger tried to interpret it as a charge density in his fourth paper, but he was unsuccessful. In 1926, just a few days after Schrödinger's fourth and final paper was published, Max Born successfully interpreted as the probability amplitude, whose absolute square is equal to probability density. Schrödinger, though, always opposed a statistical or probabilistic approach, with its associated discontinuities—much like Einstein, who believed that quantum mechanics was a statistical approximation to an underlying deterministic theory— and never reconciled with the Copenhagen interpretation.
Louis de Broglie in his later years proposed a real valued wave function connected to the complex wave function by a proportionality constant and developed the De Broglie–Bohm theory.
The wave equation for particles.
The Schrödinger equation is a wave equation, since the "solutions" are functions which describe wave-like motions. Wave equations in physics can normally be derived from other physical laws – the wave equation for mechanical vibrations on strings and in matter can be derived from Newton's laws – where the wave function represents the displacement of matter, and electromagnetic waves from Maxwell's equations, where the wave functions are electric and magnetic fields. The basis for Schrödinger's equation, on the other hand, is the energy of the system and a separate postulate of quantum mechanics: the wave function is a description of the system. The Schrödinger equation is therefore a new concept in itself; as Feynman put it:
The foundation of the equation is structured to be a linear differential equation based on classical energy conservation, and consistent with the De Broglie relations. The solution is the wave function , which contains all the information that can be known about the system. In the Copenhagen interpretation, the modulus of is related to the probability the particles are in some spatial configuration at some instant of time. Solving the equation for can be used to predict how the particles will behave under the influence of the specified potential and with each other.
The Schrödinger equation was developed principally from the De Broglie hypothesis, a wave equation that would describe particles, and can be constructed as shown informally in the following sections. For a more rigorous description of Schrödinger's equation, see also.
Consistency with energy conservation.
The total energy of a particle is the sum of kinetic energy and potential energy , this sum is also the frequent expression for the Hamiltonian in classical mechanics: 
Explicitly, for a particle in one dimension with position , mass and momentum , and potential energy which generally varies with position and time :
For three dimensions, the position vector and momentum vector must be used:
This formalism can be extended to any fixed number of particles: the total energy of the system is then the total kinetic energies of the particles, plus the total potential energy, again the Hamiltonian. However, there can be interactions between the particles (an "N"-body problem), so the potential energy can change as the spatial configuration of particles changes, and possibly with time. The potential energy, in general, is "not" the sum of the separate potential energies for each particle, it is a function of all the spatial positions of the particles. Explicitly:
Linearity.
The simplest wavefunction is a plane wave of the form:
where the is the amplitude, the wavevector, and the angular frequency, of the plane wave. In general, physical situations are not purely described by plane waves, so for generality the superposition principle is required; any wave can be made by superposition of sinusoidal plane waves. So if the equation is linear, a linear combination of plane waves is also an allowed solution. Hence a necessary and separate requirement is that the Schrödinger equation is a linear differential equation.
For discrete the sum is a superposition of plane waves:
for some real amplitude coefficients , and for continuous the sum becomes an integral, the Fourier transform of a momentum space wavefunction:
where is the differential volume element in -space, and the integrals are taken over all -space. The momentum wavefunction arises in the integrand since the position and momentum space wavefunctions are Fourier transforms of each other.
Consistency with the De Broglie relations.
Einstein's light quanta hypothesis (1905) states that the energy of a photon is proportional to the frequency (or angular frequency, ) of the corresponding quantum wavepacket of light:
Likewise De Broglie's hypothesis (1924) states that any particle can be associated with a wave, and that the momentum of the particle is inversely proportional to the wavelength of such a wave (or proportional to the wavenumber, ), in one dimension, by:
while in three dimensions, wavelength is related to the magnitude of the wavevector :
The Planck–Einstein and de Broglie relations illuminate the deep connections between energy with time, and space with momentum, and express wave–particle duality. In practice, natural units comprising are used, as the De Broglie "equations" reduce to "identities": allowing momentum, wavenumber, energy and frequency to be used interchangeably, to prevent duplication of quantities, and reduce the number of dimensions of related quantities. For familiarity SI units are still used in this article.
Schrödinger's insight, late in 1925, was to express the phase of a plane wave as a complex phase factor using these relations:
and to realize that the first order partial derivatives were:
with respect to space:
with respect to time:
Another postulate of quantum mechanics is that all observables are represented by linear Hermitian operators which act on the wavefunction, and the eigenvalues of the operator are the values the observable takes. The previous derivatives are consistent with the energy operator, corresponding to the time derivative,
where are the energy eigenvalues, and the momentum operator, corresponding to the spatial derivatives (the gradient ),
where is a vector of the momentum eigenvalues. In the above, the "hats" ( ) indicate these observables are operators, not simply ordinary numbers or vectors. The energy and momentum operators are "differential operators", while the potential energy function is just a multiplicative factor.
Substituting the energy and momentum operators into the classical energy conservation equation obtains the operator:
so in terms of derivatives with respect to time and space, acting this operator on the wavefunction immediately led Schrödinger to his equation:
Wave–particle duality can be assessed from these equations as follows. The kinetic energy is related to the square of momentum . As the particle's momentum increases, the kinetic energy increases more rapidly, but since the wavenumber increases the wavelength decreases. In terms of ordinary scalar and vector quantities (not operators):
The kinetic energy is also proportional to the second spatial derivatives, so it is also proportional to the magnitude of the "curvature" of the wave, in terms of operators:
As the curvature increases, the amplitude of the wave alternates between positive and negative more rapidly, and also shortens the wavelength. So the inverse relation between momentum and wavelength is consistent with the energy the particle has, and so the energy of the particle has a connection to a wave, all in the same mathematical formulation.
Wave and particle motion.
Schrödinger required that a wave packet solution near position with wavevector near will move along the trajectory determined by classical mechanics for times short enough for the spread in (and hence in velocity) not to substantially increase the spread in . Since, for a given spread in , the spread in velocity is proportional to Planck's constant , it is sometimes said that in the limit as approaches zero, the equations of classical mechanics are restored from quantum mechanics. Great care is required in how that limit is taken, and in what cases.
The limiting short-wavelength is equivalent to tending to zero because this is limiting case of increasing the wave packet localization to the definite position of the particle (see images right). Using the Heisenberg uncertainty principle for position and momentum, the products of uncertainty in position and momentum become zero as :
where denotes the (root mean square) measurement uncertainty in and (and similarly for the and directions) which implies the position and momentum can only be known to arbitrary precision in this limit.
The Schrödinger equation in its general form
is closely related to the Hamilton–Jacobi equation (HJE)
where is action and is the Hamiltonian function (not operator). Here the generalized coordinates for (used in the context of the HJE) can be set to the position in Cartesian coordinates as .
Substituting
where is the probability density, into the Schrödinger equation and then taking the limit in the resulting equation, yields the Hamilton–Jacobi equation.
The implications are:
Non-relativistic quantum mechanics.
The quantum mechanics of particles without accounting for the effects of special relativity, for example particles propagating at speeds much less than light, is known as non-relativistic quantum mechanics. Following are several forms of Schrödinger's equation in this context for different situations: time independence and dependence, one and three spatial dimensions, and one and particles.
In actuality, the particles constituting the system do not have the numerical labels used in theory. The language of mathematics forces us to label the positions of particles one way or another, otherwise there would be confusion between symbols representing which variables are for which particle.
Time independent.
If the Hamiltonian is not an explicit function of time, the equation is separable into a product of spatial and temporal parts. In general, the wavefunction takes the form:
where is a function of all the spatial coordinate(s) of the particle(s) constituting the system only, and is a function of time only.
Substituting for into the Schrödinger equation for the relevant number of particles in the relevant number of dimensions, solving by separation of variables implies the general solution of the time-dependent equation has the form:
Since the time dependent phase factor is always the same, only the spatial part needs to be solved for in time independent problems. Additionally, the energy operator can always be replaced by the energy eigenvalue , thus the time independent Schrödinger equation is an eigenvalue equation for the Hamiltonian operator:
This is true for any number of particles in any number of dimensions (in a time independent potential). This case describes the standing wave solutions of the time-dependent equation, which are the states with definite energy (instead of a probability distribution of different energies). In physics, these standing waves are called "stationary states" or "energy eigenstates"; in chemistry they are called "atomic orbitals" or "molecular orbitals". Superpositions of energy eigenstates change their properties according to the relative phases between the energy levels.
The energy eigenvalues from this equation form a discrete spectrum of values, so mathematically energy must be quantized. More specifically, the energy eigenstates form a basis – any wavefunction may be written as a sum over the discrete energy states or an integral over continuous energy states, or more generally as an integral over a measure. This is the spectral theorem in mathematics, and in a finite state space it is just a statement of the completeness of the eigenvectors of a Hermitian matrix.
One-dimensional examples.
For a particle in one dimension, the Hamiltonian is:
and substituting this into the general Schrödinger equation gives:
This is the only case the Schrödinger equation is an ordinary differential equation, rather than a partial differential equation. The general solutions are always of the form:
For particles in one dimension, the Hamiltonian is:
where the position of particle is . The corresponding Schrödinger equation is:
so the general solutions have the form:
For non-interacting distinguishable particles, the potential of the system only influences each particle separately, so the total potential energy is the sum of potential energies for each particle:
and the wavefunction can be written as a product of the wavefunctions for each particle:
For non-interacting identical particles, the potential is still a sum, but wavefunction is a bit more complicated - it is a sum over the permutations of products of the separate wavefunctions to account for particle exchange. In general for interacting particles, the above decompositions are "not" possible.
Free particle.
For no potential, , so the particle is free and the equation reads:
which has oscillatory solutions for (the are arbitrary constants):
and exponential solutions for 
The exponentially growing solutions have an infinite norm, and are not physical. They are not allowed in a finite volume with periodic or fixed boundary conditions.
See also free particle and wavepacket for more discussion on the free particle.
Constant potential.
For a constant potential, , the solution is oscillatory for and exponential for , corresponding to energies that are allowed or disallowed in classical mechanics. Oscillatory solutions have a classically allowed energy and correspond to actual classical motions, while the exponential solutions have a disallowed energy and describe a small amount of quantum bleeding into the classically disallowed region, due to quantum tunneling. If the potential grows to infinity, the motion is classically confined to a finite region. Viewed far enough away, every solution is reduced an exponential; the condition that the exponential is decreasing restricts the energy levels to a discrete set, called the allowed energies.
Harmonic oscillator.
The Schrödinger equation for this situation is
It is a notable quantum system to solve for; since the solutions are exact (but complicated – in terms of Hermite polynomials), and it can describe or at least approximate a wide variety of other systems, including vibrating atoms, molecules, and atoms or ions in lattices, and approximating other potentials near equilibrium points. It is also the basis of perturbation methods in quantum mechanics.
There is a family of solutions – in the position basis they are
where , and the functions are the Hermite polynomials.
Three-dimensional examples.
The extension from one dimension to three dimensions is straightforward, all position and momentum operators are replaced by their three-dimensional expressions and the partial derivative with respect to space is replaced by the gradient operator.
The Hamiltonian for one particle in three dimensions is:
generating the equation:
with stationary state solutions of the form:
where the position of the particle is r. Two useful coordinate systems for solving the Schrödinger equation are Cartesian coordinates so that and spherical polar coordinates so that , although other orthogonal coordinates are useful for solving the equation for systems with certain geometric symmetries.
For particles in three dimensions, the Hamiltonian is:
where the position of particle is and the gradient operators are partial derivatives with respect to the particle's position coordinates. In Cartesian coordinates, for particle , the position vector is while the gradient and Laplacian operator are respectively:
generating the equation:
For particles in three dimensions, the Hamiltonian is:
where the position of particle is , generating the equation:
This last equation is in a very high dimension, so the solutions are not easy to visualize.
Solution methods.
General techniques:
Methods for special cases:
Properties.
The Schrödinger equation has the following properties: some are useful, but there are shortcomings. Ultimately, these properties arise from the Hamiltonian used, and solutions to the equation.
Linearity.
In the development above, the Schrödinger equation was made to be linear for generality, though this has other implications. If two wave functions and are solutions, then so is any linear combination of the two:
where and are any complex numbers (the sum can be extended for any number of wavefunctions). This property allows superpositions of quantum states to be solutions of the Schrödinger equation. Even more generally, it holds that a general solution to the Schrödinger equation can be found by taking a weighted sum over all single state solutions achievable. For example, consider a wave function such that the wave function is a product of two functions: one time independent, and one time dependent. If states of definite energy found using the time independent Shrödinger equation are given by with amplitude and time dependent phase factor is given by
is the probability current (flow per unit area).
Hence predictions from the Schrödinger equation do not violate probability conservation.
Positive energy.
If the potential is bounded from below, meaning there is a minimum value of potential energy, the eigenfunctions of the Schrödinger equation have energy which is also bounded from below. This can be seen most easily by using the variational principle, as follows. (See also below).
For any linear operator bounded from below, the eigenvector with the smallest eigenvalue is the vector that minimizes the quantity
over all which are normalized. In this way, the smallest eigenvalue is expressed through the variational principle. For the Schrödinger Hamiltonian bounded from below, the smallest eigenvalue is called the ground state energy. That energy is the minimum value of
(using integration by parts). Due to the complex modulus of squared (which is positive definite), the right hand side always greater than the lowest value of . In particular, the ground state energy is positive when is everywhere positive.
For potentials which are bounded below and are not infinite over a region, there is a ground state which minimizes the integral above. This lowest energy wavefunction is real and positive definite – meaning the wavefunction can increase and decrease, but is positive for all positions. It physically cannot be negative: if it were, smoothing out the bends at the sign change (to minimize the wavefunction) rapidly reduces the gradient contribution to the integral and hence the kinetic energy, while the potential energy changes linearly and less quickly. The kinetic and potential energy are both changing at different rates, so the total energy is not constant, which can't happen (conservation). The solutions are consistent with Schrödinger equation if this wavefunction is positive definite.
The lack of sign changes also shows that the ground state is nondegenerate, since if there were two ground states with common energy , not proportional to each other, there would be a linear combination of the two that would also be a ground state resulting in a zero solution.
Analytic continuation to diffusion.
The above properties (positive definiteness of energy) allow the analytic continuation of the Schrödinger equation to be identified as a stochastic process. This can be interpreted as the Huygens–Fresnel principle applied to De Broglie waves; the spreading wavefronts are diffusive probability amplitudes.
For a free particle (not subject to a potential) in a random walk, substituting into the time-dependent Schrödinger equation gives:
which has the same form as the diffusion equation, with diffusion coefficient .
Relativistic quantum mechanics.
Relativistic quantum mechanics is obtained where quantum mechanics and special relativity simultaneously apply. In general, one wishes to build relativistic wave equations from the relativistic energy–momentum relation
instead of classical energy equations. The Klein–Gordon equation and the Dirac equation are two such equations. The Klein–Gordon equation,
was the first such equation to be obtained, even before the non-relativistic one, and applies to massive spinless particles. The Dirac equation arose from taking the "square root" of the Klein–Gordon equation by factorizing the entire relativistic wave operator into a product of two operators – one of these is the operator for the entire Dirac equation.
The general form of the Schrödinger equation remains true in relativity, but the Hamiltonian is less obvious. For example, the Dirac Hamiltonian for a particle of mass and electric charge in an electromagnetic field (described by the electromagnetic potentials and ) is:
in which the and are the Dirac gamma matrices related to the spin of the particle. The Dirac equation is true for all particles, and the solutions to the equation are spinor fields with two components corresponding to the particle and the other two for the antiparticle.
For the Klein–Gordon equation, the general form of the Schrödinger equation is inconvenient to use, and in practice the Hamiltonian is not expressed in an analogous way to the Dirac Hamiltonian. The equations for relativistic quantum fields can be obtained in other ways, such as starting from a Lagrangian density and using the Euler-Lagrange equations for fields, or use the representation theory of the Lorentz group in which certain representations can be used to fix the equation for a free particle of given spin (and mass).
In general, the Hamiltonian to be substituted in the general Schrödinger equation is not just a function of the position and momentum operators (and possibly time), but also of spin matrices. Also, the solutions to a relativistic wave equation, for a massive particle of spin , are complex-valued spinor fields.
Quantum field theory.
The general equation is also valid and used in quantum field theory, both in relativistic and non-relativistic situations. However, the solution is no longer interpreted as a "wave", but should be interpreted as an operator acting on states existing in a Fock space.

</doc>
<doc id="13625345" url="https://en.wikipedia.org/wiki?curid=13625345" title="Schrödinger field">
Schrödinger field

In quantum mechanics and quantum field theory, a Schrödinger field, named after Erwin Schrödinger, is a quantum field which obeys the Schrödinger equation. While any situation described by a Schrödinger field can also be described by a many-body Schrödinger equation for identical particles, the field theory is more suitable for situations where the particle number changes.
A Schrödinger field is also the classical limit of a quantum Schrödinger field, a classical wave which satisfies the Schrödinger equation. Unlike the quantum mechanical wavefunction, if there are interactions between the particles the equation will be nonlinear. These nonlinear equations describe the classical wave limit of a system of interacting identical particles.
The path integral of a Schrödinger field is also known as a coherent state path integral, because the field itself is an annihilation operator whose eigenstates can be thought of as coherent states of the harmonic oscillations of the field modes.
Schrödinger fields are useful for describing Bose–Einstein condensation, the Bogolyubov–de Gennes equation of superconductivity, superfluidity, and many-body theory in general. They are also a useful alternative formalism for nonrelativistic quantum mechanics.
A Schrödinger field is the nonrelativistic limit of a Klein–Gordon field.
Summary.
A Schrödinger field is a quantum field whose quanta obey the Schrödinger equation. In the classical limit, it can be understood as the quantized wave equation of a Bose Einstein condensate or a superfluid.
Free field.
A Schrödinger field has the free field Lagrangian
When formula_2 is a complex valued field in a path integral, or equivalently an operator with canonical commutation relations, it describes a collection of identical nonrelativistic bosons. When formula_2 is a grassmann valued field, or equivalently an operator with canonical anticommutation relations, the field describes identical fermions.
External potential.
If the particles interact with an external potential formula_4, the interaction makes a local contribution to the action:
If the ordinary Schrödinger equation for V has known energy eigenstates formula_6 with energies formula_7, then the field in the action can be rotated into a diagonal basis by a mode expansion:
The action becomes:
which is the position-momentum path integral for a collection of independent Harmonic oscillators.
To see the equivalence, note that decomposed into real and imaginary parts the action is:
after an integration by parts. Integrating over formula_11 gives the action
which, rescaling formula_13, is a harmonic oscillator action with frequency formula_7.
Pair potential.
When the particles interact with a pair potential formula_15, the interaction is a nonlocal contribution to the action:
A pair-potential is the non-relativistic limit of a relativistic field coupled to electrodynamics. Ignoring the propagating degrees of freedom, the interaction between nonrelativistic electrons is the coulomb repulsion. In 3+1 dimensions, this is:
When coupled to an external potential to model classical positions of nuclei, a Schrödinger field with this pair potential describes nearly all of condensed matter physics. The exceptions are effects like superfluidity, where the quantum mechanical interference of nuclei is important, and inner shell electrons where the electron motion can be relativistic.
Nonlinear Schrödinger equation.
A special case of a delta-function interaction formula_15 is widely studied, and is known as the nonlinear Schrödinger equation. Because the interactions always happen when two particles occupy the same point, the action for the nonlinear Schrödinger equation is local:
The interaction strength formula_20 requires renormalization in dimensions higher than 2 and in two dimensions it has logarithmic divergence. In any dimensions, and even with power-law divergence, the theory is well defined. If the particles are fermions, the interaction vanishes.
Many-body potentials.
The potentials can include many-body contributions. The interacting Lagrangian is then:
These types of potentials are important in some effective descriptions of close-packed atoms. Higher order interactions are less and less important.
Canonical formalism.
The canonical momentum association with the field formula_2 is
The canonical commutation relations are like an independent harmonic oscillator at each point:
The field Hamiltonian is
and the field equation for any interaction is a nonlinear and nonlocal version of the Schrödinger equation. For pairwise interactions:
Perturbation theory.
The expansion in Feynman diagrams is called many-body perturbation theory. The propagator is
The interaction vertex is the Fourier transform of the pair-potential. In all the interactions, the number of incoming and outgoing lines is equal.
Exposition.
Identical particles.
The many body Schrödinger equation for identical particles describes the time evolution of the many-body wavefunction ψ("x"1, "x"2..."x"N) which is the probability amplitude for "N" particles to have the listed positions. The Schrödinger equation for ψ is:
with Hamiltonian
Since the particles are indistinguishable, the wavefunction has some symmetry under switching
positions. Either
Since the particles are indistinguishable, the potential V must be unchanged under permutations.
If
then it must be the case that formula_33. If
then formula_35 and so on.
In the Schrödinger equation formalism, the restrictions on the potential are ad-hoc, and the classical wave limit is hard to reach. It also has limited usefulness if a system is open to the environment, because particles might coherently enter and leave.
Nonrelativistic Fock space.
A Schrödinger field is defined by extending the Hilbert space of states to
include configurations with arbitrary particle number. A nearly complete basis for this set of states is the collection:
labeled by the total number of particles and their position. An arbitrary state with particles at separated positions is described by a superposition of states of this form.
In this formalism, keep in mind that any two states whose positions can be permuted into each other are really the same, so the integration domains need to avoid double counting. Also keep in mind that the states with more than one particle at the same point have not yet been defined. The quantity formula_38 is the amplitude that no particles are present, and its absolute square is the probability that the system is in the vacuum.
In order to reproduce the Schrödinger description, the inner product on the basis states should be
and so on. Since the discussion is nearly formally identical for bosons and fermions, although the physical properties are different, from here on the particles will be bosons.
There are natural operators in this Hilbert space. One operator, called formula_41, is the operator which introduces an extra particle at x.
It is defined on each basis state:
with slight ambiguity when a particle is already at x.
Another operator removes a particle at x, and is called formula_2. This operator is the conjugate of the operator formula_44. Because formula_45 has no matrix elements which connect to states with no particle at x, formula_2 must give zero when acting on such a state.
The position basis is an inconvenient way to understand coincident particles because states with a particle localized at one point have infinite energy, so intuition is difficult. In order to see what happens when two particles are at exactly the same point, it is mathematically simplest either to make space into a discrete lattice, or to Fourier transform the field in a finite volume.
The operator
creates a superposition of one particle states in a plane wave state with momentum k, in other words, it produces a new particle with momentum k. The operator
annihilates a particle with momentum k.
If the potential energy for interaction of infinitely distant particles vanishes, the fourier transformed operators in infinite volume create states which are noninteracting. The states are infinitely spread out, and the chance that the particles are nearby is zero.
The matrix elements for the operators between non-coincident points reconstructs the matrix elements of the Fourier transform between all modes:
where the delta function is either the Dirac delta function or the Kronecker delta, depending on whether the volume is infinite or finite.
The commutation relations now determine the operators completely, and when the spatial volume is finite, there are no conceptual hurdle to understand coinciding momenta because momenta are discrete. In a discrete momentum basis, the basis states are:
where the n's are the number of particles at each momentum. For fermions and anyons, the number of particles at any momentum is always either zero or one. The operators formula_54 have harmonic-oscillator like matrix elements between states, independent of the interaction:
So that the operator
counts the total number of particles.
Now it is easy to see that the matrix elements of formula_58 and formula_41 have harmonic oscillator commutation relations too.
So that there really is no difficulty with coincident particles in position space.
The operator formula_62 which removes and replaces a particle, acts as a sensor to detect if a particle is present at x. The operator formula_63 acts to multiply the state by the gradient of the many body wavefunction. The operator
acts to reproduce the right hand side of the Schrödinger equation when acting on any basis state, so that
holds as an operator equation. Since this is true for an arbitrary state, it is also true without the formula_45.
To add interactions, add nonlinear terms in the field equations. The field form automatically ensures that the potentials obey the restrictions from symmetry.
Field Hamiltonian.
The field Hamiltonian which reproduces the equations of motion is
The Heisenberg equations of motion for this operator reproduces the equation of motion for the field.
To find the classical field Lagrangian, apply a Legendre transform to the classical limit of the Hamiltonian.
Although this is correct classically, the quantum mechanical transformation is not completely conceptually straightforward because the path integral is over eigenvalues of operators ψ which are not hermitian and whose eigenvalues are not orthogonal. The path integral over field states therefore seems naively to be overcounting. This is not the case, because the time derivative term in L includes the overlap between the different field states.

</doc>
<doc id="30862736" url="https://en.wikipedia.org/wiki?curid=30862736" title="Schwarzschild coordinates">
Schwarzschild coordinates

In the theory of Lorentzian manifolds, spherically symmetric spacetimes admit a family of "nested round spheres". In such a spacetime, a particularly important kind of coordinate chart is the Schwarzschild chart, a kind of polar spherical coordinate chart on a static and spherically symmetric spacetime, which is "adapted" to these nested round spheres. The defining characteristic of Schwarzschild chart is that the radial coordinate possesses a natural geometric interpretation in terms of the surface area and Gaussian curvature of each sphere. However, radial distances and angles are not accurately represented.
These charts have many applications in metric theories of gravitation such as general relativity. They are most often used in static spherically symmetric spacetimes. In the case of general relativity, Birkhoff's theorem states that every "isolated" spherically symmetric vacuum or electrovacuum solution of the Einstein field equation is static, but this is certainly not true for perfect fluids. We should also note that the extension of the exterior region of the Schwarzschild vacuum solution inside the event horizon of a spherically symmetric black hole is not static inside the horizon, and the family of (spacelike) nested spheres cannot be extended inside the horizon, so the Schwarzschild chart for this solution necessarily breaks down at the horizon.
Definition.
Specifying a metric tensor is part of the definition of any Lorentzian manifold. The simplest way to define this tensor is to define it in compatible local coordinate charts and verify that the same tensor is defined on the overlaps of the domains of the charts. In this article, we will only attempt to define the metric tensor in the domain of a single chart.
In a Schwarzschild chart (on a static spherically symmetric spacetime), the line element takes the form
Depending on context, it may be appropriate to regard "f" and "g" as undetermined functions of the radial coordinate (for example, in deriving an exact static spherically symmetric solution of the Einstein field equation). Alternatively, we can plug in specific functions (possibly depending on some parameters) to obtain a Schwarzschild coordinate chart on a specific Lorentzian spacetime.
If this turns out to admit a stress–energy tensor such that the resulting model satisfies the Einstein field equation (say, for a static spherically symmetric perfect fluid obeying suitable energy conditions and other properties expected of reasonable perfect fluid), then, with appropriate tensor fields representing physical quantities such as matter and momentum densities, we have a piece of a possibly larger spacetime; a piece which can be considered a "local solution" of the Einstein field equation.
Killing vector fields.
With respect to the Schwarzschild chart, the Lie algebra of Killing vector fields is generated by the timelike "irrotational" Killing vector field
and three spacelike Killing vector fields
Here, saying that formula_5 is irrotational means that the vorticity tensor of the corresponding timelike congruence vanishes; thus, this Killing vector field is "hypersurface orthogonal". The fact that our spacetime admits an irrotational timelike Killing vector field is in fact the defining characteristic of a static spacetime. One immediate consequence is that the "constant time coordinate surfaces" formula_6 form a family of (isometric) "spatial hyperslices". (This is not true for example in the Boyer–Lindquist chart for the exterior region of the Kerr vacuum, where the timelike coordinate vector is not hypersurface orthogonal.)
A family of static nested spheres.
In the Schwarzschild chart, the surfaces formula_7 appear as round spheres (when we plot loci in polar spherical fashion), and from the form of the line element, we see that the metric restricted to any of these surfaces is
That is, these "nested coordinate spheres" do in fact represent geometric spheres with 
That is, they are "geometric round spheres". Moreover, the angular coordinates formula_11 are exactly the usual polar spherical angular coordinates: formula_12 is sometimes called the "colatitude" and formula_13 is usually called the "longitude". This is essentially the defining geometric feature of the Schwarzschild chart.
It may help to add that the four Killing fields given above, considered as "abstract vector fields" on our Lorentzian manifold, give the truest expression of both the symmetries of a static spherically symmetric spacetime, while the "particular trigonometric form" which they take in our chart is the truest expression of the meaning of the term "Schwarzschild chart". In particular, the three spatial Killing vector fields have exactly the same form as the three nontranslational Killing vector fields in a spherically symmetric chart on E3; that is, they exhibit the notion of arbitrary Euclidean rotation about the origin or spherical symmetry.
However, note well: in general, the Schwarzschild radial coordinate "does not accurately represent radial distances", i.e. distances taken along the spacelike geodesic congruence which arise as the integral curves of formula_14. Rather, to find a suitable notion of 'spatial distance' between two of our nested spheres, we should integrate formula_15 along some coordinate ray from the origin:
Similarly, we can regard each sphere as the locus of a spherical cloud of idealized observers, who must (in general) use rocket engines to accelerate radially outward in order to maintain their position. These are "static observers", and they have world lines of form formula_17, which of course have the form of "vertical coordinate lines" in the Schwarzschild chart.
In order to compute the proper time interval between two events on the world line of one of these observers, we must integrate formula_18 along the appropriate coordinate line:
Coordinate singularities.
Looking back at the coordinate ranges above, note that the coordinate singularity at formula_20
marks the location of the "North pole" of one of our static nested spheres, while formula_21 marks the location of the "South pole". Just as for an ordinary polar spherical chart on E3, for topological reasons we cannot obtain continuous coordinates on the entire sphere; we must choose some longitude (a great circle) to act as the "prime meridian" formula_22 and cut this out of the chart. The result is that we cut out a closed half plane from each spatial hyperslice formula_6 including the axis formula_24 and a half plane extending from that axis.
When we said above that formula_25 is a Killing vector field, we omitted the pedantic but important qualifier that we are thinking of formula_13 as a "cyclic" coordinate, and indeed thinking of our three spacelike Killing vectors as acting on round spheres.
Possibly, of course, formula_27 or formula_28, in which case we must "also" excise the region outside some ball, or inside some ball, from the domain of our chart. This happens whenever f or g blow up at some value of the Schwarzschild radial coordinate r.
Visualizing the static hyperslices.
To better understand the significance of the Schwarzschild radial coordinate, it may help to embed one of the spatial hyperslices formula_6 (they are of course all isometric to one another) in a flat Euclidean space. People who find it difficult to visualize four-dimensional Euclidean space will be glad to observe that we can take advantage of the spherical symmetry to "suppress one coordinate". This may be conveniently achieved by setting formula_30. Now we have a two-dimensional Riemannian manifold with a local radial coordinate chart,
To embed this surface (or at an annular ring) in E3, we adopt a frame field in E3 which 
To wit, consider the parameterized surface
The coordinate vector fields on this surface are
The induced metric inherited when we restrict the Euclidean metric on E3 to our parameterized surface is
To identify this with the metric of our hyperslice, we should evidently choose h(r) so that
To take a somewhat silly example, we might have formula_36.
This works for surfaces in which true distances between two radially separated points are "larger" than the difference between their radial coordinates. If the true distances are "smaller", we should embed our Riemannian manifold as a spacelike surface in E1,2 instead. For example, we might have formula_37. Sometimes we might need two or more "local" embeddings of annular rings (for regions of positive or negative Gaussian curvature). In general, we should not expect to obtain a "global" embedding in any one flat space (with vanishing Riemann tensor).
The point is that the defining characteristic of a Schwarzschild chart in terms of the geometric interpretation of the radial coordinate is just what we need to carry out (in principle) this kind of spherically symmetric embedding of the spatial hyperslices.
A metric Ansatz.
The line element given above, with "f","g" regarded as undetermined functions of the Schwarzschild radial coordinate "r", is often used as a metric ansatz in deriving static spherically symmetric solutions in general relativity (or other metric theories of gravitation).
As an illustration, we will indicate how to compute the connection and curvature using Cartan's exterior calculus method. First, we read off the line element a coframe field,
where we regard "f,g" as undetermined smooth functions of "r". (The fact that our spacetime admits a frame having this particular trigonometric form is yet another equivalent expression of the notion of a Schwarzschild chart in a static, spherically symmetric Lorentzian manifold).
Second, we compute the exterior derivatives of these cobasis one-forms:
Comparing with Cartan's "first structural equation" (or rather its integrability condition),
we guess expressions for the connection one-forms. (The hats are just a notational device for reminding us that the indices refer to our cobasis one-forms, not to the coordinate one-forms formula_47.)
If we recall which pairs of indices are symmetric (space-time) and which are antisymmetric (space-space) in formula_48, we can confirm that the six connection one-forms are
We can collect these one-forms into a matrix of one-forms, or even better an SO(1,3)-valued one-form.
Note that the resulting matrix of one-forms will not quite be "antisymmetric" as for an SO(4)-valued one-form; we need to use instead a notion of transpose arising from the Lorentzian adjoint.
Third, we compute the exterior derivatives of the connection one-forms and use Cartan's "second structural equation"
to compute the curvature two forms. Fourth, using the formula
where the Bach bars indicate that we should sum only over the six "increasing pairs" of indices ("i","j"), we can read off the linearly independent components of the Riemann tensor with respect to our coframe and its dual frame field. We obtain:
Fifth, we can lower indices and organize the components formula_61 into a matrix
where E,L are symmetric (six linearly independent components, in general) and B is traceless (eight linearly independent components, in general), which we think of as representing a linear operator on the six-dimensional vector space of two forms (at each event). From this we can read off the Bel decomposition with respect to the timelike unit vector field formula_63. The electrogravitic tensor is
The magnetogravitic tensor vanishes identically, and the topogravitic tensor, from which (using the fact that formula_65 is irrotational) we can determine the three-dimensional Riemann tensor of the spatial hyperslices, is
This is all valid for any Lorentzian manifold, but we note that in general relativity, the electrogravitic tensor controls tidal stresses on small objects, as measured by the observers corresponding to our frame, and the magnetogravitic tensor controls any spin-spin forces on spinning objects, as measured by the observers corresponding to our frame.
The dual frame field of our coframe field is
The fact that the factor formula_71 only multiplies the first of the three orthonormal spacelike vector fields here means that Schwarzschild charts are "not spatially isotropic" (except in the trivial case of a locally flat spacetime); rather, the light cones appear (radially flattened) or (radially elongated). This is of course just another way of saying that Schwarzschild charts correctly represent distances within each nested round sphere, but the radial coordinate does not faithfully represent radial proper distance.
Some exact solutions admitting Schwarzschild charts.
Some examples of exact solutions which can be obtained in this way include:
Generalizations.
It is natural to consider nonstatic but spherically symmetric spacetimes, with a generalized Schwarzschild chart in which the line element takes the form
Generalizing in another direction, we can use other coordinate systems on our round two-spheres, to obtain for example a "stereographic Schwarzschild chart" which is sometimes useful:

</doc>
<doc id="264606" url="https://en.wikipedia.org/wiki?curid=264606" title="Schwarzschild metric">
Schwarzschild metric

In Einstein's theory of general relativity, the Schwarzschild metric (also known as the Schwarzschild vacuum or Schwarzschild solution) is the solution to the Einstein field equations that describes the gravitational field outside a spherical mass, on the assumption that the electric charge of the mass, angular momentum of the mass, and universal cosmological constant are all zero. The solution is a useful approximation for describing slowly rotating astronomical objects such as many stars and planets, including Earth and the Sun. The solution is named after Karl Schwarzschild, who first published the solution in 1916.
According to Birkhoff's theorem, the Schwarzschild metric is the most general spherically symmetric, vacuum solution of the Einstein field equations. A Schwarzschild black hole or static black hole is a black hole that has no charge or angular momentum. A Schwarzschild black hole is described by the Schwarzschild metric, and cannot be distinguished from any other Schwarzschild black hole except by its mass.
The Schwarzschild black hole is characterized by a surrounding spherical surface, called the event horizon, which is situated at the Schwarzschild radius, often called the radius of a black hole. Any non-rotating and non-charged mass that is smaller than its Schwarzschild radius forms a black hole. The solution of the Einstein field equations is valid for any mass "M", so in principle (according to general relativity theory) a Schwarzschild black hole of any mass could exist if conditions became sufficiently favorable to allow for its formation.
The Schwarzschild metric.
In Schwarzschild coordinates, the line element for the Schwarzschild metric has the form
where
The analogue of this solution in classical Newtonian theory of gravity corresponds to the gravitational field around a point particle.
In practice, the ratio "r""s"/"r" is almost always extremely small. For example, the Schwarzschild radius "r""s" of the Earth is roughly 8.9 mm, while the Sun, which is 3.3×105 times as massive has a Schwarzschild radius of approximately 3.0 km.
Even at the surface of the Earth, the corrections to Newtonian gravity are only one part in a billion. The ratio only becomes large close to black holes and other ultra-dense objects such as neutron stars.
The Schwarzschild metric is a solution of Einstein's field equations in empty space, meaning that it is valid only "outside" the gravitating body. That is, for a spherical body of radius "R" the solution is valid for "r" > "R". To describe the gravitational field both inside and outside the gravitating body the Schwarzschild solution must be matched with some suitable interior solution at "r" = "R".
History.
The Schwarzschild solution is named in honor of Karl Schwarzschild, who found the exact solution in 1915 and published it in 1916, a little more than a month after the publication of Einstein's theory of general relativity.
It was the first exact solution of the Einstein field equations other than the trivial flat space solution. Schwarzschild died shortly after his paper was published, as a result of a disease he contracted while serving in the German army during World War I.
Johannes Droste in 1916
independently produced the same solution as Schwarzschild, using a simpler, more direct derivation.
In the early years of general relativity there was a lot of confusion about the nature of the singularities found in the Schwarzschild and other solutions of the Einstein field equations. In Schwarzschild's original paper, he put what we now call the event horizon at the origin of his coordinate system. In this paper he also introduced what is now known as the Schwarzschild radial coordinate ("r" in the equations above), as an auxiliary variable. In his equations, Schwarzschild was using a different radial coordinate that was zero at the Schwarzschild radius.
A more complete analysis of the singularity structure was given by David Hilbert in the following year, identifying the singularities both at "r" = 0 and "r" = "r"s. Although there was general consent that the singularity at "r" = 0 was a 'genuine' physical singularity, the nature of the singularity at "r" = "r"s remained unclear.
In 1921 Paul Painlevé and in 1922 Allvar Gullstrand independently produced a metric, a spherically symmetric solution of Einstein's equations, which we now know is coordinate transformation of the Schwarzschild metric, Gullstrand–Painlevé coordinates, in which there was no singularity at "r" = "r"s. They, however, did not recognize that their solutions were just coordinate transforms, and in fact used their solution to argue that Einstein's theory was wrong. In 1924 Arthur Eddington produced the first coordinate transformation (Eddington–Finkelstein coordinates) that showed that the singularity at "r" = "r"s was a coordinate artifact, although he also seems to have been unaware of the significance of this discovery. Later, in 1932, Georges Lemaître gave a different coordinate transformation (Lemaître coordinates) to the same effect and was the first to recognize that this implied that the singularity at "r" = "r"s was not physical. In 1939 Howard Robertson showed that a free falling observer descending in the Schwarzschild metric would cross the "r" = "r"s singularity in a finite amount of proper time even though this would take an infinite amount of time in terms of coordinate time "t".
In 1950, John Synge produced a paper that showed the maximal analytic extension of the Schwarzschild metric, again showing that the singularity at "r" = "r"s was a coordinate artifact and that it represented two horizons. A similar result was later rediscovered by George Szekeres, and independently Martin Kruskal. The new coordinates nowadays known as Kruskal-Szekeres coordinates were much simpler than Synge's but both provided a single set of coordinates that covered the entire spacetime. However, perhaps due to the obscurity of the journals in which the papers of Lemaître and Synge were published their conclusions went unnoticed, with many of the major players in the field including Einstein believing that singularity at the Schwarzschild radius was physical.
Progress was only made in the 1960s when the more exact tools of differential geometry entered the field of general relativity, allowing more exact definitions of what it means for a Lorentzian manifold to be singular. This led to definitive identification of the "r" = "r"s singularity in the Schwarzschild metric as an event horizon (a hypersurface in spacetime that can only be crossed in one direction).
Singularities and black holes.
The Schwarzschild solution appears to have singularities at "r" = 0 and "r" = "r""s"; some of the metric components "blow up" at these radii. Since the Schwarzschild metric is only expected to be valid for radii larger than the radius "R" of the gravitating body, there is no problem as long as "R" > "r""s". For ordinary stars and planets this is always the case. For example, the radius of the Sun is approximately 700,000 km, while its Schwarzschild radius is only 3 km.
The singularity at "r" = "r""s" divides the Schwarzschild coordinates in two disconnected patches. The "exterior Schwarzschild solution" with "r" > "r""s" is the one that is related to the gravitational fields of stars and planets. The "interior Schwarzschild solution" with 0 < "r" < "r""s", which contains the singularity at "r" = 0, is completely separated from the outer patch by the singularity at "r" = "r""s". The Schwarzschild coordinates therefore give no physical connection between the two patches, which may be viewed as separate solutions. The singularity at "r" = "r""s" is an illusion however; it is an instance of what is called a "coordinate singularity". As the name implies, the singularity arises from a bad choice of coordinates or coordinate conditions. When changing to a different coordinate system (for example Lemaitre coordinates, Eddington–Finkelstein coordinates, Kruskal–Szekeres coordinates, Novikov coordinates, or Gullstrand–Painlevé coordinates) the metric becomes regular at "r" = "r""s" and can extend the external patch to values of "r" smaller than "r""s". Using a different coordinate transformation one can then relate the extended external patch to the inner patch.
The case "r" = 0 is different, however. If one asks that the solution be valid for all "r" one runs into a true physical singularity, or "gravitational singularity", at the origin. To see that this is a true singularity one must look at quantities that are independent of the choice of coordinates. One such important quantity is the Kretschmann invariant, which is given by
At "r" = 0 the curvature becomes infinite, indicating the presence of a singularity. At this point the metric, and space-time itself, is no longer well-defined. For a long time it was thought that such a solution was non-physical. However, a greater understanding of general relativity led to the realization that such singularities were a generic feature of the theory and not just an exotic special case.
The Schwarzschild solution, taken to be valid for all "r" > 0, is called a Schwarzschild black hole. It is a perfectly valid solution of the Einstein field equations, although it has some rather bizarre properties. For "r" < "r""s" the Schwarzschild radial coordinate "r" becomes timelike and the time coordinate "t" becomes spacelike. A curve at constant "r" is no longer a possible worldline of a particle or observer, not even if a force is exerted to try to keep it there; this occurs because spacetime has been curved so much that the direction of cause and effect (the particle's future light cone) points into the singularity. The surface "r" = "r""s" demarcates what is called the "event horizon" of the black hole. It represents the point past which light can no longer escape the gravitational field. Any physical object whose radius "R" becomes less than or equal to the Schwarzschild radius will undergo gravitational collapse and become a black hole.
Alternative coordinates.
The Schwarzschild solution can be expressed in a range of different choices of coordinates besides the Schwarzschild coordinates used above. Different choices tend to highlight different features of the solution. The table below shows some popular choices.
In table above, some shorthand has been introduced for brevity. The speed of light "c" has been set to one. The notation formula_6 is used for the metric of a two dimensional sphere. Moreover, in each entry "R" and "T" denote alternative choices of radial and time coordinate for the particular coordinates. Note, the "R" and/or "T" may vary from entry to entry.
Flamm's paraboloid.
The spatial curvature of the Schwarzschild solution for formula_7 can be visualized as the graphic shows. Consider a constant time equatorial slice through the Schwarzschild solution ("θ" = "π"/2, "t" = constant) and let the position of a particle moving in this plane be described with the remaining Schwarzschild coordinates ("r", "φ"). Imagine now that there is an additional Euclidean dimension "w", which has no physical reality (it is not part of spacetime). Then replace the ("r", "φ") plane with a surface dimpled in the "w" direction according to the equation ("Flamm's paraboloid")
This surface has the property that distances measured within it match distances in the Schwarzschild metric, because with the definition of "w" above,
Thus, Flamm's paraboloid is useful for visualizing the spatial curvature of the Schwarzschild metric. It should not, however, be confused with a gravity well. No ordinary (massive or massless) particle can have a worldline lying on the paraboloid, since all distances on it are spacelike (this is a cross-section at one moment of time, so any particle moving on it would have an infinite velocity). Even a tachyon would not move along the path that one might naively expect from a "rubber sheet" analogy: in particular, if the dimple is drawn pointing upward rather than downward, the tachyon's path still curves toward the central mass, not away. See the gravity well article for more information.
Flamm's paraboloid may be derived as follows. The Euclidean metric in the cylindrical coordinates ("r", "φ", "w") is written
Letting the surface be described by the function formula_11, the Euclidean metric can be written as
Comparing this with the Schwarzschild metric in the equatorial plane ("θ" = π/2) at a fixed time ("t" = constant, "dt" = 0)
yields an integral expression for "w"("r"):
whose solution is Flamm's paraboloid.
Orbital motion.
A particle orbiting in the Schwarzschild metric can have a stable circular orbit with formula_15. Circular orbits with formula_3 between formula_17 and formula_18 are unstable, and no circular orbits exist for formula_19. The circular orbit of minimum radius formula_17 corresponds to an orbital velocity approaching the speed of light. It is possible for a particle to have a constant value of formula_3 between formula_4 and formula_17, but only if some force acts to keep it there.
Noncircular orbits, such as Mercury's, dwell longer at small radii than would be expected classically. This can be seen as a less extreme version of the more dramatic case in which a particle passes through the event horizon and dwells inside it forever. Intermediate between the case of Mercury and the case of an object falling past the event horizon, there are exotic possibilities such as "knife-edge" orbits, in which the satellite can be made to execute an arbitrarily large number of nearly circular orbits, after which it flies back outward.
Symmetries.
The group of isometries of the Schwarzschild metric is the subgroup of the ten-dimensional Poincaré group which takes the time axis (trajectory of the star) to itself. It omits the spatial translations (three dimensions) and boosts (three dimensions). It retains the time translations (one dimension) and rotations (three dimensions). Thus it has four dimensions. Like the Poincaré group, it has four connected components: the component of the identity; the time reversed component; the spatial inversion component; and the component which is both time reversed and spatially inverted.
Quotes.
""" (It is always pleasant to have exact solutions in simple form at your disposal.)" – Karl Schwarzschild, 1916.

</doc>
<doc id="206115" url="https://en.wikipedia.org/wiki?curid=206115" title="Schwarzschild radius">
Schwarzschild radius

The Schwarzschild radius (sometimes historically referred to as the gravitational radius) is the radius of a sphere such that, if all the mass of an object were to be compressed within that sphere, the escape velocity from the surface of the sphere would equal the speed of light. An example of an object where the mass is within its Schwarzschild radius is a black hole. Once a stellar remnant collapses to or below this radius, light cannot escape and the object is no longer directly visible, thereby forming a black hole. It is a characteristic radius associated with every quantity of mass. The "Schwarzschild radius" was named after the German astronomer Karl Schwarzschild, who calculated this exact solution for the theory of general relativity in 1916.
History.
In 1916, Karl Schwarzschild obtained the exact solution to Einstein's field equations for the gravitational field outside a non-rotating, spherically symmetric body (see Schwarzschild metric). Using the definition formula_1, the solution contained a term of the form formula_2; where the value of formula_3 making this term singular has come to be known as the "Schwarzschild radius". The physical significance of this "singularity", and whether this singularity could ever occur in nature, was debated for many decades; a general acceptance of the possibility of a black hole did not occur until the second half of the 20th century.
Parameters.
The Schwarzschild radius of an object is proportional to the mass. Accordingly, the Sun has a Schwarzschild radius of approximately , whereas Earth's is only about 9.0 mm. The observable universe's mass has a Schwarzschild radius of approximately 13.7 billion light years.
Formula.
The Schwarzschild radius is proportional to the mass with a proportionality constant involving the gravitational constant and the speed of light:
where:
The proportionality constant, 2"G"/"c"2, is approximately "m"/"kg", or "km"/.
An object of any density can be large enough to fall within its own Schwarzschild radius,
where:
Black hole classification by Schwarzschild radius.
Any object whose radius is smaller than its Schwarzschild radius is called a black hole. The surface at the Schwarzschild radius acts as an event horizon in a non-rotating body (a rotating black hole operates slightly differently). Neither light nor particles can escape through this surface from the region inside, hence the name "black hole".
Black holes can be classified based on their Schwarzschild radius, or equivalently, by their density. As the radius is linearly related to mass, and the enclosed volume to the third power of radius, the mass density of such volumes at the low end of the mass scale is much higher than that of the larger black holes. The volume enclosed in the event horizon of the most massive black holes has a mass density lower than main sequence stars.
Supermassive black hole.
A supermassive black hole (SMBH) is the largest type of black hole, though there are few official criteria on how such an object is considered so, on the order of hundreds of thousands to billions of solar masses. (Supermassive black holes up to 21 billion have been detected, such as NGC 4889.) Unlike stellar mass black holes, supermassive black holes have low densities if you assume that the Schwarzschild radius is the outer edge of the black hole (note that this assumption is in contrast to the typical assumption that a black hole is a singularity, and therefore has zero radial extent). Under this assumption, the average density of a supermassive black hole can be less than the density of water.
The Schwarzschild radius of a body is proportional to its mass and therefore to its volume, assuming that the body has a constant mass-density. In contrast, the physical radius of the body is proportional to the cube root of its volume. Therefore, as the body accumulates matter at a given fixed density (in this example, 103 kg/m3, the density of water), its Schwarzschild radius will increase more quickly than its physical radius. When a body of this density has grown to around 136 million solar masses , its physical radius would be overtaken by its Schwarzschild radius, and thus it would form a supermassive black hole.
It is thought that supermassive black holes like these do not form immediately from the singular collapse of a cluster of stars.
Instead they may begin life as smaller, stellar-sized black holes and grow larger by the accretion of matter, or even of other black holes.
The Schwarzschild radius of the supermassive black hole at our Galactic Center would be approximately 13.3 million kilometres.
Stellar black hole.
Stellar black holes have much greater densities than supermassive black holes. If one accumulates matter at nuclear density (the density of the nucleus of an atom, about 1018 kg/m3; neutron stars also reach this density), such an accumulation would fall within its own Schwarzschild radius at about and thus would be a stellar black hole.
Primordial black hole.
A small mass has an extremely small Schwarzschild radius. A mass similar to Mount Everest has a Schwarzschild radius much smaller than a nanometre. Its average density at that size would be so high that no known mechanism could form such extremely compact objects. Such black holes might possibly be formed in an early stage of the evolution of the universe, just after the Big Bang, when densities were extremely high. Therefore these hypothetical miniature black holes are called primordial black holes.
Other uses.
In gravitational time dilation.
Gravitational time dilation near a large, slowly rotating, nearly spherical body, such as the Earth or Sun can be reasonably approximated using the Schwarzschild radius as follows:
where:
The results of the Pound–Rebka experiment in 1959 were found to be consistent with predictions made by general relativity. By measuring Earth’s gravitational time dilation, this experiment indirectly measured Earth’s Schwarzschild radius.
In Newtonian gravitational fields.
The Newtonian gravitational field near a large, slowly rotating, nearly spherical body can be reasonably approximated using the Schwarzschild radius as follows:
where:
On the surface of the Earth:
In Keplerian orbits.
For all circular orbits around a given central body:
where:
This equality can be generalized to elliptic orbits as follows:
where:
For the Earth orbiting the Sun:
Relativistic circular orbits and the photon sphere.
The Keplerian equation for circular orbits can be generalized to the relativistic equation for circular orbits by accounting for time dilation in the velocity term:
This final equation indicates that an object orbiting at the speed of light would have an orbital radius of 1.5 times the Schwarzschild radius. This is a special orbit known as the photon sphere.
See also.
Classification of black holes by type:
A classification of black holes by mass:

</doc>
<doc id="26873" url="https://en.wikipedia.org/wiki?curid=26873" title="Second">
Second

The second (symbol: s) (abbreviated s or sec) is the base unit of time in the International System of Units (SI). It is qualitatively defined as the "second" division of the hour by sixty, the first division by sixty being the minute. It is quantitatively defined in terms of a certain number of periods – about 9 billion – of a certain frequency of radiation from the caesium atom: a so-called atomic clock. Seconds may be measured using a mechanical, electric or atomic clock.
In the year 1000 CE, the Persian Muslim scholar al-Biruni first used the term "second" in Arabic ( "") and defined it as (that is, ) of a mean solar day. In the 13th century, scientists who wrote in Latin, including Bacon, and later Kepler and Tycho, used the Latin term "parte minutae secundae" (or "secunda" for short) to mean a unit of time which represented the "second small part" of an hour as the division of one minute by 60 (with the minute being the "pars minuta prima" or "first small part"). The use of the word "second" in English began in the late 16th century. The definition remained unchanged (and still applies in some astronomical and legal contexts) from 1000 until 1960, at which time it was defined as "the fraction 1/31,556,925.9747 of the tropical year for 1900 January 0 at 12 hours ephemeris time.". However, astronomical observations of the 19th and 20th centuries revealed that the mean solar day is slowly but measurably lengthening, and the length of a tropical year is not entirely predictable either. Thus the sun–earth motion was not considered a suitable basis for the definition. With the advent of atomic clocks, it became feasible to define the second based on a fundamental property of nature. Thus, a mere seven years later in 1967, the International Committee for Weights and Measures (abbreviated CIPM from the French Comité international des poids et mesures) changed the definition to "the duration of periods of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium 133 atom." In 1997, the CIPM added that the preceding definition "refers to a caesium atom at rest at a temperature of 0 K."
SI prefixes are combined with the word "second" to denote subdivisions of the second, "e.g.", the millisecond (one thousandth of a second), the microsecond (one millionth of a second), and the nanosecond (one billionth of a second). Though SI prefixes may also be used to form multiples of the second such as kilosecond (one thousand seconds), such units are rarely used in practice. The more common larger non-SI units of time are not formed by powers of ten; instead, the second is multiplied by 60 to form a minute, which is multiplied by 60 to form an hour, which is multiplied by 24 to form a day.
The second is also the base unit of time in other systems of measurement: the centimetre-gram-second, metre-kilogram-second, metre-tonne-second, and foot-pound-second systems of units.
International second.
Under the International System of Units (via the International Committee for Weights and Measures, or CIPM), since 1967 the second has been defined as the duration of periods of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium 133 atom. In 1997 CIPM added that the periods would be defined for a caesium atom at rest, and approaching the theoretical temperature of absolute zero (0 K), and in 1999, it included corrections from ambient radiation. Absolute zero implies no movement, and therefore zero external radiation effects (i.e., zero local electric and magnetic fields).
The second thus defined is consistent with the ephemeris second, which was based on astronomical measurements. (See History below.) The realization of the standard second is described briefly in a special publication from the National Institute of Standards and Technology, and in detail by the National Research Council of Canada.
Equivalence to other units.
Time units.
1 international second is equal to:
History of definition.
Early civilizations.
Early civilizations constructed divisions in the day, but none used the term second, and none was a precursor to the modern second:
Based on mechanical clocks.
The earliest clocks to display seconds appeared during the last half of the 16th century. The second became accurately measurable with the development of mechenical clocks keeping "mean time", as opposed to the "apparent time" displayed by sundials. The earliest spring-driven timepiece with a second hand which marked seconds is an unsigned clock depicting Orpheus in the Fremersdorf collection, dated between 1560 and During the 3rd quarter of the 16th century, Taqi al-Din built a clock with marks every 1/5 minute.
In 1579, Jost Bürgi built a clock for William of Hesse that marked In 1581, Tycho Brahe redesigned clocks that displayed minutes at his observatory so they also displayed seconds. However, they were not yet accurate enough for seconds. In 1587, Tycho complained that his four clocks disagreed by plus or minus four 
In 1644, Marin Mersenne calculated that a pendulum with a length of 39.1 inches (0.994 m) would have a period at one standard gravity of precisely two seconds, one second for a swing forward and one second for the return swing, enabling such a pendulum to tick in precise seconds.
In 1670, London clockmaker William Clement added this seconds pendulum to the original pendulum clock of Christiaan Huygens. From 1670 to 1680, Clement made many improvements to his clock and introduced the longcase or grandfather clock to the public. This clock used an anchor escapement mechanism with a seconds pendulum to display seconds in a small subdial. This mechanism required less power, caused less friction and was accurate enough to measure seconds reliably as one-sixtieth of a minute than the older verge escapement. Within a few years, most British precision clockmakers were producing longcase clocks and other clockmakers soon followed. Thus the second could now be reliably measured.
In 1832, Gauss proposed using the second as the base unit of time in his millimeter-milligram-second system of units. The British Association for the Advancement of Science (BAAS) in 1862 stated that "All men of science are agreed to use the second of mean solar time as the unit of time." BAAS formally proposed the CGS system in 1874, although this system was gradually replaced over the next 70 years by MKS units. Both the CGS and MKS systems used the same second as their base unit of time. MKS was adopted internationally during the 1940s, defining the second as of a mean solar day.
Based on a fraction of a year.
In 1956, the second was redefined in terms of a "year" (the period of the Earth's revolution around the Sun) "for a particular epoch" because, by then, it had become recognized that the Earth's rotation on its own axis was not sufficiently uniform as a standard of time. The Earth's motion was described in Newcomb's Tables of the Sun (1895), which provided a formula for estimating the motion of the Sun relative to the epoch 1900 based on astronomical observations made between 1750 and 1892.
The second was thus defined as:
This definition was ratified by the Eleventh General Conference on Weights and Measures in 1960, which also established the International System of Units.
The "tropical year" in the 1960 definition was not measured but calculated from a formula describing a mean tropical year that decreased linearly over time, hence the curious reference to a specific "instantaneous" tropical year. This was in conformity with the ephemeris time scale adopted by the IAU in 1952. This definition brings the observed positions of the celestial bodies into accord with Newtonian dynamical theories of their motion. Specifically, those tables used for most of the 20th century were Newcomb's Tables of the Sun (used from 1900 through 1983) and Brown's Tables of the Moon (used from 1923 through 1983).
Thus, the 1960 SI definition abandoned any explicit relationship between the scientific second and the length of a day, as most people understand the term.
Based on caesium microwave atomic clock.
With the development of the atomic clock in the early 1960s, it was decided to use atomic time as the basis of the definition of the second, rather than the revolution of the Earth around the Sun.
Following several years of work, Louis Essen from the National Physical Laboratory (Teddington, England) and William Markowitz from the United States Naval Observatory (USNO) determined the relationship between the hyperfine transition frequency of the caesium atom and the ephemeris second. Using a common-view measurement method based on the received signals from radio station WWV, they determined the orbital motion of the Moon about the Earth, from which the apparent motion of the Sun could be inferred, in terms of time as measured by an atomic clock. They found that the second of ephemeris time (ET) had the duration of 9,192,631,770 ± 20 cycles of the chosen caesium frequency. As a result, in 1967 the "Thirteenth General Conference on Weights and Measures" defined the SI second of atomic time as:
This SI second, referred to atomic time, was later verified to be in agreement, within 1 part in 1010, with the second of ephemeris time as determined from lunar observations. (Nevertheless, this SI second was already, when adopted, a little shorter than the then-current value of the second of mean solar time.)
During the 1970s it was realized that gravitational time dilation caused the second produced by each atomic clock to differ depending on its altitude. A uniform second was produced by correcting the output of each atomic clock to mean sea level (the rotating geoid), lengthening the second by about 1. This correction was applied at the beginning of 1977 and formalized in 1980. In relativistic terms, the SI second is defined as the proper time on the rotating geoid.
The definition of the second was later refined at the 1997 meeting of the BIPM to include the statement
The revised definition seems to imply that the ideal atomic clock contains a single caesium atom at rest emitting a single frequency. In practice, however, the definition means that high-precision realizations of the second should compensate for the effects of the ambient temperature (black-body radiation) within which atomic clocks operate, and extrapolate accordingly to the value of the second at a temperature of absolute zero.
Proposed: based on optical atomic clock.
Today, the atomic clock operating in the microwave region is challenged by atomic clocks operating in the optical region. To quote Ludlow "et al.", “In recent years, optical atomic clocks have become increasingly competitive in performance with their microwave counterparts. The overall accuracy of single-trapped-ion-based optical standards closely approaches that of the state-of-the-art caesium fountain standards. Large ensembles of ultracold alkaline earth atoms have provided impressive clock stability for short averaging times, surpassing that of single-ion-based systems. So far, interrogation of neutral-atom-based optical standards has been carried out primarily in free space, unavoidably including atomic motional effects that typically limit the overall system accuracy. An alternative approach is to explore the ultranarrow optical transitions of atoms held in an optical lattice. The atoms are tightly localized so that Doppler and photon-recoil related effects on the transition frequency are eliminated.”
The Canadian National Research Council attaches a "relative uncertainty" of 2.5 (limited by day-to-day and device-to-device reproducibility) to their atomic clock based upon the 127I2 molecule, and is advocating use of an 88Sr ion trap instead (relative uncertainty due to linewidth of 2.2). See magneto-optical trap and Such uncertainties rival that of the NIST-F1 caesium atomic clock in the microwave region, estimated as a few parts in 1016 averaged over a day.
SI multiples.
SI prefixes are commonly used to measure time less than a second, but rarely for multiples of a second (which is known as metric time). Instead, the non-SI units minutes, hours, days, Julian years, Julian centuries, and Julian millennia are used.
Other current definitions.
For specialized purposes, a second may be used as a unit of time in time scales where the precise length differs slightly from the SI definition. One such time scale is UT1, a form of universal time. McCarthy and Seidelmann refrain from stating that the SI second is the legal standard for timekeeping throughout the world, saying only that "over the years UTC [which ticks SI seconds] has become either the basis for legal time of many countries, or accepted as the "de facto" basis for standard civil time".

</doc>
<doc id="3224219" url="https://en.wikipedia.org/wiki?curid=3224219" title="Second derivative">
Second derivative

In calculus, the second derivative, or the second order derivative, of a function is the derivative of the derivative of . Roughly speaking, the second derivative measures how the rate of change of a quantity is itself changing; for example, the second derivative of the position of a vehicle with respect to time is the instantaneous acceleration of the vehicle, or the rate at which the velocity of the vehicle is changing with respect to time. In Leibniz notation:
On the graph of a function, the second derivative corresponds to the curvature or concavity of the graph. The graph of a function with positive second derivative curves upwards, while the graph of a function with negative second derivative curves downwards.
Second derivative power rule.
The power rule for the first derivative, if applied twice, will produce the second derivative power rule as follows:
formula_2
Notation.
The second derivative of a function formula_3 is usually denoted formula_4. That is:
When using Leibniz's notation for derivatives, the second derivative of a dependent variable "y" with respect to an independent variable "x" is written
This notation is derived from the following formula:
Example.
Given the function
the derivative of "f" is the function
The second derivative of "f" is the derivative of "f"′, namely
Relation to the graph.
Concavity.
The second derivative of a function "f" measures the concavity of the graph of "f". A function whose second derivative is positive will be concave up (sometimes referred to as convex), meaning that the tangent line will lie below the graph of the function. Similarly, a function whose second derivative is negative will be concave down (sometimes called simply “concave”), and its tangent lines will lie above the graph of the function.
Inflection points.
If the second derivative of a function changes sign, the graph of the function will switch from concave down to concave up, or vice versa. A point where this occurs is called an inflection point. Assuming the second derivative is continuous, it must take a value of zero at any inflection point, although not every point where the second derivative is zero is necessarily a point of inflection.
Second derivative test.
The relation between the second derivative and the graph can be used to test whether a stationary point for a function (i.e. a point where formula_11) is a local maximum or a local minimum. Specifically,
The reason the second derivative produces these results can be seen by way of a real-world analogy. Consider a vehicle that at first is moving forward at a great velocity, but with a negative acceleration. Clearly the position of the vehicle at the point where the velocity reaches zero will be the maximum distance from the starting position – after this time, the velocity will become negative and the vehicle will reverse. The same is true for the minimum, with a vehicle that at first has a very negative velocity but positive acceleration.
Limit.
It is possible to write a single limit for the second derivative:
The limit is called the second symmetric derivative. Note that the second symmetric derivative may exist even when the (usual) second derivative does not.
The expression on the right can be written as a difference quotient of difference quotients:
This limit can be viewed as a continuous version of the second difference for sequences.
Please note that the existence of the above limit does not mean that the function formula_22 has a second derivative. The limit above just gives a possibility for calculating the second derivative but does not provide a definition. As a counterexample look on the sign function formula_23 which is defined through
The sign function is not continuous at zero and therefore the second derivative for formula_25 does not exist. But the above limit exists for formula_25:
Quadratic approximation.
Just as the first derivative is related to linear approximations, the second derivative is related to the best quadratic approximation for a function "f". This is the quadratic function whose first and second derivatives are the same as those of "f" at a given point. The formula for the best quadratic approximation to a function "f" around the point "x" = "a" is
This quadratic approximation is the second-order Taylor polynomial for the function centered at "x" = "a".
Eigenvalues and eigenvectors of the second derivative.
For many combinations of boundary conditions explicit formulas for eigenvalues and eigenvectors of the second derivative can be obtained. For example, assuming formula_29 and homogeneous Dirichlet boundary conditions, i.e., formula_30, the eigenvalues are formula_31 and the corresponding eigenvectors (also called eigenfunctions) are formula_32. Here, formula_33
For other well-known cases, see the main article eigenvalues and eigenvectors of the second derivative.
Generalization to higher dimensions.
The Hessian.
The second derivative generalizes to higher dimensions through the notion of second partial derivatives. For a function "f":R3 → R, these include the three second-order partials
and the mixed partials
If the function's image and domain both have a potential, then these fit together into a symmetric matrix known as the Hessian. The eigenvalues of this matrix can be used to implement a multivariable analogue of the second derivative test. (See also the second partial derivative test.)
The Laplacian.
Another common generalization of the second derivative is the Laplacian. This is the differential operator formula_36 defined by
The Laplacian of a function is equal to the divergence of the gradient.

</doc>
<doc id="133017" url="https://en.wikipedia.org/wiki?curid=133017" title="Second law of thermodynamics">
Second law of thermodynamics

The second law of thermodynamics states that in every real process the sum of the entropies of all participating bodies is increased. In the idealized limiting case of a reversible process, this sum remains unchanged. The increase in entropy accounts for the irreversibility of natural processes, and the asymmetry between future and past.
While often applied to more general processes, the law technically pertains to an event in which bodies initially in thermodynamic equilibrium are put into contact and allowed to come to a new equilibrium. This equilibration process involves the spread, dispersal, or dissipation of matter or energy and results in an increase of entropy. 
The second law is an empirical finding that has been accepted as an axiom of thermodynamic theory. Statistical thermodynamics, classical or quantum, explains the microscopic origin of the law.
The second law has been expressed in many ways. Its first formulation is credited to the French scientist Sadi Carnot in 1824 (see Timeline of thermodynamics).
Introduction.
Intuitive meaning of the law.
The second law is about thermodynamic systems or bodies of matter and radiation, initially each in its own state of internal thermodynamic equilibrium, and separated from one another by walls that partly or wholly allow or prevent the passage of matter and energy between them.
The law envisages that the walls are changed by some external agency, making them less restrictive or constraining and more permeable in various ways. Thereby a process is defined, establishing new equilibrium states.
The process invariably spreads, disperses and dissipates matter or energy, or both, amongst the bodies. This is quantitatively described by increase of entropy. It is the consequence of decrease of constraint by a wall. An increase of constraint by a wall has no effect on an established thermodynamic equilibrium.
For an example of the spreading of matter, one may consider a gas initially confined by an impermeable wall to one of two compartments of an isolated system. The wall is then removed. The gas spreads throughout both compartments. The sum of the entropies of the two compartments increases. Reinsertion of the impermeable wall does not change the spread of the gas between the compartments. For an example of the spreading of energy, one may consider a wall impermeable to matter and energy initially separating two otherwise isolated bodies at different temperatures. A thermodynamic operation makes the wall become permeable only to heat, which then passes from the hotter to the colder body, until their temperatures become equal. The sum of the entropies of the two bodies increases. Restoration of the complete impermeability of the wall does not change the equality of the temperatures.
Though it is more or less intuitive to imagine 'spreading', such loose intuition is, for many thermodynamic processes, too vague or imprecise to be usefully quantitatively informative, because competing possibilities of spreading can coexist, for example due to an increase of some constraint combined with decrease of another. The second law justifies the concept of entropy, which makes the notion of 'spreading' suitably precise, allowing quantitative predictions of just how spreading will occur in particular circumstances. It is characteristic of the physical quantity entropy that it refers to states of thermodynamic equilibrium.
General significance of the law.
The first law of thermodynamics provides the basic definition of thermodynamic energy, also called internal energy, associated with all thermodynamic systems, but unknown in classical mechanics, and states the rule of conservation of energy in nature.
The concept of energy in the first law does not, however, account for the observation that natural processes have a preferred direction of progress. The first law is symmetrical with respect to the initial and final states of an evolving system. But the second law asserts that a natural process runs only in one sense, and is not reversible. For example, heat always flows spontaneously from hotter to colder bodies, and never the reverse, unless external work is performed on the system. The key concept for the explanation of this phenomenon through the second law of thermodynamics is the definition of a new physical quantity, the entropy.
For mathematical analysis of processes, entropy is introduced as follows. In a fictive reversible process, an infinitesimal increment in the entropy () of a system results from an infinitesimal transfer of heat () to a closed system divided by the common temperature () of the system and the surroundings which supply the heat.
The zeroth law of thermodynamics in its usual short statement allows recognition that two bodies in a relation of thermal equilibrium have the same temperature, especially that a test body has the same temperature as a reference thermometric body. For a body in thermal equilibrium with another, there are indefinitely many empirical temperature scales, in general respectively depending on the properties of a particular reference thermometric body. The second law allows a distinguished temperature scale, which defines an absolute, thermodynamic temperature, independent of the properties of any particular reference thermometric body.
Various statements of the law.
The second law of thermodynamics may be expressed in many specific ways, the most prominent classical statements being the statement by Rudolf Clausius (1854), the statement by Lord Kelvin (1851), and the statement in axiomatic thermodynamics by Constantin Carathéodory (1909). These statements cast the law in general physical terms citing the impossibility of certain processes. The Clausius and the Kelvin statements have been shown to be equivalent.
Carnot's principle.
The historical origin of the second law of thermodynamics was in Carnot's principle. It refers to a cycle of a Carnot engine, fictively operated in the limiting mode of extreme slowness known as quasi-static, so that the heat and work transfers are between subsystems that are always in their own internal states of thermodynamic equilibrium. The Carnot engine is an idealized device of special interest to engineers who are concerned with the efficiency of heat engines. Carnot's principle was recognized by Carnot at a time when the caloric theory of heat was seriously considered, before the recognition of the first law of thermodynamics, and before the mathematical expression of the concept of entropy. Interpreted in the light of the first law, it is physically equivalent to the second law of thermodynamics, and remains valid today. It states
The efficiency of a quasi-static or reversible Carnot cycle depends only on the temperatures of the two heat reservoirs, and is the same, whatever the working substance. A Carnot engine operated in this way is the most efficient possible heat engine using those two temperatures. 
Clausius statement.
The German scientist Rudolf Clausius laid the foundation for the second law of thermodynamics in 1850 by examining the relation between heat transfer and work. His formulation of the second law, which was published in German in 1854, is known as the "Clausius statement":
Heat can never pass from a colder to a warmer body without some other change, connected therewith, occurring at the same time.
The statement by Clausius uses the concept of 'passage of heat'. As is usual in thermodynamic discussions, this means 'net transfer of energy as heat', and does not refer to contributory transfers one way and the other.
Heat cannot spontaneously flow from cold regions to hot regions without external work being performed on the system, which is evident from ordinary experience of refrigeration, for example. In a refrigerator, heat flows from cold to hot, but only when forced by an external agent, the refrigeration system.
Kelvin statement.
Lord Kelvin expressed the second law as
It is impossible, by means of inanimate material agency, to derive mechanical effect from any portion of matter by cooling it below the temperature of the coldest of the surrounding objects.
Equivalence of the Clausius and the Kelvin statements.
Suppose there is an engine violating the Kelvin statement: i.e., one that drains heat and converts it completely into work in a cyclic fashion without any other result. Now pair it with a reversed Carnot engine as shown by the figure. The net and sole effect of this newly created engine consisting of the two engines mentioned is transferring heat formula_2 from the cooler reservoir to the hotter one, which violates the Clausius statement. Thus a violation of the Kelvin statement implies a violation of the Clausius statement, i.e. the Clausius statement implies the Kelvin statement. We can prove in a similar manner that the Kelvin statement implies the Clausius statement, and hence the two are equivalent.
Planck's proposition.
Planck offered the following proposition as derived directly from experience. This is sometimes regarded as his statement of the second law, but he regarded it as a starting point for the derivation of the second law.
Relation between Kelvin's statement and Planck's proposition.
It is almost customary in textbooks to speak of the "Kelvin-Planck statement" of the law, as for example in the text by ter Haar and Wergeland. One text gives a statement that for all the world looks like Planck's proposition, but attributes it to Kelvin without mention of Planck. One monograph quotes Planck's proposition as the "Kelvin-Planck" formulation, the text naming Kelvin as its author, though it correctly cites Planck in its references. The reader may compare the two statements quoted just above here.
Planck's statement.
Planck stated the second law as follows.
Principle of Carathéodory.
Constantin Carathéodory formulated thermodynamics on a purely mathematical axiomatic foundation. His statement of the second law is known as the Principle of Carathéodory, which may be formulated as follows:
In every neighborhood of any state S of an adiabatically enclosed system there are states inaccessible from S."
With this formulation, he described the concept of adiabatic accessibility for the first time and provided the foundation for a new subfield of classical thermodynamics, often called geometrical thermodynamics. It follows from Carathéodory's principle that quantity of energy quasi-statically transferred as heat is a holonomic process function, in other words, formula_3. 
Though it is almost customary in textbooks to say that Carathéodory's principle expresses the second law and to treat it as equivalent to the Clausius or to the Kelvin-Planck statements, such is not the case. To get all the content of the second law, Carathéodory's principle needs to be supplemented by Planck's principle, that isochoric work always increases the internal energy of a closed system that was initially in its own internal thermodynamic equilibrium. 
Planck's Principle.
In 1926, Max Planck wrote an important paper on the basics of thermodynamics. He indicated the principle
This formulation does not mention heat and does not mention temperature, nor even entropy, and does not necessarily implicitly rely on those concepts, but it implies the content of the second law. A closely related statement is that "Frictional pressure never does positive work." Using a now-obsolete form of words, Planck himself wrote: "The production of heat by friction is irreversible."
Not mentioning entropy, this principle of Planck is stated in physical terms. It is very closely related to the Kelvin statement given just above. Nevertheless, this principle of Planck is not actually Planck's preferred statement of the second law, which is quoted above, in a previous sub-section of the present section of this present article, and relies on the concept of entropy.
The link to Kelvin's statement is illustrated by an equivalent statement by Allahverdyan & Nieuwenhuizen, which they attribute to Kelvin: "No work can be extracted from a closed equilibrium system during a cyclic variation of a parameter by an external source."
Statement for a system that has a known expression of its internal energy as a function of its extensive state variables.
The second law has been shown to be equivalent to the internal energy "U" being a weakly convex function, when written as a function of extensive properties (mass, volume, entropy, ...). 
Gravitational systems.
In non-gravitational systems, objects always have positive heat capacity, meaning that the temperature rises with energy. Therefore, when energy flows from a high-temperature object to a low-temperature object, the source temperature is decreased while the sink temperature is increased; hence temperature differences tend to diminish over time.
However, this is not always the case for systems in which the gravitational force is important and the general theory of relativity is required. (Apart from this, the thermodynamics of systems described by the general theory of relativity is beyond the scope of the present article.) The most striking examples are black holes, which – according to theory – have negative heat capacity. The larger the black hole, the more energy it contains, but the lower its temperature. Thus, the supermassive black hole in the center of the Milky Way is supposed to have a temperature of 10−14K, much lower than the cosmic microwave background temperature of 2.7K, but as it absorbs photons of the cosmic microwave background its mass is increasing so that its low temperature further decreases with time.
For this reason, gravitational systems tend towards non-even distribution of mass and energy. The universe in large scale is importantly a gravitational system, and the second law may therefore not apply to it.
Corollaries.
Perpetual motion of the second kind.
Before the establishment of the Second Law, many people who were interested in inventing a perpetual motion machine had tried to circumvent the restrictions of First Law of Thermodynamics by extracting the massive internal energy of the environment as the power of the machine. Such a machine is called a "perpetual motion machine of the second kind". The second law declared the impossibility of such machines.
Carnot theorem.
Carnot's theorem (1824) is a principle that limits the maximum efficiency for any possible engine. The efficiency solely depends on the temperature difference between the hot and cold thermal reservoirs. Carnot's theorem states:
In his ideal model, the heat of caloric converted into work could be reinstated by reversing the motion of the cycle, a concept subsequently known as thermodynamic reversibility. Carnot, however, further postulated that some caloric is lost, not being converted to mechanical work. Hence, no real heat engine could realise the Carnot cycle's reversibility and was condemned to be less efficient.
Though formulated in terms of caloric (see the obsolete caloric theory), rather than entropy, this was an early insight into the second law.
Clausius Inequality.
The Clausius Theorem (1854) states that in a cyclic process
The equality holds in the reversible case and the '<' is in the irreversible case. The reversible case is used to introduce the state function entropy. This is because in cyclic processes the variation of a state function is zero from state functionality.
Thermodynamic temperature.
For an arbitrary heat engine, the efficiency is:
where A is the work done per cycle. Thus the efficiency depends only on qC/qH.
Carnot's theorem states that all reversible engines operating between the same heat reservoirs are equally efficient.
Thus, any reversible heat engine operating between temperatures "T"1 and "T"2 must have the same efficiency, that is to say, the efficiency is the function of temperatures only: 
formula_6
In addition, a reversible heat engine operating between temperatures "T"1 and "T"3 must have the same efficiency as one consisting of two cycles, one between "T"1 and another (intermediate) temperature "T"2, and the second between "T"2 and"T"3. This can only be the case if
Now consider the case where formula_8 is a fixed reference temperature: the temperature of the triple point of water. Then for any "T"2 and "T"3,
Therefore, if thermodynamic temperature is defined by
then the function "f", viewed as a function of thermodynamic temperature, is simply
and the reference temperature "T"1 will have the value 273.16. (Of course any reference temperature and any positive numerical value could be used—the choice here corresponds to the Kelvin scale.)
Entropy.
According to the Clausius equality, for a reversible process
That means the line integral formula_13 is path independent.
So we can define a state function S called entropy, which satisfies
With this we can only obtain the difference of entropy by integrating the above formula. To obtain the absolute value, we need the Third Law of Thermodynamics, which states that S=0 at absolute zero for perfect crystals.
For any irreversible process, since entropy is a state function, we can always connect the initial and terminal states with an imaginary reversible process and integrating on that path to calculate the difference in entropy.
Now reverse the reversible process and combine it with the said irreversible process. Applying Clausius inequality on this loop,
Thus,
where the equality holds if the transformation is reversible.
Notice that if the process is an adiabatic process, then formula_17, so formula_18.
Energy, available useful work.
An important and revealing idealized special case is to consider applying the Second Law to the scenario of an isolated system (called the total system or universe), made up of two parts: a sub-system of interest, and the sub-system's surroundings. These surroundings are imagined to be so large that they can be considered as an "unlimited" heat reservoir at temperature "TR" and pressure "PR" — so that no matter how much heat is transferred to (or from) the sub-system, the temperature of the surroundings will remain "TR"; and no matter how much the volume of the sub-system expands (or contracts), the pressure of the surroundings will remain "PR".
Whatever changes to "dS" and "dSR" occur in the entropies of the sub-system and the surroundings individually, according to the Second Law the entropy "Stot" of the isolated total system must not decrease:
According to the First Law of Thermodynamics, the change "dU" in the internal energy of the sub-system is the sum of the heat "δq" added to the sub-system, "less" any work "δw" done "by" the sub-system, "plus" any net chemical energy entering the sub-system "d ∑μiRNi", so that:
where μiR are the chemical potentials of chemical species in the external surroundings.
Now the heat leaving the reservoir and entering the sub-system is
where we have first used the definition of entropy in classical thermodynamics (alternatively, in statistical thermodynamics, the relation between entropy change, temperature and absorbed heat can be derived); and then the Second Law inequality from above.
It therefore follows that any net work "δw" done by the sub-system must obey
It is useful to separate the work "δw" done by the subsystem into the "useful" work "δwu" that can be done "by" the sub-system, over and beyond the work "pR dV" done merely by the sub-system expanding against the surrounding external pressure, giving the following relation for the useful work (exergy) that can be done:
It is convenient to define the right-hand-side as the exact derivative of a thermodynamic potential, called the "availability" or "exergy" "E" of the subsystem,
The Second Law therefore implies that for any process which can be considered as divided simply into a subsystem, and an unlimited temperature and pressure reservoir with which it is in contact,
i.e. the change in the subsystem's exergy plus the useful work done "by" the subsystem (or, the change in the subsystem's exergy less any work, additional to that done by the pressure reservoir, done "on" the system) must be less than or equal to zero.
In sum, if a proper "infinite-reservoir-like" reference state is chosen as the system surroundings in the real world, then the Second Law predicts a decrease in "E" for an irreversible process and no change for a reversible process.
This expression together with the associated reference state permits a design engineer working at the macroscopic scale (above the thermodynamic limit) to utilize the Second Law without directly measuring or considering entropy change in a total isolated system. ("Also, see process engineer"). Those changes have already been considered by the assumption that the system under consideration can reach equilibrium with the reference state without altering the reference state. An efficiency for a process or collection of processes that compares it to the reversible ideal may also be found ("See second law efficiency".)
This approach to the Second Law is widely utilized in engineering practice, environmental accounting, systems ecology, and other disciplines.
History.
The first theory of the conversion of heat into mechanical work is due to Nicolas Léonard Sadi Carnot in 1824. He was the first to realize correctly that the efficiency of this conversion depends on the difference of temperature between an engine and its environment.
Recognizing the significance of James Prescott Joule's work on the conservation of energy, Rudolf Clausius was the first to formulate the second law during 1850, in this form: heat does not flow "spontaneously" from cold to hot bodies. While common knowledge now, this was contrary to the caloric theory of heat popular at the time, which considered heat as a fluid. From there he was able to infer the principle of Sadi Carnot and the definition of entropy (1865).
Established during the 19th century, the Kelvin-Planck statement of the Second Law says, "It is impossible for any device that operates on a cycle to receive heat from a single reservoir and produce a net amount of work." This was shown to be equivalent to the statement of Clausius.
The ergodic hypothesis is also important for the Boltzmann approach. It says that, over long periods of time, the time spent in some region of the phase space of microstates with the same energy is proportional to the volume of this region, i.e. that all accessible microstates are equally probable over a long period of time. Equivalently, it says that time average and average over the statistical ensemble are the same.
It has been shown that not only classical systems but also quantum mechanical ones tend to maximize their entropy over time. Thus the second law follows, given initial conditions with low entropy. More precisely, it has been shown that the local von Neumann entropy is at its maximum value with a very high probability. The result is valid for a large class of isolated quantum systems (e.g. a gas in a container). While the full system is pure and therefore does not have any entropy, the entanglement between gas and container gives rise to an increase of the local entropy of the gas. This result is one of the most important achievements of quantum thermodynamics.
Today, much effort in the field is attempting to understand why the initial conditions early in the universe were those of low entropy, as this is seen as the origin of the second law (see below).
Informal descriptions.
The second law can be stated in various succinct ways, including:
Mathematical descriptions.
In 1856, the German physicist Rudolf Clausius stated what he called the "second fundamental theorem in the mechanical theory of heat" in the following form:
where "Q" is heat, "T" is temperature and "N" is the "equivalence-value" of all uncompensated transformations involved in a cyclical process. Later, in 1865, Clausius would come to define "equivalence-value" as entropy. On the heels of this definition, that same year, the most famous version of the second law was read in a presentation at the Philosophical Society of Zurich on April 24, in which, in the end of his presentation, Clausius concludes:
The entropy of the universe tends to a maximum.
This statement is the best-known phrasing of the second law. Because of the looseness of its language, e.g. universe, as well as lack of specific conditions, e.g. open, closed, or isolated, many people take this simple statement to mean that the second law of thermodynamics applies virtually to every subject imaginable. This, of course, is not true; this statement is only a simplified version of a more extended and precise description.
In terms of time variation, the mathematical statement of the second law for an isolated system undergoing an arbitrary transformation is:
where
The equality sign holds in the case that only reversible processes take place inside the system. If irreversible processes take place (which is the case in real systems in operation) the >-sign holds. An alternative way of formulating of the second law for isolated systems is:
with formula_32 the sum of the rate of entropy production by all processes inside the system. The advantage of this formulation is that it shows the effect of the entropy production. The rate of entropy production is a very important concept since it determines (limits) the efficiency of thermal machines. Multiplied with ambient temperature formula_33 it gives the so-called dissipated energy formula_34.
The expression of the second law for closed systems (so, allowing heat exchange and moving boundaries, but not exchange of matter) is:
Here
If heat is supplied to the system at several places we have to take the algebraic sum of the corresponding terms.
For open systems (also allowing exchange of matter):
Here formula_41 is the flow of entropy into the system associated with the flow of matter entering the system. It should not be confused with the time derivative of the entropy. If matter is supplied at several places we have to take the algebraic sum of these contributions.
Statistical mechanics gives an explanation for the second law by postulating that a material is composed of atoms and molecules which are in constant motion. A particular set of positions and velocities for each particle in the system is called a microstate of the system and because of the constant motion, the system is constantly changing its microstate. Statistical mechanics postulates that, in equilibrium, each microstate that the system might be in is equally likely to occur, and when this assumption is made, it leads directly to the conclusion that the second law must hold in a statistical sense. That is, the second law will hold on average, with a statistical variation on the order of 1/√N where "N" is the number of particles in the system. For everyday (macroscopic) situations, the probability that the second law will be violated is practically zero. However, for systems with a small number of particles, thermodynamic parameters, including the entropy, may show significant statistical deviations from that predicted by the second law. Classical thermodynamic theory does not deal with these statistical variations.
Derivation from statistical mechanics.
Due to Loschmidt's paradox, derivations of the Second Law have to make an assumption regarding the past, namely that the system is uncorrelated at some time in the past; this allows for simple probabilistic treatment. This assumption is usually thought as a boundary condition, and thus the second Law is ultimately a consequence of the initial conditions somewhere in the past, probably at the beginning of the universe (the Big Bang), though other scenarios have also been suggested.
Given these assumptions, in statistical mechanics, the Second Law is not a postulate, rather it is a consequence of the fundamental postulate, also known as the equal prior probability postulate, so long as one is clear that simple probability arguments are applied only to the future, while for the past there are auxiliary sources of information which tell us that it was low entropy. The first part of the second law, which states that the entropy of a thermally isolated system can only increase, is a trivial consequence of the equal prior probability postulate, if we restrict the notion of the entropy to systems in thermal equilibrium. The entropy of an isolated system in thermal equilibrium containing an amount of energy of formula_42 is:
where formula_44 is the number of quantum states in a small interval between formula_42 and formula_46. Here formula_47 is a macroscopically small energy interval that is kept fixed. Strictly speaking this means that the entropy depends on the choice of formula_47. However, in the thermodynamic limit (i.e. in the limit of infinitely large system size), the specific entropy (entropy per unit volume or per unit mass) does not depend on formula_47.
Suppose we have an isolated system whose macroscopic state is specified by a number of variables. These macroscopic variables can, e.g., refer to the total volume, the positions of pistons in the system, etc. Then formula_50 will depend on the values of these variables. If a variable is not fixed, (e.g. we do not clamp a piston in a certain position), then because all the accessible states are equally likely in equilibrium, the free variable in equilibrium will be such that formula_50 is maximized as that is the most probable situation in equilibrium.
If the variable was initially fixed to some value then upon release and when the new equilibrium has been reached, the fact the variable will adjust itself so that formula_50 is maximized, implies that the entropy will have increased or it will have stayed the same (if the value at which the variable was fixed happened to be the equilibrium value).
Suppose we start from an equilibrium situation and we suddenly remove a constraint on a variable. Then right after we do this, there are a number formula_50 of accessible microstates, but equilibrium has not yet been reached, so the actual probabilities of the system being in some accessible state are not yet equal to the prior probability of formula_54. We have already seen that in the final equilibrium state, the entropy will have increased or have stayed the same relative to the previous equilibrium state. Boltzmann's H-theorem, however, proves that the quantity increases monotonically as a function of time during the intermediate out of equilibrium state.
Derivation of the entropy change for reversible processes.
The second part of the Second Law states that the entropy change of a system undergoing a reversible process is given by:
where the temperature is defined as:
See here for the justification for this definition. Suppose that the system has some external parameter, x, that can be changed. In general, the energy eigenstates of the system will depend on x. According to the adiabatic theorem of quantum mechanics, in the limit of an infinitely slow change of the system's Hamiltonian, the system will stay in the same energy eigenstate and thus change its energy according to the change in energy of the energy eigenstate it is in.
The generalized force, X, corresponding to the external variable x is defined such that formula_57 is the work performed by the system if x is increased by an amount dx. E.g., if x is the volume, then X is the pressure. The generalized force for a system known to be in energy eigenstate formula_58 is given by:
Since the system can be in any energy eigenstate within an interval of formula_47, we define the generalized force for the system as the expectation value of the above expression:
To evaluate the average, we partition the formula_44 energy eigenstates by counting how many of them have a value for formula_63 within a range between formula_64 and formula_65. Calling this number formula_66, we have:
The average defining the generalized force can now be written:
We can relate this to the derivative of the entropy with respect to x at constant energy E as follows. Suppose we change x to x + dx. Then formula_44 will change because the energy eigenstates depend on x, causing energy eigenstates to move into or out of the range between formula_42 and formula_71. Let's focus again on the energy eigenstates for which formula_63 lies within the range between formula_64 and formula_65. Since these energy eigenstates increase in energy by Y dx, all such energy eigenstates that are in the interval ranging from E – Y dx to E move from below E to above E. There are
such energy eigenstates. If formula_76, all these energy eigenstates will move into the range between formula_42 and formula_71 and contribute to an increase in formula_50. The number of energy eigenstates that move from below formula_71 to above formula_71 is, of course, given by formula_82. The difference
is thus the net contribution to the increase in formula_50. Note that if Y dx is larger than formula_47 there will be the energy eigenstates that move from below E to above formula_71. They are counted in both formula_87 and formula_82, therefore the above expression is also valid in that case.
Expressing the above expression as a derivative with respect to E and summing over Y yields the expression:
The logarithmic derivative of formula_50 with respect to x is thus given by:
The first term is intensive, i.e. it does not scale with system size. In contrast, the last term scales as the inverse system size and will thus vanishes in the thermodynamic limit. We have thus found that:
Combining this with
Gives:
Derivation for systems described by the canonical ensemble.
If a system is in thermal contact with a heat bath at some temperature T then, in equilibrium, the probability distribution over the energy eigenvalues are given by the canonical ensemble:
Here Z is a factor that normalizes the sum of all the probabilities to 1, this function is known as the partition function. We now consider an infinitesimal reversible change in the temperature and in the external parameters on which the energy levels depend. It follows from the general formula for the entropy:
that
Inserting the formula for formula_98 for the canonical ensemble in here gives:
Non-equilibrium states.
The theory of classical or equilibrium thermodynamics is idealized. A main postulate or assumption, often not even explicitly stated, is the existence of systems in their own internal states of thermodynamic equilibrium. In general, a region of space containing a physical system at a given time, that may be found in nature, is not in thermodynamic equilibrium, read in the most stringent terms. In looser terms, nothing in the entire universe is or has ever been truly in exact thermodynamic equilibrium.
For purposes of physical analysis, it is often enough convenient to make an assumption of thermodynamic equilibrium. Such an assumption may rely on trial and error for its justification. If the assumption is justified, it can often be very valuable and useful because it makes available the theory of thermodynamics. Elements of the equilibrium assumption are that a system is observed to be unchanging over an indefinitely long time, and that there are so many particles in a system, that its particulate nature can be entirely ignored. Under such an equilibrium assumption, in general, there are no macroscopically detectable fluctuations. There is an exception, the case of critical states, which exhibit to the naked eye the phenomenon of critical opalescence. For laboratory studies of critical states, exceptionally long observation times are needed.
In all cases, the assumption of thermodynamic equilibrium, once made, implies as a consequence that no putative candidate "fluctuation" alters the entropy of the system.
It can easily happen that a physical system exhibits internal macroscopic changes that are fast enough to invalidate the assumption of the constancy of the entropy. Or that a physical system has so few particles that the particulate nature is manifest in observable fluctuations. Then the assumption of thermodynamic equilibrium is to be abandoned. There is no unqualified general definition of entropy for non-equilibrium states.
Non-equilibrium thermodynamics is then appropriate. There are intermediate cases, in which the assumption of local thermodynamic equilibrium is a very good approximation, but strictly speaking it is still an approximation, not theoretically ideal. For non-equilibrium situations in general, it may be useful to consider statistical mechanical definitions of quantities that may be conveniently called 'entropy'. These indeed belong to statistical mechanics, not to macroscopic thermodynamics.
The physics of macroscopically observable fluctuations is beyond the scope of this article.
Arrow of time.
The second law of thermodynamics is a physical law that is not symmetric to reversal of the time direction.
The second law has been proposed to supply an explanation of the difference between moving forward and backwards in time, such as why the cause precedes the effect (the causal arrow of time).
Controversies.
Maxwell's demon.
James Clerk Maxwell imagined one container divided into two parts, "A" and "B". Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from "A" flies towards the trapdoor, the demon opens it, and the molecule will fly from "A" to "B". The average speed of the molecules in "B" will have increased while in "A" they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in "A" and increases in "B", contrary to the second law of thermodynamics.
One of the most famous responses to this question was suggested in 1929 by Leó Szilárd and later by Léon Brillouin. Szilárd pointed out that a real-life Maxwell's demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy.
Maxwell's demon repeatedly alters the permeability of the wall between "A" and "B". It is therefore performing thermodynamic operations, not just presiding over natural processes.
Loschmidt's paradox.
Loschmidt's paradox, also known as the reversibility paradox, is the objection that it should not be possible to deduce an irreversible process from time-symmetric dynamics. This puts the time reversal symmetry of nearly all known low-level fundamental physical processes at odds with any attempt to infer from them the second law of thermodynamics which describes the behavior of macroscopic systems. Both of these are well-accepted principles in physics, with sound observational and theoretical support, yet they seem to be in conflict; hence the paradox.
One proposed resolution of this paradox is as follows. The Loschmidt scenario refers to a strictly isolated system or to a strictly adiabatically isolated system. Heat and matter transfers are not allowed. The Loschmidt reversal times are fantastically long, far longer than any laboratory isolation of the required degree of perfection could be maintained in practice. In this sense, the Loschmidt scenario will never be subjected to empirical testing. Also in this sense, the second law, stated for an isolated system, will never be subjected to empirical testing. A system, supposedly perfectly isolated, in strictly perfect thermodynamic equilibrium, can be observed only once in its entire life, because the observation must break the isolation. Two observations would be needed to check empirically for a change of state, one initial and one final. When transfer of heat or matter are permitted, the requirements of perfection are not so tight. In practical laboratory reality, therefore, the second law can be tested only for systems with transfer of heat or matter, and not for isolated systems.
Due to this paradox, derivations of the second law have to make an assumption regarding the past, namely that the system is uncorrelated at some time in the past or, equivalently, that the entropy in the past was lower than in the future. This assumption is usually thought as a boundary condition, and thus the second Law is ultimately derived from the initial conditions of the Big Bang.
Poincaré recurrence theorem.
The Poincaré recurrence theorem states that certain systems will, after a sufficiently long time, return to a state very close to the initial state. The Poincaré recurrence time is the length of time elapsed until the recurrence, which is of the order of formula_100. The result applies to physical systems in which energy is conserved. The Recurrence theorem apparently contradicts the Second law of thermodynamics, which says that large dynamical systems evolve irreversibly towards the state with higher entropy, so that if one starts with a low-entropy state, the system will never return to it. There are many possible ways to resolve this paradox, but none of them is universally accepted. The most reasonable argument is that for typical thermodynamical systems the recurrence time is so large (many many times longer than the lifetime of the universe) that, for all practical purposes, one cannot observe the recurrence.
A simple example of this can be demonstrated by a thought experiment. Consider two connected spherical chambers containing four molecules of gas, initially with chamber A containing four molecules of a gas and chamber B containing nothing. The gas molecules bounce freely from one chamber to another, and according to the second law of thermodynamics, will tend to be distributed such that they are divided evenly between the chambers, with two molecules in chamber A and two molecules in chamber B, corresponding to an increase in entropy. However, assuming the conservation of energy within the chambers, it is apparent that the molecules will occasionally bounce such that all four will be in one chamber at the same time, corresponding to a spontaneous increase in order and decrease in entropy relative to the evenly distributed state.

</doc>
<doc id="285618" url="https://en.wikipedia.org/wiki?curid=285618" title="Sectional curvature">
Sectional curvature

In Riemannian geometry, the sectional curvature is one of the ways to describe the curvature of Riemannian manifolds. The sectional curvature "K"(σ"p") depends on a two-dimensional plane σ"p" in the tangent space at "p". It is the Gaussian curvature of the surface which has the plane σ"p" as a tangent plane at "p", obtained from geodesics which start at "p" in the directions of σ"p" (in other words, the image of σ"p" under the exponential map at "p"). The sectional curvature is a smooth real-valued function on the 2-Grassmannian bundle over the manifold.
The sectional curvature determines the curvature tensor completely.
Definition.
Given a Riemannian manifold and two linearly independent tangent vectors at the same point, "u" and "v", we can define
Here "R" is the Riemann curvature tensor.
In particular, if "u" and "v" are orthonormal, then
The sectional curvature in fact depends only on the 2-plane σ"p" in the tangent space at "p" spanned by "u" and "v". It is called the sectional curvature of the 2-plane σ"p", and is denoted "K"(σ"p").
Manifolds with constant sectional curvature.
Riemannian manifolds with constant sectional curvature are the simplest. These are called space forms. By rescaling the metric there are three possible cases
The model manifolds for the three geometries are hyperbolic space, Euclidean space and a unit sphere. They are the only connected, complete, simply connected Riemannian manifolds of given sectional curvature. All other connected complete constant curvature manifolds are quotients of those by some group of isometries.
If for each point in a connected Riemannian manifold (of dimension three or greater) the sectional curvature is independent of the tangent 2-plane, then the sectional curvature is in fact constant on the whole manifold.
Toponogov's theorem.
Toponogov's theorem affords a characterization of sectional curvature in terms of how "fat" geodesic triangles appear when compared to their Euclidean counterparts. The basic intuition is that, if a space is positively curved, then the edge of a triangle opposite some given vertex will tend to bend away from that vertex, whereas if a space is negatively curved, then the opposite edge of the triangle will tend to bend towards the vertex.
More precisely, let "M" be a complete Riemannian manifold, and let "xyz" be a geodesic triangle in "M" (a triangle each of whose sides is a length-minimizing geodesic). Finally, let "m" be the midpoint of the geodesic "xy". If "M" has non-negative curvature, then for all sufficiently small triangles
where "d" is the distance function on "M". The case of equality holds precisely when the curvature of "M" vanishes, and the right-hand side represents the distance from a vertex to the opposite side of a geodesic triangle in Euclidean space having the same side-lengths as the triangle "xyz". This makes precise the sense in which triangles are "fatter" in positively curved spaces. In non-positively curved spaces, the inequality goes the other way:
If tighter bounds on the sectional curvature are known, then this property generalizes to give a comparison theorem between geodesic triangles in "M" and those in a suitable simply connected space form; see Toponogov's theorem. Simple consequences of the version stated here are:
Manifolds with non-positive sectional curvature.
In 1928, Élie Cartan proved the Cartan–Hadamard theorem: if "M" is a complete manifold with non-positive sectional curvature, then its universal cover is diffeomorphic to a Euclidean space. In particular, it is aspherical: the homotopy groups formula_7 for "i" ≥ 2 are trivial. Therefore, the topological structure of a complete non-positively curved manifold is determined by its fundamental group. Preissman's theorem restricts the fundamental group of negatively curved compact manifolds.
Manifolds with positive sectional curvature.
Little is known about the structure of positively curved manifolds. The soul theorem (; ) implies that a complete non-compact non-negatively curved manifold is diffeomorphic to a normal bundle over a compact non-negatively curved manifold. As for compact positively curved manifolds, there are two classical results:
Moreover, there are relatively few examples of compact positively curved manifolds, leaving a lot of conjectures (e.g., the Hopf conjecture on whether there is a metric of positive sectional curvature on formula_9). The most typical way of constructing new examples is the following corollary from the O'Neill curvature formulas: if formula_10 is a Riemannian manifold admitting a free isometric action of a Lie group G, and M has positive sectional curvature on all 2-planes orthogonal to the orbits of G, then the manifold formula_11 with the quotient metric has positive sectional curvature. This fact allows one to construct the classical positively curved spaces, being spheres and projective spaces, as well as these examples :

</doc>
<doc id="1931736" url="https://en.wikipedia.org/wiki?curid=1931736" title="Self-energy">
Self-energy

In most theoretical physics such as quantum field theory, a particle's self-energy formula_1 represents the contribution to the particle's energy, or effective mass, due to interactions between the particle and the system it is part of. For example, in electrostatics the self-energy of a given charge distribution is the energy required to assemble the distribution by bringing in the constituent charges from infinity, where the electric force goes to zero. In a condensed matter context relevant to electrons moving in a material, the self-energy represents the potential felt by the electron due to the surrounding medium's interactions with it: for example, the fact that electrons repel each other means that a moving electron polarizes (causes to displace) the electrons in its vicinity and this in turn changes the potential the moving electron feels; these and other effects are included in the self-energy. In basic terms, the self-energy is the energy that a particle has as a result of changes that it itself causes in its environment.
Mathematically, this energy is equal to the so-called on-the-mass-shell value of the proper self-energy "operator" (or proper mass "operator") in the momentum-energy representation (more precisely, to formula_2 times this value). In this, or other representations (such as the space-time representation), the self-energy is pictorially (and economically) represented by means of Feynman diagrams, such as the one shown below. In this particular diagram, the three arrowed straight lines represent particles, or particle propagators, and the wavy line a particle-particle interaction; removing (or "amputating") the left-most and the right-most straight lines in the diagram shown below (these so-called "external" lines correspond to prescribed values for, for instance, momentum and energy, or four-momentum), one retains a contribution to the self-energy operator (in, for instance, the momentum-energy representation). Using a small number of simple rules, each Feynman diagram can be readily expressed in its corresponding algebraic form.
In general, the on-the-mass-shell value of the self-energy operator in the momentum-energy representation is complex. In such cases, it is the real part of this self-energy that is identified with the physical self-energy (referred to above as particle's self-energy); the inverse of the imaginary part is a measure for the lifetime of the particle under investigation. For clarity, elementary excitations, or dressed particles (see quasi-particle), in interacting systems are distinct from stable particles in vacuum; their state functions consist of complicated superpositions of the eigenstates of the underlying many-particle system, which only, if at all, momentarily behave like those specific to isolated particles; the above-mentioned lifetime is the time over which a dressed particle behaves as if it were a single particle with well-defined momentum and energy.
The self-energy operator (often denoted by formula_3, and less frequently by formula_4) is related to the bare and dressed propagators (often denoted by formula_5 and formula_6 respectively) via the Dyson equation (named after Freeman John Dyson):
Multiplying on the left by the inverse formula_8 of the operator formula_9
and on the right by formula_10 yields
The photon and gluon do not get a mass through renormalization because gauge symmetry protects them from getting a mass. This is a consequence of the Ward identity. The W-boson and the Z-boson get their masses through the Higgs mechanism; they do undergo mass renormalization through the renormalization of the electroweak theory.
Neutral particles with internal quantum numbers can mix with each other through virtual pair production. The primary example of this phenomenon is the mixing of neutral kaons. Under appropriate simplifying assumptions this can be described without quantum field theory.
In chemistry, the self-energy or "Born energy" of an ion is the energy associated with the field of the ion itself.
In solid state and condensed-matter physics self-energies and a myriad related quasiparticle properties are calculated by Green's function methods and Green's function (many-body theory) of interacting low-energy excitations on the basis of electronic band structure calculations.

</doc>
<doc id="1337587" url="https://en.wikipedia.org/wiki?curid=1337587" title="Semisimple Lie algebra">
Semisimple Lie algebra

In mathematics, a Lie algebra is semisimple if it is a direct sum of simple Lie algebras, i.e., non-abelian Lie algebras formula_1 whose only ideals are {0} and formula_1 itself.
Throughout the article, unless otherwise stated, formula_1 is a finite-dimensional Lie algebra over a field of characteristic 0. The following conditions are equivalent:
Examples.
Examples of semisimple Lie algebras, with notation coming from classification by Dynkin diagrams, are:
These Lie algebras are numbered so that "n" is the rank. Except certain exceptions in low dimensions, many of these are simple Lie algebras, which are "a fortiori" semisimple. These four families, together with five exceptions (E6, E7, E8, F4, and G2), are in fact the "only" simple Lie algebras over the complex numbers.
Classification.
Every semisimple Lie algebra over an algebraically closed field is a direct sum of simple Lie algebras (by definition), and the finite-dimensional simple Lie algebras fall in four families – An, Bn, Cn, and Dn – with five exceptions
E6, E7, E8, F4, and G2. Simple Lie algebras are classified by the connected Dynkin diagrams, shown on the right, while semisimple Lie algebras correspond to not necessarily connected Dynkin diagrams, where each component of the diagram corresponds to a summand of the decomposition of the semisimple Lie algebra into simple Lie algebras.
The classification proceeds by considering a Cartan subalgebra (maximal abelian Lie algebra; corresponds to a maximal torus in a Lie group) and the adjoint action of the Lie algebra on this subalgebra. The root system of the action then both determines the original Lie algebra and must have a very constrained form, which can be classified by the Dynkin diagrams.
The classification is widely considered one of the most elegant results in mathematics – a brief list of axioms yields, via a relatively short proof, a complete but non-trivial classification with surprising structure. This should be compared to the classification of finite simple groups, which is significantly more complicated.
The enumeration of the four families is non-redundant and consists only of simple algebras if formula_16 for An, formula_17 for Bn, formula_18 for Cn, and formula_19 for Dn. If one starts numbering lower, the enumeration is redundant, and one has exceptional isomorphisms between simple Lie algebras, which are reflected in isomorphisms of Dynkin diagrams; the En can also be extended down, but below E6 are isomorphic to other, non-exceptional algebras.
Over a non-algebraically closed field, the classification is more complicated – one classifies simple Lie algebras over the algebraic closure, then for each of these, one classifies simple Lie algebras over the original field which have this form (over the closure). For example, to classify simple real Lie algebras, one classifies real Lie algebras with a given complexification, which are known as real forms of the complex Lie algebra; this can be done by Satake diagrams, which are Dynkin diagrams with additional data ("decorations").
History.
The semisimple Lie algebras over the complex numbers were first classified by Wilhelm Killing (1888–90), though his proof lacked rigor. His proof was made rigorous by Élie Cartan (1894) in his Ph.D. thesis, who also classified semisimple real Lie algebras. This was subsequently refined, and the present classification by Dynkin diagrams was given by then 22-year-old Eugene Dynkin in 1947. Some minor modifications have been made (notably by J. P. Serre), but the proof is unchanged in its essentials and can be found in any standard reference, such as .
Properties.
Complete reducibility.
A consequence of semisimplicity is a theorem due to Weyl: every finite-dimensional representation is completely reducible; that is for every invariant subspace of the representation there is an invariant complement. Infinite-dimensional representations of semisimple Lie algebras are not in general completely reducible.
Centerless.
Since the center of a Lie algebra formula_1 is an abelian ideal, if formula_1 is semisimple, then its center is zero. (Note: since formula_22 has non-trivial center, it is not semisimple.) In other words, the adjoint representation formula_23 is injective. Moreover, it can be shown that the dimension of the Lie algebra formula_24 of derivations on formula_25 is equal to the dimension of formula_26. Hence, formula_25 is Lie algebra isomorphic to formula_24. (This is a special case of Whitehead's lemma.) Every ideal, quotient and product of semisimple Lie algebras is again semisimple.
Linear.
The adjoint representation is injective, and so a semisimple Lie algebra is also a linear Lie algebra under the adjoint representation. This may lead to some ambiguity, as every Lie algebra is already linear with respect to some other vector space (Ado's theorem), although not necessarily via the adjoint representation. But in practice, such ambiguity rarely occurs.
Jordan decomposition.
Any endomorphism "x" of a finite-dimensional vector space over an algebraically closed field can be decomposed uniquely into a diagonalizable (or semisimple) and nilpotent part
such that "s" and "n" commute with each other. Moreover, each of "s" and "n" is a polynomial in "x". This is a consequence of the Jordan decomposition.
If formula_30, then the image of "x" under the adjoint map decomposes as
The elements "s" and "n" are "unique" elements of formula_1 such that "n" is nilpotent, "s" is semisimple, "n" and "s" commute, and for which such a decomposition holds. This abstract Jordan decomposition factors through any representation of formula_1 in the sense that given any representation ρ,
is the Jordan decomposition of ρ("x") in the endomorphism ring of the representation space.
Rank.
The rank of a complex semisimple Lie algebra is the dimension of any of its Cartan subalgebras.
Significance.
The significance of semisimplicity comes firstly from the Levi decomposition, which states that every finite dimensional Lie algebra is the semidirect product of a solvable ideal (its radical) and a semisimple algebra. In particular, there is no nonzero Lie algebra that is both solvable and semisimple.
Semisimple Lie algebras have a very elegant classification, in stark contrast to solvable Lie algebras. Semisimple Lie algebras over an algebraically closed field are completely classified by their root system, which are in turn classified by Dynkin diagrams. Semisimple algebras over non-algebraically closed fields can be understood in terms of those over the algebraic closure, though the classification is somewhat more intricate; see real form for the case of real semisimple Lie algebras, which were classified by Élie Cartan.
Further, the representation theory of semisimple Lie algebras is much cleaner than that for general Lie algebras. For example, the Jordan decomposition in a semisimple Lie algebra coincides with the Jordan decomposition in its representation; this is not the case for Lie algebras in general.
If formula_1 is semisimple, then formula_36. In particular, every linear semisimple Lie algebra is a subalgebra of formula_37, the special linear Lie algebra. The study of the structure of formula_37 constitutes an important part of the representation theory for semisimple Lie algebras.
Generalizations.
Semisimple Lie algebras admit certain generalizations. Firstly, many statements that are true for semisimple Lie algebras are true more generally for reductive Lie algebras. Abstractly, a reductive Lie algebra is one whose adjoint representation is completely reducible, while concretely, a reductive Lie algebra is a direct sum of a semisimple Lie algebra and an abelian Lie algebra; for example, formula_39 is semisimple, and formula_22 is reductive. Many properties of semisimple Lie algebras depend only on reducibility.
Many properties of complex semisimple/reductive Lie algebras are true not only for semisimple/reductive Lie algebras over algebraically closed fields, but more generally for split semisimple/reductive Lie algebras over other fields: semisimple/reductive Lie algebras over algebraically closed fields are always split, but over other fields this is not always the case. Split Lie algebras have essentially the same representation theory as semsimple Lie algebras over algebraically closed fields, for instance, the splitting Cartan subalgebra playing the same role as the Cartan subalgebra plays over algebraically closed fields. This is the approach followed in , for instance, which classifies representations of split semisimple/reductive Lie algebras.

</doc>
<doc id="387750" url="https://en.wikipedia.org/wiki?curid=387750" title="Separation of variables">
Separation of variables

In mathematics, separation of variables (also known as the Fourier method) is any of several methods for solving ordinary and partial differential equations, in which algebra allows one to rewrite an equation so that each of two variables occurs on a different side of the equation.
Ordinary differential equations (ODE).
Suppose a differential equation can be written in the form
which we can write more simply by letting formula_2:
As long as "h"("y") ≠ 0, we can rearrange terms to obtain:
so that the two variables "x" and "y" have been separated. "dx" (and "dy") can be viewed, at a simple level, as just a convenient notation, which provides a handy mnemonic aid for assisting with manipulations. A formal definition of "dx" as a differential (infinitesimal) is somewhat advanced.
Alternative notation.
Some who dislike Leibniz's notation may prefer to write this as
but that fails to make it quite as obvious why this is called "separation of variables". Integrating both sides of the equation with respect to formula_6, we have
or equivalently,
because of the substitution rule for integrals.
If one can evaluate the two integrals, one can find a solution to the differential equation. Observe that this process effectively allows us to treat the derivative formula_9 as a fraction which can be separated. This allows us to solve separable differential equations more conveniently, as demonstrated in the example below.
(Note that we do not need to use two constants of integration, in equation (1) as in
because a single constant formula_11 is equivalent.)
Example.
Population growth is often modeled by the differential equation
where formula_13 is the population with respect to time formula_14, formula_15 is the rate of growth, and formula_16 is the carrying capacity of the environment.
Separation of variables may be used to solve this differential equation.
To evaluate the integral on the left side, we simplify the fraction
and then, we decompose the fraction into partial fractions
Thus we have
Therefore, the solution to the logistic equation is
To find formula_35, let formula_36 and formula_37. Then we have
Noting that formula_39, and solving for A we get
Partial differential equations.
The method of separation of variables is also used to solve a wide range of linear partial differential equations with boundary and initial conditions, such as heat equation, wave equation, Laplace equation and Helmholtz equation.
Homogeneous case.
Consider the one-dimensional heat equation.The equation is
The boundary condition is homogeneous, that is
Let us attempt to find a solution which is not identically zero satisfying the boundary conditions but with the following property: "u" is a product in which the dependence of "u" on "x", "t" is separated, that is:
Substituting "u" back into equation and using the product rule,
Since the right hand side depends only on "x" and the left hand side only on "t", both sides are equal to some constant value − λ. Thus:
and
− λ here is the eigenvalue for both differential operators, and "T(t)" and "X(x)" are corresponding eigenfunctions.
We will now show that solutions for "X(x)" for values of λ ≤ 0 cannot occur:
Suppose that λ < 0. Then there exist real numbers "B", "C" such that
From we get
and therefore "B" = 0 = "C" which implies "u" is identically 0.
Suppose that λ = 0. Then there exist real numbers "B", "C" such that
From we conclude in the same manner as in 1 that "u" is identically 0.
Therefore, it must be the case that λ > 0. Then there exist real numbers "A", "B", "C" such that
and
From we get "C" = 0 and that for some positive integer "n",
This solves the heat equation in the special case that the dependence of "u" has the special form of .
In general, the sum of solutions to which satisfy the boundary conditions also satisfies and . Hence a complete solution can be given as
where "D""n" are coefficients determined by initial condition.
Given the initial condition
we can get
This is the sine series expansion of "f(x)". Multiplying both sides with formula_49 and integrating over "[0,L]" result in
This method requires that the eigenfunctions of "x", here formula_51, are orthogonal and complete. In general this is guaranteed by Sturm-Liouville theory.
Nonhomogeneous case.
Suppose the equation is nonhomogeneous,
with the boundary condition the same as .
Expand "h(x,t)", "u(x,t)" and "f(x)" into
where "h""n"("t") and "b""n" can be calculated by integration, while "u""n"("t") is to be determined.
Substitute and back to and considering the orthogonality of sine functions we get
which are a sequence of linear differential equations that can be readily solved with, for instance, Laplace transform, or Integrating factor. Finally, we can get
If the boundary condition is nonhomogeneous, then the expansion of and is no longer valid. One has to find a function "v" that satisfies the boundary condition only, and subtract it from "u". The function "u-v" then satisfies homogeneous boundary condition, and can be solved with the above method.
In orthogonal curvilinear coordinates, separation of variables can still be used, but in some details different from that in Cartesian coordinates. For instance, regularity or periodic condition may determine the eigenvalues in place of boundary conditions. See spherical harmonics for example.
Matrices.
The matrix form of the separation of variables is the Kronecker sum.
As an example we consider the 2D discrete Laplacian on a regular grid:
where formula_55 and formula_56 are 1D discrete Laplacians in the "x"- and "y"-directions, correspondingly, and formula_57 are the identities of appropriate sizes. See the main article Kronecker sum of discrete Laplacians for details.

</doc>
<doc id="15287" url="https://en.wikipedia.org/wiki?curid=15287" title="Series (mathematics)">
Series (mathematics)

A series is, informally speaking, the sum of the terms of a sequence. Finite sequences and series have defined first and last terms, whereas infinite sequences and series continue indefinitely.
In mathematics, given an infinite sequence of numbers { "a""n" }, a series is informally the result of adding all those terms together: "a"1 + "a"2 + "a"3 + · · ·. These can be written more compactly using the summation symbol ∑. An example is the famous series from Zeno's dichotomy and its mathematical representation:
The terms of the series are often produced according to a rule, such as by a formula, or by an algorithm. As there are an infinite number of terms, this notion is often called an infinite series. Unlike finite summations, infinite series need tools from mathematical analysis, and specifically the notion of limits, to be fully understood and manipulated. In addition to their ubiquity in mathematics, infinite series are also widely used in other quantitative disciplines such as physics, computer science, and finance.
Basic properties.
Definition.
For any sequence formula_2 of rational numbers, real numbers, complex numbers, functions thereof, etc., the associated series is defined as the ordered formal sum
The sequence of partial sums formula_4 associated to a series formula_5 is defined for each formula_6 as the sum of the sequence formula_2 from formula_8 to formula_9
By definition the series formula_11 converges to a limit formula_12 if and only if the associated sequence of partial sums formula_4 converges to formula_12. This definition is usually written as
More generally, if formula_16 is a function from an index set I to a set G, then the series associated to formula_17 is the formal sum of the elements formula_18 over the index elements formula_19 denoted by the
When the index set is the natural numbers formula_21, the function formula_22 is a sequence denoted by formula_23. A series indexed on the natural numbers is an ordered formal sum and so we rewrite formula_24 as formula_25 in order to emphasize the ordering induced by the natural numbers. Thus, we obtain the common notation for a series indexed by the natural numbers
When the set formula_27 is a semigroup, the sequence of partial sums formula_28 associated to a sequence formula_29 is defined for each formula_6 as the sum of the terms formula_31
When the semigroup formula_27 is also a topological space, then the series formula_11 converges to an element formula_35 if and only if the associated sequence of partial sums formula_4 converges to formula_12. This definition is usually written as
Convergent series.
A series  ∑"an"  is said to 'converge' or to 'be convergent' when the sequence "S""N" of partial sums has a finite limit. If the limit of "S""N" is infinite or does not exist, the series is said to diverge. When the limit of partial sums exists, it is called the sum of the series
An easy way that an infinite series can converge is if all the "a""n" are zero for "n" sufficiently large. Such a series can be identified with a finite sum, so it is only infinite in a trivial sense.
Working out the properties of the series that converge even if infinitely many terms are non-zero is the essence of the study of series. Consider the example
It is possible to "visualize" its convergence on the real number line: we can imagine a line of length 2, with successive segments marked off of lengths 1, ½, ¼, etc. There is always room to mark the next segment, because the amount of line remaining is always the same as the last segment marked: when we have marked off ½, we still have a piece of length ½ unmarked, so we can certainly mark the next ¼. This argument does not prove that the sum is "equal" to 2 (although it is), but it does prove that it is "at most" 2. In other words, the series has an upper bound. Given that the series converges, proving that it is equal to 2 requires only elementary algebra. If the series is denoted "S", it can be seen that
Therefore,
Mathematicians extend the idiom discussed earlier to other, equivalent notions of series. For instance, when we talk about a recurring decimal, as in
we are talking, in fact, just about the series
But since these series always converge to real numbers (because of what is called the completeness property of the real numbers), to talk about the series in this way is the same as to talk about the numbers for which they stand. In particular, it should offend no sensibilities if we make no distinction between 0.111… and 1/9. Less clear is the argument that , but it is not untenable when we consider that we can formalize the proof knowing only that limit laws preserve the arithmetic operations. See 0.999... for more.
Calculus and partial summation as an operation on sequences.
Partial summation takes as input a sequence, { "a""n" }, and gives as output another sequence, { "S""N" }. It is thus a unary operation on sequences. Further, this function is linear, and thus is a linear operator on the vector space of sequences, denoted Σ. The inverse operator is the finite difference operator, Δ. These behave as discrete analogs of integration and differentiation, only for series (functions of a natural number) instead of functions of a real variable. For example, the sequence {1, 1, 1, ...} has series {1, 2, 3, 4, ...} as its partial summation, which is analogous to the fact that formula_58
In computer science it is known as prefix sum.
Properties of series.
Series are classified not only by whether they converge or diverge, but also by the properties of the terms an (absolute or conditional convergence); type of convergence of the series (pointwise, uniform); the class of the term an (whether it is a real number, arithmetic progression, trigonometric function); etc.
Non-negative terms.
When "an" is a non-negative real number for every "n", the sequence "SN" of partial sums is non-decreasing. It follows that a series ∑"an" with non-negative terms converges if and only if the sequence "SN" of partial sums is bounded.
For example, the series
is convergent, because the inequality
and a telescopic sum argument implies that the partial sums are bounded by 2.
Absolute convergence.
A series
is said to converge absolutely if the series of absolute values
converges. This is sufficient to guarantee not only that the original series converges to a limit, but also that any reordering of it converges to the same limit.
Conditional convergence.
A series of real or complex numbers is said to be conditionally convergent (or semi-convergent) if it is convergent but not absolutely convergent. A famous example is the alternating series
which is convergent (and its sum is equal to ln 2), but the series formed by taking the absolute value of each term is the divergent harmonic series. The Riemann series theorem says that any conditionally convergent series can be reordered to make a divergent series, and moreover, if the "a""n" are real and "S" is any real number, that one can find a reordering so that the reordered series converges with sum equal to "S".
Abel's test is an important tool for handling semi-convergent series. If a series has the form
where the partial sums "B""N" = are bounded, "λ""n" has bounded variation, and exists:
then the series is convergent. This applies to the pointwise convergence of many trigonometric series, as in
with 0 < "x" < 2π. Abel's method consists in writing "b""n"+1 = "B""n"+1 − "B""n", and in performing a transformation similar to integration by parts (called summation by parts), that relates the given series to the absolutely convergent series
Series of functions.
A series of real- or complex-valued functions
converges pointwise on a set "E", if the series converges for each "x" in "E" as an ordinary series of real or complex numbers. Equivalently, the partial sums
converge to "ƒ"("x") as "N" → ∞ for each "x" ∈ "E".
A stronger notion of convergence of a series of functions is called uniform convergence. The series converges uniformly if it converges pointwise to the function "ƒ"("x"), and the error in approximating the limit by the "N"th partial sum,
can be made minimal "independently" of "x" by choosing a sufficiently large "N".
Uniform convergence is desirable for a series because many properties of the terms of the series are then retained by the limit. For example, if a series of continuous functions converges uniformly, then the limit function is also continuous. Similarly, if the "ƒ""n" are integrable on a closed and bounded interval "I" and converge uniformly, then the series is also integrable on "I" and can be integrated term-by-term. Tests for uniform convergence include the Weierstrass' M-test, Abel's uniform convergence test, Dini's test, and the Cauchy criterion.
More sophisticated types of convergence of a series of functions can also be defined. In measure theory, for instance, a series of functions converges almost everywhere if it converges pointwise except on a certain set of measure zero. Other modes of convergence depend on a different metric space structure on the space of functions under consideration. For instance, a series of functions converges in mean on a set "E" to a limit function "ƒ" provided
as "N" → ∞.
Power series.
A power series is a series of the form
The Taylor series at a point "c" of a function is a power series that, in many cases, converges to the function in a neighborhood of "c". For example, the series
is the Taylor series of formula_74 at the origin and converges to it for every "x".
Unless it converges only at "x"="c", such a series converges on a certain open disc of convergence centered at the point "c" in the complex plane, and may also converge at some of the points of the boundary of the disc. The radius of this disc is known as the radius of convergence, and can in principle be determined from the asymptotics of the coefficients "a""n". The convergence is uniform on closed and bounded (that is, compact) subsets of the interior of the disc of convergence: to wit, it is uniformly convergent on compact sets.
Historically, mathematicians such as Leonhard Euler operated liberally with infinite series, even if they were not convergent.
When calculus was put on a sound and correct foundation in the nineteenth century, rigorous proofs of the convergence of series were always required.
However, the formal operation with non-convergent series has been retained in rings of formal power series which are studied in abstract algebra. Formal power series are also used in combinatorics to describe and study sequences that are otherwise difficult to handle; this is the method of generating functions.
Laurent series.
Laurent series generalize power series by admitting terms into the series with negative as well as positive exponents. A Laurent series is thus any series of the form
If such a series converges, then in general it does so in an annulus rather than a disc, and possibly some boundary points. The series converges uniformly on compact subsets of the interior of the annulus of convergence.
Dirichlet series.
A Dirichlet series is one of the form
where "s" is a complex number. For example, if all "a""n" are equal to 1, then the Dirichlet series is the Riemann zeta function
Like the zeta function, Dirichlet series in general play an important role in analytic number theory. Generally a Dirichlet series converges if the real part of "s" is greater than a number called the abscissa of convergence. In many cases, a Dirichlet series can be extended to an analytic function outside the domain of convergence by analytic continuation. For example, the Dirichlet series for the zeta function converges absolutely when Re "s" > 1, but the zeta function can be extended to a holomorphic function defined on formula_78  with a simple pole at 1.
This series can be directly generalized to general Dirichlet series.
Trigonometric series.
A series of functions in which the terms are trigonometric functions is called a trigonometric series:
The most important example of a trigonometric series is the Fourier series of a function.
History of the theory of infinite series.
Development of infinite series.
Greek mathematician Archimedes produced the first known summation of an infinite series with a
method that is still used in the area of calculus today. He used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave a remarkably accurate approximation of π.
Mathematicians from Kerala, India studied infinite series around 1350 CE. 
In the 17th century, James Gregory worked in the new decimal system on infinite series and published several Maclaurin series. In 1715, a general method for constructing the Taylor series for all functions for which they exist was provided by Brook Taylor. Leonhard Euler in the 18th century, developed the theory of hypergeometric series and q-series.
Convergence criteria.
The investigation of the validity of infinite series is considered to begin with Gauss in the 19th century. Euler had already considered the hypergeometric series
on which Gauss published a memoir in 1812. It established simpler criteria of convergence, and the questions of remainders and the range of convergence.
Cauchy (1821) insisted on strict tests of convergence; he showed that if two series are convergent their product is not necessarily so, and with him begins the discovery of effective criteria. The terms "convergence" and "divergence" had been introduced long before by Gregory (1668). Leonhard Euler and Gauss had given various criteria, and Colin Maclaurin had anticipated some of Cauchy's discoveries. Cauchy advanced the theory of power series by his expansion of a complex function in such a form.
Abel (1826) in his memoir on the binomial series
corrected certain of Cauchy's conclusions, and gave a completely
scientific summation of the series for complex values of formula_82 and formula_83. He showed the necessity of considering the subject of continuity in questions of convergence.
Cauchy's methods led to special rather than general criteria, and
the same may be said of Raabe (1832), who made the first elaborate
investigation of the subject, of De Morgan (from 1842), whose
logarithmic test DuBois-Reymond (1873) and Pringsheim (1889) have
shown to fail within a certain region; of Bertrand (1842), Bonnet
(1843), Malmsten (1846, 1847, the latter without integration);
Stokes (1847), Paucker (1852), Chebyshev (1852), and Arndt
(1853).
General criteria began with Kummer (1835), and have been
studied by Eisenstein (1847), Weierstrass in his various
contributions to the theory of functions, Dini (1867),
DuBois-Reymond (1873), and many others. Pringsheim's memoirs (1889) present the most complete general theory.
Uniform convergence.
The theory of uniform convergence was treated by Cauchy (1821), his
limitations being pointed out by Abel, but the first to attack it
successfully were Seidel and Stokes (1847–48). Cauchy took up the
problem again (1853), acknowledging Abel's criticism, and reaching
the same conclusions which Stokes had already found. Thomae used the
doctrine (1866), but there was great delay in recognizing the
importance of distinguishing between uniform and non-uniform
convergence, in spite of the demands of the theory of functions.
Semi-convergence.
A series is said to be semi-convergent (or conditionally convergent) if it is convergent but not absolutely convergent.
Semi-convergent series were studied by Poisson (1823), who also gave a general form for the remainder of the Maclaurin formula. The most important solution of the problem is due, however, to Jacobi (1834),
who attacked the question of the remainder from a different standpoint and reached a different formula. This expression was also worked out, and another one given, by Malmsten (1847). Schlömilch ("Zeitschrift", Vol.I, p. 192, 1856) also improved Jacobi's remainder, and showed the relation between the remainder and Bernoulli's function
Genocchi (1852) has further contributed to the theory.
Among the early writers was Wronski, whose "loi suprême" (1815) was hardly recognized until Cayley (1873) brought it into
prominence.
Fourier series.
Fourier series were being investigated
as the result of physical considerations at the same time that
Gauss, Abel, and Cauchy were working out the theory of infinite
series. Series for the expansion of sines and cosines, of multiple
arcs in powers of the sine and cosine of the arc had been treated by
Jacob Bernoulli (1702) and his brother Johann Bernoulli (1701) and still
earlier by Vieta. Euler and Lagrange simplified the subject,
as did Poinsot, Schröter, Glaisher, and Kummer.
Fourier (1807) set for himself a different problem, to
expand a given function of "x" in terms of the sines or cosines of
multiples of "x", a problem which he embodied in his "Théorie analytique de la chaleur" (1822). Euler had already given the
formulas for determining the coefficients in the series;
Fourier was the first to assert and attempt to prove the general
theorem. Poisson (1820–23) also attacked the problem from a
different standpoint. Fourier did not, however, settle the question
of convergence of his series, a matter left for Cauchy (1826) to
attempt and for Dirichlet (1829) to handle in a thoroughly
scientific manner (see convergence of Fourier series). Dirichlet's treatment ("Crelle", 1829), of trigonometric series was the subject of criticism and improvement by
Riemann (1854), Heine, Lipschitz, Schläfli, and
du Bois-Reymond. Among other prominent contributors to the theory of
trigonometric and Fourier series were Dini, Hermite, Halphen,
Krause, Byerly and Appell.
Generalizations.
Asymptotic series.
Asymptotic series, otherwise asymptotic expansions, are infinite series whose partial sums become good approximations in the limit of some point of the domain. In general they do not converge. But they are useful as sequences of approximations, each of which provides a value close to the desired answer for a finite number of terms. The difference is that an asymptotic series cannot be made to produce an answer as exact as desired, the way that convergent series can. In fact, after a certain number of terms, a typical asymptotic series reaches its best approximation; if more terms are included, most such series will produce worse answers.
Divergent series.
Under many circumstances, it is desirable to assign a limit to a series which fails to converge in the usual sense. A summability method is such an assignment of a limit to a subset of the set of divergent series which properly extends the classical notion of convergence. Summability methods include Cesàro summation, ("C","k") summation, Abel summation, and Borel summation, in increasing order of generality (and hence applicable to increasingly divergent series).
A variety of general results concerning possible summability methods are known. The Silverman–Toeplitz theorem characterizes "matrix summability methods", which are methods for summing a divergent series by applying an infinite matrix to the vector of coefficients. The most general method for summing a divergent series is non-constructive, and concerns Banach limits.
Series in Banach spaces.
The notion of series can be easily extended to the case of a Banach space. If "x""n" is a sequence of elements of a Banach space "X", then the series Σ"x""n" converges to "x" ∈ "X" if the sequence of partial sums of the series tends to "x"; to wit,
as "N" → ∞.
More generally, convergence of series can be defined in any abelian Hausdorff topological group. Specifically, in this case, Σ"x""n" converges to "x" if the sequence of partial sums converges to "x".
Summations over arbitrary index sets.
Definitions may be given for sums over an arbitrary index set "I". There are two main differences with the usual notion of series: first, there is no specific order given on the set "I"; second, this set "I" may be uncountable.
Families of non-negative numbers.
When summing a family {"a""i"}, "i" ∈ "I", of non-negative numbers, one may define
When the sum is finite, the set of "i" ∈ "I" such that "ai" > 0 is countable. Indeed for every "n" ≥ 1, the set formula_87 is finite, because
If "I"  is countably infinite and enumerated as "I" = {"i"0, "i"1...} then the above defined sum satisfies
provided the value ∞ is allowed for the sum of the series.
Any sum over non-negative reals can be understood as the integral of a non-negative function with respect to the counting measure, which accounts for the many similarities between the two constructions.
Abelian topological groups.
Let "a" : "I" → "X", where "I"  is any set and "X"  is an abelian Hausdorff topological group. Let "F"  be the collection of all finite subsets of "I". Note that "F"  is a directed set ordered under inclusion with union as join. Define the sum "S"  of the family "a" as the limit
if it exists and say that the family "a" is unconditionally summable. Saying that the sum "S"  is the limit of finite partial sums means that for every neighborhood "V"  of 0 in "X", there is a finite subset "A"0 of "I"  such that
Because "F"  is not totally ordered, this is not a limit of a sequence of partial sums, but rather of a net.
For every "W", neighborhood of 0 in "X", there is a smaller neighborhood "V"  such that "V" − "V" ⊂ "W". It follows that the finite partial sums of an unconditionally summable family "ai", "i" ∈ "I", form a "Cauchy net", that is: for every "W", neighborhood of 0 in "X", there is a finite subset "A"0 of "I"  such that
When "X"  is complete, a family "a" is unconditionally summable in "X"  if and only if the finite sums satisfy the latter Cauchy net condition. When "X"  is complete and "ai", "i" ∈ "I", is unconditionally summable in "X", then for every subset "J" ⊂ "I", the corresponding subfamily "aj", "j" ∈ "J", is also unconditionally summable in "X".
When the sum of a family of non-negative numbers, in the extended sense defined before, is finite, then it coincides with the sum in the topological group "X" = R.
If a family "a" in "X"  is unconditionally summable, then for every "W", neighborhood of 0 in "X", there is a finite subset "A"0 of "I"  such that "a""i" ∈ "W"  for every "i" not in "A"0. If "X"  is first-countable, it follows that the set of "i" ∈ "I"  such that "ai" ≠ 0 is countable. This need not be true in a general abelian topological group (see examples below).
Unconditionally convergent series.
Suppose that "I" = N. If a family "a""n", "n" ∈ N, is unconditionally summable in an abelian Hausdorff topological group "X", then the series in the usual sense converges and has the same sum,
By nature, the definition of unconditional summability is insensitive to the order of the summation. When ∑"a""n" is unconditionally summable, then the series remains convergent after any permutation "σ" of the set N of indices, with the same sum,
Conversely, if every permutation of a series ∑"a""n" converges, then the series is unconditionally convergent. When "X"  is complete, then unconditional convergence is also equivalent to the fact that all subseries are convergent; if "X"  is a Banach space, this is equivalent to say that for every sequence of signs "ε""n" = 1 or &minu: this is not about convergence of functions, even less about uniform convergence. -->s;1, the series
converges in "X". If "X"  is a Banach space, then one may define the notion of absolute convergence. A series ∑"a""n" of vectors in "X"  converges absolutely if
If a series of vectors in a Banach space converges absolutely then it converges unconditionally, but the converse only holds in finite-dimensional Banach spaces (theorem of ).
Well-ordered sums.
Conditionally convergent series can be considered if "I" is a well-ordered set, for example an ordinal number "α"0. One may define by transfinite recursion:
and for a limit ordinal "α",
if this limit exists. If all limits exist up to "α"0, then the series converges.

</doc>
<doc id="270770" url="https://en.wikipedia.org/wiki?curid=270770" title="Series and parallel circuits">
Series and parallel circuits

Components of an electrical circuit or electronic circuit can be connected in many different ways. The two simplest of these are called series and parallel and occur frequently. Components connected in series are connected along a single path, so the same current flows through all of the components. Components connected in parallel are connected so the same voltage is applied to each component.
A circuit composed solely of components connected in series is known as a series circuit; likewise, one connected completely in parallel is known as a parallel circuit.
In a series circuit, the current through each of the components is the same, and the voltage across the circuit is the sum of the voltages across each component. In a parallel circuit, the voltage across each of the components is the same, and the total current is the sum of the currents through each component.
Consider a very simple circuit consisting of four light bulbs and one 6 V battery. If a wire joins the battery to one bulb, to the next bulb, to the next bulb, to the next bulb, then back to the battery, in one continuous loop, the bulbs are said to be in series. If each bulb is wired to the battery in a separate loop, the bulbs are said to be in parallel. If the four light bulbs are connected in series, there is same current through all of them, and the voltage drop is 1.5 V across each bulb, which may not be sufficient to make them glow. If the light bulbs are connected in parallel, the currents through the light bulbs combine to form the current in the battery, while the voltage drop is 6.0 V across each bulb and they all glow.
In a series circuit, every device must function for the circuit to be complete. One bulb burning out in a series circuit breaks the circuit. In parallel circuits, each light has its own circuit, so all but one light could be burned out, and the last one will still function.
Series circuits.
Series circuits are sometimes called "current"-coupled or daisy chain-coupled. The current in a series circuit goes through every component in the circuit. Therefore, all of the components in a series connection carry the same current. There is only one path in a series circuit in which the current can flow. The formula to search resistance in series is Rs = R1 + R2 + R3 ... Rn
A series circuit's main disadvantage or advantage, depending on its intended role in a product's overall design, is that because there is only one path in which its current can flow, opening or breaking a series circuit at any point causes the entire circuit to "open" or stop operating. For example, if even one of the light bulbs in an older-style string of Christmas tree lights burns out or is removed, the entire string becomes inoperable until the bulb is replaced.
Current.
In a series circuit the current is the same for all of elements.
Resistors.
The total resistance of resistors in series is equal to the sum of their individual resistances:
Electrical conductance presents a reciprocal quantity to resistance. Total conductance of a series circuits of pure resistors, therefore, can be calculated from the following expression:
For a special case of two resistors in series, the total conductance is equal to:
Inductors.
Inductors follow the same law, in that the total inductance of non-coupled inductors in series is equal to the sum of their individual inductances:
However, in some situations it is difficult to prevent adjacent inductors from influencing each other, as the magnetic field of one device couples with the windings of its neighbours. This influence is defined by the mutual inductance M. For example, if two inductors are in series, there are two possible equivalent inductances depending on how the magnetic fields of both inductors influence each other.
When there are more than two inductors, the mutual inductance between each of them and the way the coils influence each other complicates the calculation. For a larger number of coils the total combined inductance is given by the sum of all mutual inductances between the various coils including the mutual inductance of each given coil with itself, which we term self-inductance or simply inductance. For three coils, there are six mutual inductances formula_6, formula_7, formula_8 and formula_9, formula_10 and formula_11. There are also the three self-inductances of the three coils: formula_12, formula_13 and formula_14.
Therefore
By reciprocity formula_16 = formula_17 so that the last two groups can be combined. The first three terms represent the sum of the self-inductances of the various coils. The formula is easily extended to any number of series coils with mutual coupling. The method can be used to find the self-inductance of large coils of wire of any cross-sectional shape by computing the sum of the mutual inductance of each turn of wire in the coil with every other turn since in such a coil all turns are in series.
Capacitors.
"See also Capacitor networks"
Capacitors follow the same law using the reciprocals. The total capacitance of capacitors in series is equal to the reciprocal of the sum of the reciprocals of their individual capacitances:
Switches.
Two or more switches in series form a logical AND; the circuit only carries current if all switches are closed. See AND gate. While two or more switches in parallel circuits form a logical OR; the circuit will carry current if at least one of the switches are closed. See OR gate.
Cells and batteries.
A battery is a collection of electrochemical cells. If the cells are connected in series, the voltage of the battery will be the sum of the cell voltages. For example, a 12 volt car battery contains six 2-volt cells connected in series. Some vehicles, such as trucks, have two 12 volt batteries in series to feed the 24 volt system.
Parallel circuits.
If two or more components are connected in parallel they have the same potential difference (voltage) across their ends. The potential differences across the components are the same in magnitude, and they also have identical polarities. The same voltage is applicable to all circuit components connected in parallel. The total current is the sum of the currents through the individual components, in accordance with Kirchhoff’s current law.
Voltage.
In a parallel circuit the voltage is the same for all elements.
Resistors.
The current in each individual resistor is found by Ohm's law. Factoring out the voltage gives
To find the total resistance of all components, add the reciprocals of the resistances formula_21 of each component and take the reciprocal of the sum. Total resistance will always be less than the value of the smallest resistance:
For only two resistors, the unreciprocated expression is reasonably simple:
This sometimes goes by the mnemonic "product over sum".
For "N" equal resistors in parallel, the reciprocal sum expression simplifies to:
and therefore to:
To find the current in a component with resistance formula_21, use Ohm's law again:
The components divide the current according to their reciprocal resistances, so, in the case of two resistors,
An old term for devices connected in parallel is "multiple", such as a multiple connection for arc lamps.
Since electrical conductance formula_29 is reciprocal to resistance, the expression for total conductance of a parallel circuit of resistors reads:
The relations for total conductance and resistance stand in a complementary relationship: the expression for a series connection of resistances is the same as for parallel connection of conductances, and vice versa.
Inductors.
Inductors follow the same law, in that the total inductance of non-coupled inductors in parallel is equal to the reciprocal of the sum of the reciprocals of their individual inductances:
If the inductors are situated in each other's magnetic fields, this approach is invalid due to mutual inductance. If the mutual inductance between two coils in parallel is M, the equivalent inductor is:
If formula_33
The sign of formula_35 depends on how the magnetic fields influence each other. For two equal tightly coupled coils the total inductance is close to that of each single coil. If the polarity of one coil is reversed so that M is negative, then the parallel inductance is nearly zero or the combination is almost non-inductive. It is assumed in the "tightly coupled" case M is very nearly equal to L. However, if the inductances are not equal and the coils are tightly coupled there can be near short circuit conditions and high circulating currents for both positive and negative values of M, which can cause problems.
More than three inductors becomes more complex and the mutual inductance of each inductor on each other inductor and their influence on each other must be considered. For three coils, there are three mutual inductances formula_6, formula_7 and formula_8. This is best handled by matrix methods and summing the terms of the inverse of the formula_39 matrix (3 by 3 in this case).
The pertinent equations are of the form:
formula_40
Capacitors.
The total capacitance of capacitors in parallel is equal to the sum of their individual capacitances:
The working voltage of a parallel combination of capacitors is always limited by the smallest working voltage of an individual capacitor.
Switches.
Two or more switches in parallel form a logical OR; the circuit carries current if at least one switch is 'on'. See OR gate.
Cells and batteries.
If the cells of a battery are connected in parallel, the battery voltage will be the same as the cell voltage but the current supplied by each cell will be a fraction of the total current. For example, if a battery contains four cells connected in parallel and delivers a current of 1 ampere, the current supplied by each cell will be 0.25 ampere. Parallel-connected batteries were widely used to power the valve filaments in portable radios but they are now rare. Some solar electric systems have batteries in parallel to increase the storage capacity; a close approximation of total amp-hours is the sum of all batteries in parallel.
Combining conductances.
From Kirchhoff's circuit laws we can deduce the rules for combining conductances. For two conductances formula_42 and formula_43 in parallel the voltage across them is the same and from Kirchhoff's Current Law the total current is 
Substituting Ohm's law for conductances gives 
and the equivalent conductance will be,
For two conductances formula_42 and formula_43 in series the current through them will be the same and Kirchhoff's Voltage Law tells us that the voltage across them is the sum of the voltages across each conductance, that is, 
Substituting Ohm's law for conductance then gives,
which in turn gives the formula for the equivalent conductance,
This equation can be rearranged slightly, though this is a special case that will only rearrange like this for two components.
Notation.
The value of two components in parallel is often represented in equations by two vertical lines "||", borrowing the parallel lines notation from geometry.
This simplifies expressions that would otherwise become complicated by expansion of the terms. For instance, the expression formula_54 refers to 3 resistors in parallel, while the expanded expression is formula_55.
Applications.
Most common application of series circuit in consumer electronics is the 9 volt block battery, the fire alarm battery, which is internally built of six cells, 1.5 volts each.
Series circuits were formerly used for lighting in electric multiple unit trains. For example, if the supply voltage was 600 volts there might be eight 70-volt bulbs in series (total 560 volts) plus a resistor to drop the remaining 40 volts. Series circuits for train lighting were superseded, first by motor-generators, then by solid state devices.
Series resistance can also be applied to the arrangement of blood vessels within a given organ. Each organ is supplied by a large artery, smaller arteries, arterioles, capillaries, and veins arranged in series. The total resistance is the sum of the individual resistances, as expressed by the following equation: Rtotal = Rartery + Rarterioles + Rcapillaries. The largest proportion of resistance in this series is contributed by the arterioles.
Parallel resistance is illustrated by the circulatory system. Each organ is supplied by an artery that branches off the aorta. The total resistance of this parallel arrangement is expressed by the following equation: 1/Rtotal = 1/Ra + 1/Rb + ... 1/Rn. Ra, Rb, and Rn are the resistances of the renal, hepatic, and other arteries respectively. The total resistance is less than the resistance of any of the individual arteries.

</doc>
<doc id="3620291" url="https://en.wikipedia.org/wiki?curid=3620291" title="Sfermion">
Sfermion

In particle physics, a sfermion is the spin-0 superpartner particle (or "sparticle") of its associated fermion. In supersymmetric extensions to the Standard Model (SM) each particle has a superpartner with spin that differs by . Fermions in the SM have spin- and therefore sfermions have spin 0.
In general the name sfermions is formed by prefixing an 's' to the name of its superpartner, denoting that it is a scalar particle with spin 0. For instance, the electron's superpartner is the selectron and the top quark's superpartner is the stop squark.
One corollary from supersymmetry is that sparticles have the same gauge numbers as their SM partners. This means that sparticle–particle pairs have the same color charge, weak isospin charge, and hypercharge (and consequently electric charge). Unbroken supersymmetry also implies that sparticle–particle pairs have the same mass. This is evidently not the case, since these sparticles would have already been detected. Thus, sparticles must have different masses from the particle partners and supersymmetry is said to be broken.
Fundamental sfermions.
Squarks.
Squarks are the superpartners of quarks. These include the sup squark, sdown squark, scharm squark, sstrange squark, stop squark, and sbottom squark.
Sleptons.
Sleptons are the superbros of leptons. These include the selectron, smuon, stau, and the sneutrinos.

</doc>
<doc id="8529117" url="https://en.wikipedia.org/wiki?curid=8529117" title="Sigma baryon">
Sigma baryon

The Sigma baryons are a family of subatomic hadron particles which have a +2, +1 or −1 elementary charge or are neutral. They are baryons containing three quarks: two up and/or down quarks, and one third quark, which can be either a strange (symbols , , ), a charm (symbols , , ), a bottom (symbols , , ) or a top (symbols , , ) quark. However, the top Sigmas are not expected to be observed as the Standard Model predicts the mean lifetime of top quarks to be roughly . This is about 20 times shorter than the timescale for strong interactions, and therefore it does not form hadrons.
List.
The symbols encountered in these lists are: I ("isospin"), J ("total angular momentum"), P ("parity"), u ("up quark"), d ("down quark"), s ("strange quark"), c ("charm quark"), t ("top quark"), b ("bottom quark"), Q ("charge"), B ("baryon number"), S ("strangeness"), C ("charmness"), B′ ("bottomness"), T ("topness"), as well as other subatomic particles (hover for name).
Antiparticles are not listed in the table; however, they simply would have all quarks changed to antiquarks, and Q, B, S, C, B′, T, would be of opposite signs. I, J, and P values in red have not been firmly established by experiments, but are predicted by the quark model and are consistent with the measurements.
JP = + Sigma baryons.
† Particle currently unobserved, but predicted by the standard model. <br>
[a] PDG reports the resonance width (Γ). Here the conversion τ =  is given instead. <br>
[b] The specific values of the name has not been decided yet, but will likely be close to (5810).
JP = + Sigma baryons.
† Particle currently unobserved, but predicted by the standard model. <br>
[c] PDG reports the resonance width (Γ). Here the conversion τ =  is given instead.

</doc>
<doc id="292831" url="https://en.wikipedia.org/wiki?curid=292831" title="Simple Lie group">
Simple Lie group

In group theory, a simple Lie group is a connected non-abelian Lie group "G" which does not have nontrivial connected normal subgroups.
A simple Lie algebra is a non-abelian Lie algebra whose only ideals are 0 and itself (or equivalently, a Lie algebra of dimension 2 or more, whose only ideals are 0 and itself). A direct sum of simple Lie algebras is called a semisimple Lie algebra. 
An equivalent definition of a simple Lie group follows from the Lie correspondence: a connected Lie group is simple if its Lie algebra is simple. An important technical point is that
a simple Lie group may contain "discrete" normal subgroups, hence being a simple Lie group is different from being simple as an abstract group.
Simple Lie groups include many classical Lie groups, which provide a group-theoretic underpinning for spherical geometry, projective geometry and related geometries in the sense of Felix Klein's Erlangen programme. It emerged in the course of classification of simple Lie groups that there exist also several exceptional possibilities not corresponding to any familiar geometry. These "exceptional groups" account for many special examples and configurations in other branches of mathematics, as well as contemporary theoretical physics.
While the notion of a simple Lie group is satisfying from the axiomatic perspective, in applications of Lie theory, such as the theory of Riemannian symmetric spaces, somewhat more general notions of semisimple and reductive Lie groups proved to be even more useful. In particular, every connected compact Lie group is reductive, and the study of representations of general reductive groups is a major branch of representation theory.
Comments on the definition.
Unfortunately there is no single standard definition of a simple Lie group. The definition given above is sometimes varied in the following ways:
The most common definition is the one above: simple Lie groups have to be connected, they are allowed to have non-trivial centers (possibly infinite), they need not be representable by finite matrices, and they must be non-abelian.
Method of classification.
Such groups are classified using the prior classification of the complex simple Lie algebras: for which see the page on root systems. It is shown that a simple Lie group has a simple Lie algebra that will occur on the list given there, once it is complexified (that is, made into a complex vector space rather than a real one). This reduces the classification to two further matters.
Real forms.
The groups SO("p","q",R) and SO("p"+"q",R), for example, give rise to different real Lie algebras, but having the same Dynkin diagram. In general there may be different "real forms" of the same complex Lie algebra.
Relationship of simple Lie algebras to groups.
Secondly the Lie algebra only determines uniquely the simply connected (universal) cover "G*" of the component containing the identity of a Lie group "G". It may well happen that "G*" isn't actually a simple group, for example having a non-trivial center. We have therefore to worry about the global topology, by computing the fundamental group of "G" (an abelian group: a Lie group is an H-space). This was done by Élie Cartan.
For an example, take the special orthogonal groups in even dimension. With the non-identity matrix "−I" in the center, these aren't actually simple groups; and having a twofold spin cover, they aren't simply-connected either. They lie 'between' "G*" and "G", in the notation above.
Classification by Dynkin diagram.
According to Dynkin's classification, we have as possibilities these only, where "n" is the number of nodes:
Infinite series.
A series.
A1, A2, ...
Ar corresponds to the special unitary group, SU("r" + 1).
B series.
B2, B3, ...
Br corresponds to the special orthogonal group, SO(2"r" + 1).
C series.
C3, C4, ...
Cr corresponds to the symplectic group, Sp(2r).
D series.
D4, D5, ...
Dr corresponds to the special orthogonal group, SO(2"r"), starting with SO(8). The diagram D2 is two isolated nodes, the same as A1 ∪ A1, and this coincidence corresponds to the covering map homomorphism from SU(2) × SU(2) to SO(4) given by quaternion multiplication; see quaternions and spatial rotation. Thus SO(4) is not a simple group. Also, the diagram D3 is the same as A3, corresponding to a covering map homomorphism from SU(4) to SO(6). With D4 there is an 'exotic' symmetry of the diagram, corresponding to so-called triality.
Exceptional cases.
For the so-called exceptional cases see G2, F4, E6, E7, and E8. These cases are deemed 'exceptional' because they do not fall into infinite series of groups of increasing dimension. From the point of view of each group taken separately, there is nothing so unusual about them. These exceptional groups were discovered around 1890 in the classification of the simple Lie algebras, over the complex numbers (Wilhelm Killing, re-done by Élie Cartan). For some time it was a research issue to find concrete ways in which they arise, for example as a symmetry group of a differential system.
See also E7½.
Simply laced groups.
A simply laced group is a Lie group whose Dynkin diagram only contain simple links, and therefore all the nonzero roots of the corresponding Lie algebra have the same length. The A, D and E series groups are all simply laced, but no group of type B, C, F, or G is simply laced.
References.
 

</doc>
<doc id="28758" url="https://en.wikipedia.org/wiki?curid=28758" title="Spacetime">
Spacetime

In physics, spacetime is any mathematical model that combines space and time into a single interwoven continuum. The spacetime of our universe is usually interpreted from a Euclidean space perspective, which regards space as consisting of three dimensions, and time as consisting of one dimension, the "fourth dimension". By combining space and time into a single manifold called Minkowski space, physicists have significantly simplified a large number of physical theories, as well as described in a more uniform way the workings of the universe at both the supergalactic and subatomic levels.
Explanation.
In non-relativistic classical mechanics, the use of Euclidean space instead of spacetime is appropriate, because time is treated as universal with a constant rate of passage that is independent of the state of motion of an observer. In relativistic contexts, time cannot be separated from the three dimensions of space, because the observed rate at which time passes for an object depends on the object's velocity relative to the observer and also on the strength of gravitational fields, which can slow the passage of time for an object as seen by an observer outside the field.
In cosmology, the concept of spacetime combines space and time to a single abstract universe. Mathematically it is a manifold consisting of "events" which are described by some type of coordinate system. Typically three spatial dimensions (length, width, height), and one temporal dimension (time) are required. Dimensions are independent components of a coordinate grid needed to locate a point in a certain defined "space". For example, on the globe the latitude and longitude are two independent coordinates which together uniquely determine a location. In spacetime, a coordinate grid that spans the 3+1 dimensions locates events (rather than just points in space), i.e., time is added as another dimension to the coordinate grid. This way the coordinates specify "where" and "when" events occur. However, the unified nature of spacetime and the freedom of coordinate choice it allows imply that to express the temporal coordinate in one coordinate system requires both temporal and spatial coordinates in another coordinate system. Unlike in normal spatial coordinates, there are still restrictions for how measurements can be made spatially and temporally (see Spacetime intervals). These restrictions correspond roughly to a particular mathematical model which differs from Euclidean space in its manifest symmetry.
Until the beginning of the 20th century, time was believed to be independent of motion, progressing at a fixed rate in all reference frames; however, later experiments revealed that time slows at higher speeds of the reference frame relative to another reference frame. Such slowing, called time dilation, is explained in special relativity theory. Many experiments have confirmed time dilation, such as the relativistic decay of muons from cosmic ray showers and the slowing of atomic clocks aboard a Space Shuttle relative to synchronized Earth-bound inertial clocks. The duration of time can therefore vary according to events and reference frames.
When dimensions are understood as mere components of the grid system, rather than physical attributes of space, it is easier to understand the alternate dimensional views as being simply the result of coordinate transformations.
The term "spacetime" has taken on a generalized meaning beyond treating spacetime events with the normal 3+1 dimensions. It is really the combination of space and time. Other proposed spacetime theories include additional dimensions—normally spatial but there exist some speculative theories that include additional temporal dimensions and even some that include dimensions that are neither temporal nor spatial (e.g., superspace). How many dimensions are needed to describe the universe is still an open question. Speculative theories such as string theory predict 10 or 26 dimensions (with M-theory predicting 11 dimensions: 10 spatial and 1 temporal), but the existence of more than four dimensions would only appear to make a difference at the subatomic level.
Spacetime in literature.
Incas regarded space and time as a single concept, referred to as pacha (, ). The peoples of the Andes maintain a similar understanding.
Arthur Schopenhauer wrote in §18 of "On the Fourfold Root of the Principle of Sufficient Reason" (1813): "the representation of coexistence is impossible in Time alone; it depends, for its completion, upon the representation of Space; because, in mere Time, all things follow one another, and in mere Space all things are side by side; it is accordingly only by the combination of Time and Space that the representation of coexistence arises".
The idea of a unified spacetime is stated by Edgar Allan Poe in his essay on cosmology titled "Eureka" (1848) that "Space and duration are one". In 1895, in his novel "The Time Machine", H. G. Wells wrote, "There is no difference between time and any of the three dimensions of space except that our consciousness moves along it", and that "any real body must have extension in four directions: it must have Length, Breadth, Thickness, and Duration".
Marcel Proust, in his novel "Swann's Way" (published 1913), describes the village church of his childhood's Combray as "a building which occupied, so to speak, four dimensions of space—the name of the fourth being Time".
Mathematical concept.
In Encyclopedie under the term "dimension" Jean le Rond d'Alembert speculated that duration (time) might be considered a fourth dimension if the idea was not too novel.
Another early venture was by Joseph Louis Lagrange in his "Theory of Analytic Functions" (1797, 1813). He said, "One may view mechanics as a geometry of four dimensions, and mechanical analysis as an extension of geometric analysis".
The ancient idea of the cosmos gradually was described mathematically with differential equations, differential geometry, and abstract algebra. These mathematical articulations blossomed in the nineteenth century as electrical technology stimulated men like Michael Faraday and James Clerk Maxwell to describe the reciprocal relations of electric and magnetic fields.
Daniel Siegel phrased Maxwell's role in relativity as follows:
[...] the idea of the propagation of forces at the velocity of light through the electromagnetic field as described by Maxwell's equations—rather than instantaneously at a distance—formed the necessary basis for relativity theory.
Maxwell used vortex models in his papers on On Physical Lines of Force, but ultimately gave up on any substance but the electromagnetic field. Pierre Duhem wrote:
[Maxwell] was not able to create the theory that he envisaged except by giving up the use of any model, and by extending by means of analogy the abstract system of electrodynamics to displacement currents.
In Siegel's estimation, "this very abstract view of the electromagnetic fields, involving no visualizable picture of what is going on out there in the field, is Maxwell's legacy."
Describing the behaviour of electric fields and magnetic fields led Maxwell to view the combination as an electromagnetic field. These fields have a value at every point of spacetime. It is the intermingling of electric and magnetic manifestations, described by Maxwell's equations, that give spacetime its structure. In particular, the rate of motion of an observer determines the electric and magnetic profiles of the electromagnetic field. The propagation of the field is determined by the electromagnetic wave equation, which requires spacetime for description.
Spacetime was described as an affine space with quadratic form in Minkowski space of 1908. In his 1914 textbook "The Theory of Relativity", Ludwik Silberstein used biquaternions to represent events in Minkowski space. He also exhibited the Lorentz transformations between observers of differing velocities as biquaternion mappings. Biquaternions were described in 1853 by W. R. Hamilton, so while the physical interpretation was new, the mathematics was well known in English literature, making relativity an instance of applied mathematics.
The first inkling of general relativity in spacetime was articulated by W. K. Clifford.
Description of the effect of gravitation on space and time was found to be most easily visualized as a "warp" or stretching in the geometrical fabric of space and time, in a smooth and continuous way that changed smoothly from point-to-point along the spacetime fabric. In 1947 James Jeans provided a concise summary of the development of spacetime theory in his book "The Growth of Physical Science".
Basic concepts.
The basic elements of spacetime are events. In any given spacetime, an event is a unique position at a unique time. Because events are spacetime points, an example of an event in classical relativistic physics is formula_1, the location of an elementary (point-like) particle at a particular time. A spacetime itself can be viewed as the union of all events in the same way that a line is the union of all of its points, formally organized into a manifold, a space which can be described at small scales using coordinate systems.
A spacetime is independent of any observer. However, in describing physical phenomena (which occur at certain moments of time in a given region of space), each observer chooses a convenient metrical coordinate system. Events are specified by four real numbers in any such coordinate system. The trajectories of elementary (point-like) particles through space and time are thus a continuum of events called the world line of the particle. Extended or composite objects (consisting of many elementary particles) are thus a union of many world lines twisted together by virtue of their interactions through spacetime into a "world-braid".
However, in physics, it is common to treat an extended object as a "particle" or "field" with its own unique (e.g., center of mass) position at any given time, so that the world line of a particle or light beam is the path that this particle or beam takes in the spacetime and represents the history of the particle or beam. The world line of the orbit of the Earth (in such a description) is depicted in two spatial dimensions "x" and "y" (the plane of the Earth's orbit) and a time dimension orthogonal to "x" and "y". The orbit of the Earth is an ellipse in space alone, but its world line is a helix in spacetime.
The unification of space and time is exemplified by the common practice of selecting a metric (the measure that specifies the interval between two events in spacetime) such that all four dimensions are measured in terms of units of distance: representing an event as formula_2 (in the Lorentz metric) or formula_3 (in the original Minkowski metric) where formula_4 is the speed of light. The metrical descriptions of Minkowski Space and spacelike, lightlike, and timelike intervals given below follow this convention, as do the conventional formulations of the Lorentz transformation.
in flat space.
In a Euclidean space, the separation between two points is measured by the distance between the two points. The distance is purely spatial, and is always positive. In spacetime, the displacement four-vector Δ"R" is given by the space displacement vector Δ"r" and the time difference Δ"t" between the events. The "spacetime interval", also called "invariant interval", between the two events, "s"2, is defined as:
where "c" is the speed of light. The choice of signs for formula_6 above follows the space-like convention (−+++).
Spacetime intervals may be classified into three distinct types, based on whether the temporal separation (formula_7) or the spatial separation (formula_8) of the two events is greater: time-like, light-like or space-like.
Certain types of world lines are called geodesics of the spacetime – straight lines in the case of Minkowski space and their closest equivalent in the curved spacetime of general relativity. In the case of purely time-like paths, geodesics are (locally) the paths of greatest separation (spacetime interval) as measured along the path between two events, whereas in Euclidean space and Riemannian manifolds, geodesics are paths of shortest distance between two points. The concept of geodesics becomes central in general relativity, since geodesic motion may be thought of as "pure motion" (inertial motion) in spacetime, that is, free from any external influences.
Time-like interval.
For two events separated by a time-like interval, enough time passes between them that there could be a cause–effect relationship between the two events. For a particle traveling through space at less than the speed of light, any two events which occur to or by the particle must be separated by a time-like interval. Event pairs with time-like separation define a negative spacetime interval (formula_10) and may be said to occur in each other's future or past. There exists a reference frame such that the two events are observed to occur in the same spatial location, but there is no reference frame in which the two events can occur at the same time.
The measure of a time-like spacetime interval is described by the proper time interval, formula_11:
The proper time interval would be measured by an observer with a clock traveling between the two events in an inertial reference frame, when the observer's path intersects each event as that event occurs. (The proper time interval defines a real number, since the interior of the square root is positive.)
Light-like interval.
In a light-like interval, the spatial distance between two events is exactly balanced by the time between the two events. The events define a spacetime interval of zero (formula_14). Light-like intervals are also known as "null" intervals.
Events which occur to or are initiated by a photon along its path (i.e., while traveling at formula_4, the speed of light) all have light-like separation. Given one event, all those events which follow at light-like intervals define the propagation of a light cone, and all the events which preceded from a light-like interval define a second (graphically inverted, which is to say "pastward") light cone.
Space-like interval.
When a space-like interval separates two events, not enough time passes between their occurrences for there to exist a causal relationship crossing the spatial distance between the two events at the speed of light or slower. Generally, the events are considered not to occur in each other's future or past. There exists a reference frame such that the two events are observed to occur at the same time, but there is no reference frame in which the two events can occur in the same spatial location.
For these space-like event pairs with a positive spacetime interval (formula_17), the measurement of space-like separation is the proper distance, formula_18:
Like the proper time of time-like intervals, the proper distance of space-like spacetime intervals is a real number value.
Interval as area.
The interval has been presented as the area of an oriented rectangle formed by two events and isotropic lines through them. Time-like or space-like separations correspond to oppositely oriented rectangles, one type considered to have rectangles of negative area. The case of two events separated by light corresponds to the rectangle degenerating to the segment between the events and zero area. The transformations leaving interval-length invariant are the area-preserving squeeze mappings.
The parameters traditionally used rely on quadrature of the hyperbola, which is the natural logarithm. This transcendental function is essential in mathematical analysis as its inverse unites circular functions and hyperbolic functions: The exponential function, e"t",  "t" a real number, used in the hyperbola (e"t", e–"t" ), generates hyperbolic sectors and the hyperbolic angle parameter. The functions cosh and sinh, used with rapidity as hyperbolic angle, provide the common representation of squeeze in the form
formula_20 or as the split-complex unit 
formula_21
Mathematics of spacetimes.
For physical reasons, a spacetime continuum is mathematically defined as a four-dimensional, smooth, connected Lorentzian manifold formula_22. This means the smooth Lorentz metric formula_23 has signature formula_24. The metric determines the geometry of spacetime, as well as determining the geodesics of particles and light beams. About each point (event) on this manifold, coordinate charts are used to represent observers in reference frames. Usually, Cartesian coordinates formula_25 are used. Moreover, for simplicity's sake, units of measurement are usually chosen such that the speed of light formula_4 is equal to 1.
A reference frame (observer) can be identified with one of these coordinate charts; any such observer can describe any event formula_27. Another reference frame may be identified by a second coordinate chart about formula_27. Two observers (one in each reference frame) may describe the same event formula_27 but obtain different descriptions.
Usually, many overlapping coordinate charts are needed to cover a manifold. Given two coordinate charts, one containing formula_27 (representing an observer) and another containing formula_31 (representing another observer), the intersection of the charts represents the region of spacetime in which both observers can measure physical quantities and hence compare results. The relation between the two sets of measurements is given by a non-singular coordinate transformation on this intersection. The idea of coordinate charts as local observers who can perform measurements in their vicinity also makes good physical sense, as this is how one actually collects physical data—locally.
For example, two observers, one of whom is on Earth, but the other one who is on a fast rocket to Jupiter, may observe a comet crashing into Jupiter (this is the event formula_27). In general, they will disagree about the exact location and timing of this impact, i.e., they will have different 4-tuples formula_25 (as they are using different coordinate systems). Although their kinematic descriptions will differ, dynamical (physical) laws, such as momentum conservation and the first law of thermodynamics, will still hold. In fact, relativity theory requires more than this in the sense that it stipulates these (and all other physical) laws must take the same form in all coordinate systems. This introduces tensors into relativity, by which all physical quantities are represented.
Geodesics are said to be time-like, null, or space-like if the tangent vector to one point of the geodesic is of this nature. Paths of particles and light beams in spacetime are represented by time-like and null (light-like) geodesics, respectively.
Topology.
The assumptions contained in the definition of a spacetime are usually justified by the following considerations.
The connectedness assumption serves two main purposes. First, different observers making measurements (represented by coordinate charts) should be able to compare their observations on the non-empty intersection of the charts. If the connectedness assumption were dropped, this would not be possible. Second, for a manifold, the properties of connectedness and path-connectedness are equivalent, and one requires the existence of paths (in particular, geodesics) in the spacetime to represent the motion of particles and radiation.
Every spacetime is paracompact. This property, allied with the smoothness of the spacetime, gives rise to a smooth linear connection, an important structure in general relativity. Some important theorems on constructing spacetimes from compact and non-compact manifolds include the following:
Spacetime symmetries.
Often in relativity, spacetimes that have some form of symmetry are studied. As well as helping to classify spacetimes, these symmetries usually serve as a simplifying assumption in specialized work. Some of the most popular ones include:
Causal structure.
The causal structure of a spacetime describes causal relationships between pairs of points in the spacetime based on the existence of certain types of curves joining the points.
Spacetime in special relativity.
The geometry of spacetime in special relativity is described by the Minkowski metric on R4. This spacetime is called Minkowski space. The Minkowski metric is usually denoted by formula_34 and can be written as a four-by-four matrix:
where the Landau–Lifshitz space-like convention is being used. A basic assumption of relativity is that coordinate transformations must leave spacetime intervals invariant. Intervals are invariant under Lorentz transformations. This invariance property leads to the use of four-vectors (and other tensors) in describing physics.
Strictly speaking, one can also consider events in Newtonian physics as a single spacetime. This is Galilean–Newtonian relativity, and the coordinate systems are related by Galilean transformations. However, since these preserve spatial and temporal distances independently, such a spacetime can be decomposed into spatial coordinates plus temporal coordinates, which is not possible in the general case.
Spacetime in general relativity.
In general relativity, it is assumed that spacetime is curved by the presence of matter (energy), this curvature being represented by the Riemann tensor. In special relativity, the Riemann tensor is identically zero, and so this concept of "non-curvedness" is sometimes expressed by the statement "Minkowski spacetime is flat."
The earlier discussed notions of time-like, light-like and space-like intervals in special relativity can similarly be used to classify one-dimensional curves through curved spacetime. A time-like curve can be understood as one where the interval between any two infinitesimally close events on the curve is time-like, and likewise for light-like and space-like curves. Technically the three types of curves are usually defined in terms of whether the tangent vector at each point on the curve is time-like, light-like or space-like. The world line of a slower-than-light object will always be a time-like curve, the world line of a massless particle such as a photon will be a light-like curve, and a space-like curve could be the world line of a hypothetical tachyon. In the local neighborhood of any event, time-like curves that pass through the event will remain inside that event's past and future light cones, light-like curves that pass through the event will be on the surface of the light cones, and space-like curves that pass through the event will be outside the light cones. One can also define the notion of a three-dimensional "spacelike hypersurface", a continuous three-dimensional "slice" through the four-dimensional property with the property that every curve that is contained entirely within this hypersurface is a space-like curve.
Many spacetime continua have physical interpretations which most physicists would consider bizarre or unsettling. For example, a compact spacetime has closed timelike curves, which violate our usual ideas of causality (that is, future events could affect past ones). For this reason, mathematical physicists usually consider only restricted subsets of all the possible spacetimes. One way to do this is to study "realistic" solutions of the equations of general relativity. Another way is to add some additional "physically reasonable" but still fairly general geometric restrictions and try to prove interesting things about the resulting spacetimes. The latter approach has led to some important results, most notably the Penrose–Hawking singularity theorems.
Quantized spacetime.
In general relativity, spacetime is assumed to be smooth and continuous—and not just in the mathematical sense. In the theory of quantum mechanics, there is an inherent discreteness present in physics. In attempting to reconcile these two theories, it is sometimes postulated that spacetime should be quantized at the very smallest scales. Current theory is focused on the nature of spacetime at the Planck scale. Causal sets, loop quantum gravity, string theory, causal dynamical triangulation, and black hole thermodynamics all predict a quantized spacetime with agreement on the order of magnitude. Loop quantum gravity makes precise predictions about the geometry of spacetime at the Planck scale.

</doc>
<doc id="341015" url="https://en.wikipedia.org/wiki?curid=341015" title="Sparse matrix">
Sparse matrix

In numerical analysis, a sparse matrix is a matrix in which most of the elements are zero. By contrast, if most of the elements are nonzero, then the matrix is considered dense. The fraction of non-zero elements over the total number of elements (i.e., that can fit into the matrix, say a matrix of dimension of m x n can accommodate m x n total number of elements) in a matrix is called the sparsity (density).
Conceptually, sparsity corresponds to systems which are loosely coupled. Consider a line of balls connected by springs from one to the next: this is a sparse system as only adjacent balls are coupled. By contrast, if the same line of balls had springs connecting each ball to all other balls, the system would correspond to a dense matrix. The concept of sparsity is useful in combinatorics and application areas such as network theory, which have a low density of significant data or connections.
Large sparse matrices often appear in scientific or engineering applications when solving partial differential equations.
When storing and manipulating sparse matrices on a computer, it is beneficial and often necessary to use specialized algorithms and data structures that take advantage of the sparse structure of the matrix. Operations using standard dense-matrix structures and algorithms are slow and inefficient when applied to large sparse matrices as processing and memory are wasted on the zeroes. Sparse data is by nature more easily compressed and thus require significantly less storage. Some very large sparse matrices are infeasible to manipulate using standard dense-matrix algorithms.
Storing a sparse matrix.
A matrix is typically stored as a two-dimensional array. Each entry in the array represents an element of the matrix and is accessed by the two indices and . Conventionally, is the row index, numbered from top to bottom, and is the column index, numbered from left to right. For an matrix, the amount of memory required to store the matrix in this format is proportional to (disregarding the fact that the dimensions of the matrix also need to be stored).
In the case of a sparse matrix, substantial memory requirement reductions can be realized by storing only the non-zero entries. Depending on the number and distribution of the non-zero entries, different data structures can be used and yield huge savings in memory when compared to the basic approach. The trade-off is that accessing the individual elements becomes more complex and additional structures are needed to be able to recover the original matrix unambiguously.
Formats can be divided into two groups:
Dictionary of keys (DOK).
DOK consists of a dictionary that maps -pairs to the value of the elements. Elements that are missing from the dictionary are taken to be zero. The format is good for incrementally constructing a sparse matrix in random order, but poor for iterating over non-zero values in lexicographical order. One typically constructs a matrix in this format and then converts to another more efficient format for processing.
List of lists (LIL).
LIL stores one list per row, with each entry containing the column index and the value. Typically, these entries are kept sorted by column index for faster lookup. This is another format good for incremental matrix construction.
Coordinate list (COO).
COO stores a list of tuples. Ideally, the entries are sorted (by row index, then column index) to improve random access times. This is another format which is good for incremental matrix construction.
Yale.
The Yale sparse matrix format stores an initial sparse matrix, , in row form using three (one-dimensional) arrays . Let denote the number of nonzero entries in M. (Note that zero-based indices shall be used here.)
For example, the matrix
is a matrix with 4 nonzero elements, hence
 A = [ 5 8 3 6 ]
 IA = [ 0 0 2 3 4 ]
 JA = [ 0 1 2 1 ]
So, in array , the element " from has column index , " and " have index , and element " has index .
In this case the Yale representation contains 13 entries, compared to 16 in the original matrix. The Yale format saves on memory only when .
Another example, the matrix
is a matrix (24 entries) with 8 nonzero elements, so
 A = [ 10 20 30 40 50 60 70 80 ]
 IA = [ 0 2 4 7 8 ]
 JA = [ 0 1 1 3 2 3 4 5 ]
The whole is stored as 21 entries.
Note that in this format, the first value of is always zero and the last is always , so they are in some sense redundant. However, they can make accessing and traversing the array easier for the programmer.
Compressed row Storage (CRS or CSR).
CSR is effectively identical to the Yale Sparse Matrix format, except that the column array is normally stored ahead of the row index array. I.e. CSR is , where is an array of the (left-to-right, then top-to-bottom) non-zero values of the matrix; is the column indices corresponding to the values; and, is the list of value indexes where each row starts. The name is based on the fact that row index information is compressed relative to the COO format. One typically uses another format (LIL, DOK, COO) for construction. This format is efficient for arithmetic operations, row slicing, and matrix-vector products. See scipy.sparse.csr_matrix.
Compressed sparse column (CSC or CCS).
CSC is similar to CSR except that values are read first by column, a row index is stored for each value, and column pointers are stored. I.e. CSC is , where is an array of the (top-to-bottom, then left-to-right) non-zero values of the matrix; is the row indices corresponding to the values; and, is the list of indexes where each column starts. The name is based on the fact that column index information is compressed relative to the COO format. One typically uses another format (LIL, DOK, COO) for construction. This format is efficient for arithmetic operations, column slicing, and matrix-vector products. See scipy.sparse.csc_matrix.
This is the traditional format for specifying a sparse matrix in MATLAB (via the codice_3 function).
Special structure.
Banded.
An important special type of sparse matrices is band matrix, defined as follows. The "lower bandwidth" of a matrix is the smallest number such that the entry vanishes whenever . Similarly, the "upper bandwidth" is the smallest number such that whenever . For example, a tridiagonal matrix has lower bandwidth and upper bandwidth . As another example, the following sparse matrix has lower and upper bandwidth both equal to 3. Notice that zeros are represented with dots for clarity.
Matrices with reasonably small upper and lower bandwidth are known as band matrices and often lend themselves to simpler algorithms than general sparse matrices; or one can sometimes apply dense matrix algorithms and gain efficiency simply by looping over a reduced number of indices.
By rearranging the rows and columns of a matrix it may be possible to obtain a matrix with a lower bandwidth. A number of algorithms are designed for bandwidth minimization.
Diagonal.
A very efficient structure for an extreme case of band matrices, the diagonal matrix, is to store just the entries in the main diagonal as a one-dimensional array, so a diagonal matrix requires only entries.
Symmetric.
A symmetric sparse matrix arises as the adjacency matrix of an undirected graph; it can be stored efficiently as an adjacency list.
Reducing fill-in.
The fill-in of a matrix are those entries which change from an initial zero to a non-zero value during the execution of an algorithm. To reduce the memory requirements and the number of arithmetic operations used during an algorithm it is useful to minimize the fill-in by switching rows and columns in the matrix. The symbolic Cholesky decomposition can be used to calculate the worst possible fill-in before doing the actual Cholesky decomposition.
There are other methods than the Cholesky decomposition in use. Orthogonalization methods (such as QR factorization) are common, for example, when solving problems by least squares methods. While the theoretical fill-in is still the same, in practical terms the "false non-zeros" can be different for different methods. And symbolic versions of those algorithms can be used in the same manner as the symbolic Cholesky to compute worst case fill-in.
Solving sparse matrix equations.
Both iterative and direct methods exist for sparse matrix solving.
Iterative methods, such as conjugate gradient method and GMRES utilize fast computations of matrix-vector products formula_4, where matrix formula_5 is sparse. The use of preconditioners can significantly accelerate convergence of such iterative methods.

</doc>
<doc id="26962" url="https://en.wikipedia.org/wiki?curid=26962" title="Special relativity">
Special relativity

In physics, special relativity (SR, also known as the special theory of relativity or STR) is the generally accepted and experimentally well confirmed physical theory regarding the relationship between space and time. In Einstein's original pedagogical treatment, it is based on two postulates: (1) that the laws of physics are invariant (i.e. identical) in all inertial systems (non-accelerating frames of reference); and (2) that the speed of light in a vacuum is the same for all observers, regardless of the motion of the light source. It was originally proposed in 1905 by Albert Einstein in the paper "On the Electrodynamics of Moving Bodies". The inconsistency of Newtonian mechanics with Maxwell’s equations of electromagnetism and the inability to discover Earth's motion through a luminiferous aether led to the development of special relativity, which corrects mechanics to handle situations involving motions nearing the speed of light. As of today, special relativity is the most accurate model of motion at any speed. Even so, the Newtonian mechanics model is still useful (due to its simplicity and high accuracy) as an approximation at small velocities relative to the speed of light.
Special relativity implies a wide range of consequences, which have been experimentally verified, including length contraction, time dilation, relativistic mass, mass–energy equivalence, a universal speed limit, and relativity of simultaneity. It has replaced the conventional notion of an absolute universal time with the notion of a time that is dependent on reference frame and spatial position. Rather than an invariant time interval between two events, there is an invariant spacetime interval. Combined with other laws of physics, the two postulates of special relativity predict the equivalence of mass and energy, as expressed in the mass–energy equivalence formula "E" = "mc"2, where "c" is the speed of light in vacuum.
A defining feature of special relativity is the replacement of the Galilean transformations of Newtonian mechanics with the Lorentz transformations. Time and space cannot be defined separately from each other. Rather space and time are interwoven into a single continuum known as spacetime. Events that occur at the same time for one observer could occur at different times for another.
The theory is "special" in that it only applies in the special case where the curvature of spacetime due to gravity is negligible. In order to include gravity, Einstein formulated general relativity in 1915. (Special relativity, contrary to some outdated descriptions, is capable of handling accelerated frames of reference.)
As Galilean relativity is now considered an approximation of special relativity that is valid for low speeds, special relativity is considered an approximation of general relativity that is valid for weak gravitational fields, i.e. at a sufficiently small scale and in conditions of free fall. Whereas general relativity incorporates noneuclidean geometry in order to represent gravitational effects as the geometric curvature of spacetime, special relativity is restricted to the flat spacetime known as Minkowski space. A locally Lorentz-invariant frame that abides by special relativity can be defined at sufficiently small scales, even in curved spacetime.
Galileo Galilei had already postulated that there is no absolute and well-defined state of rest (no privileged reference frames), a principle now called Galileo's principle of relativity. Einstein extended this principle so that it accounted for the constant speed of light, a phenomenon that had been recently observed in the Michelson–Morley experiment. He also postulated that it holds for all the laws of physics, including both the laws of mechanics and of electrodynamics.
Postulates.
Einstein discerned two fundamental propositions that seemed to be the most assured, regardless of the exact validity of the (then) known laws of either mechanics or electrodynamics. These propositions were the constancy of the speed of light and the independence of physical laws (especially the constancy of the speed of light) from the choice of inertial system. In his initial presentation of special relativity in 1905 he expressed these postulates as:
The derivation of special relativity depends not only on these two explicit postulates, but also on several tacit assumptions (made in almost all theories of physics), including the isotropy and homogeneity of space and the independence of measuring rods and clocks from their past history.
Following Einstein's original presentation of special relativity in 1905, many different sets of postulates have been proposed in various alternative derivations. However, the most common set of postulates remains those employed by Einstein in his original paper. A more mathematical statement of the Principle of Relativity made later by Einstein, which introduces the concept of simplicity not mentioned above is:
Henri Poincaré provided the mathematical framework for relativity theory by proving that Lorentz transformations are a subset of his Poincaré group of symmetry transformations. Einstein later derived these transformations from his axioms.
Many of Einstein's papers present derivations of the Lorentz transformation based upon these two principles.
Einstein consistently based the derivation of Lorentz invariance (the essential core of special relativity) on just the two basic principles of relativity and light-speed invariance. He wrote:
Thus many modern treatments of special relativity base it on the single postulate of universal Lorentz covariance, or, equivalently, on the single postulate of Minkowski spacetime.
From the principle of relativity alone without assuming the constancy of the speed of light (i.e. using the isotropy of space and the symmetry implied by the principle of special relativity) one can show that the spacetime transformations between inertial frames are either Euclidean, Galilean, or Lorentzian. In the Lorentzian case, one can then obtain relativistic interval conservation and a certain finite limiting speed. Experiments suggest that this speed is the speed of light in vacuum.
The constancy of the speed of light was motivated by Maxwell's theory of electromagnetism and the lack of evidence for the luminiferous ether. There is conflicting evidence on the extent to which Einstein was influenced by the null result of the Michelson–Morley experiment. In any case, the null result of the Michelson–Morley experiment helped the notion of the constancy of the speed of light gain widespread and rapid acceptance.
Lack of an absolute reference frame.
The principle of relativity, which states that there is no preferred inertial reference frame, dates back to Galileo, and was incorporated into Newtonian physics. However, in the late 19th century, the existence of electromagnetic waves led physicists to suggest that the universe was filled with a substance that they called "aether", which would act as the medium through which these waves, or vibrations travelled. The aether was thought to constitute an absolute reference frame against which speeds could be measured, and could be considered fixed and motionless. Aether supposedly possessed some wonderful properties: it was sufficiently elastic to support electromagnetic waves, and those waves could interact with matter, yet it offered no resistance to bodies passing through it. The results of various experiments, including the Michelson–Morley experiment, led to the theory of special relativity, by showing that there was no aether. Einstein's solution was to discard the notion of an aether and the absolute state of rest. In relativity, any reference frame moving with uniform motion will observe the same laws of physics. In particular, the speed of light in vacuum is always measured to be "c", even when measured by multiple systems that are moving at different (but constant) velocities.
Reference frames, coordinates and the Lorentz transformation.
Reference frames play a crucial role in relativity theory. The term reference frame as used here is an observational perspective in space which is not undergoing any change in motion (acceleration), from which a position can be measured along 3 spatial axes. In addition, a reference frame has the ability to determine measurements of the time of events using a 'clock' (any reference device with uniform periodicity).
An event is an occurrence that can be assigned a single unique time and location in space relative to a reference frame: it is a "point" in spacetime. Since the speed of light is constant in relativity in each and every reference frame, pulses of light can be used to unambiguously measure distances and refer back the times that events occurred to the clock, even though light takes time to reach the clock after the event has transpired.
For example, the explosion of a firecracker may be considered to be an "event". We can completely specify an event by its four spacetime coordinates: The time of occurrence and its 3-dimensional spatial location define a reference point. Let's call this reference frame "S".
In relativity theory we often want to calculate the position of a point from a different reference point.
Suppose we have a second reference frame "S"′, whose spatial axes and clock exactly coincide with that of "S" at time zero, but it is moving at a constant velocity "v" with respect to "S" along the "x"-axis.
Since there is no absolute reference frame in relativity theory, a concept of 'moving' doesn't strictly exist, as everything is always moving with respect to some other reference frame. Instead, any two frames that move at the same speed in the same direction are said to be "comoving". Therefore, "S" and "S"′ are not "comoving".
Define the event to have spacetime coordinates in system "S" and in "S"′. Then the Lorentz transformation specifies that these coordinates are related in the following way:
where
is the Lorentz factor and "c" is the speed of light in vacuum, and the velocity "v" of "S"′ is parallel to the "x"-axis. The "y" and "z" coordinates are unaffected; only the "x" and "t" coordinates are transformed. These Lorentz transformations form a one-parameter group of linear mappings, that parameter being called rapidity.
There is nothing special about the "x"-axis, the transformation can apply to the "y" or "z" axes, or indeed in any direction, which can be done by directions parallel to the motion (which are warped by the γ factor) and perpendicular; see main article for details.
A quantity invariant under Lorentz transformations is known as a Lorentz scalar.
Writing the Lorentz transformation and its inverse in terms of coordinate differences, where for instance one event has coordinates and , another event has coordinates and , and the differences are defined as
we get
These effects are not merely appearances; they are explicitly related to our way of measuring "time intervals" between events which occur at the same place in a given coordinate system (called "co-local" events). These time intervals will be "different" in another coordinate system moving with respect to the first, unless the events are also simultaneous. Similarly, these effects also relate to our measured distances between separated but simultaneous events in a given coordinate system of choice. If these events are not co-local, but are separated by distance (space), they will "not" occur at the same "spatial distance" from each other when seen from another moving coordinate system. However, the spacetime interval will be the same for all observers. The underlying reality remains the same. Only our perspective changes.
Consequences derived from the Lorentz transformation.
The consequences of special relativity can be derived from the Lorentz transformation equations. These transformations, and hence special relativity, lead to different physical predictions than those of Newtonian mechanics when relative velocities become comparable to the speed of light. The speed of light is so much larger than anything humans encounter that some of the effects
predicted by relativity are initially counterintuitive.
Relativity of simultaneity.
Two events happening in two different locations that occur simultaneously in the reference frame of one inertial observer, may occur non-simultaneously in the reference frame of another inertial observer (lack of absolute simultaneity).
From the first equation of the Lorentz transformation in terms of coordinate differences
it is clear that two events that are simultaneous in frame "S" (satisfying ), are not necessarily simultaneous in another inertial frame "S"′ (satisfying ). Only if these events are additionally co-local in frame "S" (satisfying ), will they be simultaneous in another frame "S"′.
Time dilation.
The time lapse between two events is not invariant from one observer to another, but is dependent on the relative speeds of the observers' reference frames (e.g., the twin paradox which concerns a twin who flies off in a spaceship traveling near the speed of light and returns to discover that his or her twin sibling has aged much more).
Suppose a clock is at rest in the unprimed system "S". The location of the clock on two different ticks is then characterized by . To find the relation between the times between these ticks as measured in both systems, the first equation can be used to find:
This shows that the time (Δ"t"′) between the two ticks as seen in the frame in which the clock is moving ("S"′), is "longer" than the time (Δ"t") between these ticks as measured in the rest frame of the clock ("S"). Time dilation explains a number of physical phenomena; for example, the decay rate of muons produced by cosmic rays impinging on the Earth's atmosphere.
Length contraction.
The dimensions (e.g., length) of an object as measured by one observer may be smaller than the results of measurements of the same object made by another observer (e.g., the ladder paradox involves a long ladder traveling near the speed of light and being contained within a smaller garage).
Similarly, suppose a measuring rod is at rest and aligned along the x-axis in the unprimed system "S". In this system, the length of this rod is written as Δ"x". To measure the length of this rod in the system "S"′, in which the clock is moving, the distances "x"′ to the end points of the rod must be measured simultaneously in that system "S"′. In other words, the measurement is characterized by , which can be combined with the fourth equation to find the relation between the lengths Δ"x" and Δ"x"′:
This shows that the length (Δ"x"′) of the rod as measured in the frame in which it is moving ("S"′), is "shorter" than its length (Δ"x") in its own rest frame ("S").
Composition of velocities.
Velocities (speeds) do not simply add. If the observer in "S" measures an object moving along the "x" axis at velocity "u", then the observer in the "S"′ system, a frame of reference moving at velocity "v" in the "x" direction with respect to "S", will measure the object moving with velocity "u"′ where (from the Lorentz transformations above):
The other frame "S" will measure:
Notice that if the object were moving at the speed of light in the "S" system (i.e. "u" = "c"), then it would also be moving at the speed of light in the "S"′ system. Also, if both "u" and "v" are small with respect to the speed of light, we will recover the intuitive Galilean transformation of velocities
The usual example given is that of a train (frame "S"′ above) traveling due east with a velocity "v" with respect to the tracks (frame "S"). A child inside the train throws a baseball due east with a velocity "u"′ with respect to the train. In nonrelativistic physics, an observer at rest on the tracks will measure the velocity of the baseball (due east) as , while in special relativity this is no longer true; instead the velocity of the baseball (due east) is given by the second equation: . Again, there is nothing special about the "x" or east directions. This formalism applies to any direction by considering parallel and perpendicular motion to the direction of relative velocity "v", see main article for details.
Other consequences.
Thomas rotation.
The orientation of an object (i.e. the alignment of its axes with the observer's axes) may be different for different observers. Unlike other relativistic effects, this effect becomes quite significant at fairly low velocities as can be seen in the spin of moving particles.
Equivalence of mass and energy.
As an object's speed approaches the speed of light from an observer's point of view, its relativistic mass increases thereby making it more and more difficult to accelerate it from within the observer's frame of reference.
The energy content of an object at rest with mass "m" equals "mc"2. Conservation of energy implies that, in any reaction, a decrease of the sum of the masses of particles must be accompanied by an increase in kinetic energies of the particles after the reaction. Similarly, the mass of an object can be increased by taking in kinetic energies.
In addition to the papers referenced above—which give derivations of the Lorentz transformation and describe the foundations of special relativity—Einstein also wrote at least four papers giving heuristic arguments for the equivalence (and transmutability) of mass and energy, for .
Mass–energy equivalence is a consequence of special relativity. The energy and momentum, which are separate in Newtonian mechanics, form a four-vector in relativity, and this relates the time component (the energy) to the space components (the momentum) in a nontrivial way. For an object at rest, the energy–momentum four-vector is : it has a time component which is the energy, and three space components which are zero. By changing frames with a Lorentz transformation in the x direction with a small value of the velocity v, the energy momentum four-vector becomes . The momentum is equal to the energy multiplied by the velocity divided by "c"2. As such, the Newtonian mass of an object, which is the ratio of the momentum to the velocity for slow velocities, is equal to "E"/"c"2.
The energy and momentum are properties of matter and radiation, and it is impossible to deduce that they form a four-vector just from the two basic postulates of special relativity by themselves, because these don't talk about matter or radiation, they only talk about space and time. The derivation therefore requires some additional physical reasoning. In his 1905 paper, Einstein used the additional principles that Newtonian mechanics should hold for slow velocities, so that there is one energy scalar and one three-vector momentum at slow velocities, and that the conservation law for energy and momentum is exactly true in relativity. Furthermore, he assumed that the energy of light is transformed by the same Doppler-shift factor as its frequency, which he had previously shown to be true based on Maxwell's equations. The first of Einstein's papers on this subject was "Does the Inertia of a Body Depend upon its Energy Content?" in 1905. Although Einstein's argument in this paper is nearly universally accepted by physicists as correct, even self-evident, many authors over the years have suggested that it is wrong. Other authors suggest that the argument was merely inconclusive because it relied on some implicit assumptions.
Einstein acknowledged the controversy over his derivation in his 1907 survey paper on special relativity. There he notes that it is problematic to rely on Maxwell's equations for the heuristic mass–energy argument. The argument in his 1905 paper can be carried out with the emission of any massless particles, but the Maxwell equations are implicitly used to make it obvious that the emission of light in particular can be achieved only by doing work. To emit electromagnetic waves, all you have to do is shake a charged particle, and this is clearly doing work, so that the emission is of energy.
How far can one travel from the Earth?
Since one can not travel faster than light, one might conclude that a human can never travel farther from Earth than 40 light years if the traveler is active between the age of 20 and 60. One would easily think that a traveler would never be able to reach more than the very few solar systems which exist within the limit of 20–40 light years from the earth. But that would be a mistaken conclusion. Because of time dilation, a hypothetical spaceship can travel thousands of light years during the pilot's 40 active years. If a spaceship could be built that accelerates at a constant 1 "g", it will after a little less than a year be traveling at almost the speed of light as seen from Earth. Time dilation will increase his life span as seen from the reference system of the Earth, but his lifespan measured by a clock traveling with him will not thereby change. During his journey, people on Earth will experience more time than he does. A 5-year round trip for him will take 6½ Earth years and cover a distance of over 6 light-years. A 20-year round trip for him (5 years accelerating, 5 decelerating, twice each) will land him back on Earth having traveled for 335 Earth years and a distance of 331 light years. A full 40-year trip at 1 "g" will appear on Earth to last 58,000 years and cover a distance of 55,000 light years. A 40-year trip at 1.1 "g" will take 148,000 Earth years and cover about 140,000 light years. A one-way 28 year (14 years accelerating, 14 decelerating as measured with the cosmonaut's clock) trip at 1 "g" acceleration could reach 2,000,000 light-years to the Andromeda Galaxy. This same time dilation is why a muon traveling close to "c" is observed to travel much further than "c" times its half-life (when at rest).
Causality and prohibition of motion faster than light.
In diagram 2 the interval AB is 'time-like'; i.e., there is a frame of reference in which events A and B occur at the same location in space, separated only by occurring at different times. If A precedes B in that frame, then A precedes B in all frames. It is hypothetically possible for matter (or information) to travel from A to B, so there can be a causal relationship (with A the cause and B the effect).
The interval AC in the diagram is 'space-like'; i.e., there is a frame of reference in which events A and C occur simultaneously, separated only in space. There are also frames in which A precedes C (as shown) and frames in which C precedes A. If it were possible for a cause-and-effect relationship to exist between events A and C, then paradoxes of causality would result. For example, if A was the cause, and C the effect, then there would be frames of reference in which the effect preceded the cause. Although this in itself won't give rise to a paradox, one can show that faster than light signals can be sent back into one's own past. A causal paradox can then be constructed by sending the signal if and only if no signal was received previously.
Therefore, if causality is to be preserved, one of the consequences of special relativity is that no information signal or material object can travel faster than light in vacuum. However, some "things" can still move faster than light. For example, the location where the beam of a search light hits the bottom of a cloud can move faster than light when the search light is turned rapidly.
Even without considerations of causality, there are other strong reasons why faster-than-light travel is forbidden by special relativity. For example, if a constant force is applied to an object for a limitless amount of time, then integrating gives a momentum that grows without bound, but this is simply because formula_13 approaches infinity as formula_14 approaches "c". To an observer who is not accelerating, it appears as though the object's inertia is increasing, so as to produce a smaller acceleration in response to the same force. This behavior is observed in particle accelerators, where each charged particle is accelerated by the electromagnetic force.
Geometry of spacetime.
Comparison between flat Euclidean space and Minkowski space.
Special relativity uses a 'flat' 4-dimensional Minkowski space – an example of a spacetime. Minkowski spacetime appears to be very similar to the standard 3-dimensional Euclidean space, but there is a crucial difference with respect to time.
In 3D space, the differential of distance (line element) "ds" is defined by
where are the differentials of the three spatial dimensions. In Minkowski geometry, there is an extra dimension with coordinate "X"0 derived from time, such that the distance differential fulfills
where are the differentials of the four spacetime dimensions. This suggests a deep theoretical insight: special relativity is simply a rotational symmetry of our spacetime, analogous to the rotational symmetry of Euclidean space (see image right). Just as Euclidean space uses a Euclidean metric, so spacetime uses a Minkowski metric. Basically, special relativity can be stated as the "invariance of any spacetime interval" (that is the 4D distance between any two events) when viewed from "any inertial reference frame". All equations and effects of special relativity can be derived from this rotational symmetry (the Poincaré group) of Minkowski spacetime.
The actual form of "ds" above depends on the metric and on the choices for the "X"0 coordinate.
To make the time coordinate look like the space coordinates, it can be treated as imaginary: (this is called a Wick rotation).
According to Misner, Thorne and Wheeler (1971, §2.3), ultimately the deeper understanding of both special and general relativity will come from the study of the Minkowski metric (described below) and to take , rather than a "disguised" Euclidean metric using "ict" as the time coordinate.
Some authors use , with factors of "c" elsewhere to compensate; for instance, spatial coordinates are divided by "c" or factors of "c"±2 are included in the metric tensor.
These numerous conventions can be superseded by using natural units where . Then space and time have equivalent units, and no factors of "c" appear anywhere.
3D spacetime.
If we reduce the spatial dimensions to 2, so that we can represent the physics in a 3D space
we see that the null geodesics lie along a dual-cone (see image right) defined by the equation;
or simply
 which is the equation of a circle of radius "c dt".
4D spacetime.
If we extend this to three spatial dimensions, the null geodesics are the 4-dimensional cone:
so
This null dual-cone represents the "line of sight" of a point in space. That is, when we look at the stars and say "The light from that star which I am receiving is X years old", we are looking down this line of sight: a null geodesic. We are looking at an event a distance formula_22 away and a time "d/c" in the past. For this reason the null dual cone is also known as the 'light cone'. (The point in the lower left of the picture above right represents the star, the origin represents the observer, and the line represents the null geodesic "line of sight".)
The cone in the −"t" region is the information that the point is 'receiving', while the cone in the +"t" section is the information that the point is 'sending'.
The geometry of Minkowski space can be depicted using Minkowski diagrams, which are useful also in understanding many of the thought-experiments in special relativity.
Note that, in 4d spacetime, the concept of the center of mass becomes more complicated, see center of mass (relativistic).
Physics in spacetime.
Transformations of physical quantities between reference frames.
Above, the Lorentz transformation for the time coordinate and three space coordinates illustrates that they are intertwined. This is true more generally: certain pairs of "timelike" and "spacelike" quantities naturally combine on equal footing under the same Lorentz transformation.
The Lorentz transformation in standard configuration above, i.e. for a boost in the "x" direction, can be recast into matrix form as follows:
In Newtonian mechanics, quantities which have magnitude and direction are mathematically described as 3d vectors in Euclidean space, and in general they are parametrized by time. In special relativity, this notion is extended by adding the appropriate timelike quantity to a spacelike vector quantity, and we have 4d vectors, or "four vectors", in Minkowski spacetime. The components of vectors are written using tensor index notation, as this has numerous advantages. The notation makes it clear the equations are manifestly covariant under the Poincaré group, thus bypassing the tedious calculations to check this fact. In constructing such equations, we often find that equations previously thought to be unrelated are, in fact, closely connected being part of the same tensor equation. Recognizing other physical quantities as tensors simplifies their transformation laws. Throughout, upper indices (superscripts) are contravariant indices rather than exponents except when they indicate a square (this is should be clear from the context), and lower indices (subscripts) are covariant indices. For simplicity and consistency with the earlier equations, Cartesian coordinates will be used.
The simplest example of a four-vector is the position of an event in spacetime, which constitutes a timelike component "ct" and spacelike component , in a contravariant position four vector with components:
where we define so that the time coordinate has the same dimension of distance as the other spatial dimensions; so that space and time are treated equally. Now the transformation of the contravariant components of the position 4-vector can be compactly written as:
where there is an implied summation on "ν" from 0 to 3, and formula_26 is a matrix.
More generally, all contravariant components of a four-vector formula_27 transform from one frame to another frame by a Lorentz transformation:
Examples of other 4-vectors include the four-velocity "U"μ, defined as the derivative of the position 4-vector with respect to proper time:
where the Lorentz factor is:
The relativistic energy formula_31 and relativistic momentum formula_32 of an object are respectively the timelike and spacelike components of a contravariant four momentum vector:
where "m" is the invariant mass.
The four-acceleration is the proper time derivative of 4-velocity:
The transformation rules for "three"-dimensional velocities and accelerations are very awkward; even above in standard configuration the velocity equations are quite complicated owing to their non-linearity. On the other hand, the transformation of "four"-velocity and "four"-acceleration are simpler by means of the Lorentz transformation matrix.
The four-gradient of a scalar field φ transforms covariantly rather than contravariantly:
that is:
only in Cartesian coordinates. It's the covariant derivative which transforms in manifest covariance, in Cartesian coordinates this happens to reduce to the partial derivatives, but not in other coordinates.
More generally, the "co"variant components of a 4-vector transform according to the "inverse" Lorentz transformation:
where formula_38 is the reciprocal matrix of formula_26.
The postulates of special relativity constrain the exact form the Lorentz transformation matrices take.
More generally, most physical quantities are best described as (components of) tensors. So to transform from one frame to another, we use the well-known tensor transformation law
where formula_41 is the reciprocal matrix of formula_42. All tensors transform by this rule.
An example of a four dimensional second order antisymmetric tensor is the relativistic angular momentum, which has six components: three are the classical angular momentum, and the other three are related to the boost of the center of mass of the system. The derivative of the relativistic angular momentum with respect to proper time is the relativistic torque, also second order antisymmetric tensor.
The electromagnetic field tensor is another second order antisymmetric tensor field, with six components: three for the electric field and another three for the magnetic field. There is also the stress–energy tensor for the electromagnetic field, namely the electromagnetic stress–energy tensor.
Metric.
The metric tensor allows one to define the inner product of two vectors, which in turn allows one to assign a magnitude to the vector. Given the four-dimensional nature of spacetime the Minkowski metric "η" has components (valid in any inertial reference frame) which can be arranged in a matrix:
which is equal to its reciprocal, formula_44, in those frames. Throughout we use the signs as above, different authors use different conventions – see Minkowski metric alternative signs.
The Poincaré group is the most general group of transformations which preserves the Minkowski metric:
and this is the physical symmetry underlying special relativity.
The metric can be used for raising and lowering indices on vectors and tensors. Invariants can be constructed using the metric, the inner product of a 4-vector "T" with another 4-vector "S" is:
Invariant means that it takes the same value in all inertial frames, because it is a scalar (0 rank tensor), and so no Λ appears in its trivial transformation. The magnitude of the 4-vector "T" is the positive square root of the inner product with itself:
One can extend this idea to tensors of higher order, for a second order tensor we can form the invariants:
similarly for higher order tensors. Invariant expressions, particularly inner products of 4-vectors with themselves, provide equations that are useful for calculations, because one doesn't need to perform Lorentz transformations to determine the invariants.
Relativistic kinematics and invariance.
The coordinate differentials transform also contravariantly:
so the squared length of the differential of the position four-vector "dXμ" constructed using
is an invariant. Notice that when the line element "dX2 is negative that is the differential of proper time, while when "dX2 is positive, is differential of the proper distance.
The 4-velocity "U"μ has an invariant form:
which means all velocity four-vectors have a magnitude of "c". This is an expression of the fact that there is no such thing as being at coordinate rest in relativity: at the least, you are always moving forward through time. Differentiating the above equation by "τ" produces:
So in special relativity, the acceleration four-vector and the velocity four-vector are orthogonal.
Relativistic dynamics and invariance.
The invariant magnitude of the momentum 4-vector generates the energy–momentum relation:
We can work out what this invariant is by first arguing that, since it is a scalar, it doesn't matter in which reference frame we calculate it, and then by transforming to a frame where the total momentum is zero.
We see that the rest energy is an independent invariant. A rest energy can be calculated even for particles and systems in motion, by translating to a frame in which momentum is zero.
The rest energy is related to the mass according to the celebrated equation discussed above:
Note that the mass of systems measured in their center of momentum frame (where total momentum is zero) is given by the total energy of the system in this frame. It may not be equal to the sum of individual system masses measured in other frames.
To use Newton's third law of motion, both forces must be defined as the rate of change of momentum with respect to the same time coordinate. That is, it requires the 3D force defined above. Unfortunately, there is no tensor in 4D which contains the components of the 3D force vector among its components.
If a particle is not traveling at "c", one can transform the 3D force from the particle's co-moving reference frame into the observer's reference frame. This yields a 4-vector called the four-force. It is the rate of change of the above energy momentum four-vector with respect to proper time. The covariant version of the four-force is:
In the rest frame of the object, the time component of the four force is zero unless the "invariant mass" of the object is changing (this requires a non-closed system in which energy/mass is being directly added or removed from the object) in which case it is the negative of that rate of change of mass, times "c". In general, though, the components of the four force are not equal to the components of the three-force, because the three force is defined by the rate of change of momentum with respect to coordinate time, i.e. "dp"/"dt" while the four force is defined by the rate of change of momentum with respect to proper time, i.e. "dp"/"d"τ.
In a continuous medium, the 3D "density of force" combines with the "density of power" to form a covariant 4-vector. The spatial part is the result of dividing the force on a small cell (in 3-space) by the volume of that cell. The time component is −1/"c" times the power transferred to that cell divided by the volume of the cell. This will be used below in the section on electromagnetism.
Relativity and unifying electromagnetism.
Theoretical investigation in classical electromagnetism led to the discovery of wave propagation. Equations generalizing the electromagnetic effects found that finite propagation speed of the E and B fields required certain behaviors on charged particles. The general study of moving charges forms the Liénard–Wiechert potential, which is a step towards special relativity.
The Lorentz transformation of the electric field of a moving charge into a non-moving observer's reference frame results in the appearance of a mathematical term commonly called the magnetic field. Conversely, the "magnetic" field generated by a moving charge disappears and becomes a purely "electrostatic" field in a comoving frame of reference. Maxwell's equations are thus simply an empirical fit to special relativistic effects in a classical model of the Universe. As electric and magnetic fields are reference frame dependent and thus intertwined, one speaks of "electromagnetic" fields. Special relativity provides the transformation rules for how an electromagnetic field in one inertial frame appears in another inertial frame.
Maxwell's equations in the 3D form are already consistent with the physical content of special relativity, although they are easier to manipulate in a manifestly covariant form, i.e. in the language of tensor calculus. See main links for more detail.
Status.
Special relativity in its Minkowski spacetime is accurate only when the absolute value of the gravitational potential is much less than "c"2 in the region of interest. In a strong gravitational field, one must use general relativity. General relativity becomes special relativity at the limit of weak field. At very small scales, such as at the Planck length and below, quantum effects must be taken into consideration resulting in quantum gravity. However, at macroscopic scales and in the absence of strong gravitational fields, special relativity is experimentally tested to extremely high degree of accuracy (10−20)
and thus accepted by the physics community. Experimental results which appear to contradict it are not reproducible and are thus widely believed to be due to experimental errors.
Special relativity is mathematically self-consistent, and it is an organic part of all modern physical theories, most notably quantum field theory, string theory, and general relativity (in the limiting case of negligible gravitational fields).
Newtonian mechanics mathematically follows from special relativity at small velocities (compared to the speed of light) – thus Newtonian mechanics can be considered as a special relativity of slow moving bodies. See classical mechanics for a more detailed discussion.
Several experiments predating Einstein's 1905 paper are now interpreted as evidence for relativity. Of these it is known Einstein was aware of the Fizeau experiment before 1905, and historians have concluded that Einstein was at least aware of the Michelson–Morley experiment as early as 1899 despite claims he made in his later years that it played no role in his development of the theory.
Particle accelerators routinely accelerate and measure the properties of particles moving at near the speed of light, where their behavior is completely consistent with relativity theory and inconsistent with the earlier Newtonian mechanics. These machines would simply not work if they were not engineered according to relativistic principles. In addition, a considerable number of modern experiments have been conducted to test special relativity. Some examples:
Theories of relativity and quantum mechanics.
"Special" relativity can be combined with quantum mechanics to form relativistic quantum mechanics. It is an unsolved problem in physics how "general" relativity and quantum mechanics can be unified; quantum gravity and a "theory of everything", which require such a unification, are active and ongoing areas in theoretical research.
The early Bohr–Sommerfeld atomic model explained the fine structure of alkali metal atoms using both special relativity and the preliminary knowledge on quantum mechanics of the time.
In 1928, Paul Dirac constructed an influential relativistic wave equation, now known as the Dirac equation in his honour, that is fully compatible both with special relativity and with the final version of quantum theory existing after 1926. This equation explained not only the intrinsic angular momentum of the electrons called "spin", it also led to the prediction of the antiparticle of the electron (the positron), and fine structure could only be fully explained with special relativity. It was the first foundation of "relativistic quantum mechanics". In non-relativistic quantum mechanics, spin is and cannot be explained.
On the other hand, the existence of antiparticles leads to the conclusion that relativistic quantum mechanics is not enough for a more accurate and complete theory of particle interactions. Instead, a theory of particles interpreted as quantized fields, called "quantum field theory", becomes necessary; in which particles can be created and destroyed throughout space and time.

</doc>
<doc id="173997" url="https://en.wikipedia.org/wiki?curid=173997" title="Special unitary group">
Special unitary group

In mathematics, the special unitary group of degree , denoted , is the Lie group of unitary matrices with determinant 1 (i.e., real-valued determinant, not complex as for general unitary matrices). The group operation is that of matrix multiplication. The special unitary group is a subgroup of the unitary group , consisting of all unitary matrices. As a compact classical group, is the group that preserves the standard inner product on . It is itself a subgroup of the general linear group, .
The groups find wide application in the Standard Model of particle physics, especially in the electroweak interaction and in quantum chromodynamics.
The simplest case, , is the trivial group, having only a single element. The group is isomorphic to the group of quaternions of norm 1, and is thus diffeomorphic to the 3-sphere. Since unit quaternions can be used to represent rotations in 3-dimensional space (up to sign), there is a surjective homomorphism from to the rotation group whose kernel is }. is also identical to one of the symmetry groups of spinors, Spin(3), that enables a spinor presentation of rotations.
Properties.
The special unitary group is a real Lie group (though not a complex Lie group). Its dimension as a real manifold is . Topologically, it is compact and simply connected. Algebraically, it is a simple Lie group (meaning its Lie algebra is simple; see below). 
The center of is isomorphic to the cyclic group , and is composed of the diagonal matrices for an th root of unity and the "n"×"n" identity matrix. 
Its outer automorphism group, for , is , while the outer automorphism group of is the trivial group.
A maximal torus, of rank , is given by the set of diagonal matrices with determinant 1. The Weyl group
is the symmetric group , which is represented by signed permutation matrices (the signs being necessary to ensure the determinant is 1).
The Lie algebra of , denoted by , can be identified with the set of traceless antihermitian complex matrices, with the regular commutator as Lie bracket. Particle physicists often use a different, equivalent representation: the set of traceless hermitian complex matrices with Lie bracket given by times the commutator.
Infinitesimal generators.
The Lie algebra can be generated by operators formula_1, 
, which satisfy the commutator relationships
for = , where denotes the Kronecker delta. Additionally, the operator
satisfies
which implies that the number of "independent" generators of the Lie algebra is .
Fundamental representation.
In the defining, or fundamental, representation of the generators are represented by traceless hermitian matrices complex matrices, where:
where the are the structure constants and are antisymmetric in all indices, while the -coefficients are symmetric in all indices.
As a consequence:
We also take
as a normalization convention.
Adjoint representation.
In the -dimensional adjoint representation, the generators are represented by × matrices, whose elements are defined by the structure constants themselves:
"n" = 2.
 is the following group,
where the overline denotes complex conjugation. 
Now, consider the following map,
where denotes the set of 2 by 2 complex matrices. By considering diffeomorphic to and diffeomorphic to , we can see that is an injective real linear map and hence an embedding. Now, considering the restriction of to the 3-sphere (since modulus is 1), denoted , we can see that this is an embedding of the 3-sphere onto a compact submanifold of . However, it is also clear that . 
Therefore, as a manifold is diffeomorphic to and so is a compact, connected Lie group.
The Lie algebra of is
It is easily verified that matrices of this form have trace zero and are antihermitian. The Lie algebra is then generated by the following matrices,
which are easily seen to have the form of the general element specified above. 
These satisfy and . The commutator bracket is therefore specified by
The above generators are related to the Pauli matrices by and . This representation is routinely used in quantum mechanics to represent the spin of fundamental particles such as electrons. They also serve as unit vectors for the description of our 3 spatial dimensions in loop quantum gravity.
The Lie algebra serves to work out the representations of. 
"n" = 3.
The generators of , , in the defining representation, are:
where the Gell-Mann matrices, are the analog of the Pauli matrices for :
These formula_16 span all traceless Hermitian matrices of the Lie algebra, as required.
They obey the relations
The are the structure constants of the Lie algebra, given by:
while all other formula_23 not related to these by permutation are zero.
The symmetric coefficients take the values:
As a topological space, "SU(3)" is a direct product of a 3-sphere and a 5-sphere, "S3⊗ S5".
A generic "SU(3)" group element generated by a traceless 3×3 hermitian matrix , normalized as , is given by
where 
 for elementary representation theory facts.
Lie algebra structure.
The above representation bases generalize to , using generalized Pauli matrices.
If we choose an (arbitrary) particular basis, then the subspace of traceless diagonal matrices with imaginary entries forms an -dimensional Cartan subalgebra.
Complexify the Lie algebra, so that any traceless matrix is now allowed. The weight eigenvectors are the Cartan subalgebra itself, as well as the matrices with only one nonzero entry which is off diagonal. Even though the Cartan subalgebra is only , to simplify calculations, it is often convenient to introduce an auxiliary element, the unit matrix which commutes with everything else (which is not an element of the Lie algebra!) for the purpose of computing weights—and that only. So, we have a basis where the -th basis vector is the matrix with 1 on the -th diagonal entry and zero elsewhere. Weights would then be given by coordinates and the sum over all coordinates has to be zero (because the unit matrix is only auxiliary).
So, is of rank and its Dynkin diagram is given by , a chain of vertices, o−o−o−o---o. Its root system consists of roots spanning a Euclidean space. Here, we use redundant coordinates instead of to emphasize the symmetries of the root system (the coordinates have to add up to zero). 
In other words, we are embedding this dimensional vector space in an -dimensional one. Thus, the roots consists of all the permutations of . The construction given above explains why. A choice of simple roots is
Its Cartan matrix is
Its Weyl group or Coxeter group is the symmetric group , the symmetry group of the -simplex.
Generalized special unitary group.
For a field , the generalized special unitary group over "F", , is the group of all linear transformations of determinant 1 of a vector space of rank over which leave invariant a nondegenerate, Hermitian form of signature . This group is often referred to as the special unitary group of signature over . The field can be replaced by a commutative ring, in which case the vector space is replaced by a free module.
Specifically, fix a Hermitian matrix of signature in , then all
satisfy
Often one will see the notation without reference to a ring or field; in this case, the ring or field being referred to is and this gives one of the classical Lie groups. The standard choice for when is
However there may be better choices for for certain dimensions which exhibit more behaviour under restriction to subrings of .
Example.
An important example of this type of group is the Picard modular group which acts (projectively) on complex hyperbolic space of degree two, in the same way that acts (projectively) on real hyperbolic space of dimension two. In 2005 Gábor Francsics and Peter Lax computed an explicit fundamental domain for the action of this group on . 
A further example is , which is isomorphic to .
Important subgroups.
In physics the special unitary group is used to represent bosonic symmetries. In theories of symmetry breaking it is important to be able to find the subgroups of the special unitary group. Subgroups of that are important in GUT physics are, for ,
where × denotes the direct product and , known as the circle group, is the multiplicative group of all complex numbers with absolute value 1.
For completeness, there are also the orthogonal and symplectic subgroups,
Since the rank of is and of is 1, a useful check is that the sum of the ranks of the subgroups is less than or equal to the rank of the original group. is a subgroup of various other Lie groups,
See spin group, and simple Lie groups for E6, E7, and G2.
There are also the accidental isomorphisms: , , and .
One may finally mention that is the double covering group of , a relation that plays an important role in the theory of rotations of 2-spinors in non-relativistic quantum mechanics.

</doc>
<doc id="28748" url="https://en.wikipedia.org/wiki?curid=28748" title="Speed">
Speed

In everyday use and in kinematics, the speed of an object is the magnitude of its velocity (the rate of change of its position); it is thus a scalar quantity. The average speed of an object in an interval of time is the distance travelled by the object divided by the duration of the interval; the instantaneous speed is the limit of the average speed as the duration of the time interval approaches zero.
Like velocity, speed has the dimensions of a length divided by a time; the SI unit of speed is the metre per second, but the most common unit of speed in everyday usage is the kilometre per hour or, in the US and the UK, miles per hour. For air and marine travel the knot is commonly used.
The fastest possible speed at which energy or information can travel, according to special relativity, is the speed of light in a vacuum "c" = metres per second (approximately or ). Matter cannot quite reach the speed of light, as this would require an infinite amount of energy. In relativity physics, the concept of rapidity replaces the classical idea of speed.
Definition.
The Italian physicist Galileo Galilei is credited with being the first to measure speed by considering the distance covered and the time it takes. Galileo defined speed as the distance covered per unit of time. In equation form, this is
where formula_2 is speed, formula_3 is distance, and formula_4 is time. A cyclist who covers 30 metres in a time of 2 seconds, for example, has a speed of 15 metres per second. Objects in motion often have variations in speed (a car might travel along a street at 50 km/h, slow to 0 km/h, and then reach 30 km/h).
In mathematical terms, the speed formula_2 is defined as the magnitude of the velocity formula_6, that is, the derivative of the position formula_7 with respect to time:
If formula_9 is the length of the path travelled until time formula_4, the speed equals the time derivative of formula_9:
In the special case where the velocity is constant (that is, constant speed in a straight line), this can be simplified to formula_13. The average speed over a finite time interval is the total distance travelled divided by the time duration.
Instantaneous speed.
By looking at a speedometer, one can read the speed of a car at any instant, or its "instantaneous speed". A car travelling at 50 km/h generally goes for less than one hour at a constant speed, but if it did go at that speed for a full hour, it would travel 50 km. If the vehicle continued at that speed for half an hour, it would cover half that distance (25 km). If it continued for only one minute, it would cover about 833 m.
Average speed.
Different from instantaneous speed, "average speed" is defined as the total distance covered over the time interval. For example, if a distance of 80 kilometres is driven in 1 hour, the average speed is 80 kilometres per hour. Likewise, if 320 kilometres are travelled in 4 hours, the average speed is also 80 kilometres per hour. When a distance in kilometres (km) is divided by a time in hours (h), the result is in kilometres per hour (km/h). Average speed does not describe the speed variations that may have taken place during shorter time intervals (as it is the entire distance covered divided by the total time of travel), and so average speed is often quite different from a value of instantaneous speed. If the average speed and the time of travel are known, the distance travelled can be calculated by rearranging the definition to
Using this equation for an average speed of 80 kilometres per hour on a 4-hour trip, the distance covered is found to be 320 kilometres. 
Expressed in graphical language, the slope of a tangent line at any point of a distance-time graph is the instantaneous speed at this point, while the slope of a chord line of the same graph is the average speed during the time interval covered by the chord.
Tangential speed.
Linear speed is the distance travelled per unit of time, while tangential speed (or tangential velocity) is the linear speed of something moving along a circular path. A point on the outside edge of a merry-go-round or turntable travels a greater distance in one complete rotation than a point nearer the center. Travelling a greater distance in the same time means a greater speed, and so linear speed is greater on the outer edge of a rotating object than it is closer to the axis. This speed along a circular path is known as "tangential speed" because the direction of motion is tangent to the circumference of the circle. For circular motion, the terms linear speed and tangential speed are used interchangeably, and both use units of m/s, km/h, and others.
Rotational speed (or "angular speed") involves the number of revolutions per unit of time. All parts of a rigid merry-go-round or turntable turn about the axis of rotation in the same amount of time. Thus, all parts share the same rate of rotation, or the same number of rotations or revolutions per unit of time. It is common to express rotational rates in revolutions per minute (RPM) or in terms of the number of "radians" turned in a unit of time. There are little more than 6 radians in a full rotation (2 radians exactly). When a direction is assigned to rotational speed, it is known as rotational velocity or angular velocity. Rotational velocity is a vector whose magnitude is the rotational speed.
Tangential speed and rotational speed are related: the greater the RPMs, the larger the speed in metres per second. Tangential speed is directly proportional to rotational speed at any fixed distance from the axis of rotation. However, tangential speed, unlike rotational speed, depends on radial distance (the distance from the axis). For a platform rotating with a fixed rotational speed, the tangential speed in the centre is zero. Towards the edge of the platform the tangential speed increases proportional to the distance from the axis. In equation form:
where "v" is tangential speed and ω (Greek letter omega) is rotational speed. One moves faster if the rate of rotation increases (a larger value for ω), and one also moves faster if movement farther from the axis occurs (a larger value for "r"). Move twice as far from the rotational axis at the centre and you move twice as fast. Move out three times as far and you have three times as much tangential speed. In any kind of rotating system, tangential speed depends on how far you are from the axis of rotation.
When proper units are used for tangential speed "v", rotational speed ω, and radial distance "r", the direct proportion of "v" to both "r" and ω becomes the exact equation
Thus, tangential speed will be directly proportional to "r" when all parts of a system simultaneously have the same ω, as for a wheel, disk, or rigid wand.
Units.
Units of speed include:
Examples of different speeds.
Vehicles often have a speedometer to measure the speed they are moving.

</doc>
<doc id="28736" url="https://en.wikipedia.org/wiki?curid=28736" title="Speed of light">
Speed of light

The speed of light in vacuum, commonly denoted , is a universal physical constant important in many areas of physics. Its precise value is (approximately ), since the length of the metre is defined from this constant and the international standard for time. According to special relativity, is the maximum speed at which all matter and information in the universe can travel. It is the speed at which all massless particles and changes of the associated fields (including electromagnetic radiation such as light and gravitational waves) travel in vacuum. Such particles and waves travel at regardless of the motion of the source or the inertial reference frame of the observer. In the theory of relativity, interrelates space and time, and also appears in the famous equation of mass–energy equivalence .
The speed at which light propagates through transparent materials, such as glass or air, is less than ; similarly, the speed of radio waves in wire cables is slower than . The ratio between and the speed at which light travels in a material is called the refractive index of the material (). For example, for visible light the refractive index of glass is typically around 1.5, meaning that light in glass travels at ; the refractive index of air for visible light is about 1.0003, so the speed of light in air is about (about slower than ).
For many practical purposes, light and other electromagnetic waves will appear to propagate instantaneously, but for long distances and very sensitive measurements, their finite speed has noticeable effects. In communicating with distant space probes, it can take minutes to hours for a message to get from Earth to the spacecraft, or vice versa. The light seen from stars left them many years ago, allowing the study of the history of the universe by looking at distant objects. The finite speed of light also limits the theoretical maximum speed of computers, since information must be sent within the computer from chip to chip. The speed of light can be used with time of flight measurements to measure large distances to high precision.
Ole Rømer first demonstrated in 1676 that light travels at a finite speed (as opposed to instantaneously) by studying the apparent motion of Jupiter's moon Io. In 1865, James Clerk Maxwell proposed that light was an electromagnetic wave, and therefore travelled at the speed appearing in his theory of electromagnetism. In 1905, Albert Einstein postulated that the speed of light with respect to any inertial frame is independent of the motion of the light source, and explored the consequences of that postulate by deriving the special theory of relativity and showing that the parameter had relevance outside of the context of light and electromagnetism. 
After centuries of increasingly precise measurements, in 1975 the speed of light was known to be with a measurement uncertainty of 4 parts per billion. In 1983, the metre was redefined in the International System of Units (SI) as the distance travelled by light in vacuum in 1/ of a second. As a result, the numerical value of in metres per second is now fixed exactly by the definition of the metre.
Numerical value, notation, and units.
The speed of light in vacuum is usually denoted by a lowercase "c", for "constant" or the Latin (meaning "swiftness, celerity"). Historically, the symbol "V" was used for as an alternative symbol for the speed of light, introduced by James Clerk Maxwell in 1865. In 1856, Wilhelm Eduard Weber and Rudolf Kohlrausch had used "c" for a different constant later shown to equal times the speed of light in vacuum. In 1894, Paul Drude redefined "c" with its modern meaning. Einstein used "V" in his original German-language papers on special relativity in 1905, but in 1907 he switched to "c", which by then had become the standard symbol for the speed of light.
Sometimes "c" is used for the speed of waves in "any" material medium, and "c"0 for the speed of light in vacuum. This subscripted notation, which is endorsed in official SI literature, has the same form as other related constants: namely, "μ"0 for the vacuum permeability or magnetic constant, "ε"0 for the vacuum permittivity or electric constant, and "Z"0 for the impedance of free space. This article uses "c" exclusively for the speed of light in vacuum.
Since 1983, the metre has been defined in the International System of Units (SI) as the distance light travels in vacuum in of a second. This definition fixes the speed of light in vacuum at exactly .
As a dimensional physical constant, the numerical value of "c" is different for different unit systems.
In branches of physics in which "c" appears often, such as in relativity, it is common to use systems of natural units of measurement or the geometrized unit system where . Using these units, "c" does not appear explicitly because multiplication or division by 1 does not affect the result.
Fundamental role in physics.
The speed at which light waves propagate in vacuum is independent both of the motion of the wave source and of the inertial frame of reference of the observer. This invariance of the speed of light was postulated by Einstein in 1905, after being motivated by Maxwell's theory of electromagnetism and the lack of evidence for the luminiferous aether; it has since been consistently confirmed by many experiments. It is only possible to verify experimentally that the two-way speed of light (for example, from a source to a mirror and back again) is frame-independent, because it is impossible to measure the one-way speed of light (for example, from a source to a distant detector) without some convention as to how clocks at the source and at the detector should be synchronized. However, by adopting Einstein synchronization for the clocks, the one-way speed of light becomes equal to the two-way speed of light by definition. The special theory of relativity explores the consequences of this invariance of "c" with the assumption that the laws of physics are the same in all inertial frames of reference. One consequence is that "c" is the speed at which all massless particles and waves, including light, must travel in vacuum.
Special relativity has many counterintuitive and experimentally verified implications. These include the equivalence of mass and energy , length contraction (moving objects shorten), and time dilation (moving clocks run more slowly). The factor "γ" by which lengths contract and times dilate is known as the Lorentz factor and is given by , where "v" is the speed of the object. The difference of "γ" from 1 is negligible for speeds much slower than "c", such as most everyday speeds—in which case special relativity is closely approximated by Galilean relativity—but it increases at relativistic speeds and diverges to infinity as "v" approaches "c".
The results of special relativity can be summarized by treating space and time as a unified structure known as spacetime (with "c" relating the units of space and time), and requiring that physical theories satisfy a special symmetry called Lorentz invariance, whose mathematical formulation contains the parameter "c". Lorentz invariance is an almost universal assumption for modern physical theories, such as quantum electrodynamics, quantum chromodynamics, the Standard Model of particle physics, and general relativity. As such, the parameter "c" is ubiquitous in modern physics, appearing in many contexts that are unrelated to light. For example, general relativity predicts that "c" is also the speed of gravity and of gravitational waves. In non-inertial frames of reference (gravitationally curved space or accelerated reference frames), the "local" speed of light is constant and equal to "c", but the speed of light along a trajectory of finite length can differ from "c", depending on how distances and times are defined.
It is generally assumed that fundamental constants such as "c" have the same value throughout spacetime, meaning that they do not depend on location and do not vary with time. However, it has been suggested in various theories that the speed of light may have changed over time. No conclusive evidence for such changes has been found, but they remain the subject of ongoing research.
It also is generally assumed that the speed of light is isotropic, meaning that it has the same value regardless of the direction in which it is measured. Observations of the emissions from nuclear energy levels as a function of the orientation of the emitting nuclei in a magnetic field (see Hughes–Drever experiment), and of rotating optical resonators (see Resonator experiments) have put stringent limits on the possible two-way anisotropy.
Upper limit on speeds.
According to special relativity, the energy of an object with rest mass "m" and speed "v" is given by , where "γ" is the Lorentz factor defined above. When "v" is zero, "γ" is equal to one, giving rise to the famous formula for mass–energy equivalence. The "γ" factor approaches infinity as "v" approaches "c", and it would take an infinite amount of energy to accelerate an object with mass to the speed of light. The speed of light is the upper limit for the speeds of objects with positive rest mass, and individual photons cannot travel faster than the speed of light. This is experimentally established in many tests of relativistic energy and momentum.
More generally, it is normally impossible for information or energy to travel faster than "c". One argument for this follows from the counter-intuitive implication of special relativity known as the relativity of simultaneity. If the spatial distance between two events A and B is greater than the time interval between them multiplied by "c" then there are frames of reference in which A precedes B, others in which B precedes A, and others in which they are simultaneous. As a result, if something were travelling faster than "c" relative to an inertial frame of reference, it would be travelling backwards in time relative to another frame, and causality would be violated. In such a frame of reference, an "effect" could be observed before its "cause". Such a violation of causality has never been recorded, and would lead to paradoxes such as the tachyonic antitelephone.
Faster-than-light observations and experiments.
There are situations in which it may seem that matter, energy, or information travels at speeds greater than "c", but they do not. For example, as is discussed in the propagation of light in a medium section below, many wave velocities can exceed "c". For example, the phase velocity of X-rays through most glasses can routinely exceed "c", but phase velocity does not determine the velocity at which waves convey information.
If a laser beam is swept quickly across a distant object, the spot of light can move faster than "c", although the initial movement of the spot is delayed because of the time it takes light to get to the distant object at the speed "c". However, the only physical entities that are moving are the laser and its emitted light, which travels at the speed "c" from the laser to the various positions of the spot. Similarly, a shadow projected onto a distant object can be made to move faster than "c", after a delay in time. In neither case does any matter, energy, or information travel faster than light.
The rate of change in the distance between two objects in a frame of reference with respect to which both are moving (their closing speed) may have a value in excess of "c". However, this does not represent the speed of any single object as measured in a single inertial frame.
Certain quantum effects appear to be transmitted instantaneously and therefore faster than "c", as in the EPR paradox. An example involves the quantum states of two particles that can be entangled. Until either of the particles is observed, they exist in a superposition of two quantum states. If the particles are separated and one particle's quantum state is observed, the other particle's quantum state is determined instantaneously (i.e., faster than light could travel from one particle to the other). However, it is impossible to control which quantum state the first particle will take on when it is observed, so information cannot be transmitted in this manner.
Another quantum effect that predicts the occurrence of faster-than-light speeds is called the Hartman effect; under certain conditions the time needed for a virtual particle to tunnel through a barrier is constant, regardless of the thickness of the barrier. This could result in a virtual particle crossing a large gap faster-than-light. However, no information can be sent using this effect.
So-called superluminal motion is seen in certain astronomical objects, such as the relativistic jets of radio galaxies and quasars. However, these jets are not moving at speeds in excess of the speed of light: the apparent superluminal motion is a projection effect caused by objects moving near the speed of light and approaching Earth at a small angle to the line of sight: since the light which was emitted when the jet was farther away took longer to reach the Earth, the time between two successive observations corresponds to a longer time between the instants at which the light rays were emitted.
In models of the expanding universe, the farther galaxies are from each other, the faster they drift apart. This receding is not due to motion "through" space, but rather to the expansion of space itself. For example, galaxies far away from Earth appear to be moving away from the Earth with a speed proportional to their distances. Beyond a boundary called the Hubble sphere, the rate at which their distance from Earth increases becomes greater than the speed of light.
Propagation of light.
In classical physics, light is described as a type of electromagnetic wave. The classical behaviour of the electromagnetic field is described by Maxwell's equations, which predict that the speed "c" with which electromagnetic waves (such as light) propagate through the vacuum is related to the electric constant "ε"0 and the magnetic constant "μ"0 by the equation . In modern quantum physics, the electromagnetic field is described by the theory of quantum electrodynamics (QED). In this theory, light is described by the fundamental excitations (or quanta) of the electromagnetic field, called photons. In QED, photons are massless particles and thus, according to special relativity, they travel at the speed of light in vacuum.
Extensions of QED in which the photon has a mass have been considered. In such a theory, its speed would depend on its frequency, and the invariant speed "c" of special relativity would then be the upper limit of the speed of light in vacuum. No variation of the speed of light with frequency has been observed in rigorous testing, putting stringent limits on the mass of the photon. The limit obtained depends on the model used: if the massive photon is described by Proca theory, the experimental upper bound for its mass is about 10−57 grams; if photon mass is generated by a Higgs mechanism, the experimental upper limit is less sharp,   (roughly 2 × 10−47 g).
Another reason for the speed of light to vary with its frequency would be the failure of special relativity to apply to arbitrarily small scales, as predicted by some proposed theories of quantum gravity. In 2009, the observation of the spectrum of gamma-ray burst GRB 090510 did not find any difference in the speeds of photons of different energies, confirming that Lorentz invariance is verified at least down to the scale of the Planck length ("l"P =  ≈ ) divided by 1.2.
In a medium.
In a medium, light usually does not propagate at a speed equal to "c"; further, different types of light wave will travel at different speeds. The speed at which the individual crests and troughs of a plane wave (a wave filling the whole space, with only one frequency) propagate is called the phase velocity "v"p. An actual physical signal with a finite extent (a pulse of light) travels at a different speed. The largest part of the pulse travels at the group velocity "v"g, and its earliest part travels at the front velocity "v"f.
The phase velocity is important in determining how a light wave travels through a material or from one material to another. It is often represented in terms of a "refractive index". The refractive index of a material is defined as the ratio of "c" to the phase velocity "v"p in the material: larger indices of refraction indicate lower speeds. The refractive index of a material may depend on the light's frequency, intensity, polarization, or direction of propagation; in many cases, though, it can be treated as a material-dependent constant. The refractive index of air is approximately 1.0003. Denser media, such as water, glass, and diamond, have refractive indexes of around 1.3, 1.5 and 2.4, respectively, for visible light. In exotic materials like Bose–Einstein condensates near absolute zero, the effective speed of light may be only a few metres per second. However, this represents absorption and re-radiation delay between atoms, as do all slower-than-"c" speeds in material substances. As an extreme example of light "slowing" in matter, two independent teams of physicists claimed to bring light to a "complete standstill" by passing it through a Bose–Einstein condensate of the element rubidium, one team at Harvard University and the Rowland Institute for Science in Cambridge, Mass., and the other at the Harvard–Smithsonian Center for Astrophysics, also in Cambridge. However, the popular description of light being "stopped" in these experiments refers only to light being stored in the excited states of atoms, then re-emitted at an arbitrarily later time, as stimulated by a second laser pulse. During the time it had "stopped," it had ceased to be light. This type of behaviour is generally microscopically true of all transparent media which "slow" the speed of light.
In transparent materials, the refractive index generally is greater than 1, meaning that the phase velocity is less than "c". In other materials, it is possible for the refractive index to become smaller than 1 for some frequencies; in some exotic materials it is even possible for the index of refraction to become negative. The requirement that causality is not violated implies that the real and imaginary parts of the dielectric constant of any material, corresponding respectively to the index of refraction and to the attenuation coefficient, are linked by the Kramers–Kronig relations. In practical terms, this means that in a material with refractive index less than 1, the absorption of the wave is so quick that no signal can be sent faster than "c".
A pulse with different group and phase velocities (which occurs if the phase velocity is not the same for all the frequencies of the pulse) smears out over time, a process known as dispersion. Certain materials have an exceptionally low (or even zero) group velocity for light waves, a phenomenon called slow light, which has been confirmed in various experiments.
The opposite, group velocities exceeding "c", has also been shown in experiment. It should even be possible for the group velocity to become infinite or negative, with pulses travelling instantaneously or backwards in time.
None of these options, however, allow information to be transmitted faster than "c". It is impossible to transmit information with a light pulse any faster than the speed of the earliest part of the pulse (the front velocity). It can be shown that this is (under certain assumptions) always equal to "c". 
It is possible for a particle to travel through a medium faster than the phase velocity of light in that medium (but still slower than "c"). When a charged particle does that in a dielectric material, the electromagnetic equivalent of a shock wave, known as Cherenkov radiation, is emitted.
Practical effects of finiteness.
The speed of light is of relevance to communications: the one-way and round-trip delay time are greater than zero. This applies from small to astronomical scales. On the other hand, some techniques depend on the finite speed of light, for example in distance measurements.
Small scales.
In supercomputers, the speed of light imposes a limit on how quickly data can be sent between processors. If a processor operates at 1 gigahertz, a signal can only travel a maximum of about in a single cycle. Processors must therefore be placed close to each other to minimize communication latencies; this can cause difficulty with cooling. If clock frequencies continue to increase, the speed of light will eventually become a limiting factor for the internal design of single chips.
Large distances on Earth.
For example, given the equatorial circumference of the Earth is about and "c" about , the theoretical shortest time for a piece of information to travel half the globe along the surface is about 67 milliseconds. When light is travelling around the globe in an optical fibre, the actual transit time is longer, in part because the speed of light is slower by about 35% in an optical fibre, depending on its refractive index "n". Furthermore, straight lines rarely occur in global communications situations, and delays are created when the signal passes through an electronic switch or signal regenerator.
Spaceflights and astronomy.
Similarly, communications between the Earth and spacecraft are not instantaneous. There is a brief delay from the source to the receiver, which becomes more noticeable as distances increase. This delay was significant for communications between ground control and Apollo 8 when it became the first manned spacecraft to orbit the Moon: for every question, the ground control station had to wait at least three seconds for the answer to arrive. The communications delay between Earth and Mars can vary between five and twenty minutes depending upon the relative positions of the two planets. As a consequence of this, if a robot on the surface of Mars were to encounter a problem, its human controllers would not be aware of it until at least five minutes later, and possibly up to twenty minutes later; it would then take a further five to twenty minutes for instructions to travel from Earth to Mars.
NASA must wait several hours for information from a probe orbiting Jupiter, and if it needs to correct a navigation error, the fix will not arrive at the spacecraft for an equal amount of time, creating a risk of the correction not arriving in time.
Receiving light and other signals from distant astronomical sources can even take much longer. For example, it has taken 13 billion (13) years for light to travel to Earth from the faraway galaxies viewed in the Hubble Ultra Deep Field images. Those photographs, taken today, capture images of the galaxies as they appeared 13 billion years ago, when the universe was less than a billion years old. The fact that more distant objects appear to be younger, due to the finite speed of light, allows astronomers to infer the evolution of stars, of galaxies, and of the universe itself.
Astronomical distances are sometimes expressed in light-years, especially in popular science publications and media. A light-year is the distance light travels in one year, around 9461 billion kilometres, 5879 billion miles, or 0.3066 parsecs. In round figures, a light year is nearly 10 trillion kilometres or nearly 6 trillion miles. Proxima Centauri, the closest star to Earth after the Sun, is around 4.2 light-years away.
Distance measurement.
Radar systems measure the distance to a target by the time it takes a radio-wave pulse to return to the radar antenna after being reflected by the target: the distance to the target is half the round-trip transit time multiplied by the speed of light. A Global Positioning System (GPS) receiver measures its distance to GPS satellites based on how long it takes for a radio signal to arrive from each satellite, and from these distances calculates the receiver's position. Because light travels about () in one second, these measurements of small fractions of a second must be very precise. The Lunar Laser Ranging Experiment, radar astronomy and the Deep Space Network determine distances to the Moon, planets and spacecraft, respectively, by measuring round-trip transit times.
High-frequency trading.
The speed of light has become important in high-frequency trading, where traders seek to gain minute advantages by delivering their trades to exchanges fractions of a second ahead of other traders. For example, traders have been switching to microwave communications between trading hubs, because of the advantage which microwaves travelling at near to the speed of light in air, have over fibre optic signals which travel 30–40% slower at the speed of light through glass.
Measurement.
There are different ways to determine the value of "c". One way is to measure the actual speed at which light waves propagate, which can be done in various astronomical and earth-based setups. However, it is also possible to determine "c" from other physical laws where it appears, for example, by determining the values of the electromagnetic constants "ε"0 and "μ"0 and using their relation to "c". Historically, the most accurate results have been obtained by separately determining the frequency and wavelength of a light beam, with their product equalling "c".
In 1983 the metre was defined as "the length of the path travelled by light in vacuum during a time interval of of a second", fixing the value of the speed of light at by definition, as described below. Consequently, accurate measurements of the speed of light yield an accurate realization of the metre rather than an accurate value of "c".
Astronomical measurements.
Outer space is a convenient setting for measuring the speed of light because of its large scale and nearly perfect vacuum. Typically, one measures the time needed for light to traverse some reference distance in the solar system, such as the radius of the Earth's orbit. Historically, such measurements could be made fairly accurately, compared to how accurately the length of the reference distance is known in Earth-based units. It is customary to express the results in astronomical units (AU) per day.
Ole Christensen Rømer used an astronomical measurement to make the first quantitative estimate of the speed of light. When measured from Earth, the periods of moons orbiting a distant planet are shorter when the Earth is approaching the planet than when the Earth is receding from it. The distance travelled by light from the planet (or its moon) to Earth is shorter when the Earth is at the point in its orbit that is closest to its planet than when the Earth is at the farthest point in its orbit, the difference in distance being the diameter of the Earth's orbit around the Sun. The observed change in the moon's orbital period is caused by the difference in the time it takes light to traverse the shorter or longer distance. Rømer observed this effect for Jupiter's innermost moon Io and deduced that light takes 22 minutes to cross the diameter of the Earth's orbit.
Another method is to use the aberration of light, discovered and explained by James Bradley in the 18th century. This effect results from the vector addition of the velocity of light arriving from a distant source (such as a star) and the velocity of its observer (see diagram on the right). A moving observer thus sees the light coming from a slightly different direction and consequently sees the source at a position shifted from its original position. Since the direction of the Earth's velocity changes continuously as the Earth orbits the Sun, this effect causes the apparent position of stars to move around. From the angular difference in the position of stars (maximally 20.5 arcseconds) it is possible to express the speed of light in terms of the Earth's velocity around the Sun, which with the known length of a year can be converted to the time needed to travel from the Sun to the Earth. In 1729, Bradley used this method to derive that light travelled 10,210 times faster than the Earth in its orbit (the modern figure is 10,066 times faster) or, equivalently, that it would take light 8 minutes 12 seconds to travel from the Sun to the Earth.
Astronomical unit.
An astronomical unit (AU) is approximately the average distance between the Earth and Sun. It was redefined in 2012 as exactly . Previously the AU was not based on the International System of Units but in terms of the gravitational force exerted by the Sun in the framework of classical mechanics. The current definition uses the recommended value in metres for the previous definition of the astronomical unit, which was determined by measurement. This redefinition is analogous to that of the metre, and likewise has the effect of fixing the speed of light to an exact value in astronomical units per second (via the exact speed of light in metres per second).
Previously, the inverse of  expressed in seconds per astronomical unit was measured by comparing the time for radio signals to reach different spacecraft in the Solar System, with their position calculated from the gravitational effects of the Sun and various planets. By combining many such measurements, a best fit value for the light time per unit distance could be obtained. For example, in 2009, the best estimate, as approved by the International Astronomical Union (IAU), was:
The relative uncertainty in these measurements is 0.02 parts per billion (), equivalent to the uncertainty in Earth-based measurements of length by interferometry. Since the metre is defined to be the length travelled by light in a certain time interval, the measurement of the light time in terms of the previous definition of the astronomical unit can also be interpreted as measuring the length of an AU (old definition) in metres.
Time of flight techniques.
A method of measuring the speed of light is to measure the time needed for light to travel to a mirror at a known distance and back. This is the working principle behind the Fizeau–Foucault apparatus developed by Hippolyte Fizeau and Léon Foucault.
The setup as used by Fizeau consists of a beam of light directed at a mirror away. On the way from the source to the mirror, the beam passes through a rotating cogwheel. At a certain rate of rotation, the beam passes through one gap on the way out and another on the way back, but at slightly higher or lower rates, the beam strikes a tooth and does not pass through the wheel. Knowing the distance between the wheel and the mirror, the number of teeth on the wheel, and the rate of rotation, the speed of light can be calculated.
The method of Foucault replaces the cogwheel by a rotating mirror. Because the mirror keeps rotating while the light travels to the distant mirror and back, the light is reflected from the rotating mirror at a different angle on its way out than it is on its way back. From this difference in angle, the known speed of rotation and the distance to the distant mirror the speed of light may be calculated.
Nowadays, using oscilloscopes with time resolutions of less than one nanosecond, the speed of light can be directly measured by timing the delay of a light pulse from a laser or an LED reflected from a mirror. This method is less precise (with errors of the order of 1%) than other modern techniques, but it is sometimes used as a laboratory experiment in college physics classes.
Electromagnetic constants.
An option for deriving "c" that does not directly depend on a measurement of the propagation of electromagnetic waves is to use the relation between "c" and the vacuum permittivity "ε"0 and vacuum permeability "μ"0 established by Maxwell's theory: "c"2 = 1/("ε"0"μ"0). The vacuum permittivity may be determined by measuring the capacitance and dimensions of a capacitor, whereas the value of the vacuum permeability is fixed at exactly through the definition of the ampere. Rosa and Dorsey used this method in 1907 to find a value of .
Cavity resonance.
Another way to measure the speed of light is to independently measure the frequency "f" and wavelength "λ" of an electromagnetic wave in vacuum. The value of "c" can then be found by using the relation "c" = "fλ". One option is to measure the resonance frequency of a cavity resonator. If the dimensions of the resonance cavity are also known, these can be used to determine the wavelength of the wave. In 1946, Louis Essen and A.C. Gordon-Smith established the frequency for a variety of normal modes of microwaves of a microwave cavity of precisely known dimensions. The dimensions were established to an accuracy of about ±0.8 μm using gauges calibrated by interferometry. As the wavelength of the modes was known from the geometry of the cavity and from electromagnetic theory, knowledge of the associated frequencies enabled a calculation of the speed of light.
The Essen–Gordon-Smith result, , was substantially more precise than those found by optical techniques. By 1950, repeated measurements by Essen established a result of .
A household demonstration of this technique is possible, using a microwave oven and food such as marshmallows or margarine: if the turntable is removed so that the food does not move, it will cook the fastest at the antinodes (the points at which the wave amplitude is the greatest), where it will begin to melt. The distance between two such spots is half the wavelength of the microwaves; by measuring this distance and multiplying the wavelength by the microwave frequency (usually displayed on the back of the oven, typically 2450 MHz), the value of "c" can be calculated, "often with less than 5% error".
Interferometry.
Interferometry is another method to find the wavelength of electromagnetic radiation for determining the speed of light. A coherent beam of light (e.g. from a laser), with a known frequency ("f"), is split to follow two paths and then recombined. By adjusting the path length while observing the interference pattern and carefully measuring the change in path length, the wavelength of the light ("λ") can be determined. The speed of light is then calculated using the equation "c" = "λf".
Before the advent of laser technology, coherent radio sources were used for interferometry measurements of the speed of light. However interferometric determination of wavelength becomes less precise with wavelength and the experiments were thus limited in precision by the long wavelength (~0.4 cm) of the radiowaves. The precision can be improved by using light with a shorter wavelength, but then it becomes difficult to directly measure the frequency of the light. One way around this problem is to start with a low frequency signal of which the frequency can be precisely measured, and from this signal progressively synthesize higher frequency signals whose frequency can then be linked to the original signal. A laser can then be locked to the frequency, and its wavelength can be determined using interferometry. This technique was due to a group at the National Bureau of Standards (NBS) (which later became NIST). They used it in 1972 to measure the speed of light in vacuum with a fractional uncertainty of .
History.
Until the early modern period, it was not known whether light travelled instantaneously or at a very fast finite speed. The first extant recorded examination of this subject was in ancient Greece. The ancient Greeks, Muslim scholars, and classical European scientists long debated this until Rømer provided the first calculation of the speed of light. Einstein's Theory of Special Relativity concluded that the speed of light is constant regardless of one's frame of reference. Since then, scientists have provided increasingly accurate measurements.
Early history.
Empedocles (c. 490–430 BC) was the first to claim that light has a finite speed. He maintained that light was something in motion, and therefore must take some time to travel. Aristotle argued, to the contrary, that "light is due to the presence of something, but it is not a movement". Euclid and Ptolemy advanced Empedocles' emission theory of vision, where light is emitted from the eye, thus enabling sight. Based on that theory, Heron of Alexandria argued that the speed of light must be infinite because distant objects such as stars appear immediately upon opening the eyes.
Sayana had made commentary statements of measurement of speed of light as mentioned in Hindu Vedas. Early Islamic philosophers initially agreed with the Aristotelian view that light had no speed of travel. In 1021, Alhazen (Ibn al-Haytham) published the "Book of Optics", in which he presented a series of arguments dismissing the emission theory of vision in favour of the now accepted intromission theory, in which light moves from an object into the eye. This led Alhazen to propose that light must have a finite speed, and that the speed of light is variable, decreasing in denser bodies. He argued that light is substantial matter, the propagation of which requires time, even if this is hidden from our senses. Also in the 11th century, Abū Rayhān al-Bīrūnī agreed that light has a finite speed, and observed that the speed of light is much faster than the speed of sound.
In the 13th century, Roger Bacon argued that the speed of light in air was not infinite, using philosophical arguments backed by the writing of Alhazen and Aristotle. In the 1270s, Witelo considered the possibility of light travelling at infinite speed in vacuum, but slowing down in denser bodies.
In the early 17th century, Johannes Kepler believed that the speed of light was infinite, since empty space presents no obstacle to it. René Descartes argued that if the speed of light were to be finite, the Sun, Earth, and Moon would be noticeably out of alignment during a lunar eclipse. Since such misalignment had not been observed, Descartes concluded the speed of light was infinite. Descartes speculated that if the speed of light were found to be finite, his whole system of philosophy might be demolished. In Descartes' derivation of Snell's law, he assumed that even though the speed of light was instantaneous, the more dense the medium, the faster was light's speed. Pierre de Fermat derived Snell's law using the opposing assumption, the more dense the medium the slower light traveled. Fermat also argued in support of a finite speed of light.
First measurement attempts.
In 1629, Isaac Beeckman proposed an experiment in which a person observes the flash of a cannon reflecting off a mirror about one mile (1.6 km) away. In 1638, Galileo Galilei proposed an experiment, with an apparent claim to having performed it some years earlier, to measure the speed of light by observing the delay between uncovering a lantern and its perception some distance away. He was unable to distinguish whether light travel was instantaneous or not, but concluded that if it were not, it must nevertheless be extraordinarily rapid. In 1667, the Accademia del Cimento of Florence reported that it had performed Galileo's experiment, with the lanterns separated by about one mile, but no delay was observed. The actual delay in this experiment would have been about 11 microseconds.
The first quantitative estimate of the speed of light was made in 1676 by Rømer (see Rømer's determination of the speed of light). From the observation that the periods of Jupiter's innermost moon Io appeared to be shorter when the Earth was approaching Jupiter than when receding from it, he concluded that light travels at a finite speed, and estimated that it takes light 22 minutes to cross the diameter of Earth's orbit. Christiaan Huygens combined this estimate with an estimate for the diameter of the Earth's orbit to obtain an estimate of speed of light of , 26% lower than the actual value.
In his 1704 book "Opticks", Isaac Newton reported Rømer's calculations of the finite speed of light and gave a value of "seven or eight minutes" for the time taken for light to travel from the Sun to the Earth (the modern value is 8 minutes 19 seconds). Newton queried whether Rømer's eclipse shadows were coloured; hearing that they were not, he concluded the different colours travelled at the same speed. In 1729, James Bradley discovered stellar aberration. From this effect he determined that light must travel 10,210 times faster than the Earth in its orbit (the modern figure is 10,066 times faster) or, equivalently, that it would take light 8 minutes 12 seconds to travel from the Sun to the Earth.
Connections with electromagnetism.
In the 19th century Hippolyte Fizeau developed a method to determine the speed of light based on time-of-flight measurements on Earth and reported a value of . His method was improved upon by Léon Foucault who obtained a value of in 1862. In the year 1856, Wilhelm Eduard Weber and Rudolf Kohlrausch measured the ratio of the electromagnetic and electrostatic units of charge, 1/√"ε"0"μ"0, by discharging a Leyden jar, and found that its numerical value was very close to the speed of light as measured directly by Fizeau. The following year Gustav Kirchhoff calculated that an electric signal in a resistanceless wire travels along the wire at this speed. In the early 1860s, Maxwell showed that, according to the theory of electromagnetism he was working on, electromagnetic waves propagate in empty space at a speed equal to the above Weber/Kohrausch ratio, and drawing attention to the numerical proximity of this value to the speed of light as measured by Fizeau, he proposed that light is in fact an electromagnetic wave.
"Luminiferous aether".
It was thought at the time that empty space was filled with a background medium called the luminiferous aether in which the electromagnetic field existed. Some physicists thought that this aether acted as a preferred frame of reference for the propagation of light and therefore it should be possible to measure the motion of the Earth with respect to this medium, by measuring the isotropy of the speed of light. Beginning in the 1880s several experiments were performed to try to detect this motion, the most famous of which is the experiment performed by Albert A. Michelson and Edward W. Morley in 1887. The detected motion was always less than the observational error. Modern experiments indicate that the two-way speed of light is isotropic (the same in every direction) to within 6 nanometres per second.
Because of this experiment Hendrik Lorentz proposed that the motion of the apparatus through the aether may cause the apparatus to contract along its length in the direction of motion, and he further assumed, that the time variable for moving systems must also be changed accordingly ("local time"), which led to the formulation of the Lorentz transformation. Based on Lorentz's aether theory, Henri Poincaré (1900) showed that this local time (to first order in v/c) is indicated by clocks moving in the aether, which are synchronized under the assumption of constant light speed. In 1904, he speculated that the speed of light could be a limiting velocity in dynamics, provided that the assumptions of Lorentz's theory are all confirmed. In 1905, Poincaré brought Lorentz's aether theory into full observational agreement with the principle of relativity.
Special relativity.
In 1905 Einstein postulated from the outset that the speed of light in vacuum, measured by a non-accelerating observer, is independent of the motion of the source or observer. Using this and the principle of relativity as a basis he derived the special theory of relativity, in which the speed of light in vacuum "c" featured as a fundamental constant, also appearing in contexts unrelated to light. This made the concept of the stationary aether (to which Lorentz and Poincaré still adhered) useless and revolutionized the concepts of space and time.
Increased accuracy of "c" and redefinition of the metre and second.
In the second half of the 20th century much progress was made in increasing the accuracy of measurements of the speed of light, first by cavity resonance techniques and later by laser interferometer techniques. These were aided by new, more precise, definitions of the metre and second. In 1950, Louis Essen determined the speed as , using cavity resonance. This value was adopted by the 12th General Assembly of the Radio-Scientific Union in 1957. In 1960, the metre was redefined in terms of the wavelength of a particular spectral line of krypton-86, and, in 1967, the second was redefined in terms of the hyperfine transition frequency of the ground state of caesium-133.
In 1972, using the laser interferometer method and the new definitions, a group at NBS in Boulder, Colorado determined the speed of light in vacuum to be "c" = . This was 100 times less uncertain than the previously accepted value. The remaining uncertainty was mainly related to the definition of the metre. As similar experiments found comparable results for "c", the 15th Conférence Générale des Poids et Mesures (CGPM) in 1975 recommended using the value for the speed of light.
Defining the speed of light as an explicit constant.
In 1983 the 17th CGPM found that wavelengths from frequency measurements and a given value for the speed of light are more reproducible than the previous standard. They kept the 1967 definition of second, so the caesium hyperfine frequency would now determine both the second and the metre. To do this, they redefined the metre as: "The metre is the length of the path travelled by light in vacuum during a time interval of 1/ of a second." As a result of this definition, the value of the speed of light in vacuum is exactly and has become a defined constant in the SI system of units. Improved experimental techniques that prior to 1983 would have measured the speed of light, no longer affect the known value of the speed of light in SI units, but instead allow a more precise realization of the metre by more accurately measuring the wavelength of Krypton-86 and other light sources.
In 2011, the CGPM stated its intention to redefine all seven SI base units using what it calls "the explicit-constant formulation", where each "unit is defined indirectly by specifying explicitly an exact value for a well-recognized fundamental constant", as was done for the speed of light. It proposed a new, but completely equivalent, wording of the metre's definition: "The metre, symbol m, is the unit of length; its magnitude is set by fixing the numerical value of the speed of light in vacuum to be equal to exactly when it is expressed in the SI unit ." This is one of the proposed changes to be incorporated in the next revision of the SI also termed the "New SI".

</doc>
