<doc id="5659" url="https://en.wikipedia.org/wiki?curid=5659" title="Chemical element">
Chemical element

A chemical element or element is a chemical substance consisting of atoms having the same number of protons in their atomic nuclei (i.e. the same atomic number, "Z"). There are 118 elements that have been identified, of which the first 94 occur naturally on Earth with the remaining 24 being synthetic elements. There are 80 elements that have at least one stable isotope and 38 that have exclusively radioactive isotopes, which decay over time into other elements. Iron is the most abundant element (by mass) making up the Earth, while oxygen is the most common element in the crust of the earth.
Chemical elements constitute approximately 15% of the matter in the universe: the remainder is dark matter, the composition of which is unknown, but it is not composed of chemical elements.
The two lightest elements, hydrogen and helium were mostly formed in the Big Bang and are the most common elements in the universe. The next three elements (lithium, beryllium and boron) were formed mostly by cosmic ray spallation, and are thus more rare than those that follow. Formation of elements with from six to twenty six protons occurred and continues to occur in main sequence stars via stellar nucleosynthesis. The high abundance of oxygen, silicon, and iron on Earth reflects their common production in such stars. Elements with greater than twenty six protons are formed by supernova nucleosynthesis in supernovae, which, when they explode, blast these elements far into space as planetary nebulae, where they may become incorporated into planets when they are formed. 
When different elements are chemically combined, with the atoms held together by chemical bonds, they form chemical compounds. Only a minority of elements are found uncombined as relatively pure minerals. Among the more common of such "native elements" are copper, silver, gold, carbon (as coal, graphite, or diamonds), and sulfur. All but a few of the most inert elements, such as noble gases and noble metals, are usually found on Earth in chemically combined form, as chemical compounds. While about 32 of the chemical elements occur on Earth in native uncombined forms, most of these occur as mixtures. For example, atmospheric air is primarily a mixture of nitrogen, oxygen, and argon, and native solid elements occur in alloys, such as that of iron and nickel.
The history of the discovery and use of the elements began with primitive human societies that found native elements like carbon, sulfur, copper and gold. Later civilizations extracted elemental copper, tin, lead and iron from their ores by smelting, using charcoal. Alchemists and chemists subsequently identified many more, with almost all of the naturally-occurring elements becoming known by 1900. 
The properties of the chemical elements are summarized on the periodic table, which organizes the elements by increasing atomic number into rows ("periods") in which the columns ("groups") share recurring ("periodic") physical and chemical properties. Save for unstable radioactive elements with short half-lives, all of the elements are available industrially, most of them in high degrees of purity.
Description.
The lightest chemical elements are hydrogen and helium, both created by Big Bang nucleosynthesis during the first 20 minutes of the universe in a ratio of around 3:1 by mass (or 12:1 by number of atoms), along with tiny traces of the next two elements, lithium and beryllium. Almost all other elements found in nature were made by various natural methods of nucleosynthesis. On Earth, small amounts of new atoms are naturally produced in nucleogenic reactions, or in cosmogenic processes, such as cosmic ray spallation. New atoms are also naturally produced on Earth as radiogenic daughter isotopes of ongoing radioactive decay processes such as alpha decay, beta decay, spontaneous fission, cluster decay, and other rarer modes of decay.
Of the 94 naturally occurring elements, those with atomic numbers 1 through 82 each have at least one stable isotope, (except for technetium, element 43 and promethium, element 61, which have no stable isotopes). Isotopes considered stable are those for which no radioactive decay has yet been observed. Elements with atomic numbers 83 through 94 are unstable to the point that radioactive decay of all isotopes can be detected. Some of these elements, notably bismuth (atomic number 83), thorium (atomic number 90), uranium (atomic number 92) and plutonium (atomic number 94), have one or more isotopes with half-lives long enough to survive as remnants of the explosive stellar nucleosynthesis that produced the heavy elements before the formation of our solar system. For example, at over 1.9 years, over a billion times longer than the current estimated age of the universe, bismuth-209 (atomic number 83) has the longest known alpha decay half-life of any naturally occurring element. The very heaviest elements (those beyond plutonium, element 94) undergo radioactive decay with half-lives so short that they do not occur in nature and must be synthesized.
As of 2010, there are 118 known elements (in this context, "known" means observed well enough, even from just a few decay products, to have been differentiated from other elements). Of these 118 elements, 94 occur naturally on Earth. Six of these occur in extreme trace quantities: technetium, atomic number 43; promethium, number 61; astatine, number 85; francium, number 87; neptunium, number 93; and plutonium, number 94. These 94 elements have been detected in the universe at large, in the spectra of stars and also supernovae, where short-lived radioactive elements are newly being made. The first 94 elements have been detected directly on Earth as primordial nuclides present from the formation of the solar system, or as naturally-occurring fission or transmutation products of uranium and thorium.
The remaining 20 heavier elements, not found today either on Earth or in astronomical spectra, have been produced artificially: these are all radioactive, with very short half-lives; if any atoms of these elements were present at the formation of Earth, they are extremely likely, to the point of certainty, to have already decayed, and if present in novae, have been in quantities too small to have been noted. Technetium was the first purportedly non-naturally occurring element synthesized, in 1937, although trace amounts of technetium have since been found in nature (and also the element may have been discovered naturally in 1925). This pattern of artificial production and later natural discovery has been repeated with several other radioactive naturally-occurring rare elements.
Lists of the elements are available by name, by symbol, by atomic number, by density, by melting point, and by boiling point as well as ionization energies of the elements. The nuclides of stable and radioactive elements are also available as a list of nuclides, sorted by length of half-life for those that are unstable. One of the most convenient, and certainly the most traditional presentation of the elements, is in the form of the periodic table, which groups together elements with similar chemical properties (and usually also similar electronic structures).
Atomic number.
The atomic number of an element is equal to the number of protons in each atom, and defines the element. For example, all carbon atoms contain 6 protons in their atomic nucleus; so the atomic number of carbon is 6. Carbon atoms may have different numbers of neutrons; atoms of the same element having different numbers of neutrons are known as isotopes of the element.
The number of protons in the atomic nucleus also determines its electric charge, which in turn determines the number of electrons of the atom in its non-ionized state. The electrons are placed into atomic orbitals that determine the atom's various chemical properties. The number of neutrons in a nucleus usually has very little effect on an element's chemical properties (except in the case of hydrogen and deuterium). Thus, all carbon isotopes have nearly identical chemical properties because they all have six protons and six electrons, even though carbon atoms may, for example, have 6 or 8 neutrons. That is why the atomic number, rather than mass number or atomic weight, is considered the identifying characteristic of a chemical element.
The symbol for atomic number is "Z".
Isotopes.
Isotopes are atoms of the same element (that is, with the same number of protons in their atomic nucleus), but having "different" numbers of neutrons. Most (66 of 94) naturally occurring elements have more than one stable isotope. Thus, for example, there are three main isotopes of carbon. All carbon atoms have 6 protons in the nucleus, but they can have either 6, 7, or 8 neutrons. Since the mass numbers of these are 12, 13 and 14 respectively, the three isotopes of carbon are known as carbon-12, carbon-13, and carbon-14, often abbreviated to 12C, 13C, and 14C. Carbon in everyday life and in chemistry is a mixture of 12C (about 98.9%), 13C (about 1.1%) and about 1 atom per trillion of 14C.
Except in the case of the isotopes of hydrogen (which differ greatly from each other in relative mass—enough to cause chemical effects), the isotopes of a given element are chemically nearly indistinguishable.
All of the elements have some isotopes that are radioactive (radioisotopes), although not all of these radioisotopes occur naturally. The radioisotopes typically decay into other elements upon radiating an alpha or beta particle. If an element has isotopes that are not radioactive, these are termed "stable" isotopes. All of the known stable isotopes occur naturally (see primordial isotope). The many radioisotopes that are not found in nature have been characterized after being artificially made. Certain elements have no stable isotopes and are composed "only" of radioactive isotopes: specifically the elements without any stable isotopes are technetium (atomic number 43), promethium (atomic number 61), and all observed elements with atomic numbers greater than 82.
Of the 80 elements with at least one stable isotope, 26 have only one single stable isotope. The mean number of stable isotopes for the 80 stable elements is 3.1 stable isotopes per element. The largest number of stable isotopes that occur for a single element is 10 (for tin, element 50).
Isotopic mass and atomic mass.
The mass number of an element, "A", is the number of nucleons (protons and neutrons) in the atomic nucleus. Different isotopes of a given element are distinguished by their mass numbers, which are conventionally written as a superscript on the left hand side of the atomic symbol (e.g., 238U). The mass number is always a simple whole number and has units of "nucleons." An example of a referral to a mass number is "magnesium-24," which is an atom with 24 nucleons (12 protons and 12 neutrons).
Whereas the mass number simply counts the total number of neutrons and protons and is thus a natural (or whole) number, the atomic mass of a single atom is a real number for the mass of a particular isotope of the element, the unit being u. In general, when expressed in u it differs in value slightly from the mass number for a given nuclide (or isotope) since the mass of the protons and neutrons is not exactly 1 u, since the electrons contribute a lesser share to the atomic mass as neutron number exceeds proton number, and (finally) because of the nuclear binding energy. For example, the atomic mass of chlorine-35 to five significant digits is 34.969 u and that of chlorine-37 is 36.966 u. However, the atomic mass in u of each isotope is quite close to its simple mass number (always within 1%). The only isotope whose atomic mass is exactly a natural number is 12C, which by definition has a mass of exactly 12, because u is defined as 1/12 of the mass of a free neutral carbon-12 atom in the ground state.
The relative atomic mass (historically and commonly also called "atomic weight") of an element is the "average" of the atomic masses of all the chemical element's isotopes as found in a particular environment, weighted by isotopic abundance, relative to the atomic mass unit (u). This number may be a fraction that is "not" close to a whole number, due to the averaging process. For example, the relative atomic mass of chlorine is 35.453 u, which differs greatly from a whole number due to being made of an average of 76% chlorine-35 and 24% chlorine-37. Whenever a relative atomic mass value differs by more than 1% from a whole number, it is due to this averaging effect resulting from significant amounts of more than one isotope being naturally present in the sample of the element in question.
Chemically pure and isotopically pure.
Chemists and nuclear scientists have different definitions of a "pure element". In chemistry, a pure element means a substance whose atoms all (or in practice almost all) have the same atomic number, or number of protons. Nuclear scientists, however, define a pure element as one that consists of only one stable isotope.
For example, a copper wire is 99.99% chemically pure if 99.99% of its atoms are copper, with 29 protons each. However it is not isotopically pure since ordinary copper consists of two stable isotopes, 69% 63Cu and 31% 65Cu, with different numbers of neutrons.
Allotropes.
Atoms of chemically pure elements may bond to each other chemically in more than one way, allowing the pure element to exist in multiple structures (spatial arrangements of atoms), known as allotropes, which differ in their properties. For example, carbon can be found as diamond, which has a tetrahedral structure around each carbon atom; graphite, which has layers of carbon atoms with a hexagonal structure stacked on top of each other; graphene, which is a single layer of graphite that is very strong; fullerenes, which have nearly spherical shapes; and carbon nanotubes, which are tubes with a hexagonal structure (even these may differ from each other in electrical properties). The ability of an element to exist in one of many structural forms is known as 'allotropy'.
The standard state, also known as reference state, of an element is defined as its thermodynamically most stable state at 1 bar at a given temperature (typically at 298.15 K). In thermochemistry, an element is defined to have an enthalpy of formation of zero in its standard state. For example, the reference state for carbon is graphite, because the structure of graphite is more stable than that of the other allotropes.
Properties.
Several kinds of descriptive categorizations can be applied broadly to the elements, including consideration of their general physical and chemical properties, their states of matter under familiar conditions, their melting and boiling points, their densities, their crystal structures as solids, and their origins.
General properties.
Several terms are commonly used to characterize the general physical and chemical properties of the chemical elements. A first distinction is between metals, which readily conduct electricity, nonmetals, which do not, and a small group, (the "metalloids"), having intermediate properties and often behaving as semiconductors.
A more refined classification is often shown in colored presentations of the periodic table. This system restricts the terms "metal" and "nonmetal" to only certain of the more broadly defined metals and nonmetals, adding additional terms for certain sets of the more broadly viewed metals and nonmetals. The version of this classification used in the periodic tables presented here includes: actinides, alkali metals, alkaline earth metals, halogens, lanthanides, transition metals, post-transition metals; metalloids, noble gases, polyatomic nonmetals, diatomic nonmetals, and transition metals. In this system, the alkali metals, alkaline earth metals, and transition metals, as well as the lanthanides and the actinides, are special groups of the metals viewed in a broader sense. Similarly, the polyatomic nonmetals, diatomic nonmetals and the noble gases are nonmetals viewed in the broader sense. In some presentations, the halogens are not distinguished, with astatine identified as a metalloid and the others identified as nonmetals.
States of matter.
Another commonly used basic distinction among the elements is their state of matter (phase), whether solid, liquid, or gas, at a selected standard temperature and pressure (STP). Most of the elements are solids at conventional temperatures and atmospheric pressure, while several are gases. Only bromine and mercury are liquids at 0 degrees Celsius (32 degrees Fahrenheit) and normal atmospheric pressure; caesium and gallium are solids at that temperature, but melt at 28.4 °C (83.2 °F) and 29.8 °C (85.6 °F), respectively.
Melting and boiling points.
Melting and boiling points, typically expressed in degrees Celsius at a pressure of one atmosphere, are commonly used in characterizing the various elements. While known for most elements, either or both of these measurements is still undetermined for some of the radioactive elements available in only tiny quantities. Since helium remains a liquid even at absolute zero at atmospheric pressure, it has only a boiling point, and not a melting point, in conventional presentations.
Densities.
The density at a selected standard temperature and pressure (STP) is frequently used in characterizing the elements. Density is often expressed in grams per cubic centimeter (g/cm3). Since several elements are gases at commonly encountered temperatures, their densities are usually stated for their gaseous forms; when liquefied or solidified, the gaseous elements have densities similar to those of the other elements.
When an element has allotropes with different densities, one representative allotrope is typically selected in summary presentations, while densities for each allotrope can be stated where more detail is provided. For example, the three familiar allotropes of carbon (amorphous carbon, graphite, and diamond) have densities of 1.8–2.1, 2.267, and 3.515 g/cm3, respectively.
Crystal structures.
The elements studied to date as solid samples have eight kinds of crystal structures: cubic, body-centered cubic, face-centered cubic, hexagonal, monoclinic, orthorhombic, rhombohedral, and tetragonal. For some of the synthetically produced transuranic elements, available samples have been too small to determine crystal structures.
Occurrence and origin on Earth.
Chemical elements may also be categorized by their origin on Earth, with the first 94 considered naturally occurring, while those with atomic numbers beyond 94 have only been produced artificially as the synthetic products of man-made nuclear reactions.
Of the 94 naturally occurring elements, 84 are considered primordial and either stable or weakly radioactive. The remaining 10 naturally occurring elements possess half lives too short for them to have been present at the beginning of the Solar System, and are therefore considered transient elements. Of these 10 transient elements, 5 (polonium, radon, radium, actinium, and protactinium) are relatively common decay products of thorium, uranium, and plutonium. The remaining 5 transient elements (technetium, promethium, astatine, francium, and neptunium) occur only rarely, as products of rare decay modes or nuclear reaction processes involving uranium or other heavy elements.
Elements with atomic numbers 1 through 40 are all stable, while those with atomic numbers 41 through 82 (except technetium and promethium) are metastable. The half-lives of these metastable "theoretical radionuclides" are so long (at least 100 million times longer than the estimated age of the universe) that their radioactive decay has yet to be detected by experiment. Elements with atomic numbers 83 through 94 are unstable to the point that their radioactive decay can be detected. Four of these elements, bismuth (element 83), thorium (element 90), uranium (element 92), and plutonium (element 94), have one or more isotopes with half-lives long enough to survive as remnants of the explosive stellar nucleosynthesis that produced the heavy elements before the formation of our solar system. For example, at over 1.9 years, over a billion times longer than the current estimated age of the universe, bismuth-209 has the longest known alpha decay half-life of any naturally occurring element. The very heaviest elements (those beyond plutonium, element 94) undergo radioactive decay with short half-lives and do not occur in nature.
The periodic table.
The properties of the chemical elements are often summarized using the periodic table, which powerfully and elegantly organizes the elements by increasing atomic number into rows ("periods") in which the columns ("groups") share recurring ("periodic") physical and chemical properties. The current standard table contains 118 confirmed elements as of 10 April 2010.
Although earlier precursors to this presentation exist, its invention is generally credited to the Russian chemist Dmitri Mendeleev in 1869, who intended the table to illustrate recurring trends in the properties of the elements. The layout of the table has been refined and extended over time as new elements have been discovered and new theoretical models have been developed to explain chemical behavior.
Use of the periodic table is now ubiquitous within the academic discipline of chemistry, providing an extremely useful framework to classify, systematize and compare all the many different forms of chemical behavior. The table has also found wide application in physics, geology, biology, materials science, engineering, agriculture, medicine, nutrition, environmental health, and astronomy. Its principles are especially important in chemical engineering.
Nomenclature and symbols.
The various chemical elements are formally identified by their unique atomic numbers, by their accepted names, and by their symbols.
Atomic numbers.
The known elements have atomic numbers from 1 through 118, conventionally presented as Arabic numerals. Since the elements can be uniquely sequenced by atomic number, conventionally from lowest to highest (as in a periodic table), sets of elements are sometimes specified by such notation as "through", "beyond", or "from ... through", as in "through iron", "beyond uranium", or "from lanthanum through lutetium". The terms "light" and "heavy" are sometimes also used informally to indicate relative atomic numbers (not densities!), as in "lighter than carbon" or "heavier than lead", although technically the weight or mass of atoms of an element (their atomic weights or atomic masses) do not always increase monotonically with their atomic numbers.
Element names.
The naming of various substances now known as elements precedes the atomic theory of matter, as names were given locally by various cultures to various minerals, metals, compounds, alloys, mixtures, and other materials, although at the time it was not known which chemicals were elements and which compounds. As they were identified as elements, the existing names for anciently-known elements (e.g., gold, mercury, iron) were kept in most countries. National differences emerged over the names of elements either for convenience, linguistic niceties, or nationalism. For a few illustrative examples: German speakers use "Wasserstoff" (water substance) for "hydrogen", "Sauerstoff" (acid substance) for "oxygen" and "Stickstoff" (smothering substance) for "nitrogen", while English and some romance languages use "sodium" for "natrium" and "potassium" for "kalium", and the French, Italians, Greeks, Portuguese and Poles prefer "azote/azot/azoto" (from roots meaning "no life") for "nitrogen".
For purposes of international communication and trade, the official names of the chemical elements both ancient and more recently recognized are decided by the International Union of Pure and Applied Chemistry (IUPAC), which has decided on a sort of international English language, drawing on traditional English names even when an element's chemical symbol is based on a Latin or other traditional word, for example adopting "gold" rather than "aurum" as the name for the 79th element (Au). IUPAC prefers the British spellings "aluminium" and "caesium" over the U.S. spellings "aluminum" and "cesium", and the U.S. "sulfur" over the British "sulphur". However, elements that are practical to sell in bulk in many countries often still have locally used national names, and countries whose national language does not use the Latin alphabet are likely to use the IUPAC element names.
According to IUPAC, chemical elements are not proper nouns in English; consequently, the full name of an element is not routinely capitalized in English, even if derived from a proper noun, as in californium and einsteinium. Isotope names of chemical elements are also uncapitalized if written out, "e.g.," carbon-12 or uranium-235. Chemical element "symbols" (such as Cf for californium and Es for einsteinium), are always capitalized (see below).
In the second half of the twentieth century, physics laboratories became able to produce nuclei of chemical elements with half-lives too short for an appreciable amount of them to exist at any time. These are also named by IUPAC, which generally adopts the name chosen by the discoverer. This practice can lead to the controversial question of which research group actually discovered an element, a question that delayed the naming of elements with atomic number of 104 and higher for a considerable time. (See element naming controversy).
Precursors of such controversies involved the nationalistic namings of elements in the late 19th century. For example, "lutetium" was named in reference to Paris, France. The Germans were reluctant to relinquish naming rights to the French, often calling it "cassiopeium". Similarly, the British discoverer of "niobium" originally named it "columbium," in reference to the New World. It was used extensively as such by American publications prior to international standardization.
Chemical symbols.
Specific chemical elements.
Before chemistry became a science, alchemists had designed arcane symbols for both metals and common compounds. These were however used as abbreviations in diagrams or procedures; there was no concept of atoms combining to form molecules. With his advances in the atomic theory of matter, John Dalton devised his own simpler symbols, based on circles, to depict molecules.
The current system of chemical notation was invented by Berzelius. In this typographical system, chemical symbols are not mere abbreviations—though each consists of letters of the Latin alphabet. They are intended as universal symbols for people of all languages and alphabets.
The first of these symbols were intended to be fully universal. Since Latin was the common language of science at that time, they were abbreviations based on the Latin names of metals. Cu comes from Cuprum, Fe comes from Ferrum, Ag from Argentum. The symbols were not followed by a period (full stop) as with abbreviations. Later chemical elements were also assigned unique chemical symbols, based on the name of the element, but not necessarily in English. For example, sodium has the chemical symbol 'Na' after the Latin "natrium". The same applies to "W" (wolfram) for tungsten, "Fe" (ferrum) for iron, "Hg" (hydrargyrum) for mercury, "Sn" (stannum) for tin, "K" (kalium) for potassium, "Au" (aurum) for gold, "Ag" (argentum) for silver, "Pb" (plumbum) for lead, "Cu" (cuprum) for copper, and "Sb" (stibium) for antimony.
Chemical symbols are understood internationally when element names might require translation. There have sometimes been differences in the past. For example, Germans in the past have used "J" (for the alternate name Jod) for iodine, but now use "I" and "Iod."
The first letter of a chemical symbol is always capitalized, as in the preceding examples, and the subsequent letters, if any, are always lower case (small letters). Thus, the symbols for californium or einsteinium are Cf and Es.
General chemical symbols.
There are also symbols in chemical equations for groups of chemical elements, for example in comparative formulas. These are often a single capital letter, and the letters are reserved and not used for names of specific elements. For example, an "X" indicates a variable group (usually a halogen) in a class of compounds, while "R" is a radical, meaning a compound structure such as a hydrocarbon chain. The letter "Q" is reserved for "heat" in a chemical reaction. "Y" is also often used as a general chemical symbol, although it is also the symbol of yttrium. "Z" is also frequently used as a general variable group. "E" is used in organic chemistry to denote an electron-withdrawing group. "L" is used to represent a general ligand in inorganic and organometallic chemistry. "M" is also often used in place of a general metal.
At least two additional, two-letter generic chemical symbols are also in informal usage, "Ln" for any lanthanide element and "An" for any actinide element. "Rg" was formerly used for any rare gas element, but the group of rare gases has now been renamed noble gases and the symbol "Rg" has now been assigned to the element roentgenium.
Isotope symbols.
Isotopes are distinguished by the atomic mass number (total protons and neutrons) for a particular isotope of an element, with this number combined with the pertinent element's symbol. IUPAC prefers that isotope symbols be written in superscript notation when practical, for example 12C and 235U. However, other notations, such as carbon-12 and uranium-235, or C-12 and U-235, are also used.
As a special case, the three naturally occurring isotopes of the element hydrogen are often specified as H for 1H (protium), D for 2H (deuterium), and T for 3H (tritium). This convention is easier to use in chemical equations, replacing the need to write out the mass number for each atom. For example, the formula for heavy water may be written D2O instead of 2H2O.
Origin of the elements.
Only about 4% of the total mass of the universe is made of atoms or ions, and thus represented by chemical elements. This fraction is about 15% of the total matter, with the remainder of the matter (85%) being dark matter. The nature of dark matter is unknown, but it is not composed of atoms of chemical elements because it contains no protons, neutrons, or electrons. (The remaining non-matter part of the mass of the universe is composed of the even more mysterious dark energy).
The universe's 94 naturally occurring chemical elements are thought to have been produced by at least four cosmic processes. Most of the hydrogen and helium in the universe was produced primordially in the first few minutes of the Big Bang. Three recurrently occurring later processes are thought to have produced the remaining elements. Stellar nucleosynthesis, an ongoing process, produces all elements from carbon through iron in atomic number, but little lithium, beryllium, or boron. Elements heavier in atomic number than iron, as heavy as uranium and plutonium, are produced by explosive nucleosynthesis in supernovas and other cataclysmic cosmic events. Cosmic ray spallation (fragmentation) of carbon, nitrogen, and oxygen is important to the production of lithium, beryllium and boron.
During the early phases of the Big Bang, nucleosynthesis of hydrogen nuclei resulted in the production of hydrogen-1 (protium, 1H) and helium-4 (4He), as well as a smaller amount of deuterium (2H) and very minuscule amounts (on the order of 10−10) of lithium and beryllium. Even smaller amounts of boron may have been produced in the Big Bang, since it has been observed in some very old stars, while carbon has not. It is generally agreed that no heavier elements than boron were produced in the Big Bang. As a result, the primordial abundance of atoms (or ions) consisted of roughly 75% 1H, 25% 4He, and 0.01% deuterium, with only tiny traces of lithium, beryllium, and perhaps boron. Subsequent enrichment of galactic halos occurred due to stellar nucleosynthesis and supernova nucleosynthesis. However, the element abundance in intergalactic space can still closely resemble primordial conditions, unless it has been enriched by some means.
On Earth (and elsewhere), trace amounts of various elements continue to be produced from other elements as products of natural transmutation processes. These include some produced by cosmic rays or other nuclear reactions (see cosmogenic and nucleogenic nuclides), and others produced as decay products of long-lived primordial nuclides. For example, trace (but detectable) amounts of carbon-14 (14C) are continually produced in the atmosphere by cosmic rays impacting nitrogen atoms, and argon-40 (40Ar) is continually produced by the decay of primordially occurring but unstable potassium-40 (40K). Also, three primordially occurring but radioactive actinides, thorium, uranium, and plutonium, decay through a series of recurrently produced but unstable radioactive elements such as radium and radon, which are transiently present in any sample of these metals or their ores or compounds. Three other radioactive elements, technetium, promethium, and neptunium, occur only incidentally in natural materials, produced as individual atoms by natural fission of the nuclei of various heavy elements or in other rare nuclear processes.
Human technology has produced various additional elements beyond these first 94, with those through atomic number 118 now known.
Abundance.
The following graph (note log scale) shows the abundance of elements in our solar system. The table shows the twelve most common elements in our galaxy (estimated spectroscopically), as measured in parts per million, by mass. Nearby galaxies that have evolved along similar lines have a corresponding enrichment of elements heavier than hydrogen and helium. The more distant galaxies are being viewed as they appeared in the past, so their abundances of elements appear closer to the primordial mixture. As physical laws and processes appear common throughout the visible universe, however, scientist expect that these galaxies evolved elements in similar abundance.
The abundance of elements in the Solar System is in keeping with their origin from nucleosynthesis in the Big Bang and a number of progenitor supernova stars. Very abundant hydrogen and helium are products of the Big Bang, but the next three elements are rare since they had little time to form in the Big Bang and are not made in stars (they are, however, produced in small quantities by the breakup of heavier elements in interstellar dust, as a result of impact by cosmic rays). Beginning with carbon, elements are produced in stars by buildup from alpha particles (helium nuclei), resulting in an alternatingly larger abundance of elements with even atomic numbers (these are also more stable). In general, such elements up to iron are made in large stars in the process of becoming supernovas. Iron-56 is particularly common, since it is the most stable element that can easily be made from alpha particles (being a product of decay of radioactive nickel-56, ultimately made from 14 helium nuclei). Elements heavier than iron are made in energy-absorbing processes in large stars, and their abundance in the universe (and on Earth) generally decreases with their atomic number.
The abundance of the chemical elements on Earth varies from air to crust to ocean, and in various types of life. The abundance of elements in Earth's crust differs from that in the Solar system (as seen in the Sun and heavy planets like Jupiter) mainly in selective loss of the very lightest elements (hydrogen and helium) and also volatile neon, carbon (as hydrocarbons), nitrogen and sulfur, as a result of solar heating in the early formation of the solar system. Oxygen, the most abundant Earth element by mass, is retained on Earth by combination with silicon. Aluminum at 8% by mass is more common in the Earth's crust than in the universe and solar system, but the composition of the far more bulky mantle, which has magnesium and iron in place of aluminum (which occurs there only at 2% of mass) more closely mirrors the elemental composition of the solar system, save for the noted loss of volatile elements to space, and loss of iron which has migrated to the Earth's core.
The composition of the human body, by contrast, more closely follows the composition of seawater—save that the human body has additional stores of carbon and nitrogen necessary to form the proteins and nucleic acids, together with phosphorus in the nucleic acids and energy transfer molecule adenosine triphosphate (ATP) that occurs in the cells of all living organisms. Certain kinds of organisms require particular additional elements, for example the magnesium in chlorophyll in green plants, the calcium in mollusc shells, or the iron in the hemoglobin in vertebrate animals' red blood cells.
History.
Evolving definitions.
The concept of an "element" as an undivisible substance has developed through three major historical phases: Classical definitions (such as those of the ancient Greeks), chemical definitions, and atomic definitions.
Classical definitions.
Ancient philosophy posited a set of classical elements to explain observed patterns in nature. These "elements" originally referred to "earth", "water", "air" and "fire" rather than the chemical elements of modern science.
The term 'elements' ("stoicheia") was first used by the Greek philosopher Plato in about 360 BCE in his dialogue Timaeus, which includes a discussion of the composition of inorganic and organic bodies and is a speculative treatise on chemistry. Plato believed the elements introduced a century earlier by Empedocles were composed of small polyhedral forms: tetrahedron (fire), octahedron (air), icosahedron (water), and cube (earth).
Aristotle, c. 350 BCE, also used the term "stoicheia" and added a fifth element called aether, which formed the heavens. Aristotle defined an element as:
Chemical definitions.
In 1661, Robert Boyle proposed his theory of corpuscularism which favoured the analysis of matter as constituted by irreducible units of matter (atoms) and, choosing to side with neither Aristotle's view of the four elements nor Paracelsus' view of three fundamental elements, left open the question of the number of elements. The first modern list of chemical elements was given in Antoine Lavoisier's 1789 "Elements of Chemistry", which contained thirty-three elements, including light and caloric. By 1818, Jöns Jakob Berzelius had determined atomic weights for forty-five of the forty-nine then-accepted elements. Dmitri Mendeleev had sixty-six elements in his periodic table of 1869.
From Boyle until the early 20th century, an element was defined as a pure substance that could not be decomposed into any simpler substance. Put another way, a chemical element cannot be transformed into other chemical elements by chemical processes. Elements during this time were generally distinguished by their atomic weights, a property measurable with fair accuracy by available analytical techniques.
Atomic definitions.
The 1913 discovery by English physicist Henry Moseley that the nuclear charge is the physical basis for an atom's atomic number, further refined when the nature of protons and neutrons became appreciated, eventually led to the current definition of an element based on atomic number (number of protons per atomic nucleus). The use of atomic numbers, rather than atomic weights, to distinguish elements has greater predictive value (since these numbers are integers), and also resolves some ambiguities in the chemistry-based view due to varying properties of isotopes and allotropes within the same element. Currently, IUPAC defines an element to exist if it has isotopes with a lifetime longer than the 10−14 seconds it takes the nucleus to form an electronic cloud.
By 1914, seventy-two elements were known, all naturally occurring. The remaining naturally occurring elements were discovered or isolated in subsequent decades, and various additional elements have also been produced synthetically, with much of that work pioneered by Glenn T. Seaborg. In 1955, element 101 was discovered and named mendelevium in honor of D.I. Mendeleev, the first to arrange the elements in a periodic manner. Most recently, the synthesis of element 118 was reported in October 2006, and the synthesis of element 117 was reported in April 2010.
Discovery and recognition of various elements.
Ten materials familiar to various prehistoric cultures are now known to be chemical elements: Carbon, copper, gold, iron, lead, mercury, silver, sulfur, tin, and zinc. Three additional materials now accepted as elements, arsenic, antimony, and bismuth, were recognized as distinct substances prior to 1500 AD. Phosphorus, cobalt, and platinum were isolated before 1750.
Most of the remaining naturally occurring chemical elements were identified and characterized by 1900, including:
Elements isolated or produced since 1900 include:
Recently discovered elements.
The first transuranium element (element with atomic number greater than 92) discovered was neptunium in 1940. Since 1999 claims for the discovery of new elements have been considered by the IUPAC/IUPAP Joint Working Party. As of May 2012, only the elements up to 112, copernicium, as well as element 114 (flerovium) and element 116 (livermorium) have been confirmed as discovered by IUPAC, while claims have been made for synthesis of elements 113, 115, 117 and 118. The discovery of element 112 was acknowledged in 2009, and the name "copernicium" and the atomic symbol "Cn" were suggested for it. The name and symbol were officially endorsed by IUPAC on 19 February 2010. The heaviest element that is believed to have been synthesized to date is element 118, ununoctium, on 9 October 2006, by the Flerov Laboratory of Nuclear Reactions in Dubna, Russia. Element 117 was the latest element claimed to be discovered, in 2009. IUPAC officially recognized flerovium and livermorium, elements 114 and 116, in June 2011 and approved their names in May 2012.
List of the 118 known chemical elements.
The following sortable table includes the 118 known chemical elements, with the names linking to the "Wikipedia" articles on each.

</doc>
<doc id="226680" url="https://en.wikipedia.org/wiki?curid=226680" title="Chi-squared test">
Chi-squared test

A chi-squared test, also referred to as formula_1 test (or chi-square test), is any statistical hypothesis test in which the sampling distribution of the test statistic is a chi-square distribution when the null hypothesis is true. Chi-squared tests are often constructed from a sum of squared errors, or through the sample variance. Test statistics that follow a chi-squared distribution arise from an assumption of independent normally distributed data, which is valid in many cases due to the central limit theorem. A chi-squared test can then be used to reject the hypothesis that the data are independent.
Also considered a chi-square test is a test in which this is "asymptotically" true, meaning that the sampling distribution (if the null hypothesis is true) can be made to approximate a chi-square distribution as closely as desired by making the sample size large enough.
The chi-squared test is used to determine whether there is a significant difference between the expected
frequencies and the observed frequencies in one or more categories. Does the number of individuals or objects that
fall in each category differ significantly from the number you would expect? Is this difference between the
expected and observed due to sampling variation, or is it a real difference?
Examples of chi-square tests with samples.
One test statistic that follows a chi-square distribution exactly is the test that the variance of a normally distributed population has a given value based on a sample variance. Such tests are uncommon in practice because the true variance of the population is usually unknown. However, there are several statistical tests where the chi-square distribution is approximately valid:
Pearson's chi-square test.
Pearson's chi-square test, also known as the chi-square goodness-of-fit test or chi-square test for independence. When the chi-square test is mentioned without any modifiers or without other precluding context, this test is often meant (for an exact test used in place of formula_1, see Fisher's exact test).
Yates's correction for continuity.
Using the chi-square distribution to interpret Pearson's chi-square statistic requires one to assume that the discrete probability of observed binomial frequencies in the table can be approximated by the continuous chi-square distribution. This assumption is not quite correct, and introduces some error.
To reduce the error in approximation, Frank Yates suggested a correction for continuity that adjusts the formula for Pearson's chi-square test by subtracting 0.5 from the difference between each observed value and its expected value in a 2 × 2 contingency table. This reduces the chi-square value obtained and thus increases its p-value.
Chi-squared test for variance in a normal population.
If a sample of size "n" is taken from a population having a normal distribution, then there is a result (see distribution of the sample variance) which allows a test to be made of whether the variance of the population has a pre-determined value. For example, a manufacturing process might have been in stable condition for a long period, allowing a value for the variance to be determined essentially without error. Suppose that a variant of the process is being tested, giving rise to a small sample of "n" product items whose variation is to be tested. The test statistic "T" in this instance could be set to be the sum of squares about the sample mean, divided by the nominal value for the variance (i.e. the value to be tested as holding). Then "T" has a chi-square distribution with "n" − 1 degrees of freedom. For example if the sample size is 21, the acceptance region for "T" for a significance level of 5% is the interval 9.59 to 34.17.
Example chi-squared test for categorical data.
Suppose there is a city of 1 million residents with four neighborhoods: A, B, C, and D. A random sample of 650 residents of the city is taken and their occupation is recorded as "blue collar", "white collar", or "no collar". The null hypothesis is that each person's neighborhood of residence is independent of the person's occupational classification. The data are tabulated as:
Let us take the sample living in neighborhood A, 150/650, to estimate what proportion of the whole 1 million people live in neighborhood A. Similarly we take 349/650 to estimate what proportion of the 1 million people are white-collar workers. By the assumption of independence under the hypothesis we should "expect" the number of white-collar workers in neighborhood A to be
Then in that "cell" of the table, we have
The sum of these quantities over all of the cells is the test statistic. Under the null hypothesis, it has approximately a chi-square distribution whose number of degrees of freedom are
If the test statistic is improbably large according to that chi-square distribution, then one rejects the null hypothesis of independence.
A related issue is a test of homogeneity. Suppose that instead of giving every resident of each of the four neighborhoods an equal chance of inclusion in the sample, we decide in advance how many residents of each neighborhood to include. Then each resident has the same chance of being chosen as do all residents of the same neighborhood, but residents of different neighborhoods would have different probabilities of being chosen if the four sample sizes are not proportional to the populations of the four neighborhoods. In such a case, we would be testing "homogeneity" rather than "independence". The question is whether the proportions of blue-collar, white-collar, and no-collar workers in the four neighborhoods are the same. However, the test is done in the same way.
Applications.
In cryptanalysis, chi-square test is used to compare the distribution of plaintext and (possibly) decrypted ciphertext. The lowest value of the test means that the decryption was successful with high probability. This method can be generalized for solving modern cryptographic problems.

</doc>
<doc id="1170169" url="https://en.wikipedia.org/wiki?curid=1170169" title="Chirality (physics)">
Chirality (physics)

 
A chiral phenomenon is one that is not identical to its mirror image (see the article on mathematical chirality). The spin of a particle may be used to define a handedness, or helicity, for that particle which, in the case of a massless particle, is the same as chirality. A symmetry transformation between the two is called parity. Invariance under parity by a Dirac fermion is called chiral symmetry.
An experiment on the weak decay of cobalt-60 nuclei carried out by Chien-Shiung Wu and collaborators in 1957 demonstrated that parity is not a symmetry of the universe.
Chirality and helicity.
The helicity of a particle is right-handed if the direction of its spin is the same as the direction of its motion. It is left-handed if the directions of spin and motion are opposite. By convention for rotation, a standard clock, with its spin vector defined by the rotation of its hands, tossed with its face directed forwards, has left-handed helicity. Mathematically, helicity is the sign of the projection of the spin vector onto the momentum vector: left is negative, right is positive.
The chirality of a particle is more abstract. It is determined by whether the particle transforms in a right- or left-handed representation of the Poincaré group. (However, some representations, such as Dirac spinors, have both right- and left-handed components. In cases like this, we can define projection operators that project out either the right or left hand components and discuss the right- and left-handed portions of the representation.)
For massless particles—such as the photon, the gluon, and the (hypothetical) graviton—chirality is the same as helicity; a given massless particle appears to spin in the same direction along its axis of motion regardless of point of view of the observer.
For massive particles—such as electrons, quarks, and neutrinos—chirality and helicity must be distinguished. In the case of these particles, it is possible for an observer to change to a reference frame that overtakes the spinning particle, in which case the particle will then appear to move backwards, and its helicity (which may be thought of as 'apparent chirality') will be reversed.
A "massless" particle moves with the speed of light, so a real observer (who must always travel at less than the speed of light) cannot be in any reference frame where the particle appears to reverse its relative direction, meaning that all real observers see the same chirality. Because of this, the direction of spin of massless particles is not affected by a Lorentz boost (change of viewpoint) in the direction of motion of the particle, and the sign of the projection (helicity) is fixed for all reference frames: the helicity of massless particles is a relativistic invariant (i.e. a quantity whose value is the same in all inertial reference frames).
With the discovery of neutrino oscillation, which implies that neutrinos have mass, the only observed massless particle is the photon. The gluon is also expected to be massless, although the assumption that it is has not been conclusively tested. Hence, these are the only two particles now known for which helicity could be identical to chirality, and only one of them has been confirmed by measurement. All other observed particles have mass and thus may have different helicities in different reference frames. It is still possible that as-yet unobserved particles, like the graviton, might be massless, and hence have invariant helicity like the photon.
Chiral theories.
Only left-handed fermions interact with the weak interaction. In most circumstances, two left-handed fermions interact more strongly than right-handed or opposite-handed fermions, implying that the universe has a preference for left-handed chirality, which violates a symmetry of the other forces of nature.
Chirality for a Dirac fermion is defined through the operator , which has eigenvalues ±1. 
Any Dirac field can thus be projected into its left- or right-handed component by acting with the projection operators or on . 
The coupling of the charged weak interaction to fermions is proportional to the first projection operator, which is responsible for this interaction's parity symmetry violation.
A common source of confusion is due to conflating this operator with the helicity operator. Since the helicity of massive particles is frame-dependent, it might seem that the same particle would interact with the weak force according to one frame of reference, but not another. The resolution to this false paradox is that "the chirality operator is equivalent to helicity for massless fields only", for which helicity is not frame-dependent. By contrast, for massive particles, "chirality is not the same as helicity", so there is no frame dependence of the weak interaction: a particle that couples the weak force in one frame, does so in every frame.
A theory that is asymmetric with respect to chiralities is called a "chiral theory", while a non-chiral (i.e., parity-symmetric) theory is sometimes called a "vector theory". Many pieces of the Standard Model of physics are non-chiral, which is traceable to anomaly cancellation in chiral theories. Quantum chromodynamics is an example of a "vector theory", since both chiralities of all quarks appear in the theory, and couple to gluons in the same way.
The electroweak theory, developed in the mid 20th century, is an example of a "chiral theory". Originally, it assumed that neutrinos were massless, and only assumed the existence of left-handed neutrinos (along with their complementary right-handed antineutrinos). After the observation of neutrino oscillations, which imply that neutrinos are massive like all other fermions, the revised theories of the electroweak interaction now include both right- and left-handed neutrinos. However, it is still a chiral theory, as it does not respect parity symmetry.
The exact nature of the neutrino is still unsettled and so the electroweak theories that have been proposed are somewhat different, but most accommodate the chirality of neutrinos in the same way as was already done for all other fermions.
Chiral symmetry.
Vector gauge theories with massless Dirac fermion fields exhibit chiral symmetry, i.e., rotating the left-handed and the right-handed components independently makes no difference to the theory. We can write this as the action of rotation on the fields:
or
With flavors, we have unitary rotations instead: U("N")"L"×U(N)"R".
More generally, we write the right-handed and left-handed states as a projection operator acting on a spinor. The right-handed and left-handed projection operators are
and
Massive fermions do not exhibit chiral symmetry, as the mass term in the Lagrangian, , breaks chiral symmetry explicitly. 
Spontaneous chiral symmetry breaking may also occur in some theories, as it most notably does in quantum chromodynamics.
The chiral symmetry transformation can be divided into a component that treats the left-handed and the right-handed parts equally, known as vector symmetry, and a component that actually treats them differently, known as axial symmetry. A scalar field model encoding chiral symmetry and its breaking is the sigma model. 
The most common application is expressed as equal treatment of clockwise and counter-clockwise rotations from a fixed frame of reference.
The general principle is often referred to by the name chiral symmetry. The rule is absolutely valid in the classical mechanics of Newton and Einstein, but results from quantum mechanical experiments show a difference in the behavior of left-chiral versus right-chiral subatomic particles.
Example: "u" and "d" quarks in QCD.
Consider quantum chromodynamics (QCD) with two "massless" quarks "u" and "d" (massive fermions do not exhibit chiral symmetry). The Lagrangian reads
In terms of left-handed and right-handed spinors, it reads
Defining
it can be written as
The Lagrangian is unchanged under a rotation of "qL" by any 2 x 2 unitary matrix "L", and "qR" by any 2 x 2 unitary matrix "R". 
This symmetry of the Lagrangian is called "flavor chiral symmetry", and denoted as "U(2)L×U(2)R". It decomposes into
The singlet vector symmetry, "U(1)V", acts as
and corresponds to baryon number conservation.
The singlet axial group "U(1)A" acts as
and it does not correspond to a conserved quantity, because it is explicitly violated due to a quantum anomaly.
The remaining chiral symmetry "SU(2)L×SU(2)R" turns out to be spontaneously broken by a quark condensate 
formula_15 formed through nonperturbative action of QCD gluons, 
into the diagonal vector subgroup "SU(2)V" known as isospin. The Goldstone bosons corresponding to the three broken generators are the three pions. 
As a consequence, the effective theory of QCD bound states like the baryons, must now include mass terms for them, ostensibly disallowed by unbroken chiral symmetry. Thus, this chiral symmetry breaking induces the bulk of hadron masses, such as those for the nucleons−−in effect, the bulk of the mass of all visible matter.
In the real world, because of the nonvanishing and differing masses of the quarks, "SU(2)L×SU(2)R" is only an approximate symmetry to begin with, and therefore the pions are not massless, but have small masses: they are pseudo-Goldstone bosons.
More Flavors.
For more "light" quark species, flavors in general, the corresponding chiral symmetries are "U(N)L×U(N)R", decomposing into
and exhibiting a very analogous chiral symmetry breaking pattern. 
Most usually, =3 is taken, the "u, d", and "s" quarks taken to be light (the Eightfold way (physics)), so then approximately massless for the symmetry to be meaningful to a lowest order, while the other three quarks are sufficiently heavy to barely have a residual chiral symmetry be visible for practical purposes.
An application in Particle Physics.
In theoretical physics, the electroweak model breaks parity maximally. All its fermions are chiral Weyl fermions, which means that the charged weak gauge bosons only couple to left-handed quarks and leptons. (Note that the neutral electroweak Z boson already couples to left "and" right-handed fermions.) 
Some theorists found this objectionable, and so conjectured a GUT extension of the weak force which has new, high energy W' and Z' bosons, which now couple with right handed quarks and leptons:
to
Here, SU(2)L (pronounced SU(2) left) is none other than the above SU(2)W, while B−L is the baryon number minus the lepton number. The electric charge formula in this model is given by
where formula_20 are the weak isospin values of the fields in the theory.
There is also the chromodynamic SU(3)C. The idea was to restore parity by introducing a left-right symmetry. This is a group extension of Z2 (the left-right symmetry) by
to the semidirect product
This has two connected components where Z2 acts as an automorphism, which is the composition of an involutive outer automorphism of SU(3)C with the interchange of the left and right copies of SU(2) with the reversal of U(1)B−L. It was shown by Rabindra N. Mohapatra and Goran Senjanovic in 1975 that left-right symmetry can be spontaneously broken to give a chiral low energy theory, which is the Standard Model of Glashow, Weinberg and Salam and it also connects the small observed neutrino masses to the breaking of left-right symmetry via the seesaw mechanism.
In this setting, the chiral quarks
and
are unified into an irrep
The leptons are also unified into an irrep,
The Higgs bosons needed to implement the breaking of left-right symmetry down to the Standard Model
are
This then predicts three sterile neutrinos, which is perfectly consistent with neutrino oscillation data. Within the seesaw mechanism, the sterile neutrinos become superheavy without affecting physics at low energies.
Because the left-right symmetry is spontaneously broken, left-right models predict domain walls.
This left-right symmetry idea first appeared in the Pati–Salam model (1974), Mohapatra–Pati models (1975).

</doc>
<doc id="508177" url="https://en.wikipedia.org/wiki?curid=508177" title="Circle group">
Circle group

In mathematics, the circle group, denoted by T, is the multiplicative group of all complex numbers with absolute value 1, i.e., the unit circle in the complex plane or simply the unit complex numbers
The circle group forms a subgroup of C×, the multiplicative group of all nonzero complex numbers. Since C× is abelian, it follows that T is as well. The circle group is also the group U(1) of 1×1 unitary matrices; these act on the complex plane by rotation about the origin. The circle group can be parametrized by the angle θ of rotation by
This is the exponential map for the circle group.
The circle group plays a central role in Pontryagin duality, and in the theory of Lie groups.
The notation T for the circle group stems from the fact that, with the standard topology (see below), the circle group is a 1-torus. More generally T"n" (the direct product of T with itself "n" times) is geometrically an "n"-torus. 
Elementary introduction.
One way to think about the circle group is that it describes how to add "angles", where only angles between 0° and 360° are permitted. For example, the diagram illustrates how to add 150° to 270°. The answer should be 150° + 270° = 420°, but when thinking in terms of the circle group, we need to "forget" the fact that we have wrapped once around the circle. Therefore we adjust our answer by 360° which gives 420° = 60° (mod 360°).
Another description is in terms of ordinary addition, where only numbers between 0 and 1 are allowed (with 1 corresponding to a full rotation). To achieve this, we might need to throw away digits occurring before the decimal point. For example, when we work out 0.784 + 0.925 + 0.446, the answer should be 2.155, but we throw away the leading 2, so the answer (in the circle group) is just 0.155.
Topological and analytic structure.
The circle group is more than just an abstract algebraic object. It has a natural topology when regarded as a subspace of the complex plane. Since multiplication and inversion are continuous functions on C×, the circle group has the structure of a topological group. Moreover, since the unit circle is a closed subset of the complex plane, the circle group is a closed subgroup of C× (itself regarded as a topological group).
One can say even more. The circle is a 1-dimensional real manifold and multiplication and inversion are real-analytic maps on the circle. This gives the circle group the structure of a one-parameter group, an instance of a Lie group. In fact, up to isomorphism, it is the unique 1-dimensional compact, connected Lie group. Moreover, every "n"-dimensional compact, connected, abelian Lie group is isomorphic to T"n".
Isomorphisms.
The circle group shows up in a variety of forms in mathematics. We list some of the more common forms here. Specifically, we show that
Note that the slash (/) denotes here quotient group.
The set of all 1×1 unitary matrices clearly coincides with the circle group; the unitary condition is equivalent to the condition that its element have absolute value 1. Therefore, the circle group is canonically isomorphic to U(1), the first unitary group.
The exponential function gives rise to a group homomorphism exp : R → T from the additive real numbers R to the circle group T via the map
The last equality is Euler's formula or the complex exponential. The real number θ corresponds to the angle on the unit circle as measured counterclockwise from the positive "x"-axis. That this map is a homomorphism follows from the fact that the multiplication of unit complex numbers corresponds to addition of angles:
This exponential map is clearly a surjective function from R to T. It is not, however, injective. The kernel of this map is the set of all integer multiples of 2π. By the first isomorphism theorem we then have that
After rescaling we can also say that T is isomorphic to R/Z.
If complex numbers are realized as 2×2 real matrices (see complex number), the unit complex numbers correspond to 2×2 orthogonal matrices with unit determinant. Specifically, we have
The circle group is therefore isomorphic to the special orthogonal group SO(2). This has the geometric interpretation that multiplication by a unit complex number is a proper rotation in the complex plane, and every such rotation is of this form.
Properties.
Every compact Lie group "G" of dimension > 0 has a subgroup isomorphic to the circle group. That means that, thinking in terms of symmetry, a compact symmetry group acting "continuously" can be expected to have one-parameter circle subgroups acting; the consequences in physical systems are seen for example at rotational invariance, and spontaneous symmetry breaking.
The circle group has many subgroups, but its only proper closed subgroups consist of roots of unity: For each integer "n" > 0, the "n"th roots of unity form a cyclic group of order "n", which is unique up to isomorphism.
Representations.
The representations of the circle group are easy to describe. It follows from Schur's lemma that the irreducible complex representations of an abelian group are all 1-dimensional. Since the circle group is compact, any representation ρ : T → "GL"(1, C) ≅ C×, must take values in "U"(1) ≅ T. Therefore, the irreducible representations of the circle group are just the homomorphisms from the circle group to itself.
These representations are all inequivalent. The representation "φ"−"n" is conjugate to "φ""n",
These representations are just the characters of the circle group. The character group of T is clearly an infinite cyclic group generated by φ1:
The irreducible real representations of the circle group are the trivial representation (which is 1-dimensional) and the representations
taking values in SO(2). Here we only have positive integers "n" since the representation formula_11 is equivalent to formula_12.
Group structure.
In this section we will forget about the topological structure of the circle group and look only at its structure as an abstract group.
The circle group T is a divisible group. Its torsion subgroup is given by the set of all "n"th roots of unity for all "n", and is isomorphic to Q/Z. The structure theorem for divisible groups and the axiom of choice together tell us that T is isomorphic to the direct sum of Q/Z with a number of copies of Q. The number of copies of Q must be "c" (the cardinality of the continuum) in order for the cardinality of the direct sum to be correct. But the direct sum of "c" copies of Q is isomorphic to R, as R is a vector space of dimension "c" over Q. Thus
The isomorphism
can be proved in the same way, as C× is also a divisible abelian group whose torsion subgroup is the same as the torsion subgroup of T.

</doc>
<doc id="3367262" url="https://en.wikipedia.org/wiki?curid=3367262" title="Circular convolution">
Circular convolution

The circular convolution, also known as cyclic convolution, of two aperiodic functions (i.e. Schwartz functions) occurs when one of them is convolved in the normal way with a periodic summation of the other function.  That situation arises in the context of the Circular convolution theorem.  The identical operation can also be expressed in terms of the periodic summations of both functions, if the infinite integration interval is reduced to just one period.  That situation arises in the context of the discrete-time Fourier transform (DTFT) and is also called periodic convolution.  In particular, the DTFT of the product of two discrete sequences is the periodic convolution of the DTFTs of the individual sequences.
Let "x" be a function with a well-defined periodic summation, "x""T", where:
If "h" is any other function for which the convolution "x""T" ∗ "h" exists, then the convolution "x""T" ∗ "h" is periodic and identical to:
where "t"o is an arbitrary parameter and "h""T" is a periodic summation of "h".
The second integral is called the periodic convolution of functions "x""T" and "h""T" and is sometimes normalized by 1/"T". When "x""T" is expressed as the periodic summation of another function, "x", the same operation may also be referred to as a circular convolution of functions "h" and "x".
Discrete sequences.
Similarly, for discrete sequences and period N, we can write the circular convolution of functions "h" and "x" as:
For the special case that the non-zero extent of both "x" and "h" are "≤ N", this is reducible to matrix multiplication where the kernel of the integral transform is a circulant matrix.
Example.
A case of great practical interest is illustrated in the figure. The duration of the x sequence is N (or less), and the duration of the h sequence is significantly less. Then many of the values of the circular convolution are identical to values of x∗h,  which is actually the desired result when the h sequence is a finite impulse response (FIR) filter. Furthermore, the circular convolution is very efficient to compute, using a fast Fourier transform (FFT) algorithm and the circular convolution theorem.
There are also methods for dealing with an x sequence that is longer than a practical value for N. The sequence is divided into segments ("blocks") and processed piecewise. Then the filtered segments are carefully pieced back together. Edge effects are eliminated by overlapping either the input blocks or the output blocks. To help explain and compare the methods, we discuss them both in the context of an h sequence of length 201 and an FFT size of "N" = 1024.
Overlapping input blocks
This method uses a block size equal to the FFT size (1024). We describe it first in terms of normal or "linear" convolution. When a normal convolution is performed on each block, there are start-up and decay transients at the block edges, due to the filter "latency" (200-samples). Only 824 of the convolution outputs are unaffected by edge effects. The others are discarded, or simply not computed. That would cause gaps in the output if the input blocks are contiguous. The gaps are avoided by overlapping the input blocks by 200 samples. In a sense, 200 elements from each input block are "saved" and carried over to the next block. This method is referred to as overlap-save, although the method we describe next requires a similar "save" with the output samples.
When the DFT or FFT is used, we don't have the option of not computing the affected samples, but the leading and trailing edge-effects are overlapped and added because of circular convolution. Consequently, the 1024-point inverse FFT (IFFT) output contains only 200 samples of edge effects (which are discarded) and the 824 unaffected samples (which are kept). To illustrate this, the fourth frame of the figure at right depicts a block that has been periodically (or "circularly") extended, and the fifth frame depicts the individual components of a linear convolution performed on the entire sequence. The edge effects are where the contributions from the extended blocks overlap the contributions from the original block. The last frame is the composite output, and the section colored green represents the unaffected portion.
Overlapping output blocks
This method is known as overlap-add. In our example, it uses contiguous input blocks of size 824 and pads each one with 200 zero-valued samples. Then it overlaps and adds the 1024-element output blocks. Nothing is discarded, but 200 values of each output block must be "saved" for the addition with the next block. Both methods advance only 824 samples per 1024-point IFFT, but overlap-save avoids the initial zero-padding and final addition.

</doc>
<doc id="2217599" url="https://en.wikipedia.org/wiki?curid=2217599" title="Circular symmetry">
Circular symmetry

In geometry, circular symmetry is a type of continuous symmetry for a planar object that can be rotated by any arbitrary angle and map onto itself. 
Rotational circular symmetry is isomorphic with the circle group in the complex plane, or the special orthogonal group SO(2), and unitary group U(1). Reflective circular symmetry is isomorphic with the orthogonal group O(2).
Two dimensions.
A 2-dimensional object with circular symmetry would consist of concentric circles and annular domains. 
Rotational circular symmetry has all cyclic symmetry, Z"n"v as subgroup symmtries. Reflective circular symmetry has all dihedral symmetry, Dih"n"v as subgroup symmetries.
Three dimensions.
In 3-dimensions, a surface or solid of revolution has circular symmetry around an axis, also called cylindrical symmetry. For example: a right circular cone. Circular symmetry in 3 dimensions has all pyramidal symmetry, C"n"v as subgroups.
A double-cone, bicone, cylinder, toroid and spheroid have circular symmetry, and in addition have a bilateral symmetry perpendular to the axis of system (or half cylindrical symmetry). These reflective circular symmetries have all discrete prismatic symmetries, D"n"h as subgroups.
Four dimensions.
In four dimensions, an object can have circular symmetry, on two orthogonal axis planes, or duocylindrical symmetry. For example the duocylinder and Clifford torus have circular symmetry in two orthogonal axes. An spherinder has spherical symmetry in one 3-space, and circular symmetry in the orthogonal direction.
Mathematical physics.
In mathematical physics circular symmetry in a 2-dimensional field which can be expressed as a function of distance from a central point only. This means that all points on each circle take the same value. An example would be magnetic field intensity in a plane perpendicular to a current-carrying wire. Recognizing circular symmetry in a 2 or 3-dimensional problem can reduce its complexity to a lower physical dimension of consideration.
Spherical symmetry.
An analogous 3-dimensional equivalent term is spherical symmetry. 
Rotational spherical symmetry is isomorphic with the rotation group SO(3), and can be parametrized by the Davenport chained rotations pitch, yaw, and roll. Rotational spherical symmetry has all the discrete chiral 3D point groups as subgroups. Reflectional spherical symmetry is isomorphic with the orthogonal group O(3) and has the 3-dimensional discrete point groups as subgroups.
A scalar field has spherical symmetry if it depends on the distance to the origin only, such as the potential of a central force. A vector field has spherical symmetry if it is in radially inward or outward direction with a magnitude and orientation (inward/outward) depending on the distance to the origin only, such as a central force.

</doc>
<doc id="5916" url="https://en.wikipedia.org/wiki?curid=5916" title="Circumference">
Circumference

Circumference (from Latin "circumferentia", meaning "carrying around") is the linear distance around the edge of a closed curve or circular object. The circumference of a circle is of special importance in geometry and trigonometry. Informally "circumference" may also refer to the edge itself rather than to the length of the edge. Circumference is a special case of perimeter: the perimeter is the length around any closed figure, but conventionally "perimeter" is typically used in reference to a polygon while "circumference" typically refers to a continuously differentiable curve.
Circumference of a circle.
The circumference of a circle is the distance around it. The term is used when measuring physical objects, as well as when considering abstract geometric forms.
Relationship with Pi.
The circumference of a circle relates to one of the most important mathematical constants in all of mathematics. This constant, pi, is represented by the Greek letter . The numerical value of is 3.14159 26535 89793 ... (see ). Pi is defined as the ratio of a circle's circumference to its diameter :
Or, equivalently, as the ratio of the circumference to twice the radius. The above formula can be rearranged to solve for the circumference:
The use of the mathematical constant is ubiquitous in mathematics, engineering, and science. The constant ratio of circumference to radius formula_3 also has many uses in mathematics, engineering, and science. These uses include but are not limited to radians, computer programming, and physical constants. The Greek letter (tau) is sometimes used to represent this constant, but is not generally accepted as proper notation.
Circumference of an ellipse.
The circumference of an ellipse can be expressed in terms of the complete elliptic integral of the second kind.
Circumference of a graph.
In graph theory the circumference of a graph refers to the longest cycle contained in that graph.

</doc>
<doc id="426219" url="https://en.wikipedia.org/wiki?curid=426219" title="Classical electromagnetism">
Classical electromagnetism

Classical electromagnetism or classical electrodynamics is a branch of theoretical physics that studies the interactions between electric charges and currents using an extension of the classical Newtonian model. The theory provides an excellent description of electromagnetic phenomena whenever the relevant length scales and field strengths are large enough that quantum mechanical effects are negligible. For small distances and low field strengths, such interactions are better described by quantum electrodynamics.
Fundamental physical aspects of classical electrodynamics are presented in many texts, such as those by Feynman, Leighton and Sands, Panofsky and Phillips, and Jackson.
History.
The physical phenomena that electromagnetism describes have been studied as separate fields since antiquity. For example, there were many advances in the field of optics centuries before light was understood to be an electromagnetic wave. However, the theory of electromagnetism, as it is currently understood, grew out of Michael Faraday's experiments suggesting an electromagnetic field and James Clerk Maxwell's use of differential equations to describe it in his Treatise on Electricity and Magnetism (1873). For a detailed historical account, consult Pauli, Whittaker, Pais, and Hunt.
Lorentz force.
The electromagnetic field exerts the following force (often called the Lorentz force) on charged particles:
where all boldfaced quantities are vectors: F is the force that a charge "q" experiences, E is the electric field at the location of the charge, v is the velocity of the charge, B is the magnetic field at the location of the charge.
The above equation illustrates that the Lorentz force is the sum of two vectors. One is the cross product of the velocity and magnetic field vectors. Based on the properties of the cross product, this produces a vector that is perpendicular to both the velocity and magnetic field vectors. The other vector is in the same direction as the electric field. The sum of these two vectors is the Lorentz force.
Therefore, in the absence of a magnetic field, the force is in the direction of the electric field, and the magnitude of the force is dependent on the value of the charge and the intensity of the electric field. In the absence of an electric field, the force is perpendicular to the velocity of the particle and the direction of the magnetic field. If both electric and magnetic fields are present, the Lorentz force is the sum of both of these vectors.
The electric field E.
The electric field E is defined such that, on a stationary charge:
where "q"0 is what is known as a test charge. The size of the charge doesn't really matter, as long as it is small enough not to influence the electric field by its mere presence. What is plain from this definition, though, is that the unit of E is N/C (newtons per coulomb). This unit is equal to V/m (volts per meter); see below.
In electrostatics, where charges are not moving, around a distribution of point charges, the forces determined from Coulomb's law may be summed. The result after dividing by "q"0 is:
where "n" is the number of charges, "qi" is the amount of charge associated with the "i"th charge, r"i" is the position of the "i"th charge, r is the position where the electric field is being determined, and "ε"0 is the electric constant.
If the field is instead produced by a continuous distribution of charge, the summation becomes an integral:
where formula_5 is the charge density and formula_6 is the vector that points from the volume element formula_7 to the point in space where E is being determined.
Both of the above equations are cumbersome, especially if one wants to determine E as a function of position. A scalar function called the electric potential can help. Electric potential, also called voltage (the units for which are the volt), is defined by the line integral
where "φ(r)" is the electric potential, and "C" is the path over which the integral is being taken.
Unfortunately, this definition has a caveat. From Maxwell's equations, it is clear that is not always zero, and hence the scalar potential alone is insufficient to define the electric field exactly. As a result, one must add a correction factor, which is generally done by subtracting the time derivative of the A vector potential described below. Whenever the charges are quasistatic, however, this condition will be essentially met.
From the definition of charge, one can easily show that the electric potential of a point charge as a function of position is:
where "q" is the point charge's charge, r is the position at which the potential is being determined, and r"i" is the position of each point charge. The potential for a continuous distribution of charge is:
where formula_5 is the charge density, and formula_6 is the distance from the volume element formula_7 to point in space where "φ" is being determined.
The scalar "φ" will add to other potentials as a scalar. This makes it relatively easy to break complex problems down in to simple parts and add their potentials. Taking the definition of "φ" backwards, we see that the electric field is just the negative gradient (the del operator) of the potential. Or:
From this formula it is clear that E can be expressed in V/m (volts per meter).
Electromagnetic waves.
A changing electromagnetic field propagates away from its origin in the form of a wave. These waves travel in vacuum at the speed of light and exist in a wide spectrum of wavelengths. Examples of the dynamic fields of electromagnetic radiation (in order of increasing frequency): radio waves, microwaves, light (infrared, visible light and ultraviolet), x-rays and gamma rays. In the field of particle physics this electromagnetic radiation is the manifestation of the electromagnetic interaction. between charged particles.
General field equations.
As simple and satisfying as Coulomb's equation may be, it is not entirely correct in the context of classical electromagnetism. Problems arise because changes in charge distributions require a non-zero amount of time to be "felt" elsewhere (required by special relativity).
For the fields of general charge distributions, the retarded potentials can be computed and differentiated accordingly to yield Jefimenko's Equations.
Retarded potentials can also be derived for point charges, and the equations are known as the Liénard–Wiechert potentials. The scalar potential is:
where "q" is the point charge's charge and r is the position. r"q" and v"q" are the position and velocity of the charge, respectively, as a function of retarded time. The vector potential is similar:
These can then be differentiated accordingly to obtain the complete field equations for a moving point particle.
Models.
Branches of classical electromagnetism such as optics, electrical and electronic engineering consist of a collection of relevant mathematical models of different degrees of simplification and idealization to enhance the understanding of specific electrodynamics phenomena, cf. An electrodynamics phenomenon is determined by the particular fields, specific densities of electric charges and currents, and the particular transmission medium. Since there are infinitely many of them, in modeling there is a need for some typical, representative 
all of which have only few variable characteristics.

</doc>
<doc id="16855603" url="https://en.wikipedia.org/wiki?curid=16855603" title="Classical electromagnetism and special relativity">
Classical electromagnetism and special relativity

The theory of special relativity plays an important role in the modern theory of classical electromagnetism. First of all, it gives formulas for how electromagnetic objects, in particular the electric and magnetic fields, are altered under a Lorentz transformation from one inertial frame of reference to another. Secondly, it sheds light on the relationship between electricity and magnetism, showing that frame of reference determines if an observation follows electrostatic or magnetic laws. Third, it motivates a compact and convenient notation for the laws of electromagnetism, namely the "manifestly covariant" tensor form.
Maxwell's equations, when they were first stated in their complete form in 1865, would turn out to be compatible with special relativity. Moreover, the apparent coincidences in which the same effect was observed due to different physical phenomena by two different observers would be shown to be not coincidental in the least by special relativity. In fact, half of Einstein's 1905 first paper on special relativity, "On the Electrodynamics of Moving Bodies," explains how to transform Maxwell's equations.
Transformation of the fields between inertial frames.
The E and B fields.
This equation, also called the Joules-Bernoulli equation, considers two inertial frames. As notation, the field variables in one frame are "unprimed", and in a frame moving relative to the unprimed frame at velocity v, the fields are denoted with "primes". In addition, the fields "parallel" to the velocity v are denoted by formula_1 while the fields perpendicular to v are denoted as formula_2. In these two frames moving at relative velocity v, the E-fields and B-fields are related by:
where
is called the Lorentz factor and "c" is the speed of light in free space. The inverse transformations are the same except v → −v.
An equivalent, alternative expression is:
where v̂ is the velocity unit vector.
If one of the fields is zero in one frame of reference, that doesn't necessarily mean it is zero in all other frames of reference. This can be seen by, for instance, making the unprimed electric field zero in the transformation to the primed electric field. In this case, depending on the orientation of the magnetic field, the primed system could see an electric field, even though there is none in the unprimed system.
This does not mean two completely different sets of events are seen in the two frames, but that the same sequence of events is described in two different ways (see Moving magnet and conductor problem below).
If a particle of charge "q" moves with velocity u with respect to frame S, then the Lorentz force in frame S is:
In frame S', the Lorentz force is:
If S and S' have aligned axes then:
A derivation for the transformation of the Lorentz force for the particular case u = 0 is given here. A more general one can be seen here.
Component by component, for relative motion along the x-axis, this works out to be the following:
The transformations in this form can be made more compact by introducing the electromagnetic tensor (defined below), which is a covariant tensor.
The D and H fields.
For the electric displacement D and magnetic intensity H, using the constitutive relations and the result for "c"2:
gives
Analogously for E and B, the D and H form the electromagnetic displacement tensor.
The φ and A fields.
An alternative simpler transformation of the EM field uses the electromagnetic potentials - the electric potential φ and magnetic potential A:
where formula_13 is the parallel component of A to the direction of relative velocity between frames "v", and formula_14 is the perpendicular component. These transparently resemble the characteristic form of other Lorentz transformations (like time-position and energy-momentum), while the transformations of E and B above are slightly more complicated. The components can be collected together as:
The ρ and J fields.
Analogously for the charge density ρ and current density J,
Collecting components together:
Non-relativistic approximations.
For speeds "v" ≪ "c", the relativistic factor γ ≈ 1, which yields:
so that there is no need to distinguish between the spatial and temporal coordinates in Maxwell's equations.
Relationship between electricity and magnetism.
Deriving magnetism from electrostatics.
The chosen reference frame determines if an electromagnetic phenomenon is viewed as an effect of electrostatics or magnetism. Authors usually derive magnetism from electrostatics when special relativity and charge invariance are taken into account. The Feynman Lectures on Physics (vol. 2, ch. 13-6) uses this method to derive the "magnetic" force on a moving charge next to a current-carrying wire. See also Haskell and Landau.
Fields intermix in different frames.
The above transformation rules show that the electric field in one frame contributes to the magnetic field in another frame, and vice versa. This is often described by saying that the electric field and magnetic field are two interrelated aspects of a single object, called the electromagnetic field. Indeed, the entire electromagnetic field can be encoded in a single rank-2 tensor called the electromagnetic tensor; see below.
Moving magnet and conductor problem.
A famous example of the intermixing of electric and magnetic phenomena in different frames of reference is called the "moving magnet and conductor problem", cited by Einstein in his 1905 paper on Special Relativity.
If a conductor moves with a constant velocity through the field of a stationary magnet, eddy currents will be produced due to a "magnetic" force on the electrons in the conductor. In the rest frame of the conductor, on the other hand, the magnet will be moving and the conductor stationary. Classical electromagnetic theory predicts that precisely the same microscopic eddy currents will be produced, but they will be due to an "electric" force.
Covariant formulation in vacuum.
The laws and mathematical objects in classical electromagnetism can be written in a form which is manifestly covariant. Here, this is only done so for vacuum (or for the microscopic Maxwell equations, not using macroscopic descriptions of materials such as electric permittivity), and uses SI units.
This section uses Einstein notation, including Einstein summation convention. See also Ricci calculus for a summary of tensor index notations, and raising and lowering indices for definition of superscript and subscript indices, and how to switch between them. The Minkowski metric tensor η here has metric signature (+ − − −).
Field tensor and 4-current.
The above relativistic transformations suggest the electric and magnetic fields are coupled together, in a mathematical object with 6 components: an antisymmetric second-rank tensor, or a bivector. This is called the electromagnetic field tensor, usually written as "F"μν. In matrix form:
where "c" the speed of light - in natural units "c" = 1.
There is another way of merging the electric and magnetic fields into an antisymmetric tensor, by replacing E/"c" → B and B → − E/"c", to get the dual tensor "G"μν.
In the context of special relativity, both of these transform according to the Lorentz transformation according to
where Λαν is the Lorentz transformation tensor for a change from one reference frame to another. The same tensor is used twice in the summation.
The charge and current density, the sources of the fields, also combine into the four-vector
called the four-current.
Maxwell's equations in tensor form.
Using these tensors, Maxwell's equations reduce to:
& \frac{\partial G^{\alpha \beta}}{\partial x^\alpha} = 0 
\end{align}</math> 
where the partial derivatives may be written in various ways, see 4-gradient. The first equation listed above corresponds to both Gauss's Law (for β = 0) and the Ampère-Maxwell Law (for β = 1, 2, 3). The second equation corresponds to the two remaining equations, Gauss's law for magnetism (for β = 0) and Faraday's Law ( for β = 1, 2, 3).
These tensor equations are manifestly-covariant, meaning the equations can be seen to be covariant by the index positions. This short form of writing Maxwell's equations illustrates an idea shared amongst some physicists, namely that the laws of physics take on a simpler form when written using tensors.
By lowering the indices on "F"αβ to obtain "F"αβ (see raising and lowering indices):
the second equation can be written in terms of "F"αβ as:
where formula_25 is the contravariant Levi-Civita symbol. Notice the cyclic permutation of indices in this equation: formula_26.
Another covariant electromagnetic object is the electromagnetic stress-energy tensor, a covariant rank-2 tensor which includes the Poynting vector, Maxwell stress tensor, and electromagnetic energy density.
4-potential.
The EM field tensor can also be written
where
is the four-potential and
is the four-position.
Using the 4-potential in the Lorenz gauge, an alternative manifestly-covariant formulation can be found in a single equation (a generalization of an equation due to Bernhard Riemann by Arnold Sommerfeld, known as the Riemann–Sommerfeld equation, or the covariant form of the Maxwell equations):
where formula_30 is the d'Alembertian operator, or four-Laplacian. For a more comprehensive presentation of these topics, see Covariant formulation of classical electromagnetism.

</doc>
<doc id="1362465" url="https://en.wikipedia.org/wiki?curid=1362465" title="Classical electron radius">
Classical electron radius

The classical electron radius, also known as the Lorentz radius or the Thomson scattering length, is based on a classical (i.e. non-quantum) relativistic model of the electron. According to modern research, the electron is a point particle with a point charge and no spatial extent. However, the classical electron radius is calculated as
where formula_2 and formula_3 are the electric charge and the mass of the electron, formula_4 is the speed of light, and formula_5 is the permittivity of free space.
In cgs units, this becomes more simply
with (to three significant digits)
Using classical electrostatics, the energy required to assemble a sphere of constant charge density, of radius formula_8 and charge formula_2 is
If the charge is on the surface the energy is
If the charge has a distribution formula_12 for "r"≤"R"
The energy is: formula_13
Ignoring the factors 3/5 or 1/2, if this is equated to the relativistic energy of the electron (formula_14) and solved for formula_8, the above result is obtained.
In simple terms, the classical electron radius is roughly the size the electron would need to have for its mass to be completely due to its electrostatic potential energy – not taking quantum mechanics into account. We now know that quantum mechanics, indeed quantum field theory, is needed to understand the behavior of electrons at such short distance scales, thus the classical electron radius is no longer regarded as the actual size of an electron. Still, the classical electron radius is used in modern classical-limit theories involving the electron, such as non-relativistic Thomson scattering and the relativistic Klein–Nishina formula. Also, the classical electron radius is roughly the length scale at which renormalization becomes important in quantum electrodynamics.
The classical electron radius is one of a trio of related units of length, the other two being the Bohr radius formula_16 and the Compton wavelength of the electron formula_17. The classical electron radius is built from the electron mass formula_18, the speed of light formula_4 and the electron charge formula_2. The Bohr radius is built from formula_18,
formula_2 and Planck's constant formula_23. The Compton wavelength is built from formula_18, formula_23 and formula_4. Any one of these three lengths can be written in terms of any other using the fine structure constant formula_27:
Extrapolating from the initial equation, any charged mass can be imagined to have an 'electromagnetic radius' similar to the electron's classical radius.
where formula_30 is Coulomb's constant, formula_31 is the charge of the object, formula_32 is its mass, formula_27 is the fine structure constant and formula_34 is the reduced Planck's constant.
See also.
Electromagnetic mass

</doc>
<doc id="1293340" url="https://en.wikipedia.org/wiki?curid=1293340" title="Classical field theory">
Classical field theory

A classical field theory is a physical theory that predicts how one or more physical fields interact with matter through field equations. The term 'classical field theory' is commonly reserved for describing those physical theories that describe electromagnetism and gravitation, two of the fundamental forces of nature. Theories that incorporate quantum mechanics are called quantum field theories.
A physical field can be thought of as the assignment of a physical quantity at each point of space and time. For example, in a weather forecast, the wind velocity during a day over a country is described by assigning a vector to each point in space. Each vector represents the direction of the movement of air at that point. As the day progresses, the directions in which the vectors point change as the directions of the wind change. From the mathematical viewpoint, classical fields are described by sections of fiber bundles (covariant classical field theory).
Descriptions of physical fields were given before the advent of relativity theory and then revised in light of this theory. Consequently, classical field theories are usually categorised as "non-relativistic" and "relativistic". Modern field equations tend to be tensor equations.
In 1839 James MacCullagh presented field equations to describe reflection and refraction in "An essay toward a dynamical theory of crystalline reflection and refraction".
Non-relativistic field theories.
Some of the simplest physical fields are vector force fields. Historically, the first time fields were taken seriously was with Faraday's lines of force when describing the electric field. The gravitational field was then similarly described.
Newtonian gravitation.
The first field theory of gravity was Newton's theory of gravitation in which the mutual interaction between two masses obeys an inverse square law. This was very useful for predicting the motion of planets around the Sun.
Any massive body "M" has a gravitational field g which describes its influence on other massive bodies. The gravitational field of "M" at a point r in space is found by determining the force F that "M" exerts on a small test mass "m" located at r, and then dividing by "m":
Stipulating that "m" is much smaller than "M" ensures that the presence of "m" has a negligible influence on the behavior of "M".
According to Newton's law of universal gravitation, F(r) is given by
where formula_3 is a unit vector pointing along the line from "M" to "m", and "G" is Newton's gravitational constant. Therefore, the gravitational field of M is
The experimental observation that inertial mass and gravitational mass are equal to unprecedented levels of accuracy leads to the identification of the gravitational field strength as identical to the acceleration experienced by a particle. This is the starting point of the equivalence principle, which leads to general relativity.
For a discrete collection of masses, "Mi", located at points, r"i", the gravitational field at a point r due to the masses is
If we have a continuous mass distribution "ρ" instead, the sum is replaced by an integral,
Note that the direction of the field points from the position r to the position of the masses r"i"; this is ensured by the minus sign. In a nutshell, this means all masses attract.
In the integral form Gauss's law for gravity is
while in differential form it is
Therefore, the gravitational field g can be written in terms of the gradient of a gravitational potential φ(r):
This is a consequence of the gravitational force F being conservative.
Electromagnetism.
Electrostatics.
A charged test particle with charge "q" experiences a force F based solely on its charge. We can similarly describe the electric field E so that . Using this and Coulomb's law the electric field due to a single charged particle is
The electric field is conservative, and hence is given by the gradient of a scalar potential, "V"(r)
Gauss's law for electricity is in integral form
while in differential form
Magnetostatics.
A steady current "I" flowing along a path "ℓ" will exert a force on nearby charged particles that is quantitatively different from the electric field force described above. The force exerted by "I" on a nearby charge "q" with velocity v is
where B(r) is the magnetic field, which is determined from "I" by the Biot–Savart law:
The magnetic field is not conservative in general, and hence cannot usually be written in terms of a scalar potential. However, it can be written in terms of a vector potential, A(r):
Gauss's law for magnetism in integral form is
while in differential form it is
The physical interpretation is that there are no magnetic monopoles.
Electrodynamics.
In general, in the presence of both a charge density ρ(r, "t") and current density J(r, "t"), there will be both an electric and a magnetic field, and both will vary in time. They are determined by Maxwell's equations, a set of differential equations which directly relate E and B to the electric charge density (charge per unit volume) "ρ" and current density (electric current per unit area) J.
Alternatively, one can describe the system in terms of its scalar and vector potentials "V" and A. A set of integral equations known as "retarded potentials" allow one to calculate "φ" and A from ρ and J, and from there the electric and magnetic fields are determined via the relations
Hydrodynamics.
Fluid dynamics has fields of pressure, density, and flow rate that are connected by conservation laws for energy and momentum. The mass continuity equation is a continuity equation, representing the conservation of mass
and the Navier–Stokes equations represent the conservation of momentum in the fluid, found from Newton's laws applied to the fluid,
if the density "ρ", pressure "p", deviatoric stress tensor τ of the fluid, as well as external body forces b, are all given. The velocity field u is the vector field to solve for.
Potential theory.
The term "potential theory" arises from the fact that, in 19th century physics, the fundamental forces of nature were believed to be derived from scalar potentials which satisfied Laplace's equation. Poisson addressed the question of the stability of the planetary orbits, which had already been settled by Lagrange to the first degree of approximation from the perturbation forces, and derived the Poisson's equation, named after him. The general form of this equation is
where "σ" is a source function (as a density, a quantity per unit volume) and φ the scalar potential to solve for.
In Newtonian gravitation; masses are the sources of the field so that field lines terminate at objects that have mass. Similarly, charges are the sources and sinks of electrostatic fields: positive charges emanate electric field lines, and field lines terminate at negative charges. These field concepts are also illustrated in the general divergence theorem, specifically Gauss's law's for gravity and electricity. For the cases of time-independent gravity and electromagnetism, the fields are gradients of corresponding potentials
so substituting these into Gauss' law for each case obtains
where "ρg" is the mass density and "ρe" the charge density.
Incidentally, this similarity arises from the similarity between Newton's law of gravitation and Coulomb's law.
In the case where there is no source term (e.g. vacuum, or paired charges), these potentials obey Laplace's equation:
For a distribution of mass (or charge), the potential can be expanded in a series of spherical harmonics, and the "n"th term in the series can be viewed as a potential arising from the 2"n"-moments (see multipole expansion). For many purposes only the monopole, dipole, and quadrupole terms are needed in calculations.
Relativistic field theory.
Modern formulations of classical field theories generally require Lorentz covariance as this is now recognised as a fundamental aspect of nature. A field theory tends to be expressed mathematically by using Lagrangians. This is a function that, when subjected to an action principle, gives rise to the field equations and a conservation law for the theory. The action is a Lorentz scalar, from which the field equations and symmetries can be readily derived.
Throughout we use units such that the speed of light in vacuum is 1, i.e. "c" = 1.
Lagrangian dynamics.
Given a field tensor "φ", a scalar called the Lagrangian density
can be constructed from "φ" and its derivatives.
From this density, the functional action can be constructed by integrating over spacetime
Therefore the Lagrangian itself is equal to the integral of the Lagrangian Density over all space.
Then by enforcing the action principle, the Euler–Lagrange equations are obtained
Relativistic fields.
Two of the most well-known Lorentz-covariant classical field theories are now described.
Electromagnetism.
Historically, the first (classical) field theories were those describing the electric and magnetic fields (separately). After numerous experiments, it was found that these two fields were related, or, in fact, two aspects of the same field: the electromagnetic field. Maxwell's theory of electromagnetism describes the interaction of charged matter with the electromagnetic field. The first formulation of this field theory used vector fields to describe the electric and magnetic fields. With the advent of special relativity, a more complete formulation using tensor fields was found. Instead of using two vector fields describing the electric and magnetic fields, a tensor field representing these two fields together is used.
The electromagnetic potential is defined to be "Aa" = (-"φ", A), and the electromagnetic four-current "ja" = (-"ρ", j). The electromagnetic field at any point in spacetime is described by the antisymmetric (0,2)-rank electromagnetic field tensor
The Lagrangian.
To obtain the dynamics for this field, we try and construct a scalar from the field. In the vacuum, we have
We can use gauge field theory to get the interaction term, and this gives us
The equations.
To obtain the field equations the electromagnetic tensor in the Lagrangian density needs to be replaced by its definition in terms of the 4-potential "A", and its this potential which enter the Euler-Lagrange equations. The EM field "F" is not varied in the EL equations. Therefore
Evaluating the derivative of the Lagrangian density with respect to the field components
and the derivatives of the field components
obtains Maxwell's equations in vacuum. The source equations (Gauss' law for electricity and the Maxwell-Ampère law) are
while the other two (Gauss' law for magnetism and Faraday's law) are obtained from the fact that "F" is the 4-curl of "A"
where the comma indicates a partial derivative.
Gravitation.
After Newtonian gravitation was found to be inconsistent with special relativity, Albert Einstein formulated a new theory of gravitation called general relativity. This treats gravitation as a geometric phenomenon ('curved spacetime') caused by masses and represents the gravitational field mathematically by a tensor field called the metric tensor. The Einstein field equations describe how this curvature is produced. Newtonian gravitation is now superseded by Einstein's theory of general relativity, in which gravitation is thought of as being due to a curved spacetime, caused by masses. The Einstein field equation describes how this curvature is produced by masses
where "κ" = "8πG/c"4 is a constant which appears in the Einstein field equations (and not the action), and
is the Einstein tensor.
The vacuum solution can be obtained by varying the following Einstein–Hilbert action with respect to the metric
The "vacuum field equations" are the field equations written without matter (including sources). Solutions of the vacuum field equations are called vacuum solutions. The field equations may be derived by using the Einstein–Hilbert action. Varying the Lagrangian
where "R" = "Rabgab" is the Ricci scalar written in terms of the Ricci tensor "Rab" and "g" the determinant of the metric tensor "gab" will yield the vacuum field equations
Unification attempts.
The Kaluza–Klein theory attempts to unify gravitation and electromagnetism, in a five-dimensional spacetime.

</doc>
<doc id="10172878" url="https://en.wikipedia.org/wiki?curid=10172878" title="Classical group">
Classical group

In mathematics, the classical groups are defined as the special linear groups over the reals , the complex numbers and the quaternions together with special automorphism groups of symmetric or skew-symmetric bilinear forms and Hermitian or skew-Hermitian sesquilinear forms defined on real, complex and quaternionic finite-dimensional vector spaces. Of these, the complex classical Lie groups are four infinite families of Lie groups that together with the exceptional groups exhaust the classification of simple Lie groups. The compact classical groups are compact real forms of the complex classical groups. The finite analogues of the classical groups are the classical groups of Lie type. The term "classical group" was coined by Hermann Weyl, it being the title of his 1939 monograph "The Classical Groups".
The classical groups form the deepest and most useful part of the subject of linear Lie groups. Most types of classical groups find application in classical and modern physics. A few examples are the following. The rotation group is a symmetry of Euclidean space and all fundamental laws of physics, the Lorentz group is a symmetry group of spacetime of special relativity. The special unitary group is the symmetry group of quantum chromodynamics and the symplectic group finds application in hamiltonian mechanics and quantum mechanical versions of it.
The classical groups.
The classical groups are exactly the general linear groups over and together with the automorphism groups of non-degenerate forms discussed below. These groups are usually additionally restricted to the subgroups whose elements have determinant 1. The classical groups, with the determinant 1 condition, are listed in the table below. In the sequel, the determinant 1 condition is "not" used consistently in the interest of greater generality.
The complex classical groups are , and . A group is complex according to whether its Lie algebra is complex. The real classical groups refers to all of the classical groups since any Lie algebra is a real algebra. The compact classical groups are the compact real forms of the complex classical groups. These are, in turn, , and . One characterization of the compact real form is in terms of the Lie algebra . If , the complexification of , then if the connected group generated by is a compact, is a compact real form.
The classical groups can uniformly be characterized in a different way using real forms. The classical groups (here with the determinant 1 condition, but this is not necessary) are the following:
For instance, is a real form of , is a real form of , and is a real form of . Without the determinant 1 condition, replace the special linear groups with the corresponding general linear groups in the characterization. The algebraic groups in question are Lie groups, but the "algebraic" qualifier is needed to get the right notion of "real form".
Bilinear and sesquilinear forms.
The classical groups are defined in terms of forms defined on , , and , where and are the fields of the real and complex numbers. The quaternions, , do not constitute a field because multiplication does not commute; they form a division ring or a skew field or non-commutative field. However, it is still possible to define matrix quaternionic groups. For this reason, a vector space is allowed to be defined over , , as well as below. In the case of , is a "right" vector space to make possible the representation of the group action as matrix multiplication from the "left", just as for and .
A form on some finite-dimensional right vector space over , or is bilinear if 
It is called sesquilinear if 
These conventions are chosen because they work in all cases considered. An automorphism of is a map in the set of linear operators on such that
The set of all automorphisms of form a group, it is called the automorphism group of , denoted . This leads to a preliminary definition of a classical group:
This definition has shortcomings because there is some unnecessary redundancy. In the case of , bilinear is equivalent to sesquilinear. In the case of , there are no non-zero bilinear forms.
Symmetric, skew-symmetric, Hermitian, and skew-Hermitian forms.
A form is symmetric if
It is skew-symmetric if
It is Hermitian if
Finally, it is skew-Hermitian if
A bilinear form is uniquely a sum of a symmetric form and a skew-symmetric form. A transformation preserving preserves both parts separately. The groups preserving symmetric and skew-symmetric forms can thus be studied separately. The same applies, mutatis mutandis, to Hermitian and skew-Hermitian forms. For this reason, for the purposes of classification, only purely symmetric, skew-symmetric, Hermitian, or skew-Hermitian forms are considered. The normal forms of the forms correspond to specific suitable choices of bases. These are bases giving the following normal forms in coordinates:
The in the skew-Hermitian form is the third basis element in the basis for . Proof of existence of these bases and Sylvester's law of inertia, the independence of the number of plus- and minus-signs, and , in the symmetric and Hermitian forms, as well as the presence or absence of the fields in each expression, can be found in or . The pair , and sometimes , is called the signature of the form. 
Explanation of occurrence of the fields : There are no nontrivial bilinear forms over . In the symmetric bilinear case, only forms over have a signature. In other words, a complex bilinear form with "signature" can, by a change of basis, be reduced to a form where all signs are "" in the above expression, whereas this is impossible in the real case, in which is independent of the basis when put into this form. However, Hermitian forms have basis-independent signature in both the complex and the quaternionic case. (The real case reduces to the symmetric case.) A skew-Hermitian form on a complex vector space is rendered Hermitian by multiplication by , so in this case, only is interesting.
Automorphism groups.
The first section presents the general framework. The other sections exhaust the qualitatively different cases that arise as automorphism groups of bilinear and sesquilinear forms on finite-dimensional vector spaces over , and .
Aut("φ") – the automorphism group.
Assume that is a non-degenerate form on a finite-dimensional vector space over or . The automorphism group is defined, based on condition (), as
Every has an adjoint with respect to defined by
Using this definition in condition (), the automorphism group is seen to be given by
Fix a basis for . In terms of this basis, put
where are the components of . This is appropriate for the bilinear forms. Sesquilinear forms have similar expressions and are treated separately later. In matrix notation one finds
and
from () where is the matrix . The non-degeneracy condition means precisely that is invertible, so the adjoint always exists. expressed with this becomes
The Lie algebra of the automorphism groups can be written down immediately. Abstractly, if and only if
for all , corresponding to the condition in () under the exponential mapping of Lie algebras, so that
or in a basis
as is seen using the power series expansion of the exponential mapping and the linearity of the involved operations. Conversely, suppose that . Then, using the above result, . Thus the Lie algebra can be characterized without reference to a basis, or the adjoint, as
The normal form for will be given for each classical group below. From that normal form, the matrix can be read off directly. Consequently, expressions for the adjoint and the Lie algebras can be obtained using formulas () and (). This is demonstrated below in most of the non-trivial cases.
Bilinear case.
When the form is symmetric, is called . When it is skew-symmetric then is called . This applies to the real and the complex cases. The quaternionic case is empty since no nonzero bilinear forms exists on quaternionic vector spaces.
Real case.
The real case breaks up into two cases, the symmetric and the antisymmetric forms that should be treated separately.
O("p", "q") and O("n") – the orthogonal groups.
If is symmetric and the vector space is real, a basis may be chosen so that
The number of plus and minus-signs are independent of the particular basis. In the case one writes where is the number of plus signs and is the number of minus-signs, . If the notation is . The matrix is in this case
after reordering the basis if necessary. The adjoint operation () then becomes
which reduces to the usual transpose when or is 0. The Lie algebra is found using equation () and a suitable ansatz (this is detailed for the case of below),
and the group according to () is given by
The groups and are isomorphic through the map
For example, the Lie algebra of the Lorentz group could be written as
Naturally, it is possible to rearrange so that the -block is the upper left (or any other block). Here the "time component" end up as the fourth coordinate in a physical interpretation, and not the first as may be more common.
Sp("m", R) – the real symplectic group.
If is skew-symmetric and the vector space is real, there is a basis giving
where . For one writes In case one writes or . From the normal form one reads off
By making the ansatz
where are -dimensional matrices and considering (),
one finds the Lie algebra of ,
and the group is given by
Complex case.
Like in the real case, there are two cases, the symmetric and the antisymmetric case that each yield a family of classical groups.
O("n", C) – the complex orthogonal group.
If case is symmetric and the vector space is complex, a basis
with only plus-signs can be used. The automorphism group is in the case of called . The lie algebra is simply a special case of that for ,
and the group is given by
In terms of classification of simple Lie algebras, the are split into two classes, those with odd with root system and even with root system .
Sp("m", C) – the complex symplectic group.
For skew-symmetric and the vector space complex, the same formula,
applies as in the real case. For one writes In case one writes or . The Lie algebra parallels that of ,
and the group is given by
Sesquilinear case.
In the sequilinear case, one makes a slightly different ansatz for the form in terms of a basis,
The other expressions that get modified are
The real case, of course, provides nothing new. The complex and the quaternionic case will be considered below.
Complex case.
From a qualitative point of view, consideration of skew-Hermitean forms (up to isomorphism) provide no new groups; multiplication by renders a skew-Hermitean form Hermitean, and vice versa. Thus only the Hermitian case needs to be considered.
U("p", "q") and U("n") – the unitary groups.
A non-degenerate hermitian form has the normal form
As in the bilinear case, the signature ("p", "q") is independent of the basis. The automorphism group is denoted , or, in the case of , . If the notation is . In this case, takes the form
and the Lie algebra is given by
The group is given by
Quaternionic case.
The space is considered as a "right" vector space over . This way, for a quaternion , a quaternion column vector and quaternion matrix . If was a "left" vector space over , then matrix multiplication from the "right" on row vectors would be required to maintain linearity. This does not correspond to the usual linear operation of a group on a vector space when a basis is given, which is matrix multiplication from the "left" on column vectors. Thus is henceforth a right vector space over . Even so, care must be taken due to the non-commutative nature of . The (mostly obvious) details are skipped because complex representations will be used.
When dealing with quaternionic groups it is convenient to represent quaternions using complex ,
With this representation, quaternionic multiplication becomes matrix multiplication and quaternionic conjugation becomes taking the Hermitian adjoint. Moreover, if a quaternion according to the complex encoding is given as a column vector , then multiplication from the left by a matrix representation of a quaternion produces a new column vector representing the correct quaternion. This representation differs slightly from a more common representation found in the quaternion article. The more common convention would force multiplication from the right on a row matrix to achieve the same thing.
Incidentally, the representation above makes it clear that the group of unit quaternions () is isomorphic to .
Quaternionic -matrices matrices can, by obvious extension, be represented by block-matrices of complex numbers. If one agrees to represent a quaternionic column vector by a column vector with complex numbers according to the encoding of above, with the upper numbers being the and the lower the , then a quaternionic -matrix becomes a complex -matrix exactly of the form given above, but now with α and β -matrices. More formally
A matrix has the form displayed in () if and only if . With these identifications,
The space is a real algebra, but it is not a complex subspace of . Multiplication (from the left) by in using entry-wise quaternionic multiplication and then mapping to the image in yields a different result than multiplying entry-wise by directly in . The quaternionic multiplication rules give where the new and are inside the parentheses.
The action of the quaternionic matrices on quaternionic vectors is now represented by complex quantities, but otherwise it is the same as for "ordinary" matrices and vectors. The quaternionic groups are thus embedded in where is the dimension of the quaternionic matrices.
The determinant of a quaternionic matrix is defined in this representation as being the ordinary complex determinant of its representative matrix. The non-commutative nature of quaternionic multiplication would, in the quaternionic representation of matrices, be ambiguous. The way is embedded in is not unique, but all such embeddings are related through for , leaving the determinant unaffected. The name of in this complex guise is .
As opposed to in the case of , both the Hermitian and the skew-Hermitean case bring in something new when is considered, so these cases are considered separately.
GL("n", H) and SL("n", H).
Under the identification above,
Its Lie algebra is the set of all matrices in the image of the mapping of above,
The quaternionic special linear group is given by
where the determinant is taken on the matrices in . The Lie algebra is
Sp("p", "q") – the quaternionic unitary group.
As above in the complex case, the normal form is
and the number of plus-signs is independent of basis. When with this form, . The reason for the notation is that the group can be represented, using the above prescription, as a subgroup of preserving a complex-hermitian form of signature If or the group is denoted . It is sometimes called the hyperunitary group.
In quaternionic notation,
meaning that "quaternionic" matrices of the form
will satisfy
see the section about . Caution needs to be exercised when dealing with quaternionic matrix multiplication, but here only and are involved and these commute with every quaternion matrix. Now apply prescription () to each block,
and the relations in () will be satisfied if
The Lie algebra becomes
The group is given by
Returning to the normal form of for , make the substitutions and with . Then
viewed as a -valued form on . Thus the elements of , viewed as linear transformations of , preserve both a Hermitian form of signature and a non-degenerate skew-symmetric form. Both forms take purely complex values and due to the prefactor of of the second form, they are separately conserved. This means that
and this explains both the name of the group and the notation.
O∗(2"n")= O("n", H)- quaternionic orthogonal group.
The normal form for a skew-hermitian form is given by
where is the third basis quaternion in the ordered listing . In this case, may be realized, using the complex matrix encoding of above, as a subgroup of which preserves a non-degenerate complex skew-hermitian form of signature . From the normal form one sees that in quaternionic notation
and from () follows that
for . Now put 
according to prescription (). The same prescription yields for ,
Now the last condition in () in complex notation reads
The Lie algebra becomes
and the group is given by
The group can be characterized as
where the map is defined by .
Also, the form determining the group can be viewed as a -valued form on . Make the substitutions and in the expression for the form. Then
The form is Hermitian (while the first form on the left hand side is skew-Hermitian) of signature . The signature is made evident by a change of basis from to where are the first and last basis vectors respectively. The second form, is symmetric positive definite. Thus, due to the factor , preserves both separately and it may be concluded that
and the notation "O" is explained.
Classical groups over general fields or algebras.
Classical groups, more broadly considered in algebra, provide particularly interesting matrix groups. When the field "F" of coefficients of the matrix group is either real number or complex numbers, these groups are just the classical Lie groups. When the ground field is a finite field, then the classical groups are groups of Lie type. These groups play an important role in the classification of finite simple groups. Also, one may consider classical groups over a unital associative algebra "R" over "F"; where "R" = H (an algebra over reals) represents an important case. For the sake of generality the article will refer to groups over "R", where "R" may be the ground field "F" itself.
Considering their abstract group theory, many linear groups have a "special" subgroup, usually consisting of the elements of determinant 1 over the ground field, and most of them have associated "projective" quotients, which are the quotients by the center of the group. For orthogonal groups in characteristic 2 "S" has a different meaning.
The word "general" in front of a group name usually means that the group is allowed to multiply some sort of form by a constant, rather than leaving it fixed. The subscript "n" usually indicates the dimension of the module on which the group is acting; it is a vector space if "R" = "F". Caveat: this notation clashes somewhat with the "n" of Dynkin diagrams, which is the rank.
General and special linear groups.
The general linear group GL"n"("R") is the group of all "R"-linear automorphisms of "R""n". There is a subgroup: the special linear group SL"n"("R"), and their quotients: the projective general linear group PGL"n"("R") = GL"n"("R")/Z(GL"n"("R")) and the projective special linear group PSL"n"("R") = SL"n"("R")/Z(SL"n"("R")). The projective special linear group PSL"n"("F") over a field "F" is simple for "n" ≥ 2, except for the two cases when "n" = 2 and the field has order 2 or 3.
Unitary groups.
The unitary group U"n"("R") is a group preserving a sesquilinear form on a module. There is a subgroup, the special unitary group SU"n"("R") and their quotients the projective unitary group PU"n"("R") = U"n"("R")/Z(U"n"("R")) and the projective special unitary group PSU"n"("R") = SU"n"("R")/Z(SU"n"("R"))
Symplectic groups.
The symplectic group Sp2"n"("R") preserves a skew symmetric form on a module. It has a quotient, the projective symplectic group PSp2"n"("R"). The general symplectic group GSp2"n"("R") consists of the automorphisms of a module multiplying a skew symmetric form by some invertible scalar. The projective symplectic group PSp2"n"(F"q") over a finite field is simple for "n" ≥ 1, except for the two cases when "n" = 1 and the field has order 2 or 3.
Orthogonal groups.
The orthogonal group O"n"("R") preserves a non-degenerate quadratic form on a module. There is a subgroup, the special orthogonal group SO"n"("R") and quotients, the projective orthogonal group PO"n"("R"), and the projective special orthogonal group PSO"n"("R"). In characteristic 2 the determinant is always 1, so the special orthogonal group is often defined as the subgroup of elements of Dickson invariant 1.
There is a nameless group often denoted by Ω"n"("R") consisting of the elements of the orthogonal group of elements of spinor norm 1, with corresponding subgroup and quotient groups SΩ"n"("R"), PΩ"n"("R"), PSΩ"n"("R"). (For positive definite quadratic forms over the reals, the group Ω happens to be the same as the orthogonal group, but in general it is smaller.) There is also a double cover of Ω"n"("R"), called the pin group Pin"n"("R"), and it has a subgroup called the spin group Spin"n"("R"). The general orthogonal group GO"n"("R") consists of the automorphisms of a module multiplying a quadratic form by some invertible scalar.
Contrast with exceptional Lie groups.
Contrasting with the classical Lie groups are the exceptional Lie groups, G2, F4, E6, E7, E8, which share their abstract properties, but not their familiarity. These were only discovered around 1890 in the classification of the simple Lie algebras over the complex numbers by Wilhelm Killing and Élie Cartan.

</doc>
<doc id="2661487" url="https://en.wikipedia.org/wiki?curid=2661487" title="Classification of discontinuities">
Classification of discontinuities

Continuous functions are of utmost importance in mathematics, functions and applications. However, not all functions are continuous. If a function is not continuous at a point in its domain, one says that it has a discontinuity there. The set of all points of discontinuity of a function may be a discrete set, a dense set, or even the entire domain of the function. This article describes the classification of discontinuities in the simplest case of functions of a single real variable taking real values.
The oscillation of a function at a point quantifies these discontinuities as follows:
Classification.
For each of the following, consider a real valued function of a real variable , defined in a neighborhood of the point "x"0 at which is discontinuous.
Removable discontinuity.
1. Consider the function 
The point = 1 is a "removable discontinuity". For this kind of discontinuity:
The one-sided limit from the negative direction
and the one-sided limit from the positive direction
at exist, are finite, and are equal to = = . In other words, since the two one-sided limits exist and are equal, the limit of as approaches exists and is equal to this same value. If the actual value of is "not" equal to , then is called a removable discontinuity. This discontinuity can be 'removed to make continuous at ', or more precisely, the function
is continuous at = .
It is important to realize that the term "removable discontinuity" is sometimes used by abuse of terminology for cases in which the limits in both directions exist and are equal, while the function is undefined at the point . This use is abusive because continuity and discontinuity of a function are concepts defined only for points in the function's domain. Such a point not in the domain is properly named a removable singularity.
Jump discontinuity.
2. Consider the function 
Then, the point = 1 is a "jump discontinuity".
In this case, the limit does not exist because the one-sided limits, and , exist and are finite, but are not equal: since, ≠ , the limit does not exist. Then, is called a "jump discontinuity" or "step discontinuity". For this type of discontinuity, the function may have any value at .
Essential discontinuity.
3. Consider the function 
Then, the point formula_7 is an "essential discontinuity (sometimes called infinite discontinuity)". For it to be an essential discontinuity, it would have sufficed that only one of the two one-sided limits did not exist or were infinite.
In this case, one or both of the limits formula_8 and formula_9 does not exist or is infinite. Then, "x"0 is called an "essential discontinuity", or "infinite discontinuity". (This is distinct from the term "essential singularity" which is often used when studying functions of complex variables.)
The set of discontinuities of a function.
The set of points at which a function is continuous is always a Gδ set. The set of discontinuities is an Fσ set.
The set of discontinuities of a monotonic function is at most countable. This is Froda's theorem. 
"Thomae's function" is discontinuous at every rational point, but continuous at every irrational point.
The indicator function of the rationals, also known as the "Dirichlet function", is discontinuous everywhere.

</doc>
<doc id="669475" url="https://en.wikipedia.org/wiki?curid=669475" title="Closed manifold">
Closed manifold

In mathematics, a closed manifold is a type of topological space, namely a compact manifold without boundary. In contexts where no boundary is possible, any compact manifold is a closed manifold.
Compact manifolds are, in an intuitive sense, "finite". By the basic properties of compactness, a closed manifold is the disjoint union of a finite number of connected closed manifolds. One of the most basic objectives of geometric topology is to understand what the supply of possible closed manifolds is. 
Examples.
The simplest example is a circle, which is a compact one-dimensional manifold. 
Other examples of closed manifolds are the torus and the Klein bottle. 
As a counterexample, the real line is not a closed manifold because it is not compact. A disk is a compact two-dimensional manifold, but is not a closed manifold because it has a boundary. 
Properties.
All compact topological manifolds can be embedded into formula_1 for some "n", by the Whitney embedding theorem.
Contrasting terms.
A compact manifold means a "manifold" that is compact as a topological space, but possibly has boundary. More precisely, it is a compact manifold with boundary (the boundary may be empty).
By contrast, a closed manifold is compact "without" boundary.
An open manifold is a manifold without boundary with no compact component.
For a connected manifold, "open" is equivalent to "without boundary and non-compact", but for a disconnected manifold, open is stronger.
For instance, the disjoint union of a circle and the line is non-compact, but is not an open manifold, since one component (the circle) is compact.
The notion of closed manifold is unrelated with that of a closed set. A disk with its boundary is a closed subset of the plane, but not a closed manifold.
Use in physics.
The notion of a "closed universe" can refer to the universe being a closed manifold but more likely refers to the universe being a manifold of constant positive Ricci curvature.

</doc>
<doc id="51203" url="https://en.wikipedia.org/wiki?curid=51203" title="Coefficient">
Coefficient

In mathematics, a coefficient is a multiplicative factor in some term of a polynomial, a series or any expression; it is usually a number, but in any case does not involve any variables of the expression. For instance in
the first two terms respectively have the coefficients 7 and −3. The third term 1.5 is a constant. The final term does not have any explicitly written coefficient, but is considered to have coefficient 1, since multiplying by that factor would not change the term. Often coefficients are numbers as in this example, although they could be parameters of the problem, as "a", "b", and "c", where "c" is a constant, in
when it is understood that these are not considered variables.
Thus a polynomial in one variable "x" can be written as
for some integer formula_4, where formula_5 are coefficients; to allow this kind of expression in all cases one must allow introducing terms with 0 as coefficient.
For the largest formula_6 with formula_7 (if any), formula_8 is called the leading coefficient of the polynomial. So for example the leading coefficient of the polynomial
is 4.
Specific coefficients arise in mathematical identities, such as the binomial theorem which involves binomial coefficients; these particular coefficients are tabulated in Pascal's triangle.
Linear algebra.
In linear algebra, the leading coefficient of a row in a matrix is the first nonzero entry in that row. So, for example, given
The leading coefficient of the first row is 1; 2 is the leading coefficient of the second row; 4 is the leading coefficient of the third row, and the last row does not have a leading coefficient.
Though coefficients are frequently viewed as constants in elementary algebra, they can be variables more generally. For example, the coordinates formula_11 of a vector formula_12 in a vector space with basis formula_13, are the coefficients of the basis vectors in the expression 
Examples of physical coefficients.
A coefficient is a number placed in front of a term in a chemical equation to indicate how many molecules (or atoms) take part in the reaction. For example, in the formula 
the number 2's in front of formula_16 and formula_17 are stoichiometric coefficients.

</doc>
<doc id="180870" url="https://en.wikipedia.org/wiki?curid=180870" title="Color charge">
Color charge

Color charge is a property of quarks and gluons that is related to the particles' strong interactions in the theory of quantum chromodynamics (QCD).
Note that the "color charge" of quarks and gluons is completely unrelated to visual perception of color. The term "color" became popular simply because the charge responsible for the strong force between particles, can be analogized to the three primary colors of human vision: red, green, and blue. Another color scheme is "red, yellow, and blue", using paint, rather than light as the perceptible analogy.
Particles have corresponding antiparticles. A particle with red, green, or blue charge has a corresponding antiparticle in which the color charge must be the anticolor of red, green, and blue, respectively, for the color charge to be conserved in particle-antiparticle creation and annihilation. Particle physicists call these antired, antigreen, and antiblue. All three colors mixed together, or any one of these colors and its complement (or negative), is "colorless" or "white" and has a net color charge of zero. Free particles have a color charge of zero: baryons are composed of three quarks, but the individual quarks can have red, green, or blue charges, or negatives; mesons are made from a quark and antiquark, the quark can be any color, and the antiquark will have the negative of that color. This color charge differs from electromagnetic charges since electromagnetic charges have only one kind of value. Positive and negative electrical charges are the same kind of charge as they only differ by the sign.
Shortly after the existence of quarks was first proposed in 1964, Oscar W. Greenberg introduced the notion of color charge to explain how quarks could coexist inside some hadrons in otherwise identical quantum states without violating the Pauli exclusion principle. The theory of quantum chromodynamics has been under development since the 1970s and constitutes an important component of the Standard Model of particle physics.
Red, green, and blue.
In QCD, a quark's color can take one of three values or charges, red, green, and blue. An antiquark can take one of three anticolors, called antired, antigreen, and antiblue (represented as cyan, magenta and yellow, respectively). Gluons are mixtures of two colors, such as red and antigreen, which constitutes their color charge. QCD considers eight gluons of the possible nine color–anticolor combinations to be unique; see "eight gluon colors" for an explanation.
The following illustrates the coupling constants for color-charged particles:
Field lines from color charges.
Analogous to an electric field and electric charges, the strong force acting between color charges can be depicted using field lines. However, the color field lines do not arc outwards from one charge to another as much, because they are pulled together tightly by gluons (within 1 fm). This effect confines quarks within hadrons.
Coupling constant and charge.
In a quantum field theory, a coupling constant and a charge are different but related notions. The coupling constant sets the magnitude of the force of interaction; for example, in quantum electrodynamics, the fine-structure constant is a coupling constant. The charge in a gauge theory has to do with the way a particle transforms under the gauge symmetry; i.e., its representation under the gauge group. For example, the electron has charge −1 and the positron has charge +1, implying that the gauge transformation has opposite effects on them in some sense. Specifically, if a local gauge transformation is applied in electrodynamics, then one finds (using tensor index notation):
where formula_4 is the photon field, and is the electron field with (a bar over denotes its antiparticle — the positron). Since QCD is a non-abelian theory, the representations, and hence the color charges, are more complicated. They are dealt with in the next section.
Quark and gluon fields and color charges.
In QCD the gauge group is the non-abelian group SU(3). The "running coupling" is usually denoted by αs. Each flavor of quark belongs to the fundamental representation (3) and contains a triplet of fields together denoted by . The antiquark field belongs to the complex conjugate representation (3*) and also contains a triplet of fields. We can write
The gluon contains an octet of fields (see gluon field), and belongs to the adjoint representation (8), and can be written using the Gell-Mann matrices as
(there is an implied summation over "a" = 1, 2, ... 8). All other particles belong to the trivial representation (1) of color SU(3). The color charge of each of these fields is fully specified by the representations. Quarks have a color charge of red, green or blue and antiquarks have a color charge of antired, antigreen or antiblue. Gluons have a combination of two color charges (one of red, green or blue and one of antired, antigreen and antiblue) in a superposition of states which are given by the Gell-Mann matrices. All other particles have zero color charge. Mathematically speaking, the color charge of a particle is the value of a certain quadratic Casimir operator in the representation of the particle.
In the simple language introduced previously, the three indices "1", "2" and "3" in the quark triplet above are usually identified with the three colors. The colorful language misses the following point. A gauge transformation in color SU(3) can be written as , where is a matrix which belongs to the group SU(3). Thus, after gauge transformation, the new colors are linear combinations of the old colors. In short, the simplified language introduced before is not gauge invariant.
Color charge is conserved, but the book-keeping involved in this is more complicated than just adding up the charges, as is done in quantum electrodynamics. One simple way of doing this is to look at the interaction vertex in QCD and replace it by a color-line representation. The meaning is the following. Let represent the -th component of a quark field (loosely called the -th color). The "color" of a gluon is similarly given by which corresponds to the particular Gell-Mann matrix it is associated with. This matrix has indices and . These are the "color labels" on the gluon. At the interaction vertex one has . The color-line representation tracks these indices. Color charge conservation means that the ends of these color-lines must be either in the initial or final state, equivalently, that no lines break in the middle of a diagram.
Since gluons carry color charge, two gluons can also interact. A typical interaction vertex (called the three gluon vertex) for gluons involves g + g → g. This is shown here, along with its color-line representation. The color-line diagrams can be restated in terms of conservation laws of color; however, as noted before, this is not a gauge invariant language. Note that in a typical non-abelian gauge theory the gauge boson carries the charge of the theory, and hence has interactions of this kind; for example, the W boson in the electroweak theory. In the electroweak theory, the W also carries electric charge, and hence interacts with a photon.

</doc>
<doc id="294390" url="https://en.wikipedia.org/wiki?curid=294390" title="Commutative property">
Commutative property

In mathematics, a binary operation is commutative if changing the order of the operands does not change the result. It is a fundamental property of many binary operations, and many mathematical proofs depend on it. Most familiar as the name of the property that says or , the property can also be used in more advanced settings. The name is needed because there are operations, such as division and subtraction, that do not have it (for example, ), such operations are "not" commutative, or "noncommutative operations". The idea that simple operations, such as multiplication and addition of numbers, are commutative was for many years implicitly assumed and the property was not named until the 19th century when mathematics started to become formalized. 
Common uses.
The "commutative property" (or "commutative law") is a property generally associated with binary operations and functions. If the commutative property holds for a pair of elements under a certain binary operation then the two elements are said to "commute" under that operation.
Mathematical definitions.
The term "commutative" is used in several related senses.
Examples.
Commutative operations in mathematics.
Two well-known examples of commutative binary operations:
Noncommutative operations in mathematics.
Some noncommutative binary operations:
History and etymology.
Records of the implicit use of the commutative property go back to ancient times. The Egyptians used the commutative property of multiplication to simplify computing products. Euclid is known to have assumed the commutative property of multiplication in his book "Elements". Formal uses of the commutative property arose in the late 18th and early 19th centuries, when mathematicians began to work on a theory of functions. Today the commutative property is a well known and basic property used in most branches of mathematics.
The first recorded use of the term "commutative" was in a memoir by François Servois in 1814, which used the word "commutatives" when describing functions that have what is now called the commutative property. The word is a combination of the French word "commuter" meaning "to substitute or switch" and the suffix "-ative" meaning "tending to" so the word literally means "tending to substitute or switch." The term then appeared in English in "Philosophical Transactions of the Royal Society" in 1844.
Propositional logic.
Rule of replacement.
In truth-functional propositional logic, "commutation", or "commutativity" refer to two valid rules of replacement. The rules allow one to transpose propositional variables within logical expressions in logical proofs. The rules are:
and 
where "formula_9" is a metalogical symbol representing "can be replaced in a proof with."
Truth functional connectives.
"Commutativity" is a property of some logical connectives of truth functional propositional logic. The following logical equivalences demonstrate that commutativity is a property of particular connectives. The following are truth-functional tautologies.
Set theory.
In group and set theory, many algebraic structures are called commutative when certain operands satisfy the commutative property. In higher branches of mathematics, such as analysis and linear algebra the commutativity of well-known operations (such as addition and multiplication on real and complex numbers) is often used (or implicitly assumed) in proofs.
Related properties.
Associativity.
The associative property is closely related to the commutative property. The associative property of an expression containing two or more occurrences of the same operator states that the order operations are performed in does not affect the final result, as long as the order of terms doesn't change. In contrast, the commutative property states that the order of the terms does not affect the final result.
Most commutative operations encountered in practice are also associative. However, commutativity does not imply associativity. A counterexample is the function
which is clearly commutative (interchanging "x" and "y" does not affect the result), but it is not associative (since, for example, formula_15 but formula_16).
More such examples may be found in Commutative non-associative magmas.
Symmetry.
Some forms of symmetry can be directly linked to commutativity. When a commutative operator is written as a binary function then the resulting function is symmetric across the line "y = x". As an example, if we let a function "f" represent addition (a commutative operation) so that "f"("x","y") = "x" + "y" then "f" is a symmetric function, which can be seen in the image on the right.
For relations, a symmetric relation is analogous to a commutative operation, in that if a relation "R" is symmetric, then formula_17.
Non-commuting operators in quantum mechanics.
In quantum mechanics as formulated by Schrödinger, physical variables are represented by linear operators such as "x" (meaning multiply by "x"), and formula_18. These two operators do not commute as may be seen by considering the effect of their compositions formula_19 and formula_20 (also called products of operators) on a one-dimensional wave function formula_21:
According to the uncertainty principle of Heisenberg, if the two operators representing a pair of variables do not commute, then that pair of variables are mutually complementary, which means they cannot be simultaneously measured or known precisely. For example, the position and the linear momentum in the "x"-direction of a particle are represented respectively by the operators formula_23 and formula_24 (where formula_25 is the reduced Planck constant). This is the same example except for the constant formula_26, so again the operators do not commute and the physical meaning is that the position and linear momentum in a given direction are complementary.

</doc>
<doc id="7193" url="https://en.wikipedia.org/wiki?curid=7193" title="Commutator">
Commutator

In mathematics, the commutator gives an indication of the extent to which a certain binary operation fails to be commutative. There are different definitions used in group theory and ring theory.
Group theory.
The commutator of two elements, "g" and "h", of a group "G", is the element
It is equal to the group's identity if and only if "g" and "h" commute (i.e., if and only if "gh" = "hg"). The subgroup of "G" generated by all commutators is called the "derived group" or the "commutator subgroup" of "G". Note that one must consider the subgroup generated by the set of commutators because in general the set of commutators is not closed under the group operation. Commutators are used to define nilpotent and solvable groups.
The above definition of the commutator is used by some group theorists, as well as throughout this article. However, many other group theorists define the commutator as
Identities (group theory).
Commutator identities are an important tool in group theory. The expression "ax" denotes the conjugate of "a" by "x", defined as "x"−1"a x".
Identity 5 is also known as the "Hall–Witt identity". It is a group-theoretic analogue of the [[Jacobi identity]] for the ring-theoretic commutator (see next section).
N.B. The above definition of the conjugate of "a" by "x" is used by some group theorists. Many other group theorists define the conjugate of "a" by "x" as "xax−1". This is often written formula_9. Similar identities hold for these conventions.
A wide range of identities are used that are true modulo certain subgroups. These can be particularly useful in the study of [[solvable group]]s and [[nilpotent group]]s. For instance, in any group second powers behave well
If the [[derived subgroup]] is central, then
Ring theory.
The commutator of two elements "a" and "b" of a [[ring (algebra)|ring]] or an [[associative algebra]] is defined by
It is zero if and only if "a" and "b" commute. In [[linear algebra]], if two endomorphisms of a space are represented by commuting matrices with respect to one basis, then they are so represented with respect to every basis.
By using the commutator as a [[Lie algebra|Lie bracket]], every associative algebra can be turned into a [[Lie algebra]].
The anticommutator of two elements "a" and "b" of a ring or an associative algebra is defined by
Sometimes the brackets [ ]+ are also used to denote anticommutators. The anticommutator is used less often than the commutator, but can be used for example to define [[Clifford algebra]]s, [[Jordan algebra]]s and is utilised to derive the [[Dirac equation]] in particle physics.
The commutator of two operators acting on a [[Hilbert space]] is a central concept in [[quantum mechanics]], since it quantifies how well the two [[observable]]s described by these operators can be measured simultaneously. The [[uncertainty principle]] is ultimately a theorem about such commutators, by virtue of the [[Uncertainty relation|Robertson–Schrödinger relation]]. 
In [[phase space]], equivalent commutators of function [[Moyal product|star-products]] are called [[Moyal bracket]]s, and are completely isomorphic to the Hilbert-space commutator structures mentioned.
Identities (ring theory).
The commutator has the following properties:
"Lie-algebra identities:"
The third relation is called [[anticommutativity]], while the fourth is the [[Jacobi identity]].
"Additional identities:"
If "A" is a fixed element of a ring "R", the first additional identity can be interpreted as a [[Identity of indiscernibles|Leibniz rule]] for the map formula_26 given by formula_27. In other words, the map "adA" defines a [[derivation (abstract algebra)|derivation]] on the ring "R". The second and third identities represent Leibniz rules for more than two factors that are valid for any derivation. Identities 4-6 can also be interpreted as Leibniz rules for a certain derivation.
The following useful identity ("Hadamard Lemma") involves nested commutators and underlies the [[Baker–Campbell–Hausdorff formula#An important lemma|Campbell–Baker–Hausdorff expansion]] of log (exp "A" exp "B"):
This formula is valid in any ring or algebra where the [[exponential function]] can be meaningfully defined, for instance in a [[Banach algebra]] or in a ring of [[formal power series]].
Use of the same expansion expresses the above Lie group commutator in terms of a series of nested Lie bracket (algebra) commutators,
These identities differ slightly for the anticommutator (defined above), for instance
Graded rings and algebras.
When dealing with [[graded algebra]]s, the commutator is usually replaced by the graded commutator, defined in homogeneous components as formula_32
Derivations.
Especially if one deals with multiple commutators, another notation turns out to be useful involving the [[Adjoint representation of a Lie algebra|adjoint representation]]:
Then is a [[derivation (abstract algebra)|derivation]] and is linear, 
and, crucially, a [[Lie algebra]] homomorphism, 
By contrast, it is not always an algebra homomorphism, "i.e.", a relation formula_37 does not hold in general.
External links.
[[Category:Abstract algebra]]
[[Category:Group theory]]
[[Category:Binary operations]]
[[Category:Mathematical identities]]

</doc>
<doc id="750326" url="https://en.wikipedia.org/wiki?curid=750326" title="Compact group">
Compact group

In mathematics, a compact (topological, often understood) group is a topological group whose topology is compact. Compact groups are a natural generalisation of finite groups with the discrete topology and have properties that carry over in significant fashion. Compact groups have a well-understood theory, in relation to group actions and representation theory.
In the following we will assume all groups are Hausdorff spaces.
Compact Lie groups.
Lie groups form a very nice class of topological groups, and the compact Lie groups have a particularly well-developed theory. Basic examples of compact Lie groups include
The classification theorem of compact Lie groups states that up to finite extensions and finite covers this exhausts the list of examples (which already includes some redundancies).
Classification.
Given any compact Lie group "G" one can take its identity component "G"0, which is connected. The quotient group "G"/"G"0 is the group of components π0("G") which must be finite since "G" is compact. We therefore have a finite extension
Now every compact, connected Lie group "G"0 has a finite covering
where formula_3 is a finite abelian group and formula_4 is a product of a torus and a compact, connected, simply-connected Lie group "K":
Finally, every compact, connected, simply-connected Lie group "K" is a product of compact, connected, simply-connected simple Lie groups "K""i" each of which is isomorphic to exactly one of
G2, F4, E6, E7, and E8
Further examples.
Amongst groups that are not Lie groups, and so do not carry the structure of a manifold, examples are the additive group "Z""p" of p-adic integers, and constructions from it. In fact any profinite group is a compact group. This means that Galois groups are compact groups, a basic fact for the theory of algebraic extensions in the case of infinite degree.
Pontryagin duality provides a large supply of examples of compact commutative groups. These are in duality with abelian discrete groups.
Haar measure.
Compact groups all carry a Haar measure, which will be invariant by both left and right translation (the modulus function must be a continuous homomorphism to positive reals (ℝ+, ×), and so 1). In other words, these groups are unimodular. Haar measure is easily normalised to be a probability measure, analogous to dθ/2π on the circle.
Such a Haar measure is in many cases easy to compute; for example for orthogonal groups it was known to Hurwitz, and in the Lie group cases can always be given by an invariant differential form. In the profinite case there are many subgroups of finite index, and Haar measure of a coset will be the reciprocal of the index. Therefore integrals are often computable quite directly, a fact applied constantly in number theory.
Representation theory.
The representation theory of compact groups was founded by the Peter–Weyl theorem. Hermann Weyl went on to give the detailed character theory of the compact connected Lie groups, based on maximal torus theory. The resulting Weyl character formula was one of the influential results of twentieth century mathematics.
A combination of Weyl's work and Cartan's theorem gives a survey of the whole representation theory of compact groups "G" . That is, by the Peter–Weyl theorem the irreducible unitary representations ρ of "G" are into a unitary group (of finite dimension) and the image will be a closed subgroup of the unitary group by compactness. Cartan's theorem states that Im(ρ) must itself be a Lie subgroup in the unitary group. If "G" is not itself a Lie group, there must be a kernel to ρ. Further one can form an inverse system, for the kernel of ρ smaller and smaller, of finite-dimensional unitary representations, which identifies "G" as an inverse limit of compact Lie groups. Here the fact that in the limit a faithful representation of "G" is found is another consequence of the Peter–Weyl theorem,
The unknown part of the representation theory of compact groups is thereby, roughly speaking, thrown back onto the complex representations of finite groups. This theory is rather rich in detail, but is qualitatively well understood.
Duality.
The topic of recovering a compact group from its representation theory is the subject of the Tannaka–Krein duality, now often recast in term of tannakian category theory.
From compact to non-compact groups.
The influence of the compact group theory on non-compact groups was formulated by Weyl in his unitarian trick. Inside a general semisimple Lie group there is a maximal compact subgroup, and the representation theory of such groups, developed largely by Harish-Chandra, uses intensively the restriction of a representation to such a subgroup, and also the model of Weyl's character theory.

</doc>
<doc id="6690773" url="https://en.wikipedia.org/wiki?curid=6690773" title="Compact operator on Hilbert space">
Compact operator on Hilbert space

In functional analysis, compact operators on Hilbert spaces are a direct extension of matrices: in the Hilbert spaces, they are precisely the closure of finite-rank operators in the uniform operator topology. As such, results from matrix theory can sometimes be extended to compact operators using similar arguments. In contrast, the study of general operators on infinite-dimensional spaces often requires a genuinely different approach.
For example, the spectral theory of compact operators on Banach spaces takes a form that is very similar to the Jordan canonical form of matrices. In the context of Hilbert spaces, a square matrix is unitarily diagonalizable if and only if it is normal. A corresponding result holds for normal compact operators on Hilbert spaces. (More generally, the compactness assumption can be dropped. But, as stated above, the techniques used are less routine.)
This article will discuss a few results for compact operators on Hilbert space, starting with general properties before considering subclasses of compact operators.
Some general properties.
Let "H" be a Hilbert space, "L"("H") be the bounded operators on "H". "T" ∈ "L"("H") is a compact operator if the image of each bounded set under "T" is relatively compact. We list some general properties of compact operators.
If "X" and "Y" are Hilbert spaces (in fact "X" Banach and "Y" normed will suffice), then "T:" "X" → "Y" is compact if and only if it is continuous when viewed as a map from "X" with the weak topology to "Y" (with the norm topology). (See , and note in this reference that the uniform boundedness will apply in the situation where "F" ⊆ "X" satisfies (∀φ ∈ Hom("X", "K")) sup{"x**"(φ) = φ("x"):"x"} < ∞, where "K" is the underlying field. The uniform boundedness principle applies since Hom("X", "K") with the norm topology will be a Banach space, and the maps "x**":Hom("X","K") → "K" are continuous homomorphisms with respect to this topology.)
The family of compact operators is a norm-closed, two-sided, *-ideal in "L"("H"). Consequently, a compact operator "T" cannot have a bounded inverse if "H" is infinite-dimensional. If "ST" = "TS" = "I", then the identity operator would be compact, a contradiction.
If a sequence of bounded operators "Sn" → "S" in the strong operator topology and "T" is compact, then "SnT" converges to "ST" in norm. For example, consider the Hilbert space "l"2(N), with standard basis {"en"}. Let "Pm" be the orthogonal projection on the linear span of {"e"1 ... "em"}. The sequence {"Pm"} converges to the identity operator "I" strongly but not uniformly. Define "T" by "Ten" = (1/"n")2 · "en". "T" is compact, and, as claimed above, "PmT" → "I T" = "T" in the uniform operator topology: for all "x",
Notice each "Pm" is a finite-rank operator. Similar reasoning shows that if "T" is compact, then "T" is the uniform limit of some sequence of finite-rank operators.
By the norm-closedness of the ideal of compact operators, the converse is also true.
The quotient C*-algebra of "L"("H") modulo the compact operators is called the Calkin algebra, in which one can consider properties of an operator up to compact perturbation.
Compact self adjoint operator.
A bounded operator "T" on a Hilbert space "H" is said to be self-adjoint if "T" = "T*", or equivalently,
It follows that <"Tx", "x"> is real for every "x" ∈ "H", thus eigenvalues of "T", when they exist, are real. When a closed linear subspace "L" of "H" is invariant under "T", then the restriction of "T" to "L" is a self-adjoint operator on "L", and furthermore, the orthogonal complement "L"⊥ of "L" is also invariant under "T". For example, the space "H" can be decomposed as orthogonal direct sum of two "T"–invariant closed linear subspaces: the kernel of "T", and the orthogonal complement of the kernel (which is equal to the closure of the range of "T", for any bounded self-adjoint operator). These basic facts play an important role in the proof of the spectral theorem below.
The classification result for Hermitian matrices is the spectral theorem: If "M" = "M*", then "M" is unitarily diagonalizable and the diagonalization of "M" has real entries. Let "T" be a compact self adjoint operator on a Hilbert space "H". We will prove the same statement for "T": the operator "T" can be diagonalized by an orthonormal set of eigenvectors, each of which corresponds to a real eigenvalue.
Spectral theorem.
Theorem For every compact self-adjoint operator "T" on a real or complex Hilbert space "H", there exists an orthonormal basis of "H" consisting of eigenvectors of "T". More specifically, the orthogonal complement of the kernel of "T" admits, either a finite orthonormal basis of eigenvectors of "T", or a countably infinite orthonormal basis {"en"} of eigenvectors of "T", with corresponding eigenvalues , such that .
In other words, a compact self-adjoint operator can be unitarily diagonalized. This is the spectral theorem.
When "H" is separable, one can mix the basis {"en"} with a countable orthonormal basis for the kernel of "T", and obtain an orthonormal basis {"f""n"} for "H", consisting of eigenvectors of "T" with real eigenvalues {μ"n"} such that .
Corollary For every compact self-adjoint operator "T" on a real or complex separable infinite-dimensional Hilbert space "H", there exists a countably infinite orthonormal basis {"fn"} of "H" consisting of eigenvectors of "T", with corresponding eigenvalues , such that .
The idea.
Proving the spectral theorem for a Hermitian "n" × "n" matrix "T" hinges on showing the existence of one eigenvector "x". Once this is done, Hermiticity implies that both the linear span and orthogonal complement of "x" are invariant subspaces of "T". The desired result is then obtained by iteration. The existence of an eigenvector can be shown in at least two ways:
Note. In the finite-dimensional case, part of the first approach works in much greater generailty; any square matrix, not necessarily Hermitian, has an eigenvector. This is simply not true for general operators on Hilbert spaces.
The spectral theorem for the compact self adjoint case can be obtained analogously: one finds an eigenvector by extending the second finite-dimensional argument above, then apply induction. We first sketch the argument for matrices.
Since the closed unit sphere "S" in R2"n" is compact, and "f" is continuous, "f"("S") is compact on the real line, therefore "f" attains a maximum on "S", at some unit vector "y". By Lagrange's multiplier theorem, "y" satisfies
for some λ. By Hermiticity, .
However, the Lagrange multipliers do not generalize easily to the infinite-dimensional case. Alternatively, let "z" ∈ C"n" be any vector. Notice that if a unit vector "y" maximizes <"Tx", "x"> on the unit sphere (or on the unit ball), it also maximizes the Rayleigh quotient:
Consider the function:
By calculus, , "i.e.",
Define:
After some algebra the above expression becomes ("Re" denotes the real part of a complex number)
But "z" is arbitrary, therefore . This is the crux of proof for spectral theorem in the matricial case.
Details.
Claim  If "T" is a compact self-adjoint operator on a non-zero Hilbert space "H" and
then "m"("T") or −"m"("T") is an eigenvalue of "T".
If , then "T" = 0 by the polarization identity, and this case is clear. Consider the function
Replacing "T" by −"T" if necessary, one may assume that the supremum of "f" on the closed unit ball "B" ⊂ "H" is equal to . If "f" attains its maximum "m"("T") on "B" at some unit vector "y", then, by the same argument used for matrices, "y" is an eigenvector of "T", with corresponding eigenvalue = .
By the Banach–Alaoglu theorem and the reflexivity of "H", the closed unit ball "B" is weakly compact. Also, the compactness of "T" means (see above) that "T" : "X" with the weak topology → "X" with the norm topology, is continuous. These two facts imply that "f" is continuous on "B" equipped with the weak topology, and "f" attains therefore its maximum "m" on "B" at some . By maximality, ||"y"|| = 1, which in turn implies that "y" also maximizes the Rayleigh quotient "g"("x") (see above). This shows that "y" is an eigenvector of "T", and ends the proof of the claim.
Note. The compactness of "T" is crucial. In general, "f" need not be continuous for the weak topology on the unit ball "B". For example, let "T" be the identity operator, which is not compact when "H" is infinite-dimensional. Take any orthonormal sequence {"yn"}. Then "yn" converges to 0 weakly, but lim "f"("yn") = 1 ≠ 0 = "f"(0).
Let "T" be a compact operator on a Hilbert space "H". A finite (possibly empty) or countably infinite orthonormal sequence {"en"} of eigenvectors of "T", with corresponding non-zero eigenvalues, is constructed by induction as follows. Let "H"0 = "H" and "T"0 = "T". If "m"("T"0) = 0, then "T" = 0 and the construction stops without producing any eigenvector "en". Suppose that orthonormal eigenvectors of "T" have been found. Then is invariant under "T", and by self-adjointness, the orthogonal complement "Hn" of "E""n" is an invariant subspace of "T". Let "Tn" denote the restriction of "T" to "Hn". If "m"("Tn") = 0, then "Tn" = 0, and the construction stops. Otherwise, by the "claim" applied to "Tn", there is a norm one eigenvector "en" of "T" in "H""n", with corresponding non-zero eigenvalue λ"n" = .
Let "F" = (span{"en"})⊥, where {"en"} is the finite or infinite sequence constructed by the inductive process; by self-adjointness, "F" is invariant under "T". Let "S" denote the restriction of "T" to "F". If the process was stopped after finitely many steps, with a last vector "e""m"−1, then "F"= "Hm" and "S" = "Tm" = 0 by construction. In the infinite case, compactness of "T" and the weak-convergence of "en" to 0 imply that , therefore . Since "F" is contained in "Hn" for every "n", it follows that "m"("S") ≤ "m"({"Tn"}) = |λ"n"| for every "n", hence "m"("S") = 0. This implies again that .
The fact that "S" = 0 means that "F" is contained in the kernel of "T". Conversely, if "x" ∈ ker("T"), then by self-adjointness, "x" is orthogonal to every eigenvector {"en"} with non-zero eigenvalue. It follows that , and that {"en"} is an orthonormal basis for the orthogonal complement of the kernel of "T". One can complete the diagonalization of "T" by selecting an orthonormal basis of the kernel. This proves the spectral theorem.
A shorter but more abstract proof goes as follows: by Zorn's lemma, select "U" to be a maximal subset of "H" with the following three properties: all elements of "U" are eigenvectors of "T", they have norm one, and any two distinct elements of "U" are orthogonal. Let "F" be the orthogonal complement of the linear span of "U". If "F" ≠ {0}, it is a non-trivial invariant subspace of "T", and by the initial claim there must exist a norm one eigenvector "y" of "T" in "F". But then "U" ∪ {"y"} contradicts the maximality of "U". It follows that "F" = {0}, hence span("U") is dense in "H". This shows that "U" is an orthonormal basis of "H" consisting of eigenvectors of "T".
Functional calculus.
If "T" is compact on an infinite-dimensional Hilbert space "H", then "T" is not invertible, hence σ("T"), the spectrum of "T", always contains 0. The spectral theorem shows that σ("T") consists of the eigenvalues {λ"n"} of "T", and of 0 (if 0 is not already an eigenvalue). The set σ("T") is a compact subset of the real line, and the eigenvalues are dense in σ("T").
Any spectral theorem can be reformulated in terms of a functional calculus. In the present context we have:
Theorem. Let "C"(σ("T")) denote the C*-algebra of continuous functions on σ("T"). There exists a unique isometric homomorphism such that Φ(1) = "I" and, if "f" is the identity function "f"(λ) = λ, then . Moreover, .
The functional calculus map Φ is defined in a natural way: let {"en"} be an orthonormal basis of eigenvectors for "H", with corresponding eigenvalues {λ"n"}; for , the operator Φ("f"), diagonal with respect to the orthonormal basis {"en"}, is defined by setting
for every "n". Since Φ("f") is diagonal with respect to an orthonormal basis, its norm is equal to the supremum of the modulus of diagonal coefficients,
The other properties of Φ can be readily verified. Conversely, any homomorphism Ψ satisfying the requirements of the theorem must coincide with Φ when "f" is a polynomial. By the Weierstrass approximation theorem, polynomial functions are dense in "C"(σ("T")), and it follows that . This shows that Φ is unique.
The more general continuous functional calculus can be defined for any self-adjoint (or even normal, in the complex case) bounded linear operator on a Hilbert space. The compact case, described here, is a particularly simple instance of this functional calculus.
Simultaneous diagonalisation.
Consider an Hilbert space "H" (e.g. the finite-dimensional C"n"), and a commuting set formula_13 of self-adjoint operators. Then under suitable conditions, can be simultaneously (unitarily) diagonalised. "Viz.", there exists an orthonormal basis "Q" consisting of common eigenvectors for the operators — "i.e."
Lemma. Suppose all the operators in formula_15 are compact. Then every closed non-zero formula_15-invariant sub-space "S" ⊆ "H" has a common eigenvector for formula_15.
Proof. "Case I:" all the operators have each exactly one eigenvalue. Then take any formula_18 of unit length. This is a common eigenvector.
"Case II:" there is some operator formula_19 with at least 2 eigenvalues and let formula_20. Since "T" is compact and α is non-zero, we have formula_21 is a finite-dimensional (and therefore closed) non-zero formula_15-invariant sub-space (because the operators all commute with "T", we have for formula_23 and formula_24, that formula_25). In particular we definitely have formula_26. Thus we could in principle argue by induction over dimension, yielding that formula_27 has a common eigenvector for formula_15.
Theorem 1. If all the operators in formula_15 are compact then the operators can be simultaneously (unitarily) diagonalised.
Proof. The following set
is partially ordered by inclusion. This clearly has the Zorn property. So taking "Q" a maximal member, if "Q" is a basis for the whole hilbert space "H", we are done. If this were not the case, then letting formula_31, it is easy to see that this would be an formula_15-invariant non-trivial closed subspace; and thus by the lemma above, therein would lie a common eigenvector for the operators (necessarily orthogonal to "Q"). But then there would then be a proper extension of "Q" within P; a contradiction to its maximality.
Theorem 2. If there is an injective compact operator in formula_15; then the operators can be simultaneously (unitarily) diagonalised.
Proof. Fix formula_34 compact injective. Then we have, by the spectral theory of compact symmetric operators on hilbert spaces:
where formula_36 is a discrete, countable subset of positive real numbers, and all the eigenspaces are finite-dimensional. Since formula_15 a commuting set, we have all the eigenspaces are invariant. Since the operators restricted to the eigenspaces (which are finite-dimensional) are automatically all compact, we can apply Theorem 1 to each of these, and find orthonormal bases "Q"σ for the formula_38. Since "T"0 is symmetric, we have that
is a (countable) orthonormal set. It is also, by the decomposition we first stated, a basis for "H".
Theorem 3. If "H" a finite-dimensional Hilbert space, and formula_13 a commutative set of operators, each of which is diagonalisable; then the operators can be simultaneously diagonalised.
Proof. "Case I:" all operators have exactly one eigenvalue. Then any basis for "H" will do.
"Case II:" Fix formula_34 an operator with at least two eigenvalues, and let formula_42 so that formula_43 is a symmetric operator. Now let α be an eigenvalue of formula_44. Then it is easy to see that both:
are non-trivial formula_46-invariant subspaces. By induction over dimension we have that there are linearly independent bases "Q"1, "Q"2 for the subspaces, which demonstrate that the operators in formula_46 can be simultaneously diagonalisable on the subspaces. Clearly then formula_48 demonstrates that the operators in formula_15 can be simultaneously diagonalised.
Notice we did not have to directly use the machinery of matrices at all in this proof. There are other versions which do.
We can strengthen the above to the case where all the operators merely commute with their adjoint; in this case we remove the term "orthogonal" from the diagonalisation. There are weaker results for operators arising from representations due to Weyl–Peter. Let "G" be a fixed locally compact hausdorff group, and formula_50 (the space of square integrable measurable functions with respect to the unique-up-to-scale Haar measure on "G"). Consider the continuous shift action:
Then if "G" were compact then there is a unique decomposition of "H" into a countable direct sum of finite-dimensional, irreducible, invariant subspaces (this is essentially diagonalisation of the family of operators formula_52). If "G" were not compact, but were abelien, then diagonalisation is not achieved, but we get a unique "continuous" decomposition of "H" into 1-dimensional invariant subspaces.
Compact normal operator.
The family of Hermitian matrices is a proper subset of matrices that are unitarily diagonalizable. A matrix "M" is unitarily diagonalizable if and only if it is normal, i.e. "M*M" = "MM*". Similar statements hold for compact normal operators.
Let "T" be compact and "T*T" = "TT*". Apply the "Cartesian decomposition" to "T": define
The self adjoint compact operators "R" and "J" are called the real and imaginary parts of "T" respectively. "T" is compact means "T*", consequently "R" and "J", are compact. Furthermore, the normality of "T" implies "R" and "J" commute. Therefore they can be simultaneously diagonalized, from which follows the claim.
A hyponormal compact operator (in particular, a subnormal operator) is normal.
Unitary operator.
The spectrum of a unitary operator "U" lies on the unit circle in the complex plane; it could be the entire unit circle. However, if "U" is identity plus a compact perturbation, "U" has only countable spectrum, containing 1 and possibly, a finite set or a sequence tending to 1 on the unit circle. More precisely, suppose where "C" is compact. The equations and show that "C" is normal. The spectrum of "C" contains 0, and possibly, a finite set or a sequence tending to 0. Since , the spectrum of "U" is obtained by shifting the spectrum of "C" by 1.
Examples.
be compact.

</doc>
<doc id="6042" url="https://en.wikipedia.org/wiki?curid=6042" title="Compact space">
Compact space

In mathematics, and more specifically in general topology, compactness is a property that generalizes the notion of a subset of Euclidean space being closed (that is, containing all its limit points) and bounded (that is, having all its points lie within some fixed distance of each other). Examples include a closed interval, a rectangle, or a finite set of points. This notion is defined for more general topological spaces than Euclidean space in various ways.
One such generalization is that a space is "sequentially" compact if any infinite sequence of points sampled from the space must frequently (infinitely often) get arbitrarily close to some point of the space. An equivalent definition is that every sequence of points must have an infinite subsequence that converges to some point of the space. The Heine-Borel theorem states that a subset of Euclidean space is compact in this sequential sense if and only if it is closed and bounded. Thus, if one chooses an infinite number of points in the "closed" unit interval some of those points must get arbitrarily close to some real number in that space. For instance, some of the numbers accumulate to 0 (others accumulate to 1). The same set of points would not accumulate to any point of the "open" unit interval ; so the open unit interval is not compact. Euclidean space itself is not compact since it is not bounded. In particular, the sequence of points has no subsequence that converges to any given real number.
Apart from closed and bounded subsets of Euclidean space, typical examples of compact spaces include spaces consisting not of geometrical points but of functions. The term "compact" was introduced into mathematics by Maurice Fréchet in 1904 as a distillation of this concept. Compactness in this more general situation plays an extremely important role in mathematical analysis, because many classical and important theorems of 19th century analysis, such as the extreme value theorem, are easily generalized to this situation. A typical application is furnished by the Arzelà–Ascoli theorem or the Peano existence theorem, in which one is able to conclude the existence of a function with some required properties as a limiting case of some more elementary construction.
Various equivalent notions of compactness, including sequential compactness and limit point compactness, can be developed in general metric spaces. In general topological spaces, however, different notions of compactness are not necessarily equivalent. The most useful notion, which is the standard definition of the unqualified term "compactness", is phrased in terms of the existence of finite families of open sets that "cover" the space in the sense that each point of the space must lie in some set contained in the family. This more subtle notion, introduced by Pavel Alexandrov and Pavel Urysohn in 1929, exhibits compact spaces as generalizations of finite sets. In spaces that are compact in this sense, it is often possible to patch together information that holds locally—that is, in a neighborhood of each point—into corresponding statements that hold throughout the space, and many theorems are of this character.
The term compact set is sometimes a synonym for compact space, but usually refers to a compact subspace of a topological space.
Historical development.
In the 19th century, several disparate mathematical properties were understood that would later be seen as consequences of compactness. On the one hand, Bernard Bolzano (1817) had been aware that any bounded sequence of points (in the line or plane, for instance) has a subsequence that must eventually get arbitrarily close to some other point, called a limit point. Bolzano's proof relied on the method of bisection: the sequence was placed into an interval that was then divided into two equal parts, and a part containing infinitely many terms of the sequence was selected. The process could then be repeated by dividing the resulting smaller interval into smaller and smaller parts until it closes down on the desired limit point. The full significance of Bolzano's theorem, and its method of proof, would not emerge until almost 50 years later when it was rediscovered by Karl Weierstrass.
In the 1880s, it became clear that results similar to the Bolzano–Weierstrass theorem could be formulated for spaces of functions rather than just numbers or geometrical points. The idea of regarding functions as themselves points of a generalized space dates back to the investigations of Giulio Ascoli and Cesare Arzelà. The culmination of their investigations, the Arzelà–Ascoli theorem, was a generalization of the Bolzano–Weierstrass theorem to families of continuous functions, the precise conclusion of which was that it was possible to extract a uniformly convergent sequence of functions from a suitable family of functions. The uniform limit of this sequence then played precisely the same role as Bolzano's "limit point". Towards the beginning of the twentieth century, results similar to that of Arzelà and Ascoli began to accumulate in the area of integral equations, as investigated by David Hilbert and Erhard Schmidt. For a certain class of Green functions coming from solutions of integral equations, Schmidt had shown that a property analogous to the Arzelà–Ascoli theorem held in the sense of mean convergence—or convergence in what would later be dubbed a Hilbert space. This ultimately led to the notion of a compact operator as an offshoot of the general notion of a compact space. It was Maurice Fréchet who, in 1906, had distilled the essence of the Bolzano–Weierstrass property and coined the term "compactness" to refer to this general phenomenon (he used the term already in his 1904 paper which led to the famous 1906 thesis) .
However, a different notion of compactness altogether had also slowly emerged at the end of the 19th century from the study of the continuum, which was seen as fundamental for the rigorous formulation of analysis. In 1870, Eduard Heine showed that a continuous function defined on a closed and bounded interval was in fact uniformly continuous. In the course of the proof, he made use of a lemma that from any countable cover of the interval by smaller open intervals, it was possible to select a finite number of these that also covered it. The significance of this lemma was recognized by Émile Borel (1895), and it was generalized to arbitrary collections of intervals by Pierre Cousin (1895) and Henri Lebesgue (1904). The Heine–Borel theorem, as the result is now known, is another special property possessed by closed and bounded sets of real numbers.
This property was significant because it allowed for the passage from local information about a set (such as the continuity of a function) to global information about the set (such as the uniform continuity of a function). This sentiment was expressed by , who also exploited it in the development of the integral now bearing his name. Ultimately the Russian school of point-set topology, under the direction of Pavel Alexandrov and Pavel Urysohn, formulated Heine–Borel compactness in a way that could be applied to the modern notion of a topological space. showed that the earlier version of compactness due to Fréchet, now called (relative) sequential compactness, under appropriate conditions followed from the version of compactness that was formulated in terms of the existence of finite subcovers. It was this notion of compactness that became the dominant one, because it was not only a stronger property, but it could be formulated in a more general setting with a minimum of additional technical machinery, as it relied only on the structure of the open sets in a space.
Basic examples.
An example of a compact space is the (closed) unit interval of real numbers. If one chooses an infinite number of distinct points in the unit interval, then there must be some accumulation point in that interval. For instance, the odd-numbered terms of the sequence get arbitrarily close to 0, while the even-numbered ones get arbitrarily close to 1. The given example sequence shows the importance of including the boundary points of the interval, since the limit points must be in the space itself — an open (or half-open) interval of the real numbers is not compact. It is also crucial that the interval be bounded, since in the interval one could choose the sequence of points , of which no sub-sequence ultimately gets arbitrarily close to any given real number.
In two dimensions, closed disks are compact since for any infinite number of points sampled from a disk, some subset of those points must get arbitrarily close either to a point within the disc, or to a point on the boundary. However, an open disk is not compact, because a sequence of points can tend to the boundary without getting arbitrarily close to any point in the interior. Likewise, spheres are compact, but a sphere missing a point is not since a sequence of points can tend to the missing point, thereby not getting arbitrarily close to any point "within" the space. Lines and planes are not compact, since one can take a set of equally-spaced points in any given direction without approaching any point.
Definitions.
Various definitions of compactness may apply, depending on the level of generality. A subset of Euclidean space in particular is called compact if it is closed and bounded. This implies, by the Bolzano–Weierstrass theorem, that any infinite sequence from the set has a subsequence that converges to a point in the set. Various equivalent notions of compactness, such as sequential compactness and limit point compactness, can be developed in general metric spaces.
In general topological spaces, however, the different notions of compactness are not equivalent, and the most useful notion of compactness—originally called "bicompactness"—is defined using covers consisting of open sets (see "Open cover definition" below). That this form of compactness holds for closed and bounded subsets of Euclidean space is known as the Heine–Borel theorem. Compactness, when defined in this manner, often allows one to take information that is known locally—in a neighbourhood of each point of the space—and to extend it to information that holds globally throughout the space. An example of this phenomenon is Dirichlet's theorem, to which it was originally applied by Heine, that a continuous function on a compact interval is uniformly continuous; here, continuity is a local property of the function, and uniform continuity the corresponding global property.
Open cover definition.
Formally, a topological space "X" is called "compact" if each of its open covers has a finite subcover. Otherwise, it is called "non-compact". Explicitly, this means that for every arbitrary collection
of open subsets of such that
there is a finite subset of such that
Some branches of mathematics such as algebraic geometry, typically influenced by the French school of Bourbaki, use the term "quasi-compact" for the general notion, and reserve the term "compact" for topological spaces that are both Hausdorff and "quasi-compact". A compact set is sometimes referred to as a "compactum", plural "compacta".
Equivalent definitions.
Assuming the axiom of choice, the following are equivalent:
Euclidean space.
For any subset "A" of Euclidean space R"n", "A" is compact if and only if it is closed and bounded; this is the Heine–Borel theorem.
As a Euclidean space is a metric space, the conditions in the next subsection also apply to all of its subsets. Of all of the equivalent conditions, it is in practice easiest to verify that a subset is closed and bounded, for example, for a closed interval or closed "n"-ball.
Metric spaces.
For any metric space ("X,d"), the following are equivalent:
A compact metric space (X,d) also satisfies the following properties:
Characterization by continuous functions.
Let "X" be a topological space and C("X") the ring of real continuous functions on "X". For each "p"∈"X", the evaluation map
given by ev"p"("f")="f"("p") is a ring homomorphism. The kernel of ev"p" is a maximal ideal, since the residue field is the field of real numbers, by the first isomorphism theorem. A topological space "X" is pseudocompact if and only if every maximal ideal in C("X") has residue field the real numbers. For completely regular spaces, this is equivalent to every maximal ideal being the kernel of an evaluation homomorphism. There are pseudocompact spaces that are not compact, though.
In general, for non-pseudocompact spaces there are always maximal ideals "m" in C("X") such that the residue field C("X")/"m" is a (non-archimedean) hyperreal field. The framework of non-standard analysis allows for the following alternative characterization of compactness: a topological space "X" is compact if and only if every point "x" of the natural extension "*X" is infinitely close to a point "x"0 of "X" (more precisely, "x" is contained in the monad of "x"0).
Hyperreal definition.
A space "X" is compact if its natural extension "*X" (for example, an ultrapower) has the property that every point of "*X" is infinitely close to a suitable point of formula_5. For example, an open real interval "X"=(0,1) is not compact because its hyperreal extension *(0,1) contains infinitesimals, which are infinitely close to 0, which is not a point of "X".
Compactness of subspaces.
A subset "K" of a topological space "X" is called compact if it is compact as a subspace. Explicitly, this means that for every arbitrary collection
of open subsets of such that
there is a finite subset "J" of "A" such that
Properties of compact spaces.
Functions and compact spaces.
A continuous image of a compact space is compact.
This implies the extreme value theorem: a continuous real-valued function on a nonempty compact space is bounded above and attains its supremum. (Slightly more generally, this is true for an upper semicontinuous function.) As a sort of converse to the above statements, the pre-image of a compact space under a proper map is compact.
Compact spaces and set operations.
A closed subset of a compact space is compact., and a finite union of compact sets is compact.
The product of any collection of compact spaces is compact. (Tychonoff's theorem, which is equivalent to the axiom of choice)
Every topological space "X" is an open dense subspace of a compact space having at most one point more than "X", by the Alexandroff one-point compactification. By the same construction, every locally compact Hausdorff space "X" is an open dense subspace of a compact Hausdorff space having at most one point more than "X".
Ordered compact spaces.
A nonempty compact subset of the real numbers has a greatest element and a least element.
Let "X" be a simply ordered set endowed with the order topology. Then "X" is compact if and only if "X" is a complete lattice (i.e. all subsets have suprema and infima).

</doc>
<doc id="303500" url="https://en.wikipedia.org/wiki?curid=303500" title="Completing the square">
Completing the square

In elementary algebra, completing the square is a technique for converting a quadratic polynomial of the form
to the form
In this context, "constant" means not depending on "x". The expression inside the parenthesis is of the form ("x" + constant). Thus 
for some values of "h" and "k".
Completing the square is used in
In mathematics, completing the square is often applied in any computation involving quadratic polynomials. Completing the square is also used to derive the quadratic formula.
Overview.
Background.
There is a simple formula in elementary algebra for computing the square of a binomial:
For example:
In any perfect square, the number "p" is always half the coefficient of "x", and the constant term is equal to "p"2.
Basic example.
Consider the following quadratic polynomial:
This quadratic is not a perfect square, since 28 is not the square of 5:
However, it is possible to write the original quadratic as the sum of this square and a constant:
This is called completing the square.
General description.
Given any monic quadratic
it is possible to form a square that has the same first two terms:
This square differs from the original quadratic only in the value of the constant
term. Therefore, we can write
where "k" is a constant. This operation is known as completing the square.
For example:
Non-monic case.
Given a quadratic polynomial of the form
it is possible to factor out the coefficient "a", and then complete the square for the resulting monic polynomial.
Example:
This allows us to write any quadratic polynomial in the form
Formula.
The result of completing the square may be written as a formula. For the general case:
Specifically, when "a"=1:
The matrix case looks very similar:
where formula_20 has to be symmetric.
If formula_20 is not symmetric the formulae for formula_22 and formula_23 have
to be generalized to:
Relation to the graph.
In analytic geometry, the graph of any quadratic function is a parabola in the "xy"-plane. Given a quadratic polynomial of the form
the numbers "h" and "k" may be interpreted as the Cartesian coordinates of the vertex of the parabola. That is, "h" is the "x"-coordinate of the axis of symmetry, and "k" is the minimum value (or maximum value, if "a" < 0) of the quadratic function.
One way to see this is to note that the graph of the function "ƒ"("x") = "x"2 is a parabola whose vertex is at the origin (0, 0). Therefore, the graph of the function "ƒ"("x" − "h") = ("x" − "h")2 is a parabola shifted to the right by "h" whose vertex is at ("h", 0), as shown in the top figure. In contrast, the graph of the function "ƒ"("x") + "k" = "x"2 + "k" is a parabola shifted upward by "k" whose vertex is at (0, "k"), as shown in the center figure. Combining both horizontal and vertical shifts yields "ƒ"("x" − "h") + "k" = ("x" − "h")2 + "k" is a parabola shifted to the right by "h" and upward by "k" whose vertex is at ("h", "k"), as shown in the bottom figure.
Solving quadratic equations.
Completing the square may be used to solve any quadratic equation. For example:
The first step is to complete the square:
Next we solve for the squared term:
Then either
and therefore
This can be applied to any quadratic equation. When the "x"2 has a coefficient other than 1, the first step is to divide out the equation by this coefficient: for an example see the non-monic case below.
Irrational and complex roots.
Unlike methods involving factoring the equation, which is reliable only if the roots are rational, completing the square will find the roots of a quadratic equation even when those roots are irrational or complex. For example, consider the equation
Completing the square gives
so
Then either
so
In terser language:
Equations with complex roots can be handled in the same way. For example:
Non-monic case.
For an equation involving a non-monic quadratic, the first step to solving them is to divide through by the coefficient of "x"2. For example:
Other applications.
Integration.
Completing the square may be used to evaluate any integral of the form
using the basic integrals
For example, consider the integral
Completing the square in the denominator gives:
This can now be evaluated by using the substitution
"u" = "x" + 3, which yields
Complex numbers.
Consider the expression
where "z" and "b" are complex numbers, "z"* and "b"* are the complex conjugates of "z" and "b", respectively, and "c" is a real number. Using the identity |"u"|2 = "uu"* we can rewrite this as
which is clearly a real quantity. This is because
As another example, the expression
where "a", "b", "c", "x", and "y" are real numbers, with "a" > 0 and "b" > 0, may be expressed in terms of the square of the absolute value of a complex number. Define
Then
so
Idempotent matrix.
A matrix "M" is idempotent when "M" 2 = "M". Idempotent matrices generalize the idempotent properties of 0 and 1. The completion of the square method of addressing the equation 
shows that some idempotent 2 × 2 matrices are parametrized by a circle in the ("a,b")-plane:
The matrix formula_52 will be idempotent provided formula_51 which, upon completing the square, becomes
In the ("a,b")-plane, this is the equation of a circle with center (1/2, 0) and radius 1/2.
Geometric perspective.
Consider completing the square for the equation
Since "x"2 represents the area of a square with side of length "x", and "bx" represents the area of a rectangle with sides "b" and "x", the process of completing the square can be viewed as visual manipulation of rectangles.
Simple attempts to combine the "x"2 and the "bx" rectangles into a larger square result in a missing corner. The term ("b"/2)2 added to each side of the above equation is precisely the area of the missing corner, whence derives the terminology "completing the square". 
A variation on the technique.
As conventionally taught, completing the square consists of adding the third term, "v" 2 to
to get a square. There are also cases in which one can add the middle term, either 2"uv" or −2"uv", to
to get a square.
Example: the sum of a positive number and its reciprocal.
By writing
we show that the sum of a positive number "x" and its reciprocal is always greater than or equal to 2. The square of a real expression is always greater than or equal to zero, which gives the stated bound; and here we achieve 2 just when "x" is 1, causing the square to vanish.
Example: factoring a simple quartic polynomial.
Consider the problem of factoring the polynomial
This is
so the middle term is 2("x"2)(18) = 36"x"2. Thus we get
(the last line being added merely to follow the convention of decreasing degrees of terms).

</doc>
<doc id="5759" url="https://en.wikipedia.org/wiki?curid=5759" title="Complex analysis">
Complex analysis

Complex analysis, traditionally known as the theory of functions of a complex variable, is the branch of mathematical analysis that investigates functions of complex numbers. It is useful in many branches of mathematics, including algebraic geometry, number theory, applied mathematics; as well as in physics, including hydrodynamics and thermodynamics and also in engineering fields such as nuclear, aerospace, mechanical and electrical engineering.
Murray R. Spiegel described complex analysis as "one of the most beautiful as well as useful branches of Mathematics".
Complex analysis is particularly concerned with analytic functions of complex variables (or, more generally, meromorphic functions). Because the separate real and imaginary parts of any analytic function must satisfy Laplace's equation, complex analysis is widely applicable to two-dimensional problems in physics.
History.
Complex analysis is one of the classical branches in mathematics with roots in the 19th century and just prior. Important mathematicians associated with complex analysis include Euler, Gauss, Riemann, Cauchy, Weierstrass, and many more in the 20th century. Complex analysis, in particular the theory of conformal mappings, has many physical applications and is also used throughout analytic number theory. In modern times, it has become very popular through a new boost from complex dynamics and the pictures of fractals produced by iterating holomorphic functions. Another important application of complex analysis is in string theory which studies conformal invariants in quantum field theory.
Complex functions.
A complex function is one in which the independent variable and the dependent variable are both complex numbers. More precisely, a complex function is a function whose domain and range are subsets of the complex plane.
For any complex function, both the independent variable and the dependent variable may be separated into real and imaginary parts:
In other words, the components of the function ,
can be interpreted as real-valued functions of the two real variables, and .
The basic concepts of complex analysis are often introduced by extending the elementary real functions (e.g., exponential functions, logarithmic functions, and trigonometric functions) into the complex domain.
Holomorphic functions.
Holomorphic functions are complex functions, defined on an open subset of the complex plane, that are differentiable. Complex differentiability has much stronger consequences than usual (real) differentiability. For instance, holomorphic functions are infinitely differentiable, whereas some real differentiable functions are not. Most elementary functions, including the exponential function, the trigonometric functions, and all polynomial functions, are holomorphic.
"See also": analytic function, holomorphic sheaf and vector bundles.
Major results.
One of the central tools in complex analysis is the line integral. The line integral around a closed path of a function that is holomorphic everywhere inside the area bounded by the closed path is always zero, which is what the Cauchy integral theorem states. The values of such a holomorphic function inside a disk can be computed by a path integral on the disk's boundary, as shown in (Cauchy's integral formula). Path integrals in the complex plane are often used to determine complicated real integrals, and here the theory of residues among others is applicable (see methods of contour integration). A "pole" (or isolated singularity) of a function is a point where the function's value becomes unbounded, or "blows up". If a function has such a pole, then one can compute the function's residue there, which can be used to compute path integrals involving the function; this is the content of the powerful residue theorem. The remarkable behavior of holomorphic functions near essential singularities is described by Picard's Theorem. Functions that have only poles but no essential singularities are called meromorphic. Laurent series are the complex-valued equivalent to Taylor series, but can be used to study the behavior of functions near singularities through infinite sums of more well understood functions, such as polynomials.
A bounded function that is holomorphic in the entire complex plane must be constant; this is Liouville's theorem. It can be used to provide a natural and short proof for the fundamental theorem of algebra which states that the field of complex numbers is algebraically closed.
If a function is holomorphic throughout a connected domain then its values are fully determined by its values on any smaller subdomain. The function on the larger domain is said to be analytically continued from its values on the smaller domain. This allows the extension of the definition of functions, such as the Riemann zeta function, which are initially defined in terms of infinite sums that converge only on limited domains to almost the entire complex plane. Sometimes, as in the case of the natural logarithm, it is impossible to analytically continue a holomorphic function to a non-simply connected domain in the complex plane but it is possible to extend it to a holomorphic function on a closely related surface known as a Riemann surface.
All this refers to complex analysis in one variable. There is also a very rich theory of complex analysis in more than one complex dimension in which the analytic properties such as power series expansion carry over whereas most of the geometric properties of holomorphic functions in one complex dimension (such as conformality) do not carry over. The Riemann mapping theorem about the conformal relationship of certain domains in the complex plane, which may be the most important result in the one-dimensional theory, fails dramatically in higher dimensions.

</doc>
<doc id="173918" url="https://en.wikipedia.org/wiki?curid=173918" title="Complex conjugate">
Complex conjugate

In mathematics, the complex conjugate of a complex number is the number with equal real part and imaginary part equal in magnitude but opposite in sign. For example, the complex conjugate of 3 + 4"i" is 3 − 4"i".
In polar form, the conjugate of formula_1 is formula_2. This can be shown using Euler's formula.
Complex conjugates are important for finding roots of polynomials. According to the complex conjugate root theorem, if a complex number is a root to a polynomial in one variable with real coefficients (such as the quadratic equation or the cubic equation), so is its conjugate.
Notation.
The complex conjugate of a complex number formula_3 is written as formula_4 or formula_5. The first notation avoids confusion with the notation for the conjugate transpose of a matrix, which can be thought of as a generalization of the complex conjugate. The second is preferred in physics, where dagger is used for the conjugate transpose, while the bar-notation is more common in pure mathematics. If a complex number is represented as a 2×2 matrix, the notations are identical. In some texts, the complex conjugate of a previous known number is abbreviated as "c.c.". For example, writing formula_6 means formula_7
Properties.
The following properties apply for all complex numbers "z" and "w", unless stated otherwise, and can be proven by writing "z" and "w" in the form "a" + "ib".
A significant property of the complex conjugate is that a complex number is equal to its complex conjugate if its imaginary part is zero, that is, if the complex number is real.
For any two complex numbers "w,z":
Thus the pair of variables formula_13 and formula_14 also serve up the plane as do "x,y" and formula_15 and formula_16. Furthermore, the formula_14 variable is useful in specifying lines in the plane:
is a line through the origin and perpendicular to formula_19 since the real part of formula_20 is zero only when the cosine of the angle between formula_13 and formula_19 is zero. Similarly, for a fixed complex unit "u" = exp("b" i), the equation:
determines the line through formula_24 in the direction of u.
These uses of the conjugate of "z" as a variable are illustrated in Frank Morley's book "Inversive Geometry" (1933), written with his son Frank Vigor Morley.
Generalizations.
The other planar real algebras, dual numbers, and split-complex numbers are also explicated by use of complex conjugation.
For matrices of complex numbers formula_25, where formula_26 represents the element-by-element conjugation of formula_27. Contrast this to the property formula_28, where formula_29 represents the conjugate transpose of formula_27.
Taking the conjugate transpose (or adjoint) of complex matrices generalizes complex conjugation. Even more general is the concept of adjoint operator for operators on (possibly infinite-dimensional) complex Hilbert spaces. All this is subsumed by the *-operations of C*-algebras.
One may also define a conjugation for quaternions and coquaternions: the conjugate of formula_31 is formula_32.
Note that all these generalizations are multiplicative only if the factors are reversed:
Since the multiplication of planar real algebras is commutative, this reversal is not needed there.
There is also an abstract notion of conjugation for vector spaces formula_34 over the complex numbers. In this context, 
any antilinear map formula_35 that satisfies
is called a "complex conjugation", or a real structure. As the involution formula_46 is antilinear, it cannot be the identity map on formula_34.
Of course, formula_46 is a formula_49-linear transformation of formula_34, if one notes that every complex space "V" has a real form obtained by taking the same vectors as in the original space and restricting the scalars to be real. The above properties actually define a real structure on the complex vector space formula_34. 
One example of this notion is the conjugate transpose operation of complex matrices defined above. It should be remarked that on generic complex vector spaces there is no "canonical" notion of complex conjugation.

</doc>
<doc id="5826" url="https://en.wikipedia.org/wiki?curid=5826" title="Complex number">
Complex number

A complex number is a number that can be expressed in the form , where and are real numbers and is the imaginary unit, that satisfies the equation . In this expression, is the "real part" and is the "imaginary part" of the complex number.
Complex numbers extend the concept of the one-dimensional number line to the two-dimensional complex plane by using the horizontal axis for the real part and the vertical axis for the imaginary part. The complex number can be identified with the point in the complex plane. A complex number whose real part is zero is said to be purely imaginary, whereas a complex number whose imaginary part is zero is a real number. In this way, the complex numbers contain the ordinary real numbers while extending them in order to solve problems that cannot be solved with real numbers alone.
As well as their use within mathematics, complex numbers have practical applications in many fields, including physics, chemistry, biology, economics, electrical engineering, and statistics. The Italian mathematician Gerolamo Cardano is the first known to have introduced complex numbers. He called them "fictitious" during his attempts to find solutions to cubic equations in the 16th century.
Overview.
Complex numbers allow for solutions to certain equations that have no solutions in real numbers. For example, the equation
has no real solution, since the square of a real number cannot be negative. Complex numbers provide a solution to this problem. The idea is to extend the real numbers with the imaginary unit where , so that solutions to equations like the preceding one can be found. In this case the solutions are and , as can be verified using the fact that :
According to the fundamental theorem of algebra, all polynomial equations with real or complex coefficients in a single variable have a solution in complex numbers.
Definition.
A complex number is a number of the form , where and are real numbers and is the "imaginary unit", satisfying . For example, is a complex number.
The real number is called the "real part" of the complex number ; the real number is called the "imaginary part" of . By this convention the imaginary part does not include the imaginary unit: hence , not , is the imaginary part. The real part of a complex number is denoted by or ; the imaginary part of a complex number is denoted by or . For example,
Hence, in terms of its real and imaginary parts, a complex number is equal to formula_5. This expression is sometimes known as the Cartesian form of .
A real number can be regarded as a complex number whose imaginary part is 0. A purely imaginary number is a complex number whose real part is zero. It is common to write for and for . Moreover, when the imaginary part is negative, it is common to write with instead of , for example instead of .
The set of all complex numbers is denoted by , formula_6 or formula_7.
Notation.
Some authors write instead of , particularly when "b" is a radical. In some disciplines, in particular electromagnetism and electrical engineering, is used instead of , since is frequently used for electric current. In these cases complex numbers are written as or .
Complex plane.
A complex number can be viewed as a point or position vector in a two-dimensional Cartesian coordinate system called the complex plane or Argand diagram (see and ), named after Jean-Robert Argand. The numbers are conventionally plotted using the real part as the horizontal component, and imaginary part as vertical (see Figure 1). These two values used to identify a given complex number are therefore called its "Cartesian", "rectangular", or "algebraic form".
A position vector may also be defined in terms of its magnitude and direction relative to the origin. These are emphasized in a complex number's "polar form". Using the polar form of the complex number in calculations may lead to a more intuitive interpretation of mathematical results. Notably, the operations of addition and multiplication take on a very natural geometric character when complex numbers are viewed as position vectors: addition corresponds to vector addition while multiplication corresponds to multiplying their magnitudes and adding their arguments (i.e. the angles they make with the "x" axis). Viewed in this way the multiplication of a complex number by corresponds to rotating the position vector counterclockwise by a quarter turn (90°) about the origin: = 2 = .
History in brief.
The solution in radicals (without trigonometric functions) of a general cubic equation contains the square roots of negative numbers when all three roots are real numbers, a situation that cannot be rectified by factoring aided by the rational root test if the cubic is irreducible (the so-called casus irreducibilis). This conundrum led Italian mathematician Gerolamo Cardano to conceive of complex numbers in around 1545, though his understanding was rudimentary.
Work on the problem of general polynomials ultimately led to the fundamental theorem of algebra, which shows that with complex numbers, a solution exists to every polynomial equation of degree one or higher. Complex numbers thus form an algebraically closed field, where any polynomial equation has a root.
Many mathematicians contributed to the full development of complex numbers. The rules for addition, subtraction, multiplication, and division of complex numbers were developed by the Italian mathematician Rafael Bombelli. A more abstract formalism for the complex numbers was further developed by the Irish mathematician William Rowan Hamilton, who extended this abstraction to the theory of quaternions.
Relations.
Equality.
Two complex numbers are equal if and only if both their real and imaginary parts are equal. In symbols:
Ordering.
Because complex numbers are naturally thought of as existing on a two-dimensional plane, there is no natural linear ordering on the set of complex numbers.
There is no linear ordering on the complex numbers that is compatible with addition and multiplication. Formally, we say that the complex numbers cannot have the structure of an ordered field. This is because any square in an ordered field is at least , but .
Elementary operations.
Conjugation.
The "complex conjugate" of the complex number is defined to be . It is denoted formula_9 or .
Formally, for any complex number "z":
Geometrically, formula_9 is the "reflection" of about the real axis. Conjugating twice gives the original complex number: formula_12.
The real and imaginary parts of a complex number can be extracted using the conjugate:
Moreover, a complex number is real if and only if it equals its conjugate.
Conjugation distributes over the standard arithmetic operations:
The reciprocal of a nonzero complex number is given by
This formula can be used to compute the multiplicative inverse of a complex number if it is given in rectangular coordinates. Inversive geometry, a branch of geometry studying reflections more general than ones about a line, can also be expressed in terms of complex numbers. In the network analysis of electrical circuits, the complex conjugate is used in finding the equivalent impedance when the maximum power transfer theorem is used.
Addition and subtraction.
Complex numbers are added by adding the real and imaginary parts of the summands. That is to say:
Similarly, subtraction is defined by
Using the visualization of complex numbers in the complex plane, the addition has the following geometric interpretation: the sum of two complex numbers "A" and "B", interpreted as points of the complex plane, is the point "X" obtained by building a parallelogram three of whose vertices are "O", "A" and "B". Equivalently, "X" is the point such that the triangles with vertices "O", "A", "B", and "X", "B", "A", are congruent.
Multiplication and division.
The multiplication of two complex numbers is defined by the following formula:
In particular, the square of the imaginary unit is −1:
The preceding definition of multiplication of general complex numbers follows naturally from this fundamental property of the imaginary unit. Indeed, if is treated as a number so that means times , the above multiplication rule is identical to the usual rule for multiplying two sums of two terms.
The division of two complex numbers is defined in terms of complex multiplication, which is described above, and real division. When at least one of and is non-zero, we have
Division can be defined in this way because of the following observation:
As shown earlier, is the complex conjugate of the denominator . At least one of the real part and the imaginary part of the denominator must be nonzero for division to be defined. This is called "rationalization" of the denominator (although the denominator in the final expression might be an irrational real number).
Square root.
The square roots of (with ) are formula_30, where
and
where sgn is the signum function. This can be seen by squaring formula_30 to obtain . Here formula_34 is called the modulus of , and the square root sign indicates the square root with non-negative real part, called the principal square root; also formula_35, where formula_36.
Polar form.
Absolute value and argument.
An alternative way of defining a point "P" in the complex plane, other than using the "x"- and "y"-coordinates, is to use the distance of the point from "O", the point whose coordinates are (the origin), together with the angle subtended between the positive real axis and the line segment "OP" in a counterclockwise direction. This idea leads to the polar form of complex numbers.
The "absolute value" (or "modulus" or "magnitude") of a complex number is
If is a real number (i.e., ), then . In general, by Pythagoras' theorem, is the distance of the point "P" representing the complex number to the origin. The square of the absolute value is
where formula_9 is the complex conjugate of formula_40.
The "argument" of (in many applications referred to as the "phase") is the angle of the radius "OP" with the positive real axis, and is written as formula_41. As with the modulus, the argument can be found from the rectangular form formula_42:
The value of is expressed in radians in this article. It can increase by any integer multiple of and still give the same angle. Hence, the arg function is sometimes considered as multivalued. Normally, as given above, the principal value in the interval is chosen. Values in the range are obtained by adding if the value is negative. The polar angle for the complex number 0 is indeterminate, but arbitrary choice of the angle 0 is common.
The value of equals the result of atan2: formula_44.
Together, and give another way of representing complex numbers, the "polar form", as the combination of modulus and argument fully specify the position of a point on the plane. Recovering the original rectangular co-ordinates from the polar form is done by the formula called "trigonometric form"
Using Euler's formula this can be written as
Using the cis function, this is sometimes abbreviated to
In angle notation, often used in electronics to represent a phasor with amplitude and phase , it is written as
Multiplication and division in polar form.
Formulas for multiplication, division and exponentiation are simpler in polar form than the corresponding formulas in Cartesian coordinates. Given two complex numbers and , because of the well-known trigonometric identities 
we may derive
In other words, the absolute values are multiplied and the arguments are added to yield the polar form of the product. For example, multiplying by corresponds to a quarter-turn counter-clockwise, which gives back . The picture at the right illustrates the multiplication of
Since the real and imaginary part of are equal, the argument of that number is 45 degrees, or π/4 (in radian). On the other hand, it is also the sum of the angles at the origin of the red and blue triangles are arctan(1/3) and arctan(1/2), respectively. Thus, the formula
holds. As the arctan function can be approximated highly efficiently, formulas like this—known as Machin-like formulas—are used for high-precision approximations of π.
Similarly, division is given by
Exponentiation.
Euler's formula.
Euler's formula states that, for any real number "x",
where "e" is the base of the natural logarithm. This can be proved through induction by observing that
and so on, and by considering the Taylor series expansions of "e""ix", "cos(x)" and "sin(x)":
The rearrangement of terms is justified because each series is absolutely convergent.
Natural logarithm.
Euler's formula allows us to observe that, for any complex number
where "r" is a non-negative real number, one possible value for "z"'s natural logarithm is
Because cos and sin are periodic functions, the natural logarithm may be considered a multi-valued function, with:
Integer and fractional exponents.
We may use the identity
to define complex exponentiation, which is likewise multi-valued:
When "n" is an integer, this simplifies to de Moivre's formula:
The th roots of are given by
for any integer satisfying . Here is the usual (positive) th root of the positive real number . While the th root of a positive real number is chosen to be the "positive" real number satisfying there is no natural way of distinguishing one particular complex th root of a complex number. Therefore, the th root of is considered as a multivalued function (in ), as opposed to a usual function , for which is a uniquely defined number. Formulas such as
(which holds for positive real numbers), do in general not hold for complex numbers.
Properties.
Field structure.
The set C of complex numbers is a field. Briefly, this means that the following facts hold: first, any two complex numbers can be added and multiplied to yield another complex number. Second, for any complex number , its additive inverse is also a complex number; and third, every nonzero complex number has a reciprocal complex number. Moreover, these operations satisfy a number of laws, for example the law of commutativity of addition and multiplication for any two complex numbers and :
These two laws and the other requirements on a field can be proven by the formulas given above, using the fact that the real numbers themselves form a field.
Unlike the reals, C is not an ordered field, that is to say, it is not possible to define a relation that is compatible with the addition and multiplication. In fact, in any ordered field, the square of any element is necessarily positive, so precludes the existence of an ordering on C.
When the underlying field for a mathematical topic or construct is the field of complex numbers, the topic's name is usually modified to reflect that fact. For example: complex analysis, complex matrix, complex polynomial, and complex Lie algebra.
Solutions of polynomial equations.
Given any complex numbers (called coefficients) , the equation
has at least one complex solution "z", provided that at least one of the higher coefficients is nonzero. This is the statement of the "fundamental theorem of algebra". Because of this fact, C is called an algebraically closed field. This property does not hold for the field of rational numbers Q (the polynomial does not have a rational root, since is not a rational number) nor the real numbers R (the polynomial does not have a real root for , since the square of is positive for any real number ).
There are various proofs of this theorem, either by analytic methods such as Liouville's theorem, or topological ones such as the winding number, or a proof combining Galois theory and the fact that any real polynomial of "odd" degree has at least one real root.
Because of this fact, theorems that hold "for any algebraically closed field", apply to C. For example, any non-empty complex square matrix has at least one (complex) eigenvalue.
Algebraic characterization.
The field C has the following three properties: first, it has characteristic 0. This means that for any number of summands (all of which equal one). Second, its transcendence degree over Q, the prime field of C, is the cardinality of the continuum. Third, it is algebraically closed (see above). It can be shown that any field having these properties is isomorphic (as a field) to C. For example, the algebraic closure of Q"p" also satisfies these three properties, so these two fields are isomorphic. Also, C is isomorphic to the field of complex Puiseux series. However, specifying an isomorphism requires the axiom of choice. Another consequence of this algebraic characterization is that C contains many proper subfields that are isomorphic to C.
Characterization as a topological field.
The preceding characterization of C describes only the algebraic aspects of C. That is to say, the properties of nearness and continuity, which matter in areas such as analysis and topology, are not dealt with. The following description of C as a topological field (that is, a field that is equipped with a topology, which allows the notion of convergence) does take into account the topological properties. C contains a subset (namely the set of positive real numbers) of nonzero elements satisfying the following three conditions:
Moreover, C has a nontrivial involutive automorphism (namely the complex conjugation), such that is in for any nonzero in C.
Any field with these properties can be endowed with a topology by taking the sets as a base, where ranges over the field and ranges over . With this topology is isomorphic as a "topological" field to C.
The only connected locally compact topological fields are R and C. This gives another characterization of C as a topological field, since C can be distinguished from R because the nonzero complex numbers are connected, while the nonzero real numbers are not.
Formal construction.
Formal development.
Above, complex numbers have been defined by introducing , the imaginary unit, as a symbol. More rigorously, the set of complex numbers can be defined as the set of ordered pairs of real numbers. In this notation, the above formulas for addition and multiplication read
It is then just a matter of notation to express as .
Though this low-level construction does accurately describe the structure of the complex numbers, the following equivalent definition reveals the algebraic nature of more immediately. This characterization relies on the notion of fields and polynomials. A field is a set endowed with addition, subtraction, multiplication and division operations that behave as is familiar from, say, rational numbers. For example, the distributive law
must hold for any three elements , and of a field. The set of real numbers does form a field. A polynomial with real coefficients is an expression of the form
where the are real numbers. The usual addition and multiplication of polynomials endows the set of all such polynomials with a ring structure. This ring is called polynomial ring.
The quotient ring can be shown to be a field.
This extension field contains two square roots of , namely (the cosets of) and , respectively. (The cosets of) and form a basis of as a real vector space, which means that each element of the extension field can be uniquely written as a linear combination in these two elements. Equivalently, elements of the extension field can be written as ordered pairs of real numbers. Moreover, the above formulas for addition etc. correspond to the ones yielded by this abstract algebraic approach—the two definitions of the field are said to be isomorphic (as fields). Together with the above-mentioned fact that is algebraically closed, this also shows that is an algebraic closure of .
Matrix representation of complex numbers.
Complex numbers can also be represented by matrices that have the following form:
Here the entries and are real numbers. The sum and product of two such matrices is again of this form, and the sum and product of complex numbers corresponds to the sum and product of such matrices. The geometric description of the multiplication of complex numbers can also be expressed in terms of rotation matrices by using this correspondence between complex numbers and such matrices. Moreover, the square of the absolute value of a complex number expressed as a matrix is equal to the determinant of that matrix:
The conjugate formula_77 corresponds to the transpose of the matrix.
Though this representation of complex numbers with matrices is the most common, many other representations arise from matrices "other than" formula_78 that square to the negative of the identity matrix. See the article on 2 × 2 real matrices for other representations of complex numbers.
Complex analysis.
The study of functions of a complex variable is known as complex analysis and has enormous practical use in applied mathematics as well as in other branches of mathematics. Often, the most natural proofs for statements in real analysis or even number theory employ techniques from complex analysis (see prime number theorem for an example). Unlike real functions, which are commonly represented as two-dimensional graphs, complex functions have four-dimensional graphs and may usefully be illustrated by color-coding a three-dimensional graph to suggest four dimensions, or by animating the complex function's dynamic transformation of the complex plane.
Complex exponential and related functions.
The notions of convergent series and continuous functions in (real) analysis have natural analogs in complex analysis. A sequence of complex numbers is said to converge if and only if its real and imaginary parts do. This is equivalent to the (ε, δ)-definition of limits, where the absolute value of real numbers is replaced by the one of complex numbers. From a more abstract point of view, C, endowed with the metric
is a complete metric space, which notably includes the triangle inequality
for any two complex numbers and .
Like in real analysis, this notion of convergence is used to construct a number of elementary functions: the "exponential function" , also written , is defined as the infinite series
and the series defining the real trigonometric functions sine and cosine, as well as hyperbolic functions such as sinh also carry over to complex arguments without change. "Euler's identity" states:
for any real number "φ", in particular
Unlike in the situation of real numbers, there is an infinitude of complex solutions of the equation
for any complex number . It can be shown that any such solution —called complex logarithm of —satisfies
where arg is the argument defined above, and ln the (real) natural logarithm. As arg is a multivalued function, unique only up to a multiple of 2"π", log is also multivalued. The principal value of log is often taken by restricting the imaginary part to the interval .
Complex exponentiation is defined as
Consequently, they are in general multi-valued. For , for some natural number , this recovers the non-uniqueness of th roots mentioned above.
Complex numbers, unlike real numbers, do not in general satisfy the unmodified power and logarithm identities, particularly when naïvely treated as single-valued functions; see failure of power and logarithm identities. For example, they do not satisfy
Both sides of the equation are multivalued by the definition of complex exponentiation given here, and the values on the left are a subset of those on the right.
Holomorphic functions.
A function "f" : C → C is called holomorphic if it satisfies the Cauchy–Riemann equations. For example, any R-linear map C → C can be written in the form
with complex coefficients and . This map is holomorphic if and only if . The second summand formula_89 is real-differentiable, but does not satisfy the Cauchy–Riemann equations.
Complex analysis shows some features not apparent in real analysis. For example, any two holomorphic functions and that agree on an arbitrarily small open subset of C necessarily agree everywhere. Meromorphic functions, functions that can locally be written as with a holomorphic function , still share some of the features of holomorphic functions. Other functions have essential singularities, such as at .
Applications.
Complex numbers have essential concrete applications in a variety of scientific and related areas such as signal processing, control theory, electromagnetism, fluid dynamics, quantum mechanics, cartography, and vibration analysis. Some applications of complex numbers are:
Control theory.
In control theory, systems are often transformed from the time domain to the frequency domain using the Laplace transform. The system's poles and zeros are then analyzed in the "complex plane". The root locus, Nyquist plot, and Nichols plot techniques all make use of the complex plane.
In the root locus method, it is especially important whether the poles and zeros are in the left or right half planes, i.e. have real part greater than or less than zero. If a linear, time-invariant (LTI) system has poles that are
If a system has zeros in the right half plane, it is a nonminimum phase system.
Improper integrals.
In applied fields, complex numbers are often used to compute certain real-valued improper integrals, by means of complex-valued functions. Several methods exist to do this; see methods of contour integration.
Fluid dynamics.
In fluid dynamics, complex functions are used to describe potential flow in two dimensions.
Dynamic equations.
In differential equations, it is common to first find all complex roots of the characteristic equation of a linear differential equation or equation system and then attempt to solve the system in terms of base functions of the form . Likewise, in difference equations, the complex roots of the characteristic equation of the difference equation system are used, to attempt to solve the system in terms of base functions of the form .
Electromagnetism and electrical engineering.
In electrical engineering, the Fourier transform is used to analyze varying voltages and currents. The treatment of resistors, capacitors, and inductors can then be unified by introducing imaginary, frequency-dependent resistances for the latter two and combining all three in a single complex number called the impedance. This approach is called phasor calculus.
In electrical engineering, the imaginary unit is denoted by , to avoid confusion with , which is generally in use to denote electric current, or, more particularly, , which is generally in use to denote instantaneous electric current.
Since the voltage in an AC circuit is oscillating, it can be represented as
To obtain the measurable quantity, the real part is taken:
The complex-valued signal formula_92 is called the analytic representation of the real-valued, measurable signal formula_93.
Signal analysis.
Complex numbers are used in signal analysis and other fields for a convenient description for periodically varying signals. For given real functions representing actual physical quantities, often in terms of sines and cosines, corresponding complex functions are considered of which the real parts are the original quantities. For a sine wave of a given frequency, the absolute value of the corresponding is the amplitude and the argument is the phase.
If Fourier analysis is employed to write a given real-valued signal as a sum of periodic functions, these periodic functions are often written as complex valued functions of the form
and
where ω represents the angular frequency and the complex number "A" encodes the phase and amplitude as explained above.
This use is also extended into digital signal processing and digital image processing, which utilize digital versions of Fourier analysis (and wavelet analysis) to transmit, compress, restore, and otherwise process digital audio signals, still images, and video signals.
Another example, relevant to the two side bands of amplitude modulation of AM radio, is:
Quantum mechanics.
The complex number field is intrinsic to the mathematical formulations of quantum mechanics, where complex Hilbert spaces provide the context for one such formulation that is convenient and perhaps most standard. The original foundation formulas of quantum mechanics—the Schrödinger equation and Heisenberg's matrix mechanics—make use of complex numbers.
Relativity.
In special and general relativity, some formulas for the metric on spacetime become simpler if one takes the time component of the spacetime continuum to be imaginary. (This approach is no longer standard in classical relativity, but is used in an essential way in quantum field theory.) Complex numbers are essential to spinors, which are a generalization of the tensors used in relativity.
Geometry.
Fractals.
Certain fractals are plotted in the complex plane, e.g. the Mandelbrot set and Julia sets.
Triangles.
Every triangle has a unique Steiner inellipse—an ellipse inside the triangle and tangent to the midpoints of the three sides of the triangle. The foci of a triangle's Steiner inellipse can be found as follows, according to Marden's theorem: Denote the triangle's vertices in the complex plane as , , and . Write the cubic equation formula_97, take its derivative, and equate the (quadratic) derivative to zero. Marden's Theorem says that the solutions of this equation are the complex numbers denoting the locations of the two foci of the Steiner inellipse.
Algebraic number theory.
As mentioned above, any nonconstant polynomial equation (in complex coefficients) has a solution in C. A fortiori, the same is true if the equation has rational coefficients. The roots of such equations are called algebraic numbers – they are a principal object of study in algebraic number theory. Compared to , the algebraic closure of Q, which also contains all algebraic numbers, C has the advantage of being easily understandable in geometric terms. In this way, algebraic methods can be used to study geometric questions and vice versa. With algebraic methods, more specifically applying the machinery of field theory to the number field containing roots of unity, it can be shown that it is not possible to construct a regular nonagon using only compass and straightedge – a purely geometric problem.
Another example are Gaussian integers, that is, numbers of the form , where and are integers, which can be used to classify sums of squares.
Analytic number theory.
Analytic number theory studies numbers, often integers or rationals, by taking advantage of the fact that they can be regarded as complex numbers, in which analytic methods can be used. This is done by encoding number-theoretic information in complex-valued functions. For example, the Riemann zeta function is related to the distribution of prime numbers.
History.
The earliest fleeting reference to square roots of negative numbers can perhaps be said to occur in the work of the Greek mathematician Hero of Alexandria in the 1st century AD, where in his "Stereometrica" he considers, apparently in error, the volume of an impossible frustum of a pyramid to arrive at the term formula_98 in his calculations, although negative quantities were not conceived of in Hellenistic mathematics and Heron merely replaced it by its positive (formula_99).
The impetus to study complex numbers proper first arose in the 16th century when algebraic solutions for the roots of cubic and quartic polynomials were discovered by Italian mathematicians (see Niccolò Fontana Tartaglia, Gerolamo Cardano). It was soon realized that these formulas, even if one was only interested in real solutions, sometimes required the manipulation of square roots of negative numbers. As an example, Tartaglia's formula for a cubic equation of the form formula_100 gives the solution to the equation as
At first glance this looks like nonsense. However formal calculations with complex numbers show that the equation has solutions , formula_102 and formula_103. Substituting these in turn for formula_104 in Tartaglia's cubic formula and simplifying, one gets 0, 1 and −1 as the solutions of . Of course this particular equation can be solved at sight but it does illustrate that when general formulas are used to solve cubic equations with real roots then, as later mathematicians showed rigorously, the use of complex numbers is unavoidable. Rafael Bombelli was the first to explicitly address these seemingly paradoxical solutions of cubic equations and developed the rules for complex arithmetic trying to resolve these issues.
The term "imaginary" for these quantities was coined by René Descartes in 1637, although he was at pains to stress their imaginary nature
A further source of confusion was that the equation formula_105 seemed to be capriciously inconsistent with the algebraic identity formula_106, which is valid for non-negative real numbers and , and which was also used in complex number calculations with one of , positive and the other negative. The incorrect use of this identity (and the related identity formula_107) in the case when both and are negative even bedeviled Euler. This difficulty eventually led to the convention of using the special symbol in place of to guard against this mistake. Even so, Euler considered it natural to introduce students to complex numbers much earlier than we do today. In his elementary algebra text book, Elements of Algebra, he introduces these numbers almost at once and then uses them in a natural way throughout.
In the 18th century complex numbers gained wider use, as it was noticed that formal manipulation of complex expressions could be used to simplify calculations involving trigonometric functions. For instance, in 1730 Abraham de Moivre noted that the complicated identities relating trigonometric functions of an integer multiple of an angle to powers of trigonometric functions of that angle could be simply re-expressed by the following well-known formula which bears his name, de Moivre's formula:
In 1748 Leonhard Euler went further and obtained Euler's formula of complex analysis:
by formally manipulating complex power series and observed that this formula could be used to reduce any trigonometric identity to much simpler exponential identities.
The idea of a complex number as a point in the complex plane (above) was first described by Caspar Wessel in 1799, although it had been anticipated as early as 1685 in Wallis's "De Algebra tractatus".
Wessel's memoir appeared in the Proceedings of the Copenhagen Academy but went largely unnoticed. In 1806 Jean-Robert Argand independently issued a pamphlet on complex numbers and provided a rigorous proof of the fundamental theorem of algebra. Gauss had earlier published an essentially topological proof of the theorem in 1797 but expressed his doubts at the time about "the true metaphysics of the square root of −1". It was not until 1831 that he overcame these doubts and published his treatise on complex numbers as points in the plane, largely establishing modern notation and terminology. The English mathematician G. H. Hardy remarked that Gauss was the first mathematician to use complex numbers in 'a really confident and scientific way' although mathematicians such as Niels Henrik Abel and Carl Gustav Jacob Jacobi were necessarily using them routinely before Gauss published his 1831 treatise. Augustin Louis Cauchy and Bernhard Riemann together brought the fundamental ideas of complex analysis to a high state of completion, commencing around 1825 in Cauchy's case.
The common terms used in the theory are chiefly due to the founders. Argand called formula_110 the "direction factor", and formula_111 the "modulus"; Cauchy (1828) called formula_112 the "reduced form" (l'expression réduite) and apparently introduced the term "argument"; Gauss used for formula_113, introduced the term "complex number" for , and called the "norm". The expression "direction coefficient", often used for formula_112, is due to Hankel (1867), and "absolute value," for "modulus," is due to Weierstrass.
Later classical writers on the general theory include Richard Dedekind, Otto Hölder, Felix Klein, Henri Poincaré, Hermann Schwarz, Karl Weierstrass and many others.
Generalizations and related notions.
The process of extending the field R of reals to C is known as Cayley–Dickson construction. It can be carried further to higher dimensions, yielding the quaternions H and octonions O which (as a real vector space) are of dimension 4 and 8, respectively.
However, just as applying the construction to reals loses the property of ordering, more properties familiar from real and complex numbers vanish with increasing dimension. The quaternions are only a skew field, i.e. for some : for two quaternions, the multiplication of octonions fails (in addition to not being commutative) to be associative: for some : .
Reals, complex numbers, quaternions and octonions are all normed division algebras over R. However, by Hurwitz's theorem they are the only ones. The next step in the Cayley–Dickson construction, the sedenions, in fact fails to have this structure.
The Cayley–Dickson construction is closely related to the regular representation of C, thought of as an R-algebra (an R-vector space with a multiplication), with respect to the basis . This means the following: the R-linear map
for some fixed complex number can be represented by a matrix (once a basis has been chosen). With respect to the basis , this matrix is
i.e., the one mentioned in the section on matrix representation of complex numbers above. While this is a linear representation of C in the 2 × 2 real matrices, it is not the only one. Any matrix
has the property that its square is the negative of the identity matrix: . Then
is also isomorphic to the field C, and gives an alternative complex structure on R2. This is generalized by the notion of a linear complex structure.
Hypercomplex numbers also generalize R, C, H, and O. For example, this notion contains the split-complex numbers, which are elements of the ring (as opposed to ). In this ring, the equation has four solutions.
The field R is the completion of Q, the field of rational numbers, with respect to the usual absolute value metric. Other choices of metrics on Q lead to the fields Q"p" of "p"-adic numbers (for any prime number "p"), which are thereby analogous to R. There are no other nontrivial ways of completing Q than R and Q"p", by Ostrowski's theorem. The algebraic closure formula_119 of Q"p" still carry a norm, but (unlike C) are not complete with respect to it. The completion formula_120 of formula_119 turns out to be algebraically closed. This field is called "p"-adic complex numbers by analogy.
The fields R and Q"p" and their finite field extensions, including C, are local fields.

</doc>
<doc id="217628" url="https://en.wikipedia.org/wiki?curid=217628" title="Complex plane">
Complex plane

In mathematics, the complex plane or "z"-plane is a geometric representation of the complex numbers established by the real axis and the orthogonal imaginary axis. It can be thought of as a modified Cartesian plane, with the real part of a complex number represented by a displacement along the x-axis, and the imaginary part by a displacement along the y-axis.
The concept of the complex plane allows a geometric interpretation of complex numbers. Under addition, they add like vectors. The multiplication of two complex numbers can be expressed most easily in polar coordinates—the magnitude or "modulus" of the product is the product of the two absolute values, or moduli, and the angle or "argument" of the product is the sum of the two angles, or arguments. In particular, multiplication by a complex number of modulus 1 acts as a rotation.
The complex plane is sometimes called the Argand plane because it is used in Argand diagrams. These are named after Jean-Robert Argand (1768–1822), although they were first described by Danish land surveyor and mathematician Caspar Wessel (1745–1818). Argand diagrams are frequently used to plot the positions of the poles and zeroes of a function in the complex plane.
Notational conventions.
In complex analysis, the complex numbers are customarily represented by the symbol "z", which can be separated into its real ("x") and imaginary ("y") parts:
for example: "z" = 4 + 5"i", where "x" and "y" are real numbers, and "i" is the imaginary unit. In this customary notation the complex number "z" corresponds to the point ("x", "y") in the Cartesian plane.
In the Cartesian plane the point ("x", "y") can also be represented in polar coordinates as
In the Cartesian plane it may be assumed that the arctangent takes values from −"π/2" to "π/2" (in radians), and some care must be taken to define the "real" arctangent function for points ("x", "y") when "x" ≤ 0. In the complex plane these polar coordinates take the form
where
Here |"z"| is the "absolute value" or "modulus" of the complex number "z"; "θ", the "argument" of "z", is usually taken on the interval 0 ≤ "θ" < 2"π"; and the last equality (to |"z"|"e""iθ") is taken from Euler's formula. Notice that the "argument" of "z" is multi-valued, because the complex exponential function is periodic, with period 2"πi". Thus, if "θ" is one value of arg("z"), the other values are given by arg("z") = "θ" + 2"nπ", where "n" is any integer ≠ 0. While seldom used explicitly, the geometric view of the complex numbers is implicitly based on its structure of a Euclidean vector space of dimension 2, where the inner product of complex numbers and is given by formula_5; then for a complex number its absolute value || coincides with its Euclidean norm, and its argument with the angle turning from 1 to .
The theory of contour integration comprises a major part of complex analysis. In this context the direction of travel around a closed curve is important – reversing the direction in which the curve is traversed multiplies the value of the integral by −1. By convention the "positive" direction is counterclockwise. For example, the unit circle is traversed in the positive direction when we start at the point "z" = 1, then travel up and to the left through the point "z" = "i", then down and to the left through −1, then down and to the right through −"i", and finally up and to the right to "z" = 1, where we started.
Almost all of complex analysis is concerned with complex functions – that is, with functions that map some subset of the complex plane into some other (possibly overlapping, or even identical) subset of the complex plane. Here it is customary to speak of the domain of "f"("z") as lying in the "z"-plane, while referring to the range or "image" of "f"("z") as a set of points in the "w"-plane. In symbols we write
and often think of the function "f" as a transformation of the "z"-plane (with coordinates ("x", "y")) into the "w"-plane (with coordinates ("u", "v")).
Stereographic projections.
It can be useful to think of the complex plane as if it occupied the surface of a sphere. Given a sphere of unit radius, place its center at the origin of the complex plane, oriented so that the equator on the sphere coincides with the unit circle in the plane, and the north pole is "above" the plane.
We can establish a one-to-one correspondence between the points on the surface of the sphere minus the north pole and the points in the complex plane as follows. Given a point in the plane, draw a straight line connecting it with the north pole on the sphere. That line will intersect the surface of the sphere in exactly one other point. The point "z" = 0 will be projected onto the south pole of the sphere. Since the interior of the unit circle lies inside the sphere, that entire region (|"z"| < 1) will be mapped onto the southern hemisphere. The unit circle itself (|"z"| = 1) will be mapped onto the equator, and the exterior of the unit circle (|"z"| > 1) will be mapped onto the northern hemisphere, minus the north pole. Clearly this procedure is reversible – given any point on the surface of the sphere that is not the north pole, we can draw a straight line connecting that point to the north pole and intersecting the flat plane in exactly one point.
Under this stereographic projection the north pole itself is not associated with any point in the complex plane. We perfect the one-to-one correspondence by adding one more point to the complex plane – the so-called "point at infinity"—and identifying it with the north pole on the sphere. This topological space, the complex plane plus the point at infinity, is known as the extended complex plane. We speak of a single "point at infinity" when discussing complex analysis. There are two points at infinity (positive, and negative) on the real number line, but there is only one point at infinity (the north pole) in the extended complex plane.
Imagine for a moment what will happen to the lines of latitude and longitude when they are projected from the sphere onto the flat plane. The lines of latitude are all parallel to the equator, so they will become perfect circles centered on the origin "z" = 0. And the lines of longitude will become straight lines passing through the origin (and also through the "point at infinity", since they pass through both the north and south poles on the sphere).
This is not the only possible yet plausible stereographic situation of the projection of a sphere onto a plane consisting of two or more values. For instance, the north pole of the sphere might be placed on top of the origin "z" = −1 in a plane that's tangent to the circle. The details don't really matter. Any stereographic projection of a sphere onto a plane will produce one "point at infinity", and it will map the lines of latitude and longitude on the sphere into circles and straight lines, respectively, in the plane.
Cutting the plane.
When discussing functions of a complex variable it is often convenient to think of a cut in the complex plane. This idea arises naturally in several different contexts.
Multi-valued relationships and branch points.
Consider the simple two-valued relationship
Before we can treat this relationship as a single-valued function, the range of the resulting value must be restricted somehow. When dealing with the square roots of non-negative real numbers this is easily done. For instance, we can just define
to be the non-negative real number "y" such that "y"2 = "x". This idea doesn't work so well in the two-dimensional complex plane. To see why, let's think about the way the value of "f"("z") varies as the point "z" moves around the unit circle. We can write
Evidently, as "z" moves all the way around the circle, "w" only traces out one-half of the circle. So one continuous motion in the complex plane has transformed the positive square root "e"0 = 1 into the negative square root "e""iπ" = −1.
This problem arises because the point "z" = 0 has just one square root, while every other complex number "z" ≠ 0 has exactly two square roots. On the real number line we could circumvent this problem by erecting a "barrier" at the single point "x" = 0. A bigger barrier is needed in the complex plane, to prevent any closed contour from completely encircling the branch point "z" = 0. This is commonly done by introducing a branch cut; in this case the "cut" might extend from the point "z" = 0 along the positive real axis to the point at infinity, so that the argument of the variable "z" in the cut plane is restricted to the range 0 ≤ arg("z") < 2"π".
We can now give a complete description of "w" = "z"½. To do so we need two copies of the "z"-plane, each of them cut along the real axis. On one copy we define the square root of 1 to be e0 = 1, and on the other we define the square root of 1 to be "e""iπ" = −1. We call these two copies of the complete cut plane "sheets". By making a continuity argument we see that the (now single-valued) function "w" = "z"½ maps the first sheet into the upper half of the "w"-plane, where 0 ≤ arg("w") < "π", while mapping the second sheet into the lower half of the "w"-plane (where "π" ≤ arg("w") < 2"π").
The branch cut in this example doesn't have to lie along the real axis. It doesn't even have to be a straight line. Any continuous curve connecting the origin "z" = 0 with the point at infinity would work. In some cases the branch cut doesn't even have to pass through the point at infinity. For example, consider the relationship
Here the polynomial "z"2 − 1 vanishes when "z" = ±1, so "g" evidently has two branch points. We can "cut" the plane along the real axis, from −1 to 1, and obtain a sheet on which "g"("z") is a single-valued function. Alternatively, the cut can run from "z" = 1 along the positive real axis through the point at infinity, then continue "up" the negative real axis to the other branch point, "z" = −1.
This situation is most easily visualized by using the stereographic projection described above. On the sphere one of these cuts runs longitudinally through the southern hemisphere, connecting a point on the equator ("z" = −1) with another point on the equator ("z" = 1), and passing through the south pole (the origin, "z" = 0) on the way. The second version of the cut runs longitudinally through the northern hemisphere and connects the same two equatorial points by passing through the north pole (that is, the point at infinity).
Restricting the domain of meromorphic functions.
A meromorphic function is a complex function that is holomorphic and therefore analytic everywhere in its domain except at a finite, or countably infinite, number of points. The points at which such a function cannot be defined are called the poles of the meromorphic function. Sometimes all these poles lie in a straight line. In that case mathematicians may say that the function is "holomorphic on the cut plane". Here's a simple example.
The gamma function, defined by
where "γ" is the Euler–Mascheroni constant, and has simple poles at 0, −1, −2, −3, ... because exactly one denominator in the infinite product vanishes when "z" is zero, or a negative integer. Since all its poles lie on the negative real axis, from "z" = 0 to the point at infinity, this function might be described as "holomorphic on the cut plane, the cut extending along the negative real axis, from 0 (inclusive) to the point at infinity."
Alternatively, Γ("z") might be described as "holomorphic in the cut plane with −"π" < arg("z") < "π" and excluding the point "z" = 0."
Notice that this cut is slightly different from the branch cut we've already encountered, because it actually "excludes" the negative real axis from the cut plane. The branch cut left the real axis connected with the cut plane on one side (0 ≤ "θ"), but severed it from the cut plane along the other side ("θ" < 2"π").
Of course, it's not actually necessary to exclude the entire line segment from "z" = 0 to −∞ to construct a domain in which Γ("z") is holomorphic. All we really have to do is puncture the plane at a countably infinite set of points {0, −1, −2, −3, ...}. But a closed contour in the punctured plane might encircle one or more of the poles of Γ("z"), giving a contour integral that is not necessarily zero, by the residue theorem. By cutting the complex plane we ensure not only that Γ("z") is holomorphic in this restricted domain – we also ensure that the contour integral of Γ over any closed curve lying in the cut plane is identically equal to zero.
Specifying convergence regions.
Many complex functions are defined by infinite series, or by continued fractions. A fundamental consideration in the analysis of these infinitely long expressions is identifying the portion of the complex plane in which they converge to a finite value. A cut in the plane may facilitate this process, as the following examples show.
Consider the function defined by the infinite series
Since "z"2 = (−"z")2 for every complex number "z", it's clear that "f"("z") is an even function of "z", so the analysis can be restricted to one half of the complex plane. And since the series is undefined when
it makes sense to cut the plane along the entire imaginary axis and establish the convergence of this series where the real part of "z" is not zero before undertaking the more arduous task of examining "f"("z") when "z" is a pure imaginary number.
In this example the cut is a mere convenience, because the points at which the infinite sum is undefined are isolated, and the "cut" plane can be replaced with a suitably "punctured" plane. In some contexts the cut is necessary, and not just convenient. Consider the infinite periodic continued fraction
It can be shown that "f"("z") converges to a finite value if and only if "z" is not a negative real number such that "z" < −¼. In other words, the convergence region for this continued fraction is the cut plane, where the cut runs along the negative real axis, from −¼ to the point at infinity.
Gluing the cut plane back together.
We have already seen how the relationship
can be made into a single-valued function by splitting the domain of "f" into two disconnected sheets. It is also possible to "glue" those two sheets back together to form a single Riemann surface on which "f"("z") = "z"1/2 can be defined as a holomorphic function whose image is the entire "w"-plane (except for the point "w" = 0). Here's how that works.
Imagine two copies of the cut complex plane, the cuts extending along the positive real axis from "z" = 0 to the point at infinity. On one sheet define 0 ≤ arg("z") < 2"π", so that 11/2 = "e"0 = 1, by definition. On the second sheet define 2"π" ≤ arg("z") < 4"π", so that 11/2 = "e""iπ" = −1, again by definition. Now flip the second sheet upside down, so the imaginary axis points in the opposite direction of the imaginary axis on the first sheet, with both real axes pointing in the same direction, and "glue" the two sheets together (so that the edge on the first sheet labeled ""θ" = 0" is connected to the edge labeled ""θ" < 4"π" on the second sheet, and the edge on the second sheet labeled "θ" = 2"π" is connected to the edge labeled "θ" < 2"π"" on the first sheet). The result is the Riemann surface domain on which "f"("z") = "z"1/2 is single-valued and holomorphic (except when "z" = 0).
To understand why "f" is single-valued in this domain, imagine a circuit around the unit circle, starting with "z" = 1 on the first sheet. When 0 ≤ "θ" < 2"π" we are still on the first sheet. When "θ" = 2"π" we have crossed over onto the second sheet, and are obliged to make a second complete circuit around the branch point "z" = 0 before returning to our starting point, where "θ" = 4"π" is equivalent to "θ" = 0, because of the way we glued the two sheets together. In other words, as the variable "z" makes two complete turns around the branch point, the image of "z" in the "w"-plane traces out just one complete circle.
Formal differentiation shows that
from which we can conclude that the derivative of "f" exists and is finite everywhere on the Riemann surface, except when "z" = 0 (that is, "f" is holomorphic, except when "z" = 0).
How can the Riemann surface for the function
also discussed above, be constructed? Once again we begin with two copies of the "z"-plane, but this time each one is cut along the real line segment extending from "z" = −1 to "z" = 1 – these are the two branch points of "g"("z"). We flip one of these upside down, so the two imaginary axes point in opposite directions, and glue the corresponding edges of the two cut sheets together. We can verify that "g" is a single-valued function on this surface by tracing a circuit around a circle of unit radius centered at "z" = 1. Commencing at the point "z" = 2 on the first sheet we turn halfway around the circle before encountering the cut at "z" = 0. The cut forces us onto the second sheet, so that when "z" has traced out one full turn around the branch point "z" = 1, "w" has taken just one-half of a full turn, the sign of "w" has been reversed (since "e""iπ" = −1), and our path has taken us to the point "z" = 2 on the second sheet of the surface. Continuing on through another half turn we encounter the other side of the cut, where "z" = 0, and finally reach our starting point ("z" = 2 on the first sheet) after making two full turns around the branch point.
The natural way to label "θ" = arg("z") in this example is to set −"π" < "θ" ≤ "π" on the first sheet, with "π" < "θ" ≤ 3"π" on the second. The imaginary axes on the two sheets point in opposite directions so that the counterclockwise sense of positive rotation is preserved as a closed contour moves from one sheet to the other (remember, the second sheet is "upside down"). Imagine this surface embedded in a three-dimensional space, with both sheets parallel to the "xy"-plane. Then there appears to be a vertical hole in the surface, where the two cuts are joined together. What if the cut is made from "z" = −1 down the real axis to the point at infinity, and from "z" = 1, up the real axis until the cut meets itself? Again a Riemann surface can be constructed, but this time the "hole" is horizontal. Topologically speaking, both versions of this Riemann surface are equivalent – they are orientable two-dimensional surfaces of genus one.
Use of the complex plane in control theory.
In control theory, one use of the complex plane is known as the 's-plane'. It is used to visualise the roots of the equation describing a system's behaviour (the characteristic equation) graphically. The equation is normally expressed as a polynomial in the parameter 's' of the Laplace transform, hence the name 's' plane. Points in the s-plane take the form formula_18, where "'j'" is used instead of the usual "'i"' to represent the imaginary component.
Another related use of the complex plane is with the Nyquist stability criterion. This is a geometric principle which allows the stability of a closed-loop feedback system to be determined by inspecting a Nyquist plot of its open-loop magnitude and phase response as a function of frequency (or loop transfer function) in the complex plane.
The 'z-plane' is a discrete-time version of the s-plane, where z-transforms are used instead of the Laplace transformation.
Other meanings of "complex plane".
The preceding sections of this article deal with the complex plane as the geometric analogue of the complex numbers. Although this usage of the term "complex plane" has a long and mathematically rich history, it is by no means the only mathematical concept that can be characterized as "the complex plane". There are at least three additional possibilities.
Terminology.
While the terminology "complex plane" is historically accepted, the object could be more appropriately named "complex line" as it is a 1-dimensional complex vector space.

</doc>
<doc id="1474467" url="https://en.wikipedia.org/wiki?curid=1474467" title="Compton wavelength">
Compton wavelength

The Compton wavelength is a quantum mechanical property of a particle. It was introduced by Arthur Compton in his explanation of the scattering of photons by electrons (a process known as Compton scattering). The Compton wavelength of a particle is equivalent to the wavelength of a photon whose energy is the same as the rest-mass energy of the particle.
The standard Compton wavelength, "λ", of a particle is given by
where "h" is the Planck constant, "m" is the particle's rest mass, and "c" is the speed of light. The significance of this formula is shown in the derivation of the Compton shift formula.
The CODATA 2010 value for the Compton wavelength of the electron is . Other particles have different Compton wavelengths.
Reduced Compton wavelength.
When the Compton wavelength is divided by formula_2, one obtains a smaller or “reduced” Compton wavelength formula_3:
Role in equations for massive particles.
The reduced Compton wavelength is a natural representation for mass on the quantum scale, and as such, it appears in many of the fundamental equations of quantum mechanics. The reduced Compton wavelength appears in the relativistic Klein–Gordon equation for a free particle:
It appears in the Dirac equation (the following is an explicitly covariant form employing the Einstein summation convention):
The reduced Compton wavelength also appears in Schrödinger's equation, although its presence is obscured in traditional representations of the equation. The following is the traditional representation of Schrödinger's equation for an electron in a hydrogen-like atom:
Dividing through by formula_8, and rewriting in terms of the fine structure constant, one obtains:
Relationship between the reduced and non-reduced Compton wavelength.
The reduced Compton wavelength is a natural representation for mass on the quantum scale. Equations that pertain to mass in the form of mass, like Klein-Gordon and Schrödinger's, use the reduced Compton wavelength. The non-reduced Compton wavelength is a natural representation for mass that has been converted into energy. Equations that pertain to the conversion of mass into energy, or to the wavelengths of photons interacting with mass, use the non-reduced Compton wavelength.
A particle of rest mass "m" has a rest energy of .
The non-reduced Compton wavelength for this particle is the wavelength of a photon of the same energy. For photons of frequency "f", energy is given by
which yields the non-reduced or standard Compton wavelength formula if solved for "λ".
Limitation on measurement.
The Compton wavelength expresses a fundamental limitation on measuring the position of a particle, taking into account quantum mechanics and special relativity.
This limitation depends on the rest mass "m" of the particle.
To see how, note that we can measure the position of a particle 
by bouncing light off it - but measuring the position accurately requires light of short wavelength. Light with a short wavelength consists of photons of high energy. If the energy of these photons exceeds "mc"2, when one hits the particle whose position is being measured the collision may yield enough energy to create a new particle of the same type. This
renders moot the question of the original particle's location.
This argument also shows that the reduced Compton wavelength is the cutoff below which quantum field theory – which can describe particle creation and annihilation – becomes important.
We can make the above argument a bit more precise as follows. Suppose we wish to measure the position of a particle to within an accuracy Δ"x". 
Then the uncertainty relation for position and momentum says that
so the uncertainty in the particle's momentum satisfies
Using the relativistic relation between momentum and energy "E2" = "(pc)2" + "(mc2)2", when "Δp" exceeds "mc" then the uncertainty in energy is greater than "mc"2, which is enough energy to create another particle of the same type. But we must exclude this. It follows that there is a fundamental limitation on Δ"x":
Thus the uncertainty in position must be greater than half of the reduced Compton wavelength "ħ"/"mc".
The Compton wavelength can be contrasted with the de Broglie wavelength, which depends on the momentum of a particle and determines the cutoff between particle and wave behavior in quantum mechanics.
Relationship to other constants.
Typical atomic lengths, wave numbers, and areas in physics can be related to the reduced Compton wavelength for the electron (formula_14) and the electromagnetic fine structure constant (formula_15)
The Bohr radius is related to the Compton wavelength by:
The classical electron radius is about 3 times larger than the proton radius, and is written:
The Rydberg constant is written:
For fermions, the reduced Compton wavelength sets the cross-section of interactions. For example, the cross-section for Thomson scattering of a photon from an electron is equal to
which is roughly the same as the cross-sectional area of an iron-56 nucleus. For gauge bosons, the Compton wavelength sets the effective range of the Yukawa interaction: since the photon has no rest mass, electromagnetism has infinite range.
Typical lengths and areas in gravitational physics can be related to the Compton wavelength and the gravitational coupling constant (formula_20 which is the gravitational analog of the fine structure constant):
The Planck mass is special because the Compton wavelength and the Schwarzschild radius for this mass are equal. Their common value is called the Planck length (formula_21). This is a simple case of dimensional analysis: the Schwarzschild radius is proportional to the mass, whereas the Compton wavelength is proportional to the inverse of the mass. The Planck length is written:

</doc>
<doc id="280911" url="https://en.wikipedia.org/wiki?curid=280911" title="Confidence interval">
Confidence interval

In statistics, a confidence interval (CI) is a type of interval estimate of a population parameter. It is an observed interval (i.e., it is calculated from the observations), in principle different from sample to sample, that frequently includes the value of an unobservable parameter of interest if the experiment is repeated. How frequently the observed interval contains the parameter is determined by the confidence level or confidence coefficient. More specifically, the meaning of the term "confidence level" is that, if CI are constructed across many separate data analyses of replicated (and possibly different) experiments, the proportion of such intervals that contain the true value of the parameter will match the given confidence level. Whereas two-sided confidence limits form a confidence interval, their one-sided counterparts are referred to as lower or upper confidence bounds.
Confidence intervals consist of a range of values (interval) that act as good estimates of the unknown population parameter; however, the interval computed from a particular sample does not necessarily include the true value of the parameter. When we say, "we are 99% confident that the true value of the parameter is in our confidence interval", we express that 99% of the hypothetically observed confidence intervals will hold the true value of the parameter. After any particular sample is taken, the population parameter is either in the interval realized or not; it is not a matter of chance. The desired level of confidence is set by the researcher (not determined by data). If a corresponding hypothesis test is performed, the confidence level is the complement of respective level of significance, i.e. a 95% confidence interval reflects a significance level of 0.05. The confidence interval contains the parameter values that, when tested, should not be rejected with the same sample. Greater levels of variance yield larger confidence intervals, and hence less precise estimates of the parameter. Confidence intervals of difference parameters not containing 0 imply that there is a statistically significant difference between the populations.
In applied practice, confidence intervals are typically stated at the 95% confidence level. However, when presented graphically, confidence intervals can be shown at several confidence levels, for example 90%, 95% and 99%.
Certain factors may affect the confidence interval size including size of sample, level of confidence, and population variability. A larger sample size normally will lead to a better estimate of the population parameter.
Confidence intervals were introduced to statistics by Jerzy Neyman in a paper published in 1937.
Conceptual basis.
Introduction.
Interval estimates can be contrasted with point estimates. A point estimate is a single value given as the estimate of a population parameter that is of interest, for example the mean of some quantity. An interval estimate specifies instead a range within which the parameter is estimated to lie. Confidence intervals are commonly reported in tables or graphs along with point estimates of the same parameters, to show the reliability of the estimates.
For example, a confidence interval can be used to describe how reliable survey results are. In a poll of election voting-intentions, the result might be that 40% of respondents intend to vote for a certain party. A 99% confidence interval for the proportion in the whole population having the same intention on the survey might be 30% to 50%. From the same data one may calculate a 90% confidence interval, which in this case might be 37% to 43%. A major factor determining the length of a confidence interval is the size of the sample used in the estimation procedure, for example the number of people taking part in a survey.
Meaning and interpretation.
For users of frequentist methods, various interpretations of a confidence interval can be given (taking the 90% confidence interval as an example in the following).
In each of the above, the following applies: If the true value of the parameter lies outside the 90% confidence interval once it has been calculated, then an event has occurred which had a probability of 10% (or less) of happening by chance.
Misunderstandings.
Confidence intervals are frequently misunderstood, and published studies have shown that even professional scientists often misinterpret them.
Philosophical issues.
The principle behind confidence intervals was formulated to provide an answer to the question raised in statistical inference of how to deal with the uncertainty inherent in results derived from data that are themselves only a randomly selected subset of a population. There are other answers, notably that provided by Bayesian inference in the form of credible intervals. Confidence intervals correspond to a chosen rule for determining the confidence bounds, where this rule is essentially determined before any data are obtained, or before an experiment is done. The rule is defined such that over all possible datasets that might be obtained, there is a high probability ("high" is specifically quantified) that the interval determined by the rule will include the true value of the quantity under consideration. That is a fairly straightforward and reasonable way of specifying a rule for determining uncertainty intervals. The Bayesian approach appears to offer intervals that can, subject to acceptance of an interpretation of "probability" as Bayesian probability, be interpreted as meaning that the specific interval calculated from a given dataset has a certain probability of including the true value, conditional on the data and other information available. The confidence interval approach does not allow this, since in this formulation and at this same stage, both the bounds of the interval and the true values are fixed values and there is no randomness involved.
For example, in the poll example outlined in the introduction, to be 95% confident that the actual number of voters intending to vote for the party in question is between 36% and 44%, should not be interpreted in the common-sense interpretation that there is a 95% probability that the actual number of voters intending to vote for the party in question is between 36% and 44%. The actual meaning of confidence levels and confidence intervals is rather more subtle. In the above case, a correct interpretation would be as follows: If the polling were repeated a large number of times (you could produce a 95% confidence interval for your polling confidence interval), each time generating about a 95% confidence interval from the poll sample, then 95% of the generated intervals would contain the true percentage of voters who intend to vote for the given party. Each time the polling is repeated, a "different" confidence interval is produced; hence, it is not possible to make absolute statements about probabilities for any one given interval. For more information, see the section on meaning and interpretation.
The questions concerning how an interval expressing uncertainty in an estimate might be formulated, and of how such intervals might be interpreted, are not strictly mathematical problems and are philosophically problematic. Mathematics can take over once the basic principles of an approach to 'inference' have been established, but it has only a limited role in saying why one approach should be preferred to another: For example, a confidence level of 95% is often used in the biological sciences, but this is a matter of convention or arbitration. In the physical sciences, a much higher level may be used.
Relationship with other statistical topics.
Statistical hypothesis testing.
Confidence intervals are closely related to statistical significance testing. For example, if for some estimated parameter "θ" one wants to test the null hypothesis that "θ" = 0 against the alternative that "θ" ≠ 0, then this test can be performed by determining whether the confidence interval for "θ" contains 0.
More generally, given the availability of a hypothesis testing procedure that can test the null hypothesis "θ" = "θ"0 against the alternative that "θ" ≠ "θ"0 for any value of "θ"0, then a confidence interval with confidence level "γ" = 1 − "α" can be defined as containing any number "θ"0 for which the corresponding null hypothesis is not rejected at significance level "α".
If the estimates of two parameters (for example, the mean values of a variable in two independent groups) have confidence intervals that do not overlap, then the difference between the two values is more significant than indicated by the individual values of "α". So, this "test" is too conservative and can lead to a result that is more significant than the individual values of "α" would indicate. If two confidence intervals overlap, the two means still may be significantly different. Accordingly, and consistent with the Mantel-Haenszel Chi-squared test, is a proposed fix whereby one reduces the error bounds for the two means by multiplying them by the square root of 1/2 (0.707107) before making the comparison.
While the formulations of the notions of confidence intervals and of statistical hypothesis testing are distinct they are in some senses related and to some extent complementary. While not all confidence intervals are constructed in this way, one general purpose approach to constructing confidence intervals is to define a 100(1 − "α")% confidence interval to consist of all those values "θ"0 for which a test of the hypothesis "θ" = "θ"0 is not rejected at a significance level of 100α%. Such an approach may not always be available since it presupposes the practical availability of an appropriate significance test. Naturally, any assumptions required for the significance test would carry over to the confidence intervals.
It may be convenient to make the general correspondence that parameter values within a confidence interval are equivalent to those values that would not be rejected by a hypothesis test, but this would be dangerous. In many instances the confidence intervals that are quoted are only approximately valid, perhaps derived from "plus or minus twice the standard error", and the implications of this for the supposedly corresponding hypothesis tests are usually unknown.
It is worth noting that the confidence interval for a parameter is not the same as the acceptance region of a test for this parameter, as is sometimes thought. The confidence interval is part of the parameter space, whereas the acceptance region is part of the sample space. For the same reason the confidence level is not the same as the complementary probability of the level of significance.
Confidence region.
Confidence regions generalize the confidence interval concept to deal with multiple quantities. Such regions can indicate not only the extent of likely sampling errors but can also reveal whether (for example) it is the case that if the estimate for one quantity is unreliable then the other is also likely to be unreliable.
Confidence band.
A confidence band is used in statistical analysis to represent the uncertainty in an estimate of a curve or function based on limited or noisy data. Similarly, a prediction band is used to represent the uncertainty about the value of a new data-point on the curve, but subject to noise. Confidence and prediction bands are often used as part of the graphical presentation of results of a regression analysis.
Confidence bands are closely related to confidence intervals, which represent the uncertainty in an estimate of a single numerical value. "As confidence intervals, by construction, only refer to a single point, they are narrower (at this point) than a confidence band which is supposed to hold simultaneously at many points."
Statistical theory.
Definition.
Let "X" be a random sample from a probability distribution with statistical parameters "θ", which is a quantity to be estimated, and "ϕ", representing quantities that are not of immediate interest. A "confidence interval" for the parameter "θ", with confidence level or confidence coefficient "γ", is an interval with random endpoints ("u"("X"), "v"("X")), determined by the pair of random variables "u"("X") and "v"("X"), with the property:
The quantities "ϕ" in which there is no immediate interest are called nuisance parameters, as statistical theory still needs to find some way to deal with them.
The number "γ", with typical values close to but not greater than 1, is sometimes given in the form 1 − "α" (or as a percentage 100%·(1 − "α")), where "α" is a small non-negative number, close to 0.
Here Pr"θ","ϕ" indicates the probability distribution of "X" characterised by ("θ", "ϕ"). An important part of this specification is that the random interval ("u"("X"), "v"("X")) covers the unknown value "θ" with a high probability no matter what the true value of "θ" actually is.
Note that here Pr"θ","ϕ" need not refer to an explicitly given parameterised family of distributions, although it often does. Just as the random variable "X" notionally corresponds to other possible realizations of "x" from the same population or from the same version of reality, the parameters ("θ", "ϕ") indicate that we need to consider other versions of reality in which the distribution of "X" might have different characteristics.
In a specific situation, when "x" is the outcome of the sample "X", the interval ("u"("x"), "v"("x")) is also referred to as a confidence interval for "θ". Note that it is no longer possible to say that the (observed) interval ("u"("x"), "v"("x")) has probability "γ" to contain the parameter "θ". This observed interval is just one realization of all possible intervals for which the probability statement holds.
Approximate confidence intervals.
In many applications, confidence intervals that have exactly the required confidence level are hard to construct. But practically useful intervals can still be found: the rule for constructing the interval may be accepted as providing a confidence interval at level "γ" if
to an acceptable level of approximation. Alternatively, some authors simply require that
which is useful if the probabilities are only partially identified, or imprecise.
Desirable properties.
When applying standard statistical procedures, there will often be standard ways of constructing confidence intervals. These will have been devised so as to meet certain desirable properties, which will hold given that the assumptions on which the procedure rely are true. These desirable properties may be described as: validity, optimality and invariance. Of these "validity" is most important, followed closely by "optimality". "Invariance" may be considered as a property of the method of derivation of a confidence interval rather than of the rule for constructing the interval. In non-standard applications, the same desirable properties would be sought.
Methods of derivation.
For non-standard applications, there are several routes that might be taken to derive a rule for the construction of confidence intervals. Established rules for standard procedures might be justified or explained via several of these routes. Typically a rule for constructing confidence intervals is closely tied to a particular way of finding a point estimate of the quantity being considered.
formula_5
Examples.
Practical example.
A machine fills cups with a liquid, and is supposed to be adjusted so that the content of the cups is 250 g of liquid. As the machine cannot fill every cup with exactly 250.0 g, the content added to individual cups shows some variation, and is considered a random variable X. This variation is assumed to be normally distributed (although this assumption is not necessary for the theory to work) around the desired average of 250 g, with a standard deviation, σ, of 2.5 g. To determine if the machine is adequately calibrated, a sample of "n" = 25 cups of liquid are chosen at random and the cups are weighed. The resulting measured masses of liquid are "X"1, ..., "X"25, a random sample from "X".
To get an impression of the expectation "μ", it is sufficient to give an estimate. The appropriate estimator is the sample mean:
The sample shows actual weights "x"1, ..., "x"25, with mean:
If we take another sample of 25 cups, we could easily expect to find mean values like 250.4 or 251.1 grams. A sample mean value of 280 grams however would be extremely rare if the mean content of the cups is in fact close to 250 grams. There is a whole interval around the observed value 250.2 grams of the sample mean within which, if the whole population mean actually takes a value in this range, the observed data would not be considered particularly unusual. Such an interval is called a confidence interval for the parameter "μ". How do we calculate such an interval? The endpoints of the interval have to be calculated from the sample, so they are statistics, functions of the sample "X"1, ..., "X"25 and hence random variables themselves.
In our case we may determine the endpoints by considering that the sample mean from a normally distributed sample is also normally distributed, with the same expectation "μ", but with a standard error of:
By standardizing, we get a random variable:
dependent on the parameter "μ" to be estimated, but with a standard normal distribution independent of the parameter "μ". Hence it is possible to find numbers −"z" and "z", independent of "μ", between which "Z" lies with probability 1 − α, a measure of how confident we want to be.
We take 1 − α = 0.95, for example. So we have:
The number "z" follows from the cumulative distribution function, in this case the cumulative normal distribution function:
and we get:
In other words, the lower endpoint of the 95% confidence interval is:
and the upper endpoint of the 95% confidence interval is: 
With the values in this example, the confidence interval is:
As the standard deviation of the population σ is known in this case, the distribution of the sample mean formula_16 is a normal distribution with formula_17 the only unknown parameter. In the theoretical example below, the parameter σ is also unknown, which calls for using the Student's t-distribution.
Interpretation.
This might be interpreted as: with probability 0.95 we will find a confidence interval in which the value of parameter "μ" will be between the stochastic endpoints
and
This does not mean there is 0.95 probability that the value of parameter "μ" is in the interval obtained by using the currently computed value of the sample mean,
Instead, every time the measurements are repeated, there will be another value for the mean  of the sample. In 95% of the cases "μ" will be between the endpoints calculated from this mean, but in 5% of the cases it will not be. The actual confidence interval is calculated by entering the measured masses in the formula. Our 0.95 confidence interval becomes:
In other words, the 95% confidence interval is between the lower endpoint 249.22 g and the upper endpoint 251.18 g.
As the desired value 250 of "μ" is within the resulted confidence interval, there is no reason to believe the machine is wrongly calibrated.
The calculated interval has fixed endpoints, where μ might be in between (or not). Thus this event has probability either 0 or 1. One cannot say: "with probability (1 − α) the parameter "μ" lies in the confidence interval." One only knows that by repetition in 100(1 − α) % of the cases, "μ" will be in the calculated interval. In 100α% of the cases however it does not. And unfortunately one does not know in which of the cases this happens. That is (instead of using the term "probability") why one can say: "with confidence level 100(1 − α) %, "μ" lies in the confidence interval."
The maximum error is calculated to be 0.98 since it is the difference between value that we are confident of with upper or lower endpoint.
The figure on the right shows 50 realizations of a confidence interval for a given population mean "μ". If we randomly choose one realization, the probability is 95% we end up having chosen an interval that contains the parameter; however we may be unlucky and have picked the wrong one. We will never know; we are stuck with our interval.
Theoretical example.
Suppose {"X"1, ..., "X""n"} is an independent sample from a normally distributed population with unknown (parameters) mean "μ" and variance σ2. Let
Where is the sample mean, and "S2" is the sample variance. Then
has a Student's t-distribution with "n" − 1 degrees of freedom. Note that the distribution of "T" does not depend on the values of the unobservable parameters "μ" and "σ"2; i.e., it is a pivotal quantity. Suppose we wanted to calculate a 95% confidence interval for "μ". Then, denoting "c" as the 97.5th percentile of this distribution,
Consequently
and we have a theoretical (stochastic) 95% confidence interval for "μ".
After observing the sample we find values for and "s" for "S", from which we compute the confidence interval
an interval with fixed numbers as endpoints, of which we can no longer say there is a certain probability it contains the parameter "μ"; either "μ" is in this interval or isn't.
Alternatives and critiques.
Confidence intervals are one method of interval estimation, and the most widely used in frequentist statistics.
An analogous concept in Bayesian statistics is credible intervals, while an alternative frequentist method is that of prediction intervals which, rather than estimating "parameters," estimate the outcome of "future" samples. For other approaches to expressing uncertainty using intervals, see interval estimation.
Comparison to prediction intervals.
A prediction interval for a random variable is defined similarly to a confidence interval for a statistical parameter. Consider an additional random variable "Y" which may or may not be statistically dependent on the random sample "X". Then ("u"("X"), "v"("X")) provides a prediction interval for the as-yet-to-be observed value "y" of "Y" if
Here Pr"θ","φ" indicates the joint probability distribution of the random variables ("X", "Y"), where this distribution depends on the statistical parameters ("θ", "φ").
Comparison to Bayesian interval estimates.
A Bayesian interval estimate is called a credible interval. Using much of the same notation as above, the definition of a credible interval for the unknown true value of "θ" is, for a given "γ",
Here Θ is used to emphasize that the unknown value of "θ" is being treated as a random variable. The definitions of the two types of intervals may be compared as follows.
Note that the treatment of the nuisance parameters above is often omitted from discussions comparing confidence and credible intervals but it is markedly different between the two cases.
In some simple standard cases, the intervals produced as confidence and credible intervals from the same data set can be identical. They are very different if informative prior information is included in the Bayesian analysis; and may be very different for some parts of the space of possible data even if the Bayesian prior is relatively uninformative.
Users of Bayesian methods, if they produced an interval estimate, would in contrast to confidence intervals, want to say ""My degree of "belief" that the parameter is in fact in this interval is 90%,"" while users of prediction intervals would instead say "I "predict" that the "next sample" will fall in this interval 90% of the time."
There is disagreement about which of these methods produces the most useful results: the mathematics of the computations are rarely in question–confidence intervals being based on sampling distributions, credible intervals being based on Bayes' theorem–but the application of these methods, the utility and interpretation of the produced statistics, is debated.
Confidence intervals for proportions and related quantities.
An approximate confidence interval for a population mean can be constructed for random variables that are not normally distributed in the population, relying on the central limit theorem, if the sample sizes and counts are big enough. The formulae are identical to the case above (where the sample mean is actually normally distributed about the population mean). The approximation will be quite good with only a few dozen observations in the sample if the probability distribution of the random variable is not too different from the normal distribution (e.g. its cumulative distribution function does not have any discontinuities and its skewness is moderate).
One type of sample mean is the mean of an indicator variable, which takes on the value 1 for true and the value 0 for false. The mean of such a variable is equal to the proportion that have the variable equal to one (both in the population and in any sample). This is a useful property of indicator variables, especially for hypothesis testing. To apply the central limit theorem, one must use a large enough sample. A rough rule of thumb is that one should see at least 5 cases in which the indicator is 1 and at least 5 in which it is 0. Confidence intervals constructed using the above formulae may include negative numbers or numbers greater than 1, but proportions obviously cannot be negative or exceed 1. Additionally, sample proportions can only take on a finite number of values, so the central limit theorem and the normal distribution are not the best tools for building a confidence interval. See "Binomial proportion confidence interval" for better methods which are specific to this case.

</doc>
<doc id="39330" url="https://en.wikipedia.org/wiki?curid=39330" title="Congruence (geometry)">
Congruence (geometry)

In geometry, two figures or objects are congruent if they have the same shape and size, or if one has the same shape and size as the mirror image of the other. More formally, two sets of points are called congruent if, and only if, one can be transformed into the other by an isometry, i.e., a combination of rigid motions, namely a translation, a rotation, and a reflection. This means that either object can be repositioned and reflected (but not resized) so as to coincide precisely with the other object. So two distinct plane figures on a piece of paper are congruent if we can cut them out and then match them up completely. Turning the paper over is permitted.
In elementary geometry the word "congruent" is often used as follows. The word "equal" is often used in place of "congruent" for these objects.
In this sense, "two plane figures are congruent" implies that their corresponding characteristics are "congruent" or "equal" including not just their corresponding sides and angles, but also their corresponding diagonals, perimeters and areas.
The related concept of similarity applies if the objects differ in size but not in shape.
Determining congruence of polygons.
For two polygons to be congruent, they must have an equal number of sides (and hence an equal number—the same number—of vertices). Two polygons with "n" sides are congruent if and only if they each have numerically identical sequences (even if clockwise for one polygon and counterclockwise for the other) side-angle-side-angle-... for "n" sides and "n" angles.
Congruence of polygons can be established graphically as follows:
If at any time the step cannot be completed, the polygons are not congruent.
Congruence of triangles.
Two triangles are congruent if their corresponding sides are equal in length, in which case their corresponding angles are equal in size.
If triangle ABC is congruent to triangle DEF, the relationship can be written mathematically as:
In many cases it is sufficient to establish the equality of three corresponding parts and use one of the following results to deduce the congruence of the two triangles.
Determining congruence.
Sufficient evidence for congruence between two triangles in Euclidean space can be shown through the following comparisons:
Side-side-angle.
The SSA condition (Side-Side-Angle) which specifies two sides and a non-included angle (also known as ASS, or Angle-Side-Side) does not by itself prove congruence. In order to show congruence, additional information is required such as the measure of the corresponding angles and in some cases the lengths of the two pairs of corresponding sides. There are a few possible cases:
If two triangles satisfy the SSA condition and the length of the side opposite the angle is greater than or equal to the length of the adjacent side (SsA, or long side-short side-angle), then the two triangles are congruent. The opposite side is sometimes longer when the corresponding angles are acute, but it is "always" longer when the corresponding angles are right or obtuse. Where the angle is a right angle, also known as the Hypotenuse-Leg (HL) postulate or the Right-angle-Hypotenuse-Side (RHS) condition, the third side can be calculated using the Pythagorean Theorem thus allowing the SSS postulate to be applied.
If two triangles satisfy the SSA condition and the corresponding angles are acute and the length of the side opposite the angle is equal to the length of the adjacent side multiplied by the sine of the angle, then the two triangles are congruent.
If two triangles satisfy the SSA condition and the corresponding angles are acute and the length of the side opposite the angle is greater than the length of the adjacent side multiplied by the sine of the angle (but less than the length of the adjacent side), then the two triangles cannot be shown to be congruent. This is the ambiguous case and two different triangles can be formed from the given information, but further information distinguishing them can lead to a proof of congruence.
Angle-angle-angle.
In Euclidean geometry, AAA (Angle-Angle-Angle) (or just AA, since in Euclidean geometry the angles of a triangle add up to 180°) does not provide information regarding the size of the two triangles and hence proves only similarity and not congruence in Euclidean space.
However, in spherical geometry and hyperbolic geometry (where the sum of the angles of a triangle varies with size) AAA is sufficient for congruence on a given curvature of surface.
Definition of congruence in analytic geometry.
In a Euclidean system, congruence is fundamental; it is the counterpart of equality for numbers. In analytic geometry, congruence may be defined intuitively thus: two mappings of figures onto one Cartesian coordinate system are congruent if and only if, for "any" two points in the first mapping, the Euclidean distance between them is equal to the Euclidean distance between the corresponding points in the second mapping.
A more formal definition states that two subsets "A" and "B" of Euclidean space R"n" are called congruent if there exists an isometry "f" : R"n" → R"n" (an element of the Euclidean group "E"("n")) with "f"("A") = "B". Congruence is an equivalence relation.
Congruent conic sections.
Two conic sections are congruent if their eccentricities and one other distinct parameter characterizing them are equal. Their eccentricities establish their shapes, equality of which is sufficient to establish similarity, and the second parameter then establishes size. Since two circles, parabolas, or rectangular hyperbolas always have the same eccentricity (specifically 0 in the case of circles, 1 in the case of parabolas, and formula_2 in the case of rectangular hyperbolas), two circles, parabolas, or rectangular hyperbolas need to have only one other common parameter value, establishing their size, for them to be congruent.
Congruent polyhedra.
For two polyhedra with the same number "E" of edges, the same number of faces, and the same number of sides on corresponding faces, there exists a set of at most "E" measurements that can establish whether or not the polyhedra are congruent. For cubes, which have 12 edges, only 9 measurements are necessary.
Congruent triangles on a sphere.
As with plane triangles, on a sphere two triangles sharing the same sequence of angle-side-angle (ASA) are necessarily congruent (that is, they have three identical sides and three identical angles). This can be seen as follows: One can situate one of the vertices with a given angle at the south pole and run the side with given length up the prime meridian. Knowing both angles at either end of the segment of fixed length ensures that the other two sides emanate with a uniquely determined trajectory, and thus will meet each other at a uniquely determined point; thus ASA is valid. 
The congruence theorems side-angle-side (SAS) and side-side-side (SSS) also hold on a sphere; in addition, if two spherical triangles have an identical angle-angle-angle (AAA) sequence, they are congruent (unlike for plane triangles).
The plane-triangle congruence theorem angle-angle-side (AAS) does not hold for spherical triangles. As in plane geometry, side-side-angle (SSA) does not imply congruence.

</doc>
<doc id="19008673" url="https://en.wikipedia.org/wiki?curid=19008673" title="Conic section">
Conic section

In mathematics, a conic section (or just conic) is a curve obtained as the intersection of a cone (more precisely, a right circular conical surface) with a plane. In analytic geometry, a conic may be defined as a plane algebraic curve of degree 2, and as a quadric of dimension 1. There are a number of other geometric definitions possible. One of the most useful, in that it involves only the plane, is that a non-circular conic consists of those points whose distances to some point, called a "focus", and some line, called a "directrix", are in a fixed ratio, called the "eccentricity".
Traditionally, the three types of conic section are the hyperbola, the parabola, and the ellipse. The circle is a special case of the ellipse, and is of sufficient interest in its own right that it is sometimes called the fourth type of conic section. The type of a conic corresponds to its eccentricity, those with eccentricity less than 1 being ellipses, those with eccentricity equal to 1 being parabolas, and those with eccentricity greater than 1 being hyperbolas. In the focus-directrix definition of a conic the circle is a limiting case with eccentricity 0. In modern geometry certain degenerate cases, such as the union of two lines, are included as conics as well.
The conic sections have been named and studied at least since 200 BC, when Apollonius of Perga undertook a systematic study of their properties.
History.
Menaechmus and early works.
It is believed that the first definition of a conic section is due to Menaechmus (died 320 BC). His work did not survive and is only known through secondary accounts. The definition used at that time differs from the one commonly used today in that it requires the plane cutting the cone to be perpendicular to one of the lines, (a generatrix), that generates the cone as a surface of revolution. Thus the shape of the conic is determined by the angle formed at the vertex of the cone (between two opposite generatrices): If the angle is acute then the conic is an ellipse; if the angle is right then the conic is a parabola; and if the angle is obtuse then the conic is a hyperbola. Note that the circle cannot be defined this way and was not considered a conic at this time.
Euclid ( fl. 300 BC ) is said to have written four books on conics but these were lost as well. Archimedes (died c. 212 BC) is known to have studied conics, having determined the area bounded by a parabola and an ellipse. The only part of this work to survive is a book on the solids of revolution of conics.
Apollonius of Perga.
The greatest progress in the study of conics by the ancient Greeks is due to Apollonius of Perga (died c. 190 BC), whose eight-volume "Conic Sections" or "Conics" summarized and greatly extended existing knowledge. Apollonius's major innovation was to characterize a conic using properties within the plane and intrinsic to the curve; this greatly simplified analysis. With this tool, it was now possible to show that any plane cutting the cone, regardless of its angle, will produce a conic according to the earlier definition, leading to the definition commonly used today.
Pappus of Alexandria (died c. 350 CE) is credited with discovering the importance of the concept of a conic's focus, and with the discovery of the related concept of a directrix.
Al-Kuhi.
An instrument for drawing conic sections was first described in 1000 CE by the Islamic mathematician Al-Kuhi.
Omar Khayyám.
Apollonius's work was translated into Arabic (the technical language of the time) and much of his work only survives through the Arabic version. Persians found applications to the theory; the most notable of these was the Persian mathematician and poet Omar Khayyám who used conic sections to solve algebraic equations.
Europe.
Johannes Kepler extended the theory of conics through the "principle of continuity", a precursor to the concept of limits.
Girard Desargues and Blaise Pascal developed a theory of conics using an early form of projective geometry and this helped to provide impetus for the study of this new field. In particular, Pascal discovered a theorem known as the hexagrammum mysticum from which many other properties of conics can be deduced.
Meanwhile, René Descartes applied his newly discovered Analytic geometry to the study of conics. This had the effect of reducing the geometrical problems of conics to problems in algebra.
Features.
The three types of conics are the ellipse, parabola, and hyperbola. The circle can be considered as a fourth type (as it was by Apollonius) or as a kind of ellipse. The circle and the ellipse arise when the intersection of cone and plane is a closed curve. The circle is obtained when the cutting plane is parallel to the plane of the generating circle of the cone – for a right cone as in the picture at the top of the page this means that the cutting plane is perpendicular to the symmetry axis of the cone. If the cutting plane is parallel to exactly one generating line of the cone, then the conic is unbounded and is called a parabola. In the remaining case, the figure is a hyperbola. In this case, the plane will intersect "both" halves ("nappes") of the cone, producing two separate unbounded curves.
Various parameters are associated with a conic section, as shown in the following table. (For the ellipse, the table gives the case of "a">"b", for which the major axis is horizontal; for the reverse case, interchange the symbols "a" and "b". For the hyperbola the east-west opening case is given. In all cases, "a" and "b" are positive.)
The non-circular conic sections are exactly those curves that, for a point "F", a line "L" not containing "F" and a non-negative number "e", are the locus of points whose distance to "F" equals "e" times their distance to "L". "F" is called the focus, "L" the directrix, and "e" the eccentricity.
The linear eccentricity ("c") is the distance between the center and the focus (or one of the two foci).
The latus rectum (2"ℓ") is the chord parallel to the directrix and passing through the focus (or one of the two foci).
The semi-latus rectum ("ℓ") is half the latus rectum.
The focal parameter ("p") is the distance from the focus (or one of the two foci) to the directrix.
The following relations hold:
Construction.
There are many methods to construct a conic. One of them, which is useful in engineering applications, is the parallelogram method, where a conic is constructed point by point by means of connecting certain equally spaced points on a horizontal line and a vertical line (see Ellipse, Hyperbola).
Properties.
Just as two (distinct) points determine a line, five points determine a conic. Formally, given any five points in the plane in general linear position, meaning no three collinear, there is a unique conic passing through them, which will be non-degenerate; this is true over both the affine plane and projective plane. Indeed, given any five points there is a conic passing through them, but if three of the points are collinear the conic will be degenerate (reducible, because it contains a line), and may not be unique; see further discussion.
Four points in the plane in general linear position determine a unique conic passing through the first three points and having the fourth point as its center. Thus knowing the center is equivalent to knowing two points on the conic for the purpose of determining the curve.
Furthermore, a conic is determined by any combination of "k" points in general position that it passes through and 5–"k" lines that are tangent to it, for 0≤"k"≤5.
Irreducible conic sections are always "smooth". This is important for many applications, such as aerodynamics, where a smooth surface is required to ensure laminar flow and to prevent turbulence.
Intersection at infinity.
An algebro-geometrically intrinsic form of this classification is by the intersection of the conic with the line at infinity, which gives further insight into their geometry:
Degenerate cases.
There are five degenerate cases: three in which the plane passes through apex of the cone, and three that arise when the cone itself degenerates to a cylinder (a doubled line can occur in both cases).
When the plane passes through the apex, the resulting conic is always degenerate, and is either: a point (when the angle between the plane and the axis of the cone is larger than tangential); a straight line (when the plane is tangential to the surface of the cone); or a pair of intersecting lines (when the angle is smaller than the tangential). These correspond respectively to degeneration of an ellipse, parabola, and a hyperbola, which are characterized in the same way by angle. The straight line is more precisely a "double" line (a line with multiplicity 2) because the plane is tangent to the cone, and thus the intersection should be counted twice.
Where the cone is a cylinder, i.e. with the vertex at infinity, cylindric sections are obtained; this corresponds to the apex being at infinity. Cylindrical sections are ellipses (or circles), unless the plane is vertical (which corresponds to passing through the apex at infinity), in which case three degenerate cases occur: two parallel lines, known as a ribbon (corresponding to an ellipse with one axis infinite and the other axis real and non-zero, the distance between the lines), a double line (an ellipse with one infinite axis and one axis zero), and no intersection (an ellipse with one infinite axis and the other axis imaginary).
Eccentricity, focus and directrix.
The four defining conditions above can be combined into one condition that depends on a fixed point formula_3 (the focus), a line formula_4 (the directrix) not containing formula_3 and a nonnegative real number formula_6 (the "eccentricity"). The corresponding conic section consists of the locus of all points whose distance to formula_3 equals formula_6 times their distance to formula_4. For formula_10 we obtain an ellipse, for formula_11 a parabola, and for formula_12 a hyperbola.
For an ellipse and a hyperbola, two focus-directrix combinations can be taken, each giving the same full ellipse or hyperbola. The distance from the center to the directrix is formula_13, where formula_14 is the semi-major axis of the ellipse, or the distance from the center to the tops of the hyperbola. The distance from the center to a focus is formula_15.
The circle is a limiting case and is not defined by a focus and directrix "in the plane". However, if one were to consider the directrix to be infinitely far from the center (the line at infinity), then by taking the eccentricity to be formula_16 a circle will have the focus-directrix property, but is still not defined by that property. One must be careful in this situation to correctly use the definition of eccentricity as the ratio of the distance of a point on the circle to the focus (length of a radius) to the distance of that point to the directrix (this distance is infinite) which gives the limiting value of zero.
The eccentricity of a conic section is thus a measure of how far it deviates from being circular.
For a given formula_17, the closer formula_6 is to 1, the smaller is the semi-minor axis.
Generalizations.
Conics may be defined over other fields, and may also be classified in the projective plane rather than in the affine plane.
Over the complex numbers ellipses and hyperbolas are not distinct, since −1 is a square; precisely, the ellipse formula_19 becomes a hyperbola under the substitution formula_20 geometrically a complex rotation, yielding formula_21 – a hyperbola is simply an ellipse with an imaginary axis length. Thus there is a 2-way classification: ellipse/hyperbola and parabola. Geometrically, this corresponds to intersecting the line at infinity in either 2 distinct points (corresponding to two asymptotes) or in 1 double point (corresponding to the axis of a parabola), and thus the real hyperbola is a more suggestive image for the complex ellipse/hyperbola, as it also has 2 (real) intersections with the line at infinity.
In projective space, over any division ring, but in particular over either the real or complex numbers, all non-degenerate conics are equivalent, and thus in projective geometry one simply speaks of "a conic" without specifying a type, as type is not meaningful. Geometrically, the line at infinity is no longer special (distinguished), so while some conics intersect the line at infinity differently, this can be changed by a projective transformation – pulling an ellipse out to infinity or pushing a parabola off infinity to an ellipse or a hyperbola.
A generalization of a non degenerate conic in a projective plane is an oval. An oval is a point set that has the following properties, which are held by conics: 1) any line intersects an oval in none, one or two points, 2) at any point of the oval there exists a unique tangent line.
In other areas of mathematics.
The classification into elliptic, parabolic, and hyperbolic is pervasive in mathematics, and often divides a field into sharply distinct subfields. The classification mostly arises due to the presence of a quadratic form (in two variables this corresponds to the associated discriminant), but can also correspond to eccentricity.
Quadratic form classifications:
Eccentricity classifications include:
Cartesian coordinates.
In the Cartesian coordinate system, the graph of a quadratic equation in two variables is always a conic section (though it may be degenerate), and all conic sections arise in this way. The equation will be of the form
As scaling all six constants yields the same locus of zeros, one can consider conics as points in the five-dimensional projective space formula_34
Discriminant classification.
The conic sections described by this equation can be classified with the discriminant
If the conic is non-degenerate, then:
To distinguish the degenerate cases from the non-degenerate cases, let "∆" be the determinant of the 3×3 matrix ["A", "B"/2, "D"/2 ; "B"/2, "C", "E"/2 ; "D"/2, "E"/2, "F" ]: that is, "∆" = ("AC" - "B"2/4)"F" + "BED"/4 - "CD"2/4 - "AE"2/4. Then the conic section is non-degenerate if and only if "∆" ≠ 0. If "∆"=0 we have a point ellipse, two parallel lines (possibly coinciding with each other) in the case of a parabola, or two intersecting lines in the case of a hyperbola.
Moreover, in the case of a non-degenerate ellipse (with formula_36 and "∆"≠0), we have a real ellipse if "C∆" < 0 but an imaginary ellipse if "C∆" > 0. An example is formula_43, which has no real-valued solutions.
Note that A and B are polynomial coefficients, not the lengths of semi-major/minor axis as defined in some sources.
Matrix notation.
The above equation can be written in matrix notation as
The type of conic section is solely determined by the determinant of middle matrix: if it is positive, zero, or negative then the conic is an ellipse, parabola, or hyperbola respectively (see geometric meaning of a quadratic form). If both the eigenvalues of the middle matrix are non-zero (i.e. it is an ellipse or a hyperbola), we can do a transformation of variables to obtain
formula_45
where a,c, and G satisfy formula_46 and formula_47.
The quadratic can also be written as
If the determinant of this 3×3 matrix is non-zero, the conic section is not degenerate. If the determinant equals zero, the conic is a degenerate parabola (two parallel or coinciding lines), a degenerate ellipse (a point ellipse), or a degenerate hyperbola (two intersecting lines).
Note that in the centered equation with constant term "G", "G" equals minus one times the ratio of the 3×3 determinant to the 2×2 determinant.
As slice of quadratic form.
The equation
can be rearranged by taking the affine linear part to the other side, yielding
In this form, a conic section is realized exactly as the intersection of the graph of the quadratic form formula_51 and the plane formula_52 Degenerate conics correspond to degenerate intersections, such as taking slices such as formula_53 of a positive-definite form.
Eccentricity in terms of parameters of the quadratic form.
When the conic section is written algebraically as
the eccentricity can be written as a function of the parameters of the quadratic equation. If 4"AC" = "B"2 the conic is a parabola and its eccentricity equals 1 (if it is non-degenerate). Otherwise, assuming the equation represents either a non-degenerate hyperbola or a non-degenerate, non-imaginary ellipse, the eccentricity is given by
where η = 1 if the determinant of the 3×3 matrix is negative and η = −1 if that determinant is positive.
Standard form.
Through a change of coordinates (a rotation of axes and a translation of axes) these equations can be put into standard forms:
The first four of these forms are symmetric about both the -axis and -axis (for the circle, ellipse and hyperbola), or about either but not both (for the parabola). The rectangular hyperbola, however, is instead symmetric about the lines and .
These standard forms can be written as parametric equations,
Invariants of conics.
The trace and determinant of formula_56 are both invariant with respect to both rotation of axes and translation of the plane (movement of the origin).
The constant term "F" is invariant under rotation only.
Modified form.
For some practical applications, it is important to re-arrange the standard form so that the focal-point can be placed at the origin. The mathematical formulation for a general conic section, with the other focus if any placed at a positive value (for an ellipse) or a negative value (for a hyperbola) on the horizontal axis, is then given in the polar form by
and in the Cartesian form by
From the above equation, the linear eccentricity ("c") is given by
formula_59.
From the general equations given above, different conic sections can be represented as shown below:
Homogeneous coordinates.
In homogeneous coordinates a conic section can be represented as:
Or in matrix notation
The matrix formula_66 is called "the matrix of the conic section".
formula_67 is called the determinant of the conic section. If Δ = 0 then the "conic section" is said to be "degenerate"; this means that the conic section is either a union of two straight lines, a repeated line, a point or the empty set.
For example, the conic section formula_68 reduces to the union of two lines:
Similarly, a conic section sometimes reduces to a (single) repeated line:
formula_71 is called the discriminant of the conic section. If δ = 0 then the "conic section" is a parabola, if δ < 0, it is an hyperbola and if δ > 0, it is an ellipse. A conic section is a circle if δ > 0 and A1 = A2 and B1 = 0, it is an rectangular hyperbola if δ < 0 and A1 = −A2. It can be proven that in the complex projective plane CP2 two conic sections have four points in common (if one accounts for multiplicity), so there are never more than 4 intersection points and there is always one "intersection point" (possibilities: four distinct intersection points, two singular intersection points and one double intersection points, two double intersection points, one singular intersection point and 1 with multiplicity 3, 1 intersection point with multiplicity 4). If there exists at least one intersection point with multiplicity > 1, then the two conic sections are said to be tangent. If there is only one intersection point, which has multiplicity 4, the two conic sections are said to be osculating.
Furthermore each straight line intersects each conic section twice. If the intersection point is double, the line is said to be tangent and it is called the tangent line.
Because every straight line intersects a conic section twice, each conic section has two points at infinity (the intersection points with the line at infinity). If these points are real, the conic section must be a hyperbola, if they are imaginary conjugated, the conic section must be an ellipse, if the conic section has one double point at infinity it is a parabola. If the points at infinity are (1,i,0) and (1,-i,0), the conic section is a circle (see circular points at infinity). If a conic section has one real and one imaginary point at infinity or it has two imaginary points that are not conjugated then it not a real conic section (its coefficients are complex).
Polar coordinates.
In polar coordinates, a conic section with one focus at the origin and, if any, the other at a negative value (for an ellipse) or a positive value (for a hyperbola) on the "x"-axis, is given by the equation
where "e" is the eccentricity and "l" is the semi-latus rectum (see above).
As above, for "e" = 0, we have a circle, for 0 < "e" < 1 we obtain an ellipse, for "e" = 1 a parabola, and for "e" > 1 a hyperbola.
Pencil of conics.
A (non-degenerate) conic is completely determined by five points in general position (no three collinear) in a plane and the system of conics which pass through a fixed set of four points (again in a plane and no three collinear) is called a pencil of conics. The four common points are called the "base points" of the pencil. Through any point other than a base point, there passes a single conic of the pencil. This concept generalizes a pencil of circles.
In a projective plane defined over an algebraically closed field any two conics meet in four points (counted with multiplicity) and so, determine the pencil of conics based on these four points. Furthermore, the four base points determine three line pairs (degenerate conics through the base points, each line of the pair containing exactly two base points) and so each pencil of conics will contain at most three degenerate conics.
A pencil of conics can represented algebraically in the following way. Let "C"1 and "C"2 be two distinct conics in a projective plane defined over an algebraically closed field "K". For every pair λ, μ of elements of "K", not both zero, the expression:
represents a conic in the pencil determined by "C"1 and "C"2. This symbolic representation can be made concrete with a slight abuse of notation (using the same notation to denote the object as well as the equation defining the object.) Thinking of "C"1, say, as a ternary quadratic form, then "C"1 = 0 is the equation of the "conic "C"1". Another concrete realization would be obtained by thinking of "C"1 as the 3×3 symmetric matrix which represents it. If "C"1 and "C"2 have such concrete realizations then every member of the above pencil will as well. Since the setting uses homogeneous coordinates in a projective plane, two concrete representations (either equations or matrices) give the same conic if they differ by a non-zero multiplicative constant.
Intersecting two conics.
The solutions to a system of two second degree equations in two variables may be viewed as the coordinates of the points of intersection of two generic conic sections.
In particular two conics may possess none, two or four possibly coincident intersection points.
An efficient method of locating these solutions exploits the homogeneous matrix representation of conic sections, i.e. a 3x3 symmetric matrix which depends on six parameters.
The procedure to locate the intersection points follows these steps, where the conics are represented by matrices:
Applications.
Conic sections are important in astronomy: the orbits of two massive objects that interact according to Newton's law of universal gravitation are conic sections if their common center of mass is considered to be at rest. If they are bound together, they will both trace out ellipses; if they are moving apart, they will both follow parabolas or hyperbolas. See two-body problem.
In projective geometry, the conic sections in the projective plane are equivalent to each other up to projective transformations.
For specific applications of each type of conic section, see the articles circle, ellipse, parabola, and hyperbola.
For certain fossils in paleontology, understanding conic sections can help understand the three-dimensional shape of certain organisms.

</doc>
<doc id="217548" url="https://en.wikipedia.org/wiki?curid=217548" title="Conjugate transpose">
Conjugate transpose

In mathematics, the conjugate transpose or Hermitian transpose of an "m"-by-"n" matrix ' with complex entries is the "n"-by-"m" matrix '* obtained from "" by taking the transpose and then taking the complex conjugate of each entry (i.e., negating their imaginary parts but not their real parts). The conjugate transpose is formally defined by
where the subscripts denote the "i","j"-th entry, for 1 ≤ "i" ≤ "n" and 1 ≤ "j" ≤ "m", and the overbar denotes a scalar complex conjugate. (The complex conjugate of formula_2, where "a" and "b" are reals, is formula_3.)
This definition can also be written as
where formula_5 denotes the transpose and formula_6 denotes the matrix with complex conjugated entries.
Other names for the conjugate transpose of a matrix are Hermitian conjugate, bedaggered matrix, adjoint matrix or transjugate. The conjugate transpose of a matrix " can be denoted by any of these symbols:
In some contexts, formula_7 denotes the matrix with complex conjugated entries, and the conjugate transpose is then denoted by formula_12 or formula_13.
Example.
If
then
Basic remarks.
A square matrix with entries formula_16 is called
Even if is not square, the two matrices and are both Hermitian and in fact positive semi-definite matrices.
The conjugate transpose "adjoint" matrix should not be confused with the adjugate , which is also sometimes called "adjoint".
Finding the conjugate transpose of a matrix with real entries reduces to finding the transpose of , as the conjugate of a real number is the number itself.
Motivation.
The conjugate transpose can be motivated by noting that complex numbers can be usefully represented by 2×2 real matrices, obeying matrix addition and multiplication:
That is, denoting each "complex" number "z" by the "real" 2×2 matrix of the linear transformation on the Argand diagram (viewed as the "real" vector space formula_20) affected by complex "z"-multiplication on formula_21.
An "m"-by-"n" matrix of complex numbers could therefore equally well be represented by a "2m"-by-"2n" matrix of real numbers. The conjugate transpose therefore arises very naturally as the result of simply transposing such a matrix, when viewed back again as "n"-by-"m" matrix made up of complex numbers.
Generalizations.
The last property given above shows that if one views as a linear transformation from Euclidean Hilbert space to , then the matrix corresponds to the adjoint operator of . The concept of adjoint operators between Hilbert spaces can thus be seen as a generalization of the conjugate transpose of matrices with respect to an orthonormal basis.
Another generalization is available: suppose is a linear map from a complex vector space to another, , then the complex conjugate linear map as well as the transposed linear map are defined, and we may thus take the conjugate transpose of to be the complex conjugate of the transpose of . It maps the conjugate dual of to the conjugate dual of .

</doc>
<doc id="6956" url="https://en.wikipedia.org/wiki?curid=6956" title="Conservation law">
Conservation law

In physics, a conservation law states that a particular measurable property of an isolated physical system does not change as the system evolves over time. Exact conservation laws include conservation of energy, conservation of linear momentum, conservation of angular momentum, and conservation of electric charge. There are also many approximate conservation laws, which apply to such quantities as mass, parity, lepton number, baryon number, strangeness, hypercharge, etc. These quantities are conserved in certain classes of physics processes, but not in all.
A local conservation law is usually expressed mathematically as a continuity equation, a partial differential equation which gives a relation between the amount of the quantity and the "transport" of that quantity. It states that the amount of the conserved quantity at a point or within a volume can only change by the amount of the quantity which flows in or out of the volume.
From Noether's theorem, each conservation law is associated with a symmetry in the underlying physics.
Conservation laws as fundamental laws of nature.
Conservation laws are fundamental to our understanding of the physical world, in that they describe which processes can or cannot occur in nature. For example, the conservation law of energy states that the total quantity of energy in an isolated system does not change, though it may change form. In general, the total quantity of the property governed by that law remains unchanged during physical processes. With respect to classical physics, conservation laws include conservation of energy, mass (or matter), linear momentum, angular momentum, and electric charge. With respect to particle physics, particles cannot be created or destroyed except in pairs, where one is ordinary and the other is an antiparticle. With respect to symmetries and invariance principles, three special conservation laws have been described, associated with inversion or reversal of space, time, and charge.
Conservation laws are considered to be fundamental laws of nature, with broad application in physics, as well as in other fields such as chemistry, biology, geology, and engineering.
Most conservation laws are exact, or absolute, in the sense that they apply to all possible processes. Some conservation laws are partial, in that they hold for some processes but not for others.
One particularly important result concerning conservation laws is Noether's theorem, which states that there is a one-to-one correspondence between each one of them and a differentiable symmetry in the system. For example, the conservation of energy follows from the time-invariance of physical systems, and the fact that physical systems behave the same regardless of how they are oriented in space gives rise to the conservation of angular momentum.
Exact laws.
A partial listing of physical conservation equations due to symmetry that are said to be exact laws, or more precisely "have never been [proven to be] violated:"
Approximate laws.
There are also approximate conservation laws. These are approximately true in particular situations, such as low speeds, short time scales, or certain interactions.
Differential forms.
In continuum mechanics, the most general form of an exact conservation law is given by a continuity equation. For example, conservation of electric charge "q" is
where ∇⋅ is the divergence operator, "ρ" is the density of "q" (amount per unit volume), j is the flux of "q" (amount crossing a unit area in unit time), and "t" is time.
If we assume that the motion u of the charge is a continuous function of position and time, then
In one space dimension this can be put into the form of a homogeneous first-order quasilinear hyperbolic equation:
where the dependent variable "y" is called the "density" of a "conserved quantity", and "A(y)" is called the "current jacobian", and the subscript notation for partial derivatives has been employed. The more general inhomogeneous case:
is not a conservation equation but the general kind of balance equation describing a dissipative system. The dependent variable "y" is called a "nonconserved quantity", and the inhomogeneous term "s(y,x,t)" is the-"source", or dissipation. For example balance equations of this kind are the momentum and energy Navier-Stokes equations, or the entropy balance for a general isolated system.
In the one-dimensional space a conservation equation is a first-order quasilinear hyperbolic equation that can be put into the "advection" form:
where the dependent variable "y(x,t)" is called the density of the "conserved" (scalar) quantity (c.q.(d.) = conserved quantity (density)), and "a(y)" is called the current coefficient, usually corresponding to the partial derivative in the conserved quantity of a current density (c.d.) of the conserved quantity "j(y)":
In this case since the chain rule applies:
the conservation equation can be put into the current density form:
In a space with more than one dimension the former definition can be extended to an equation that can be put into the form:
where the "conserved quantity" is "y(r,t)", "formula_11" denotes the scalar product, "∇" is the nabla operator, here indicating a gradient, and "a(y)" is a vector of current coefficients, analogously corresponding to the divergence of a vector c.d. associated to the c.q. j(y):
This is the case for the continuity equation:
Here the conserved quantity is the mass, with density "ρ"(r,t) and current density "ρ"u, identical to the momentum density, while u(r,t) is the flow velocity.
In the general case a conservation equation can be also a system of this kind of equations (a vector equation) in the form:
where y is called the "conserved" (vector) quantity, ∇ y is its gradient, 0 is the zero vector, and A(y) is called the Jacobian of the current density. In fact as in the former scalar case, also in the vector case A(y) usually corresponding to the Jacobian of a current density matrix J(y):
and the conservation equation can be put into the form:
For example this the case for Euler equations (fluid dynamics). In the simple incompressible case they are:
where:
It can be shown that the conserved (vector) quantity and the c.d. matrix for these equations are respectively:
where "formula_19" denotes the outer product.
Integral and weak forms.
Conservation equations can be also expressed in integral form: the advantage of the latter is substantially that it requires less smoothness of the solution, which paves the way to weak form, extending the class of admissible solutions to include discontinuous solutions. By integrating in any space-time domain the current density form in 1-D space:
and by using Green's theorem, the integral form is:
In a similar fashion, for the scalar multidimensional space, the integral form is:
where the line integration is performed along the boundary of the domain, in an anticlock-wise manner.
Moreover, by defining a test function φ(r,t) continuously differentiable both in time and space with compact support, the weak form can be obtained pivoting on the initial condition. In 1-D space it is:
Note that in the weak form all the partial derivatives of the density and current density have been passed on to the test function, which with the former hypothesis is sufficiently smooth to admit these derivatives.

</doc>
<doc id="3638803" url="https://en.wikipedia.org/wiki?curid=3638803" title="Constant curvature">
Constant curvature

In mathematics, constant curvature is a concept from differential geometry. Here, curvature refers to the sectional curvature of a space (more precisely a manifold) and is a single number determining its local geometry. The sectional curvature is said to be constant if it has the same value at every point and for every two-dimensional tangent plane at that point. For example, a sphere is a surface of constant positive curvature.
Classification.
The Riemannian manifolds of constant curvature can be classified into the following three cases:

</doc>
<doc id="6122" url="https://en.wikipedia.org/wiki?curid=6122" title="Continuous function">
Continuous function

In mathematics, a continuous function is, roughly speaking, a function for which small changes in the input result in small changes in the output. Otherwise, a function is said to be a "discontinuous" function. A continuous function with a continuous inverse function is called a homeomorphism.
Continuity of functions is one of the core concepts of topology, which is treated in full generality below. The introductory portion of this article focuses on the special case where the inputs and outputs of functions are real numbers. In addition, this article discusses the definition for the more general case of functions between two metric spaces. In order theory, especially in domain theory, one considers a notion of continuity known as Scott continuity. Other forms of continuity do exist but they are not discussed in this article.
As an example, consider the function "h"("t"), which describes the height of a growing flower at time "t". This function is continuous. By contrast, if "M"("t") denotes the amount of money in a bank account at time "t", then the function jumps whenever money is deposited or withdrawn, so the function "M"("t") is discontinuous.
History.
A form of this epsilon-delta definition of continuity was first given by Bernard Bolzano in 1817. Augustin-Louis Cauchy defined continuity of formula_1 as follows: an infinitely small increment formula_2 of the independent variable "x" always produces an infinitely small change formula_3 of the dependent variable "y" (see e.g., "Cours d'Analyse", p. 34). Cauchy defined infinitely small quantities in terms of variable quantities, and his definition of continuity closely parallels the infinitesimal definition used today (see microcontinuity). The formal definition and the distinction between pointwise continuity and uniform continuity were first given by Bolzano in the 1830s but the work wasn't published until the 1930s. Eduard Heine provided the first published definition of uniform continuity in 1872, but based these ideas on lectures given by Peter Gustav Lejeune Dirichlet in 1854.
Real-valued continuous functions.
Definition.
A function from the set of real numbers to the real numbers can be represented by a graph in the Cartesian plane; such a function is continuous if, roughly speaking, the graph is a single unbroken curve with no "holes" or "jumps".
A function is "continuous at a point" if it does not have a hole or jump. A "hole" or "jump" in the graph of a function occurs if the value of the function at a point "c" differs from its limiting value along points that are nearby. Such a point is called a "discontinuity". A function is then "continuous" if it has no holes or jumps: that is, if it is continuous at every point of its domain. Otherwise, a function is "discontinuous", at the points where the value of the function differs from its limiting value (if any).
There are several ways to make this definition mathematically rigorous. These definitions are equivalent to one another, so the most convenient definition can be used to determine whether a given function is continuous or not. In the definitions below,
is a function defined on a subset "I" of the set R of real numbers. This subset "I" is referred to as the domain of "f". Some possible choices include "I"=R, the whole set of real numbers, an open interval
or a closed interval
Here, "a" and "b" are real numbers.
Definition in terms of limits of functions.
The function "f" is "continuous at some point" "c" of its domain if the limit of "f"("x") as "x" approaches "c" through the domain of "f" exists and is equal to "f"("c"). In mathematical notation, this is written as
In detail this means three conditions: first, "f" has to be defined at "c". Second, the limit on the left hand side of that equation has to exist. Third, the value of this limit must equal "f"("c").
Definition in terms of neighborhoods.
A neighborhood of a point "c" is a set that contains all points of the domain within some fixed distance of "c". Intuitively, a function is continuous at a point "c" if the range of the restriction of "f" to a neighborhood of "c" shrinks to a single point "f"("c") as the width of the neighborhood shrinks to zero. More precisely, a function "f" is continuous at a point "c" of its domain if, for any neighborhood formula_8 there is a neighborhood formula_9 such that formula_10 whenever formula_11.
This definition does not require any assumption on the nature of the domain. For instance, the function "f" is automatically continuous at every isolated point of its domain. As a specific example, every real valued function on the set of integers is continuous.
Definition in terms of limits of sequences.
One can instead require that for any sequence formula_12 of points in the domain which converges to "c", the corresponding sequence formula_13 converges to "f"("c"). In mathematical notation, formula_14
Weierstrass definition (epsilon–delta) of continuous functions.
Explicitly including the definition of the limit of a function, we obtain a self-contained definition:
Given a function "f" as above and an element "c" of the domain "I", "f" is said to be continuous at the point "c" if the following holds: For any number "ε" > 0, however small, there exists some number "δ" > 0 such that for all "x" in the domain of "f" with "c" − "δ" < "x" < "c" + "δ", the value of "f"("x") satisfies
Alternatively written, continuity of "f" : "I" → "R" at "c" ∈ "I" means that for every "ε" > 0 there exists a "δ" > 0 such that for all "x" ∈ "I",:
More intuitively, we can say that if we want to get all the "f"("x") values to stay in some small neighborhood around "f"("c"), we simply need to choose a small enough neighborhood for the "x" values around "c", and we can do that no matter how small the "f"("x") neighborhood is; "f" is then continuous at "c".
In modern terms, this is generalized by the definition of continuity of a function with respect to a basis for the topology, here the metric topology.
Definition using oscillation.
Continuity can also be defined in terms of oscillation: a function "f" is continuous at a point "x"0 if and only if its oscillation at that point is zero; in symbols, formula_17 A benefit of this definition is that it "quantifies" discontinuity: the oscillation gives how "much" the function is discontinuous at a point.
This definition is useful in descriptive set theory to study the set of discontinuities and continuous points – the continuous points are the intersection of the sets where the oscillation is less than "ε" (hence a Gδ set) – and gives a very quick proof of one direction of the Lebesgue integrability condition.
The oscillation is equivalent to the "ε"-"δ" definition by a simple re-arrangement, and by using a limit (lim sup, lim inf) to define oscillation: if (at a given point) for a given "ε"0 there is no "δ" that satisfies the "ε"-"δ" definition, then the oscillation is at least "ε"0, and conversely if for every "ε" there is a desired "δ," the oscillation is 0. The oscillation definition can be naturally generalized to maps from a topological space to a metric space.
Definition using the hyperreals.
Cauchy defined continuity of a function in the following intuitive terms: an infinitesimal change in the independent variable corresponds to an infinitesimal change of the dependent variable (see "Cours d'analyse", page 34). Non-standard analysis is a way of making this mathematically rigorous. The real line is augmented by the addition of infinite and infinitesimal numbers to form the hyperreal numbers. In nonstandard analysis, continuity can be defined as follows.
(see microcontinuity). In other words, an infinitesimal increment of the independent variable always produces to an infinitesimal change of the dependent variable, giving a modern expression to Augustin-Louis Cauchy's definition of continuity.
Examples.
All polynomial functions, such as
(pictured), are continuous. This is a consequence of the fact that, given two continuous functions
defined on the same domain "I", then the sum "f" + "g" and the product "fg" of the two functions are continuous (on the same domain "I"). Moreover, the function
is continuous. (The points where "g"("x") is zero are discarded, as they are not in the domain of "f"/"g".) For example, the function (pictured)
is defined for all real numbers and is continuous at every such point. Thus it is a continuous function. The question of continuity at does not arise, since is not in the domain of "f". There is no continuous function "F": R → R that agrees with "f"("x") for all . The sinc function "g"("x") = (sin "x")/"x", defined for all "x"≠0 is continuous at these points. Thus it is a continuous function, too. However, unlike the one of the previous example, this one "can" be extended to a continuous function on all real numbers, namely
since the limit of "g"("x"), when "x" approaches 0, is 1. Therefore, the point "x"=0 is called a removable singularity of "g".
Given two continuous functions
the composition
is continuous.
Non-examples.
An example of a discontinuous function is the function "f" defined by "f"("x") = 1 if "x" > 0, "f"("x") = 0 if "x" ≤ 0. Pick for instance ε = . There is no δ-neighborhood around "x" = 0 that will force all the "f"("x") values to be within ε of "f"(0). Intuitively we can think of this type of discontinuity as a sudden jump in function values. Similarly, the signum or sign function
is discontinuous at "x" = 0 but continuous everywhere else. Yet another example: the function
is continuous everywhere apart from "x" = 0.
Thomae's function,
is continuous at all irrational numbers and discontinuous at all rational numbers. In a similar vein, Dirichlet's function
is nowhere continuous.
Properties.
Intermediate value theorem.
The intermediate value theorem is an existence theorem, based on the real number property of completeness, and states:
For example, if a child grows from 1 m to 1.5 m between the ages of two and six years, then, at some time between two and six years of age, the child's height must have been 1.25 m.
As a consequence, if "f" is continuous on ["a", "b"] and "f"("a") and "f"("b") differ in sign, then, at some point "c" in ["a", "b"], "f"("c") must equal zero.
Extreme value theorem.
The extreme value theorem states that if a function "f" is defined on a closed interval ["a","b"] (or any closed and bounded set) and is continuous there, then the function attains its maximum, i.e. there exists "c" ∈ ["a","b"] with "f"("c") ≥ "f"("x") for all "x" ∈ ["a","b"]. The same is true of the minimum of "f". These statements are not, in general, true if the function is defined on an open interval ("a","b") (or any set that is not both closed and bounded), as, for example, the continuous function "f"("x") = 1/"x", defined on the open interval (0,1), does not attain a maximum, being unbounded above.
Relation to differentiability and integrability.
Every differentiable function
is continuous, as can be shown. The converse does not hold: for example, the absolute value function
is everywhere continuous. However, it is not differentiable at "x" = 0 (but is so everywhere else). Weierstrass's function is also everywhere continuous but nowhere differentiable.
The derivative "f′"("x") of a differentiable function "f"("x") need not be continuous. If "f′"("x") is continuous, "f"("x") is said to be continuously differentiable. The set of such functions is denoted "C"1(). More generally, the set of functions
(from an open interval (or open subset of R) Ω to the reals) such that "f" is "n" times differentiable and such that the "n"-th derivative of "f" is continuous is denoted "C""n"(Ω). See differentiability class. In the field of computer graphics, these three levels are sometimes called "G"0 (continuity of position), "G"1 (continuity of tangency), and "G"2 (continuity of curvature).
Every continuous function
is integrable (for example in the sense of the Riemann integral). The converse does not hold, as the (integrable, but discontinuous) sign function shows.
Pointwise and uniform limits.
Given a sequence
of functions such that the limit
exists for all "x" in "I", the resulting function "f"("x") is referred to as the pointwise limit of the sequence of functions ("f""n")"n"∈N. The pointwise limit function need not be continuous, even if all functions "f""n" are continuous, as the animation at the right shows. However, "f" is continuous when the sequence converges uniformly, by the uniform convergence theorem. This theorem can be used to show that the exponential functions, logarithms, square root function, trigonometric functions are continuous.
Directional and semi-continuity.
Discontinuous functions may be discontinuous in a restricted way, giving rise to the concept of directional continuity (or right and left continuous functions) and semi-continuity. Roughly speaking, a function is "right-continuous" if no jump occurs when the limit point is approached from the right. Formally, "f" is said to be right-continuous at the point "c" if the following holds: For any number "ε" > 0 however small, there exists some number "δ" > 0 such that for all "x" in the domain with , the value of "f"("x") will satisfy
This is the same condition as for continuous functions, except that it is required to hold for "x" strictly larger than "c" only. Requiring it instead for all "x" with yields the notion of "left-continuous" functions. A function is continuous if and only if it is both right-continuous and left-continuous.
A function "f" is "lower semi-continuous" if, roughly, any jumps that might occur only go down, but not up. That is, for any "ε" > 0, there exists some number "δ" > 0 such that for all "x" in the domain with , the value of "f"("x") satisfies
The reverse condition is "upper semi-continuity".
Continuous functions between metric spaces.
The concept of continuous real-valued functions can be generalized to functions between metric spaces. A metric space is a set "X" equipped with a function (called metric) "d""X", that can be thought of as a measurement of the distance of any two elements in "X". Formally, the metric is a function
that satisfies a number of requirements, notably the triangle inequality. Given two metric spaces ("X", d"X") and ("Y", d"Y") and a function
then "f" is continuous at the point "c" in "X" (with respect to the given metrics) if for any positive real number ε, there exists a positive real number δ such that all "x" in "X" satisfying d"X"("x", "c") < δ will also satisfy d"Y"("f"("x"), "f"("c")) < ε. As in the case of real functions above, this is equivalent to the condition that for every sequence ("x""n") in "X" with limit lim "x""n" = "c", we have lim "f"("x""n") = "f"("c"). The latter condition can be weakened as follows: "f" is continuous at the point "c" if and only if for every convergent sequence ("x""n") in "X" with limit "c", the sequence ("f"("x""n")) is a Cauchy sequence, and "c" is in the domain of "f".
The set of points at which a function between metric spaces is continuous is a Gδ set – this follows from the ε-δ definition of continuity.
This notion of continuity is applied, for example, in functional analysis. A key statement in this area says that a linear operator
between normed vector spaces "V" and "W" (which are vector spaces equipped with a compatible norm, denoted ||"x"||)
is continuous if and only if it is bounded, that is, there is a constant "K" such that
for all "x" in "V".
Uniform, Hölder and Lipschitz continuity.
The concept of continuity for functions between metric spaces can be strengthened in various ways by limiting the way δ depends on ε and "c" in the definition above. Intuitively, a function "f" as above is uniformly continuous if the δ does
not depend on the point "c". More precisely, it is required that for every real number "ε" > 0 there exists "δ" > 0 such that for every "c", "b" ∈ "X" with "d""X"("b", "c") < "δ", we have that "d""Y"("f"("b"), "f"("c")) < "ε". Thus, any uniformly continuous function is continuous. The converse does not hold in general, but holds when the domain space "X" is compact. Uniformly continuous maps can be defined in the more general situation of uniform spaces.
A function is Hölder continuous with exponent α (a real number) if there is a constant "K" such that for all "b" and "c" in "X", the inequality
holds. Any Hölder continuous function is uniformly continuous. The particular case is referred to as Lipschitz continuity. That is, a function is Lipschitz continuous if there is a constant "K" such that the inequality
holds any "b", "c" in "X". The Lipschitz condition occurs, for example, in the Picard–Lindelöf theorem concerning the solutions of ordinary differential equations.
Continuous functions between topological spaces.
Another, more abstract, notion of continuity is continuity of functions between topological spaces in which there generally is no formal notion of distance, as there is in the case of metric spaces. A topological space is a set "X" together with a topology on "X", which is a set of subsets of "X" satisfying a few requirements with respect to their unions and intersections that generalize the properties of the open balls in metric spaces while still allowing to talk about the neighbourhoods of a given point. The elements of a topology are called open subsets of "X" (with respect to the topology).
A function
between two topological spaces "X" and "Y" is continuous if for every open set "V" ⊆ "Y", the inverse image
is an open subset of "X". That is, "f" is a function between the sets "X" and "Y" (not on the elements of the topology "TX"), but the continuity of "f" depends on the topologies used on "X" and "Y".
This is equivalent to the condition that the preimages of the closed sets (which are the complements of the open subsets) in "Y" are closed in "X".
An extreme example: if a set "X" is given the discrete topology (in which every subset is open), all functions
to any topological space "T" are continuous. On the other hand, if "X" is equipped with the indiscrete topology (in which the only open subsets are the empty set and "X") and the space "T" set is at least T0, then the only continuous functions are the constant functions. Conversely, any function whose range is indiscrete is continuous.
Alternative definitions.
Several equivalent definitions for a topological structure exist and thus there are several equivalent ways to define a continuous function.
Neighborhood definition.
Neighborhoods continuity for functions between topological spaces formula_45 and formula_46 at a point may be defined:
A function formula_47 is continuous at a point formula_48 iff for any neighborhood of its image formula_49 the preimage is again a neighborhood of that point: formula_50
According to the property that neighborhood systems being upper sets this can be restated as follows:
formula_51
formula_52
The second one being a restatement involving the image rather than the preimage.
Literally, this means no matter how small the neighborhood is chosen one can always find a neighborhood mapped into it.
Besides, there's a simplification involving only open neighborhoods. In fact, they're equivalent:
formula_53
formula_54
The second one again being a restatement using images rather than preimages.
If "X" and "Y" are metric spaces, it is equivalent to consider the neighborhood system of open balls centered at "x" and "f"("x") instead of all neighborhoods. This gives back the above δ-ε definition of continuity in the context of metric spaces. However, in general topological spaces, there is no notion of nearness or distance.
Note, however, that if the target space is Hausdorff, it is still true that "f" is continuous at "a" if and only if the limit of "f" as "x" approaches "a" is "f"("a"). At an isolated point, every function is continuous.
Sequences and nets.
In several contexts, the topology of a space is conveniently specified in terms of limit points. In many instances, this is accomplished by specifying when a point is the limit of a sequence, but for some spaces that are too large in some sense, one specifies also when a point is the limit of more general sets of points indexed by a directed set, known as nets. A function is (Heine-)continuous only if it takes limits of sequences to limits of sequences. In the former case, preservation of limits is also sufficient; in the latter, a function may preserve all limits of sequences yet still fail to be continuous, and preservation of nets is a necessary and sufficient condition.
In detail, a function "f": "X" → "Y" is sequentially continuous if whenever a sequence ("x""n") in "X" converges to a limit "x", the sequence ("f"("x""n")) converges to "f"("x"). Thus sequentially continuous functions "preserve sequential limits". Every continuous function is sequentially continuous. If "X" is a first-countable space and countable choice holds, then the converse also holds: any function preserving sequential limits is continuous. In particular, if "X" is a metric space, sequential continuity and continuity are equivalent. For non first-countable spaces, sequential continuity might be strictly weaker than continuity. (The spaces for which the two properties are equivalent are called sequential spaces.) This motivates the consideration of nets instead of sequences in general topological spaces. Continuous functions preserve limits of nets, and in fact this property characterizes continuous functions.
Closure operator definition.
Instead of specifying the open subsets of a topological space, the topology can also be determined by a closure operator (denoted cl) which assigns to any subset "A" ⊆ "X" its closure, or an interior operator (denoted int), which assigns to any subset "A" of "X" its interior. In these terms, a function
between topological spaces is continuous in the sense above if and only if for all subsets "A" of "X"
That is to say, given any element "x" of "X" that is in the closure of any subset "A", "f"("x") belongs to the closure of "f"("A"). This is equivalent to the requirement that for all subsets "A"' of "X"'
Moreover,
is continuous if and only if
for any subset "A"' of "Y".
Properties.
If "f": "X" → "Y" and "g": "Y" → "Z" are continuous, then so is the composition "g" ∘ "f": "X" → "Z". If "f": "X" → "Y" is continuous and
The possible topologies on a fixed set "X" are partially ordered: a topology τ1 is said to be coarser than another topology τ2 (notation: τ1 ⊆ τ2) if every open subset with respect to τ1 is also open with respect to τ2. Then, the identity map
is continuous if and only if τ1 ⊆ τ2 (see also comparison of topologies). More generally, a continuous function
stays continuous if the topology τ"Y" is replaced by a coarser topology and/or τ"X" is replaced by a finer topology.
Homeomorphisms.
Symmetric to the concept of a continuous map is an open map, for which "images" of open sets are open. In fact, if an open map "f" has an inverse function, that inverse is continuous, and if a continuous map "g" has an inverse, that inverse is open. Given a bijective function "f" between two topological spaces, the inverse function "f"−1 need not be continuous. A bijective continuous function with continuous inverse function is called a "homeomorphism".
If a continuous bijection has as its domain a compact space and its codomain is Hausdorff, then it is a homeomorphism.
Defining topologies via continuous functions.
Given a function
where "X" is a topological space and "S" is a set (without a specified topology), the final topology on "S" is defined by letting the open sets of "S" be those subsets "A" of "S" for which "f"−1("A") is open in "X". If "S" has an existing topology, "f" is continuous with respect to this topology if and only if the existing topology is coarser than the final topology on "S". Thus the final topology can be characterized as the finest topology on "S" that makes "f" continuous. If "f" is surjective, this topology is canonically identified with the quotient topology under the equivalence relation defined by "f".
Dually, for a function "f" from a set "S" to a topological space, the initial topology on "S" has as open subsets "A" of "S" those subsets for which "f"("A") is open in "X". If "S" has an existing topology, "f" is continuous with respect to this topology if and only if the existing topology is finer than the initial topology on "S". Thus the initial topology can be characterized as the coarsest topology on "S" that makes "f" continuous. If "f" is injective, this topology is canonically identified with the subspace topology of "S", viewed as a subset of "X".
More generally, given a set "S", specifying the set of continuous functions
into all topological spaces "X" defines a topology. Dually, a similar idea can be applied to maps
This is an instance of a universal property.
Related notions.
Various other mathematical domains use the concept of continuity in different, but related meanings. For example, in order theory, an order-preserving function "f": "X" → "Y" between particular types of partially ordered sets "X" and "Y" is continuous if for each directed subset "A" of "X", we have sup("f"("A")) = "f"(sup("A")). Here sup is the supremum with respect to the orderings in "X" and "Y", respectively. This notion of continuity is the same as topological continuity when the partially ordered sets are given the Scott topology.
In category theory, a functor
between two categories is called "continuous", if it commutes with small limits. That is to say,
for any small (i.e., indexed by a set "I", as opposed to a class) diagram of objects in formula_66.
A "continuity space" is a generalization of metric spaces and posets, which uses the concept of quantales, and that can be used to unify the notions of metric spaces and domains.

</doc>
<doc id="3667668" url="https://en.wikipedia.org/wiki?curid=3667668" title="Continuous symmetry">
Continuous symmetry

In mathematics, continuous symmetry is an intuitive idea corresponding to the concept of viewing some symmetries as motions, as opposed to e.g. reflection symmetry, which is invariance under a kind of flip from one state to another. 
Formalization.
The notion of continuous symmetry has largely and successfully been formalised in the mathematical notions of topological group, Lie group and group action. For most practical purposes continuous symmetry is modelled by a "group action" of a topological group. 
One-parameter subgroups.
The simplest motions follow a one-parameter subgroup of a Lie group, such as the Euclidean group of three-dimensional space. For example translation parallel to the "x"-axis by "u" units, as "u" varies, is a one-parameter group of motions. Rotation around the "z"-axis is also a one-parameter group.
Noether's theorem.
Continuous symmetry has a basic role in Noether's theorem in theoretical physics, in the derivation of conservation laws from symmetry principles, specifically for continuous symmetries. The search for continuous symmetries only intensified with the further developments of quantum field theory.

</doc>
<doc id="7519" url="https://en.wikipedia.org/wiki?curid=7519" title="Convolution">
Convolution

In mathematics and, in particular, functional analysis, convolution is a mathematical operation on two functions "f" and "g", producing a third function that is typically viewed as a modified version of one of the original functions, giving the integral of the pointwise multiplation of the two functions as a function of the amount that one of the original functions is translated. Convolution is similar to cross-correlation. It has applications that include probability, statistics, computer vision, natural language processing, image and signal processing, engineering, and differential equations.
The convolution can be defined for functions on groups other than Euclidean space. For example, periodic functions, such as the discrete-time Fourier transform, can be defined on a circle and convolved by "periodic convolution". (See row 10 at DTFT#Properties.)  A "discrete convolution" can be defined for functions on the set of integers. Generalizations of convolution have applications in the field of numerical analysis and numerical linear algebra, and in the design and implementation of finite impulse response filters in signal processing.
Computing the inverse of the convolution operation is known as deconvolution.
Definition.
The convolution of "f" and "g" is written "f"∗"g", using an asterisk or star. It is defined as the integral of the product of the two functions after one is reversed and shifted. As such, it is a particular kind of integral transform:
While the symbol "t" is used above, it need not represent the time domain. But in that context, the convolution formula can be described as a weighted average of the function "f"("τ") at the moment "t" where the weighting is given by "g"(−"τ") simply shifted by amount "t". As "t" changes, the weighting function emphasizes different parts of the input function.
For functions "f", "g" supported on only formula_1 (i.e., zero for negative arguments), the integration limits can be truncated, resulting in
In this case, the Laplace transform is more appropriate than the Fourier transform below and boundary terms become relevant.
For the multi-dimensional formulation of convolution, see Domain of definition (below).
Derivations.
Convolution describes the output (in terms of the input) of an important class of operations known as "linear time-invariant" (LTI). See LTI system theory for a derivation of convolution as the result of LTI constraints. In terms of the Fourier transforms of the input and output of an LTI operation, no new frequency components are created. The existing ones are only modified (amplitude and/or phase). In other words, the output transform is the pointwise product of the input transform with a third transform (known as a transfer function). See Convolution theorem for a derivation of that property of convolution. Conversely, convolution can be derived as the inverse Fourier transform of the pointwise product of two Fourier transforms.
Historical developments.
One of the earliest uses of the convolution integral appeared in D'Alembert's derivation of Taylor's theorem in "Recherches sur différents points importants du système du monde," published in 1754.
Also, an expression of the type:
is used by Sylvestre François Lacroix on page 505 of his book entitled "Treatise on differences and series", which is the last of 3 volumes of the encyclopedic series: "Traité du calcul différentiel et du calcul intégral", Chez Courcier, Paris, 1797-1800. Soon thereafter, convolution operations appear in the works of Pierre Simon Laplace, Jean Baptiste Joseph Fourier, Siméon Denis Poisson, and others. The term itself did not come into wide use until the 1950s or 60s. Prior to that it was sometimes known as "faltung" (which means "folding" in German), "composition product", "superposition integral", and "Carson's integral".
Yet it appears as early as 1903, though the definition is rather unfamiliar in older uses.
The operation:
is a particular case of composition products considered by the Italian mathematician Vito Volterra in 1913.
Circular convolution.
When a function "g""T" is periodic, with period "T", then for functions, "f", such that "f"∗"g""T" exists, the convolution is also periodic and identical to:
where "t"o is an arbitrary choice. The summation is called a periodic summation of the function "f".
When "g""T" is a periodic summation of another function, "g", then "f"∗"g""T" is known as a "circular" or "cyclic" convolution of "f" and "g".<br>
And if the periodic summation above is replaced by "f""T", the operation is called a "periodic" convolution of "f""T" and "g""T".
Discrete convolution.
For complex-valued functions "f", "g" defined on the set Z of integers, the discrete convolution of "f" and "g" is given by:
The convolution of two finite sequences is defined by extending the sequences to finitely supported functions on the set of integers. When the sequences are the coefficients of two polynomials, then the coefficients of the ordinary product of the two polynomials are the convolution of the original two sequences. This is known as the Cauchy product of the coefficients of the sequences.
Thus when "g" has finite support in the set formula_7 (representing, for instance, a finite impulse response), a finite summation may be used:
Circular discrete convolution.
When a function "gN" is periodic, with period "N", then for functions, "f", such that "f"∗"gN" exists, the convolution is also periodic and identical to:
The summation on "k" is called a periodic summation of the function "f".
If "gN" is a periodic summation of another function, "g", then "f"∗"gN" is known as a circular convolution of "f" and "g".
When the non-zero durations of both "f" and "g" are limited to the interval [0, "N" − 1], "f"∗"gN" reduces to these common forms:
] \equiv (f *_N g)[n]
The notation ("f" ∗"N" "g") for "cyclic convolution" denotes convolution over the cyclic group of integers modulo "N".
Circular convolution arises most often in the context of fast convolution with an FFT algorithm.
Fast convolution algorithms.
In many situations, discrete convolutions can be converted to circular convolutions so that fast transforms with a convolution property can be used to implement the computation. For example, convolution of digit sequences is the kernel operation in multiplication of multi-digit numbers, which can therefore be efficiently implemented with transform techniques (; ).
 requires "N" arithmetic operations per output value and "N"2 operations for "N" outputs. That can be significantly reduced with any of several fast algorithms. Digital signal processing and other applications typically use fast convolution algorithms to reduce the cost of the convolution to O("N" log "N") complexity.
The most common fast convolution algorithms use fast Fourier transform (FFT) algorithms via the circular convolution theorem. Specifically, the circular convolution of two finite-length sequences is found by taking an FFT of each sequence, multiplying pointwise, and then performing an inverse FFT. Convolutions of the type defined above are then efficiently implemented using that technique in conjunction with zero-extension and/or discarding portions of the output. Other fast convolution algorithms, such as the Schönhage–Strassen algorithm or the Mersenne transform, use fast Fourier transforms in other rings.
If one sequence is much longer than the other, zero-extension of the shorter sequence and fast circular convolution is not the most computationally efficient method available. Instead, decomposing the longer sequence into blocks and convolving each block allows for faster algorithms such as the Overlap–save method and Overlap–add method. A hybrid convolution method that combines block and FIR algorithms allows for a zero input-output latency that is useful for real-time convolution computations.
Domain of definition.
The convolution of two complex-valued functions on R"d" is itself a complex-valued function on R"d", defined by:
is well-defined only if "f" and "g" decay sufficiently rapidly at infinity in order for the integral to exist. Conditions for the existence of the convolution may be tricky, since a blow-up in "g" at infinity can be easily offset by sufficiently rapid decay in "f". The question of existence thus may involve different conditions on "f" and "g":
Compactly supported functions.
If "f" and "g" are compactly supported continuous functions, then their convolution exists, and is also compactly supported and continuous . More generally, if either function (say "f") is compactly supported and the other is locally integrable, then the convolution "f"∗"g" is well-defined and continuous.
Convolution of "f" and "g" is also well defined when both functions are locally square integrable on R and supported on an interval of the form [a, +∞) (or both supported on [-∞, a]).
Integrable functions.
The convolution of "f" and "g" exists if "f" and "g" are both Lebesgue integrable functions in L1(R"d"), and in this case "f"∗"g" is also integrable . This is a consequence of Tonelli's theorem. This is also true for functions in formula_11, under the discrete convolution, or more generally for the convolution on any group.
Likewise, if "f" ∈ "L"1(R"d") and "g" ∈ "L""p"(R"d") where 1 ≤ "p" ≤ ∞, then "f"∗"g" ∈ "L""p"(R"d") and
In the particular case "p" = 1, this shows that "L"1 is a Banach algebra under the convolution (and equality of the two sides holds if "f" and "g" are non-negative almost everywhere).
More generally, Young's inequality implies that the convolution is a continuous bilinear map between suitable "L""p" spaces. Specifically, if 1 ≤ "p","q","r" ≤ ∞ satisfy
then
so that the convolution is a continuous bilinear mapping from "L""p"×"L""q" to "L""r".
The Young inequality for convolution is also true in other contexts (circle group, convolution on Z). The preceding inequality is not sharp on the real line: when , there exists a constant such that:
The optimal value of was discovered in 1975.
A stronger estimate is true provided :
where formula_17 is the weak "Lq" norm. Convolution also defines a bilinear continuous map formula_18 for formula_19, owing to the weak Young inequality:
Functions of rapid decay.
In addition to compactly supported functions and integrable functions, functions that have sufficiently rapid decay at infinity can also be convolved. An important feature of the convolution is that if "f" and "g" both decay rapidly, then "f"∗"g" also decays rapidly. In particular, if "f" and "g" are rapidly decreasing functions, then so is the convolution "f"∗"g". Combined with the fact that convolution commutes with differentiation (see Properties), it follows that the class of Schwartz functions is closed under convolution .
Distributions.
Under some circumstances, it is possible to define the convolution of a function with a distribution, or of two distributions. If "f" is a compactly supported function and "g" is a distribution, then "f"∗"g" is a smooth function defined by a distributional formula analogous to
More generally, it is possible to extend the definition of the convolution in a unique way so that the associative law
remains valid in the case where "f" is a distribution, and "g" a compactly supported distribution .
Measures.
The convolution of any two Borel measures μ and ν of bounded variation is the measure λ defined by 
This agrees with the convolution defined above when μ and ν are regarded as distributions, as well as the convolution of L1 functions when μ and ν are absolutely continuous with respect to the Lebesgue measure.
The convolution of measures also satisfies the following version of Young's inequality
where the norm is the total variation of a measure. Because the space of measures of bounded variation is a Banach space, convolution of measures can be treated with standard methods of functional analysis that may not apply for the convolution of distributions.
Properties.
Algebraic properties.
The convolution defines a product on the linear space of integrable functions. This product satisfies the following algebraic properties, which formally mean that the space of integrable functions with the product given by convolution is a commutative algebra without identity . Other linear spaces of functions, such as the space of continuous functions of compact support, are closed under the convolution, and so also form commutative algebras.
for any real (or complex) number formula_29.
No algebra of functions possesses an identity for the convolution. The lack of identity is typically not a major inconvenience, since most collections of functions on which the convolution is performed can be convolved with a delta distribution or, at the very least (as is the case of "L"1) admit approximations to the identity. The linear space of compactly supported distributions does, however, admit an identity under the convolution. Specifically,
where δ is the delta distribution.
Some distributions have an inverse element for the convolution, "S"(−1), which is defined by
The set of invertible distributions forms an abelian group under the convolution.
Integration.
If "f" and "g" are integrable functions, then the integral of their convolution on the whole space is simply obtained as the product of their integrals:
This follows from Fubini's theorem. The same result holds if "f" and "g" are only assumed to be nonnegative measurable functions, by Tonelli's theorem.
Differentiation.
In the one-variable case,
where "d"/"dx" is the derivative. More generally, in the case of functions of several variables, an analogous formula holds with the partial derivative:
A particular consequence of this is that the convolution can be viewed as a "smoothing" operation: the convolution of "f" and "g" is differentiable as many times as "f" and "g" are in total.
These identities hold under the precise condition that "f" and "g" are absolutely integrable and at least one of them has an absolutely integrable (L1) weak derivative, as a consequence of Young's inequality. For instance, when "f" is continuously differentiable with compact support, and "g" is an arbitrary locally integrable function,
These identities also hold much more broadly in the sense of tempered distributions if one of "f" or "g" is a compactly supported distribution or a Schwartz function and the other is a tempered distribution. On the other hand, two positive integrable and infinitely differentiable functions may have a nowhere continuous convolution.
In the discrete case, the difference operator "D" "f"("n") = "f"("n" + 1) − "f"("n") satisfies an analogous relationship:
Convolution theorem.
The convolution theorem states that
where formula_39 denotes the Fourier transform of formula_40, and formula_41 is a constant that depends on the specific normalization of the Fourier transform. Versions of this theorem also hold for the Laplace transform, two-sided Laplace transform, Z-transform and Mellin transform.
See also the less trivial Titchmarsh convolution theorem.
Translation invariance.
The convolution commutes with translations, meaning that
where τ"x"f is the translation of the function "f" by "x" defined by
If "f" is a Schwartz function, then τ"x""f" is the convolution with a translated Dirac delta function τ"x""f" = "f"∗"τ""x" "δ". So translation invariance of the convolution of Schwartz functions is a consequence of the associativity of convolution.
Furthermore, under certain conditions, convolution is the most general translation invariant operation. Informally speaking, the following holds
Thus any translation invariant operation can be represented as a convolution. Convolutions play an important role in the study of time-invariant systems, and especially LTI system theory. The representing function "g""S" is the impulse response of the transformation "S".
A more precise version of the theorem quoted above requires specifying the class of functions on which the convolution is defined, and also requires assuming in addition that "S" must be a continuous linear operator with respect to the appropriate topology. It is known, for instance, that every continuous translation invariant continuous linear operator on "L"1 is the convolution with a finite Borel measure. More generally, every continuous translation invariant continuous linear operator on "L""p" for 1 ≤ "p" < ∞ is the convolution with a tempered distribution whose Fourier transform is bounded. To wit, they are all given by bounded Fourier multipliers.
Convolutions on groups.
If "G" is a suitable group endowed with a measure λ, and if "f" and "g" are real or complex valued integrable functions on "G", then we can define their convolution by
It is not commutative in general. In typical cases of interest "G" is a locally compact Hausdorff topological group and λ is a (left-) Haar measure. In that case, unless "G" is unimodular, the convolution defined in this way is not the same as formula_45. The preference of one over the other is made so that convolution with a fixed function "g" commutes with left translation in the group:
Furthermore, the convention is also required for consistency with the definition of the convolution of measures given below. However, with a right instead of a left Haar measure, the latter integral is preferred over the former.
On locally compact abelian groups, a version of the convolution theorem holds: the Fourier transform of a convolution is the pointwise product of the Fourier transforms. The circle group T with the Lebesgue measure is an immediate example. For a fixed "g" in "L"1(T), we have the following familiar operator acting on the Hilbert space "L"2(T):
The operator "T" is compact. A direct calculation shows that its adjoint "T*" is convolution with
By the commutativity property cited above, "T" is normal: "T"*"T" = "TT"*. Also, "T" commutes with the translation operators. Consider the family "S" of operators consisting of all such convolutions and the translation operators. Then "S" is a commuting family of normal operators. According to spectral theory, there exists an orthonormal basis {"hk"} that simultaneously diagonalizes "S". This characterizes convolutions on the circle. Specifically, we have
which are precisely the characters of T. Each convolution is a compact multiplication operator in this basis. This can be viewed as a version of the convolution theorem discussed above.
A discrete example is a finite cyclic group of order "n". Convolution operators are here represented by circulant matrices, and can be diagonalized by the discrete Fourier transform.
A similar result holds for compact groups (not necessarily abelian): the matrix coefficients of finite-dimensional unitary representations form an orthonormal basis in "L"2 by the Peter–Weyl theorem, and an analog of the convolution theorem continues to hold, along with many other aspects of harmonic analysis that depend on the Fourier transform.
Convolution of measures.
Let "G" be a topological group.
If μ and ν are finite Borel measures on "G", then their convolution μ∗ν is defined by
for each measurable subset "E" of "G". The convolution is also a finite measure, whose total variation satisfies
In the case when "G" is locally compact with (left-)Haar measure λ, and μ and ν are absolutely continuous with respect to a λ, so that each has a density function, then the convolution μ∗ν is also absolutely continuous, and its density function is just the convolution of the two separate density functions.
If μ and ν are probability measures on the topological group then the convolution μ∗ν is the probability distribution of the sum "X" + "Y" of two independent random variables "X" and "Y" whose respective distributions are μ and ν.
Bialgebras.
Let ("X", Δ, ∇, "ε", "η") be a bialgebra with comultiplication Δ, multiplication ∇, unit η, and counit ε. The convolution is a product defined on the endomorphism algebra End("X") as follows. Let φ, ψ ∈ End("X"), that is, φ,ψ : "X" → "X" are functions that respect all algebraic structure of "X", then the convolution φ∗ψ is defined as the composition
The convolution appears notably in the definition of Hopf algebras . A bialgebra is a Hopf algebra if and only if it has an antipode: an endomorphism "S" such that
Applications.
Convolution and related operations are found in many applications in science, engineering and mathematics.

</doc>
<doc id="81931" url="https://en.wikipedia.org/wiki?curid=81931" title="Coordinate system">
Coordinate system

In geometry, a coordinate system is a system which uses one or more numbers, or coordinates, to uniquely determine the position of a point or other geometric element on a manifold such as Euclidean space. The order of the coordinates is significant and they are sometimes identified by their position in an ordered tuple and sometimes by a letter, as in "the "x"-coordinate". The coordinates are taken to be real numbers in elementary mathematics, but may be complex numbers or elements of a more abstract system such as a commutative ring. The use of a coordinate system allows problems in geometry to be translated into problems about numbers and "vice versa"; this is the basis of analytic geometry.
Common coordinate systems.
Number line.
The simplest example of a coordinate system is the identification of points on a line with real numbers using the "number line". In this system, an arbitrary point "O" (the "origin") is chosen on a given line. The coordinate of a point "P" is defined as the signed distance from "O" to "P", where the signed distance is the distance taken as positive or negative depending on which side of the line "P" lies. Each point is given a unique coordinate and each real number is the coordinate of a unique point.
Cartesian coordinate system.
The prototypical example of a coordinate system is the Cartesian coordinate system. In the plane, two perpendicular lines are chosen and the coordinates of a point are taken to be the signed distances to the lines. 
In three dimensions, three perpendicular planes are chosen and the three coordinates of a point are the signed distances to each of the planes. This can be generalized to create "n" coordinates for any point in "n"-dimensional Euclidean space.
Depending on the direction and order of the coordinate axis the system may be a right-hand or a left-hand system.
Polar coordinate system.
Another common coordinate system for the plane is the "polar coordinate system". A point is chosen as the "pole" and a ray from this point is taken as the "polar axis". For a given angle θ, there is a single line through the pole whose angle with the polar axis is θ (measured counterclockwise from the axis to the line). Then there is a unique point on this line whose signed distance from the origin is "r" for given number "r". For a given pair of coordinates ("r", θ) there is a single point, but any point is represented by many pairs of coordinates. For example, ("r", θ), ("r", θ+2π) and (−"r", θ+π) are all polar coordinates for the same point. The pole is represented by (0, θ) for any value of θ. 
Cylindrical and spherical coordinate systems.
There are two common methods for extending the polar coordinate system to three dimensions. In the cylindrical coordinate system, a "z"-coordinate with the same meaning as in Cartesian coordinates is added to the "r" and θ polar coordinates giving a triple (ρ, φ, z). Spherical coordinates take this a step further by converting the pair of cylindrical coordinates ("r", "z") to polar coordinates (ρ, φ) giving a triple ("ρ", "θ", φ).
Homogeneous coordinate system.
A point in the plane may be represented in "homogeneous coordinates" by a triple ("x", "y", "z") where "x"/"z" and "y"/"z" are the Cartesian coordinates of the point. This introduces an "extra" coordinate since only two are needed to specify a point on the plane, but this system is useful in that it represents any point on the projective plane without the use of infinity. In general, a homogeneous coordinate system is one where only the ratios of the coordinates are significant and not the actual values.
Other commonly used systems.
Some other common coordinate systems are the following:
orthogonal coordinates
There are ways of describing curves without coordinates, using intrinsic equations that use invariant quantities such as curvature and arc length. These include:
Coordinates of geometric objects.
Coordinates systems are often used to specify the position of a point, but they may also be used to specify the position of more complex figures such as lines, planes, circles or spheres. For example, Plücker coordinates are used to determine the position of a line in space. When there is a need, the type of figure being described is used to distinguish the type of coordinate system, for example the term "line coordinates" is used for any coordinate system that specifies the position of a line.
It may occur that systems of coordinates for two different sets of geometric figures are equivalent in terms of their analysis. An example of this is the systems of homogeneous coordinates for points and lines in the projective plane. The two systems in a case like this are said to be "dualistic". Dualistic systems have the property that results from one system can be carried over to the other since these results are only different interpretations of the same analytical result; this is known as the "principle of duality".
Transformations.
Because there are often many different possible coordinate systems for describing geometrical figures, it is important to understand how they are related. Such relations are described by "coordinate transformations" which give formulas for the coordinates in one system in terms of the coordinates in another system. For example, in the plane, if Cartesian coordinates ("x", "y") and polar coordinates ("r", "θ") have the same origin, and the polar axis is the positive "x" axis, then the coordinate transformation from polar to Cartesian coordinates is given by "x" = "r" cos"θ" and "y" = "r" sin"θ".
With every bijection from the space to itself two coordinate transformations can be associated:
For example, in 1D, if the mapping is a translation of 3 to the right, the first moves the origin from 0 to 3, so that the coordinate of each point becomes 3 less, while the second moves the origin from 0 to −3, so that the coordinate of each point becomes 3 more.
Coordinate curves and surfaces.
In two dimensions, if one of the coordinates in a point coordinate system is held constant and the other coordinate is allowed to vary, then the resulting curve is called a coordinate curve (some authors use the phrase "coordinate line"). This procedure does not always make sense, for example there are no coordinate curves in a homogeneous coordinate system. In the Cartesian coordinate system the coordinate curves are, in fact, straight lines. Specifically, they are the lines parallel to one of the coordinate axes. For other coordinate systems the coordinates curves may be general curves. For example, the coordinate curves in polar coordinates obtained by holding "r" constant are the circles with center at the origin. Coordinates systems for Euclidean space other than the Cartesian coordinate system are called curvilinear coordinate systems.
In three-dimensional space, if one coordinate is held constant and the other two are allowed to vary, then the resulting surface is called a coordinate surface. For example, the coordinate surfaces obtained by holding ρ constant in the spherical coordinate system are the spheres with center at the origin. In three-dimensional space the intersection of two coordinate surfaces is a coordinate curve. 
Similarly, coordinate hypersurfaces are the "n-1"-dimensional spaces resulting from fixing a single coordinate of an "n"-dimensional coordinate system.
Coordinate maps.
The concept of a "coordinate map", or "chart" is central to the theory of manifolds. A coordinate map is essentially a coordinate system for a subset of a given space with the property that each point has exactly one set of coordinates. More precisely, a coordinate map is a homeomorphism from an open subset of a space "X" to an open subset of R"n". It is often not possible to provide one consistent coordinate system for an entire space. In this case, a collection of coordinate maps are put together to form an atlas covering the space. A space equipped with such an atlas is called a "manifold" and additional structure can be defined on a manifold if the structure is consistent where the coordinate maps overlap. For example, a differentiable manifold is a manifold where the change of coordinates from one coordinate map to another is always a differentiable function.
Orientation-based coordinates.
In geometry and kinematics, coordinate systems are used not only to describe the (linear) position of points, but also to describe the angular position of axes, planes, and rigid bodies. In the latter case, the orientation of a second (typically referred to as "local") coordinate system, fixed to the node, is defined based on the first (typically referred to as "global" or "world" coordinate system). For instance, the orientation of a rigid body can be represented by an orientation matrix, which includes, in its three columns, the Cartesian coordinates of three points. These points are used to define the orientation of the axes of the local system; they are the tips of three unit vectors aligned with those axes.

</doc>
<doc id="5863" url="https://en.wikipedia.org/wiki?curid=5863" title="Copenhagen interpretation">
Copenhagen interpretation

The Copenhagen interpretation is an expression of the meaning of quantum mechanics that was largely devised in the years 1925 to 1927 by Niels Bohr and Werner Heisenberg. It remains one of the most commonly taught interpretations of quantum mechanics. 
According to the Copenhagen interpretation, physical systems generally do not have definite properties prior to being measured, and quantum mechanics can only predict the probabilities that measurements will produce certain results. The act of measurement affects the system, causing the set of probabilities to reduce to only one of the possible values immediately after the measurement. This feature is known as wavefunction collapse.
There have been many objections to the Copenhagen Interpretation over the years. Some have objected to the discontinuous jumps when there is an observation, or the probabilistic element introduced upon observation; or to the subjectiveness of requiring an observer; or to the difficulty of defining a measuring device; or to the necessity of invoking classical physics to describe the "laboratory" in which the results are measured. 
Alternatives to the Copenhagen Interpretation include the many-worlds interpretation, the De Broglie-Bohm (pilot-wave) interpretation, and quantum decoherence theories.
Background.
In the early work of Max Planck, Albert Einstein, and Niels Bohr, the occurrence of energy in discrete quantities was postulated in order to explain phenomena such as the spectrum of black-body radiation, the photoelectric effect, and the stability and spectrum of atoms. These phenomena had eluded explanation by classical physics and even appeared to be in contradiction with it. While elementary particles show predictable properties in many experiments, they become thoroughly unpredictable in others, such as attempts to identify individual particle trajectories through a simple physical apparatus.
Classical physics draws a distinction between particles and waves. It also relies on continuity, and on determinism, in natural phenomena. In the early twentieth century, newly discovered atomic and subatomic phenomena seemed to defy those conceptions. In 1925–1926, quantum mechanics was invented as a mathematical formalism that accurately describes the experiments, yet appears to reject those classical conceptions. Instead, it posits that probability, and discontinuity, are fundamental in the physical world. Classical physics also relies on causality. The standing of causality for quantum mechanics is disputed.
Quantum mechanics cannot easily be reconciled with everyday language and observation. Its interpretation has often seemed counter-intuitive to physicists, including its inventors.
The Copenhagen interpretation intends to indicate the proper ways of thinking and speaking about the physical meaning of the mathematical formulations of quantum mechanics and the corresponding experimental results. It offers due respect to discontinuity, probability, and a conception of wave–particle dualism. In some respects, it denies standing to causality.
Origin of the term.
Werner Heisenberg had been an assistant to Niels Bohr at his institute in Copenhagen during part of the 1920s, when they helped originate quantum mechanical theory. In 1929, Heisenberg gave a series of invited lectures at the University of Chicago explaining the new field of quantum mechanics. The lectures then served as the basis for his textbook, "The Physical Principles of the Quantum Theory", published in 1930. In the book's preface, Heisenberg wrote:
On the whole the book contains nothing that is not to be found in previous publications, particularly in the investigations of Bohr. The purpose of the book seems to me to be fulfilled if it contributes somewhat to the diffusion of that 'Kopenhagener Geist der Quantentheorie' [i.e., Copenhagen spirit of quantum theory] if I may so express myself, which has directed the entire development of modern atomic physics.
The term 'Copenhagen interpretation' suggests something more than just a spirit, such as some definite set of rules for interpreting the mathematical formalism of quantum mechanics, presumably dating back to the 1920s. However, no such text exists, apart from some informal popular lectures by Bohr and Heisenberg, which contradict each other on several important issues. It appears that the particular term, with its more definite sense, was coined by Heisenberg in the 1950s, while criticizing alternate "interpretations" (e.g., David Bohm's) that had been developed. Lectures with the titles 'The Copenhagen Interpretation of Quantum Theory' and 'Criticisms and Counterproposals to the Copenhagen Interpretation', that Heisenberg delivered in 1955, are reprinted in the collection "Physics and Philosophy". Before the book was released for sale, Heisenberg privately expressed regret for having used the term, due to its suggestion of the existence of other interpretations, that he considered to be "nonsense".
Current status of the term.
According to an opponent of the Copenhagen interpretation, John G. Cramer, "Despite an extensive literature which refers to, discusses, and criticizes the Copenhagen interpretation of quantum mechanics, nowhere does there seem to be any concise statement which defines the full Copenhagen interpretation."
Principles.
Because it consists of the views developed by a number of scientists and philosophers during the second quarter of the 20th Century, there is no uniquely definitive statement of the Copenhagen interpretation. Moreover, by different commentators and researchers, various ideas have been associated with it; Asher Peres remarked that very different, sometimes opposite, views are presented as "the Copenhagen interpretation" by different authors. Nonetheless, there are several basic principles that are generally accepted as being part of the interpretation:
Metaphysics of the wave function.
The Copenhagen Interpretation denies that the wave function is anything more than a theoretical concept, or is at least noncommittal about its being a discrete entity or a discernible component of some discrete entity.
The subjective view, that the wave function is merely a mathematical tool for calculating the probabilities in a specific experiment, has some similarities to the ensemble interpretation in that it takes probabilities to be the essence of the quantum state, but unlike the ensemble interpretation, it takes these probabilities to be perfectly applicable to single experimental outcomes, as it interprets them in terms of subjective probability.
There are some who say that there are objective variants of the Copenhagen Interpretation that allow for a "real" wave function, but it is questionable whether that view is really consistent with some of Bohr's statements. Bohr emphasized that science is concerned with predictions of the outcomes of experiments, and that any additional propositions offered are not scientific but metaphysical. Some authors have proposed that Bohr was influenced by positivism (or even pragmatism). On the other hand, Bohr and Heisenberg were not in complete agreement, and they held different views at different times. Heisenberg in particular was prompted to move towards realism.
Even if the wave function is not regarded as real, there is still a divide between those who treat it as definitely and entirely subjective, and those who are noncommittal or agnostic about the subject. An example of the agnostic view is given by Carl Friedrich von Weizsäcker, who, while participating in a colloquium at Cambridge, denied that the Copenhagen interpretation asserted "What cannot be observed does not exist." He suggested instead that the Copenhagen interpretation follows the principle "What is observed certainly exists; about what is not observed we are still free to make suitable assumptions. We use that freedom to avoid paradoxes."
Born rule.
Max Born speaks of his probability interpretation as a "statistical interpretation" of the wave function, and the Born rule is essential to the Copenhagen interpretation. But writers do not all follow the same terminology.
The phrase 'statistical interpretation' often indicates an interpretation of the Born rule somewhat different from the Copenhagen interpretation. For the Copenhagen interpretation, it is axiomatic that the wave function exhausts all that can ever be known in advance about a particular occurrence of the system. The 'statistical' or 'ensemble' interpretation, on the other hand, is explicitly agnostic about whether the information in the wave function is exhaustive of what might be known in advance. It sees itself as more 'minimal' than the Copenhagen interpretation in its claims. It only goes as far as saying that on every occasion of observation, some actual value of some property is found, and that such values are found probabilistically, as detected by many occasions of observation of the same system. The many occurrences of the system are said to constitute an 'ensemble', and they jointly reveal the probability through these occasions of observation. Though they all have the same wave function, the elements of the ensemble might not be identical to one another in all respects, according to the 'agnostic' interpretations. They may, for all we know, beyond current knowledge and beyond the wave function, have individual distinguishing properties. For present-day science, the experimental significance of these various forms of Born's rule is the same, since they make the same predictions about the probability distribution of outcomes of observations, and the unobserved or unactualized potential properties are not accessible to experiment.
Nature of collapse.
Those who hold to the Copenhagen interpretation are willing to say that a wave function involves the various probabilities that a given event will proceed to certain different outcomes. But when the apparatus registers one of those outcomes, no probabilities or superposition of the others linger.
According to Howard, wave function collapse is not mentioned in the writings of Bohr.
Some argue that the concept of the collapse of a "real" wave function was introduced by Heisenberg and later developed by John von Neumann in 1932. However, Heisenberg spoke of the wavefunction as representing available knowledge of a system, and did not use the term "collapse" per se, but instead termed it "reduction" of the wavefunction to a new state representing the change in available knowledge which occurs once a particular phenomenon is registered by the apparatus (often called "measurement").
In 1952 David Bohm developed decoherence, an "explanatory mechanism" for the "appearance" of wave function collapse. Bohm applied decoherence to Louis DeBroglie's pilot wave theory, producing Bohmian mechanics, the first successful hidden variables interpretation of quantum mechanics. Decoherence was then used by Hugh Everett in 1957 to form the core of his many-worlds interpretation. However decoherence was largely ignored until the 1980s.
Non-separability of the wave function.
The domain of the wave function is configuration space, an abstract object quite different from ordinary physical space–time. At a single "point" of configuration space, the wave function collects probabilistic information about several distinct particles, that respectively have physically space-like separation. So the wave function is said to supply a non-separable representation. This reflects a feature of the quantum world that was recognized by Einstein as early as 1905.
In 1927, Bohr drew attention to a consequence of non-separability. The evolution of the system, as determined by the Schrödinger equation, does not display particle trajectories through space–time. It is possible to extract trajectory information from such evolution, but not simultaneously to extract energy–momentum information. This incompatibility is expressed in the Heisenberg uncertainty principle. The two kinds of information have to be extracted on different occasions, because of the non-separability of the wave function representation. In Bohr's thinking, space–time visualizability meant trajectory information. Again, in Bohr's thinking, 'causality' referred to energy–momentum transfer; in his view, lack of energy–momentum knowledge meant lack of 'causality' knowledge. Therefore Bohr thought that knowledge respectively of 'causality' and of space–time visualizability were incompatible but complementary.
Wave–particle dilemma.
The term 'Copenhagen interpretation' was, it seems, invented by Heisenberg in 1955. It is often assumed that the 'Copenhagen interpretation' was agreed between Bohr and Heisenberg, with perhaps Born included. The term Copenhagen interpretation, however, is not well defined when one asks about the wave–particle dilemma, because Bohr and Heisenberg had different or perhaps disagreeing views on it. Which was the true 'Copenhagenist'? Which is the true 'Copenhagen' position on this? What is the true "orthodoxy"?
According to Camilleri, Bohr thought that the distinction between a wave view and a particle view was defined by a distinction between experimental setups, while, differing, Heisenberg thought that it was defined by the possibility of viewing the mathematical formulas as referring to waves or particles. Bohr thought that a particular experimental setup would display either a wave picture or a particle picture, but not both. Heisenberg thought that every mathematical formulation was capable of both wave and particle interpretations. Looking at it slightly differently, Heisenberg's view was about quantum field theory. Thus one is left in a dilemma to know whether the 'Copenhagen interpretation' is the one of Bohr (one or the other) or the one of Heisenberg (always both).
Alfred Landé was for a long time considered orthodox. He did, however, take the Heisenberg viewpoint, in so far as he thought that the wave function was always mathematically open to both interpretations. Eventually this led to his being considered unorthodox, partly because he did not accept Bohr's one-or-the-other view, preferring Heisenberg's always-both view. Another part of the reason for branding Landé unorthodox was that he recited, as did Heisenberg, the 1923 work of old-quantum-theorist William Duane, which anticipated a quantum mechanical theorem that had not been recognized by Born. That theorem seems to make the always-both view, like the one adopted by Heisenberg, rather cogent. One might say "It's there in the mathematics", but that is not a physical statement that would have convinced Bohr. Perhaps the main reason for attacking Landé is that his work demystified the phenomenon of diffraction of particles of matter, such as buckyballs.
Acceptance among physicists.
Throughout much of the twentieth century the Copenhagen interpretation had overwhelming acceptance among physicists. Although astrophysicist and science writer John Gribbin described it as having fallen from primacy after the 1980s, according to a poll conducted at a quantum mechanics conference in 1997, the Copenhagen interpretation remained the most widely accepted specific interpretation of quantum mechanics among physicists. In more recent polls conducted at various quantum mechanics conferences, varying results have been found. Often, as is the case with the 4 referenced sources, the acceptance of the Copenhagen interpretation as the preferred view of the underlying nature was below 50% amongst the surveyed.
Consequences.
The nature of the Copenhagen Interpretation is exposed by considering a number of experiments and paradoxes.
1. Schrödinger's cat
2. Wigner's Friend
3. Double-slit diffraction
4. EPR (Einstein–Podolsky–Rosen) paradox
Criticism.
The completeness of quantum mechanics (thesis 1) was attacked by the Einstein–Podolsky–Rosen thought experiment which was intended to show that quantum mechanics could not be a complete theory.
Experimental tests of Bell's inequality using particles have supported the quantum mechanical prediction of entanglement.
The Copenhagen Interpretation gives special status to measurement processes without clearly defining them or explaining their peculiar effects. In his article entitled "Criticism and Counterproposals to the Copenhagen Interpretation of Quantum Theory," countering the view of Alexandrov that (in Heisenberg's paraphrase) "the wave function in configuration space characterizes the objective state of the electron." Heisenberg says,
Many physicists and philosophers have objected to the Copenhagen interpretation, both on the grounds that it is non-deterministic and that it includes an undefined measurement process that converts probability functions into non-probabilistic measurements. Einstein's comments "I, at any rate, am convinced that He (God) does not throw dice." and "Do you really think the moon isn't there if you aren't looking at it?" exemplify this. Bohr, in response, said, "Einstein, don't tell God what to do."
Steven Weinberg in "Einstein's Mistakes", "Physics Today", November 2005, page 31, said:
The problem of thinking in terms of classical measurements of a quantum system becomes particularly acute in the field of quantum cosmology, where the quantum system is the universe.
E. T. Jaynes, from a Bayesian point of view, argued that probability is a measure of a state of information about the physical world. Quantum mechanics under the Copenhagen Interpretation interpreted probability as a physical phenomenon, which is what Jaynes called a Mind Projection Fallacy.
Common criticisms of the Copenhagen interpretation often lead to the problem of continuum of random occurrences: whether in time (as subsequent measurements, which under certain interpretations of the measurement problem may happen continuously) or even in space. A recent experiment showed that a particle may leave a trace about the path which it used when travelling as a wave – and that this trace exhibits equality of both paths. If such result is raised to the rank of a wave-only non-transactional worldview and proved better – i.e. that a particle is in fact a continuum of points capable of acting independently but under a common wavefunction – it would support rather theories such as Bohm's one (with its guiding towards the centre of orbital and spreading of physical properties over it) than interpretations which presuppose full randomness, because with the latter it will be problematic to demonstrate universally and in all practical cases how can a particle remain coherent in time, in spite of non-zero probabilities of its individual points going into regions distant from the centre of mass (through a continuum of different random determinations). An alternative possibility would be to assume that there is a finite number of instants/points within a given time or area, but theories which try to quantize the space or time itself seem to be fatally incompatible with the special relativity.
The view that particle diffraction logically guarantees the need for a wave interpretation has been questioned. A recent experiment has carried out the two-slit protocol with helium atoms. The basic physics of quantal momentum transfer considered here was originally pointed out in 1923, by William Duane, before quantum mechanics was invented. It was later recognized by Heisenberg and by Pauling. It was championed against orthodox ridicule by Alfred Landé. It has also recently been considered by Van Vliet. If the diffracting slits are considered as classical objects, theoretically ideally seamless, then a wave interpretation seems necessary, but if the diffracting slits are considered physically, as quantal objects exhibiting collective quantal motions, then the particle-only and wave-only interpretations seem perhaps equally valid.
Alternatives.
The Ensemble interpretation is similar; it offers an interpretation of the wave function, but not for single particles. The consistent histories interpretation advertises itself as "Copenhagen done right". Although the Copenhagen interpretation is often confused with the idea that consciousness causes collapse, it defines an "observer" merely as that which collapses the wave function. Quantum information theories are more recent, and have attracted growing support.
Under realism and indeterminism, if the wave function is regarded as ontologically real, and collapse is entirely rejected, a many worlds theory results. If wave function collapse is regarded as ontologically real as well, an objective collapse theory is obtained. Under realism and determinism (as well as non-localism), a hidden variable theory exists, e.g., the de Broglie–Bohm interpretation, which treats the wavefunction as real, position and momentum as definite and resulting from the expected values, and physical properties as spread in space. For an atemporal indeterministic interpretation that “makes no attempt to give a ‘local’ account on the level of determinate particles”, the conjugate wavefunction, ("advanced" or time-reversed) of the relativistic version of the wavefunction, and the so-called "retarded" or time-forward version are both regarded as real and the transactional interpretation results.
Many physicists have subscribed to the instrumentalist interpretation of quantum mechanics, a position often equated with eschewing all interpretation. It is summarized by the sentence "Shut up and calculate!". While this slogan is sometimes attributed to Paul Dirac or Richard Feynman, it seems to be due to David Mermin.

</doc>
<doc id="26288711" url="https://en.wikipedia.org/wiki?curid=26288711" title="Coulomb's law">
Coulomb's law

Coulomb's law or Coulomb's inverse-square law, is a law of physics describing the electrostatic interaction between electrically charged particles. The law was first published in 1784 by French physicist Charles Augustin de Coulomb and was essential to the development of the theory of electromagnetism. It is analogous to Isaac Newton's inverse-square law of universal gravitation. Coulomb's law can be used to derive Gauss's law, and vice versa. The law has been tested heavily, and all observations have upheld the law's principle. 
History.
Ancient cultures around the Mediterranean knew that certain objects, such as rods of amber, could be rubbed with cat's fur to attract light objects like feathers. Thales of Miletus made a series of observations on static electricity around 600 BC, from which he believed that friction rendered amber magnetic, in contrast to minerals such as magnetite, which needed no rubbing. Thales was incorrect in believing the attraction was due to a magnetic effect, but later science would prove a link between magnetism and electricity. Electricity would remain little more than an intellectual curiosity for millennia until 1600, when the English scientist William Gilbert made a careful study of electricity and magnetism, distinguishing the lodestone effect from static electricity produced by rubbing amber. He coined the New Latin word "electricus" ("of amber" or "like amber", from "ήλεκτρον" ["elektron"], the Greek word for "amber") to refer to the property of attracting small objects after being rubbed. This association gave rise to the English words "electric" and "electricity", which made their first appearance in print in Thomas Browne's "Pseudodoxia Epidemica" of 1646.
Early investigators of the 18th century who suspected that the electrical force diminished with distance as the force of gravity did (i.e., as the inverse square of the distance) included Daniel Bernoulli and Alessandro Volta, both of whom measured the force between plates of a capacitor, and Franz Aepinus who supposed the inverse-square law in 1758.
Based on experiments with electrically charged spheres, Joseph Priestley of England was among the first to propose that electrical force followed an inverse-square law, similar to Newton's law of universal gravitation. However, he did not generalize or elaborate on this. In 1767, he conjectured that the force between charges varied as the inverse square of the distance.
In 1769, Scottish physicist John Robison announced that, according to his measurements, the force of repulsion between two spheres with charges of the same sign varied as x−2.06.
In the early 1770s, the dependence of the force between charged bodies upon both distance and charge had already been discovered, but not published, by Henry Cavendish of England.
Finally, in 1785, the French physicist Charles-Augustin de Coulomb published his first three reports of electricity and magnetism where he stated his law. This publication was essential to the development of the theory of electromagnetism. He used a torsion balance to study the repulsion and attraction forces of charged particles, and determined that the magnitude of the electric force between two point charges is directly proportional to the product of the charges and inversely proportional to the square of the distance between them.
The torsion balance consists of a bar suspended from its middle by a thin fiber. The fiber acts as a very weak torsion spring. In Coulomb's experiment, the torsion balance was an insulating rod with a metal-coated ball attached to one end, suspended by a silk thread. The ball was charged with a known charge of static electricity, and a second charged ball of the same polarity was brought near it. The two charged balls repelled one another, twisting the fiber through a certain angle, which could be read from a scale on the instrument. By knowing how much force it took to twist the fiber through a given angle, Coulomb was able to calculate the force between the balls and derive his inverse-square proportionality law.
The law.
Coulomb's law states that:
Coulomb's law can also be stated as a simple mathematical expression. The scalar and vector forms of the mathematical equation are
Units.
Electromagnetic theory is usually expressed using the standard SI units. Force is measured in newtons, charge in coulombs, and distance in metres. Coulomb's constant is given by formula_10. The constant formula_11 is the permittivity of free space in C2 m−2 N−1. And formula_12 is the relative permittivity of the material in which the charges are immersed, and is dimensionless.
The SI derived units for the electric field are volts per meter, newtons per coulomb, or tesla meters per second.
Coulomb's law and Coulomb's constant can also be interpreted in various terms:
Electric field.
An electric field is a vector field that associates to each point in space the Coulomb force experienced by a test charge. In the simplest case, the field is considered to be generated solely by a single source point charge. The strength and direction of the Coulomb force formula_13 on a test charge formula_14 depends on the electric field formula_15 that it finds itself in, such that formula_16. If the field is generated by a positive source point charge formula_17, the direction of the electric field points along lines directed radially outwards from it, i.e. in the direction that a positive point test charge formula_14 would move if placed in the field. For a negative point source charge, the direction is radially inwards.
The magnitude of the electric field formula_15 can be derived from Coulomb's law. By choosing one of the point charges to be the source, and the other to be the test charge, it follows from Coulomb's law that the magnitude of the electric field formula_15 created by a single source point charge formula_17 at a certain distance from it formula_22 in vacuum is given by:
In this way, the verification is limited to measuring the distance between the charges and check that the division approximates the theoretical value.
Electrostatic approximation.
In either formulation, Coulomb’s law is fully accurate only when the objects are stationary, and remains approximately correct only for slow movement. These conditions are collectively known as the electrostatic approximation. When movement takes place, magnetic fields that alter the force on the two objects are produced. The magnetic interaction between moving charges may be thought of as a manifestation of the force from the electrostatic field but with Einstein’s theory of relativity taken into consideration.
Atomic forces.
Coulomb's law holds even within atoms, correctly describing the force between the positively charged atomic nucleus and each of the negatively charged electrons. This simple law also correctly accounts for the forces that bind atoms together to form molecules and for the forces that bind atoms and molecules together to form solids and liquids. Generally, as the distance between ions increases, the energy of attraction approaches zero and ionic bonding is less favorable. As the magnitude of opposing charges increases, energy increases and ionic bonding is more favorable.

</doc>
<doc id="191752" url="https://en.wikipedia.org/wiki?curid=191752" title="Covariance matrix">
Covariance matrix

In probability theory and statistics, a covariance matrix (also known as dispersion matrix or variance–covariance matrix) is a matrix whose element in the "i", "j" position is the covariance between the "i" th and "j" th elements of a random vector (that is, of a vector of random variables). Each element of the vector is a scalar random variable, either with a finite number of observed empirical values or with a finite or infinite number of potential values specified by a theoretical joint probability distribution of all the random variables.
Intuitively, the covariance matrix generalizes the notion of variance to multiple dimensions. As an example, the variation in a collection of random points in two-dimensional space cannot be characterized fully by a single number, nor would the variances in the "x" and "y" directions contain all of the necessary information; a 2×2 matrix would be necessary to fully characterize the two-dimensional variation.
Because the covariance of the "i" th random variable with itself is simply that random variable's variance, each element on the principal diagonal of the covariance matrix is the variance of one of the random variables. Because the covariance of the "i" th random variable with the "j" th one is the same thing as the covariance of the "j" th random variable with the "i" th one, every covariance matrix is symmetric. In addition, every covariance matrix is positive semi-definite.
Definition.
Throughout this article, boldfaced unsubscripted X and Y are used to refer to random vectors, and unboldfaced subscripted Xi and Yi are used to refer to random scalars.
If the entries in the column vector
are random variables, each with finite variance, then the covariance matrix Σ is the matrix whose ("i", "j") entry is the covariance
where
is the expected value of the "i"th entry in the vector X. In other words,
The inverse of this matrix, formula_5 is the inverse covariance matrix, also known as the concentration matrix or precision matrix; see precision (statistics). The elements of the precision matrix have an interpretation in terms of partial correlations and partial variances.
Generalization of the variance.
The definition above is equivalent to the matrix equality
This form can be seen as a generalization of the scalar-valued variance to higher dimensions. Recall that for a scalar-valued random variable "X"
Indeed, the entries on the diagonal of the covariance matrix formula_8 are the variances of each element of the vector formula_9.
Correlation matrix.
A quantity closely related to the covariance matrix is the correlation matrix, the matrix of Pearson product-moment correlation coefficients between each of the random variables in the random vector formula_9, which can be written
where formula_12 is the matrix of the diagonal elements of formula_8 (i.e., a diagonal matrix of the variances of formula_14 for formula_15).
Equivalently, the correlation matrix can be seen as the covariance matrix of the standardized random variables formula_16 for formula_15.
Each element on the principal diagonal of a correlation matrix is the correlation of a random variable with itself, which always equals 1. Each off-diagonal element is between 1 and –1 inclusive.
Conflicting nomenclatures and notations.
Nomenclatures differ. Some statisticians, following the probabilist William Feller, call the matrix formula_8 the variance of the random vector formula_19, because it is the natural generalization to higher dimensions of the 1-dimensional variance. Others call it the covariance matrix, because it is the matrix of covariances between the scalar components of the vector formula_19. Thus
However, the notation for the cross-covariance "between" two vectors is standard:
The var notation is found in William Feller's two-volume book "An Introduction to Probability Theory and Its Applications", but both forms are quite standard and there is no ambiguity between them.
The matrix formula_8 is also often called the variance-covariance matrix since the diagonal terms are in fact variances.
Properties.
For formula_24 and formula_25, where X is a random "p"-dimensional variable and Y a random "q"-dimensional variable, the following basic properties apply:
where formula_36 and formula_37 are random "p"×1 vectors, formula_34 is a random "q"×1 vector, formula_39 is a "q"×1 vector, formula_40 is a "p"×1 vector, and formula_41 and formula_42 are "q"×"p" matrices of constants.
This covariance matrix is a useful tool in many different areas. From it a transformation matrix can be derived, called a whitening transformation, that allows one to completely decorrelate the data or, from a different point of view, to find an optimal basis for representing the data in a compact way (see Rayleigh quotient for a formal proof and additional properties of covariance matrices).
This is called principal components analysis (PCA) and the Karhunen-Loève transform (KL-transform).
Block matrices.
The joint mean formula_43 and joint covariance matrix formula_44 of formula_45 and formula_46 can be written in block form
where formula_48 and formula_49.
formula_50 and formula_51 can be identified as the variance matrices of the marginal distributions for formula_45 and formula_46 respectively.
If formula_45 and formula_46 are jointly normally distributed,
then the conditional distribution for formula_46 given formula_45 is given by
defined by conditional mean
and conditional variance
The matrix ΣYXΣXX−1 is known as the matrix of regression coefficients, while in linear algebra ΣY|X is the Schur complement of ΣXX in ΣX,Y
The matrix of regression coefficients may often be given in transpose form, ΣXX−1ΣXY, suitable for post-multiplying a row vector of explanatory variables xT rather than pre-multiplying a column vector x. In this form they correspond to the coefficients obtained by inverting the matrix of the normal equations of ordinary least squares (OLS).
As a linear operator.
Applied to one vector, the covariance matrix maps a linear combination, c, of the random variables, X, onto a vector of covariances with those variables: formula_62. Treated as a bilinear form, it yields the covariance between the two linear combinations: formula_63. The variance of a linear combination is then formula_64, its covariance with itself.
Similarly, the (pseudo-)inverse covariance matrix provides an inner product, formula_65 which induces the Mahalanobis distance, a measure of the "unlikelihood" of "c".
Which matrices are covariance matrices?
From the identity just above, let formula_40 be a formula_67 real-valued vector, then
which must always be nonnegative since it is the variance of a real-valued random variable. From the symmetry of the covariance matrix's definition it follows that only a positive-semidefinite matrix can be a covariance matrix. Conversely, every symmetric positive semi-definite matrix is a covariance matrix. To see this, suppose M is a "p"×"p" positive-semidefinite matrix. From the finite-dimensional case of the spectral theorem, it follows that M has a nonnegative symmetric square root, that can be denoted by M1/2. Let formula_9 be any "p"×1 column vector-valued random variable whose covariance matrix is the "p"×"p" identity matrix. Then
How to find a valid correlation matrix.
In some applications (e.g., building data models from only partially observed data) one wants to find the "nearest" correlation matrix to a given symmetric matrix (e.g., of observed covariances). In 2002, Higham formalized the notion of nearness using a weighted Frobenius norm and provided a method for computing the nearest correlation matrix.
Complex random vectors.
The variance of a complex scalar-valued random variable with expected value μ is conventionally defined using complex conjugation:
where the complex conjugate of a complex number formula_72 is denoted formula_73; thus the variance of a complex number is a real number.
If formula_74 is a column-vector of complex-valued random variables, then the conjugate transpose is formed by "both" transposing and conjugating. In the following expression, the product of a vector with its conjugate transpose results in a square matrix, as its expectation:
where formula_76 denotes the conjugate transpose, which is applicable to the scalar case since the transpose of a scalar is still a scalar. The matrix so obtained will be Hermitian positive-semidefinite, with real numbers in the main diagonal and complex numbers off-diagonal.
Estimation.
If formula_77 and formula_78 are centred data matrices of dimension "n"-by-"p" and "n-by-q" respectively, i.e. with "n" rows of observations of "p" and "q" columns of variables, from which the column means have been subtracted, then, if the column means were estimated from the data, sample correlation matrices formula_79 and formula_80 can be defined to be
or, if the column means were known a-priori,
These empirical sample correlation matrices are the most straightforward and most often used estimators for the correlation matrices, but other estimators also exist, including regularised or shrinkage estimators, which may have better properties.
As a parameter of a distribution.
If a vector of "n" possibly correlated random variables is jointly normally distributed, or more generally elliptically distributed, then its probability density function can be expressed in terms of the covariance matrix.
Applications.
In financial economics.
The covariance matrix plays a key role in financial economics, especially in portfolio theory and its mutual fund separation theorem and in the capital asset pricing model. The matrix of covariances among various assets' returns is used to determine, under certain assumptions, the relative amounts of different assets that investors should (in a normative analysis) or are predicted to (in a positive analysis) choose to hold in a context of diversification.

</doc>
<doc id="431848" url="https://en.wikipedia.org/wiki?curid=431848" title="Covariant derivative">
Covariant derivative

In mathematics, the covariant derivative is a way of specifying a derivative along tangent vectors of a manifold. Alternatively, the covariant derivative is a way of introducing and working with a connection on a manifold by means of a differential operator, to be contrasted with the approach given by a principal connection on the frame bundle – see affine connection. In the special case of a manifold isometrically embedded into a higher-dimensional Euclidean space, the covariant derivative can be viewed as the orthogonal projection of the Euclidean derivative along a tangent vector onto the manifold's tangent space. In this case the Euclidean derivative is broken into two parts, the extrinsic normal component and the intrinsic covariant derivative component.
This article presents an introduction to the covariant derivative of a vector field with respect to a vector field, both in a coordinate free language and using a local coordinate system and the traditional index notation. The covariant derivative of a tensor field is presented as an extension of the same concept. The covariant derivative generalizes straightforwardly to a notion of differentiation associated to a connection on a vector bundle, also known as a Koszul connection.
Introduction and history.
Historically, at the turn of the 20th century, the covariant derivative was introduced by Gregorio Ricci-Curbastro and Tullio Levi-Civita in the theory of Riemannian and pseudo-Riemannian geometry. Ricci and Levi-Civita (following ideas of Elwin Bruno Christoffel) observed that the Christoffel symbols used to define the curvature could also provide a notion of differentiation which generalized the classical directional derivative of vector fields on a manifold. This new derivative – the Levi-Civita connection – was "covariant" in the sense that it satisfied Riemann's requirement that objects in geometry should be independent of their description in a particular coordinate system.
It was soon noted by other mathematicians, prominent among these being Hermann Weyl, Jan Arnoldus Schouten, and Élie Cartan, that a covariant derivative could be defined abstractly without the presence of a metric. The crucial feature was not a particular dependence on the metric, but that the Christoffel symbols satisfied a certain precise second order transformation law. This transformation law could serve as a starting point for defining the derivative in a covariant manner. Thus the theory of covariant differentiation forked off from the strictly Riemannian context to include a wider range of possible geometries.
In the 1940s, practitioners of differential geometry began introducing other notions of covariant differentiation in general vector bundles which were, in contrast to the classical bundles of interest to geometers, not part of the tensor analysis of the manifold. By and large, these generalized covariant derivatives had to be specified "ad hoc" by some version of the connection concept. In 1950, Jean-Louis Koszul unified these new ideas of covariant differentiation in a vector bundle by means of what is known today as a Koszul connection or a connection on a vector bundle. Using ideas from Lie algebra cohomology, Koszul successfully converted many of the analytic features of covariant differentiation into algebraic ones. In particular, Koszul connections eliminated the need for awkward manipulations of Christoffel symbols (and other analogous non-tensorial) objects in differential geometry. Thus they quickly supplanted the classical notion of covariant derivative in many post-1950 treatments of the subject.
Motivation.
The covariant derivative is a generalization of the directional derivative from vector calculus. As with the directional derivative, the covariant derivative is a rule, formula_1, which takes as its inputs: (1) a vector, u, defined at a point "P", and (2) a vector field, v, defined in a neighborhood of "P". The output is the vector formula_2, also at the point "P". The primary difference from the usual directional derivative is that formula_1 must, in a certain precise sense, be "independent" of the manner in which it is expressed in a coordinate system.
A vector may be "described" as a list of numbers in terms of a basis, but as a geometrical object a vector retains its own identity regardless of how one chooses to describe it in a basis. This persistence of identity is reflected in the fact that when a vector is written in one basis, and then the basis is changed, the components of the vector transform according to a change of basis formula. Such a transformation law is known as a covariant transformation. The covariant derivative is required to transform, under a change in coordinates, in the same way as a basis does: the covariant derivative must change by a covariant transformation (hence the name).
In the case of Euclidean space, one tends to define the derivative of a vector field in terms of the difference between two vectors at two nearby points.
In such a system one translates one of the vectors to the origin of the other, keeping it parallel. With a Cartesian (fixed orthonormal) coordinate system we thus obtain the simplest example: covariant derivative which is obtained by taking the derivative of the components.
In the general case, however, one must take into account the change of the coordinate system. For example, if the same covariant derivative is written in polar coordinates in a two dimensional Euclidean plane, then it contains extra terms that describe how the coordinate grid itself "rotates". In other cases the extra terms describe how the coordinate grid expands, contracts, twists, interweaves, etc.
Consider the example of moving along a curve "γ"("t") in the Euclidean plane. In polar coordinates, γ may be written in terms of its radial and angular coordinates by "γ"("t") = ("r"("t"), "θ"("t")). A vector at a particular time "t" (for instance, the acceleration of the curve) is expressed in terms of formula_4, where formula_5 and formula_6 are unit tangent vectors for the polar coordinates, serving as a basis to decompose a vector in terms of radial and tangential components. At a slightly later time, the new basis in polar coordinates appears slightly rotated with respect to the first set. The covariant derivative of the basis vectors (the Christoffel symbols) serve to express this change.
In a curved space, such as the surface of the Earth (regarded as a sphere), the translation is not well defined and its analog, parallel transport, depends on the path along which the vector is translated.
A vector e on a globe on the equator in Q is directed to the north. Suppose we parallel transport the vector first along the equator until P and then (keeping it parallel to itself) drag it along a meridian to the pole N and (keeping the direction there) subsequently transport it along another meridian back to Q. Then we notice that the parallel-transported vector along a closed circuit does not return as the same vector; instead, it has another orientation. This would not happen in Euclidean space and is caused by the "curvature" of the surface of the globe. The same effect can be noticed if we drag the vector along an infinitesimally small closed surface subsequently along two directions and then back. The infinitesimal change of the vector is a measure of the curvature.
Informal definition using an embedding into Euclidean space.
Suppose a (pseudo) Riemann manifold formula_8, is embedded into Euclidean space formula_9 via a (twice continuously) differentiable mapping formula_10 such that the tangent space at formula_11 is spanned by the vectors
and the scalar product on formula_13 is compatible with the metric on "M": formula_14. (Since the manifold metric is always assumed to be regular, the compatibility condition implies linear independence of the partial derivative tangent vectors.)
For a tangent vector field
one has formula_16.
The last term is not tangential to "M", but can be expressed as a linear combination of the tangent space base vectors using the Christoffel symbols as linear factors plus a vector normal to the tangent space:
The covariant derivative formula_18
and yields the Christoffel symbols for the Levi-Civita connection in terms of the metric:
For a very simple example that captures the essence of the description above, draw a circle on a flat sheet of paper. Travel around the circle at a constant speed. The derivative of your velocity, your acceleration vector, always points radially inward. Roll this sheet of paper into a cylinder. Now the (Euclidean) derivative of your velocity has a component that sometimes points inward toward the axis of the cylinder depending on whether you're near a solstice or an equinox. This is the (Euclidean) normal component. The covariant derivative component is the component parallel to the cylinder's surface, and is the same as that before you rolled the sheet into a cylinder.
Formal definition.
A covariant derivative is a (Koszul) connection on the tangent bundle and other tensor bundles. Thus it has a certain behavior on vector fields that extends that of the usual differential on functions. It also extends in a unique way to the duals of vector fields (i.e., covector fields), and to arbitrary tensor fields, that ensures compatibility with the tensor product and trace operations (tensor contraction).
Functions.
Given a point "p" of the manifold, a real function "f" on the manifold, and a tangent vector v at "p", the covariant derivative of "f" at "p" along v is the scalar at "p", denoted formula_20, that represents the principal part of the change in the value of "f" when the argument of f is changed by the infinitesimal displacement vector v. (This is the differential of "f" evaluated against the vector v.) Formally, there is a differentiable curve formula_21 such that formula_22 and formula_23, and the covariant derivative of "f" at "p" is defined by
When v is a vector field, the covariant derivative formula_25 is the function that associates with each point "p" in the common domain of "f" and v the scalar formula_26. This coincides with the usual Lie derivative of "f" along the vector field v.
Vector fields.
A covariant derivative formula_27 at a point "p" in a smooth manifold assigns a tangent vector formula_28 to each pair formula_29, consisting of a tangent vector v at "p" and vector field u defined in a neighborhood of "p", such that the following properties hold (for any vectors v, x and y at "p", vector fields u and w defined in a neighborhood of "p", scalar values "g" and "h" at "p", and scalar function "f" defined in a neighborhood of "p"):
If u and v are both vector fields defined over a common domain, then formula_39 denotes the vector field whose value at each point "p" of the domain is the tangent vector formula_40. Note that formula_28 depends only on the value of v at "p" but on values of u in an infinitesimal neighbourhood of "p" because of the last property, the product rule.
Covector fields.
Given a field of covectors (or one-form) formula_42 defined in a neighborhood of "p", its covariant derivative formula_43 is defined in a way to make the resulting operation compatible with tensor contraction and the product rule. That is, formula_43 is defined as the unique one-form at "p" such that the following identity is satisfied for all vector fields u in a neighborhood of "p"
The covariant derivative of a covector field along a vector field v is again a covector field.
Tensor fields.
Once the covariant derivative is defined for fields of vectors and covectors it can be defined for arbitrary tensor fields by imposing the following identities for every pair of tensor fields formula_46 and formula_47 in a neighborhood of the point "p":
and for formula_46 and formula_50 of the same valence
The covariant derivative of a tensor field along a vector field v is again a tensor field of the same type.
Explicitly, let "T" be a tensor field of type . Consider "T" to be a differentiable multilinear map of smooth sections "α"1, "α"2, ..., "α""q" of the cotangent bundle "T"∗"M" and of sections "X"1, "X"2, ... "X""p" of the tangent bundle "TM", written "T"(α1, α2, ..., "X"1, "X"2, ...) into R. The covariant derivative of "T" along "Y" is given by the formula
Coordinate description.
Given coordinate functions
any tangent vector can be described by its components in the basis
The covariant derivative of a basis vector along a basis vector is again a vector and so can be expressed as a linear combination formula_57.
To specify the covariant derivative it is enough to specify the covariant derivative of each basis vector field formula_58 along formula_59.
the coefficients formula_61 are called Christoffel symbols.
Then using the rules in the definition, we find that for general vector fields formula_62 and formula_63 we get
so
The first term in this formula is responsible for "twisting" the coordinate system with respect to the covariant derivative and the second for changes of components of the vector field "u". In particular
In words: the covariant derivative is the usual derivative along the coordinates with correction terms which tell how the coordinates change.
For covectors similarly we have
where formula_68.
The covariant derivative of a type {{nowrap|("r", "s")}} tensor field along formula_69 is given by the expression:
Or, in words: take the partial derivative of the tensor and add: a formula_72 for every upper index formula_73, and a formula_74 for every lower index formula_75.
If instead of a tensor, one is trying to differentiate a "tensor density" (of weight +1), then you also add a term
If it is a tensor density of weight "W", then multiply that term by "W".
For example, formula_77 is a scalar density (of weight +1), so we get:
where semicolon ";" indicates covariant differentiation and comma "," indicates partial differentiation. Incidentally, this particular expression is equal to zero, because the covariant derivative of a function solely of the metric is always zero.
Examples.
For a scalar field formula_79, covariant differentiation is simply partial differentiation:
For a contravariant vector field formula_81, we have:
For a covariant vector field formula_83, we have:
For a type (2,0) tensor field formula_85, we have:
For a type (0,2) tensor field formula_87, we have:
For a type (1,1) tensor field formula_89, we have:
The notation above is meant in the sense
One must always remember that covariant derivatives do not commute, i.e. formula_92. It is actually easy to show that:
where formula_94 is the Riemann tensor. Similarly,
and
The latter can be shown by taking (without loss of generality) that formula_97.
Notation.
In textbooks on physics, the covariant derivative is sometimes simply stated in terms of its components in this equation.
Often a notation is used in which the covariant derivative is given with a semicolon, while a normal partial derivative is indicated by a comma. In this notation we write the same as:
Once again this shows that the covariant derivative of a vector field is not just simply obtained by differentiating to the coordinates formula_99, but also depends on the vector v itself through formula_100.
In some older texts (notably Adler, Bazin & Schiffer, "Introduction to General Relativity"), the covariant derivative is denoted by a double pipe:
Derivative along curve.
Since the covariant derivative formula_102 of a tensor field formula_103 at a point formula_104 depends only on value of the vector field formula_105 at formula_104 one can define the covariant derivative along a smooth curve formula_107 in a manifold:
Note that the tensor field formula_103 only needs to be defined on the curve formula_107 for this definition to make sense.
In particular, formula_111 is a vector field along the curve formula_112 itself. If formula_113 vanishes then the curve is called a geodesic of the covariant derivative. If the covariant derivative is the Levi-Civita connection of a certain metric then the geodesics for the connection are precisely the geodesics of the metric that are parametrised by arc length.
The derivative along a curve is also used to define the parallel transport along the curve.
Sometimes the covariant derivative along a curve is called absolute or intrinsic derivative.
Relation to Lie derivative.
A covariant derivative introduces an extra geometric structure on a manifold which allows vectors in neighboring tangent spaces to be compared. This extra structure is necessary because there is no canonical way to compare vectors from different vector spaces, as is necessary for this generalization of the directional derivative. There is however another generalization of directional derivatives which "is" canonical: the Lie derivative. The Lie derivative evaluates the change of one vector field along the flow of another vector field. Thus, one must know both vector fields in an open neighborhood. The covariant derivative on the other hand introduces its own change for vectors in a given direction, and it only depends on the vector direction at a single point, rather than a vector field in an open neighborhood of a point. In other words, the covariant derivative is linear (over "C"∞("M")) in the direction argument, while the Lie derivative is linear in neither argument.
Note that the antisymmetrized covariant derivative {{nowrap|∇"u""v" − ∇"v""u"}}, and the Lie derivative "L""u""v" differ by the torsion of the connection, so that if a connection is torsion free, then its antisymmetrization "is" the Lie derivative.

</doc>
<doc id="166008" url="https://en.wikipedia.org/wiki?curid=166008" title="Cramer's rule">
Cramer's rule

In linear algebra, Cramer's rule is an explicit formula for the solution of a system of linear equations with as many equations as unknowns, valid whenever the system has a unique solution. It expresses the solution in terms of the determinants of the (square) coefficient matrix and of matrices obtained from it by replacing one column by the vector of right hand sides of the equations. It is named after Gabriel Cramer (1704–1752), who published the rule for an arbitrary number of unknowns in 1750, although Colin Maclaurin also published special cases of the rule in 1748 (and possibly knew of it as early as 1729).
Cramer's rule is computationally very inefficient for systems of more than two or three equations; its asymptotic complexity is O(n·n!) compared to elimination methods that have polynomial time complexity. Cramer's rule is also numerically unstable even for 2×2 systems.
General case.
Consider a system of linear equations for unknowns, represented in matrix multiplication form as follows:
where the matrix has a nonzero determinant, and the vector formula_2 is the column vector of the variables. Then the theorem states that in this case the system has a unique solution, whose individual values for the unknowns are given by:
where formula_4 is the matrix formed by replacing the -th column of by the column vector .
A more general version of Cramer's rule considers the matrix equation
where the matrix has a nonzero determinant, and , are matrices. Given sequences formula_6 and formula_7, let formula_8 be the submatrix of with rows in formula_9 and columns in formula_10. Let formula_11 be the matrix formed by replacing the formula_12 column of by formula_13 column of B, for all formula_14. Then
In the case formula_16, this reduces to the normal Cramer's rule.
The rule holds for systems of equations with coefficients and unknowns in any field, not just in the real numbers. It has recently been shown that Cramer's rule can be implemented in O("n"3) time, which is comparable to more common methods of solving systems of linear equations, such as Gaussian elimination (consistently requiring 2.5 times as many arithmetic operations for all matrix sizes, while exhibiting comparable numeric stability in most cases).
Proof.
The proof for Cramer's rule uses just two properties of determinants: linearity with respect to any given column (taking for that column a linear combination of column vectors produces as determinant the corresponding linear combination of their determinants), and the fact that the determinant is zero whenever two columns are equal (which is implied by the basic property that the sign of the determinant flips if you switch two columns).
Fix the index "j" of a column. Linearity means that if we consider only column "j" as variable (fixing the others arbitrarily), the resulting function (assuming matrix entries are in ) can be given by a matrix, with one row and "n" columns, that acts on column "j". In fact this is precisely what Laplace expansion does, writing for certain coefficients "C"1, ..., "Cn" that depend on the columns of other than column "j" (the precise expression for these cofactors is not important here). The value is then the result of applying the one-line matrix to column "j" of . If is applied to any "other" column "k" of , then the result is the determinant of the matrix obtained from by replacing column "j" by a copy of column "k", so the resulting determinant is 0 (the case of two equal columns).
Now consider a system of linear equations in unknowns formula_17, whose coefficient matrix is , with det("A") assumed to be nonzero:
If one combines these equations by taking "C"1 times the first equation, plus "C"2 times the second, and so forth until "C""n" times the last, then the coefficient of will become , while the coefficients of all other unknowns become 0; the left hand side becomes simply det("A")"xj". The right hand side is , which is applied to the column vector b of the right hand sides . In fact what has been done here is multiply the matrix equation on the left by . Dividing by the nonzero number det("A") one finds the following equation, necessary to satisfy the system:
But by construction the numerator is the determinant of the matrix obtained from by replacing column "j" by b, so we get the expression of Cramer's rule as a necessary condition for a solution. The same procedure can be repeated for other values of "j" to find values for the other unknowns.
The only point that remains to prove is that these values for the unknowns, the only possible ones, do indeed together form a solution. But if the matrix is invertible with inverse , then will be a solution, thus showing its existence. To see that is invertible when det("A") is nonzero, consider the matrix "M" obtained by stacking the one-line matrices on top of each other for "j" = 1, ..., "n" (this gives the adjugate matrix for ). It was shown that where appears at the position "j"; from this it follows that . Therefore
completing the proof.
For other proofs, see below.
Finding inverse matrix.
Let be an matrix. Then
where Adj("A") denotes the adjugate matrix of , is the determinant, and "I" is the identity matrix. If det("A") is invertible in "R", then the inverse matrix of is
If "R" is a field (such as the field of real numbers), then this gives a formula for the inverse of , provided . In fact, this formula will work whenever "R" is a commutative ring, provided that det("A") is a unit. If det("A") is not a unit, then is not invertible.
Applications.
Explicit formulas for small systems.
Consider the linear system
which in matrix format is
Assume nonzero. Then, with help of determinants and can be found with Cramer's rule as
The rules for matrices are similar. Given
which in matrix format is
formula_27
Then the values of and can be found as follows:
Differential geometry.
Cramer's rule is also extremely useful for solving problems in differential geometry. Consider the two equations formula_29 and formula_30. When "u" and "v" are independent variables, we can define formula_31 and formula_32
Finding an equation for formula_33 is a trivial application of Cramer's rule.
First, calculate the first derivatives of "F", "G", "x", and "y":
Substituting "dx", "dy" into "dF" and "dG", we have:
Since "u", "v" are both independent, the coefficients of "du", "dv" must be zero. So we can write out equations for the coefficients:
Now, by Cramer's rule, we see that:
This is now a formula in terms of two Jacobians:
Similar formulas can be derived for formula_39
Integer programming.
Cramer's rule can be used to prove that an integer programming problem whose constraint matrix is totally unimodular and whose right-hand side is integer, has integer basic solutions. This makes the integer program substantially easier to solve.
Ordinary differential equations.
Cramer's rule is used to derive the general solution to an inhomogeneous linear differential equation by the method of variation of parameters.
Geometric interpretation.
Cramer's rule has a geometric interpretation that can be considered also a proof or simply giving insight about its geometric nature. These geometric arguments work in general and not only in the case of two equations with two unknowns presented here.
Given the system of equations
it can be considered as an equation between vectors
The area of the parallelogram determined by formula_42 and formula_43 is given by the determinant of the system of equations:
In general, when there are more variables and equations, the determinant of vectors of length will give the "volume" of the "parallelepiped" determined by those vectors in the -th dimensional Euclidean space.
Therefore the area of the parallelogram determined by formula_45 and formula_43 has to be formula_47 times the area of the first one since one of the sides has been multiplied by this factor. Now, this last parallelogram, by Cavalieri's principle, has the same area as the parallelogram determined by formula_48 and formula_43.
Equating the areas of this last and the second parallelogram gives the equation
from which Cramer's rule follows.
Other proofs.
A short proof.
A short proof of Cramer's rule can be given by noticing that formula_47 is the determinant of the matrix
On the other hand, assuming that our original matrix is invertible, this matrix formula_53 has columns formula_54, where formula_55 is the -th column of the matrix . Recall that the matrix formula_56 has columns formula_57. Hence we have
The proof for other formula_59 is similar.
Proof using Clifford algebra.
Consider the system of three scalar equations in three unknown scalars formula_60
and assign an orthonormal vector basis formula_62 for formula_63 as
Let the vectors
Adding the system of equations, it is seen that
Using the exterior product, each unknown scalar formula_67 can be solved as
For equations in unknowns, the solution for the -th unknown formula_67 generalizes to
If are linearly independent, then the formula_67 can be expressed in determinant form identical to Cramer’s Rule as
where denotes the substitution of vector with vector in the -th numerator position.
Incompatible and indeterminate cases.
A system of equations is said to be incompatible or inconsistent when there are no solutions and it is called indeterminate when there is more than one solution. For linear equations, an indeterminate system will have infinitely many solutions (if it is over an infinite field), since the solutions can be expressed in terms of one or more parameters that can take arbitrary values.
Cramer's rule applies to the case where the coefficient determinant is nonzero. In the 2 × 2 case, if the coefficient determinant is zero, then the system is incompatible if the numerator determinants are nonzero, or indeterminate if the numerator determinants are zero.
For 3x3 or higher systems, the only thing one can say when the coefficient determinant equals zero is that if any of the numerator determinants are nonzero, then the system must be incompatible. However, having all determinants zero does not imply that the system is indeterminate. A simple example where all determinants vanish (equal zero) but the system is still incompatible is the 3x3 system x+y+z=1, x+y+z=2, x+y+z=3.

</doc>
<doc id="701991" url="https://en.wikipedia.org/wiki?curid=701991" title="Creation and annihilation operators">
Creation and annihilation operators

Creation and annihilation operators are mathematical operators that have widespread applications in quantum mechanics, notably in the study of quantum harmonic oscillators and many-particle systems. An annihilation operator lowers the number of particles in a given state by one. A creation operator increases the number of particles in a given state by one, and it is the adjoint of the annihilation operator. In many subfields of physics and chemistry, the use of these operators instead of wavefunctions is known as second quantization.
Creation and annihilation operators can act on states of various types of particles. For example, in quantum chemistry and many-body theory the creation and annihilation operators often act on electron states.
They can also refer specifically to the ladder operators for the quantum harmonic oscillator. In the latter case, the raising operator is interpreted as a creation operator, adding a quantum of energy to the oscillator system (similarly for the lowering operator). They can be used to represent phonons.
The mathematics for the creation and annihilation operators for bosons is the same as for the ladder operators of the quantum harmonic oscillator. For example, the commutator of the creation and annihilation operators that are associated with the same boson state equals one, while all other commutators vanish. However, for fermions the mathematics is different, involving anticommutators instead of commutators.
Ladder operators for the quantum harmonic oscillator.
In the context of the quantum harmonic oscillator, we reinterpret the ladder operators as creation and annihilation operators, adding or subtracting fixed quanta of energy to the oscillator system. Creation/annihilation operators are different for bosons (integer spin) and fermions (half-integer spin). This is because their wavefunctions have different symmetry properties.
First consider the simpler bosonic case of the phonons of the quantum harmonic oscillator.
Start with the Schrödinger equation for the one-dimensional time independent quantum harmonic oscillator
Make a coordinate substitution to nondimensionalize the differential equation
and the Schrödinger equation for the oscillator becomes
Note that the quantity formula_4 is the same energy as that found for light quanta and that the parenthesis in the Hamiltonian can be written as
The last two terms can be simplified by considering their effect on an arbitrary differentiable function f(q),
which implies,
Therefore
and the Schrödinger equation for the oscillator becomes, with substitution of the above and rearrangement of the factor of 1/2,
If we define
then the Schrödinger equation for the oscillator becomes
This is "significantly" simpler than the original form. Further simplifications of this equation enables one to derive all the properties listed above thus far.
Letting formula_13, where "p" is the nondimensionalized momentum operator
then we have
and
Note that these imply that
The operators formula_18 and formula_19 may be contrasted with normal operators, which commute with their adjoints. A normal operator has a representation formula_20 where formula_21 are self-adjoint and commute, i.e. formula_22. By contrast, formula_18 has the representation formula_24 where formula_25 are self-adjoint but formula_26. Then formula_27 and formula_28 have a common set of eigenfunctions (and are simultaneously diagonalizable), whereas p and q famously don't and aren't.
Despite this, we go on. Using the commutation relations given above, the Hamiltonian operator can be expressed as
One can compute the commutation relations between the formula_18 and formula_19 operators and the Hamiltonian:
These relations can be used to easily find all the energy eigenstates of the quantum harmonic oscillator. Assuming that formula_34 is an eigenstate of the Hamiltonian formula_35. Using these commutation relations, it follows that
This shows that formula_38 and formula_39 are also eigenstates of the Hamiltonian, with eigenvalues formula_40 and formula_41 respectively. This identifies the operators formula_18 and formula_19 as "lowering" and "raising" operators between the eigenstates. The energy difference between adjacent eigenstates is formula_44.
The ground state can be found by assuming that the lowering operator possesses a nontrivial kernel, formula_45 with formula_46. Using the formula above for the Hamiltonian,
one obtains
so formula_48 is an eigenfunction of the Hamiltonian. This gives the ground state energy formula_49. This allows one to identify the energy eigenvalue of any eigenstate formula_34 as
Furthermore, it turns out that the first-mentioned operator in (*), the number operator formula_52 plays a most important role in applications, while the second one, formula_53 can simply be replaced by formula_54 So one simply gets
Explicit eigenfunctions.
The ground state formula_56 of the quantum harmonic oscillator can be found by imposing the condition that
Written out as a differential equation, the wavefunction satisfies
which has the solution
The normalization constant "C" is found to be  formula_60  from formula_61,  using the Gaussian integral. Explicit formulas for all the eigenfunctions can now be found by repeated application of formula_62 to formula_63. This, and further operator formalism, can be found in Glimm and Jaffe, Quantum Physics, pp. 12-20.
Matrix representation.
The matrix expression of the creation and annihilation operators of the quantum harmonic oscillator with respect to the above orthonormal basis is
These can be obtained via the relationships
formula_66 and
formula_67. The eigenvectors formula_68 are those of the quantum harmonic oscillator, and are sometimes called the "number basis".
Generalized creation and annihilation operators.
The operators derived above are actually a specific instance of a more generalized notion of creation and annihilation operators. The more abstract form of the operators are constructed as follows. Let "H" be a one-particle Hilbert space (that is, any Hilbert space, viewed as representing the state of a single particle). 
The (bosonic) CCR algebra over "H" is the algebra-with-conjugation-operator (called "*") abstractly generated by elements "a"("f"), where "f" ranges freely over "H", subject to the relations
where we are using bra–ket notation. The map "a" : "f" ↦ "a"("f") from "H" to the bosonic CCR algebra is required to be complex antilinear (this adds more relations). Its adjoint is "a"†("f"), and the map "f" ↦ "a"†("f") is complex linear in "H". Thus "H" embeds as a complex vector subspace of its own CCR algebra. In a representation of this algebra, the element "a"("f") will be realized as an annihilation operator, and "a"†("f") as a creation operator.
In general, the CCR algebra is infinite dimensional. If we take a Banach space completion, it becomes a C* algebra. The CCR algebra over "H" is closely related to, but not identical to, a Weyl algebra.
For fermions, the (fermionic) CAR algebra over "H" is constructed similarly, but using anticommutator relations instead, namely
The CAR algebra is finite dimensional only if "H" is finite dimensional. If we take a Banach space completion (only necessary in the infinite dimensional case), it becomes a C* algebra. The CAR algebra is closely related to, but not identical to, a Clifford algebra.
Physically speaking, "a"("f") removes (i.e. annihilates) a particle in the state whereas "a"†("f") creates a particle in the state .
The free field vacuum state is the state with no particles, characterized by
If is normalized so that = 1, then "N" = "a"†("f") "a"("f") gives the number of particles in the state .
Creation and annihilation operators for reaction-diffusion equations.
The annihilation and creation operator description has also been useful to analyze classical reaction diffusion equations, such as the situation when a gas of molecules "A" diffuse and interact on contact, forming an inert product: To see how this kind of reaction can be described by the annihilation and creation operator formalism, consider formula_74 particles at a site formula_75 on a 1-d lattice. Each particle moves to the right or left with a certain probability, and each pair of particles at the same site annihilates each other with a certain other probability. 
The probability that one particle leaves the site during the short time period formula_76 is proportional to formula_77, let us say a probability formula_78 to hop left and formula_78 to hop right. All formula_80 particles will stay put with a probability formula_81. (Since formula_76 is so short, the probability that two or more will leave during formula_76 is very small and will be ignored.)
We can now describe the occupation of particles on the lattice as a `ket' of the form . It represents the juxtaposition (or conjunction, or tensor product) of the number states ..., , , , ... located at the individual sites of the lattice. A slight modification of the annihilation and creation operators is needed so that
and
for all "n≥0". This modification preserves the commutation relation
Now let "ai=a πi", where "πi" selects the "i"th component of ψ. That is, "ai" makes a copy of the state in an abstract place and then applies "a" to it. Then "ai†=ιi a†", where "ιi" inserts an abstract state at the "i"th site. Thus, for example, the net effect of "ai-1†ai" is to move an eigenstate from the "i"th to the ("i-1")th site while multiplying with the appropriate factor.
This allows us to write the pure diffusive behaviour of the particles as
where the sum is over "i". 
The reaction term can be deduced by noting that formula_88 particles can interact in formula_89 different ways, so that the probability that a pair annihilates is formula_90, yielding a term
where number state "n" is replaced by number state "n-2" at site "i" at a certain rate. Thus the state evolves by 
Other kinds of interactions can be included in a similar manner.
This kind of notation allows the use of quantum field theoretic techniques to be used in the analysis of reaction diffusion systems.
Creation and annihilation operators in quantum field theories.
In quantum field theories and many-body problems one works with creation and annihilation operators of quantum states, formula_93 and formula_94. These operators change the eigenvalues of the number operator,
by one, in analogy to the harmonic oscillator. The indices (such as formula_75) represent quantum numbers that label the single-particle states of the system; hence, they are not necessarily single numbers. For example, a tuple of quantum numbers formula_97 is used to label states in the hydrogen atom.
The commutation relations of creation and annihilation operators in a multiple-boson system are,
where formula_100 is the commutator and formula_101 is the Kronecker delta.
For fermions, the commutator is replaced by the anticommutator formula_102,
Therefore, exchanging disjoint (i.e. formula_105) operators in a product of creation of annihilation operators will reverse the sign in fermion systems, but not in boson systems.
If the states labelled by "i" are an orthonormal basis of a Hilbert space "H", then the result of this construction coincides with the CCR algebra and CAR algebra construction in the previous section but one. If they represent "eigenvectors" corresponding to the continuous spectrum of some operator, as for unbound particles in QFT, then the interpretation is more subtle.

</doc>
<doc id="157092" url="https://en.wikipedia.org/wiki?curid=157092" title="Cross product">
Cross product

In mathematics and vector calculus, the cross product or vector product (occasionally directed area product to emphasize the geometric significance) is a binary operation on two vectors in three-dimensional space (R3) and is denoted by the symbol ×. Given two linearly independent vectors a and b, the cross product, a × b, is a vector that is perpendicular to both and therefore normal to the plane containing them. It has many applications in mathematics, physics, engineering, and computer programming. It should not be confused with dot product (projection product).
If two vectors have the same direction (or have the exact opposite direction from one another, i.e. are "not" linearly independent) or if either one has zero length, then their cross product is zero. More generally, the magnitude of the product equals the area of a parallelogram with the vectors for sides; in particular, the magnitude of the product of two perpendicular vectors is the product of their lengths. The cross product is anticommutative (i.e. ) and is distributive over addition (i.e. ). The space R3 together with the cross product is an algebra over the real numbers, which is neither commutative nor associative, but is a Lie algebra with the cross product being the Lie bracket.
Like the dot product, it depends on the metric of Euclidean space, but unlike the dot product, it also depends on a choice of orientation or "handedness". The product can be generalized in various ways; it can be made independent of orientation by changing the result to pseudovector, or in arbitrary dimensions the exterior product of vectors can be used with a bivector or two-form result. Also, using the orientation and metric structure just as for the traditional 3-dimensional cross product, one can in "n" dimensions take the product of vectors to produce a vector perpendicular to all of them. But if the product is limited to non-trivial binary products with vector results, it exists only in three and seven dimensions. If one adds the further requirement that the product be uniquely defined, then only the 3-dimensional cross product qualifies. (See § Generalizations, below, for other dimensions.) 
Definition.
The cross product of two vectors a and b is defined only in three-dimensional space and is denoted by . In physics, sometimes the notation is used, though this is avoided in mathematics to avoid confusion with the exterior product.
The cross product is defined as a vector c that is perpendicular to both a and b, with a direction given by the right-hand rule and a magnitude equal to the area of the parallelogram that the vectors span.
The cross product is defined by the formula
where "θ" is the angle between a and b in the plane containing them (hence, it is between 0° and 180°), ‖a‖ and ‖b‖ are the magnitudes of vectors a and b, and n is a unit vector perpendicular to the plane containing a and b in the direction given by the right-hand rule (illustrated). If the vectors a and b are parallel (i.e., the angle "θ" between them is either 0° or 180°), by the above formula, the cross product of a and b is the zero vector 0.
By convention, the direction of the vector n is given by the right-hand rule, where one simply points the forefinger of the right hand in the direction of a and the middle finger in the direction of b. Then, the vector n is coming out of the thumb (see the picture on the right). Using this rule implies that the cross-product is anti-commutative, i.e., . By pointing the forefinger toward b first, and then pointing the middle finger toward a, the thumb will be forced in the opposite direction, reversing the sign of the product vector.
Using the cross product requires the handedness of the coordinate system to be taken into account (as explicit in the definition above). If a left-handed coordinate system is used, the direction of the vector n is given by the left-hand rule and points in the opposite direction.
This, however, creates a problem because transforming from one arbitrary reference system to another (e.g., a mirror image transformation from a right-handed to a left-handed coordinate system), should not change the direction of n. The problem is clarified by realizing that the cross product of two vectors is not a (true) vector, but rather a "pseudovector". See cross product and handedness for more detail.
Names.
In 1881, Josiah Willard Gibbs, and independently Oliver Heaviside, introduced both the dot product and the cross product using a period () and an "x" (), respectively, to denote them.
In 1877, to emphasize the fact that the result of a dot product is a scalar while the result of a cross product is a vector, William Kingdon Clifford coined the alternative names scalar product and vector product for the two operations. These alternative names are still widely used in the literature.
Both the cross notation () and the name cross product were possibly inspired by the fact that each scalar component of is computed by multiplying non-corresponding components of a and b. Conversely, a dot product involves multiplications between corresponding components of a and b. As explained below, the cross product can be expressed in the form of a determinant of a special 3×3 matrix. According to Sarrus' rule, this involves multiplications between matrix elements identified by crossed diagonals.
Computing the cross product.
Coordinate notation.
The standard basis vectors i, j, and k satisfy the following equalities in a right hand coordinate system:
which imply, by the anticommutativity of the cross product, that 
The definition of the cross product also implies that
These equalities, together with the distributivity and linearity of the cross product (but both do not follow easily from the definition given above), are sufficient to determine the cross product of any two vectors u and v. Each vector can be defined as the sum of three orthogonal components parallel to the standard basis vectors:
Their cross product can be expanded using distributivity:
This can be interpreted as the decomposition of into the sum of nine simpler cross products involving vectors aligned with i, j, or k. Each one of these nine cross products operates on two vectors that are easy to handle as they are either parallel or orthogonal to each other. From this decomposition, by using the above-mentioned equalities and collecting similar terms, we obtain:
meaning that the three scalar components of the resulting vector s = "s"1i + "s"2j + "s"3k = are
Using column vectors, we can represent the same result as follows:
Matrix notation.
The cross product can also be expressed as the formal determinant:
This determinant can be computed using Sarrus' rule or cofactor expansion.
Using Sarrus' rule, it expands to
Using cofactor expansion along the first row instead, it expands to
which gives the components of the resulting vector directly.
Properties.
Geometric meaning.
The magnitude of the cross product can be interpreted as the positive area of the parallelogram having a and b as sides (see Figure 1):
Indeed, one can also compute the volume "V" of a parallelepiped having a, b and c as edges by using a combination of a cross product and a dot product, called scalar triple product (see Figure 2):
Since the result of the scalar triple product may be negative, the volume of the parallelepiped is given by its absolute value. For instance,
Because the magnitude of the cross product goes by the sine of the angle between its arguments, the cross product can be thought of as a measure of "perpendicularity" in the same way that the dot product is a measure of "parallelism". Given two unit vectors, their cross product has a magnitude of 1 if the two are perpendicular and a magnitude of zero if the two are parallel. The converse is true for the dot product of two unit vectors.
Unit vectors enable two convenient identities: the dot product of two unit vectors yields the cosine (which may be positive or negative) of the angle between the two unit vectors. The magnitude of the cross product of the two unit vectors yields the sine (which will always be positive).
Algebraic properties.
Distributivity, linearity and Jacobi identity show that the R3 vector space together with vector addition and the cross product forms a Lie algebra, the Lie algebra of the real orthogonal group in 3 dimensions, SO(3).
From the definition of the cross product, the angle between a and must be zero, and these vectors must be parallel. That is, they are related by a scale factor "t", leading to:
for some scalar "t".
so is both parallel and perpendicular to the non-zero vector "a", something that is only possible if so they are identical.
More generally, the cross product obeys the following identity under matrix transformations:
where formula_28 is a 3-by-3 matrix and formula_29 is the transpose of the inverse. It can be readily seen how this formula reduces to the former one if formula_28 is a rotation matrix.
Differentiation.
The product rule of differential calculus applies to any bilinear operation, and therefore also to the cross product:
where a and b are vectors that depend on the real variable "t".
Triple product expansion.
The cross product is used in both forms of the triple product. The scalar triple product of three vectors is defined as
It is the signed volume of the parallelepiped with edges a, b and c and as such the vectors can be used in any order that's an even permutation of the above ordering. The following therefore are equal:
The vector triple product is the cross product of a vector with the result of another cross product, and is related to the dot product by the following formula
The mnemonic "BAC minus CAB" is used to remember the order of the vectors in the right hand member. This formula is used in physics to simplify vector calculations. A special case, regarding gradients and useful in vector calculus, is
where ∇2 is the vector Laplacian operator.
Other identities relate the cross product to the scalar triple product:
where I is the identity matrix.
Alternative formulation.
The cross product and the dot product are related by:
The right-hand side is the Gram determinant of a and b, the square of the area of the parallelogram defined by the vectors. This condition determines the magnitude of the cross product. Namely, since the dot product is defined, in terms of the angle "θ" between the two vectors, as:
the above given relationship can be rewritten as follows:
Invoking the Pythagorean trigonometric identity one obtains:
which is the magnitude of the cross product expressed in terms of "θ", equal to the area of the parallelogram defined by a and b (see definition above).
The combination of this requirement and the property that the cross product be orthogonal to its constituents a and b provides an alternative definition of the cross product.
Lagrange's identity.
The relation:
can be compared with another relation involving the right-hand side, namely Lagrange's identity expressed as:
where a and b may be "n"-dimensional vectors. This also shows that the Riemannian volume form for surfaces is exactly the surface element from vector calculus. In the case where , combining these two equations results in the expression for the magnitude of the cross product in terms of its components:
The same result is found directly using the components of the cross-product found from:
In R3 Lagrange's equation is a special case of the multiplicativity |vw| = |v||w| of the norm in the quaternion algebra.
It is a special case of another formula, also sometimes called Lagrange's identity, which is the three dimensional case of the Binet-Cauchy identity:
If and this simplifies to the formula above.
Infinitesimal generators of rotations.
The cross product conveniently describes the infinitesimal generators of rotations in R3. Specifically, if n is a unit vector in R3 and "R"(φ, n) denotes a rotation about the axis through the origin specified by n, with angle φ (measured in radians, counterclockwise when viewed from the tip of n), then
for every vector x in R3. The cross product with n therefore describes the infinitesimal generator of the rotations about n. These infinitesimal generators form the Lie algebra so(3) of the rotation group SO(3), and we obtain the result that the Lie algebra R3 with cross product is isomorphic to the Lie algebra so(3).
Alternative ways to compute the cross product.
Conversion to matrix multiplication.
The vector cross product also can be expressed as the product of a skew-symmetric matrix and a vector:
where superscript T refers to the transpose operation, and [a]× is defined by:
It should be noted that [a]× is a singular matrix where a is its (right and left) null-vector.
Also, if a is itself a cross product:
then
This result can be generalized to higher dimensions using geometric algebra. In particular in any dimension bivectors can be identified with skew-symmetric matrices, so the product between a skew-symmetric matrix and vector is equivalent to the grade-1 part of the product of a bivector and vector. In three dimensions bivectors are dual to vectors so the product is equivalent to the cross product, with the bivector instead of its vector dual. In higher dimensions the product can still be calculated but bivectors have more degrees of freedom and are not equivalent to vectors.
This notation is also often much easier to work with, for example, in epipolar geometry.
From the general properties of the cross product follows immediately that
and from fact that [a]× is skew-symmetric it follows that
The above-mentioned triple product expansion (bac–cab rule) can be easily proven using this notation.
As mentioned above, the Lie algebra R3 with cross product is isomorphic to the Lie algebra so(3), whose elements can be identified with the 3×3 skew-symmetric matrices. The map a → [a]× provides an isomorphism between R3 and so(3). Under this map, the cross product of 3-vectors corresponds to the commutator of 3x3 skew-symmetric matrices.
Index notation for tensors.
The cross product can alternatively be defined in terms of the Levi-Civita symbol εijk and a dot product "ηmi" (= δ"mi" for an orthonormal basis), which are useful in converting vector notation for tensor applications:
where the indices formula_59 correspond to vector components. This characterization of the cross product is often expressed more compactly using the Einstein summation convention as
in which repeated indices are summed over the values 1 to 3. Note that this representation is another form of the skew-symmetric representation of the cross product:
In classical mechanics: representing the cross-product by using the Levi-Civita symbol can cause mechanical symmetries to be obvious when physical systems are isotropic. (An example: consider a particle in a Hooke's Law potential in three-space, free to oscillate in three dimensions; none of these dimensions are "special" in any sense, so symmetries lie in the cross-product-represented angular momentum, which are made clear by the abovementioned Levi-Civita representation).
Mnemonic.
The word "xyzzy" can be used to remember the definition of the cross product.
If
where:
then:
The second and third equations can be obtained from the first by simply vertically rotating the subscripts, . The problem, of course, is how to remember the first equation, and two options are available for this purpose: either to remember the relevant two diagonals of Sarrus's scheme (those containing i), or to remember the xyzzy sequence.
Since the first diagonal in Sarrus's scheme is just the main diagonal of the above-mentioned 3×3 matrix, the first three letters of the word xyzzy can be very easily remembered.
Cross visualization.
Similarly to the mnemonic device above, a "cross" or X can be visualized between the two vectors in the equation. This may be helpful for remembering the correct cross product formula.
If
then:
If we want to obtain the formula for formula_69 we simply drop the formula_70 and formula_71 from the formula, and take the next two components down:
It should be noted that when doing this for formula_73 the next two elements down should "wrap around" the matrix so that after the z component comes the x component. For clarity, when performing this operation for formula_73, the next two components should be z and x (in that order). While for formula_75 the next two components should be taken as x and y.
For formula_69 then, if we visualize the cross operator as pointing from an element on the left to an element on the right, we can take the first element on the left and simply multiply by the element that the cross points to in the right hand matrix. We then subtract the next element down on the left, multiplied by the element that the cross points to here as well. This results in our formula_69 formula –
We can do this in the same way for formula_73 and formula_75 to construct their associated formulas.
Applications.
The cross product has applications in various contexts: e.g. it is used in computational geometry, physics and engineering.
A non-exhaustive list of examples follows.
Computational geometry.
The cross product appears in the calculation of the distance of two skew lines (lines not in the same plane) from each other in three-dimensional space.
The cross product can be used to calculate the normal for a triangle or polygon, an operation frequently performed in computer graphics. For example, the winding of a polygon (clockwise or anticlockwise) about a point within the polygon can be calculated by triangulating the polygon (like spoking a wheel) and summing the angles (between the spokes) using the cross product to keep track of the sign of each angle.
In computational geometry of the plane, the cross product is used to determine the sign of the acute angle defined by three points formula_82, formula_83 and formula_84. It corresponds to the direction of the cross product of the two coplanar vectors defined by the pairs of points formula_85 and formula_86, i.e., by the sign of the expression formula_87. In the "right-handed" coordinate system, if the result is 0, the points are collinear; if it is positive, the three points constitute a positive angle of rotation around formula_88 from formula_89 to formula_90, otherwise a negative angle. From another point of view, the sign of formula_91 tells whether formula_90 lies to the left or to the right of line formula_85.
The cross product is used in calculating the volume of a polyhedron such as a tetrahedron or parallelepiped.
Angular momentum and torque.
The angular momentum formula_94 of
a particle about a given origin is defined as:
where formula_96 is the position vector of the particle relative to the origin, formula_97 is the linear momentum of the particle.
In the same way, the moment formula_98 of a force
formula_99 applied at point B around point A is given as:
In mechanics the "moment of a force" is also called "torque" and written as formula_101
Since position formula_96, linear momentum formula_97
and force formula_104 are all "true" vectors,
both the angular momentum formula_94
and the moment of a force formula_98
are "pseudovectors" or "axial vectors".
Rigid body.
The cross product frequently appears in the description of rigid motions.
Two points "P" and "Q" on a rigid body can be related by:
where formula_96 is the point's position, formula_109 is its velocity
and formula_110 is the body's angular velocity.
Since position formula_96
and velocity formula_109
are "true" vectors,
the angular velocity formula_110
is a "pseudovector" or "axial vector".
Lorentz force.
The cross product is used to describe the Lorentz force experienced by a moving electric charge formula_114:
Since velocity formula_109, force formula_104
and electric field formula_118 are all "true" vectors, 
the magnetic field formula_119 is a "pseudovector".
Other.
In vector calculus, the cross product is used to define the formula for the vector operator curl.
The trick of rewriting a cross product in terms of a matrix multiplication appears frequently in epipolar and multi-view geometry, in particular when deriving matching constraints.
Cross product as an exterior product.
The cross product can be viewed in terms of the exterior product. This view allows for a natural geometric interpretation of the cross product. In exterior algebra the exterior product (or wedge product) of two vectors is a bivector. A bivector is an oriented plane element, in much the same way that a vector is an oriented line element. Given two vectors "a" and "b", one can view the bivector as the oriented parallelogram spanned by "a" and "b". The cross product is then obtained by taking the Hodge dual of the bivector , mapping 2-vectors to vectors:
This can be thought of as the oriented multi-dimensional element "perpendicular" to the bivector. Only in three dimensions is the result an oriented line element – a vector – whereas, for example, in 4 dimensions the Hodge dual of a bivector is two-dimensional – another oriented plane element. So, only in three dimensions is the cross product of "a" and "b" the vector dual to the bivector : it is perpendicular to the bivector, with orientation dependent on the coordinate system's handedness, and has the same magnitude relative to the unit normal vector as has relative to the unit bivector; precisely the properties described above.
Cross product and handedness.
When measurable quantities involve cross products, the "handedness" of the coordinate systems used cannot be arbitrary. However, when physics laws are written as equations, it should be possible to make an arbitrary choice of the coordinate system (including handedness). To avoid problems, one should be careful to never write down an equation where the two sides do not behave equally under all transformations that need to be considered. For example, if one side of the equation is a cross product of two vectors, one must take into account that when the handedness of the coordinate system is "not" fixed a priori, the result is not a (true) vector but a pseudovector. Therefore, for consistency, the other side must also be a pseudovector.
More generally, the result of a cross product may be either a vector or a pseudovector, depending on the type of its operands (vectors or pseudovectors). Namely, vectors and pseudovectors are interrelated in the following ways under application of the cross product:
So by the above relationships, the unit basis vectors i, j and k of an orthonormal, right-handed (Cartesian) coordinate frame must all be pseudovectors (if a basis of mixed vector types is disallowed, as it normally is) since , and .
Because the cross product may also be a (true) vector, it may not change direction with a mirror image transformation. This happens, according to the above relationships, if one of the operands is a (true) vector and the other one is a pseudovector (e.g., the cross product of two vectors). For instance, a vector triple product involving three (true) vectors is a (true) vector.
A handedness-free approach is possible using exterior algebra.
Generalizations.
There are several ways to generalize the cross product to the higher dimensions.
Lie algebra.
The cross product can be seen as one of the simplest Lie products,
and is thus generalized by Lie algebras, which are axiomatized as binary products satisfying the axioms of multilinearity, skew-symmetry, and the Jacobi identity. Many Lie algebras exist, and their study is a major field of mathematics, called Lie theory.
For example, the Heisenberg algebra gives another Lie algebra structure on formula_121 In the basis formula_122 the product is formula_123
Quaternions.
The cross product can also be described in terms of quaternions, and this is why the letters i, j, k are a convention for the standard basis on R3. The unit vectors i, j, k correspond to "binary" (180 deg) rotations about their respective axes (Altmann, S. L., 1986, Ch. 12), said rotations being represented by "pure" quaternions (zero scalar part) with unit norms.
For instance, the above given cross product relations among i, j, and k agree with the multiplicative relations among the quaternions "i", "j", and "k". In general, if a vector is represented as the quaternion , the cross product of two vectors can be obtained by taking their product as quaternions and deleting the real part of the result. The real part will be the negative of the dot product of the two vectors.
Alternatively, using the above identification of the 'purely imaginary' quaternions with R3, the cross product may be thought of as half of the commutator of two quaternions.
Octonions.
A cross product for 7-dimensional vectors can be obtained in the same way by using the octonions instead of the quaternions. The nonexistence of nontrivial vector-valued cross products of two vectors in other dimensions is related to the result from Hurwitz's theorem that the only normed division algebras are the ones with dimension 1, 2, 4, and 8.
Wedge product.
In general dimension, there is no direct analogue of the binary cross product that yields specifically a vector. There is however the wedge product, which has similar properties, except that the wedge product of two vectors is now a 2-vector instead of an ordinary vector. As mentioned above, the cross product can be interpreted as the wedge product in three dimensions by using the Hodge dual to map 2-vectors to vectors. The Hodge dual of the wedge product yields an -vector, which is a natural generalization of the cross product in any number of dimensions.
The wedge product and dot product can be combined (through summation) to form the geometric product.
Multilinear algebra.
In the context of multilinear algebra, the cross product can be seen as the (1,2)-tensor (a mixed tensor, specifically a bilinear map) obtained from the 3-dimensional volume form, a (0,3)-tensor, by raising an index.
In detail, the 3-dimensional volume form defines a product formula_124 by taking the determinant of the matrix given by these 3 vectors.
By duality, this is equivalent to a function formula_125 (fixing any two inputs gives a function formula_126 by evaluating on the third input) and in the presence of an inner product (such as the dot product; more generally, a non-degenerate bilinear form), we have an isomorphism formula_127 and thus this yields a map formula_128 which is the cross product: a (0,3)-tensor (3 vector inputs, scalar output) has been transformed into a (1,2)-tensor (2 vector inputs, 1 vector output) by "raising an index".
Translating the above algebra into geometry, the function "volume of the parallelepiped defined by formula_129" (where the first two vectors are fixed and the last is an input), which defines a function formula_126, can be "represented" uniquely as the dot product with a vector: this vector is the cross product formula_131 From this perspective, the cross product is "defined" by the scalar triple product, formula_132
In the same way, in higher dimensions one may define generalized cross products by raising indices of the "n"-dimensional volume form, which is a formula_133-tensor.
The most direct generalizations of the cross product are to define either:
These products are all multilinear and skew-symmetric, and can be defined in terms of the determinant and parity.
The formula_136-ary product can be described as follows: given formula_135 vectors formula_141 in formula_142 define their generalized cross product formula_143 as:
This is the unique multilinear, alternating product which evaluates to formula_148, formula_149 and so forth for cyclic permutations of indices.
In coordinates, one can give a formula for this formula_136-ary analogue of the cross product in R"n" by:
This formula is identical in structure to the determinant formula for the normal cross product in R3 except that the row of basis vectors is the last row in the determinant rather than the first. The reason for this is to ensure that the ordered vectors (v1, ...,v"n"−1, Λ(v1, ...,v"n"−1)) have a positive orientation with respect to (e1, ..., e"n"). If "n" is odd, this modification leaves the value unchanged, so this convention agrees with the normal definition of the binary product. In the case that "n" is even, however, the distinction must be kept. This formula_136-ary form enjoys many of the same properties as the vector cross product: it is alternating and linear in its arguments, it is perpendicular to each argument, and its magnitude gives the hypervolume of the region bounded by the arguments. And just like the vector cross product, it can be defined in a coordinate independent way as the Hodge dual of the wedge product of the arguments.
Skew-symmetric matrix.
If the cross product is defined as a binary operation, it takes as "input" exactly two vectors. If its "output" is not required to be a vector or a pseudovector but instead a "matrix", then it can be generalized in an arbitrary number of dimensions.
In mechanics, for example, the angular velocity can be interpreted either as a pseudovector formula_153 or as a anti-symmetric matrix or skew-symmetric tensor formula_154. In the latter case, the velocity law for a rigid body looks:
where Ω is formally defined from the rotation matrix formula_156 associated to body's frame: formula_157. In three-dimensions holds:
In quantum Mechanics the angular momentum formula_159 is often represented as an anti-symmetric matrix or tensor. More precisely, it is the result of cross product involving position formula_160 and linear momentum formula_161:
Since both formula_160 and formula_161 can have an arbitrary number formula_165 of components, that kind of cross product can be extended to any dimension, holding the "physical" interpretation of the operation.
See for numerical details.
History.
In 1773, the Italian mathematician Joseph Louis Lagrange, (born Giuseppe Luigi Lagrancia), introduced the component form of both the dot and cross products in order to study the tetrahedron in three dimensions. In 1843 the Irish mathematical physicist Sir William Rowan Hamilton introduced the quaternion product, and with it the terms "vector" and "scalar". Given two quaternions and , where u and v are vectors in R3, their quaternion product can be summarized as . James Clerk Maxwell used Hamilton's quaternion tools to develop his famous electromagnetism equations, and for this and other reasons quaternions for a time were an essential part of physics education.
In 1878 William Kingdon Clifford published his Elements of Dynamic which was an advanced text for its time. He defined the product of two vectors to have magnitude equal to the area of the parallelogram of which they are two sides, and direction perpendicular to their plane.
Oliver Heaviside in England and Josiah Willard Gibbs, a professor at Yale University in Connecticut, also felt that quaternion methods were too cumbersome, often requiring the scalar or vector part of a result to be extracted. Thus, about forty years after the quaternion product, the dot product and cross product were introduced—to heated opposition. Pivotal to (eventual) acceptance was the efficiency of the new approach, allowing Heaviside to reduce the equations of electromagnetism from Maxwell's original 20 to the four commonly seen today.
Largely independent of this development, and largely unappreciated at the time, Hermann Grassmann created a geometric algebra not tied to dimension two or three, with the exterior product playing a central role. In 1853 Augustin-Louis Cauchy, a contemporary of Grassmann, published a paper on algebraic keys which were used to solve equations and had the same multiplication properties as the cross product. William Kingdon Clifford combined the algebras of Hamilton and Grassmann to produce Clifford algebra, where in the case of three-dimensional vectors the bivector produced from two vectors dualizes to a vector, thus reproducing the cross product.
The cross notation and the name "cross product" began with Gibbs. Originally they appeared in privately published notes for his students in 1881 as "Elements of Vector Analysis". The utility for mechanics was noted by Aleksandr Kotelnikov. Gibbs's notation and the name "cross product" later reached a wide audience through Vector Analysis, a textbook by Edwin Bidwell Wilson, a former student. Wilson rearranged material from Gibbs's lectures, together with material from publications by Heaviside, Föpps, and Hamilton. He divided vector analysis into three parts:
Two main kinds of vector multiplications were defined, and they were called as follows:
Several kinds of triple products and products of more than three vectors were also examined. The above-mentioned triple product expansion was also included.

</doc>
<doc id="6123" url="https://en.wikipedia.org/wiki?curid=6123" title="Curl (mathematics)">
Curl (mathematics)

In vector calculus, the curl is a vector operator that describes the infinitesimal rotation of a 3-dimensional vector field. At every point in the field, the curl of that point is represented by a vector. The attributes of this vector (length and direction) characterize the rotation at that point.
The direction of the curl is the axis of rotation, as determined by the right-hand rule, and the magnitude of the curl is the magnitude of rotation. If the vector field represents the flow velocity of a moving fluid, then the curl is the circulation density of the fluid. A vector field whose curl is zero is called irrotational.
The curl is a form of differentiation for vector fields. The corresponding form of the fundamental theorem of calculus is Stokes' theorem, which relates the surface integral of the curl of a vector field to the line integral of the vector field around the boundary curve.
The alternative terminology "rotor" or "rotational" and alternative notations rot F and ∇ × F are often used (the former especially in many European countries, the latter, using the del operator and the cross product, is more used in other countries) for "curl" and curl F.
Unlike the gradient and divergence, curl does not generalize as simply to other dimensions; some generalizations are possible, but only in three dimensions is the geometrically defined curl of a vector field again a vector field. This is a similar phenomenon as in the 3 dimensional cross product, and the connection is reflected in the notation ∇ × for the curl.
The name "curl" was first suggested by James Clerk Maxwell in 1871 but the concept was apparently first used in the construction of an optical field theory by James MacCullagh in 1839.
Definition.
The curl of a vector field F, denoted by curl F, or ∇ × F, or rot F, at a point is defined in terms of its projection onto various lines through the point. If formula_1 is any unit vector, the projection of the curl of F onto formula_1 is defined to be the limiting value of a closed line integral in a plane orthogonal to formula_1 as the path used in the integral becomes infinitesimally close to the point, divided by the area enclosed.
As such, the curl operator maps continuously differentiable functions f : R3 → R3 to continuous functions g : R3 → R3. In fact, it maps "C"k functions in R3 to "C"k-1 functions in R3. 
Implicitly, curl is defined by:
where formula_5 is a line integral along the boundary of the area in question, and |"A"| is the magnitude of the area. If formula_6 is an outward pointing in-plane normal, whereas formula_1 is the unit vector perpendicular to the plane (see caption at right), then the orientation of C is chosen so that a tangent vector formula_8 to C is positively oriented if and only if formula_9 forms a positively oriented basis for R3 (right-hand rule).
The above formula means that the curl of a vector field is defined as the infinitesimal area density of the "circulation" of that field. To this definition fit naturally 
Note that the equation for each component, formula_13 can be obtained by exchanging each occurrence of a subscript 1, 2, 3 in cyclic permutation: 1→2, 2→3, and 3→1 (where the subscripts represent the relevant indices).
If ("x"1, "x"2, "x"3) are the Cartesian coordinates and ("u"1,"u"2,"u"3) are the orthogonal coordinates, then 
is the length of the coordinate vector corresponding to "ui". The remaining two components of curl result from cyclic permutation of indices: 3,1,2 → 1,2,3 → 2,3,1.
Intuitive interpretation.
Suppose the vector field describes the velocity field of a fluid flow (such as a large tank of liquid or gas) and a small ball is located within the fluid or gas (the centre of the ball being fixed at a certain point). If the ball has a rough surface, the fluid flowing past it will make it rotate. The rotation axis (oriented according to the right hand rule) points in the direction of the curl of the field at the centre of the ball, and the angular speed of the rotation is half the magnitude of the curl at this point.
Usage.
In practice, the above definition is rarely used because in virtually all cases, the curl operator can be applied using some set of curvilinear coordinates, for which simpler representations have been derived.
The notation ∇ × F has its origins in the similarities to the 3 dimensional cross product, and it is useful as a mnemonic in Cartesian coordinates if ∇ is taken as a vector differential operator del. Such notation involving operators is common in physics and algebra. However, in certain coordinate systems, such as polar-toroidal coordinates (common in plasma physics), using the notation ∇ × F will yield an incorrect result.
Expanded in Cartesian coordinates (see Del in cylindrical and spherical coordinates for spherical and cylindrical coordinate representations), ∇ × F is, for F composed of ["F"x, "F"y, "F"z]:
where i, j, and k are the unit vectors for the "x"-, "y"-, and "z"-axes, respectively. This expands as follows:
Although expressed in terms of coordinates, the result is invariant under proper rotations of the coordinate axes but the result inverts under reflection.
In a general coordinate system, the curl is given by
where ε denotes the Levi-Civita tensor and formula_18 the covariant derivative, the metric tensor is used to lower the index on F, and the Einstein summation convention implies that repeated indices are summed over. Equivalently,
where e"k" are the coordinate vector fields. Equivalently, using the exterior derivative, the curl can be expressed as:
Here formula_21 and formula_22 are the musical isomorphisms, and formula_23 is the Hodge dual. This formula shows how to calculate the curl of F in any coordinate system, and how to extend the curl to any oriented three-dimensional Riemannian manifold. Since this depends on a choice of orientation, curl is a chiral operation. In other words, if the orientation is reversed, then the direction of the curl is also reversed.
Examples.
A simple vector field.
Take the vector field, which depends on "x" and "y" linearly:
Its plot looks like this:
Simply by visual inspection, we can see that the field is rotating. If we place a paddle wheel anywhere, we see immediately its tendency to rotate clockwise. Using the right-hand rule, we expect the curl to be into the page. If we are to keep a right-handed coordinate system, into the page will be in the negative z direction. The lack of x and y directions is analogous to the cross product operation.
If we calculate the curl:
Which is indeed in the negative "z" direction, as expected. In this case, the curl is actually a constant, irrespective of position. The "amount" of rotation in the above vector field is the same at any point ("x", "y"). Plotting the curl of "F" is not very interesting:
A more involved example.
Suppose we now consider a slightly more complicated vector field:
Its plot:
We might not see any rotation initially, but if we closely look at the right, we see a larger field at, say, x=4 than at x=3. Intuitively, if we placed a small paddle wheel there, the larger "current" on its right side would cause the paddlewheel to rotate clockwise, which corresponds to a curl in the negative z direction. By contrast, if we look at a point on the left and placed a small paddle wheel there, the larger "current" on its left side would cause the paddlewheel to rotate counterclockwise, which corresponds to a curl in the positive z direction. Let's check out our guess by doing the math:
Indeed the curl is in the positive z direction for negative x and in the negative z direction for positive x, as expected. Since this curl is not the same at every point, its plot is a bit more interesting:
We note that the plot of this curl has no dependence on y or z (as it shouldn't) and is in the negative z direction for positive x and in the positive z direction for negative x.
Identities.
In general curvilinear coordinates (not only in Cartesian coordinates), the curl of a cross product of vector fields v and F can be shown to be 
Interchanging the vector field v and ∇ operator, we arrive at the cross product of a vector field with curl of a vector field:
using the Feynman subscript notation, ∇F, which operates only on the vector field F.
Another example is the curl of a curl of a vector field. It can be shown that in general coordinates
and this identity defines the vector Laplacian of F, symbolized as ∇2F.
The curl of the gradient of "any" scalar field φ is always the zero vector field
which follows from the antisymmetry in the definition of the curl, and the symmetry of second derivatives.
If φ is a scalar valued function and F is a vector field, then
Generalizations.
The vector calculus operations of grad, curl, and div are most easily generalized and understood in the context of differential forms, which involves a number of steps. In a nutshell, they correspond to the derivatives of 0-forms, 1-forms, and 2-forms, respectively. The geometric interpretation of curl as rotation corresponds to identifying bivectors (2-vectors) in 3 dimensions with the special orthogonal Lie algebra so(3) of infinitesimal rotations (in coordinates, skew-symmetric 3 × 3 matrices), while representing rotations by vectors corresponds to identifying 1-vectors (equivalently, 2-vectors) and so(3), these all being 3-dimensional spaces.
Differential forms.
In 3 dimensions, a differential 0-form is simply a function "f"("x", "y", "z"); a differential 1-form is the following expression: formula_33 a differential 2-form is the formal sum: formula_34 and a differential 3-form is defined by a single term: formula_35 (Here the a-coefficients are real functions; the "wedge products", e.g. formula_36 can be interpreted as some kind of oriented area elements, formula_37, etc.) The exterior derivative of a "k"-form in R3 is defined as the ("k"+1)-form from above (and in R"n" if, e.g.,
then the exterior derivative "d" leads to
The exterior derivative of a 1-form is therefore a 2-form, and that of a 2-form is a 3-form. On the other hand, because of the interchangeability of mixed derivatives, e.g. because of
the twofold application of the exterior derivative leads to 0.
Thus, denoting the space of "k"-forms by formula_41 and the exterior derivative by "d" one gets a sequence:
Here formula_43 is the space of sections of the exterior algebra formula_44 vector bundle over Rn, whose dimension is the binomial coefficient formula_45 note that formula_46 for "k" > 3 or "k" < 0. Writing only dimensions, one obtains a row of Pascal's triangle:
the 1-dimensional fibers correspond to functions, and the 3-dimensional fibers to vector fields, as described below. Note that modulo suitable identifications, the three nontrivial occurrences of the exterior derivative correspond to grad, curl, and div.
Differential forms and the differential can be defined on any Euclidean space, or indeed any manifold, without any notion of a Riemannian metric. On a Riemannian manifold, or more generally pseudo-Riemannian manifold, "k"-forms can be identified with "k"-vector fields ("k"-forms are "k"-covector fields, and a pseudo-Riemannian metric gives an isomorphism between vectors and covectors), and on an "oriented" vector space with a nondegenerate form (an isomorphism between vectors and covectors), there is an isomorphism between "k"-vectors and ("n"−"k")-vectors; in particular on (the tangent space of) an oriented pseudo-Riemannian manifold. Thus on an oriented pseudo-Riemannian manifold, one can interchange "k"-forms, "k"-vector fields, ("n"−"k")-forms, and ("n"−"k")-vector fields; this is known as Hodge duality. Concretely, on R3 this is given by:
Thus, identifying 0-forms and 3-forms with functions, and 1-forms and 2-forms with vector fields:
On the other hand the fact that "d"2 = 0 corresponds to the identities curl grad "f" = 0 and formula_54 for any function "f" or vector field formula_55
Grad and div generalize to all oriented pseudo-Riemannian manifolds, with the same geometric interpretation, because the spaces of 0-forms and "n"-forms is always (fiberwise) 1-dimensional and can be identified with scalar functions, while the spaces of 1-forms and ("n"−1)-forms are always fiberwise "n"-dimensional and can be identified with vector fields.
Curl does not generalize in this way to 4 or more dimensions (or down to 2 or fewer dimensions); in 4 dimensions the dimensions are
so the curl of a 1-vector field (fiberwise 4-dimensional) is a "2-vector field", which is fiberwise 6-dimensional, one has
which yields a sum of six independent terms, and cannot be identified with a 1-vector field. Nor can one meaningfully go from a 1-vector field to a 2-vector field to a 3-vector field (4 → 6 → 4), as taking the differential twice yields zero ("d"2 = 0). Thus there is no curl function from vector fields to vector fields in other dimensions arising in this way.
However, one can define a curl of a vector field as a "2-vector field" in general, as described below.
Curl geometrically.
2-vectors correspond to the exterior power Λ2"V"; in the presence of an inner product, in coordinates these are the skew-symmetric matrices, which are geometrically considered as the special orthogonal Lie algebra so("V") of infinitesimal rotations. This has formula_57 dimensions, and allows one to interpret the differential of a 1-vector field as its infinitesimal rotations. Only in 3 dimensions (or trivially in 0 dimensions) is formula_58 which is the most elegant and common case. In 2 dimensions the curl of a vector field is not a vector field but a function, as 2-dimensional rotations are given by an angle (a scalar - an orientation is required to choose whether one counts clockwise or counterclockwise rotations as positive); note that this is not the div, but is rather perpendicular to it. In 3 dimensions the curl of a vector field is a vector field as is familiar (in 1 and 0 dimensions the curl of a vector field is 0, because there are no non-trivial 2-vectors), while in 4 dimensions the curl of a vector field is, geometrically, at each point an element of the 6-dimensional Lie algebra so(4).
Note also that the curl of a 3-dimensional vector field which only depends on 2 coordinates (say "x, y") is simply a vertical vector field (in the "z" direction) whose magnitude is the curl of the 2-dimensional vector field, as in the examples on this page.
Considering curl as a 2-vector field (an antisymmetric 2-tensor) has been used to generalize vector calculus and associated physics to higher dimensions.

</doc>
<doc id="60770" url="https://en.wikipedia.org/wiki?curid=60770" title="Curvature">
Curvature

In mathematics, curvature is any of a number of loosely related concepts in different areas of geometry. Intuitively, curvature is the amount by which a geometric object deviates from being "flat", or "straight" in the case of a line, but this is defined in different ways depending on the context. There is a key distinction between extrinsic curvature, which is defined for objects embedded in another space (usually a Euclidean space) in a way that relates to the radius of curvature of circles that touch the object, and "intrinsic curvature", which is defined at each point in a Riemannian manifold. This article deals primarily with the first concept.
The canonical example of extrinsic curvature is that of a circle, which everywhere has curvature equal to the reciprocal of its radius. Smaller circles bend more sharply, and hence have higher curvature. The curvature of a smooth curve is defined as the curvature of its osculating circle at each point.
More commonly this is a scalar quantity, but one may also define a curvature vector that takes into account the direction of the bend as well as its sharpness. The curvature of more complex objects (such as surfaces or even curved "n"-dimensional spaces) is described by more complex objects from linear algebra, such as the general Riemann curvature tensor.
The remainder of this article discusses, from a mathematical perspective, some geometric examples of curvature: the curvature of a curve embedded in a plane and the curvature of a surface in Euclidean space.
See the links below for further reading.
Curvature of plane curves.
Cauchy defined the centre of curvature "C" as the intersection point of two infinitely close normals to the curve, the radius of curvature as the distance from the point to "C", and the curvature itself as the inverse of the radius of curvature.
Let "C" be a plane curve (the precise technical assumptions are given below). The curvature of "C" at a point is a measure of how sensitive its tangent line is to moving the point to other nearby points. There are a number of equivalent ways that this idea can be made precise.
One way is geometrical. It is natural to define the curvature of a straight line to be identically zero. The curvature of a circle of radius "R" should be large if "R" is small and small if "R" is large. Thus the curvature of a circle is defined to be the reciprocal of the radius:
Given any curve "C" and a point "P" on it, there is a unique circle or line which most closely approximates the curve near "P", the osculating circle at "P". The curvature of "C" at "P" is then defined to be the curvature of that circle or line. The radius of curvature is defined as the reciprocal of the curvature.
Another way to understand the curvature is physical. Suppose that a particle moves along the curve with unit speed. Taking the time "s" as the parameter for "C", this provides a natural parametrization for the curve. The unit tangent vector T (which is also the velocity vector, since the particle is moving with unit speed) also depends on time. The curvature is then the magnitude of the rate of change of T. Symbolically,
This is the magnitude of the acceleration of the particle and the vector formula_3 is the acceleration vector. Geometrically, the curvature formula_4 measures how fast the unit tangent vector to the curve rotates. If a curve keeps close to the same direction, the unit tangent vector changes very little and the curvature is small; where the curve undergoes a tight turn, the curvature is large.
These two approaches to the curvature are related geometrically by the following observation. In the first definition, the curvature of a circle is equal to the ratio of the angle of an arc to its length. Likewise, the curvature of a plane curve at any point is the limiting ratio of "dθ", an infinitesimal angle (in radians) between tangents to that curve at the ends of an infinitesimal segment of the curve, to the length of that segment "ds", i.e., "dθ/ds". If the tangents at the ends of the segment are represented by unit vectors, it is easy to show that in this limit, the magnitude of the difference vector is equal to "dθ", which leads to the given expression in the second definition of curvature.
Precise definition.
Suppose that "C" is a twice continuously differentiable immersed plane curve, which here means that there exists a parametric representation of "C" by a pair of functions such that the first and second derivatives of "x" and "y" both exist and are continuous, and
throughout the domain. For such a plane curve, there exists a reparametrization with respect to arc length "s". This is a parametrization of "C" such that
The velocity vector T("s") is the unit tangent vector. The unit normal vector N("s"), the curvature "κ"("s"), the oriented or signed curvature "k"("s"), and the radius of curvature "R(s)" are given by
Expressions for calculating the curvature in arbitrary coordinate systems are given below.
Signed curvature.
The sign of the signed curvature "k" indicates the direction in which the unit tangent vector rotates as a function of the parameter along the curve. If the unit tangent rotates counterclockwise, then "k" > 0. If it rotates clockwise, then "k" < 0. So, for example, the sign of the curvature of a graph is the same as the sign of the second derivative (see below).
The signed curvature depends on the particular parametrization chosen for a curve. For example the unit circle can be parametrised by (counterclockwise, with "k" > 0), or by (clockwise, with "k" < 0). More precisely, the signed curvature depends only on the choice of orientation of an immersed curve. Every immersed curve in the plane admits two possible orientations.
Local expressions.
For a plane curve given parametrically in Cartesian coordinates as , the curvature is
where primes refer to derivatives with respect to parameter "t" . The signed curvature "k"  is
The expression reflects the geometric meaning discussed above, that the curvature is influenced by the change amount of the tangent vector in the direction of the normal vector, as
These can be expressed in a coordinate-independent manner via
Curvature of a graph.
For the less general case of a plane curve given explicitly as formula_13, and now using primes for derivatives with respect to coordinate  "x" , the curvature is
and the signed curvature is
This quantity is common in physics and engineering; for example, in the equations of bending in beams, the 1D vibration of a tense string, approximations to the fluid flow around surfaces (in aeronautics), and the free surface boundary conditions in ocean waves. In such applications, the assumption is almost always made that the slope is small compared with unity, so that the approximation:
may be used. This approximation yields a straightforward linear equation describing the phenomenon.
If a curve is defined in polar coordinates as formula_17, then its curvature is
where here the prime now refers to differentiation with respect to formula_19.
Examples.
Consider the parabola . We can parametrize the curve simply as . If we use primes for derivatives with respect to parameter  "t" , then
Substituting and dropping unnecessary absolute values, get
And the same result may be obtained immediately from the above formula of the curvature of a graph, without parametrizing.
A Lissajous curve with ratio of frequencies (3:2) can be parametrized in this way:
Applying the formula it turns out to have
signed curvature "k"("t")
(Other details about this example are in Wikipedia in the page of osculating circle).
Curvature of space curves.
As in the case of curves in two dimensions, the curvature of a regular space curve "C" in three dimensions (and higher) is the magnitude of the acceleration of a particle moving with unit speed along a curve. Thus if γ("s") is the arclength parametrization of "C" then the unit tangent vector T("s") is given by
and the curvature is the magnitude of the acceleration:
The direction of the acceleration is the unit normal vector N("s"), which is defined by
The plane containing the two vectors T("s") and N("s") is called the osculating plane to the curve at γ("s"). The curvature has the following geometrical interpretation. There exists a circle in the osculating plane tangent to γ("s") whose Taylor series to second order at the point of contact agrees with that of "γ"("s"). This is the osculating circle to the curve. The radius of the circle "R"("s") is called the radius of curvature, and the curvature is the reciprocal of the radius of curvature:
The tangent, curvature, and normal vector together describe the second-order behavior of a curve near a point. In three-dimensions, the third order behavior of a curve is described by a related notion of torsion, which measures the extent to which a curve tends to move in a helical path in space. The torsion and curvature are related by the Frenet–Serret formulas (in three dimensions) and their generalization (in higher dimensions).
Local expressions.
For a parametrically defined space curve in three-dimensions given in Cartesian coordinates by ,
the curvature is
where the prime denotes differentiation with respect to the parameter "t". This can be expressed independently of the coordinate system by means of the formula
where formula_30 is the vector cross product. Equivalently,
Here the "t" denotes the matrix transpose. This last formula is also valid for the curvature of curves in a Euclidean space of any dimension.
Curvature from arc and chord length.
Given two points "P" and "Q" on "C", let "s"("P","Q") be the arc length of the portion of the curve between "P" and "Q" and let "d"("P","Q") denote the length of the line segment from "P" to "Q". The curvature of "C" at "P" is given by the limit
where the limit is taken as the point "Q" approaches "P" on "C". The denominator can equally well be taken to be "d"("P","Q")3. The formula is valid in any dimension. Furthermore, by considering the limit independently on either side of "P", this definition of the curvature can sometimes accommodate a singularity at "P". The formula follows by verifying it for the osculating circle.
Curves on surfaces.
When a one-dimensional curve lies on a two dimensional surface embedded in three dimensions R3, further measures of curvature are
available, which take the surface's unit-normal vector, u into account. These are the normal curvature, geodesic curvature and geodesic torsion.
Any non-singular curve on a smooth surface will have its tangent vector T lying in the tangent plane of the surface orthogonal
to the normal vector. The normal curvature, "kn", is the curvature of the curve projected onto the plane containing the curve's tangent T and the surface normal u; the geodesic curvature, "kg", is the curvature of the curve projected onto the
surface's tangent plane; and the geodesic torsion (or relative torsion), "τr", measures the rate of change of the surface normal around the curve's tangent.
Let the curve be a unit speed curve and let t = u × T so that T, u, t form an orthonormal basis: the Darboux frame. The above quantities are related by:
Principal curvature.
All curves with the same tangent vector will have the same normal curvature, which is the same as the curvature of the curve obtained by intersecting the surface with the plane containing T and u. Taking all possible tangent vectors
then the maximum and minimum values of the normal curvature at a point are called the principal curvatures, "k"1 and "k"2, and the directions of the corresponding tangent vectors are called principal directions.
Curvature of surfaces.
Gaussian curvature.
In contrast to curves, which do not have intrinsic curvature, but do have extrinsic curvature (they only have a curvature given an embedding), surfaces can have intrinsic curvature, independent of an embedding. The Gaussian curvature, named after Carl Friedrich Gauss, is equal to the product of the principal curvatures, "k"1"k"2. It has the dimension of 1/length2 and is positive for spheres, negative for one-sheet hyperboloids and zero for planes. It determines whether a surface is locally convex (when it is positive) or locally saddle (when it is negative).
This definition of Gaussian curvature is "extrinsic" in that it uses the surface's embedding in R3, normal vectors, external planes etc. Gaussian curvature is however in fact an "intrinsic" property of the surface, meaning it does not depend on the particular embedding of the surface; intuitively, this means that ants living on the surface could determine the Gaussian curvature. For example, an ant living on a sphere could measure the sum of the interior angles of a triangle and determine that it was greater than 180 degrees, implying that the space it inhabited had positive curvature. On the other hand, an ant living on a cylinder would not detect any such departure from Euclidean geometry; in particular the ant could not detect that the two surfaces have different mean curvatures (see below), which is a purely extrinsic type of curvature.
Formally, Gaussian curvature only depends on the Riemannian metric of the surface. This is Gauss's celebrated Theorema Egregium, which he found while concerned with geographic surveys and mapmaking.
An intrinsic definition of the Gaussian curvature at a point "P" is the following: imagine an ant which is tied to "P" with a short thread of length "r". She runs around "P" while the thread is completely stretched and measures the length C("r") of one complete trip around "P". If the surface were flat, she would find C("r") = 2π"r". On curved surfaces, the formula for C("r") will be different, and the Gaussian curvature "K" at the point "P" can be computed by the Bertrand–Diquet–Puiseux theorem as
The integral of the Gaussian curvature over the whole surface is closely related to the surface's Euler characteristic; see the Gauss–Bonnet theorem.
The discrete analog of curvature, corresponding to curvature being concentrated at a point and particularly useful for polyhedra, is the (angular) defect; the analog for the Gauss–Bonnet theorem is Descartes' theorem on total angular defect.
Because (Gaussian) curvature can be defined without reference to an embedding space, it is not necessary that a surface be embedded in a higher-dimensional space in order to be curved. Such an intrinsically curved two-dimensional surface is a simple example of a Riemannian manifold.
Mean curvature.
The mean curvature is equal to half the sum of the principal curvatures, ("k"1+"k"2)/2. It has the dimension of 1/length. Mean curvature is closely related to the first variation of surface area, in particular a minimal surface such as a soap film, has mean curvature zero and a soap bubble has constant mean curvature. Unlike Gauss curvature, the mean curvature is extrinsic and depends on the embedding, for instance, a cylinder and a plane are locally isometric but the mean curvature of a plane is zero while that of a cylinder is nonzero.
Second fundamental form.
The intrinsic and extrinsic curvature of a surface can be combined in the second fundamental form. This is a quadratic form in the tangent plane to the surface at a point whose value at a particular tangent vector "X" to the surface is the normal component of the acceleration of a curve along the surface tangent to "X"; that is, it is the normal curvature to a curve tangent to "X" (see above). Symbolically,
where "N" is the unit normal to the surface. For unit tangent vectors "X", the second fundamental form assumes the maximum value "k"1 and minimum value "k"2, which occur in the principal directions "u"1 and "u"2, respectively. Thus, by the principal axis theorem, the second fundamental form is
Thus the second fundamental form encodes both the intrinsic and extrinsic curvatures.
A related notion of curvature is the shape operator, which is a linear operator from the tangent plane to itself. When applied to a tangent vector "X" to the surface, the shape operator is the tangential component of the rate of change of the normal vector when moved along a curve on the surface tangent to "X". The principal curvatures are the eigenvalues of the shape operator, and in fact the shape operator and second fundamental form have the same matrix representation with respect to a pair of orthonormal vectors of the tangent plane. The Gauss curvature is thus the determinant of the shape tensor and the mean curvature is half its trace.
Higher dimensions: Curvature of space.
By extension of the former argument, a space of three or more dimensions can be intrinsically curved. The curvature is "intrinsic" in the sense that it is a property defined at every point in the space, rather than a property defined with respect to a larger space that contains it. In general, a curved space may or may not be conceived as being embedded in a higher-dimensional ambient space; if not then its curvature can only be defined intrinsically.
After the discovery of the intrinsic definition of curvature, which is closely connected with non-Euclidean geometry, many mathematicians and scientists questioned whether ordinary physical space might be curved, although the success of Euclidean geometry up to that time meant that the radius of curvature must be astronomically large. In the theory of general relativity, which describes gravity and cosmology, the idea is slightly generalised to the "curvature of space-time"; in relativity theory space-time is a pseudo-Riemannian manifold. Once a time coordinate is defined, the three-dimensional space corresponding to a particular time is generally a curved Riemannian manifold; but since the time coordinate choice is largely arbitrary, it is the underlying space-time curvature that is physically significant.
Although an arbitrarily curved space is very complex to describe, the curvature of a space which is locally isotropic and homogeneous is described by a single Gaussian curvature, as for a surface; mathematically these are strong conditions, but they correspond to reasonable physical assumptions (all points and all directions are indistinguishable). A positive curvature corresponds to the inverse square radius of curvature; an example is a sphere or hypersphere. An example of negatively curved space is hyperbolic geometry. A space or space-time with zero curvature is called flat. For example, Euclidean space is an example of a flat space, and Minkowski space is an example of a flat space-time. There are other examples of flat geometries in both settings, though. A torus or a cylinder can both be given flat metrics, but differ in their topology. Other topologies are also possible for curved space. See also shape of the universe.
Generalizations.
The mathematical notion of "curvature" is also defined in much more general contexts. Many of these generalizations emphasize different aspects of the curvature as it is understood in lower dimensions.
One such generalization is kinematic. The curvature of a curve can naturally be considered as a kinematic quantity, representing the force felt by a certain observer moving along the curve; analogously, curvature in higher dimensions can be regarded as a kind of tidal force (this is one way of thinking of the sectional curvature). This generalization of curvature depends on how nearby test particles diverge or converge when they are allowed to move freely in the space; see Jacobi field.
Another broad generalization of curvature comes from the study of parallel transport on a surface. For instance, if a vector is moved around a loop on the surface of a sphere keeping parallel throughout the motion, then the final position of the vector may not be the same as the initial position of the vector. This phenomenon is known as holonomy. Various generalizations capture in an abstract form this idea of curvature as a measure of holonomy; see curvature form. A closely related notion of curvature comes from gauge theory in physics, where the curvature represents a field and a vector potential for the field is a quantity that is in general path-dependent: it may change if an observer moves around a loop.
Two more generalizations of curvature are the scalar curvature and Ricci curvature. In a curved surface such as the sphere, the area of a disc on the surface differs from the area of a disc of the same radius in flat space. This difference (in a suitable limit) is measured by the scalar curvature. The difference in area of a sector of the disc is measured by the Ricci curvature. Each of the scalar curvature and Ricci curvature are defined in analogous ways in three and higher dimensions. They are particularly important in relativity theory, where they both appear on the side of Einstein's field equations that represents the geometry of spacetime (the other side of which represents the presence of matter and energy). These generalizations of curvature underlie, for instance, the notion that curvature can be a property of a measure; see curvature of a measure.
Another generalization of curvature relies on the ability to compare a curved space with another space that has "constant" curvature. Often this is done with triangles in the spaces. The notion of a triangle makes senses in metric spaces, and this gives rise to CAT(k) spaces.

</doc>
<doc id="489440" url="https://en.wikipedia.org/wiki?curid=489440" title="Curvature form">
Curvature form

In differential geometry, the curvature form describes curvature of a connection on a principal bundle. It can be considered as an alternative to or generalization of the curvature tensor in Riemannian geometry.
Definition.
Let "G" be a Lie group with Lie algebra formula_1, and "P" → "B" be a principal "G"-bundle. Let ω be an Ehresmann connection on "P" (which is a formula_1-valued one-form on "P"). 
Then the curvature form is the formula_1-valued 2-form on "P" defined by
Here formula_5 stands for exterior derivative, formula_6 is defined in the article "Lie algebra-valued form" and "D" denotes the exterior covariant derivative. In other terms, 
where "X", "Y" are tangent vectors to "P".
There is also another expression for Ω:
where "hZ" means the horizontal component of "Z" and on the right we identified a vertical vector field and a Lie algebra element generating it (fundamental vector field).
A connection is said to be flat if its curvature vanishes: Ω = 0. Equivalently, a connection is flat if the structure group can be reduced to the same underlying group but with the discrete topology. See also: flat vector bundle.
Curvature form in a vector bundle.
If "E" → "B" is a vector bundle, then one can also think of ω as 
a matrix of 1-forms and the above formula becomes the structure equation of E. Cartan:
where formula_10 is the wedge product. More precisely, if formula_11 and formula_12 denote components of ω and Ω correspondingly, (so each formula_11 is a usual 1-form and each formula_12 is a usual 2-form) then
For example, for the tangent bundle of a Riemannian manifold, the structure group is O("n") and Ω is a 2-form with values in the Lie algebra of O("n"), i.e. the antisymmetric matrices. In this case the form Ω is an alternative description of the curvature tensor, i.e.
using the standard notation for the Riemannian curvature tensor.
Bianchi identities.
If formula_17 is the canonical vector-valued 1-form on the frame bundle, 
the torsion formula_18 of the connection form
formula_19 
is the vector-valued 2-form defined by the structure equation
where as above "D" denotes the exterior covariant derivative.
The first Bianchi identity takes the form
The second Bianchi identity takes the form
and is valid more generally for any connection in a principal bundle.

</doc>
<doc id="651361" url="https://en.wikipedia.org/wiki?curid=651361" title="Curvature of Riemannian manifolds">
Curvature of Riemannian manifolds

In mathematics, specifically differential geometry, the infinitesimal geometry of Riemannian manifolds with dimension at least 3 is "too complicated" to be described by a single number at a given point. Riemann introduced an abstract and rigorous way to define it, now known as the curvature tensor. Similar notions have found applications everywhere in differential geometry.
For a more elementary discussion see the article on curvature which discusses the curvature of curves and surfaces in 2 and 3 dimensions, as well as the differential geometry of surfaces.
The curvature of a pseudo-Riemannian manifold can be expressed in the same way with only slight modifications.
Ways to express the curvature of a Riemannian manifold.
The Riemann curvature tensor.
The curvature of a Riemannian manifold can be described in various ways; the most standard one is the curvature tensor, given in terms of a Levi-Civita connection (or covariant differentiation) formula_1 and Lie bracket formula_2 by the following formula:
Here formula_4 is a linear transformation of the tangent space of the manifold; it is linear in each argument.
If formula_5 and formula_6 are coordinate vector fields then formula_7 and therefore the formula simplifies to
i.e. the curvature tensor measures "noncommutativity of the covariant derivative".
The linear transformation formula_9 is also called the curvature transformation or endomorphism.
NB. There are a few books where the curvature tensor is defined with opposite sign.
Symmetries and identities.
The curvature tensor has the following symmetries:
The last identity was discovered by Ricci, but is often called the "first Bianchi identity", just because it looks similar to the Bianchi identity below. The first two should be addressed as "antisymmetry" and "Lie algebra property" resp., since the second means, that the for all "u", "v" are elements of the pseudo-orthogonal Lie algebra. All three together should be named "pseudo-orthogonal curvature structure". They give rise to a "tensor" only by identifications with objects of the tensor algebra - but likewise there are identifications with concepts in the Clifford-algebra. Let us note, that these three axioms of a curvature structure give rise to a well-developed structure theory, formulated in terms of projectors (a Weyl projector, giving rise to "Weyl curvature" and an Einstein projector, needed for the setup of the Einsteinian gravitational equations). This structure theory is compatible with the action of the pseudo-orthogonal groups plus dilatations. It has strong ties with the theory of Lie groups and algebras, Lie triples and Jordan algebras. See the references given in the discussion.
The three identities form a complete list of symmetries of the curvature tensor, i.e. given any tensor which satisfies the identities above, one could find a Riemannian manifold with such a curvature tensor at some point. Simple calculations show that such a tensor has formula_13 independent components.
Yet another useful identity follows from these three:
The Bianchi identity (often the second Bianchi identity)
involves the covariant derivatives:
Sectional curvature.
Sectional curvature is a further, equivalent but more geometrical, description of the curvature of Riemannian manifolds. It is a function formula_16 which depends on a "section" formula_17 (i.e. a 2-plane in the tangent spaces). It is the Gauss curvature of the formula_18-"section" at "p"; here formula_18-"section" is a locally-defined piece of surface which has the plane formula_18 as a tangent plane at "p", obtained from geodesics which start at "p" in the directions of the image of formula_18 under the exponential map at "p".
If formula_22 are two linearly independent vectors in formula_17 then
The following formula indicates that sectional curvature describes the curvature tensor completely:
Or in a simpler formula:
formula_28
Curvature form.
The connection form gives an alternative way to describe curvature. It is used more for general vector bundles, and for principal bundles, but it works just as well for the tangent bundle with the Levi-Civita connection. The curvature of "n"-dimensional Riemannian manifold is given by an antisymmetric "n"×"n" matrix formula_29 of 2-forms (or equivalently a 2-form with values in formula_30, the Lie algebra of the orthogonal group formula_31, which is the structure group of the tangent bundle of a Riemannian manifold).
Let formula_32 be a local section of orthonormal bases. Then one can define the connection form, an antisymmetric matrix of 1-forms formula_33 which satisfy from the following identity
Then the curvature form formula_35 is defined by
The following describes relation between curvature form and curvature tensor:
This approach builds in all symmetries of curvature tensor except the "first Bianchi identity", which takes form
where formula_39 is an "n"-vector of 1-forms defined by formula_40.
The "second Bianchi identity" takes form
"D" denotes the exterior covariant derivative
The curvature operator.
It is sometimes convenient to think about curvature as an operator formula_42 
on tangent bivectors (elements of formula_43), which is uniquely defined by the following identity:
It is possible to do this precisely because of the symmetries of the curvature tensor (namely antisymmetry in the first and last pairs of indices, and block-symmetry of those pairs).
Further curvature tensors.
In general the following tensors and functions do not describe the curvature tensor completely, 
however they play an important role.
Scalar curvature.
Scalar curvature is a function on any Riemannian manifold, usually denoted by "Sc". 
It is the full trace of the curvature tensor; given an orthonormal basis 
formula_45 in the tangent space at "p" we have
where "Ric" denotes Ricci tensor. The result does not depend on the choice of orthonormal basis. Starting with dimension 3, scalar curvature does not describe the curvature tensor completely.
Ricci curvature.
Ricci curvature is a linear operator on tangent space at a point, usually denoted by "Ric". 
Given an orthonormal basis 
formula_45 in the tangent space at "p" we have
The result does not depend on the choice of orthonormal basis. 
With four or more dimensions, Ricci curvature does not describe the curvature tensor completely.
Explicit expressions for the Ricci tensor in terms of the Levi-Civita connection is given in the article on Christoffel symbols.
Weyl curvature tensor.
The Weyl curvature tensor has the same symmetries as the curvature tensor, plus one extra: its trace (as used to define the Ricci curvature) must vanish.
In dimensions 2 and 3 Weyl curvature vanishes, but if the dimension "n" > 3 then the second part can be non-zero.
Ricci decomposition.
Although individually, the Weyl tensor and Ricci tensor do not in general determine the full curvature tensor, the Riemann curvature tensor can be decomposed into a Weyl part and a Ricci part. This decomposition is known as the Ricci decomposition, and plays an important role in the conformal geometry of Riemannian manifolds. In particular, it can be used to show that if the metric is rescaled by a conformal factor of formula_49, then the Riemann curvature tensor changes to (seen as a (0, 4)-tensor):
where formula_51 denotes the Kulkarni–Nomizu product and Hess is the Hessian.
Calculation of curvature.
For calculation of curvature

</doc>
<doc id="89246" url="https://en.wikipedia.org/wiki?curid=89246" title="Curve">
Curve

In mathematics, a curve (also called a curved line in older texts) is, generally speaking, an object similar to a line but which is not required to be straight. This entails that a line is a special case of curve, namely a curve with null curvature.
Often curves in two-dimensional (plane curves) or three-dimensional (space curves) Euclidean space are of interest.
Various disciplines within mathematics have given the term different meanings depending on the area of study, so the precise meaning depends on context. However, many of these meanings are special instances of the definition which follows. A curve is a topological space which is locally homeomorphic to a line. In everyday language, this means that a curve is a set of points which, near each of its points, looks like a line, up to a deformation. A simple example of a curve is the parabola, shown to the right. A large number of other curves have been studied in multiple mathematical fields.
A closed curve is a curve that forms a path whose starting point is also its ending point—i.e., a path from any of its points to the same point.
Closely related meanings are "graph of a function" (as in "Phillips curve") and "two-dimensional graph".
In non-mathematical language, the term is often used metaphorically, as in "learning curve".
History.
Interest in curves began long before they were the subject of mathematical study. This can be seen in numerous examples of their decorative use in art and on everyday objects dating back to prehistoric
times. Curves, or at least their graphical representations, are simple to create, for example by a stick in the sand on a beach.
Historically, the term "line" was used in place of the more modern term "curve". Hence the phrases "straight line" and "right line" were used to distinguish what are today called lines from "curved lines". For example, in Book I of Euclid's Elements, a line is defined as a "breadthless length" (Def. 2), while a "straight" line is defined as "a line that lies evenly with the points on itself" (Def. 4). Euclid's idea of a line is perhaps clarified by the statement "The extremities of a line are points," (Def. 3). Later commentators further classified lines according to various schemes. For example:
The Greek geometers had studied many other kinds of curves. One reason was their interest in solving geometrical problems that could not be solved using standard compass and straightedge construction.
These curves include:
A fundamental advance in the theory of curves was the advent of analytic geometry in the seventeenth century. This enabled a curve to be described using an equation rather than an elaborate geometrical construction. This not only allowed new curves to be defined and studied, but it enabled a formal distinction to be made between curves that can be defined using algebraic equations, algebraic curves, and those that cannot, transcendental curves. Previously, curves had been described as "geometrical" or "mechanical" according to how they were, or supposedly could be, generated.
Conic sections were applied in astronomy by Kepler.
Newton also worked on an early example in the calculus of variations. Solutions to variational problems, such as the brachistochrone and tautochrone questions, introduced properties of curves in new ways (in this case, the cycloid). The catenary gets its name as the solution to the problem of a hanging chain, the sort of question that became routinely accessible by means of differential calculus.
In the eighteenth century came the beginnings of the theory of plane algebraic curves, in general. Newton had studied the cubic curves, in the general description of the real points into 'ovals'. The statement of Bézout's theorem showed a number of aspects which were not directly accessible to the geometry of the time, to do with singular points and complex solutions.
From the nineteenth century there is not a separate curve theory, but rather the appearance of curves as the one-dimensional aspect of projective geometry, and differential geometry; and later topology, when for example the Jordan curve theorem was understood to lie quite deep, as well as being required in complex analysis. The era of the space-filling curves finally provoked the modern definitions of curve.
Topology.
In topology, a curve is defined as follows. Let formula_1 be an interval of real numbers (i.e. a non-empty connected subset of formula_2). Then a curve formula_3 is a continuous mapping formula_4, where formula_5 is a topological space.
In other words, this curve "does not cross itself and has no missing points".
A plane curve is a curve for which formula_5 is the Euclidean plane—these are the examples first encountered—or in some cases the projective plane. A space curve is a curve for which formula_5 is of three dimensions, usually Euclidean space; a skew curve is a space curve which lies in no plane. These definitions also apply to algebraic curves (see below). However, in the case of algebraic curves it is very common to consider number systems more general than the reals.
This definition of curve captures our intuitive notion of a curve as a connected, continuous geometric figure that is "like" a line, without thickness and drawn without interruption, although it also includes figures that can hardly be called curves in common usage. For example, the image of a curve can cover a square in the plane (space-filling curve). The image of simple plane curve can have Hausdorff dimension bigger than one (see Koch snowflake) and even positive Lebesgue measure (the last example can be obtained by small variation of the Peano curve construction). The dragon curve is another unusual example.
Conventions and terminology.
The distinction between a curve and its image is important. Two distinct curves may have the same image. For example, a line segment can be traced out at different speeds, or a circle can be traversed a different number of times. Many times, however, we are just interested in the image of the curve. It is important to pay attention to context and convention in reading.
Terminology is also not uniform. Often, topologists use the term "path" for what we are calling a curve, and "curve" for what we are calling the image of a curve. The term "curve" is more common in vector calculus and differential geometry.
An arc or segment of a curve is a part of a curve that is bounded by two distinct end points and contains every point on the curve between its end points. Depending on how the arc is defined, either of the two end points may or may not be part of it. When the arc is straight, it is typically called a line segment (see also chord).
Lengths of curves.
If formula_5 is a metric space with metric formula_26, then we can define the "length" of a curve formula_27 by
where the sup is over all formula_29 and all partitions formula_30 of formula_13.
A is a curve with finite length.
A parametrization of formula_32 is called natural (or unit speed or parametrised by arc length) if for any formula_33, we have
If formula_3 is a Lipschitz-continuous function, then it is automatically rectifiable. Moreover, in this case, one can define the speed (or metric derivative) of formula_3 at formula_37 as
and then
In particular, if formula_40 is an Euclidean space and formula_41 is differentiable then
Differential geometry.
While the first examples of curves that are met are mostly plane curves (that is, in everyday words, "curved lines" in "two-dimensional space"), there are obvious examples such as the helix which exist naturally in three dimensions. The needs of geometry, and also for example classical mechanics are to have a notion of curve in space of any number of dimensions. In general relativity, a world line is a curve in spacetime.
If formula_5 is a differentiable manifold, then we can define the notion of "differentiable curve" in formula_5. This general idea is enough to cover many of the applications of curves in mathematics. From a local point of view one can take formula_5 to be Euclidean space. On the other hand, it is useful to be more general, in that (for example) it is possible to define the tangent vectors to formula_5 by means of this notion of curve.
If formula_5 is a smooth manifold, a "smooth curve" in formula_5 is a smooth map
This is a basic notion. There are less and more restricted ideas, too. If formula_5 is a formula_51 manifold (i.e., a manifold whose charts are formula_52 times continuously differentiable), then a formula_51 curve in formula_5 is such a curve which is only assumed to be formula_51 (i.e. formula_52 times continuously differentiable). If formula_5 is an analytic manifold (i.e. infinitely differentiable and charts are expressible as power series), and formula_3 is an analytic map, then formula_3 is said to be an "analytic curve".
A differentiable curve is said to be "regular" if its derivative never vanishes. (In words, a regular curve never slows to a stop or backtracks on itself.) Two formula_51 differentiable curves
are said to be "equivalent" if there is a bijective formula_51 map
such that the inverse map
is also formula_51, and
for all formula_68. The map formula_69 is called a "reparametrisation" of formula_70; and this makes an equivalence relation on the set of all formula_51 differentiable curves in formula_5. A formula_51 "arc" is an equivalence class of formula_51 curves under the relation of reparametrisation.
Algebraic curve.
Algebraic curves are the curves considered in algebraic geometry. A plane algebraic curve is the locus of the points of coordinates "x", "y" such that "f"("x", "y") = 0, where "f" is a polynomial in two variables defined over some field "F". Algebraic geometry normally looks not only on points with coordinates in "F" but on all the points with coordinates in an algebraically closed field "K". If "C" is a curve defined by a polynomial "f" with coefficients in "F", the curve is said defined over "F". The points of the curve "C" with coordinates in a field "G" are said rational over "G" and can be denoted "C"("G")); thus the full curve "C" = "C"("K").
Algebraic curves can also be space curves, or curves in a space of higher dimension, say . They are defined as algebraic varieties of dimension one. They may be obtained as the common solutions of at least polynomial equations in variables. If polynomials are sufficient to define a curve in a space of dimension , the curve is said to be a complete intersection. By eliminating variables (by any tool of elimination theory), an algebraic curve may be projected onto a plane algebraic curve, which however may introduce new singularities such as cusps or double points.
A plane curve may also be completed in a curve in the projective plane: if a curve is defined by a polynomial "f" of total degree "d", then "w""d""f"("u"/"w", "v"/"w") simplifies to a homogeneous polynomial "g"("u", "v", "w") of degree "d". The values of "u", "v", "w" such that "g"("u", "v", "w") = 0 are the homogeneous coordinates of the points of the completion of the curve in the projective plane and the points of the initial curve are those such "w" is not zero. An example is the Fermat curve "u""n" + "v""n" = "w""n", which has an affine form "x""n" + "y""n" = 1. A similar process of homogenization may be defined for curves in higher dimensional spaces
Important examples of algebraic curves are the conics, which are nonsingular curves of degree two and genus zero, and elliptic curves, which are nonsingular curves of genus one studied in number theory and which have important applications to cryptography. Because algebraic curves in fields of characteristic zero are most often studied over the complex numbers, algebraic curves in algebraic geometry may be considered as real surfaces. In particular, the non-singular complex projective algebraic curves are called Riemann surfaces.

</doc>
<doc id="755300" url="https://en.wikipedia.org/wiki?curid=755300" title="Curvilinear coordinates">
Curvilinear coordinates

In geometry, curvilinear coordinates are a coordinate system for Euclidean space in which the coordinate lines may be curved. These coordinates may be derived from a set of Cartesian coordinates by using a transformation that is locally invertible (a one-to-one map) at each point. This means that one can convert a point given in a Cartesian coordinate system to its curvilinear coordinates and back. The name "curvilinear coordinates", coined by the French mathematician Lamé, derives from the fact that the coordinate surfaces of the curvilinear systems are curved.
Well-known examples of curvilinear coordinate systems in three-dimensional Euclidean space (R3) are Cartesian, cylindrical and spherical polar coordinates. A Cartesian coordinate surface in this space is a plane; for example "z" = 0 defines the "x"-"y" plane. In the same space, the coordinate surface "r" = 1 in spherical polar coordinates is the surface of a unit sphere, which is curved. The formalism of curvilinear coordinates provides a unified and general description of the standard coordinate systems.
Curvilinear coordinates are often used to define the location or distribution of physical quantities which may be, for example, scalars, vectors, or tensors. Mathematical expressions involving these quantities in vector calculus and tensor analysis (such as the gradient, divergence, curl, and Laplacian) can be transformed from one coordinate system to another, according to transformation rules for scalars, vectors, and tensors. Such expressions then become valid for any curvilinear coordinate system.
Depending on the application, a curvilinear coordinate system may be simpler to use than the Cartesian coordinate system. For instance, a physical problem with spherical symmetry defined in R3 (for example, motion of particles under the influence of central forces) is usually easier to solve in spherical polar coordinates than in Cartesian coordinates. Equations with boundary conditions that follow coordinate surfaces for a particular curvilinear coordinate system may be easier to solve in that system. One would for instance describe the motion of a particle in a rectangular box in Cartesian coordinates, whereas one would prefer spherical coordinates for a particle in a sphere. Spherical coordinates are one of the most used curvilinear coordinate systems in such fields as Earth sciences, cartography, and physics (in particular quantum mechanics, relativity), and engineering.
Orthogonal curvilinear coordinates in 3d.
Coordinates, basis, and vectors.
For now, consider 3d space. A point "P" in 3d space can be defined using Cartesian coordinates ("x", "y", "z") [equivalently written ("x"1, "x"2, "x"3)], or in another system ("q"1, "q"2, "q"3), as shown in Fig. 1. The latter is a curvilinear coordinate system, and ("q"1, "q"2, "q"3) are the curvilinear coordinates of the point "P".
The surfaces "q"1 = constant, "q"2 = constant, "q"3 = constant are called the coordinate surfaces; and the space curves formed by their intersection in pairs are called the coordinate curves. The coordinate axes are determined by the tangents to the coordinate curves at the intersection of three surfaces. They are not in general fixed directions in space, which happens to be the case for simple Cartesian coordinates.
A basis whose vectors change their direction and/or magnitude from point to point is called local basis. All bases associated with curvilinear coordinates are necessarily local. Basis vectors that are the same at all points are global bases, and can be associated only with linear or affine coordinate systems.
The relation between the coordinates is given by the invertible transformation functions:
The point "P" can be written as a position vector r in Cartesian coordinates:
where "x, y, z" are the coordinates of the position vector with respect to the "standard basis vectors" e"x", e"y", e"z".
Note: for this article e is reserved for the standard basis (Cartesian) and b is for the curvilinear basis.
However, in a general curvilinear system, there may well not be any natural global basis vectors. Instead, we note that in the Cartesian system, the standard basis vectors can be derived from the derivative of the location of point "P" with respect to the local coordinate
We can apply the same derivatives to the curvilinear system locally at point P to determine a system of basis vectors specific to location "P". We define
These may not have unit length, and may also not be orthogonal. In the case that they "are" orthogonal at all points where the derivatives are well-defined, we define the Lamé coefficients (after Gabriel Lamé) by
and the curvilinear orthonormal basis vectors by
It is important to note that these basis vectors may well depend upon the position of "P"; it is therefore necessary that they are not assumed to be constant over a region. (They technically form a basis for the tangent bundle of formula_8 at "P", and so are local to "P".)
In general, curvilinear coordinates allow the generality of basis vectors not all mutually perpendicular to each other, and not required to be of unit length: they can be of arbitrary magnitude and direction. The use of an orthogonal basis makes vector manipulations simpler than for non-orthogonal. However, some areas of physics and engineering, particularly fluid mechanics and continuum mechanics, require non-orthogonal bases to describe deformations and fluid transport to account for complicated directional dependences of physical quantities. A discussion of the general case appears later on this page.
Vector calculus.
Differential elements.
Since the total differential change in r is
so scale factors are
They can also be written for each component of r:
However, this designation is very rarely used, largely replaced with the components of the metric tensor "gik" (see below).
Covariant and contravariant bases.
Spatial gradients, distances, time derivatives and scale factors are interrelated within a coordinate system by two groups of basis vectors:
which transforms like covariant vectors (denoted by lowered indices), or
which transforms like contravariant vectors (denoted by raised indices), ∇ is the del operator.
Consequently, a general curvilinear coordinate system has two sets of basis vectors for every point: {b1, b2, b3} is the covariant basis, and {b1, b2, b3} is the contravariant (a.k.a reciprocal) basis. The covariant and contravariant basis vectors types have identical direction for orthogonal curvilinear coordinate systems, but as usual have inverted units with respect to each other. 
Note the following important equality:
wherein formula_14 denotes the generalized Kronecker delta.
A vector v can be specified in terms either basis, i.e.,
Using the Einstein summation convention, the basis vectors relate to the components by
and
where "g" is the metric tensor (see below).
A vector can be specified with covariant coordinates (lowered indices, written "vk") or contravariant coordinates (raised indices, written "vk"). From the above vector sums, it can be seen that contravariant coordinates are associated with covariant basis vectors, and covariant coordinates are associated with contravariant basis vectors.
A key feature of the representation of vectors and tensors in terms of indexed components and basis vectors is "invariance" in the sense that vector components which transform in a covariant manner (or contravariant manner) are paired with basis vectors that transform in a contravariant manner (or covariant manner).
Covariant basis.
Constructing a covariant basis in one dimension.
Consider the one-dimensional curve shown in Fig. 3. At point "P", taken as an origin, "x" is one of the Cartesian coordinates, and "q"1 is one of the curvilinear coordinates (Fig. 3). The local (non-unit) basis vector is b1 (notated h1 above, with b reserved for unit vectors) and it is built on the "q"1 axis which is a tangent to that coordinate line at the point "P". The axis "q"1 and thus the vector b1 form an angle "α" with the Cartesian "x" axis and the Cartesian basis vector e1.
It can be seen from triangle "PAB" that
where |e1|, |b1| are the magnitudes of the two basis vectors, i.e., the scalar intercepts "PB" and "PA". Note that "PA" is also the projection of b1 on the "x" axis.
However, this method for basis vector transformations using "directional cosines" is inapplicable to curvilinear coordinates for the following reasons:
The angles that the "q"1 line and that axis form with the "x" axis become closer in value the closer one moves towards point "P" and become exactly equal at "P".
Let point "E" be located very close to "P", so close that the distance "PE" is infinitesimally small. Then "PE" measured on the "q"1 axis almost coincides with "PE" measured on the "q"1 line. At the same time, the ratio "PD/PE" ("PD" being the projection of "PE" on the "x" axis) becomes almost exactly equal to cos "α".
Let the infinitesimally small intercepts "PD" and "PE" be labelled, respectively, as "dx" and d"q"1. Then
Thus, the directional cosines can be substituted in transformations with the more exact ratios between infinitesimally small coordinate intercepts. It follows that the component (projection) of b1 on the "x" axis is
If "qi" = "qi"("x"1, "x"2, "x"3) and "xi" = "xi"("q"1, "q"2, "q"3) are smooth (continuously differentiable) functions the transformation ratios can be written as formula_23 and formula_24. That is, those ratios are partial derivatives of coordinates belonging to one system with respect to coordinates belonging to the other system.
Constructing a covariant basis in three dimensions.
Doing the same for the coordinates in the other 2 dimensions, b1 can be expressed as:
Similar equations hold for b2 and b3 so that the standard basis {e1, e2, e3} is transformed to a local (ordered and normalised) basis {b1, b2, b3} by the following system of equations:
By analogous reasoning, one can obtain the inverse transformation from local basis to standard basis:
Jacobian of the transformation.
The above systems of linear equations can be written in matrix form using the Einstein summation convention as
This coefficient matrix of the linear system is the Jacobian matrix (and its inverse) of the transformation. These are the equations that can be used to transform a Cartesian basis into a curvilinear basis, and vice versa.
In three dimensions, the expanded forms of these matrices are
In the inverse transformation (second equation system), the unknowns are the curvilinear basis vectors. For any specific location there can only exist one and only one set of basis vectors (else the basis is not well defined at that point). This condition is satisfied if and only if the equation system has a single solution, from linear algebra, a linear equation system has a single solution (non-trivial) only if the determinant of its system matrix is non-zero:
which shows the rationale behind the above requirement concerning the inverse Jacobian determinant.
Generalization to "n" dimensions.
The formalism extends to any finite dimension as follows.
Consider the real Euclidean "n"-dimensional space, that is R"n" = R × R × ... × R ("n" times) where R is the set of real numbers and × denotes the Cartesian product, which is a vector space.
The coordinates of this space can be denoted by: x = ("x"1, "x"2...,"xn"). Since this is a vector (an element of the vector space), it can be written as:
where e1 = (1,0,0...,0), e2 = (0,1,0...,0), e3 = (0,0,1...,0)...,e"n" = (0,0,0...,1) is the "standard basis set of vectors" for the space R"n", and "i" = 1, 2..."n" is an index labelling components. Each vector has exactly one component in each dimension (or "axis") and they are mutually orthogonal (perpendicular) and normalized (has unit magnitude).
More generally, we can define basis vectors b"i" so that they depend on q = ("q"1, "q"2...,"qn"), i.e. they change from point to point: b"i" = b"i"(q). In which case to define the same point x in terms of this alternative basis: the "coordinates" with respect to this basis "vi" also necessarily depend on x also, that is "vi" = "vi"(x). Then a vector v in this space, with respect to these alternative coordinates and basis vectors, can be expanded as a linear combination in this basis (which simply means to multiply each basis vector e"i" by a number "v""i" – scalar multiplication):
The vector sum that describes v in the new basis is composed of different vectors, although the sum itself remains the same.
Transformation of coordinates.
From a more general and abstract perspective, a curvilinear coordinate system is simply a coordinate patch on the differentiable manifold En (n-dimensional Euclidean space) that is diffeomorphic to the Cartesian coordinate patch on the manifold. Note that two diffeomorphic coordinate patches on a differential manifold need not overlap differentiably. With this simple definition of a curvilinear coordinate system, all the results that follow below are simply applications of standard theorems in differential topology.
The transformation functions are such that there's a one-to-one relationship between points in the "old" and "new" coordinates, that is, those functions are bijections, and fulfil the following requirements within their domains:
Vector and tensor algebra in three-dimensional curvilinear coordinates.
Elementary vector and tensor algebra in curvilinear coordinates is used in some of the older scientific literature in mechanics and physics and can be indispensable to understanding work from the early and mid-1900s, for example the text by Green and Zerna. Some useful relations in the algebra of vectors and second-order tensors in curvilinear coordinates are given in this section. The notation and contents are primarily from Ogden, Naghdi, Simmonds, Green and Zerna, Basar and Weichert, and Ciarlet.
Tensors in curvilinear coordinates.
A second-order tensor can be expressed as
where formula_34 denotes the tensor product. The components "Sij" are called the contravariant components, "Si j" the mixed right-covariant components, "Si j" the mixed left-covariant components, and "Sij" the covariant components of the second-order tensor. The components of the second-order tensor are related by
The metric tensor in orthogonal curvilinear coordinates.
At each point, one can construct a small line element , so the square of the length of the line element is the scalar product dx • dx and is called the metric of the space, given by:
The following portion of the above equation
is a "symmetric" tensor called the fundamental (or metric) tensor of the Euclidean space in curvilinear coordinates.
Indices can be raised and lowered by the metric:
Relation to Lamé coefficients.
Defining the scale factors "hi" by
gives a relation between the metric tensor and the Lamé coefficients. Note also that
where "hij" are the Lamé coefficients. For an orthogonal basis we also have:
Example: Polar coordinates.
If we consider polar coordinates for R2, note that
(r, θ) are the curvilinear coordinates, and the Jacobian determinant of the transformation ("r",θ) → ("r" cos θ, "r" sin θ) is "r".
The orthogonal basis vectors are b"r" = (cos θ, sin θ), bθ = (−sin θ, cos θ). The scale factors are "h""r" = 1 and "h"θ= "r". The fundamental tensor is "g"11 =1, "g"22 ="r"2, "g"12 = "g"21 =0.
The alternating tensor.
In an orthonormal right-handed basis, the third-order alternating tensor is defined as
In a general curvilinear basis the same tensor may be expressed as
It can also be shown that
Christoffel symbols.
where the comma denotes a partial derivative (see Ricci calculus). To express Γ"ijk" in terms of "gij" we note that
Since
using these to rearrange the above relations gives
This implies that
Other relations that follow are
Vector and tensor calculus in three-dimensional curvilinear coordinates.
Adjustments need to be made in the calculation of line, surface and volume integrals. For simplicity, the following restricts to three dimensions and orthogonal curvilinear coordinates. However, the same arguments apply for "n"-dimensional spaces. When the coordinate system is not orthogonal, there are some additional terms in the expressions.
Simmonds, in his book on tensor analysis, quotes Albert Einstein saying
The magic of this theory will hardly fail to impose itself on anybody who has truly understood it; it represents a genuine triumph of the method of absolute differential calculus, founded by Gauss, Riemann, Ricci, and Levi-Civita.
Vector and tensor calculus in general curvilinear coordinates is used in tensor analysis on four-dimensional curvilinear manifolds in general relativity, in the mechanics of curved shells, in examining the invariance properties of Maxwell's equations which has been of interest in metamaterials and in many other fields.
Some useful relations in the calculus of vectors and second-order tensors in curvilinear coordinates are given in this section. The notation and contents are primarily from Ogden, Simmonds, Green and Zerna, Basar and Weichert, and Ciarlet.
Let φ = φ(x) be a well defined scalar field and v = v(x) a well-defined vector field, and "λ"1, "λ"2... be parameters of the coordinates
Geometric elements.
 = \sqrt{ g_{ij}\cfrac{\partial q^i}{\partial \lambda}\cfrac{\partial q^j}{\partial \lambda}} = \sqrt{h_{i}^2\left(\cfrac{\partial q^i}{\partial \lambda}\right)^2} </math>
where formula_54 is the permutation symbol. In determinant form:
Differentiation.
The expressions for the gradient, divergence, and Laplacian can be directly extended to "n"-dimensions, however the curl is only defined in 3d.
The vector field b"i" is tangent to the "qi" coordinate curve and forms a natural basis at each point on the curve. This basis, as discussed at the beginning of this article, is also called the covariant curvilinear basis. We can also define a reciprocal basis, or contravariant curvilinear basis, b"i". All the algebraic relations between the basis vectors, as discussed in the section on tensor algebra, apply for the natural basis and its reciprocal at each point x.
Fictitious forces in general curvilinear coordinates.
An inertial coordinate system is defined as a system of space and time coordinates "x"1, "x"2, "x"3, "t" in terms of which the equations of motion of a particle free of external forces are simply d2"x""j"/d"t"2 = 0. In this context, a coordinate system can fail to be “inertial” either due to non-straight time axis or non-straight space axes (or both). In other words, the basis vectors of the coordinates may vary in time at fixed positions, or they may vary with position at fixed times, or both. When equations of motion are expressed in terms of any non-inertial coordinate system (in this sense), extra terms appear, called Christoffel symbols. Strictly speaking, these terms represent components of the absolute acceleration (in classical mechanics), but we may also choose to continue to regard d2"x""j"/d"t"2 as the acceleration (as if the coordinates were inertial) and treat the extra terms as if they were forces, in which case they are called fictitious forces. The component of any such fictitious force normal to the path of the particle and in the plane of the path’s curvature is then called centrifugal force.
This more general context makes clear the correspondence between the concepts of centrifugal force in rotating coordinate systems and in stationary curvilinear coordinate systems. (Both of these concepts appear frequently in the literature.) For a simple example, consider a particle of mass "m" moving in a circle of radius "r" with angular speed "w" relative to a system of polar coordinates rotating with angular speed "W". The radial equation of motion is "mr"” = "F""r" + "mr"("w" + "W")2. Thus the centrifugal force is "mr" times the square of the absolute rotational speed "A" = "w" + "W" of the particle. If we choose a coordinate system rotating at the speed of the particle, then "W" = "A" and "w" = 0, in which case the centrifugal force is "mrA"2, whereas if we choose a stationary coordinate system we have "W" = 0 and "w" = "A", in which case the centrifugal force is again "mrA"2. The reason for this equality of results is that in both cases the basis vectors at the particle’s location are changing in time in exactly the same way. Hence these are really just two different ways of describing exactly the same thing, one description being in terms of rotating coordinates and the other being in terms of stationary curvilinear coordinates, both of which are non-inertial according to the more abstract meaning of that term.
When describing general motion, the actual forces acting on a particle are often referred to the instantaneous osculating circle tangent to the path of motion, and this circle in the general case is not centered at a fixed location, and so the decomposition into centrifugal and Coriolis components is constantly changing. This is true regardless of whether the motion is described in terms of stationary or rotating coordinates.

</doc>
<doc id="52327" url="https://en.wikipedia.org/wiki?curid=52327" title="Cyclic group">
Cyclic group

In algebra, a cyclic group is a group that is generated by a single element. That is, it consists of a set of elements with a single invertible associative operation, and it contains an element "g" such that every other element of the group may be obtained by repeatedly applying the group operation or its inverse to "g". Each element can be written as a power of "g" in multiplicative notation, or as a multiple of "g" in additive notation. This element "g" is called a "generator" of the group.
Every infinite cyclic group is isomorphic to the additive group of Z, the integers. Every finite cyclic group of order "n" is isomorphic to the additive group of Z/"n"Z, the integers modulo "n". Every cyclic group is an abelian group (meaning that its group operation is commutative), and every finitely generated abelian group is a direct product of cyclic groups.
Definition.
A group "G" is called cyclic if there exists an element "g" in "G" such that Since any group generated by an element in a group is a subgroup of that group, showing that the only subgroup of a group "G" that contains "g" is "G" itself suffices to show that "G" is cyclic.
For example, if "G" = { "g"0, "g"1, "g"2, "g"3, "g"4, "g"5 } is a group, then "g"6 = "g"0, and "G" is cyclic. In fact, "G" is essentially the same as (that is, isomorphic to) the set with addition modulo 6. For example, corresponds to , and corresponds to , and so on. One can use the isomorphism χ defined by .
The name "cyclic" may be misleading: it is possible to generate infinitely many elements and not form any literal cycles; that is, every "g""n" is distinct. (It can be thought of as having one infinitely long cycle.) A group generated in this way (for example, the first frieze group, p1) is called an infinite cyclic group, and is isomorphic to the additive group of the integers, .
Examples.
Integer and modular addition.
The set of integers, with the operation of addition, forms a group. It is an infinite cyclic group, because all integers can be written as a finite sum or difference of copies of the number 1. In this group, 1 and −1 are the only generators. Every infinite cyclic group is isomorphic to this group.
For every positive integer "n", the set of integers modulo "n", again with the operation of addition, forms a finite cyclic group, the group Z/"n".
An element "g" is a generator of this group if "g" is relatively prime to "n".
Thus, the number of different generators is φ("n"), where φ is the Euler totient function, the function that counts the number of numbers modulo "n" that are relatively prime to "n".
Every finite cyclic group is isomorphic to a group Z/"n", where "n" is the order of the group.
The integer and modular addition operations, used to define the cyclic groups, are both the addition operations of commutative rings, also denoted Z and Z/"n". If "p" is a prime, then Z/"p" is a finite field, and is usually instead written as F"p" or GF("p"). Every field with "p" elements is isomorphic to this one.
Modular multiplication.
For every positive integer "n", the subset of the integers modulo "n" that are relatively prime to "n", with the operation of multiplication, forms a finite group that for many values of "n" is again cyclic.
It is the group under multiplication modulo "n", and it is cyclic whenever "n" is 1, 2, 4, a power of an odd prime, or twice a power of an odd prime. 
Its elements are the units of the ring Z/"nZ; there are φ("n") of them, where again φ is the totient function. This group is written as (Z/"nZ)×. For example, (Z/6Z)× has as its elements {1,5}; 6 is twice a prime, so this is a cyclic group. In contrast, (Z/8Z)× (with elements {1,3,5,7}) is the Klein group and is not cyclic. When (Z/"nZ)× is cyclic, every generator of (Z/"nZ)× is called a primitive root modulo "n".
The cyclic group (Z/"pZ)× for a prime number "p", is also written (Z/"pZ)* because it consists of the non-zero elements of the finite field of order "p". More generally, every finite subgroup of the multiplicative group of any field is cyclic.
Rotational symmetries.
The set of rotational symmetries of a polygon forms a finite cyclic group. If there are "n" different ways of mapping the polygon to itself by a rotation (including the null rotation) then this group is isomorphic to Zn. In three or higher dimensions there can exist other finite symmetry groups that are cyclic, but that do not form the set of rotations around a single axis.
The group "S"1 of all rotations of a circle (the circle group) is "not" cyclic. Unlike the infinite cyclic group, it is not even countable. There also exist other infinite rotation groups (such as the set of rotations by rational angles) that are countable but not cyclic.
Galois theory.
An "n"th root of unity may be thought of as a complex number whose "n"th power is 1. That is, it is a root of the polynomial "x""n" − 1.
The "n"th roots of unity form a cyclic group of order "n" under multiplication. For example, the polynomial factors as , where ; the set {"s"0, "s"1, "s"2} forms a cyclic group under multiplication. The Galois group of the field extension of the rational numbers generated by the "n"th roots of unity forms a different group. It is isomorphic to the multiplicative group modulo "n", which has order φ("n") and is cyclic for some but not all "n".
A field extension is called a cyclic extension if its Galois group is a cyclic group. The Galois group of every finite extension of a finite field is finite and cyclic, with an iterate of the Frobenius endomorphism as its generator. Conversely, given a finite field "F" and a finite cyclic group "G", there is a finite field extension of "F" whose Galois group is "G".
Subgroups and notation.
All subgroups and quotient groups of cyclic groups are cyclic. Specifically, all subgroups of Z are of the form "m"Z, with "m" an integer ≥0. All of these subgroups are distinct from each other, and apart from the trivial group (for ) all are isomorphic to Z. The lattice of subgroups of Z is isomorphic to the dual of the lattice of natural numbers ordered by divisibility. In particular, because the prime numbers are the numbers with no nontrivial divisors, a cyclic group is simple if and only if its order (the number of its elements) is prime.
Since the cyclic groups are abelian, they are often written additively and denoted Z"n" with the identity written 0. However, this notation can be problematic for number theorists because it conflicts with the usual notation for "p"-adic number rings or localization at a prime ideal. The quotient notations Z/"n"Z, Z/"n", and Z/("n") are standard alternatives.
One may instead write the group multiplicatively, and denote it by C"n", where "n" is the order for finite groups and by C for the infinite cyclic group. For example, in C5, whereas in Z/5Z.
All quotient groups of Z are finite, except for the trivial exception . For every positive divisor "d" of "n", the quotient group Z/"n"Z has precisely one subgroup of order "d", the one generated by the residue class of "n"/"d". There are no other subgroups.
Using the quotient group formalism, Z/"nZ is a standard notation for the additive cyclic group with "n" elements. In ring terminology, the subgroup "nZ is also the ideal ("n"), so the quotient can also be written Z/("n") or Z/"n" without abuse of notation. These alternatives do not conflict with the notation for the "p"-adic integers. The last form is very common in informal calculations; it has the additional advantage that it reads the same way that the group or ring is often described verbally in English, "Zee mod en".
Additional properties.
Every cyclic group is abelian. That is, its group operation is commutative: (for all "g" and "h" in "G"). This is clear for the groups of integer and modular addition since , and it follows for all cyclic groups since they are all isomorphic to a group generated by an addition operation.
For a finite cyclic group of order "n", and every element "e" of the group, "e""n" is the identity element of the group. This again follows by using the isomorphism to modular addition, since for every integer "k".
If "d" is a divisor of "n", then the number of elements in Z/"n" which have order "d" is φ("d"), and the number of elements whose order divides "d" is exactly "d".
If "G" is a finite group in which, for each , "G" contains at most "n" elements of order dividing "n", then "G" must be cyclic.
The order of an element "m" of the group is "n"/gcd("n","m").
The direct product of two cyclic groups Z/"n" and Z/"m" is cyclic if and only if "n" and "m" are coprime. Thus e.g. Z/12 is the direct product of Z/3 and Z/4, but not the direct product of Z/6 and Z/2.
If "p" is a prime number, then the only group (up to isomorphism) with "p" elements is Z/"p".
it is called a primary cyclic group. The fundamental theorem of abelian groups states that every finitely generated abelian group is the direct product of finitely many finite primary cyclic and infinite cyclic groups.
A number "n" is called a cyclic number if it has the property that Z/"n" is the only group of order "n", which is true exactly when gcd("n",φ("n")) = 1. The cyclic numbers include all prime numbers, but also include some composite numbers such as 15. However, except 2, all cyclic numbers are odd. The cyclic numbers are:
In fact, a number "n" is a cyclic number if and only if gcd("n", φ("n")) = 1, where φ is the Euler's totient function.
The definition immediately implies that cyclic groups have group presentation and for finite "n".
Associated objects.
Representations.
The representation theory of the cyclic group is a critical base case for the representation theory of more general finite groups. In the complex case, a representation of a cyclic group decomposes into a direct sum of linear characters, making the connection between character theory and representation theory transparent. In the positive characteristic case, the indecomposable representations of the cyclic group form a model and inductive basis for the representation theory of groups with cyclic Sylow subgroups and more generally the representation theory of blocks of cyclic defect.
Cycle graph.
A cycle graph illustrates the various cycles of a group and is particularly useful in visualizing the structure of small finite groups. A cycle graph for a cyclic group is simply a circular graph, where the group order is equal to the number of nodes. A single generator defines the group as a directional path on the graph, and the inverse generator defines a backwards path. Trivial paths (identity) can be drawn as a loop but are usually suppressed. Z2 is sometimes drawn with two curved edges as a multigraph.
Cyclic groups Z"n", order "n", is a single cycle graphed simply as an "n"-sided polygon with the elements at the vertices. A cyclic group Z"n" can be decomposed into a direct product Z"a"×Z"b" where "n"="ab", where "a" and "b" are relatively prime (gcd(a,b)=1).
Cayley graph.
A Cayley graph is a graph defined from a pair ("G","S") where "G" is a group and "S" is a set of generators for the group; it has a vertex for each group element, and an edge for each product of an element with a generator. In the case of a finite cyclic group, with its single generator, the Cayley graph is a cycle graph, and for an infinite cyclic group with its generator the Cayley graph is a doubly infinite path graph. However, Cayley graphs can be defined from other sets of generators as well. The Cayley graphs of cyclic groups with arbitrary generator sets are called circulant graphs. These graphs may be represented geometrically as a set of equally spaced points on a circle or on a line, with each point connected to neighbors with the same set of distances as each other point. They are exactly the vertex-transitive graphs whose symmetry group includes a transitive cyclic group.
Endomorphisms.
The endomorphism ring of the abelian group Z/"nZ is isomorphic to Z/"nZ itself as a ring. Under this isomorphism, the number "r" corresponds to the endomorphism of Z/"nZ that maps each element to the sum of "r" copies of it. This is a bijection if and only if "r" is coprime with "n", so the automorphism group of Z/"nZ is isomorphic to the unit group (Z/"n"Z)×.
Similarly, the endomorphism ring of the additive group of Z is isomorphic to the ring Z. Its automorphism group is isomorphic to the group of units of the ring Z, i.e. to .
Tensor product and Hom of cyclic groups.
The tensor product formula_1 and the group of homomorphisms formula_2 can be shown to both be isomorphic to formula_3.
For the tensor product, this is a consequence of the general fact formula_4. For the Hom group, recall that it is isomorphic to the subgroup of formula_5 consisting of the elements of order dividing "m". That subgroup is cyclic of order gcd (m, n), which completes the proof.
Related classes of groups.
Several other classes of groups have been defined by their relation to the cyclic groups:
Virtually cyclic groups.
A group is called virtually cyclic if it contains a cyclic subgroup of finite index (the number of cosets that the subgroup has). In other words, any element in a virtually cyclic group can be arrived at by applying a member of the cyclic subgroup to a member in a certain finite set. Every cyclic group is virtually cyclic, as is every finite group. An infinite group is virtually cyclic if and only if it is finitely generated and has exactly two ends; an example of such a group is the product of Z/"n" and Z, in which the Z factor has finite index "n". Every abelian subgroup of a Gromov hyperbolic group is virtually cyclic.
Locally cyclic groups.
A locally cyclic group is a group in which each finitely generated subgroup is cyclic.
An example is the additive group of the rational numbers: every finite set of rational numbers is a set of integer multiples of a single unit fraction, the inverse of their lowest common denominator, and generates as a subgroup a cyclic group of integer multiples of this unit fraction.
A group is locally cyclic if and only if its lattice of subgroups is a distributive lattice.
Cyclically ordered groups.
A cyclically ordered group is a group together with a cyclic order preserved by the group structure.
Every cyclic group can be given a structure as a cyclically ordered group, consistent with the ordering of the integers (or the integers modulo the order of the group).
Every finite subgroup of a cyclically ordered group is cyclic.
Metacyclic and polycyclic groups.
A metacyclic group is a group containing a cyclic normal subgroup whose quotient is also cyclic.
These groups include the cyclic groups, the dicyclic groups, and the direct products of two cyclic groups.
The polycyclic groups generalize metacyclic groups by allowing more than one level of group extension. A group is polycyclic if it has a finite descending sequence of subgroups, each of which is normal in the previous subgroup with a cyclic quotient, ending in the trivial group. Every finitely generated abelian group or nilpotent group is polycyclic.

</doc>
<doc id="16920816" url="https://en.wikipedia.org/wiki?curid=16920816" title="D meson">
D meson

The D mesons are the lightest particle containing charm quarks. They are often studied to gain knowledge on the weak interaction. The strange D mesons (Ds) were called the "F mesons" prior to 1986.
Overview.
The D mesons were discovered in 1976 by the Mark I detector at the Stanford Linear Accelerator Center.
Since the D mesons are the lightest mesons containing a single charm quark (or antiquark), they must change the charm (anti)quark into an (anti)quark of another type to decay. Such transitions violate the internal charm quantum number, and can take place only via the weak interaction. In D mesons, the charm quark preferentially changes into a strange quark via an exchange of a W particle, therefore the D meson preferentially decays into s and s.
In November 2011, researchers at the LHCb experiment at CERN reported (3.5 sigma significance) that they have observed a direct CP violation in the neutral D meson decay, possibly beyond the Standard Model. This result builds on earlier measurements at the CDF, Belle, and BaBAR experiments.
List of D mesons.
[a] PDG reports the resonance width (Γ). Here the conversion τ =  is given instead.

</doc>
<doc id="8651" url="https://en.wikipedia.org/wiki?curid=8651" title="Dark matter">
Dark matter

Dark matter is a hypothetical kind of matter that cannot be seen with telescopes but accounts for most of the matter in the universe. The existence and properties of dark matter are inferred from its gravitational effects on visible matter, on radiation, and on the large-scale structure of the universe. Dark matter has not been detected directly, making it one of the greatest mysteries in modern astrophysics.
Dark matter neither emits nor absorbs light or any other electromagnetic radiation at any significant level. According to the Planck mission team, and based on the standard model of cosmology, the total mass–energy of the known universe contains 4.9% ordinary (baryonic) matter, 26.8% dark matter and 68.3% dark energy. Thus, dark matter is estimated to constitute 84.5% of the total matter in the universe, while dark energy plus dark matter constitute 95.1% of the total mass–energy content of the universe.
Astrophysicists hypothesized the existence of dark matter to account for discrepancies between the mass of large astronomical objects determined from their gravitational effects, and their mass as calculated from the observable matter (stars, gas, and dust) that they can be seen to contain. Their gravitational effects suggest that their masses are much greater than the observable matter survey suggests. 
Dark matter was postulated by Jan Oort in 1932, albeit based upon insufficient evidence, to account for the orbital velocities of stars in the Milky Way. In 1933, Fritz Zwicky was the first to use the virial theorem to infer the existence of unseen matter, which he referred to as "dunkle Materie" 'dark matter'. More robust evidence from galaxy rotation curves was discovered by Horace W. Babcock in 1939, but was not attributed to dark matter. The first hypothesis to postulate "dark matter" based upon robust evidence was formulated by Vera Rubin and Kent Ford in the 1960s–1970s, using galaxy rotation curves. Subsequently, many other observations have indicated the presence of dark matter in the universe, including gravitational lensing of background objects by galaxy clusters such as the Bullet Cluster, the temperature distribution of hot gas in galaxies and clusters of galaxies and, more recently, the pattern of anisotropies in the cosmic microwave background. According to consensus among cosmologists, dark matter is composed primarily of a not yet characterized type of subatomic particle.
The search for this particle, by a variety of means, is one of the major efforts in particle physics today.
Although the existence of dark matter is generally accepted by the mainstream scientific community, some alternative theories of gravity have been proposed, such as MOND and TeVeS, which try to account for the anomalous observations without requiring additional matter. However, these theories cannot account for the properties of galaxy clusters.
Overview.
Dark matter's existence is inferred from gravitational effects on visible matter and gravitational lensing of background radiation, and was originally hypothesized to account for discrepancies between calculations of the mass of galaxies, clusters of galaxies and the entire universe made through dynamical and general relativistic means, and calculations based on the mass of the visible "luminous" matter these objects contain: stars and the gas and dust of the interstellar and intergalactic medium.
The most widely accepted explanation for these phenomena is that dark matter exists and that it is most probably composed of weakly interacting massive particles (WIMPs) that interact only through gravity and the weak force. Alternative explanations have been proposed, and there is not yet sufficient experimental evidence to determine whether any of them are correct. Many experiments to detect proposed dark matter particles through non-gravitational means are under way.
One other theory suggests the existence of a “Hidden Valley”, a parallel world made of dark matter having very little in common with matter we know, and that could only interact with our visible universe through gravity.
According to observations of structures larger than star systems, as well as Big Bang cosmology interpreted under the Friedmann equations and the Friedmann–Lemaître–Robertson–Walker metric, dark matter accounts for 26.8% of the mass-energy content of the observable universe. In comparison, ordinary (baryonic) matter accounts for only 4.9% of the mass-energy content of the observable universe, with the remainder being attributable to dark energy. From these figures, matter accounts for 31.7% of the mass-energy content of the universe, and 84.5% of the matter is dark matter.
Dark matter plays a central role in state-of-the-art modeling of cosmic structure formation and galaxy formation and evolution and has measurable effects on the anisotropies observed in the cosmic microwave background (CMB). All these lines of evidence suggest that galaxies, clusters of galaxies, and the universe as a whole contain far more matter than that which is easily visible with electromagnetic radiation.
Though the theory of dark matter remains the most widely accepted theory to explain the anomalies in observed galactic rotation, some alternative theoretical approaches have been developed which broadly fall into the categories of modified gravitational laws and quantum gravitational laws.
Baryonic and nonbaryonic dark matter.
There are three separate lines of evidence that suggest the majority of dark matter is not made of baryons (ordinary matter including protons and neutrons):
A small proportion of dark matter may be baryonic dark matter: astronomical bodies, such as massive compact halo objects, which are composed of ordinary matter but emit little or no electromagnetic radiation. The study of nucleosynthesis in the Big Bang gives an upper bound on the amount of baryonic matter in the universe, which indicates that the vast majority of dark matter in the universe cannot be baryons, and thus does not form atoms. It also cannot interact with ordinary matter via electromagnetic forces; in particular, dark matter particles do not carry any electric charge.
Candidates for nonbaryonic dark matter are hypothetical particles such as axions, or supersymmetric particles; neutrinos can only form a small fraction of the dark matter, due to limits from large-scale structure and high-redshift galaxies. Unlike baryonic dark matter, nonbaryonic dark matter does not contribute to the formation of the elements in the early universe ("Big Bang nucleosynthesis") and so its presence is revealed only via its gravitational attraction. In addition, if the particles of which it is composed are supersymmetric, they can undergo annihilation interactions with themselves, possibly resulting in observable by-products such as gamma rays and neutrinos ("indirect detection").
Nonbaryonic dark matter is classified in terms of the mass of the particle(s) that is assumed to make it up, and/or the typical velocity dispersion of those particles (since more massive particles move more slowly). There are three prominent hypotheses on nonbaryonic dark matter, called cold dark matter (CDM), warm dark matter (WDM), and hot dark matter (HDM); some combination of these is also possible. The most widely discussed models for nonbaryonic dark matter are based on the cold dark matter hypothesis, and the corresponding particle is most commonly assumed to be a weakly interacting massive particle (WIMP). Hot dark matter may include (massive) neutrinos, but observations imply that only a small fraction of dark matter can be hot. Cold dark matter leads to a "bottom-up" formation of structure in the universe while hot dark matter would result in a "top-down" formation scenario; since the late 1990s, the latter has been ruled out by observations of high-redshift galaxies such as the Hubble Ultra-Deep Field.
Observational evidence.
The first person to interpret evidence and infer the presence of dark matter was Dutch astronomer Jan Oort, a pioneer in radio astronomy, in 1932. Oort was studying stellar motions in the local galactic neighbourhood and found that the mass in the galactic plane must be greater than what was observed, but this measurement was later determined to be essentially erroneous. 
In 1933, the Swiss astrophysicist Fritz Zwicky, who studied clusters of galaxies while working at the California Institute of Technology, made a similar inference. Zwicky applied the virial theorem to the Coma cluster of galaxies and obtained evidence of unseen mass. Zwicky estimated the cluster's total mass based on the motions of galaxies near its edge and compared that estimate to one based on the number of galaxies and total brightness of the cluster. He estimated that there was about 400 times more mass than was visually observable. The gravity effect of the visible galaxies in the cluster would be far too small for such fast orbits, unless there was mass hidden from visual observation. This is known as the "missing mass problem". Based on these conclusions, Zwicky inferred that there must be some non-visible form of matter which would provide enough mass and gravitation attraction to hold the cluster together. This was the first formal inference about the existence of dark matter. 
Zwicky's estimates were not accurate and were off by more than an order of magnitude. Notwithstanding, although the same calculation today shows a smaller factor, based on greater values for the mass of luminous material, it is still clear that the great majority of matter in Zwicky's calculations was correctly inferred to be dark.
Much of the evidence for dark matter comes from the study of the motions of galaxies. Many of these appear to be fairly uniform, so by the virial theorem, the total kinetic energy should be half the total gravitational binding energy of the galaxies. Observationally, however, the total kinetic energy is found to be much greater. In particular, assuming the gravitational mass is due to only the visible matter of the galaxy, stars far from the center of galaxies have much higher velocities than are predicted by the virial theorem. Galactic rotation curves, which illustrate the velocity of rotation versus the distance from the galactic center, show the well known phenomenology that cannot be explained by the presence of the visible matter only. Assuming that the visible material makes up only a small part of the cluster's mass is the most straightforward way of accounting for this discrepancy. The distribution of dark matter in galaxies required to explain the motion of the observed baryonic matter suggests the presence of a roughly spherically symmetric, centrally concentrated halo of dark matter with the visible matter concentrated in a disc at the center. Low surface brightness dwarf galaxies are important sources of information for studying dark matter, as they have an uncommonly low ratio of visible matter to dark matter, and have few bright stars at the center which would otherwise impair observations of the rotation curve of outlying stars.
Gravitational lensing observations of galaxy clusters allow direct estimates of the gravitational mass based on its effect on light coming from background galaxies, since large collections of matter (dark or otherwise) will gravitationally deflect light. In clusters such as Abell 1689, lensing observations confirm the presence of considerably more mass than is indicated by the clusters' light alone. In the Bullet Cluster, lensing observations show that much of the lensing mass is separated from the X-ray-emitting baryonic mass. In July 2012, lensing observations were used to identify a "filament" of dark matter between two clusters of galaxies, as cosmological simulations have predicted.
Galaxy rotation curves.
The first robust indications that the mass to light ratio was anything other than unity came from measurements of galaxy rotation curves. In 1939, Horace W. Babcock reported in his PhD thesis measurements of the rotation curve for the Andromeda nebula which suggested that the mass-to-luminosity ratio increases radially. He, however, attributed it to either absorption of light within the galaxy or modified dynamics in the outer portions of the spiral and not to any form of missing matter.
In the late 1960s and early 1970s, Vera Rubin was the first to both make robust measurements indicating the existence of dark matter and attribute them to dark matter. Rubin worked with a new sensitive spectrograph that could measure the velocity curve of edge-on spiral galaxies to a greater degree of accuracy than previously. Together with fellow staff-member Kent Ford, Rubin announced at a 1975 meeting of the American Astronomical Society the discovery that most stars in spiral galaxies orbit at roughly the same speed, which implied that the mass densities of the galaxies were uniform well beyond the regions containing most of the stars (the galactic bulge), a result independently found in 1978. An influential paper presented Rubin's results in 1980. Rubin's observations and calculations showed that most galaxies must contain about six times as much “dark” mass as could be accounted for by the visible stars. Eventually other astronomers began to corroborate her work. It soon became well-established that most galaxies were dominated by "dark matter":
Simulated dark matter haloes have significantly steeper density profiles (having central cusps) than are inferred from observations, which is a problem for cosmological models with dark matter at the smallest scale of galaxies (as of 2008). This may only be a problem of resolution: star-forming regions which might alter the dark matter distribution via outflows of gas have been too small to resolve and model simultaneously with larger dark matter clumps. A recent simulation of a dwarf galaxy, that included these star-forming regions, reported that strong outflows from supernovae remove low-angular-momentum gas, which inhibits the formation of a galactic bulge and decreases the dark matter density to less than half of what it would have been in the central kiloparsec. These simulation predictions—bulgeless and with shallow central dark matter density profiles—correspond closely to observations of actual dwarf galaxies. There are no such discrepancies at the larger scales of clusters of galaxies and greater, or in the outer regions of haloes of galaxies.
The exceptions to this general picture of dark matter haloes for galaxies appear to be galaxies with mass-to-light ratios that are close to that of the stars they contain. Otherwise, numerous observations have been made that do indicate the presence of dark matter in various parts of the cosmos, such as observations of the cosmic microwave background, of supernovas used as distance measures, of gravitational lensing at various scales, and many types of sky survey. Starting with Rubin's findings for spiral galaxies, such robust observational evidence for dark matter has collected over the decades to the point that by the 1980s most astrophysicists have accepted its existence. As a unifying concept, dark matter is one of the dominant features considered in the analysis of structures on the order of galactic scale and larger.
Velocity dispersions of galaxies.
Rubin's pioneering work has stood the test of time. Measurements of velocity curves in spiral galaxies were soon followed up with velocity dispersions of elliptical galaxies. While some elliptical galaxies display lower mass-to-light ratios, measurements of ellipticals generally indicate a relatively high dark matter content. Likewise, measurements of the diffuse interstellar gas found at the edge of galaxies indicate not only dark matter distributions that extend beyond the visible limit of the galaxies, but also that the galaxies are virialized (i.e., gravitationally bound and orbiting each other with velocities which appear to disproportionately correspond to predicted orbital velocities of general relativity) up to ten times their visible radii. This has the effect of pushing up the dark matter as a fraction of the total matter from 50% as measured by Rubin to the now accepted value of nearly 95%.
There are places where dark matter seems to be a small component or totally absent. Globular clusters show little evidence that they contain dark matter, though their orbital interactions with galaxies do show evidence for galactic dark matter. For some time, measurements of the velocity profile of stars seemed to indicate concentration of dark matter in the disk of the Milky Way. It now appears, however, that the high concentration of baryonic matter in the disk of the galaxy (especially in the interstellar medium) can account for this motion. Galaxy mass profiles are thought to look very different from the light profiles. The typical model for dark matter galaxies is a smooth, spherical distribution in virialized halos. Such would have to be the case to avoid small-scale (stellar) dynamical effects. Recent research reported in January 2006 from the University of Massachusetts Amherst would explain the previously mysterious warp in the disk of the Milky Way by the interaction of the Large and Small Magellanic Clouds and the predicted 20 fold increase in mass of the Milky Way taking into account dark matter.
In 2005, astronomers from Cardiff University claimed to have discovered a galaxy made almost entirely of dark matter, 50 million light years away in the Virgo Cluster, which was named VIRGOHI21. Unusually, VIRGOHI21 does not appear to contain any visible stars: it was seen with radio frequency observations of hydrogen. Based on rotation profiles, the scientists estimate that this object contains approximately 1000 times more dark matter than hydrogen and has a total mass of about 1/10 that of the Milky Way. For comparison, the Milky Way is estimated to have roughly 10 times as much dark matter as ordinary matter. Models of the Big Bang and structure formation have suggested that such dark galaxies should be very common in the universe, but none had previously been detected.
There are some galaxies, such as NGC 3379, whose velocity profile indicates an absence of dark matter.
Galaxy clusters and gravitational lensing.
Galaxy clusters are especially important for dark matter studies since their masses can be estimated in three independent ways:
Generally these three methods are in reasonable agreement, that clusters contain much more matter than suggested by the visible components of galaxies and gas.
A gravitational lens is formed when the light from a more distant source (such as a quasar) is "bent" around a massive object (such as a cluster of galaxies) lying inline between the source object and the observer. The process is known as gravitational lensing.
The galaxy cluster Abell 2029 is composed of thousands of galaxies enveloped in a cloud of hot gas, and an amount of dark matter equivalent to more than . At the center of this cluster is an enormous, elliptically shaped galaxy that is thought to have been formed from the mergers of many smaller galaxies. The measured orbital velocities of galaxies within galactic clusters have been found to be consistent with dark matter observations.
Another important tool for future dark matter observations is gravitational lensing. Lensing relies on the bending of light, as described by general relativity, to predict masses without relying on observations of the distant galaxies dynamics, and so is a completely independent means of measuring the dark matter. Strong lensing, the observed distortion of background galaxies into arcs when their light passes through such a gravitational lens, has been observed around a few distant clusters including Abell 1689 (pictured). By measuring the distortion geometry, the mass of the intervening cluster causing the phenomena can be obtained. In the dozens of cases where this has been done, the mass-to-light ratios obtained correspond to the dynamical dark matter measurements of clusters.
Weak gravitational lensing investigates minute distortions of galaxies, using statistical analyses of vast galaxy surveys, caused by foreground objects. By examining the apparent shear deformation of the adjacent background galaxies, astrophysicists can characterize the mean distribution of dark matter and have found mass-to-light ratios that correspond to dark matter densities predicted by other large-scale structure measurements. The correspondence of the two gravitational lens techniques to other dark matter measurements has convinced almost all astrophysicists that dark matter actually exists as a major component of the universe's composition.
The most direct observational evidence to date for dark matter comes from a system known as the Bullet Cluster. In most regions of the universe, dark matter and visible matter are found together, as expected due to their mutual gravitational attraction. In the Bullet Cluster however, a collision between two galaxy clusters appears to have caused a separation of dark matter and baryonic matter. X-ray observations show that much of the baryonic matter (in the form of 107–108 Kelvin gas or plasma) in the system is concentrated in the center of the system. Electromagnetic interactions between passing gas particles caused them to slow and settle near the point of impact of those galaxies. However, weak gravitational lensing observations of the same system show that much of the mass resides outside of the central region of baryonic gas. Because dark matter does not interact by electromagnetic forces, it would not have been slowed as the X-ray visible gas, so the dark matter components of the two clusters passed through each other without slowing substantially, throwing the dark matter further out than that of the baryonic gas. This accounts for the separation. Unlike the galactic rotation curves, this evidence for dark matter is independent of the details of Newtonian gravity, so it is claimed to be direct evidence of the existence of dark matter.
Another galaxy cluster, known as the Train Wreck Cluster/Abell 520, initially appeared to have an unusually massive and dark matter core containing few of the cluster's galaxies, which presented problems for standard dark matter models. However, more precise observations since that time have shown that the earlier observations were misleading, and that the distribution of dark matter and its ratio to normal matter are very similar to those in galaxies in general, making novel explanations unnecessary.
The observed behavior of dark matter in clusters constrains whether and how much dark matter scatters off other dark matter particles, quantified as its self-interaction cross section. More simply, the question is whether the dark matter has pressure, and thus can be described as a perfect fluid that has no damping. The distribution of mass (and thus dark matter) in galaxy clusters has been used to argue both for and against the existence of significant self-interaction in dark matter. Specifically, the distribution of dark matter in merging clusters such as the Bullet Cluster shows that dark matter scatters off other dark matter particles only very weakly if at all.
A currently ongoing survey using the Subaru telescope is using weak lensing to analyze background light, bent by dark matter, to determine how dark matter is distributed in the foreground. The analysis of dark matter and its effects could determine how dark matter assembled over time, which can be related to the history of the expansion of the universe, and could reveal some physical properties of dark energy, its strength and how it has changed over time. The survey is observing galaxies more than a billion light-years away, across an area greater than a thousand square degrees (about one fortieth of the entire sky).
Cosmic microwave background.
Angular fluctuations in the cosmic microwave background (CMB) spectrum provide evidence for dark matter. Since the 1964 discovery and confirmation of the CMB radiation,
many measurements of the CMB have supported and constrained this theory. The NASA Cosmic Background Explorer (COBE) found that the CMB spectrum to be a blackbody spectrum with a temperature of 2.726 K. In 1992, COBE detected fluctuations (anisotropies) in the CMB spectrum, at a level of about one part in 105. In the following decade, CMB anisotropies were further investigated by a large number of ground-based and balloon experiments. The primary goal of those was to measure the angular scale of the first acoustic peak of the power spectrum of the anisotropies, for which COBE did not have sufficient resolution. In 2000–2001, several experiments, most notably BOOMERanG found the universe to be almost spatially flat by measuring the typical angular size of the anisotropies. During the 1990s, the first peak was measured with increasing sensitivity and by 2000 the BOOMERanG experiment reported that the highest power fluctuations occur at scales of approximately one degree. These measurements were able to rule out cosmic strings as the leading theory of cosmic structure formation, and suggested cosmic inflation was the correct theory.
A number of ground-based interferometers provided measurements of the fluctuations with higher accuracy over the next three years, including the Very Small Array, the Degree Angular Scale Interferometer (DASI) and the Cosmic Background Imager (CBI). DASI made the first detection of the polarization of the CMB, and the CBI provided the first E-mode polarization spectrum with compelling evidence that it is out of phase with the T-mode spectrum. COBE's successor, the Wilkinson Microwave Anisotropy Probe (WMAP) has provided the most detailed measurements of (large-scale) anisotropies in the CMB as of 2009 with ESA's Planck spacecraft returning more detailed results in 2012-2014. WMAP's measurements played the key role in establishing the current Standard Model of Cosmology, namely the Lambda-CDM model, a flat universe dominated by dark energy, supplemented by dark matter
and atoms with density fluctuations seeded by a Gaussian, adiabatic, nearly scale invariant
process. The basic properties of this universe are determined by five numbers: the density of
matter, the density of atoms, the age of the universe (or equivalently, the Hubble constant
today), the amplitude of the initial fluctuations, and their scale dependence.
A successful Big Bang cosmology theory must fit with all available astronomical observations, including the CMB. In cosmology, the CMB is explained as relic radiation from shortly after the big bang. The anisotropies in the CMB are explained as being the result of acoustic oscillations in the photon-baryon plasma (prior to the emission of the CMB after the photons decouple from the baryons 379,000 years after the Big Bang) whose restoring force is gravity. Ordinary (baryonic) matter interacts strongly by way of radiation whereas dark matter particles, such as WIMPs for example, do not; both affect the oscillations by way of their gravity, so the two forms of matter will have different effects. The typical angular scales of the oscillations in the CMB, measured as the power spectrum of the CMB anisotropies, thus reveal the different effects of baryonic matter and dark matter. The CMB power spectrum shows a large first peak and smaller successive peaks, with three peaks resolved as of 2009. The first peak tells mostly about the density of baryonic matter and the third peak mostly about the density of dark matter, measuring the density of matter and the density of atoms in the universe.
Sky surveys and baryon acoustic oscillations.
The acoustic oscillations in the early universe (see the previous section) have left their imprint on visible matter by way of Baryon Acoustic Oscillation (BAO) clustering, in a way that can be measured with sky surveys such as the Sloan Digital Sky Survey and the 2dF Galaxy Redshift Survey. These measurements are consistent with those of the CMB derived from the WMAP spacecraft and further constrain the Lambda CDM model and dark matter. Note that the CMB data and the BAO data measure the acoustic oscillations at very different distance scales.
Type Ia supernovae distance measurements.
Type Ia supernovae can be used as "standard candles" to measure extragalactic distances, and extensive data sets of these supernovae can be used to constrain cosmological models. They constrain the dark energy density ΩΛ = ~0.713 for a flat, Lambda CDM universe and the parameter formula_1 for a quintessence model. Once again, the values obtained are roughly consistent with those derived from the WMAP observations and further constrain the Lambda CDM model and (indirectly) dark matter.
Lyman-alpha forest.
In astronomical spectroscopy, the Lyman-alpha forest is the sum of the absorption lines arising from the Lyman-alpha transition of the neutral hydrogen in the spectra of distant galaxies and quasars.
Observations of the Lyman-alpha forest can also be used to constrain cosmological models. These constraints are again in agreement with those obtained from WMAP data.
Structure formation.
Dark matter is crucial to the Big Bang model of cosmology as a component which corresponds directly to measurements of the parameters associated with Friedmann cosmology solutions to general relativity. In particular, measurements of the cosmic microwave background anisotropies correspond to a cosmology where much of the matter interacts with photons more weakly than the known forces that couple light interactions to baryonic matter. Likewise, a significant amount of non-baryonic, cold matter is necessary to explain the large-scale structure of the universe.
Observations suggest that structure formation in the universe proceeds hierarchically, with the smallest structures collapsing first and followed by galaxies and then clusters of galaxies. As the structures collapse in the evolving universe, they begin to "light up" as the baryonic matter heats up through gravitational contraction and approaches hydrostatic pressure balance. Originally, baryonic matter had too high a temperature, and pressure left over from the Big Bang to allow collapse and form smaller structures, such as stars, via the Jeans instability. Dark matter acts as a compactor allowing the creation of structure where there would not have been any. This model not only corresponds with statistical surveying of the visible structure in the universe but also corresponds precisely to the dark matter predictions of the cosmic microwave background.
This "bottom up" model of structure formation requires something like cold dark matter to succeed. Large computer simulations of billions of dark matter particles have been used to confirm that the cold dark matter model of structure formation is consistent with the structures observed in the universe through galaxy surveys, such as the Sloan Digital Sky Survey and 2dF Galaxy Redshift Survey, as well as observations of the Lyman-alpha forest. These studies have been crucial in constructing the Lambda-CDM model which measures the cosmological parameters, including the fraction of the universe made up of baryons and dark matter. The recent discovery of the structure of Laniakea, a 500 million light year structure is currently the limit to structural formation in the universe. However, Laniakea is not gravitationally bound and is projected to be torn apart by dark energy.
There are, however, several points of tension between observation and simulations of structure formation driven by dark matter.
There is evidence that there exist 10 to 100 times fewer small galaxies than permitted by what the dark matter theory of galaxy formation predicts. This is known as the dwarf galaxy problem.
In addition, the simulations predict dark matter distributions with a very dense cusp near the centers of galaxies, but the observed halos are smoother than predicted.
History of the search for its composition.
Although dark matter had historically been inferred from many astronomical observations, its composition long remained speculative. Early theories of dark matter concentrated on hidden heavy normal objects (such as black holes, neutron stars, faint old white dwarfs, and brown dwarfs) as the possible candidates for dark matter, collectively known as massive compact halo objects or MACHOs. Astronomical surveys for gravitational microlensing, including the MACHO, EROS and OGLE projects, along with Hubble telescope searches for ultra-faint stars, have not found enough of these hidden MACHOs. Some hard-to-detect baryonic matter, such as MACHOs and some forms of gas, were additionally speculated to make a contribution to the overall dark matter content, but evidence indicated such would constitute only a small portion.
Furthermore, data from a number of lines of other evidence, including galaxy rotation curves, gravitational lensing, structure formation, and the fraction of baryons in clusters and the cluster abundance combined with independent evidence for the baryon density, indicated that 85–90% of the mass in the universe does not interact with the electromagnetic force. This "nonbaryonic dark matter" is evident through its gravitational effect. Consequently, the most commonly held view was that dark matter is primarily non-baryonic, made of one or more elementary particles other than the usual electrons, protons, neutrons, and known neutrinos. The most commonly proposed particles then became WIMPs (Weakly Interacting Massive Particles, including neutralinos), axions, or sterile neutrinos, though many other possible candidates have been proposed.
Dark matter candidates can be divided into three classes, called
"cold", "warm" and "hot" dark matter.
These categories do not correspond to an actual temperature, but instead refer to how fast the particles were moving, thus how far they moved due to random motions in the early universe, before they slowed due to the expansion of the universe – this is an important distance called the "free streaming length". Primordial density fluctuations smaller than this free-streaming length get washed out as particles move from overdense to underdense regions, while fluctuations larger than the free-streaming length are unaffected; therefore this free-streaming length sets a minimum scale for structure formation.
Though a fourth category had been considered early on, called mixed dark matter, it was quickly eliminated (from the 1990s) since the discovery of dark energy.
As an example, Davis "et al." wrote in 1985:
The full calculations are quite technical, but an approximate dividing line is that "warm" dark matter particles became non-relativistic when the universe was approximately 1 year old and 1 millionth of its present size; standard hot big bang theory implies the universe was then in the radiation-dominated era (photons and neutrinos), with a photon temperature 2.7 million K. Standard physical cosmology gives the particle horizon size as 2ct in the radiation-dominated era, thus 2 light-years, and a region of this size would expand to 2 million light years today (if there were no structure formation). The actual free-streaming length is roughly 5 times larger than the above length, since the free-streaming length continues to grow slowly as particle velocities decrease inversely with the scale factor after they become non-relativistic; therefore, in this example the free-streaming length would correspond to 10 million light-years or 3 Mpc today, which is around the size containing on average the mass of a large galaxy.
The above temperature of 2.7 million K gives a typical photon energy of 250 electron-volts, thereby setting a typical mass scale for "warm" dark matter: particles much more massive than this, such as GeV – TeV mass WIMPs, would become non-relativistic much earlier than 1 year after the Big Bang and thus have a free-streaming length much smaller than a proto-galaxy, making them cold dark matter. Conversely, much lighter particles, such as neutrinos with masses of only a few eV, have a free-streaming length much larger than a proto-galaxy, thus making them hot dark matter.
Cold dark matter.
Today, cold dark matter is the simplest explanation for most cosmological observations. "Cold" dark matter is dark matter composed of constituents with a free-streaming length much smaller than the ancestor of a galaxy-scale perturbation. This is currently the area of greatest interest for dark matter research, as hot dark matter does not seem to be viable for galaxy and galaxy cluster formation, and most particle candidates become non-relativistic at very early times, hence are classified as cold.
The composition of the constituents of cold dark matter is currently unknown. Possibilities range from large objects like MACHOs (such as black holes) or RAMBOs, to new particles like WIMPs and axions. Possibilities involving normal baryonic matter include brown dwarfs, other stellar remnants such as white dwarfs, or perhaps small, dense chunks of heavy elements.
Studies of big bang nucleosynthesis and gravitational lensing have convinced most scientists that MACHOs of any type cannot be more than a small fraction of the total dark matter. Black holes of nearly any mass are ruled out as a primary dark matter constituent by a variety of searches and constraints. According to A. Peter: "... the only "really plausible" dark-matter candidates are new particles."
The DAMA/NaI experiment and its successor DAMA/LIBRA have claimed to directly detect dark matter particles passing through the Earth, but many scientists remain skeptical, as negative results from similar experiments seem incompatible with the DAMA results.
Many supersymmetric models naturally give rise to stable dark matter candidates in the form of the Lightest Supersymmetric Particle (LSP). Separately, heavy sterile neutrinos exist in non-supersymmetric extensions to the standard model that explain the small neutrino mass through the seesaw mechanism.
Warm dark matter.
Warm dark matter refers to particles with a free-streaming length comparable to the size of a region which subsequently evolved into a dwarf galaxy. This leads to predictions which are very similar to cold dark matter on large scales, including the CMB, galaxy clustering and large galaxy rotation curves, but with less small-scale density perturbations. This reduces the predicted abundance of dwarf galaxies and may lead to lower density of dark matter in the central parts of large galaxies; some researchers consider this may be a better fit to observations. A challenge for this model is that there are no very well-motivated particle physics candidates with the required mass ~ 300 eV to 3000 eV.
There have been no particles discovered so far that can be categorized as warm dark matter. There is a postulated candidate for the warm dark matter category, which is the sterile neutrino: a heavier, slower form of neutrino which does not even interact through the Weak force unlike regular neutrinos. Interestingly, some modified gravity theories, such as Scalar-tensor-vector gravity, also require that a warm dark matter exist to make their equations work out.
Hot dark matter.
Hot dark matter consists of particles that have a free-streaming length much larger than that of a proto-galaxy.
An example of hot dark matter is already known: the neutrino. Neutrinos were discovered quite separately from the search for dark matter, and long before it seriously began: they were first postulated in 1930, and first detected in 1956. Neutrinos have a very small mass: at least 100,000 times less massive than an electron. Other than gravity, neutrinos only interact with normal matter via the weak force making them very difficult to detect (the weak force only works over a small distance, thus a neutrino will only trigger a weak force event if it hits a nucleus directly head-on). This would make them 'weakly interacting light particles' (WILPs), as opposed to cold dark matter's theoretical candidates, the weakly interacting massive particles (WIMPs).
There are three different known flavors of neutrinos (i.e., the "electron", "muon", and "tau" neutrinos), and their masses are slightly different. The resolution to the solar neutrino problem demonstrated that these three types of neutrinos actually change and oscillate from one flavor to the others and back as they are in-flight. It is hard to determine an exact upper bound on the collective average mass of the three neutrinos (let alone a mass for any of the three individually). For example, if the average neutrino mass were chosen to be over 50 "eV/c"2 (which is still less than 1/10,000th of the mass of an electron), just by the sheer number of them in the universe, the universe would collapse due to their mass. So other observations have served to estimate an upper-bound for the neutrino mass. Using cosmic microwave background data and other methods, the current conclusion is that their average mass probably does not exceed 0.3 "eV/c"2 Thus, the normal forms of neutrinos cannot be responsible for the measured dark matter component from cosmology.
Hot dark matter was popular for a time in the early 1980s, but it suffers from a severe problem: because all galaxy-size density fluctuations get washed out by free-streaming, the first objects that can form are huge supercluster-size pancakes, which then were theorised somehow to fragment into galaxies. Deep-field observations clearly show that galaxies formed at early times, with clusters and superclusters forming later as galaxies clump together, so any model dominated by hot dark matter is seriously in conflict with observations.
Mixed dark matter.
Mixed dark matter is a now obsolete model, with a specifically chosen mass ratio of 80% cold dark matter and 20% hot dark matter (neutrinos) content. Though it is presumable that hot dark matter coexists with cold dark matter in any case, there was a very specific reason for choosing this particular ratio of hot to cold dark matter in this model.
During the early 1990s it became steadily clear that a universe with critical density of
cold dark matter did not fit the COBE and large-scale galaxy clustering observations; either the 80/20 mixed dark matter model, or LambdaCDM, were able to reconcile these. With the discovery of the accelerating universe from supernovae, and more accurate measurements of CMB anisotropy and galaxy clustering, the mixed dark matter model was essentially ruled out while the concordance LambdaCDM model remained a good fit.
Detection.
If the dark matter within the Milky Way is made up of Weakly Interacting Massive Particles (WIMPs), then millions, possibly billions, of WIMPs must pass through every square centimeter of the Earth each second.
There are many experiments currently running, or planned, aiming to test this hypothesis by searching for WIMPs. Although WIMPs are the historically more popular dark matter candidate for searches, there are experiments searching for other particle candidates; the Axion Dark Matter eXperiment (ADMX) is currently searching for the dark matter axion, a well-motivated and constrained dark matter source. It is also possible that dark matter consists of very heavy hidden sector particles which only interact with ordinary matter via gravity.
These experiments can be divided into two classes: direct detection experiments, which search for the scattering of dark matter particles off atomic nuclei within a detector; and indirect detection, which look for the products of WIMP annihilations.
An alternative approach to the detection of WIMPs in nature is to produce them in the laboratory. Experiments with the Large Hadron Collider (LHC) may be able to detect WIMPs produced in collisions of the LHC proton beams. Because a WIMP has negligible interactions with matter, it may be detected indirectly as (large amounts of) missing energy and momentum which escape the LHC detectors, provided all the other (non-negligible) collision products are detected. These experiments could show that WIMPs can be created, but it would still require a direct detection experiment to show that they exist in sufficient numbers to account for dark matter.
Direct detection experiments.
Direct detection experiments usually operate in deep underground laboratories to reduce the background from cosmic rays. These include: the Stawell mine (Australia); the Soudan mine; the SNOLAB underground laboratory at Sudbury, Ontario (Canada); the Gran Sasso National Laboratory (Italy); the Canfranc Underground Laboratory (Spain); the Boulby Underground Laboratory (United Kingdom); the Deep Underground Science and Engineering Laboratory, South Dakota (United States); and the Particle and Astrophysical Xenon Detector (China).
The majority of present experiments use one of two detector technologies: cryogenic detectors, operating at temperatures below 100mK, detect the heat produced when a particle hits an atom in a crystal absorber such as germanium. Noble liquid detectors detect the flash of scintillation light produced by a particle collision in liquid xenon or argon. Cryogenic detector experiments include: CDMS, CRESST, EDELWEISS, EURECA. Noble liquid experiments include ZEPLIN, XENON, DEAP, ArDM, WARP, DarkSide, PandaX, and LUX, the Large Underground Xenon experiment. Both of these detector techniques are capable of distinguishing background particles which scatter off electrons, from dark matter particles which scatter off nuclei. Other experiments include SIMPLE and PICASSO.
The DAMA/NaI, DAMA/LIBRA experiments have detected an annual modulation in the event rate, which they claim is due to dark matter particles. (As the Earth orbits the Sun, the velocity of the detector relative to the dark matter halo will vary by a small amount depending on the time of year). This claim is so far unconfirmed and difficult to reconcile with the negative results of other experiments assuming that the WIMP scenario is correct.
Directional detection of dark matter is a search strategy based on the motion of the Solar System around the Galactic Center.
By using a low pressure TPC, it is possible to access information on recoiling tracks (3D reconstruction if possible) and to constrain the WIMP-nucleus kinematics. WIMPs coming from the direction in which the Sun is travelling (roughly in the direction of the Cygnus constellation) may then be separated from background noise, which should be isotropic. Directional dark matter experiments include DMTPC, DRIFT, Newage and MIMAC.
On 17 December 2009, CDMS researchers reported two possible WIMP candidate events. They estimate that the probability that these events are due to a known background (neutrons or misidentified beta or gamma events) is 23%, and conclude "this analysis cannot be interpreted as significant evidence for WIMP interactions, but we cannot reject either event as signal."
More recently, on 4 September 2011, researchers using the CRESST detectors presented evidence of 67 collisions occurring in detector crystals from subatomic particles, calculating there is a less than 1 in 10,000 chance that all were caused by known sources of interference or contamination. It is quite possible then that many of these collisions were caused by WIMPs, and/or other unknown particles.
Indirect detection experiments.
Indirect detection experiments search for the products of WIMP annihilation or decay. If WIMPs are Majorana particles (WIMPs are their own antiparticle) then two WIMPs could annihilate to produce gamma rays or Standard Model particle-antiparticle pairs. Additionally, if the WIMP is unstable, WIMPs could decay into standard model particles. These processes could be detected indirectly through an excess of gamma rays, antiprotons or positrons emanating from regions of high dark matter density. The detection of such a signal is not conclusive evidence for dark matter, as the production of gamma rays from other sources is not fully understood.
The EGRET gamma ray telescope observed more gamma rays than expected from the Milky Way, but scientists concluded that this was most likely due to a mis-estimation of the telescope's sensitivity.
The Fermi Gamma-ray Space Telescope, launched 11 June 2008, is searching for gamma rays from dark matter annihilation and decay. In April 2012, an analysis of previously available data from its Large Area Telescope instrument produced strong statistical evidence of a 130 GeV line in the gamma radiation coming from the center of the Milky Way. At the time, WIMP annihilation was the most probable explanation for that line.
At higher energies, ground-based gamma-ray telescopes have set limits on the annihilation of dark matter in dwarf spheroidal galaxies and in clusters of galaxies.
The PAMELA experiment (launched 2006) has detected a larger number of positrons than expected. These extra positrons could be produced by dark matter annihilation, but may also come from pulsars. No excess of anti-protons has been observed.
The Alpha Magnetic Spectrometer on the International Space Station is designed to directly measure the fraction of cosmic rays which are positrons. The first results, published in April 2013, indicate an excess of high-energy cosmic rays which could potentially be due to annihilation of dark matter.
A few of the WIMPs passing through the Sun or Earth may scatter off atoms and lose energy. This way a large population of WIMPs may accumulate at the center of these bodies, increasing the chance that two will collide and annihilate. This could produce a distinctive signal in the form of high-energy neutrinos originating from the center of the Sun or Earth.
It is generally considered that the detection of such a signal would be the strongest indirect proof of WIMP dark matter. High-energy neutrino telescopes such as AMANDA, IceCube and ANTARES are searching for this signal.
WIMP annihilation from the Milky Way Galaxy as a whole may also be detected in the form of various annihilation products.
The Galactic Center is a particularly good place to look because the density of dark matter may be very high there.
Alternative theories.
Mass in extra dimensions.
In some multidimensional theories, the force of gravity is the unique force able to have an effect across all the various extra dimensions, which would explain the relative weakness of the force of gravity compared to the other known forces of nature that would not be able to cross into extra dimensions: electromagnetism, strong interaction, and weak interaction.
In that case, dark matter would be a perfect candidate for matter that would exist in other dimensions and that could only interact with the matter on our dimensions through gravity. That dark matter located on different dimensions could potentially aggregate in the same way as the matter in our visible universe does, forming exotic galaxies.
Topological defects.
Dark matter could consist of primordial defects (defects originating with the birth of the universe) in the topology of quantum fields, which would contain energy and therefore gravitate. This possibility may be investigated by the use of an orbital network of atomic clocks, which would register the passage of topological defects by monitoring the synchronization of the clocks. The Global Positioning System may be able to operate as such a network.
Modified gravity.
Numerous alternative theories have been proposed to explain these observations without the need for a large amount of undetected matter. Most of these theories modify the laws of gravity established by Newton and Einstein.
The earliest modified gravity model to emerge was Mordehai Milgrom's Modified Newtonian Dynamics (MOND) in 1983, which adjusts Newton's laws to create a stronger gravitational field when gravitational acceleration levels become tiny (such as near the rim of a galaxy). It had some success explaining galactic-scale features, such as rotational velocity curves of elliptical galaxies, and dwarf elliptical galaxies, but did not successfully explain galaxy cluster gravitational lensing. However, MOND was not relativistic, since it was just a straight adjustment of the older Newtonian account of gravitation, not of the newer account in Einstein's general relativity. Soon after 1983, attempts were made to bring MOND into conformity with general relativity; this is an ongoing process, and many competing hypotheses have emerged based around the original MOND model—including TeVeS, MOG or STV gravity, and phenomenological covariant approach, among others.
In 2007, John W. Moffat proposed a modified gravity hypothesis based on the nonsymmetric gravitational theory (NGT) that claims to account for the behavior of colliding galaxies. This model requires the presence of non-relativistic neutrinos, or other candidates for (cold) dark matter, to work.
Another proposal uses a gravitational backreaction in an emerging theoretical field that seeks to explain gravity between objects as an action, a reaction, and then a back-reaction. Simply, an object A affects an object B, and the object B then re-affects object A, and so on: creating a sort of feedback loop that strengthens gravity.
In 2008, another group has proposed a modification of large-scale gravity in a hypothesis named "dark fluid". In this formulation, the attractive gravitational effects attributed to dark matter are instead a side-effect of dark energy. Dark fluid combines dark matter and dark energy in a single energy field that produces different effects at different scales. This treatment is a simplified approach to a previous fluid-like model called the generalized Chaplygin gas model where the whole of spacetime is a compressible gas. Dark fluid can be compared to an atmospheric system. Atmospheric pressure causes air to expand, but part of the air can collapse to form clouds. In the same way, the dark fluid might generally expand, but it also could collect around galaxies to help hold them together.
Another set of proposals is based on the possibility of a double metric tensor for space-time. It has been argued that time-reversed solutions in general relativity require such double metric for consistency, and that both dark matter and dark energy can be understood in terms of time-reversed solutions of general relativity.
Fractality of spacetime.
Applying relativity to fractal non-differentiable spacetime, Laurent Nottale suggests that potential energy may arise due to the fractality of spacetime, which would account for the missing mass-energy observed at cosmological scales.
Popular culture.
Mention of dark matter is made in some video games and other works of fiction. In such cases, it is usually attributed extraordinary physical or magical properties. Such descriptions are often inconsistent with the properties of dark matter proposed in physics and cosmology.

</doc>
<doc id="197774" url="https://en.wikipedia.org/wiki?curid=197774" title="Decay chain">
Decay chain

In nuclear science, the decay chain refers to the radioactive decay of different discrete radioactive decay products as a chained series of transformations. They are also known as "radioactive cascades". Most radioisotopes do not decay directly to a stable state, but rather undergo a series of decays until eventually a stable isotope is reached.
Decay stages are referred to by their relationship to previous or subsequent stages. A "parent isotope" is one that undergoes decay to form a "daughter isotope". One example of this is uranium (atomic number 92) decaying into thorium (atomic number 90). The daughter isotope may be stable or it may decay to form a daughter isotope of its own. The daughter of a daughter isotope is sometimes called a "granddaughter isotope".
The time it takes for a single parent atom to decay to an atom of its daughter isotope can vary widely, not only for different parent-daughter chains, but also for identical pairings of parent and daughter isotopes. While the decay of a single atom occurs spontaneously, the decay of an initial population of identical atoms over time "t", follows a decaying exponential distribution, "e−λt", where λ is called a decay constant. Because of this exponential nature, one of the properties of an isotope is its half-life, the time by which half of an initial number of identical parent radioisotopes have decayed to their daughters. Half-lives have been determined in laboratories for thousands of radioisotopes (or, radionuclides). These can range from nearly instantaneous to as much as 1019 years or more.
The intermediate stages each emit the same amount of radioactivity as the original radioisotope (though not the same energy). When equilibrium is achieved, a granddaughter isotope is present in direct proportion to its half-life; but since its activity is inversely proportional to its half-life, each nuclide in the decay chain finally contributes as much radioactivity as the head of the chain, though not the same energy. For example, uranium-238 is weakly radioactive, but pitchblende, a uranium ore, is 13 times more radioactive than the pure uranium metal of the same amount because of the radium and other daughter isotopes it contains. Not only are unstable radium isotopes significant radioactivity emitters, but as the next stage in the decay chain they also generate radon, a heavy, inert, naturally occurring radioactive gas. Rock containing thorium and/or uranium (such as some granites) emits radon gas that can accumulate in enclosed places such as basements or underground mines. Radon exposure is considered the leading cause of lung cancer in non-smokers.
History.
All the elements and isotopes we encounter on Earth, with the exceptions of hydrogen, deuterium, helium, helium-3, and perhaps trace amounts of stable lithium and beryllium isotopes which were created in the Big Bang, were created by the s-process or the r-process in stars, and for those to be today a part of the Earth, must have been created not later than 4.5 billion years ago. All the elements created more than 4.5 billion years ago are termed "primordial", meaning they were generated by the universe's stellar processes. At the time when they were created, those that were unstable began decaying immediately. All the isotopes which have half-lives less than 100 million years have been reduced to 0.0000000000028% (%) or less of whatever original amounts were created and captured by Earth's accretion; they are of trace quantity today, or have decayed away altogether. There are only two other methods to create isotopes: "artificially", inside a man-made (or perhaps a natural) reactor, or through decay of a parent isotopic species, the process known as the "decay chain".
Unstable isotopes are in a continual struggle to become more stable; the ultimate goal is becoming one of the 200 or so stable isotopes in the universe. Stable isotopes have ratios of neutrons to protons in their nucleus that start out at 1 in stable helium-4 and smoothly rise to ~1.5 for lead (there is no complete stability for anything heavier than lead-208). The elements heavier than that have to shed weight to achieve stability, most usually as alpha decay. The other common method for isotopes of the proper weight but high n/p ratio is beta decay, in which the nuclide changes elemental identity while keeping the same weight and lowering its n/p ratio. Also there is an inverse beta decay, which assists isotopes too light in neutrons to approach the ideal; however, since fission almost always produces products which are neutron heavy, positron emission is relatively rare compared to beta emission. There are many relatively short beta decay chains, at least two (a heavy, beta decay and a light, positron decay) for every discrete weight up to around 207 and some beyond, but for the higher weight elements (often referred to as "transuranics", but actually used for all isotopes heavier than lead) there are only four pathways in which all are represented. This fact is made inevitable by the two decay methods possible: alpha radiation, which reduces the weight by 4 AMUs, and beta, which does not change the weight at all (just the atomic number and the p/n ratio). The four paths are termed 4n, 4n + 1, 4n + 2, and 4n + 3; the remainder of the atomic weight divided by four gives the chain the isotope will use to decay. There are other decay modes, but they invariably occur at a lower probability than alpha or beta decay.
Three of those chains have a long-lived isotope near the top; they are bottlenecks in the process through which the chain flows very slowly, and keep the chain below them "alive" with flow. The three materials are uranium-238 (half-life=4.5 billion years), uranium-235 (half-life=700 million years) and thorium-232 (half-life=14 billion years). The fourth chain has no such long lasting bottleneck isotope, so almost all of the isotopes in that chain have long since decayed down to very near the stability at the bottom. Near the end of that chain is bismuth-209, which was long thought to be stable. Recently, however, Bi-209 was found to be unstable with a half-life of 19 billion billion years; it is the last step before stable thallium-205. In the far past, around the time that the solar system formed, there were more kinds of unstable high-weight isotopes available, and the four chains were longer with isotopes that have since decayed away. Today we have manufactured extinct isotopes, which again take their places: plutonium-239, the nuclear bomb fuel, as the major example has a half-life of "only" 24,500 years, and decays by alpha emission into uranium-235.
Types of decay.
 The four most common modes of radioactive decay are: alpha decay, beta decay, inverse beta decay (considered as both positron emission and electron capture), and isomeric transition. Of these decay processes, only alpha decay changes the atomic mass number ("A") of the nucleus, and always decreases it by four. Because of this, almost any decay will result in a nucleus whose atomic mass number has the same residue mod 4, dividing all nuclides into four chains. The members of any possible decay chain must be drawn entirely from one of these classes. All four chains also produce helium-4 (alpha particles are helium-4 nuclei).
Three main decay chains (or families) are observed in nature, commonly called the thorium series, the radium or uranium series, and the actinium series, representing three of these four classes, and ending in three different, stable isotopes of lead. The mass number of every isotope in these chains can be represented as "A" = 4"n", "A" = 4"n" + 2, and A = 4"n" + 3, respectively. The long-lived starting isotopes of these three isotopes, respectively thorium-232, uranium-238, and uranium-235, have existed since the formation of the earth, ignoring the artificial isotopes and their decays since the 1940s.
Due to the quite short half-life of its starting isotope neptunium-237 (2.14 million years), the fourth chain, the neptunium series with "A" = 4"n" + 1, is already extinct in nature, except for the final rate-limiting step, decay of bismuth-209. The ending isotope of this chain is now known to be thallium-205. Some older sources give the final isotope as bismuth-209, but it was recently discovered that it is radioactive, with a half-life of .
There are also non-transuranic decay chains, for example those of magnesium-28 and chlorine-39. On Earth, most of the starting isotopes of these chains before 1945 were generated by cosmic radiation. Since 1945, the testing and use of nuclear weapons has also released numerous radioactive fission products. Almost all such isotopes decay by either beta− or beta+ decay modes, changing from one element to the another without changing atomic mass. These later daughter products, being closer to stability, generally have longer half-lives until they finally decay into stability.
Actinide alpha decay chains.
In the four tables below, the minor branches of decay (with the branching probability of less than 0.0001%) are omitted. The energy release includes the total kinetic energy of all the emitted particles (electrons, alpha particles, gamma quanta, neutrinos, Auger electrons and X-rays) and the recoil nucleus, assuming that the original nucleus was at rest. The letter 'a' represents a year (from the Latin "annus").
In the tables below (except neptunium), the historic names of the naturally occurring nuclides are also given. These names were used at the time when the decay chains were first discovered and investigated. From these historical names one can locate the particular chain to which the nuclide belongs, and replace it with its modern name.
The three naturally-occurring actinide alpha decay chains given below—thorium, uranium/radium (from U-238), and actinium (from U-235)—each ends with its own specific lead isotope (Pb-208, Pb-206, and Pb-207 respectively). All these isotopes are stable and are also present in nature as primordial nuclides, but their excess amounts in comparison with lead-204 (which has only a primordial origin) can be used in the technique of uranium-lead dating to date rocks.
Thorium series.
The 4n chain of Th-232 is commonly called the "thorium series" or "thorium cascade". Beginning with naturally occurring thorium-232, this series includes the following elements: actinium, bismuth, lead, polonium, radium, and radon. All are present, at least transiently, in any natural thorium-containing sample, whether metal, compound, or mineral. The series terminates with lead-208.
The total energy released from thorium-232 to lead-208, including the energy lost to neutrinos, is 42.6 MeV.
Neptunium series.
The 4n + 1 chain of Np-237 is commonly called the "neptunium series" or "neptunium cascade". In this series, only two of the isotopes involved are found naturally, namely the final two: bismuth-209 and thallium-205. A smoke detector containing an americium-241 ionization chamber accumulates a significant amount of neptunium-237 as its americium decays; the following elements are also present in it, at least transiently, as decay products of the neptunium: actinium, astatine, bismuth, francium, lead, polonium, protactinium, radium, thallium, thorium, and uranium. Since this series was only studied more recently, its nuclides do not have historic names. One unique trait of this decay chain is that it does not include the noble-gas radon, and thus does not migrate through rock nearly as much as the other three decay chains.
The total energy released from californium-249 to thallium-205, including the energy lost to neutrinos, is 66.8 MeV.
Uranium series.
The 4n+2 chain of U-238 is called the "uranium series" or "radium series". Beginning with naturally occurring uranium-238, this series includes the following elements: astatine, bismuth, lead, polonium, protactinium, radium, radon, thallium, and thorium. All are present, at least transiently, in any natural uranium-containing sample, whether metal, compound, or mineral. The series terminates with lead-206.
The total energy released from uranium-238 to lead-206, including the energy lost to neutrinos, is 51.7 MeV.
Actinium series.
The 4n+3 chain of uranium-235 is commonly called the "actinium series" or "plutonium cascade". Beginning with the naturally-occurring isotope U-235, this decay series includes the following elements: actinium, astatine, bismuth, francium, lead, polonium, protactinium, radium, radon, thallium, and thorium. All are present, at least transiently, in any sample containing uranium-235, whether metal, compound, ore, or mineral. This series terminates with the stable isotope lead-207.
The total energy released from uranium-235 to lead-207, including the energy lost to neutrinos, is 46.4 MeV.
Beta decay chains in fission products.
Since the heavy original nuclei always have a greater proportion of neutrons, the fission product nuclei almost always start out with a neutron/proton ratio significantly greater than what is stable for their mass range. Therefore they undergo multiple beta decays in succession, each converting a neutron to a proton. The first decays tend to have higher decay energy and shorter half-life. These last decays may have low decay energy and/or long half-life.
For example, uranium-235 has 92 protons and 143 neutrons. Fission takes one more neutron, then produces two or three more neutrons; assume that 92 protons and 142 neutrons are available for the two fission product nuclei. Suppose they have mass 99 with 39 protons and 60 neutrons (yttrium-99), and mass 135 with 53 protons and 82 neutrons (iodine-135), then the decay chains can be found in the tables below.

</doc>
<doc id="275626" url="https://en.wikipedia.org/wiki?curid=275626" title="Deconvolution">
Deconvolution

In mathematics, deconvolution is an algorithm-based process used to reverse the effects of convolution on recorded data. The concept of deconvolution is widely used in the techniques of signal processing and image processing. Because these techniques are in turn widely used in many scientific and engineering disciplines, deconvolution finds many applications.
In general, the object of deconvolution is to find the solution of a convolution equation of the form:
Usually, "h" is some recorded signal, and "f" is some signal that we wish to recover, but has been convolved with some other signal "g" before we recorded it. The function "g" might represent the transfer function of an instrument or a driving force that was applied to a physical system. If we know "g", or at least know the form of "g", then we can perform deterministic deconvolution. However, if we do not know "g" in advance, then we need to estimate it. This is most often done using methods of statistical estimation.
In physical measurements, the situation is usually closer to
In this case "ε" is noise that has entered our recorded signal. If we assume that a noisy signal or image is noiseless when we try to make a statistical estimate of "g", our estimate will be incorrect. In turn, our estimate of "ƒ" will also be incorrect. The lower the signal-to-noise ratio, the worse our estimate of the deconvolved signal will be. That is the reason why inverse filtering the signal is usually not a good solution. However, if we have at least some knowledge of the type of noise in the data (for example, white noise), we may be able to improve the estimate of "ƒ" through techniques such as Wiener deconvolution.
The foundations for deconvolution and time-series analysis were largely laid by Norbert Wiener of the Massachusetts Institute of Technology in his book "Extrapolation, Interpolation, and Smoothing of Stationary Time Series" (1949). The book was based on work Wiener had done during World War II but that had been classified at the time. Some of the early attempts to apply these theories were in the fields of weather forecasting and economics.
Applications of deconvolution.
Seismology.
The concept of deconvolution had an early application in reflection seismology. In 1950, Enders Robinson was a graduate student at MIT. He worked with others at MIT, such as Norbert Wiener, Norman Levinson, and economist Paul Samuelson, to develop the "convolutional model" of a reflection seismogram. This model assumes that the recorded seismogram "s"("t") is the convolution of an Earth-reflectivity function "e"("t") and a seismic wavelet "w"("t") from a point source, where "t" represents recording time. Thus, our convolution equation is
The seismologist is interested in "e", which contains information about the Earth's structure. By the convolution theorem, this equation may be Fourier transformed to
in the frequency domain. By assuming that the reflectivity is white, we can assume that the power spectrum of the reflectivity is constant, and that the power spectrum of the seismogram is the spectrum of the wavelet multiplied by that constant. Thus,
If we assume that the wavelet is minimum phase, we can recover it by calculating the minimum phase equivalent of the power spectrum we just found. The reflectivity may be recovered by designing and applying a Wiener filter that shapes the estimated wavelet to a Dirac delta function (i.e., a spike). The result may be seen as a series of scaled, shifted delta functions (although this is not mathematically rigorous):
where "N" is the number of reflection events, "τ" "i" "τ" "i" are the reflection times of each event, and "r" "i" are the reflection coefficients.
In practice, since we are dealing with noisy, finite bandwidth, finite length, discretely sampled datasets, the above procedure only yields an approximation of the filter required to deconvolve the data. However, by formulating the problem as the solution of a Toeplitz matrix and using Levinson recursion, we can relatively quickly estimate a filter with the smallest mean squared error possible. We can also do deconvolution directly in the frequency domain and get similar results. The technique is closely related to linear prediction.
Optics and other imaging.
In optics and imaging, the term "deconvolution" is specifically used to refer to the process of reversing the optical distortion that takes place in an optical microscope, electron microscope, telescope, or other imaging instrument, thus creating clearer images. It is usually done in the digital domain by a software algorithm, as part of a suite of microscope image processing techniques. Deconvolution is also practical to sharpen images that suffer from fast motion or jiggles during capturing. Early Hubble Space Telescope images were distorted by a flawed mirror and could be sharpened by deconvolution.
The usual method is to assume that the optical path through the instrument is optically perfect, convolved with a point spread function (PSF), that is, a mathematical function that describes the distortion in terms of the pathway a theoretical point source of light (or other waves) takes through the instrument. Usually, such a point source contributes a small area of fuzziness to the final image. If this function can be determined, it is then a matter of computing its inverse or complementary function, and convolving the acquired image with that. The result is the original, undistorted image.
In practice, finding the true PSF is impossible, and usually an approximation of it is used, theoretically calculated or based on some experimental estimation by using known probes. Real optics may also have different PSFs at different focal and spatial locations, and the PSF may be non-linear. The accuracy of the approximation of the PSF will dictate the final result. Different algorithms can be employed to give better results, at the price of being more computationally intensive. Since the original convolution discards data, some algorithms use additional data acquired at nearby focal points to make up some of the lost information. Regularization in iterative algorithms (as in expectation-maximization algorithms) can be applied to avoid unrealistic solutions.
When the PSF is unknown, it may be possible to deduce it by systematically trying different possible PSFs and assessing whether the image has improved. This procedure is called "blind deconvolution". Blind deconvolution is a well-established image restoration technique in astronomy, where the point nature of the objects photographed exposes the PSF thus making it more feasible. It is also used in fluorescence microscopy for image restoration, and in fluorescence spectral imaging for spectral separation of multiple unknown fluorophores. The most common iterative algorithm for the purpose is the Richardson–Lucy deconvolution algorithm; the Wiener deconvolution (and approximations) are the most common non-iterative algorithms.
Radio astronomy.
When performing image synthesis in radio interferometry, a specific kind of radio astronomy, one step consists of deconvolving the produced image with the "dirty beam", which is a different name for the point spread function. A commonly used method is the CLEAN algorithm.
Absorption spectra.
Deconvolution has been applied extensively to absorption spectra. The (in German) may be used.

</doc>
<doc id="26998547" url="https://en.wikipedia.org/wiki?curid=26998547" title="Degrees of freedom (physics and chemistry)">
Degrees of freedom (physics and chemistry)

In physics, a degree of freedom is an independent physical parameter in the formal description of the state of a physical system. The set of all dimensions of a system is known as a phase space, and degrees of freedom are sometimes referred to as its dimensions.
Definition.
A degree of freedom of a physical system is an independent parameter that is necessary to characterize the state of a physical system. In general, a degree of freedom may be any useful property that is not dependent on other variables.
The location of a particle in three-dimensional space requires three position coordinates. Similarly, the direction and speed at which a particle moves can be described in terms of three velocity components, each in reference to the three dimensions of space. If the time evolution of the system is deterministic, where the state at one instant uniquely determines its past and future position and velocity as a function of time, such a system has six degrees of freedom. If the motion of the particle is constrained to a lower number of dimensions, for example, the particle must move along a wire or on a fixed surface, then the system has fewer than six degrees of freedom. On the other hand, a system with an extended object that can rotate or vibrate can have more than six degrees of freedom.
In classical mechanics, the state of a point particle at any given time is often described with position and velocity coordinates in the Lagrangian formalism, or with position and momentum coordinates in the Hamiltonian formalism.
In statistical mechanics, a degree of freedom is a single scalar number describing the microstate of a system. The specification of all microstates of a system is a point in the system's phase space.
In the 3D ideal chain model in chemistry, two angles are necessary to describe the orientation of each monomer.
It is often useful to specify quadratic degrees of freedom. These are degrees of freedom that contribute in a quadratic function to the energy of the system.
Degrees of freedom of gas molecules.
In three-dimensional space, three degrees of freedom are associated with the movement of a particle. A diatomic gas molecule thus has 6 degrees of freedom. This set may be decomposed in terms of translations, rotations, and vibrations of the molecule. The center of mass motion of the entire molecule accounts for 3 degrees of freedom. In addition, the molecule has two rotational degrees of motion and one vibrational mode. The rotations occur around the two axes perpendicular to the line between the two atoms. The rotation around the atom–atom bond is not a physical rotation. This yields, for a diatomic molecule, a decomposition of:
For a general (non-linear) molecule with atoms, all 3 rotational degrees of freedom are considered, resulting in the decomposition:
which means that an -atom molecule has vibrational degrees of freedom for . In special cases, such as adsorbed large molecules, the rotational degrees of freedom can be limited to only one.
As defined above one can also count degrees of freedom using the minimum number of coordinates required to specify a position. This is done as follows:
Let's say one particle in this body has coordinate and the other has coordinate with unknown. Application of the formula for distance between two coordinates
results in one equation with one unknown, in which we can solve for .
One of , , , , , or can be unknown.
Contrary to the classical equipartition theorem, at room temperature, the vibrational motion of molecules typically makes negligible contributions to the heat capacity. This is because these degrees of freedom are "frozen" because the spacing between the energy eigenvalues exceeds the energy corresponding to ambient temperatures (). In the following table such degrees of freedom are disregarded because of their low effect on total energy. However, at very high temperatures they cannot be neglected.
Independent degrees of freedom.
The set of degrees of freedom of a system is independent if the energy associated with the set can be written in the following form:
where is a function of the sole variable .
example: if and are two degrees of freedom, and is the associated energy:
For from 1 to , the value of the th degree of freedom is distributed according to the Boltzmann distribution. Its probability density function is the following:
In this section, and throughout the article the brackets formula_8 denote the mean of the quantity they enclose.
The internal energy of the system is the sum of the average energies associated with each of the degrees of freedom:
Quadratic degrees of freedom.
A degree of freedom is quadratic if the energy terms associated with this degree of freedom can be written as
where is a linear combination of other quadratic degrees of freedom.
example: if and are two degrees of freedom, and is the associated energy:
For example, in Newtonian mechanics, the dynamics of a system of quadratic degrees of freedom are controlled by a set of homogeneous linear differential equations with constant coefficients.
Quadratic and independent degree of freedom.
 are quadratic and independent degrees of freedom if the energy associated with a microstate of the system they represent can be written as:
Equipartition theorem.
In the classical limit of statistical mechanics, at thermodynamic equilibrium, the internal energy of a system of quadratic and independent degrees of freedom is:
Here, the mean energy associated with a degree of freedom is:
Since the degrees of freedom are independent, the internal energy of the system is equal to the sum of the mean energy associated with each degree of freedom, which demonstrates the result.
Generalizations.
The description of a system's state as a point in its phase space, although mathematically convenient, is thought to be fundamentally inaccurate. In quantum mechanics, the motion degrees of freedom are superseded with the concept of wave function, and operators which correspond to other degrees of freedom have discrete spectra. For example, intrinsic angular momentum operator (which corresponds to the rotational freedom) for an electron or photon have only two eigenvalues. This discreteness becomes apparent when action has an order of magnitude of the Planck constant, and individual degrees of freedom can be distinguished.

</doc>
<doc id="151925" url="https://en.wikipedia.org/wiki?curid=151925" title="Del">
Del

Del, or nabla, is an operator used in mathematics, in particular, in vector calculus, as a vector differential operator, usually represented by the nabla symbol ∇. When applied to a function defined on a one-dimensional domain, it denotes its standard derivative as defined in calculus. When applied to a field (a function defined on a multi-dimensional domain), del may denote the gradient (locally steepest slope) of a scalar field (or sometimes of a vector field, as in the Navier–Stokes equations), the divergence of a vector field, or the curl (rotation) of a vector field, depending on the way it is applied.
Strictly speaking, del is not a specific operator, but rather a convenient mathematical notation for those three operators, that makes many equations easier to write and remember. The del symbol can be interpreted as a vector of partial derivative operators, and its three possible meanings—gradient, divergence, and curl—can be formally viewed as the product with a scalar, dot product, and cross product, respectively, of the del "operator" with the field. These formal products do not necessarily commute with other operators or products.
Definition.
In the Cartesian coordinate system R with coordinates formula_1 and standard basis formula_2, del is defined in terms of partial derivative operators as
In three-dimensional Cartesian coordinate system R3 with coordinates formula_4 and standard basis formula_5, del is written as
Del can also be expressed in other coordinate systems, see for example del in cylindrical and spherical coordinates.
Notational uses.
Del is used as a shorthand form to simplify many long mathematical expressions. It is most commonly used to simplify expressions for the gradient, divergence, curl, directional derivative, and Laplacian.
Gradient.
The vector derivative of a scalar field formula_7 is called the gradient, and it can be represented as:
It always points in the direction of greatest increase of formula_7, and it has a magnitude equal to the maximum rate of increase at the point—just like a standard derivative. In particular, if a hill is defined as a height function over a plane formula_10, the 2d projection of the gradient at a given location will be a vector in the xy-plane (visualizable as an arrow on a map) pointing along the steepest direction. The magnitude of the gradient is the value of this steepest slope.
In particular, this notation is powerful because the gradient product rule looks very similar to the 1d-derivative case:
However, the rules for dot products do not turn out to be simple, as illustrated by:
Divergence.
The divergence of a vector field
formula_13 is a scalar function that can be represented as:
The divergence is roughly a measure of a vector field's increase in the direction it points; but more accurately, it is a measure of that field's tendency to converge toward or repel from a point.
The power of the del notation is shown by the following product rule:
The formula for the vector product is slightly less intuitive, because this product is not commutative:
Curl.
The curl of a vector field formula_17 is a vector function that can be represented as:
The curl at a point is proportional to the on-axis torque to which a tiny pinwheel would be subjected if it were centered at that point.
The vector product operation can be visualized as a pseudo-determinant:
Again the power of the notation is shown by the product rule:
Unfortunately the rule for the vector product does not turn out to be simple:
Directional derivative.
The directional derivative of a scalar field formula_22 in the direction
formula_23 is defined as:
This gives the rate of change of a field formula_7 in the direction of formula_26. In operator notation, the element in parentheses can be considered a single coherent unit; fluid dynamics uses this convention extensively, terming it the convective derivative—the "moving" derivative of the fluid.
Note that formula_27 is a scalar. When operating on a vector it must be distributed to each component.
Laplacian.
The Laplace operator is a scalar operator that can be applied to either vector or scalar fields; for cartesian coordinate systems it is defined as:
and the definition for more general coordinate systems is given in Vector Laplacian.
The Laplacian is ubiquitous throughout modern mathematical physics, appearing in Laplace's equation, Poisson's equation, the heat equation, the wave equation, and the Schrödinger equation—to name a few.
Tensor derivative.
Del can also be applied to a vector field with the result being a tensor. The tensor derivative of a vector field formula_29 is a 9-term second-rank tensor (that is, a 3×3 matrix), but can be denoted simply as formula_30, where formula_31 represents the dyadic product. This quantity is equivalent to the transpose of the Jacobian matrix of the vector field with respect to space.
For a small displacement formula_32, the change in the vector field is given by:
Second derivatives.
When del operates on a scalar or vector, either a scalar or vector is returned. Because of the diversity of vector products (scalar, dot, cross) one application of del already gives rise to three major derivatives: the gradient (scalar product), divergence (dot product), and curl (cross product). Applying these three sorts of derivatives again to each other gives five possible second derivatives, for a scalar field "f" or a vector field v; the use of the scalar Laplacian and vector Laplacian gives two more:
These are of interest principally because they are not always unique or independent of each other. As long as the functions are well-behaved, two of them are always zero:
Two of them are always equal:
The 3 remaining vector derivatives are related by the equation:
And one of them can even be expressed with the tensor product, if the functions are well-behaved:
Precautions.
Most of the above vector properties (except for those that rely explicitly on del's differential properties—for example, the product rule) rely only on symbol rearrangement, and must necessarily hold if the del symbol is replaced by any other vector. This is part of the value to be gained in notationally representing this operator as a vector.
Though one can often replace del with a vector and obtain a vector identity, making those identities mnemonic, the reverse is "not" necessarily reliable, because del does not commute in general.
A counterexample that relies on del's failure to commute:
A counterexample that relies on del's differential properties:
Central to these distinctions is the fact that del is not simply a vector; it is a vector operator. Whereas a vector is an object with both a magnitude and direction, del has neither a magnitude nor a direction until it operates on a function.
For that reason, identities involving del must be derived with care, using both vector identities and "differentiation" identities such as the product rule.

</doc>
<doc id="4890070" url="https://en.wikipedia.org/wiki?curid=4890070" title="Delta baryon">
Delta baryon

The Delta baryons (or Δ baryons, also called Delta resonances) are a family of subatomic particle made of three up or down quarks (u or d quarks).
Four Δ baryons exist: (constituent quarks: uuu), (uud), (udd), and (ddd), which respectively carry an electric charge of +2 "e", +1 "e", 0 "e", and −1 "e".
The Δ baryons have a mass of about , a spin of , and an isospin of . In many ways, Δ baryons are 'excited' nucleon (symbol N), which are made of the same constituent quarks in a lower-energy spin configuration (spin ). The (uud), (udd) are the higher-energy equivalent of the proton (, uud) and neutron (, udd). However, the and have no nucleon equivalent.
Composition.
The four Δ baryons are distinguished by their electrical charges, which is the sum of the charges of the quarks from which they are composed. There are also four antiparticles with opposite charges, made up of the corresponding antiquarks. The existence of the , with its unusual +2 charge, was a crucial clue in the development of the quark model.
Decay.
All varieties of Δ baryons quickly decay via the strong force into a nucleon (proton or neutron) and a pion of appropriate charge. The amplitudes of various final charge states given by their respective isospin couplings. More rarely and more slowly, the can decay into a proton and a photon and the can decay into a neutron and a photon.
List.
[a] PDG reports the resonance width (Γ). Here the conversion "τ" =  is given instead.

</doc>
<doc id="62844" url="https://en.wikipedia.org/wiki?curid=62844" title="Density matrix">
Density matrix

A density matrix is a matrix that describes a quantum system in a "mixed state", a statistical ensemble of several quantum states. This should be contrasted with a single state vector that describes a quantum system in a "pure state". The density matrix is the quantum-mechanical analogue to a phase-space probability measure (probability distribution of position and momentum) in classical statistical mechanics. 
Explicitly, suppose a quantum system may be found in state formula_1 with probability "p"1, or it may be found in state formula_2 with probability "p"2, or it may be found in state formula_3 with probability "p"3, and so on. The density operator for this system is
where formula_5 need not be orthogonal and formula_6. By choosing an orthonormal basis formula_7, one may resolve the density operator into the density matrix, whose elements are
The density operator can also be defined in terms of the density matrix,
For an operator formula_10 (which describes an observable formula_11 of the system), the expectation value formula_12 is given by
In words, the expectation value of "A" for the mixed state is the sum of the expectation values of "A" for each of the pure states formula_14 weighted by the probabilities "pi" and can be computed as the trace of the product of the density matrix with the matrix representation of formula_11 in the same basis.
Mixed states arise in situations where the experimenter does not know which particular states are being manipulated. Examples include a system in thermal equilibrium (or additionally chemical equilibrium) or a system with an uncertain or randomly varying preparation history (so one does not know which pure state the system is in). Also, if a quantum system has two or more subsystems that are entangled, then each subsystem must be treated as a mixed state even if the complete system is in a pure state. The density matrix is also a crucial tool in quantum decoherence theory.
The density matrix is a representation of a linear operator called the "density operator". The close relationship between matrices and operators is a basic concept in linear algebra. In practice, the terms "density matrix" and "density operator" are often used interchangeably. Both matrix and operator are self-adjoint (or Hermitian), positive semi-definite, of trace one, and may
be infinite-dimensional. The formalism was introduced by John von Neumann in 1927 and independently, but less systematically by Lev Landau and Felix Bloch in 1927 and 1946 respectively.
Pure and mixed states.
In quantum mechanics, a quantum system is represented by a state vector (or ket) formula_16. A quantum system with a state vector formula_16 is called a "pure state". However, it is also possible for a system to be in a statistical ensemble of different state vectors: For example, there may be a 50% probability that the state vector is formula_18 and a 50% chance that the state vector is formula_2. This system would be in a "mixed state". The density matrix is especially useful for mixed states, because any state, pure or mixed, can be characterized by a single density matrix.
A mixed state is different from a quantum superposition. In fact, a quantum superposition of pure states is another pure state, for example formula_20.
A state is pure if and only if its density matrix formula_21 satisfies formula_22.
Example: Light polarization.
An example of pure and mixed states is light polarization. Photons can have two helicities, corresponding to two orthogonal quantum states, formula_23 (right circular polarization) and formula_24 (left circular polarization). A photon can also be in a superposition state, such as formula_25 (vertical polarization) or formula_26 (horizontal polarization). More generally, it can be in any state formula_27, corresponding to linear, circular, or elliptical polarization. If we pass formula_25 polarized light through a circular polarizer which allows either only formula_23 polarized light, or only formula_24 polarized light, intensity would be reduced by half in both cases. This may make it "seem" like half of the photons are in state formula_23 and the other half in state formula_24. But this is not correct: Both formula_23 and formula_24 photons are partly absorbed by a vertical linear polarizer, but the formula_25 light will pass through that polarizer with no absorption whatsoever.
However, unpolarized light (such as the light from an incandescent light bulb) is different from any state like formula_27 (linear, circular, or elliptical polarization). Unlike linearly or elliptically polarized light, it passes through a polarizer with 50% intensity loss whatever the orientation of the polarizer; and unlike circularly polarized light, it cannot be made linearly polarized with any wave plate. Indeed, unpolarized light cannot be described as "any" state of the form formula_27. However, unpolarized light "can" be described perfectly by assuming that each photon is either formula_38 with 50% probability or formula_39 with 50% probability. The same behavior would occur if each photon was either vertically polarized with 50% probability or horizontally polarized with 50% probability.
Therefore, unpolarized light cannot be described by any pure state, but can be described as a statistical ensemble of pure states in at least two ways (the ensemble of half left and half right circularly polarized, or the ensemble of half vertically and half horizontally linearly polarized). These two ensembles are completely indistinguishable experimentally, and therefore they are considered the same mixed state. One of the advantages of the density matrix is that there is just one density matrix for each mixed state, whereas there are many statistical ensembles of pure states for each mixed state. Nevertheless, the density matrix contains all the information necessary to calculate any measurable property of the mixed state.
Where do mixed states come from? To answer that, consider how to generate unpolarized light. One way is to use a system in thermal equilibrium, a statistical mixture of enormous numbers of microstates, each with a certain probability (the Boltzmann factor), switching rapidly from one to the next due to thermal fluctuations. Thermal randomness explains why an incandescent light bulb, for example, emits unpolarized light. A second way to generate unpolarized light is to introduce uncertainty in the preparation of the system, for example, passing it through a birefringent crystal with a rough surface, so that slightly different parts of the beam acquire different polarizations. A third way to generate unpolarized light uses an EPR setup: A radioactive decay can emit two photons traveling in opposite directions, in the quantum state formula_40. The two photons "together" are in a pure state, but if you only look at one of the photons and ignore the other, the photon behaves just like unpolarized light.
More generally, mixed states commonly arise from a statistical mixture of the starting state (such as in thermal equilibrium), from uncertainty in the preparation procedure (such as slightly different paths that a photon can travel), or from looking at a subsystem entangled with something else.
Mathematical description.
The state vector formula_16 of a pure state completely determines the statistical behavior of a measurement. For concreteness, take an observable quantity, and let "A" be the associated observable operator that has a representation on the Hilbert space formula_42 of the quantum system. For any real-valued, analytical function "F" defined on the real numbers, suppose that "F"("A") is the result of applying "F" to the outcome of a measurement. The expectation value of "F"("A") is
Now consider a mixed state prepared by statistically combining two different pure states formula_44 and formula_45, with the associated probabilities "p" and , respectively. The associated probabilities mean that the preparation process for the quantum system ends in the state formula_46 with probability "p" and in the state formula_45 with probability .
It is not hard to show that the statistical properties of the observable for the system prepared in such a mixed state are completely determined. However, there is no state vector formula_48 which determines this statistical behavior in the sense that the expectation value of "F"("A") is
Nevertheless, there "is" a unique operator "ρ" such that the expectation value of "F(A)" can be written as
where the operator "ρ" is the density operator of the mixed system. A simple calculation shows that the operator "ρ" for the above discussion is given by
For the above example of unpolarized light, the density operator is
Formulation.
For a finite-dimensional function space, the most general density operator is of the form
where the coefficients "p""j" are non-negative and add up to one. This represents a statistical mixture of pure states. If the given system is closed, then one can think of a mixed state as representing a single system with an uncertain preparation history, as explicitly detailed above; "or" we can regard the mixed state as representing an ensemble of systems, i.e. a large number of copies of the system in question, where "p""j" is the proportion of the ensemble being in the state formula_54. An ensemble is described by a pure state if every copy of the system in that ensemble is in the same state, i.e. it is a "pure ensemble". If the system is not closed, however, then it is simply not correct to claim that it has some definite but unknown state vector, as the density operator may record physical entanglements to other systems.
Consider a quantum ensemble of size "N" with occupancy numbers "n"1, "n"2...,"nk" corresponding to the orthonormal states formula_55, respectively, where "n"1+...+"nk" = "N", and, thus, the coefficients "pj" = "nj" /"N". For a pure ensemble, where all "N" particles are in state formula_56, we have "nj" = 0, for all "j" ≠ "i", from which we recover the corresponding density operator formula_57. However, the density operator of a mixed state does not capture all the information about the ingredients that went into the mixture; in particular, the coefficients "p""j" and the kets ψ"j" are not recoverable from the operator ρ without additional information. This non-uniqueness implies that different ensembles or mixtures may correspond to the same density operator. Such equivalent ensembles or mixtures cannot be distinguished by measurement of observables alone. This equivalence can be characterized precisely. Two ensembles ψ, ψ' define the same density operator if and only if there is a matrix U with
i.e., U is unitary and such that
This is simply a restatement of the following fact from linear algebra: for two square matrices "M" and "N", "M M"* = "N N"* if and only if "M" = "NU" for some unitary "U". (See square root of a matrix for more details.) Thus there is a unitary freedom in the ket mixture or ensemble that gives the same density operator. However if the kets making up the mixture are restricted to be orthonormal, then the original probabilities "p""j" are recoverable as the eigenvalues of the density matrix.
In operator language, a density operator is a positive semidefinite, hermitian operator of trace 1 acting on the state space. A density operator describes a pure state if it is a rank one projection. Equivalently, a density operator ρ describes a pure state if and only if
i.e. the state is idempotent. This is true regardless of whether "H" is finite-dimensional or not.
Geometrically, when the state is not expressible as a convex combination of other states, it is a pure state. The family of mixed states is a convex set and a state is pure if it is an extremal point of that set.
It follows from the spectral theorem for compact self-adjoint operators that every mixed state is a countable convex combination of pure states. This representation is not unique. Furthermore, a theorem of Andrew Gleason states that certain functions defined on the family of projections and taking values in [0,1] (which can be regarded as quantum analogues of probability measures) are determined by unique mixed states. See quantum logic for more details.
Measurement.
Let "A" be an observable of the system, and suppose the ensemble is in a mixed state such that each of the pure states formula_61 occurs with probability "pj". Then the corresponding density operator is:
The expectation value of the measurement can be calculated by extending from the case of pure states (see Measurement in quantum mechanics):
where formula_64 denotes trace. Moreover, if "A" has spectral resolution
where formula_66, the corresponding density operator after the measurement is given by:
Note that the above density operator describes the full ensemble after measurement. The sub-ensemble for which the measurement result was the particular value "ai" is described by the different density operator
This is true assuming that formula_69 is the only eigenket (up to phase) with eigenvalue "ai"; more generally, "Pi" in this expression would be replaced by the projection operator into the eigen"space" corresponding to eigenvalue "ai".
Entropy.
The von Neumann entropy formula_70 of a mixture can be expressed in terms of the eigenvalues of formula_71 or in terms of the trace and logarithm of the density operator formula_71. Since formula_73 is a positive semi-definite operator, it has a spectral decomposition such that formula_74 where formula_75 are orthonormal vectors, formula_76 and formula_77. Then the entropy of a quantum system with density matrix formula_73 is
Also it can be shown that
when formula_81 have orthogonal support, where formula_82 is the Shannon entropy.
This entropy can increase but never decrease with a projective measurement, however generalised measurements can decrease entropy. The entropy of a pure state is zero, while that of a proper mixture always greater than zero. Therefore a pure state may be converted into a mixture by a measurement, but a proper mixture can "never" be converted into a pure state. Thus the act of measurement induces a fundamental irreversible change on the density matrix; this is analogous to the "collapse" of the state vector, or wavefunction collapse. Perhaps counterintuitively, the measurement actually "decreases information" by erasing quantum interference in the composite system—cf. quantum entanglement, einselection, and quantum decoherence.
The von Neumann equation for time evolution.
Just as the Schrödinger equation describes how pure states evolve in time, the von Neumann equation (also known as the Liouville–von Neumann equation) describes how a density operator evolves in time (in fact, the two equations are equivalent, in the sense that either can be derived from the other.) The von Neumann equation dictates that
where the brackets denote a commutator.
Note that this equation only holds when the density operator is taken to be in the Schrödinger picture, even though this equation seems at first look to emulate the Heisenberg equation of motion in the Heisenberg picture, with a crucial sign difference:
where formula_85 is some "Heisenberg picture" operator; but in this picture the density matrix is "not time-dependent", and the relative sign ensures that the time derivative of the expected value formula_12 comes out "the same as in the Schrödinger picture".
Taking the density operator to be in the Schrödinger picture makes sense, since it is composed of 'Schrödinger' kets and bras evolved in time, as per the Schrödinger picture.
If the Hamiltonian is time-independent, this differential equation can be easily solved to yield
"Quantum Liouville", Moyal's equation.
The density matrix operator may also be realized in phase space. Under the Wigner map, the density matrix transforms into the equivalent Wigner function,
The equation for the time-evolution of the Wigner function is then the Wigner-transform of the above von Neumann equation,
where "H(q,p)" is the Hamiltonian, and { { •,• } } is the Moyal bracket, the transform of the quantum commutator.
The evolution equation for the Wigner function is then analogous to that of its classical limit, the Liouville equation of classical physics. In the limit of vanishing Planck's constant ħ, "W(q,p,t)" reduces to the classical Liouville probability density function in phase space.
The classical Liouville equation can be solved using the method of characteristics for partial differential equations, the characteristic equations being Hamilton's equations. The Moyal equation in quantum mechanics similarly admits formal solutions in terms of quantum characteristics, predicated on the ∗−product of phase space, although, in actual practice, solution-seeking follows different methods.
Composite systems.
The joint density matrix of a composite system of two systems A and B is described by formula_90. Then the subsystems are described by their reduced density operator.
formula_92 is called "partial trace" over system B.
If A and B are two distinct and independent systems then formula_93 which is a "product state".
C*-algebraic formulation of states.
It is now generally accepted that the description of quantum mechanics in which all self-adjoint operators represent observables is untenable. For this reason, observables are identified with elements of an abstract C*-algebra "A" (that is one without a distinguished representation as an algebra of operators) and states are positive linear functionals on "A". However, by using the GNS construction, we can recover Hilbert spaces which realize "A" as a subalgebra of operators.
Geometrically, a pure state on a C*-algebra "A" is a state which is an extreme point of the set of all states on "A". By properties of the GNS construction these states correspond to irreducible representations of "A".
The states of the C*-algebra of compact operators "K"("H") correspond exactly to the density operators, and therefore the pure states of "K"("H") are exactly the pure states in the sense of quantum mechanics.
The C*-algebraic formulation can be seen to include both classical and quantum systems. When the system is classical, the algebra of observables become an abelian C*-algebra. In that case the states become probability measures, as noted in the introduction.

</doc>
<doc id="7921" url="https://en.wikipedia.org/wiki?curid=7921" title="Derivative">
Derivative

The derivative of a function of a real variable measures the sensitivity to change of a quantity (a function value or dependent variable) which is determined by another quantity (the independent variable). Derivatives are a fundamental tool of calculus. For example, the derivative of the position of a moving object with respect to time is the object's velocity: this measures how quickly the position of the object changes when time is advanced. 
The derivative of a function of a single variable at a chosen input value is the slope of the tangent line to the graph of the function at that point. This means that it describes the best linear approximation of the function near that input value. For this reason, the derivative is often described as the "instantaneous rate of change", the ratio of the instantaneous change in the dependent variable to that of the independent variable.
Derivatives may be generalized to functions of several real variables. In this generalization, the derivative is reinterpreted as a linear transformation whose graph is (after an appropriate translation) the best linear approximation to the graph of the original function. The Jacobian matrix is the matrix that represents this linear transformation with respect to the basis given by the choice of independent and dependent variables. It can be calculated in terms of the partial derivatives with respect to the independent variables. For a real-valued function of several variables, the Jacobian matrix reduces to the gradient vector.
The process of finding a derivative is called differentiation. The reverse process is called "antidifferentiation". The fundamental theorem of calculus states that antidifferentiation is the same as integration. Differentiation and integration constitute the two fundamental operations in single-variable calculus.
Differentiation and derivative.
"Differentiation" is the action of computing a derivative. The derivative of a function of a variable is a measure of the rate at which the value of the function changes with respect to the change of the variable. It is called the "derivative" of with respect to . If and are real numbers, and if the graph of is plotted against , the derivative is the slope of this graph at each point.
The simplest case, apart from the trivial case of a constant function, is when is a linear function of , meaning that the graph of divided by is a line. In this case, , for real numbers and , and the slope is given by
where the symbol (Delta) is an abbreviation for "change in." This formula is true because
Thus, since
it follows that
This gives an exact value for the slope of a line.
If the function is not linear (i.e. its graph is not a straight line), however, then the change in divided by the change in varies: differentiation is a method to find an exact value for this rate of change at any given value of .
The idea, illustrated by Figures 1 to 3, is to compute the rate of change as the limit value of the ratio of the differences as becomes infinitely small.
Notation.
Two distinct notations are commonly used for the derivative, one deriving from Leibniz and the other from Joseph Louis Lagrange.
In Leibniz's notation, an infinitesimal change in is denoted by , and the derivative of with respect to is written
suggesting the ratio of two infinitesimal quantities. (The above expression is read as "the derivative of "y" with respect to "x"", "d y by d x", or "d y over d x". The oral form "d y d x" is often used conversationally, although it may lead to confusion.)
In Lagrange's notation, the derivative with respect to of a function is denoted (read as "f prime of x") or (read as "f prime x of x"), in case of ambiguity of the variable implied by the derivation. Lagrange's notation is sometimes incorrectly attributed to Newton.
Rigorous definition.
The most common approach to turn this intuitive idea into a precise definition is to define the derivative as a limit of difference quotients of real numbers. This is the approach described below.
Let be a real valued function defined in an open neighborhood of a real number . In classical geometry, the tangent line to the graph of the function at was the unique line through the point that did "not" meet the graph of transversally, meaning that the line did not pass straight through the graph. The derivative of with respect to at is, geometrically, the slope of the tangent line to the graph of at . The slope of the tangent line is very close to the slope of the line through and a nearby point on the graph, for example . These lines are called secant lines. A value of close to zero gives a good approximation to the slope of the tangent line, and smaller values (in absolute value) of will, in general, give better approximations. The slope of the secant line is the difference between the values of these points divided by the difference between the values, that is, 
This expression is Newton's difference quotient. Passing from an approximation to an exact answer is done using a limit. Geometrically, the limit of the secant lines is the tangent line. Therefore, the limit of the difference quotient as approaches zero, if it exists, should represent the slope of the tangent line to . This limit is defined to be the derivative of the function at :
When the limit exists, is said to be "differentiable" at . Here is one of several common notations for the derivative (see below).
Equivalently, the derivative satisfies the property that
which has the intuitive interpretation (see Figure 1) that the tangent line to at gives the "best linear approximation"
to near (i.e., for small ). This interpretation is the easiest to generalize to other settings (see below).
Substituting 0 for in the difference quotient causes division by zero, so the slope of the tangent line cannot be found directly using this method. Instead, define to be the difference quotient as a function of :
 is the slope of the secant line between and . If is a continuous function, meaning that its graph is an unbroken curve with no gaps, then is a continuous function away from . If the limit exists, meaning that there is a way of choosing a value for that makes a continuous function, then the function is differentiable at , and its derivative at equals .
In practice, the existence of a continuous extension of the difference quotient to is shown by modifying the numerator to cancel in the denominator. Such manipulations can make the limit value of for small clear even though is still not defined at . This process can be long and tedious for complicated functions, and many shortcuts are commonly used to simplify the process.
Definition over the hyperreals.
Relative to a hyperreal extension of the real numbers, the derivative of a real function at a real point can be defined as the shadow of the quotient for infinitesimal , where . Here the natural extension of to the hyperreals is still denoted . Here the derivative is said to exist if the shadow is independent of the infinitesimal chosen.
Example.
The squaring function is differentiable at , and its derivative there is 6. This result is established by calculating the limit as approaches zero of the difference quotient of :
The last expression shows that the difference quotient equals when and is undefined when , because of the definition of the difference quotient. However, the definition of the limit says the difference quotient does not need to be defined when . The limit is the result of letting go to zero, meaning it is the value that tends to as becomes very small:
Hence the slope of the graph of the squaring function at the point is 6, and so its derivative at is .
More generally, a similar computation shows that the derivative of the squaring function at is .
Continuity and differentiability.
If is differentiable at , then must also be continuous at . As an example, choose a point and let be the step function that returns a value, say 1, for all less than , and returns a different value, say 10, for all greater than or equal to . cannot have a derivative at . If is negative, then is on the low part of the step, so the secant line from to is very steep, and as tends to zero the slope tends to infinity. If is positive, then is on the high part of the step, so the secant line from to has slope zero. Consequently, the secant lines do not approach any single slope, so the limit of the difference quotient does not exist.
However, even if a function is continuous at a point, it may not be differentiable there. For example, the absolute value function is continuous at , but it is not differentiable there. If is positive, then the slope of the secant line from 0 to is one, whereas if is negative, then the slope of the secant line from 0 to is negative one. This can be seen graphically as a "kink" or a "cusp" in the graph at . Even a function with a smooth graph is not differentiable at a point where its tangent is vertical: For instance, the function is not differentiable at .
In summary: for a function to have a derivative it is "necessary" for the function to be continuous, but continuity alone is not "sufficient".
Most functions that occur in practice have derivatives at all points or at almost every point. Early in the history of calculus, many mathematicians assumed that a continuous function was differentiable at most points. Under mild conditions, for example if the function is a monotone function or a Lipschitz function, this is true. However, in 1872 Weierstrass found the first example of a function that is continuous everywhere but differentiable nowhere. This example is now known as the Weierstrass function. In 1931, Stefan Banach proved that the set of functions that have a derivative at some point is a meager set in the space of all continuous functions. Informally, this means that hardly any continuous functions have a derivative at even one point.
The derivative as a function.
Let be a function that has a derivative at every point in the domain of . Because every point has a derivative, there is a function that sends the point to the derivative of at . This function is written and is called the "derivative function" or the "derivative" of . The derivative of collects all the derivatives of at all the points in the domain of .
Sometimes has a derivative at most, but not all, points of its domain. The function whose value at equals whenever is defined and elsewhere is undefined is also called the derivative of . It is still a function, but its domain is strictly smaller than the domain of .
Using this idea, differentiation becomes a function of functions: The derivative is an operator whose domain is the set of all functions that have derivatives at every point of their domain and whose range is a set of functions. If we denote this operator by , then is the function . Since is a function, it can be evaluated at a point . By the definition of the derivative function, .
For comparison, consider the doubling function ; is a real-valued function of a real number, meaning that it takes numbers as inputs and has numbers as outputs:
The operator , however, is not defined on individual numbers. It is only defined on functions:
Because the output of is a function, the output of can be evaluated at a point. For instance, when is applied to the squaring function, , outputs the doubling function , which we named . This output function can then be evaluated to get , , and so on.
Higher derivatives.
Let be a differentiable function, and let be its derivative. The derivative of (if it has one) is written and is called the "second derivative of ". Similarly, the derivative of a second derivative, if it exists, is written and is called the "third derivative of ". Continuing this process, one can define, if it exists, the th derivative as the derivative of the th derivative. These repeated derivatives are called "higher-order derivatives". The th derivative is also called the derivative of order .
If represents the position of an object at time , then the higher-order derivatives of have physical interpretations. The second derivative of is the derivative of , the velocity, and by definition this is the object's acceleration. The third derivative of is defined to be the jerk, and the fourth derivative is defined to be the jounce.
A function need not have a derivative, for example, if it is not continuous. Similarly, even if does have a derivative, it may not have a second derivative. For example, let
Calculation shows that is a differentiable function whose derivative is
 is twice the absolute value function, and it does not have a derivative at zero. Similar examples show that a function can have derivatives for any non-negative integer but no th-order derivative. A function that has successive derivatives is called " times differentiable". If in addition the th derivative is continuous, then the function is said to be of differentiability class . (This is a stronger condition than having derivatives. For an example, see differentiability class.) A function that has infinitely many derivatives is called "infinitely differentiable" or "smooth".
On the real line, every polynomial function is infinitely differentiable. By standard differentiation rules, if a polynomial of degree is differentiated times, then it becomes a constant function. All of its subsequent derivatives are identically zero. In particular, they exist, so polynomials are smooth functions.
The derivatives of a function at a point provide polynomial approximations to that function near . For example, if is twice differentiable, then
in the sense that
If is infinitely differentiable, then this is the beginning of the Taylor series for evaluated at around .
Inflection point.
A point where the second derivative of a function changes sign is called an "inflection point". At an inflection point, the second derivative may be zero, as in the case of the inflection point of the function , or it may fail to exist, as in the case of the inflection point of the function . At an inflection point, a function switches from being a convex function to being a concave function or vice versa.
Notation (details).
Leibniz's notation.
The notation for derivatives introduced by Gottfried Leibniz is one of the earliest. It is still commonly used when the equation is viewed as a functional relationship between dependent and independent variables. Then the first derivative is denoted by
and was once thought of as an infinitesimal quotient. Higher derivatives are expressed using the notation
for the "n"th derivative of (with respect to "x"). These are abbreviations for multiple applications of the derivative operator. For example,
With Leibniz's notation, we can write the derivative of "y" at the point in two different ways:
Leibniz's notation allows one to specify the variable for differentiation (in the denominator). This is especially relevant for partial differentiation. It also makes the chain rule easy to remember:
Lagrange's notation.
Sometimes referred to as "prime notation", one of the most common modern notation for differentiation is due to Joseph-Louis Lagrange and uses the prime mark, so that the derivative of a function "f"("x") is denoted "f"′("x") or simply "f"′. Similarly, the second and third derivatives are denoted
To denote the number of derivatives beyond this point, some authors use Roman numerals in superscript, whereas others place the number in parentheses:
The latter notation generalizes to yield the notation "f" ("n") for the "n"th derivative of "f" – this notation is most useful when we wish to talk about the derivative as being a function itself, as in this case the Leibniz notation can become cumbersome.
Newton's notation.
Newton's notation for differentiation, also called the dot notation, places a dot over the function name to represent a time derivative. If , then
denote, respectively, the first and second derivatives of "y" with respect to "t". This notation is used exclusively for time derivatives, meaning that the independent variable of the function represents time. It is very common in physics and in mathematical disciplines connected with physics such as differential equations. While the notation becomes unmanageable for high-order derivatives, in practice only very few derivatives are needed.
Fluent and fluxions.
Newton tried to explain calculus using fluent and fluxions. He said that the rate of generation is the fluxion of the fluent, which is denoted by the variable with a dot over it. Then the rate of the fluxion is the second fluxion, which has two dots over it. These fluxions were thought of, as very close to zero but not quite zero. But when you multiply two fluxions together you get something that is so close to zero that it is treated as zero. Newton took derivatives by replacing all the "x" values with formula_30 and all the y values with formula_31 and then used derivative rules to take the derivative and solve for formula_32 
Here is an example:
formula_33
Using the fact that formula_34 we can see formula_35 and formula_36 so formula_37.
Newton described mathematical quantities to be like continuous motion. This motion, he said, could be thought of in the same way that a point traces a curve. He defined this quantity and called it a “fluent”. He went on to name the rate at which these quantities change. Newton called this the “fluxion of the fluent” and he represented it by formula_38.
So, if the fluent was represented by "x", Newton denoted its fluxion by formula_38, the second fluxion by formula_40, and so on. This can be related to the modern language we use to describe derivatives. In modern language, the fluxion of the variable "x" relative to an independent time-variable "t" would be its velocity . In other words, the derivative of "f"("x") with respect to time, "t", is .
Moment of the fluent.
Newton called "o" the moment of the fluent. The moment of the fluent represents the infinitely small part by which a fluent was increased in a small time interval. Once he allowed himself to divide through by "o" (although "o" can not be treated as zero because that would make the division illegitimate). Newton decided it was justifiable to drop all terms containing "o".
Euler's notation.
Euler's notation uses a differential operator "D", which is applied to a function "f" to give the first derivative "Df". The second derivative is denoted "D"2"f", and the "n"th derivative is denoted "D""n""f".
If is a dependent variable, then often the subscript "x" is attached to the "D" to clarify the independent variable "x".
Euler's notation is then written
although this subscript is often omitted when the variable "x" is understood, for instance when this is the only variable present in the expression.
Euler's notation is useful for stating and solving linear differential equations.
Rules of computation.
The derivative of a function can, in principle, be computed from the definition by considering the difference quotient, and computing its limit. In practice, once the derivatives of a few simple functions are known, the derivatives of other functions are more easily computed using "rules" for obtaining derivatives of more complicated functions from simpler ones.
Rules for basic functions.
Most derivative computations eventually require taking the derivative of some common functions. The following incomplete list gives some of the most frequently used functions of a single real variable and their derivatives.
where "r" is any real number, then
wherever this function is defined. For example, if formula_45, then
and the derivative function is defined only for positive "x", not for . When , this rule implies that "f"′("x") is zero for , which is almost the constant rule (stated below).
Rules for combined functions.
In many cases, complicated limit calculations by direct application of Newton's difference quotient can be avoided using differentiation rules. Some of the most basic rules are the following.
Computation example.
The derivative of
is
Here the second term was computed using the chain rule and third using the product rule. The known derivatives of the elementary functions "x"2, "x"4, sin("x"), ln("x") and , as well as the constant 7, were also used.
Derivatives in higher dimensions.
Derivatives of vector valued functions.
A vector-valued function y("t") of a real variable sends real numbers to vectors in some vector space R"n". A vector-valued function can be split up into its coordinate functions "y"1("t"), "y"2("t"), …, "y""n"("t"), meaning that . This includes, for example, parametric curves in R2 or R3. The coordinate functions are real valued functions, so the above definition of derivative applies to them. The derivative of y("t") is defined to be the vector, called the tangent vector, whose coordinates are the derivatives of the coordinate functions. That is,
Equivalently,
if the limit exists. The subtraction in the numerator is subtraction of vectors, not scalars. If the derivative of y exists for every value of "t", then y′ is another vector valued function.
If e1, …, e"n" is the standard basis for R"n", then y("t") can also be written as . If we assume that the derivative of a vector-valued function retains the linearity property, then the derivative of y("t") must be
because each of the basis vectors is a constant.
This generalization is useful, for example, if y("t") is the position vector of a particle at time "t"; then the derivative y′("t") is the velocity vector of the particle at time "t".
Partial derivatives.
Suppose that "f" is a function that depends on more than one variable—for instance,
"f" can be reinterpreted as a family of functions of one variable indexed by the other variables:
In other words, every value of "x" chooses a function, denoted "fx", which is a function of one real number. That is,
Once a value of "x" is chosen, say "a", then determines a function "fa" that sends "y" to :
In this expression, "a" is a "constant", not a "variable", so "fa" is a function of only one real variable. Consequently, the definition of the derivative for a function of one variable applies:
The above procedure can be performed for any choice of "a". Assembling the derivatives together into a function gives a function that describes the variation of "f" in the "y" direction:
This is the partial derivative of "f" with respect to "y". Here ∂ is a rounded "d" called the partial derivative symbol. To distinguish it from the letter "d", ∂ is sometimes pronounced "der", "del", or "partial" instead of "dee".
In general, the partial derivative of a function in the direction "xi" at the point ("a"1 …, "a""n") is defined to be:
In the above difference quotient, all the variables except "xi" are held fixed. That choice of fixed values determines a function of one variable
and, by definition,
In other words, the different choices of "a" index a family of one-variable functions just as in the example above. This expression also shows that the computation of partial derivatives reduces to the computation of one-variable derivatives.
An important example of a function of several variables is the case of a scalar-valued function on a domain in Euclidean space R"n" (e.g., on R2 or R3). In this case "f" has a partial derivative ∂"f"/∂"x""j" with respect to each variable "x""j". At the point "a", these partial derivatives define the vector
This vector is called the gradient of "f" at "a". If "f" is differentiable at every point in some domain, then the gradient is a vector-valued function ∇"f" that takes the point "a" to the vector ∇"f"("a"). Consequently, the gradient determines a vector field.
Directional derivatives.
If "f" is a real-valued function on Rn, then the partial derivatives of "f" measure its variation in the direction of the coordinate axes. For example, if "f" is a function of "x" and "y", then its partial derivatives measure the variation in "f" in the "x" direction and the "y" direction. They do not, however, directly measure the variation of "f" in any other direction, such as along the diagonal line . These are measured using directional derivatives. Choose a vector
The directional derivative of "f" in the direction of v at the point x is the limit
In some cases it may be easier to compute or estimate the directional derivative after changing the length of the vector. Often this is done to turn the problem into the computation of a directional derivative in the direction of a unit vector. To see how this works, suppose that . Substitute into the difference quotient. The difference quotient becomes:
This is λ times the difference quotient for the directional derivative of "f" with respect to u. Furthermore, taking the limit as "h" tends to zero is the same as taking the limit as "k" tends to zero because "h" and "k" are multiples of each other. Therefore, . Because of this rescaling property, directional derivatives are frequently considered only for unit vectors.
If all the partial derivatives of "f" exist and are continuous at x, then they determine the directional derivative of "f" in the direction v by the formula:
This is a consequence of the definition of the total derivative. It follows that the directional derivative is linear in v, meaning that .
The same definition also works when "f" is a function with values in Rm. The above definition is applied to each component of the vectors. In this case, the directional derivative is a vector in Rm.
Total derivative, total differential and Jacobian matrix.
When "f" is a function from an open subset of R"n" to R"m", then the directional derivative of "f" in a chosen direction is the best linear approximation to "f" at that point and in that direction. But when , no single directional derivative can give a complete picture of the behavior of "f". The total derivative gives a complete picture by considering all directions at once. That is, for any vector v starting at a, the linear approximation formula holds:
Just like the single-variable derivative, is chosen so that the error in this approximation is as small as possible.
If "n" and "m" are both one, then the derivative is a number and the expression is the product of two numbers. But in higher dimensions, it is impossible for to be a number. If it were a number, then would be a vector in R"n" while the other terms would be vectors in R"m", and therefore the formula would not make sense. For the linear approximation formula to make sense, must be a function that sends vectors in R"n" to vectors in R"m", and must denote this function evaluated at v.
To determine what kind of function it is, notice that the linear approximation formula can be rewritten as
Notice that if we choose another vector w, then this approximate equation determines another approximate equation by substituting w for v. It determines a third approximate equation by substituting both w for v and for a. By subtracting these two new equations, we get
If we assume that v is small and that the derivative varies continuously in a, then is approximately equal to , and therefore the right-hand side is approximately zero. The left-hand side can be rewritten in a different way using the linear approximation formula with substituted for v. The linear approximation formula implies:
This suggests that is a linear transformation from the vector space R"n" to the vector space R"m". In fact, it is possible to make this a precise derivation by measuring the error in the approximations. Assume that the error in these linear approximation formula is bounded by a constant times ||v||, where the constant is independent of v but depends continuously on a. Then, after adding an appropriate error term, all of the above approximate equalities can be rephrased as inequalities. In particular, is a linear transformation up to a small error term. In the limit as v and w tend to zero, it must therefore be a linear transformation. Since we define the total derivative by taking a limit as v goes to zero, must be a linear transformation.
In one variable, the fact that the derivative is the best linear approximation is expressed by the fact that it is the limit of difference quotients. However, the usual difference quotient does not make sense in higher dimensions because it is not usually possible to divide vectors. In particular, the numerator and denominator of the difference quotient are not even in the same vector space: The numerator lies in the codomain R"m" while the denominator lies in the domain R"n". Furthermore, the derivative is a linear transformation, a different type of object from both the numerator and denominator. To make precise the idea that is the best linear approximation, it is necessary to adapt a different formula for the one-variable derivative in which these problems disappear. If , then the usual definition of the derivative may be manipulated to show that the derivative of "f" at "a" is the unique number such that
This is equivalent to
because the limit of a function tends to zero if and only if the limit of the absolute value of the function tends to zero. This last formula can be adapted to the many-variable situation by replacing the absolute values with norms.
The definition of the total derivative of "f" at a, therefore, is that it is the unique linear transformation such that
Here h is a vector in R"n", so the norm in the denominator is the standard length on R"n". However, "f"′(a)h is a vector in R"m", and the norm in the numerator is the standard length on R"m". If "v" is a vector starting at "a", then is called the pushforward of v by "f" and is sometimes written .
If the total derivative exists at a, then all the partial derivatives and directional derivatives of "f" exist at a, and for all v, is the directional derivative of "f" in the direction v. If we write "f" using coordinate functions, so that , then the total derivative can be expressed using the partial derivatives as a matrix. This matrix is called the Jacobian matrix of "f" at a:
The existence of the total derivative "f"′(a) is strictly stronger than the existence of all the partial derivatives, but if the partial derivatives exist and are continuous, then the total derivative exists, is given by the Jacobian, and depends continuously on a.
The definition of the total derivative subsumes the definition of the derivative in one variable. That is, if "f" is a real-valued function of a real variable, then the total derivative exists if and only if the usual derivative exists. The Jacobian matrix reduces to a 1×1 matrix whose only entry is the derivative "f"′("x"). This 1×1 matrix satisfies the property that is approximately zero, in other words that
Up to changing variables, this is the statement that the function formula_97 is the best linear approximation to "f" at "a".
The total derivative of a function does not give another function in the same way as the one-variable case. This is because the total derivative of a multivariable function has to record much more information than the derivative of a single-variable function. Instead, the total derivative gives a function from the tangent bundle of the source to the tangent bundle of the target.
The natural analog of second, third, and higher-order total derivatives is not a linear transformation, is not a function on the tangent bundle, and is not built by repeatedly taking the total derivative. The analog of a higher-order derivative, called a jet, cannot be a linear transformation because higher-order derivatives reflect subtle geometric information, such as concavity, which cannot be described in terms of linear data such as vectors. It cannot be a function on the tangent bundle because the tangent bundle only has room for the base space and the directional derivatives. Because jets capture higher-order information, they take as arguments additional coordinates representing higher-order changes in direction. The space determined by these additional coordinates is called the jet bundle. The relation between the total derivative and the partial derivatives of a function is paralleled in the relation between the "k"th order jet of a function and its partial derivatives of order less than or equal to "k".
By repeatedly taking the total derivative, one obtains higher versions of the Fréchet derivative, specialized to R"p". The "k"th order total derivative may be interpreted as a map
which takes a point x in Rn and assigns to it an element of the space of "k"-linear maps from Rn to Rm – the "best" (in a certain precise sense) "k"-linear approximation to "f" at that point. By precomposing it with the diagonal map Δ, , a generalized Taylor series may be begun as
where f(a) is identified with a constant function, are the components of the vector , and and are the components of and as linear transformations.
Generalizations.
The concept of a derivative can be extended to many other settings. The common thread is that the derivative of a function at a point serves as a linear approximation of the function at that point.

</doc>
<doc id="8468" url="https://en.wikipedia.org/wiki?curid=8468" title="Determinant">
Determinant

In linear algebra, the determinant is a useful value that can be computed from the elements of a square matrix. The determinant of a matrix "A" is denoted det("A"), det "A", or |"A"|.
In the case of a 2 × 2 matrix, the specific formula for the determinant is simply the upper left element times the lower right element, minus the product of the other two elements. Similarly, suppose we have a 3 × 3 matrix "A", and we want the specific formula for its determinant |"A"|:
Each determinant of a 2 × 2 matrix in this equation is called a "minor" of the matrix "A". The same sort of procedure can be used to find the determinant of a 4 × 4 matrix, the determinant of a 5 × 5 matrix, and so forth.
Determinants occur throughout mathematics. For example, a matrix is often used to represent the coefficients in a system of linear equations, and the determinant can be used to solve those equations, although more efficient techniques are actually used, some of which are determinant-revealing and consist of computationally effective ways of computing the determinant itself. The use of determinants in calculus includes the Jacobian determinant in the change of variables rule for integrals of functions of several variables. Determinants are also used to define the characteristic polynomial of a matrix, which is essential for eigenvalue problems in linear algebra. In analytical geometry, determinants express the signed n-dimensional volumes of n-dimensional parallelepipeds. Sometimes, determinants are used merely as a compact notation for expressions that would otherwise be unwieldy to write down.
It can be proven that any matrix has a unique inverse if its determinant is nonzero. Various other theorems can be proved as well, including that the determinant of a product of matrices is always equal to the product of determinants; and, the determinant of a Hermitian matrix is always real.
Definition.
There are various ways to define the determinant of a square matrix "A", i.e. one with the same number of rows and columns. Perhaps the simplest way to express the determinant is by considering the elements in the top row and the respective minors; starting at the left, multiply the element by the minor, then subtract the product of the next element and its minor, and alternate adding and subtracting such products until all elements in the top row have been exhausted. For example, here is the result for a 4 × 4 matrix:
Another way to define the determinant is expressed in terms of the columns of the matrix. If we write an matrix "A" in terms of its column vectors
where the formula_10 are vectors of size "n", then the determinant of "A" is defined so that
where "b" and "c" are scalars, "v" is any vector of size "n" and "I" is the identity matrix of size "n". These equations say that the determinant is a linear function of each column, that interchanging adjacent columns reverses the sign of the determinant, and that the determinant of the identity matrix is 1. These properties mean that the determinant is an alternating multilinear function of the columns that maps the identity matrix to the underlying unit scalar. These suffice to uniquely calculate the determinant of any square matrix. Provided the underlying scalars form a field (more generally, a commutative ring with unity), the definition below shows that such a function exists, and it can be shown to be unique.
Equivalently, the determinant can be expressed as a sum of products of entries of the matrix where each product has "n" terms and the coefficient of each product is −1 or 1 or 0 according to a given rule: it is a polynomial expression of the matrix entries. This expression grows rapidly with the size of the matrix (an matrix contributes "n"! terms), so it will first be given explicitly for the case of matrices and matrices, followed by the rule for arbitrary size matrices, which subsumes these two cases.
Assume "A" is a square matrix with "n" rows and "n" columns, so that it can be written as
The entries can be numbers or expressions (as happens when the determinant is used to define a characteristic polynomial); the definition of the determinant depends only on the fact that they can be added and multiplied together in a commutative manner.
The determinant of "A" is denoted as det("A"), or it can be denoted directly in terms of the matrix entries by writing enclosing bars instead of brackets:
2 × 2 matrices.
The determinant of a matrix is defined by
If the matrix entries are real numbers, the matrix "A" can be used to represent two linear maps: one that maps the standard basis vectors to the rows of "A", and one that maps them to the columns of "A". In either case, the images of the basis vectors form a parallelogram that represents the image of the unit square under the mapping. The parallelogram defined by the rows of the above matrix is the one with vertices at , , , and , as shown in the accompanying diagram.
The absolute value of is the area of the parallelogram, and thus represents the scale factor by which areas are transformed by "A". (The parallelogram formed by the columns of "A" is in general a different parallelogram, but since the determinant is symmetric with respect to rows and columns, the area will be the same.)
The absolute value of the determinant together with the sign becomes the "oriented area" of the parallelogram. The oriented area is the same as the usual area, except that it is negative when the angle from the first to the second vector defining the parallelogram turns in a clockwise direction (which is opposite to the direction one would get for the identity matrix).
Thus the determinant gives the scaling factor and the orientation induced by the mapping represented by "A". When the determinant is equal to one, the linear mapping defined by the matrix is equi-areal and orientation-preserving.
The object known as the "bivector" is related to these ideas. In 2D, it can be interpreted as an "oriented plane segment" formed by imagining two vectors each with origin , and coordinates and . The bivector magnitude (denoted ) is the "signed area", which is also the determinant .
3 × 3 matrices.
The determinant of a matrix is defined by
The rule of Sarrus is a mnemonic for the matrix determinant: the sum of the products of three diagonal north-west to south-east lines of matrix elements, minus the sum of the products of three diagonal south-west to north-east lines of elements, when the copies of the first two columns of the matrix are written beside it as in the illustration. This scheme for calculating the determinant of a matrix does not carry over into higher dimensions.
"n" × "n" matrices.
The determinant of a matrix of arbitrary size can be defined by the Leibniz formula or the Laplace formula.
The Leibniz formula for the determinant of an matrix "A" is
Here the sum is computed over all permutations σ of the set A permutation is a function that reorders this set of integers. The value in the "i"th position after the reordering σ is denoted σ"i". For example, for , the original sequence 1, 2, 3 might be reordered to , with , , and . The set of all such permutations (also known as the symmetric group on "n" elements) is denoted S"n". For each permutation σ, sgn(σ) denotes the signature of σ, a value that is +1 whenever the reordering given by σ can be achieved by successively interchanging two entries an even number of times, and −1 whenever it can be achieved by an odd number of such interchanges.
In any of the formula_17 summands, the term
is notation for the product of the entries at positions , where "i" ranges from 1 to "n":
For example, the determinant of a matrix "A" () is
Levi-Civita symbol.
It is sometimes useful to extend the Leibniz formula to a summation in which not only permutations, but all sequences of "n" indices in the range occur, ensuring that the contribution of a sequence will be zero unless it denotes a permutation. Thus the totally antisymmetric Levi-Civita symbol formula_21 extends the signature of a permutation, by setting formula_22 for any permutation σ of "n", and formula_23 when no permutation σ exists such that formula_24 for formula_25 (or equivalently, whenever some pair of indices are equal). The determinant for an matrix can then be expressed using an "n"-fold summation as
or using two epsilon symbols as
where now each "ir" and each "jr" should be summed over .
Properties of the determinant.
The determinant has many properties. Some basic properties of determinants are
Property 5 says that the determinant on matrices is homogeneous of degree "n". These properties can be used to facilitate the computation of determinants by simplifying the matrix to the point where the determinant can be determined immediately. Specifically, for matrices with coefficients in a field, properties 11 and 12 can be used to transform any matrix into a triangular matrix, whose determinant is given by property 6; this is essentially the method of Gaussian elimination.
For example, the determinant of
can be computed using the following matrices:
Here, "B" is obtained from "A" by adding −1/2×the first row to the second, so that . "C" is obtained from "B" by adding the first to the third row, so that . Finally, "D" is obtained from "C" by exchanging the second and third row, so that . The determinant of the (upper) triangular matrix "D" is the product of its entries on the main diagonal: . Therefore, .
Multiplicativity and matrix groups.
The determinant of a matrix product of square matrices equals the product of their determinants:
Thus the determinant is a "multiplicative map". This property is a consequence of the characterization given above of the determinant as the unique "n"-linear alternating function of the columns with value 1 on the identity matrix, since the function that maps can easily be seen to be "n"-linear and alternating in the columns of "M", and takes the value det("A") at the identity. The formula can be generalized to (square) products of rectangular matrices, giving the Cauchy–Binet formula, which also provides an independent proof of the multiplicative property.
The determinant det("A") of a matrix "A" is non-zero if and only if "A" is invertible or, yet another equivalent statement, if its rank equals the size of the matrix. If so, the determinant of the inverse matrix is given by
In particular, products and inverses of matrices with determinant one still have this property. Thus, the set of such matrices (of fixed size "n") form a group known as the special linear group. More generally, the word "special" indicates the subgroup of another matrix group of matrices of determinant one. Examples include the special orthogonal group (which if "n" is 2 or 3 consists of all rotation matrices), and the special unitary group.
Laplace's formula and the adjugate matrix.
Laplace's formula expresses the determinant of a matrix in terms of its minors. The minor "M""i","j" is defined to be the determinant of the -matrix that results from "A" by removing the "i"th row and the "j"th column. The expression is known as cofactor. The determinant of "A" is given by
Calculating det("A") by means of that formula is referred to as expanding the determinant along a row or column. For the example matrix
Laplace expansion along the second column (, the sum runs over "i") yields:
However, Laplace expansion is efficient for small matrices only.
The adjugate matrix adj("A") is the transpose of the matrix consisting of the cofactors, i.e.,
In terms of the adjugate matrix, Laplace's expansion can be written as
Sylvester's determinant theorem.
Sylvester's determinant theorem states that for "A", an matrix, and "B", an matrix (so that "A" and "B" have dimensions allowing them to be multiplied in either order):
where "I""m" and "I""n" are the and identity matrices, respectively.
From this general result several consequences follow.
Properties of the determinant in relation to other notions.
Relation to eigenvalues and trace.
Let formula_44 be an arbitrary formula_45 matrix of complex numbers with eigenvalues formula_46, formula_47, ... formula_48. (Here it is understood that an eigenvalue with algebraic multiplicities formula_49 occurs formula_49 times in this list.) Then the determinant of formula_44 is the product of all eigenvalues:
The product of all non-zero eigenvalues is referred to as pseudo-determinant.
Conversely, determinants can be used to find the eigenvalues of the matrix "A": they are the solutions of the characteristic equation
where "I" is the identity matrix of the same dimension as "A".
A Hermitian matrix is positive definite if all its eigenvalues are positive. Sylvester's criterion asserts that this is equivalent to the determinants of the submatrices
being positive, for all "k" between 1 and "n".
The trace tr("A") is by definition the sum of the diagonal entries of "A" and also equals the sum of the eigenvalues. Thus, for complex matrices "A",
or, for real matrices "A",
Here exp("A") denotes the matrix exponential of "A", because every eigenvalue λ of "A" corresponds to the eigenvalue exp(λ) of exp("A"). In particular, given any logarithm of "A", that is, any matrix "L" satisfying
the determinant of "A" is given by
For example, for , , and , respectively,
cf. Cayley-Hamilton theorem. Such expressions are deducible from Newton's identities.
In the general case,
where the sum is taken over the set of all integers "kl" ≥ 0 satisfying the equation
This formula can also be used to find the determinant of a matrix "AIJ" with multidimensional indices and . The product and trace of such matrices are defined in a natural way as
An arbitrary dimension "n" identity can be obtained from the Mercator series expansion of the logarithm when formula_65
where "I" is the identity matrix. The sum and the expansion of the exponential only need to go up to "n" instead of ∞, since the determinant cannot exceed "O"("An").
Upper and lower bounds.
For a positive definite matrix "A", the trace operator gives the following tight lower and upper bounds on the log determinant
with equality if and only if formula_68. This relationship can be derived via the formula for the KL-divergence between two multivariate normal distributions.
Cramer's rule.
For a matrix equation
the solution is given by Cramer's rule:
where "A""i" is the matrix formed by replacing the "i"th column of "A" by the column vector "b". This follows immediately by column expansion of the determinant, i.e.
where the vectors formula_10 are the columns of "A". The rule is also implied by the identity
It has recently been shown that Cramer's rule can be implemented in O("n"3) time, which is comparable to more common methods of solving systems of linear equations, such as LU, QR, or singular value decomposition.
Block matrices.
Suppose "A", "B", "C", and "D" are matrices of dimension , , , and , respectively. Then
This can be seen from the Leibniz formula, or from a decomposition like (for the former case)
When "A" is invertible, one has
as can be seen by employing the decomposition
When "D" is invertible, a similar identity with formula_78 factored out can be derived analogously, that is,
When the blocks are square matrices of the same order further formulas hold. For example, if "C" and "D" commute (i.e., ), then the following formula comparable to the determinant of a matrix holds:
When "A" = "D" and "B" = "C", the blocks are square matrices of the same order and the following formula holds (even if "A" and "B" do not commute)
When "D" is a 1×1 matrix, "B" is a column vector, and "C" is a row vector then
Derivative.
By definition, e.g., using the Leibniz formula, the determinant of real (or analogously for complex) square matrices is a polynomial function from to R. As such it is everywhere differentiable. Its derivative can be expressed using Jacobi's formula:
where adj("A") denotes the adjugate of "A". In particular, if "A" is invertible, we have
Expressed in terms of the entries of "A", these are
Yet another equivalent formulation is
using big O notation. The special case where formula_87, the identity matrix, yields
This identity is used in describing the tangent space of certain matrix Lie groups.
If the matrix A is written as formula_89 where a, b, c are column vectors of length 3, then the gradient over one of the three vectors may be written as the cross product of the other two:
Abstract algebraic aspects.
Determinant of an endomorphism.
The above identities concerning the determinant of products and inverses of matrices imply that similar matrices have the same determinant: two matrices "A" and "B" are similar, if there exists an invertible matrix "X" such that . Indeed, repeatedly applying the above identities yields
The determinant is therefore also called a similarity invariant. The determinant of a linear transformation
for some finite-dimensional vector space "V" is defined to be the determinant of the matrix describing it, with respect to an arbitrary choice of basis in "V". By the similarity invariance, this determinant is independent of the choice of the basis for "V" and therefore only depends on the endomorphism "T".
Exterior algebra.
The determinant of a linear transformation of an "n"-dimensional vector space "V" can be formulated in a coordinate-free manner by considering the "n"th exterior power Λ"n""V" of "V". "A" induces a linear map
As Λ"n""V" is one-dimensional, the map Λ"n"A is given by multiplying with some scalar. This scalar coincides with the determinant of "A", that is to say
This definition agrees with the more concrete coordinate-dependent definition. This follows from the characterization of the determinant given above. For example, switching two columns changes the sign of the determinant; likewise, permuting the vectors in the exterior product to , say, also changes its sign.
For this reason, the highest non-zero exterior power Λ"n"("V") is sometimes also called the determinant of "V" and similarly for more involved objects such as vector bundles or chain complexes of vector spaces. Minors of a matrix can also be cast in this setting, by considering lower alternating forms Λ"k""V" with .
Transformation on alternating multilinear "n"-forms.
The vector space "W" of all alternating multilinear "n"-forms on an "n"-dimensional vector space "V" has dimension one. To each linear transformation "T" on "V" we associate a linear transformation "T"′ on "W", where for each "w" in "W" we define . As a linear transformation on a one-dimensional space, "T"′ is equivalent to a scalar multiple. We call this scalar the determinant of "T".
Square matrices over commutative rings and abstract properties.
The determinant can also be characterized as the unique function
from the set of all matrices with entries in a field "K" to this field satisfying the following three properties: first, "D" is an "n"-linear function: considering all but one column of "A" fixed, the determinant is linear in the remaining column, that is
for any column vectors "v"1, ..., "v""n", and "w" and any scalars (elements of "K") "a" and "b". Second, "D" is an alternating function: for any matrix "A" with two identical columns . Finally, "D"("I""n") = 1. Here "I""n" is the identity matrix.
This fact also implies that every other "n"-linear alternating function satisfies
This definition can also be extended where "K" is a commutative ring "R", in which case a matrix is invertible if and only if its determinant is a invertible element in "R". For example, a matrix "A" with entries in Z, the integers, is invertible (in the sense that there exists an inverse matrix with integer entries) if the determinant is +1 or −1. Such a matrix is called unimodular.
The determinant defines a mapping
between the group of invertible matrices with entries in "R" and the multiplicative group of units in "R". Since it respects the multiplication in both groups, this map is a group homomorphism. Secondly, given a ring homomorphism , there is a map given by replacing all entries in "R" by their images under "f". The determinant respects these maps, i.e., given a matrix with entries in "R", the identity
holds. For example, the determinant of the complex conjugate of a complex matrix (which is also the determinant of its conjugate transpose) is the complex conjugate of its determinant, and for integer matrices: the reduction modulo "m" of the determinant of such a matrix is equal to the determinant of the matrix reduced modulo "m" (the latter determinant being computed using modular arithmetic). In the language of category theory, the determinant is a natural transformation between the two functors GL"n" and (⋅)×. Adding yet another layer of abstraction, this is captured by saying that the determinant is a morphism of algebraic groups, from the general linear group to the multiplicative group,
Generalizations and related notions.
Infinite matrices.
For matrices with an infinite number of rows and columns, the above definitions of the determinant do not carry over directly. For example, in the Leibniz formula, an infinite sum (all of whose terms are infinite products) would have to be calculated. Functional analysis provides different extensions of the determinant for such infinite-dimensional situations, which however only work for particular kinds of operators.
The Fredholm determinant defines the determinant for operators known as trace class operators by an appropriate generalization of the formula
Another infinite-dimensional notion of determinant is the functional determinant.
Related notions for non-commutative rings.
For square matrices with entries in a non-commutative ring, there are various difficulties in defining determinants analogously to that for commutative rings. A meaning can be given to the Leibniz formula provided that the order for the product is specified, and similarly for other ways to define the determinant, but non-commutativity then leads to the loss of many fundamental properties of the determinant, for instance the multiplicative property or the fact that the determinant is unchanged under transposition of the matrix. Over non-commutative rings, there is no reasonable notion of a multilinear form (existence of a nonzero bilinear form with a regular element of "R" as value on some pair of arguments implies that "R" is commutative). Nevertheless, various notions of non-commutative determinant have been formulated, which preserve some of the properties of determinants, notably quasideterminants and the Dieudonné determinant. It may be noted that if one considers certain specific classes of matrices with non-commutative elements, then there are examples where one can define the determinant and prove linear algebra theorems that are very similar to their commutative analogs. Examples include quantum groups and "q"-determinant, Capelli matrix and Capelli determinant, super-matrices and Berezinian; Manin matrices is the class of matrices which is most close to matrices with commutative elements.
Further variants.
Determinants of matrices in superrings (that is, Z2-graded rings) are known as Berezinians or superdeterminants.
The permanent of a matrix is defined as the determinant, except that the factors sgn(σ) occurring in Leibniz's rule are omitted. The immanant generalizes both by introducing a character of the symmetric group S"n" in Leibniz's rule.
Calculation.
Determinants are mainly used as a theoretical tool. They are rarely calculated explicitly in numerical linear algebra, where for applications like checking invertibility and finding eigenvalues the determinant has largely been supplanted by other techniques. Nonetheless, explicitly calculating determinants is required in some situations, and different methods are available to do so.
Naive methods of implementing an algorithm to compute the determinant include using the Leibniz formula or Laplace's formula. Both these approaches are extremely inefficient for large matrices, though, since the number of required operations grows very quickly: it is of order "n"! ("n" factorial) for an matrix "M". For example, Leibniz's formula requires calculating "n"! products. Therefore, more involved techniques have been developed for calculating determinants.
Decomposition methods.
Given a matrix "A", some methods compute its determinant by writing "A" as a product of matrices whose determinants can be more easily computed. Such techniques are referred to as decomposition methods. Examples include the LU decomposition, the QR decomposition or the Cholesky decomposition (for positive definite matrices). These methods are of order O("n"3), which is a significant improvement over O("n"!)
The LU decomposition expresses "A" in terms of a lower triangular matrix "L", an upper triangular matrix "U" and a permutation matrix "P":
The determinants of "L" and "U" can be quickly calculated, since they are the products of the respective diagonal entries. The determinant of "P" is just the sign formula_104 of the corresponding permutation (which is +1 for an even number of permutations and is −1 for an uneven number of permutations). The determinant of "A" is then
Moreover, the decomposition can be chosen such that "L" is a unitriangular matrix and therefore has determinant 1, in which case the formula further simplifies to
Further methods.
If the determinant of "A" and the inverse of "A" have already been computed, the matrix determinant lemma allows to quickly calculate the determinant of , where "u" and "v" are column vectors.
Since the definition of the determinant does not need divisions, a question arises: do fast algorithms exist that do not need divisions? This is especially interesting for matrices over rings. Indeed, algorithms with run-time proportional to "n"4 exist. An algorithm of Mahajan and Vinay, and Berkowitz is based on closed ordered walks (short "clow"). It computes more products than the determinant definition requires, but some of these products cancel and the sum of these products can be computed more efficiently. The final algorithm looks very much like an iterated product of triangular matrices.
If two matrices of order "n" can be multiplied in time "M"("n"), where for some , then the determinant can be computed in time O("M"("n")). This means, for example, that an O("n"2.376) algorithm exists based on the Coppersmith–Winograd algorithm.
Algorithms can also be assessed according to their bit complexity, i.e., how many bits of accuracy are needed to store intermediate values occurring in the computation. For example, the Gaussian elimination (or LU decomposition) method is of order O("n"3), but the bit length of intermediate values can become exponentially long. The Bareiss Algorithm, on the other hand, is an exact-division method based on Sylvester's identity is also of order "n"3, but the bit complexity is roughly the bit size of the original entries in the matrix times "n".
History.
Historically, determinants were used long before matrices: originally, a determinant was defined as a property of a system of linear equations. The determinant "determines" whether the system has a unique solution (which occurs precisely if the determinant is non-zero). In this sense, determinants were first used in the Chinese mathematics textbook "The Nine Chapters on the Mathematical Art" (九章算術, Chinese scholars, around the 3rd century BCE). In Europe, determinants were considered by Cardano at the end of the 16th century and larger ones by Leibniz.
In Japan, Seki Takakazu (関 孝和) is credited with the discovery of the resultant and the determinant (at first in 1683, the complete version no later than 1710). In Europe, Cramer (1750) added to the theory, treating the subject in relation to sets of equations. The recurrence law was first announced by Bézout (1764).
It was Vandermonde (1771) who first recognized determinants as independent functions. Laplace (1772) gave the general method of expanding a determinant in terms of its complementary minors: Vandermonde had already given a special case. Immediately following, Lagrange (1773) treated determinants of the second and third order and applied it to questions of elimination theory; he proved many special cases of general identities.
Gauss (1801) made the next advance. Like Lagrange, he made much use of determinants in the theory of numbers. He introduced the word determinant (Laplace had used "resultant"), though not in the present signification, but rather as applied to the discriminant of a quantic. Gauss also arrived at the notion of reciprocal (inverse) determinants, and came very near the multiplication theorem.
The next contributor of importance is Binet (1811, 1812), who formally stated the theorem relating to the product of two matrices of "m" columns and "n" rows, which for the special case of reduces to the multiplication theorem. On the same day (November 30, 1812) that Binet presented his paper to the Academy, Cauchy also presented one on the subject. (See Cauchy–Binet formula.) In this he used the word determinant in its present sense, summarized and simplified what was then known on the subject, improved the notation, and gave the multiplication theorem with a proof more satisfactory than Binet's. With him begins the theory in its generality.
The next important figure was Jacobi (from 1827). He early used the functional determinant which Sylvester later called the Jacobian, and in his memoirs in "Crelle" for 1841 he specially treats this subject, as well as the class of alternating functions which Sylvester has called "alternants". About the time of Jacobi's last memoirs, Sylvester (1839) and Cayley began their work.
The study of special forms of determinants has been the natural result of the completion of the general theory. Axisymmetric determinants have been studied by Lebesgue, Hesse, and Sylvester; persymmetric determinants by Sylvester and Hankel; circulants by Catalan, Spottiswoode, Glaisher, and Scott; skew determinants and Pfaffians, in connection with the theory of orthogonal transformation, by Cayley; continuants by Sylvester; Wronskians (so called by Muir) by Christoffel and Frobenius; compound determinants by Sylvester, Reiss, and Picquet; Jacobians and Hessians by Sylvester; and symmetric gauche determinants by Trudi. Of the textbooks on the subject Spottiswoode's was the first. In America, Hanus (1886), Weld (1893), and Muir/Metzler (1933) published treatises.
Applications.
Linear independence.
As mentioned above, the determinant of a matrix (with real or complex entries, say) is zero if and only if the column vectors (or the row vectors) of the matrix are linearly dependent. Thus, determinants can be used to characterize linearly dependent vectors. For example, given two linearly independent vectors "v"1, "v"2 in R3, a third vector "v"3 lies in the plane spanned by the former two vectors exactly if the determinant of the matrix consisting of the three vectors is zero. The same idea is also used in the theory of differential equations: given "n" functions "f"1("x"), ..., "f""n"("x") (supposed to be times differentiable), the Wronskian is defined to be
It is non-zero (for some "x") in a specified interval if and only if the given functions and all their derivatives up to order "n"−1 are linearly independent. If it can be shown that the Wronskian is zero everywhere on an interval then, in the case of analytic functions, this implies the given functions are linearly dependent. See the Wronskian and linear independence.
Orientation of a basis.
The determinant can be thought of as assigning a number to every sequence of "n" vectors in R"n", by using the square matrix whose columns are the given vectors. For instance, an orthogonal matrix with entries in R"n" represents an orthonormal basis in Euclidean space. The determinant of such a matrix determines whether the orientation of the basis is consistent with or opposite to the orientation of the standard basis. If the determinant is +1, the basis has the same orientation. If it is −1, the basis has the opposite orientation.
More generally, if the determinant of "A" is positive, "A" represents an orientation-preserving linear transformation (if "A" is an orthogonal or matrix, this is a rotation), while if it is negative, "A" switches the orientation of the basis.
Volume and Jacobian determinant.
As pointed out above, the absolute value of the determinant of real vectors is equal to the volume of the parallelepiped spanned by those vectors. As a consequence, if is the linear map represented by the matrix "A", and "S" is any measurable subset of R"n", then the volume of "f"("S") is given by |det("A")| times the volume of "S". More generally, if the linear map is represented by the matrix "A", then the "n"-dimensional volume of "f"("S") is given by:
By calculating the volume of the tetrahedron bounded by four points, they can be used to identify skew lines. The volume of any tetrahedron, given its vertices a, b, c, and d, is , or any other combination of pairs of vertices that would form a spanning tree over the vertices.
For a general differentiable function, much of the above carries over by considering the Jacobian matrix of "f". For
the Jacobian is the matrix whose entries are given by
Its determinant, the Jacobian determinant, appears in the higher-dimensional version of integration by substitution: for suitable functions "f" and an open subset "U" of R"n" (the domain of "f"), the integral over "f"("U") of some other function is given by
The Jacobian also occurs in the inverse function theorem.
Vandermonde determinant (alternant).
Third order
In general, the "n"th-order Vandermonde determinant is 
where the right-hand side is the continued product of all the differences that can be formed from the "n"("n"−1)/2 pairs of numbers taken from "x"1, "x"2, ..., "x""n", with the order of the differences taken in the reversed order of the suffixes that are involved.
Circulants.
Second order
Third order
where ω and ω2 are the complex cube roots of 1. In general, the "n"th-order circulant determinant is
where ω"j" is an "n"th root of 1.

</doc>
<doc id="349251" url="https://en.wikipedia.org/wiki?curid=349251" title="Diagonal">
Diagonal

In geometry, a diagonal is a line segment joining two nonconsecutive vertices of a polygon or polyhedron. Informally, any sloping line is called diagonal. The word "diagonal" derives from the ancient Greek διαγώνιος "diagonios", "from angle to angle" (from διά- "dia-", "through", "across" and γωνία "gonia", "angle", related to "gony" "knee"); it was used by both Strabo and Euclid to refer to a line connecting two vertices of a rhombus or cuboid, and later adopted into Latin as "diagonus" ("slanting line").
In matrix algebra, a diagonal of a square matrix is a set of entries extending from one corner to the farthest corner.
There are also other, non-mathematical uses.
Non-mathematical uses.
In engineering, a diagonal brace is a beam used to brace a rectangular structure (such as scaffolding) to withstand strong forces pushing into it; although called a diagonal, due to practical considerations diagonal braces are often not connected to the corners of the rectangle.
Diagonal pliers are wire-cutting pliers defined by the cutting edges of the jaws intersects the joint rivet at an angle or "on a diagonal", hence the name.
A diagonal lashing is a type of lashing used to bind spars or poles together applied so that the lashings cross over the poles at an angle.
In association football, the diagonal system of control is the method referees and assistant referees use to position themselves in one of the four quadrants of the pitch.
Polygons.
As applied to a polygon, a diagonal is a line segment joining any two non-consecutive vertices. Therefore, a quadrilateral has two diagonals, joining opposite pairs of vertices. For any convex polygon, all the diagonals are inside the polygon, but for re-entrant polygons, some diagonals are outside of the polygon.
Any "n"-sided polygon ("n" ≥ 3), convex or concave, has
or
diagonals, as each vertex has diagonals to all other vertices except itself and the two adjacent vertices, or "n" − 3 diagonals.
Matrices.
In the case of a square matrix, the "main" or "principal diagonal" is the diagonal line of entries running from the top-left corner to the bottom-right corner. For a matrix formula_3 with row index specified by formula_4 and column index specified by formula_5, these would be entries formula_6 with formula_7. For example, the identity matrix can be defined as having entries of 1 on the main diagonal and zeroes elsewhere:
The top-right to bottom-left diagonal is sometimes described as the "minor" diagonal or "antidiagonal". The "off-diagonal" entries are those not on the main diagonal. A "diagonal matrix" is one whose off-diagonal entries are all zero.
A "superdiagonal" entry is one that is directly above and to the right of the main diagonal. Just as diagonal entries are those formula_6 with formula_10, the superdiagonal entries are those with formula_11. For example, the non-zero entries of the following matrix all lie in the superdiagonal:
Likewise, a "subdiagonal" entry is one that is directly below and to the left of the main diagonal, that is, an entry formula_6 with formula_14. General matrix diagonals can be specified by an index formula_15 measured relative to the main diagonal: the main diagonal has formula_16; the superdiagonal has formula_17; the subdiagonal has formula_18; and in general, the formula_15-diagonal consists of the entries formula_6 with formula_21.
Geometry.
By analogy, the subset of the Cartesian product "X"×"X" of any set "X" with itself, consisting of all pairs (x,x), is called the diagonal, and is the graph of the equality relation on "X" or equivalently the graph of the identity function from "X" to "x". This plays an important part in geometry; for example, the fixed points of a mapping "F" from "X" to itself may be obtained by intersecting the graph of "F" with the diagonal.
In geometric studies, the idea of intersecting the diagonal "with itself" is common, not directly, but by perturbing it within an equivalence class. This is related at a deep level with the Euler characteristic and the zeros of vector fields. For example, the circle "S"1 has Betti numbers 1, 1, 0, 0, 0, and therefore Euler characteristic 0. A geometric way of expressing this is to look at the diagonal on the two-torus "S"1xS1 and observe that it can move "off itself" by the small motion (θ, θ) to (θ, θ + ε). In general, the intersection number of the graph of a function with the diagonal may be computed using homology via the Lefschetz fixed point theorem; the self-intersection of the diagonal is the special case of the identity function.

</doc>
<doc id="174080" url="https://en.wikipedia.org/wiki?curid=174080" title="Diagonal matrix">
Diagonal matrix

In linear algebra, a diagonal matrix is a matrix (usually a square matrix) in which the entries outside the main diagonal (↘) are all zero. The diagonal entries themselves may or may not be zero. Thus, the matrix with "n" columns and "n" rows is diagonal if:
For example, the following matrix is diagonal:
The term "diagonal matrix" may sometimes refer to a rectangular diagonal matrix, which is an "m"-by-"n" matrix with all the entries not of the form "d""i","i" being zero. For example:
However, in the remainder of this article we will consider only square matrices. Any square diagonal matrix is also a symmetric matrix. Also, if the entries come from the field R or C, then it is a normal matrix as well. Equivalently, we can define a diagonal matrix as a matrix that is both upper- and lower-triangular. The identity matrix "I""n" and any square zero matrix are diagonal. A one-dimensional matrix is always diagonal.
Scalar matrix.
A diagonal matrix with all its main diagonal entries equal is a scalar matrix, that is, a scalar multiple "λI" of the identity matrix "I". Its effect on a vector is scalar multiplication by "λ". For example, a 3×3 scalar matrix has the form:
The scalar matrices are the center of the algebra of matrices: that is, they are precisely the matrices that commute with all other square matrices of the same size.
For an abstract vector space "V" (rather than the concrete vector space formula_6), or more generally a module "M" over a ring "R", with the endomorphism algebra End("M") (algebra of linear operators on "M") replacing the algebra of matrices, the analog of scalar matrices are scalar transformations. Formally, scalar multiplication is a linear map, inducing a map formula_7 (send a scalar "λ" to the corresponding scalar transformation, multiplication by "λ") exhibiting End("M") as a "R"-algebra. For vector spaces, or more generally free modules formula_8, for which the endomorphism algebra is isomorphic to a matrix algebra, the scalar transforms are exactly the center of the endomorphism algebra, and similarly invertible transforms are the center of the general linear group GL("V"), where they are denoted by Z("V"), follow the usual notation for the center.
Matrix operations.
The operations of matrix addition and matrix multiplication are especially simple for diagonal matrices. Write for a diagonal matrix whose diagonal entries starting in the upper left corner are "a"1, ..., "a""n". Then, for addition, we have
and for matrix multiplication,
The diagonal matrix is invertible if and only if the entries "a"1, ..., "a""n" are all non-zero. In this case, we have
In particular, the diagonal matrices form a subring of the ring of all "n"-by-"n" matrices.
Multiplying an "n"-by-"n" matrix "A" from the "left" with amounts to multiplying the "i"th "row" of "A" by "a""i" for all "i"; multiplying the matrix "A" from the "right" with amounts to multiplying the "i"th "column" of "A" by "a""i" for all "i".
Operator matrix in eigenbasis.
As explained in determining coefficients of operator matrix, there is a special basis, "e"1, ..., "e""n", for which the matrix takes the diagonal form. Being diagonal means that all coefficients formula_9 but formula_10 are zeros in the defining equation formula_11, leaving only one term per sum. The surviving diagonal elements, formula_10, are known as eigenvalues and designated with formula_13 in the equation, which reduces to formula_14. The resulting equation is known as eigenvalue equation and used to derive the characteristic polynomial and, further, eigenvalues and eigenvectors.
In other words, the eigenvalues of are "λ"1, ..., "λ""n" with associated eigenvectors of "e"1, ..., "e""n".
Other properties.
The determinant of is the product "a"1..."a""n".
The adjugate of a diagonal matrix is again diagonal.
A square matrix is diagonal if and only if it is triangular and normal.
Uses.
Diagonal matrices occur in many areas of linear algebra. Because of the simple description of the matrix operation and eigenvalues/eigenvectors given above, it is always desirable to represent a given matrix or linear map by a diagonal matrix.
In fact, a given "n"-by-"n" matrix "A" is similar to a diagonal matrix (meaning that there is a matrix "X" such that "X"−1"AX" is diagonal) if and only if it has "n" linearly independent eigenvectors. Such matrices are said to be diagonalizable.
Over the field of real or complex numbers, more is true. The spectral theorem says that every normal matrix is unitarily similar to a diagonal matrix (if "AA"∗ = "A"∗"A" then there exists a unitary matrix "U" such that "UAU"∗ is diagonal). Furthermore, the singular value decomposition implies that for any matrix "A", there exist unitary matrices "U" and "V" such that "UAV"∗ is diagonal with positive entries.
Operator theory.
In operator theory, particularly the study of PDEs, operators are particularly easy to understand and PDEs easy to solve if the operator is diagonal with respect to the basis with which one is working; this corresponds to a separable partial differential equation. Therefore, a key technique to understanding operators is a change of coordinates – in the language of operators, an integral transform – which changes the basis to an eigenbasis of eigenfunctions: which makes the equation separable. An important example of this is the Fourier transform, which diagonalizes constant coefficient differentiation operators (or more generally translation invariant operators), such as the Laplacian operator, say, in the heat equation.
Especially easy are multiplication operators, which are defined as multiplication by (the values of) a fixed function – the values of the function at each point correspond to the diagonal entries of a matrix.

</doc>
<doc id="246325" url="https://en.wikipedia.org/wiki?curid=246325" title="Diagonalizable matrix">
Diagonalizable matrix

In linear algebra, a square matrix "A" is called diagonalizable if it is similar to a diagonal matrix, i.e., if there exists an invertible matrix "P" such that "P"−1"AP" is a diagonal matrix. If "V" is a finite-dimensional vector space, then a linear map "T" : "V" → "V" is called diagonalizable if there exists an ordered basis of "V" with respect to which "T" is represented by a diagonal matrix. Diagonalization is the process of finding a corresponding diagonal matrix for a diagonalizable matrix or linear map. A square matrix that is not diagonalizable is called "defective."
Diagonalizable matrices and maps are of interest because diagonal matrices are especially easy to handle: their eigenvalues and eigenvectors are known and one can raise a diagonal matrix to a power by simply raising the diagonal entries to that same power. Geometrically, a diagonalizable matrix is an inhomogeneous dilation (or "anisotropic scaling") — it scales the space, as does a "homogeneous dilation", but by a different factor in each direction, determined by the scale factors on each axis (diagonal entries).
Characterization.
The fundamental fact about diagonalizable maps and matrices is expressed by the following:
Another characterization: A matrix or linear map is diagonalizable over the field "F" if and only if its minimal polynomial is a product of distinct linear factors over "F". (Put in another way, a matrix is diagonalizable if and only if all of its elementary divisors are linear.)
The following sufficient (but not necessary) condition is often useful.
Let "A" be a matrix over "F". If "A" is diagonalizable, then so is any power of it. Conversely, if "A" is invertible, "F" is algebraically closed, and "An" is diagonalizable for some "n" that is not an integer multiple of the characteristic of "F", then "A" is diagonalizable. Proof: If "An" is diagonalizable, then "A" is annihilated by some polynomial formula_4, which has no multiple root (since formula_5) and is divided by the minimal polynomial of "A".
As a rule of thumb, over C almost every matrix is diagonalizable. More precisely: the set of complex "n"×"n" matrices that are "not" diagonalizable over C, considered as a subset of C"n"×"n", has Lebesgue measure zero. One can also say that the diagonalizable matrices form a dense subset with respect to the Zariski topology: the complement lies inside the set where the discriminant of the characteristic polynomial vanishes, which is a hypersurface. From that follows also density in the usual ("strong") topology given by a norm. The same is not true over R.
The Jordan–Chevalley decomposition expresses an operator as the sum of its semisimple (i.e., diagonalizable) part and its nilpotent part. Hence, a matrix is diagonalizable if and only if its nilpotent part is zero. Put in another way, a matrix is diagonalizable if each block in its Jordan form has no nilpotent part; i.e., each "block" is a one-by-one matrix.
Diagonalization.
If a matrix "A" can be diagonalized, that is,
then:
Writing "P" as a block matrix of its column vectors formula_8
the above equation can be rewritten as
So the column vectors of "P" are right eigenvectors of "A", and the corresponding diagonal entry is the corresponding eigenvalue. The invertibility of "P" also suggests that the eigenvectors are linearly independent and form a basis of "F""n". This is the necessary and sufficient condition for diagonalizability and the canonical approach of diagonalization. The row vectors of "P"−1 are the left eigenvectors of "A".
When the matrix "A" is a Hermitian matrix (resp. symmetric matrix), eigenvectors of "A" can be chosen to form an orthonormal basis of C"n" (resp. R"n"). Under such circumstance "P" will be a unitary matrix (resp. orthogonal matrix) and "P"−1 equals the conjugate transpose (resp. transpose) of "P".
Simultaneous diagonalization.
A set of matrices are said to be "simultaneously diagonalizable" if there exists a single invertible matrix "P" such that "P"−1"AP" is a diagonal matrix for every "A" in the set. The following theorem characterises simultaneously diagonalisable matrices: A set of diagonalizable matrices commutes if and only if the set is simultaneously diagonalisable.
The set of all "n"×"n" diagonalisable matrices (over C) with "n" > 1 is not simultaneously diagonalisable. For instance, the matrices
are diagonalizable but not simultaneously diagonalizable because they do not commute.
A set consists of commuting normal matrices if and only if it is simultaneously diagonalisable by a unitary matrix; that is, there exists a unitary matrix "U" such that "U*AU" is diagonal for every "A" in the set.
In the language of Lie theory, a set of simultaneously diagonalisable matrices generate a toral Lie algebra.
Examples.
Matrices that are not diagonalizable.
In general, a rotation matrix is not diagonalizable over the reals, but all rotation matrices are diagonalizable over the complex field. Even if a matrix is not diagonalizable, it is always possible to "do the best one can", and find a matrix with the same properties consisting of eigenvalues on the leading diagonal, and either ones or zeroes on the superdiagonal - known as Jordan normal form.
Some matrices are not diagonalizable over any field, most notably nonzero nilpotent matrices. This happens more generally if the algebraic and geometric multiplicities of an eigenvalue do not coincide. For instance, consider
This matrix is not diagonalizable: there is no matrix "U" such that "U"−1"CU" is a diagonal matrix. Indeed, "C" has one eigenvalue (namely zero) and this eigenvalue has algebraic multiplicity 2 and geometric multiplicity 1.
Some real matrices are not diagonalizable over the reals. Consider for instance the matrix
The matrix "B" does not have any real eigenvalues, so there is no real matrix "Q" such that "Q"−1"BQ" is a diagonal matrix. However, we can diagonalize "B" if we allow complex numbers. Indeed, if we take
then "Q"−1"BQ" is diagonal.
Note that the above examples show that the sum of diagonalizable matrices need not be diagonalizable.
How to diagonalize a matrix.
Consider a matrix
This matrix has eigenvalues
"A" is a 3×3 matrix with 3 different eigenvalues; therefore, it is diagonalizable. Note that if there are exactly "n" distinct eigenvalues in an "n"×"n" matrix then this matrix is diagonalizable.
These eigenvalues are the values that will appear in the diagonalized form of matrix "A", so by finding the eigenvalues of "A" we have diagonalized it. We could stop here, but it is a good check to use the eigenvectors to diagonalize "A".
The eigenvectors of "A" are
One can easily check that formula_18
Now, let "P" be the matrix with these eigenvectors as its columns:
Note there is no preferred order of the eigenvectors in "P"; changing the order of the eigenvectors in "P" just changes the order of the eigenvalues in the diagonalized form of "A".
Then "P" diagonalizes "A", as a simple computation confirms, having calculated "P" −1 using any suitable method:
Note that the eigenvalues formula_21 appear in the diagonal matrix.
An application.
Diagonalization can be used to compute the powers of a matrix "A" efficiently, provided the matrix is diagonalizable. Suppose we have found that
is a diagonal matrix. Then, as the matrix product is associative,
and the latter is easy to calculate since it only involves the powers of a diagonal matrix. This approach can be generalized to matrix exponential and other matrix functions since they can be defined as power series.
This is particularly useful in finding closed form expressions for terms of linear recursive sequences, such as the Fibonacci numbers.
Particular application.
For example, consider the following matrix:
Calculating the various powers of "M" reveals a surprising pattern:
The above phenomenon can be explained by diagonalizing "M". To accomplish this, we need a basis of R2 consisting of eigenvectors of "M". One such eigenvector basis is given by
where e"i" denotes the standard basis of R"n". The reverse change of basis is given by
Straightforward calculations show that
Thus, "a" and "b" are the eigenvalues corresponding to u and v, respectively. By linearity of matrix multiplication, we have that
Switching back to the standard basis, we have
The preceding relations, expressed in matrix form, are
thereby explaining the above phenomenon.
Quantum mechanical application.
In quantum mechanical and quantum chemical computations matrix diagonalization is one of the most frequently applied numerical processes. The basic reason is that the time-independent Schrödinger equation is an eigenvalue equation, albeit in most of the physical situations on an infinite dimensional space (a Hilbert space). A very common approximation is to truncate Hilbert space to finite dimension, after which the Schrödinger equation can be formulated as an eigenvalue problem of a real symmetric, or complex Hermitian, matrix. Formally this approximation is founded on the variational principle, valid for Hamiltonians that are bounded from below.
But also first-order perturbation theory for degenerate states leads to a matrix eigenvalue problem.

</doc>
<doc id="330206" url="https://en.wikipedia.org/wiki?curid=330206" title="Differentiable function">
Differentiable function

In calculus (a branch of mathematics), a differentiable function of one real variable is a function whose derivative exists at each point in its domain. As a result, the graph of a differentiable function must have a (non-vertical) tangent line at each point in its domain, be relatively smooth, and cannot contain any breaks, bends, or cusps.
More generally, if "x"0 is a point in the domain of a function "f", then "f" is said to be differentiable at "x"0 if the derivative "f" ′("x"0) exists. This means that the graph of "f" has a non-vertical tangent line at the point ("x"0, "f"("x"0)). The function "f" may also be called locally linear at "x"0, as it can be well approximated by a linear function near this point.
Differentiability and continuity.
If is differentiable at a point , then must also be continuous at . In particular, any differentiable function must be continuous at every point in its domain. "The converse does not hold": a continuous function need not be differentiable. For example, a function with a bend, cusp, or vertical tangent may be continuous, but fails to be differentiable at the location of the anomaly.
Most functions that occur in practice have derivatives at all points or at almost every point. However, a result of Stefan Banach states that the set of functions that have a derivative at some point is a meager set in the space of all continuous functions. Informally, this means that differentiable functions are very atypical among continuous functions. The first known example of a function that is continuous everywhere but differentiable nowhere is the Weierstrass function.
Differentiability classes.
A function "f" is said to be continuously differentiable if the derivative "f"'("x") exists and is itself a continuous function. Though the derivative of a differentiable function never has a jump discontinuity, it is possible for the derivative to have an essential discontinuity. For example, the function
is differentiable at 0, since
exists. However, for "x"≠0,
which has no limit as "x" → 0. Nevertheless, Darboux's theorem implies that the derivative of any function satisfies the conclusion of the intermediate value theorem.
Sometimes continuously differentiable functions are said to be of class "C"1. A function is of class "C"2 if the first and second derivative of the function both exist and are continuous. More generally, a function is said to be of class "C""k" if the first "k" derivatives "f"′("x"), "f"″("x"), ..., "f"("k")("x") all exist and are continuous. If derivatives f(n) exist for all positive integers n, the function is smooth or equivalently, of class "C""∞".
Differentiability in higher dimensions.
If all the partial derivatives of a function all exist and are continuous in a neighborhood of a point, then the function must be differentiable at that point, and it is of class "C"1.
Formally, a function of several real variables is said to be differentiable at a point if there exists a linear map such that
If a function is differentiable at , then all of the partial derivatives must exist at , in which case the linear map is given by the Jacobian matrix. A similar formulation of the higher-dimensional derivative is provided by the fundamental increment lemma found in single-variable calculus.
Note that existence of the partial derivatives (or even all of the directional derivatives) does not in general guarantee that a function is differentiable at a point. For example, the function defined by
is not differentiable at , but all of the partial derivatives and directional derivatives exist at this point. For a continuous example, the function
is not differentiable at , but again all of the partial derivatives and directional derivatives exist.
Differentiability in complex analysis.
In complex analysis, any function that is complex-differentiable in a neighborhood of a point is called holomorphic. Such a function is necessarily infinitely differentiable, and in fact analytic.
Differentiable functions on manifolds.
If "M" is a differentiable manifold, a real or complex-valued function "f" on "M" is said to be differentiable at a point "p" if it is differentiable with respect to some (or any) coordinate chart defined around "p". More generally, if "M" and "N" are differentiable manifolds, a function "f": "M" → "N" is said to be differentiable at a point "p" if it is differentiable with respect to some (or any) coordinate charts defined around "p" and "f"("p").

</doc>
<doc id="2119219" url="https://en.wikipedia.org/wiki?curid=2119219" title="Differentiable manifold">
Differentiable manifold

In mathematics, a differentiable manifold is a type of manifold that is locally similar enough to a linear space to allow one to do calculus. Any manifold can be described by a collection of charts, also known as an atlas. One may then apply ideas from calculus while working within the individual charts, since each chart lies within a linear space to which the usual rules of calculus apply. If the charts are suitably compatible (namely, the transition from one chart to another is differentiable), then computations done in one chart are valid in any other differentiable chart.
In formal terms, a differentiable manifold is a topological manifold with a globally defined differential structure. Any topological manifold can be given a differential structure "locally" by using the homeomorphisms in its atlas and the standard differential structure on a linear space. To induce a global differential structure on the local coordinate systems induced by the homeomorphisms, their composition on chart intersections in the atlas must be differentiable functions on the corresponding linear space. In other words, where the domains of charts overlap, the coordinates defined by each chart are required to be differentiable with respect to the coordinates defined by every chart in the atlas. The maps that relate the coordinates defined by the various charts to one another are called "transition maps."
Differentiability means different things in different contexts including: continuously differentiable, "k" times differentiable, smooth, and holomorphic. Furthermore, the ability to induce such a differential structure on an abstract space allows one to extend the definition of differentiability to spaces without global coordinate systems. A differential structure allows one to define the globally differentiable tangent space, differentiable functions, and differentiable tensor and vector fields. Differentiable manifolds are very important in physics. Special kinds of differentiable manifolds form the basis for physical theories such as classical mechanics, general relativity, and Yang–Mills theory. It is possible to develop a calculus for differentiable manifolds. This leads to such mathematical machinery as the exterior calculus. The study of calculus on differentiable manifolds is known as differential geometry.
History.
The emergence of differential geometry as a distinct discipline is generally credited to Carl Friedrich Gauss and Bernhard Riemann. Riemann first described manifolds in his famous habilitation lecture before the faculty at Göttingen. He motivated the idea of a manifold by an intuitive process of varying a given object in a new direction, and presciently described the role of coordinate systems and charts in subsequent formal developments:
The works of physicists such as James Clerk Maxwell, and mathematicians Gregorio Ricci-Curbastro and Tullio Levi-Civita led to the development of tensor analysis and the notion of covariance, which identifies an intrinsic geometric property as one that is invariant with respect to coordinate transformations. These ideas found a key application in Einstein's theory of general relativity and its underlying equivalence principle. A modern definition of a 2-dimensional manifold was given by Hermann Weyl in his 1913 book on Riemann surfaces. The widely accepted general definition of a manifold in terms of an atlas is due to Hassler Whitney.
Definition.
A "presentation" of a topological manifold is a second countable Hausdorff space that is locally homeomorphic to a linear space, by a collection (called an "atlas") of homeomorphisms called "charts". The composition of one chart with the inverse of another chart is a function called a "transition map", and defines a homeomorphism of an open subset of the linear space onto another open subset of the linear space. This formalizes the notion of "patching together pieces of a space to make a manifold" – the manifold produced also contains the data of how it has been patched together. However, different atlases (patchings) may produce "the same" manifold; a manifold does not come with a preferred atlas. And, thus, one defines a topological manifold to be a space as above with an "equivalence class" of atlases, where one defines equivalence of atlases below.
There are a number of different types of differentiable manifolds, depending on the precise differentiability requirements on the transition functions. Some common examples include the following.
While there is a meaningful notion of a "Ck" "atlas," there is no distinct notion of a "Ck" "manifold" other than "C"0 (continuous maps: a topological manifold) and "C"∞ (smooth maps: a smooth manifold), because for every "Ck"-structure with "k" > 0, there is a unique "Ck"-equivalent "C"∞-structure (every "Ck"-structure is "uniquely smoothable" to a "C"∞-structure) – a result of Whitney. In fact, every "Ck"-structure is uniquely smoothable to a "C"ω-structure. Furthermore, two "Ck" atlases that are equivalent to a single "C"∞ atlas are equivalent as "Ck" atlases, so two distinct "Ck" atlases do not collide. See Differential structure: Existence and uniqueness theorems for details. Thus one uses the terms "differentiable manifold" and "smooth manifold" interchangeably; this is in stark contrast to "Ck" "maps," where there are meaningful differences for different "k." For example, the Nash embedding theorem states that any manifold can be "Ck" isometrically embedded in Euclidean space R"N" – for any 1 ≤ "k" ≤ ∞ there is a sufficiently large "N", but "N" depends on "k".
On the other hand, complex manifolds are significantly more restrictive. As an example, Chow's theorem states that any projective complex manifold is in fact a projective variety – it has an algebraic structure.
Atlases.
An atlas on a topological space "X" is a collection of pairs {("U"α,φα)} called "charts", where the "U"α are open sets that cover "X", and for each index α
is a homeomorphism of "U"α onto an open subset of "n"-dimensional real space. The transition maps of the atlas are the functions
Every topological manifold has an atlas. A "Ck"-atlas is an atlas whose transition maps are "Ck". A topological manifold has a "C"0-atlas and in general a "Ck"-manifold has a "Ck"-atlas. A continuous atlas is a "C"0 atlas, a smooth atlas is a "C"∞ atlas and an analytic atlas is a "C"ω atlas. If the atlas is at least "C"1, it is also called a "differential structure" or "differentiable structure". A "holomorphic atlas" is an atlas whose underlying Euclidean space is defined on the complex field and whose transition maps are biholomorphic.
Compatible atlases.
Different atlases can give rise to, in essence, the same manifold. The circle can be mapped by two coordinate charts, but if the domains of these charts are changed slightly a different atlas for the same manifold is obtained. These different atlases can be combined into a bigger atlas. It can happen that the transition maps of such a combined atlas are not as smooth as those of the constituent atlases. If "Ck" atlases can be combined to form a "Ck" atlas, then they are called compatible. Compatibility of atlases is an equivalence relation; by combining all the atlases in an equivalence class, a maximal atlas can be constructed. Each "Ck" atlas belongs to a unique maximal "Ck" atlas.
Alternative definitions.
Pseudogroups.
The notion of a pseudogroup provides a flexible generalization of atlases in order to allow a variety of different structures to be defined on manifolds in a uniform way. A "pseudogroup" consists of a topological space "S" and a collection Γ consisting of homeomorphisms from open subsets of "S" to other open subsets of "S" such that
These last three conditions are analogous to the definition of a group. Note that Γ need not be a group, however, since the functions are not globally defined on "S". For example, the collection of all local "Ck" diffeomorphisms on R"n" form a pseudogroup. All biholomorphisms between open sets in C"n" form a pseudogroup. More examples include: orientation preserving maps of R"n", symplectomorphisms, Möbius transformations, affine transformations, and so on. Thus a wide variety of function classes determine pseudogroups.
An atlas ("Ui", φ"i") of homeomorphisms φ"i" from "Ui" ⊂ "M" to open subsets of a topological space "S" is said to be "compatible" with a pseudogroup Γ provided that the transition functions φ"j" o φ"i"−1: φ"i"("Ui" ∩ "Uj") → φ"j"("Ui" ∩ "Uj") are all in Γ.
A differentiable manifold is then an atlas compatible with the pseudogroup of "C""k" functions on R"n". A complex manifold is an atlas compatible with the biholomorphic functions on open sets in C"n". And so forth. Thus pseudogroups provide a single framework in which to describe many structures on manifolds of importance to differential geometry and topology.
Structure sheaf.
Sometimes it can be useful to use an alternative approach to endow a manifold with a "Ck"-structure. Here "k" = 1, 2, ..., ∞, or ω for real analytic manifolds. Instead of considering coordinate charts, it is possible to start with functions defined on the manifold itself. The structure sheaf of "M", denoted C"k", is a sort of functor that defines, for each open set "U" ⊂ "M", an algebra C"k"("U") of continuous functions "U" → R. A structure sheaf C"k" is said to give "M" the structure of a "C""k" manifold of dimension "n" provided that, for any "p" ∈ "M", there exists a neighborhood "U" of "p" and "n" functions "x"1...,"x""n" ∈ C"k"("U") such that the map "f" = ("x"1, ..., "xn"): "U" → R"n" is a homeomorphism onto an open set in R"n", and such that C"k"|"U" is the pullback of the sheaf of "k"-times continuously differentiable functions on R"n".
In particular, this latter condition means that any function "h" in C"k"("V"), for "V", can be written uniquely as "h"("x") = "H"("x"1("x")...,"x""n"("x")), where "H" is a "k"-times differentiable function on "f"("V") (an open set in R"n"). Thus, the sheaf-theoretic viewpoint is that the functions on a differentiable manifold can be expressed in local coordinates as differentiable functions on R"n", and "a fortiori" this is sufficient to characterize the differential structure on the manifold.
Sheaves of local rings.
A similar, but more technical, approach to defining differentiable manifolds can be formulated using the notion of a ringed space. This approach is strongly influenced by the theory of schemes in algebraic geometry, but uses local rings of the germs of differentiable functions. It is especially popular in the context of "complex" manifolds.
We begin by describing the basic structure sheaf on R"n". If "U" is an open set in R"n", let
consist of all real-valued "k"-times continuously differentiable functions on "U". As "U" varies, this determines a sheaf of rings on Rn. The stalk O"p" for "p" ∈ R"n" consists of germs of functions near "p", and is an algebra over R. In particular, this is a local ring whose unique maximal ideal consists of those functions that vanish at "p". The pair (R"n", O) is an example of a locally ringed space: it is a topological space equipped with a sheaf whose stalks are each local rings.
A differentiable manifold (of class "Ck") consists of a pair ("M", O"M") where "M" is a second countable Hausdorff space, and O"M" is a sheaf of local R-algebras defined on "M", such that the locally ringed space ("M", O"M") is locally isomorphic to (R"n", O). In this way, differentiable manifolds can be thought of as schemes modelled on R"n". This means that, for each point "p" ∈ "M", there is a neighborhood "U" of "p", and a pair of functions ("f", "f"#) where
There are a number of important motivations for studying differentiable manifolds within this abstract framework. First, there is no "a priori" reason that the model space needs to be Rn. For example (in particular in algebraic geometry), one could take this to be the space of complex numbers C"n" equipped with the sheaf of holomorphic functions (thus arriving at the spaces of complex analytic geometry), or the sheaf of polynomials (thus arriving at the spaces of interest in complex "algebraic" geometry). In broad terms, this concept can be adapted for any suitable notion of a scheme (see topos theory). Second, coordinates are no longer explicitly necessary to the construction. The analog of a coordinate system is the pair ("f", "f"#), but these merely quantify the idea of "local isomorphism" rather than being central to the discussion (as in the case of charts and atlases). Third, the sheaf O"M" is not manifestly a sheaf of functions at all. Rather, it emerges as a sheaf of functions as a "consequence" of the construction (via the quotients of local rings by their maximal ideals). Hence it is a more primitive definition of the structure (see synthetic differential geometry).
A final advantage of this approach is that it allows for natural direct descriptions of many of the fundamental objects of study to differential geometry and topology.
Differentiable functions.
A real valued function "f" on an "n"-dimensional differentiable manifold "M" is called differentiable at a point "p" ∈ "M" if it is differentiable in any coordinate chart defined around "p". In more precise terms, if ("U", φ) is a chart where "U" is an open set in "M" containing "p" and φ: "U" → Rn is the map defining the chart, then "f" is differentiable if and only if
is differentiable at φ("p"). In general there will be many available charts; however, the definition of differentiability does not depend on the choice of chart at "p". It follows from the chain rule applied to the transition functions between one chart and another that if "f" is differentiable in any particular chart at "p", then it is differentiable in all charts at "p". Analogous considerations apply to defining "Ck" functions, smooth functions, and analytic functions.
Differentiation of functions.
There are various ways to define the derivative of a function on a differentiable manifold, the most fundamental of which is the directional derivative. The definition of the directional derivative is complicated by the fact that a manifold will lack a suitable affine structure with which to define vectors. The directional derivative therefore looks at curves in the manifold instead of vectors.
Directional differentiation.
Given a real valued function "f" on an "m" dimensional differentiable manifold "M", the directional derivative of "f" at a point "p" in "M" is defined as follows. Suppose that γ("t") is a curve in "M" with γ(0) = "p", which is "differentiable" in the sense that its composition with any chart is a differentiable curve in R"m". Then the directional derivative of "f" at "p" along γ is
If γ1 and γ2 are two curves such that γ1(0) = γ2(0) = "p", and in any coordinate chart φ,
then, by the chain rule, "f" has the same directional derivative at "p" along γ1 as along γ2. This means that the directional derivative depends only on the tangent vector of the curve at "p". Thus the more abstract definition of directional differentiation adapted to the case of differentiable manifolds ultimately captures the intuitive features of directional differentiation in an affine space.
Tangent vectors and the differential.
A tangent vector at "p" ∈ "M" is an equivalence class of differentiable curves γ with γ(0) = "p", modulo the equivalence relation of first-order contact between the curves. Therefore,
in every coordinate chart "φ". Therefore, the equivalence classes are curves through "p" with a prescribed velocity vector at "p". The collection of all tangent vectors at "p" forms a vector space: the tangent space to "M" at "p", denoted "T""p""M".
If "X" is a tangent vector at "p" and "f" a differentiable function defined near "p", then differentiating "f" along any curve in the equivalence class defining "X" gives a well-defined directional derivative along "X":
Once again, the chain rule establishes that this is independent of the freedom in selecting γ from the equivalence class, since any curve with the same first order contact will yield the same directional derivative.
If the function "f" is fixed, then the mapping
is a linear functional on the tangent space. This linear functional is often denoted by "df"("p") and is called the differential of "f" at "p":
Partitions of unity.
One of the topological features of the sheaf of differentiable functions on a differentiable manifold is that it admits partitions of unity. This distinguishes the differential structure on a manifold from stronger structures (such as analytic and holomorphic structures) that in general fail to have partitions of unity.
Suppose that "M" is a manifold of class "Ck", where 0 ≤ "k" ≤ ∞. Let {"U"α} be an open covering of "M". Then a partition of unity subordinate to the cover {"U"α} is a collection of real-valued "Ck" functions φ"i" on "M" satisfying the following conditions:
Every open covering of a "Ck" manifold "M" has a "Ck" partition of unity. This allows for certain constructions from the topology of "Ck" functions on R"n" to be carried over to the category of differentiable manifolds. In particular, it is possible to discuss integration by choosing a partition of unity subordinate to a particular coordinate atlas, and carrying out the integration in each chart of R"n". Partitions of unity therefore allow for certain other kinds of function spaces to be considered: for instance Lp spaces, Sobolev spaces, and other kinds of spaces that require integration.
Differentiability of mappings between manifolds.
Suppose "M" and "N" are two differentiable manifolds with dimensions "m" and "n", respectively, and "f" is a function from "M" to "N". Since differentiable manifolds are topological spaces we know what it means for "f" to be continuous. But what does ""f" is "Ck"("M", "N")" mean for "k" ≥ 1? We know what that means when "f" is a function between Euclidean spaces, so if we compose "f" with a chart of "M" and a chart of "N" such that we get a map that goes from Euclidean space to "M" to "N" to Euclidean space we know what it means for that map to be "Ck"(R"m", R"n"). We define ""f" is "Ck"("M", "N")" to mean that all such compositions of "f" with charts are "Ck"(R"m", R"n"). Once again the chain rule guarantees that the idea of differentiability does not depend on which charts of the atlases on "M" and "N" are selected. However, defining the derivative itself is more subtle. If "M" or "N" is itself already a Euclidean space, then we don't need a chart to map it to one.
Algebra of scalars.
For a "Ck" manifold "M", the set of real-valued "Ck" functions on the manifold forms an algebra under pointwise addition and multiplication, called the "algebra of scalar fields" or simply the "algebra of scalars". This algebra has the constant function 1 as the multiplicative identity, and is a differentiable analog of the ring of regular functions in algebraic geometry.
It is possible to reconstruct a manifold from its algebra of scalars, first as a set, but also as a topological space – this is an application of the Banach–Stone theorem, and is more formally known as the spectrum of a C*-algebra. First, there is a one-to-one correspondence between the points of "M" and the algebra homomorphisms φ: "Ck"("M") → R, as such a homomorphism "φ" corresponds a codimension one ideal in "Ck"("M") (namely the kernel of "φ"), which is necessarily a maximal ideal. On the converse, every maximal ideal in this algebra is an ideal of functions vanishing at a single point, which demonstrates that MSpec (the Max Spec) of "Ck"("M") recovers "M" as a point set, though in fact it recovers "M" as a topological space.
One can define various geometric structures algebraically in terms of the algebra of scalars, and these definitions often generalize to algebraic geometry (interpreting rings geometrically) and operator theory (interpreting Banach spaces geometrically). For example, the tangent bundle to "M" can be defined as the derivations of the algebra of smooth functions on "M".
This "algebraization" of a manifold (replacing a geometric object with an algebra) leads to the notion of a C*-algebra – a commutative C*-algebra being precisely the ring of scalars of a manifold, by Banach–Stone, and allows one to consider "non"commutative C*-algebras as non-commutative generalizations of manifolds. This is the basis of the field of noncommutative geometry.
Bundles.
Tangent bundle.
The tangent space of a point consists of the possible directional derivatives at that point, and has the same dimension "n" as does the manifold. For a set of (non-singular) coordinates "xk" local to the point, the coordinate derivatives formula_13 typically define a basis of the tangent space. The collection of tangent spaces at all points can in turn be made into a manifold, the tangent bundle, whose dimension is 2"n". The tangent bundle is where tangent vectors lie, and is itself a differentiable manifold. The Lagrangian is a function on the tangent bundle. One can also define the tangent bundle as the bundle of 1-jets from R (the real line) to "M".
One may construct an atlas for the tangent bundle consisting of charts based on "U"α × R"n", where "U"α denotes one of the charts in the atlas for "M". Each of these new charts is the tangent bundle for the charts "U"α. The transition maps on this atlas are defined from the transition maps on the original manifold, and retain the original differentiability class.
Cotangent bundle.
The dual space of a vector space is the set of real valued linear functions on the vector space. The cotangent space at a point is the dual of the tangent space at that point, and the cotangent bundle is the collection of all cotangent spaces.
Like the tangent bundle the cotangent bundle is again a differentiable manifold. The Hamiltonian is a scalar on the cotangent bundle. The total space of a cotangent bundle has the structure of a symplectic manifold. Cotangent vectors are sometimes called "covectors". One can also define the cotangent bundle as the bundle of 1-jets of functions from "M" to R.
Elements of the cotangent space can be thought of as infinitesimal displacements: if "f" is a differentiable function we can define at each point "p" a cotangent vector "dfp", which sends a tangent vector "Xp" to the derivative of "f" associated with "Xp". However, not every covector field can be expressed this way. Those that can are referred to as exact differentials. For a given set of local coordinates "xk" the differentials "dx" form a basis of the cotangent space at "p".
Tensor bundle.
The tensor bundle is the direct sum of all tensor products of the tangent bundle and the cotangent bundle. Each element of the bundle is a tensor field, which can act as a multilinear operator on vector fields, or on other tensor fields.
The tensor bundle cannot be a differentiable manifold, since it is infinite dimensional. It is however an algebra over the ring of scalar functions. Each tensor is characterized by its ranks, which indicate how many tangent and cotangent factors it has. Sometimes these ranks are referred to as "covariant" and "contravariant" ranks, signifying tangent and cotangent ranks, respectively.
Frame bundle.
A frame (or, in more precise terms, a tangent frame) is an ordered basis of particular tangent space. Likewise, a tangent frame is a linear isomorphism of R"n" to this tangent space. A moving tangent frame is an ordered list of vector fields that give a basis at every point of their domain. One may also regard a moving frame as a section of the frame bundle F("M"), a GL("n", R) principal bundle made up of the set of all frames over "M". The frame bundle is useful because tensor fields on "M" can be regarded as equivariant vector-valued functions on F("M").
Jet bundles.
On a manifold that is sufficiently smooth, various kinds of jet bundles can also be considered. The (first-order) tangent bundle of a manifold is the collection of curves in the manifold modulo the equivalence relation of first-order contact. By analogy, the "k"-th order tangent bundle is the collection of curves modulo the relation of "k"-th order contact. Likewise, the cotangent bundle is the bundle of 1-jets of functions on the manifold: the "k"-jet bundle is the bundle of their "k"-jets. These and other examples of the general idea of jet bundles play a significant role in the study of differential operators on manifolds.
The notion of a frame also generalizes to the case of higher-order jets. Define a "k"-th order frame to be the "k"-jet of a diffeomorphism from R"n" to "M". The collection of all "k"-th order frames, "Fk"("M"), is a principal "Gk" bundle over "M", where "Gk" is the group of "k"-jets; i.e., the group made up of "k"-jets of diffeomorphisms of R"n" that fix the origin. Note that GL("n", R) is naturally isomorphic to "G"1, and a subgroup of every "Gk", "k" ≥ 2. In particular, a section of "F"2("M") gives the frame components of a connection on "M". Thus, the quotient bundle "F"2("M")/ GL("n", R) is the bundle of linear connections over "M".
Calculus on manifolds.
Many of the techniques from multivariate calculus also apply, "mutatis mutandis", to differentiable manifolds. One can define the directional derivative of a differentiable function along a tangent vector to the manifold, for instance, and this leads to a means of generalizing the total derivative of a function: the differential. From the perspective of calculus, the derivative of a function on a manifold behaves in much the same way as the ordinary derivative of a function defined on a Euclidean space, at least locally. For example, there are versions of the implicit and inverse function theorems for such functions.
There are, however, important differences in the calculus of vector fields (and tensor fields in general). In brief, the directional derivative of a vector field is not well-defined, or at least not defined in a straightforward manner. Several generalizations of the derivative of a vector field (or tensor field) do exist, and capture certain formal features of differentiation in Euclidean spaces. The chief among these are:
Ideas from integral calculus also carry over to differential manifolds. These are naturally expressed in the language of exterior calculus and differential forms. The fundamental theorems of integral calculus in several variables — namely Green's theorem, the divergence theorem, and Stokes' theorem — generalize to a theorem (also called Stokes' theorem) relating the exterior derivative and integration over submanifolds.
Differential calculus of functions.
Differentiable functions between two manifolds are needed in order to formulate suitable notions of submanifolds, and other related concepts. If "f": "M" → "N" is a differentiable function from a differentiable manifold "M" of dimension "m" to another differentiable manifold "N" of dimension "n", then the differential of "f" is a mapping "df": T"M" → T"N". It also denoted by "Tf" and called the tangent map. At each point of "M", this is a linear transformation from one tangent space to another:
The rank of "f" at "p" is the rank of this linear transformation.
Usually the rank of a function is a pointwise property. However, if the function has maximal rank, then the rank will remain constant in a neighborhood of a point. A differentiable function "usually" has maximal rank, in a precise sense given by Sard's theorem. Functions of maximal rank at a point are called immersions and submersions:
Lie derivative.
A Lie derivative, named after Sophus Lie, is a derivation on the algebra of tensor fields over a manifold "M". The vector space of all Lie derivatives on "M" forms an infinite dimensional Lie algebra with respect to the Lie bracket defined by
The Lie derivatives are represented by vector fields, as infinitesimal generators of flows (active diffeomorphisms) on "M". Looking at it the other way round, the group of diffeomorphisms of "M" has the associated Lie algebra structure, of Lie derivatives, in a way directly analogous to the Lie group theory.
Exterior calculus.
The exterior calculus allows for a generalization of the gradient, divergence and curl operators.
The bundle of differential forms, at each point, consists of all totally antisymmetric multilinear maps on the tangent space at that point. It is naturally divided into "n"-forms for each "n" at most equal to the dimension of the manifold; an "n"-form is an "n"-variable form, also called a form of degree "n". The 1-forms are the cotangent vectors, while the 0-forms are just scalar functions. In general, an "n"-form is a tensor with cotangent rank "n" and tangent rank 0. But not every such tensor is a form, as a form must be antisymmetric.
Exterior derivative.
There is a map from scalars to covectors called the exterior derivative
such that 
This map is the one that relates covectors to infinitesimal displacements, mentioned above; some covectors are the exterior derivatives of scalar functions. It can be generalized into a map from the "n"-forms onto the ("n"+1)-forms. Applying this derivative twice will produce a zero form. Forms with zero derivative are called closed forms, while forms that are themselves exterior derivatives are known as exact forms.
The space of differential forms at a point is the archetypal example of an exterior algebra; thus it possesses a wedge product, mapping a "k"-form and "l"-form to a ("k"+"l")-form. The exterior derivative extends to this algebra, and satisfies a version of the product rule:
From the differential forms and the exterior derivative, one can define the de Rham cohomology of the manifold. The rank "n" cohomology group is the quotient group of the closed forms by the exact forms.
Topology of differentiable manifolds.
Relationship with topological manifolds.
Every topological manifold in dimension 1, 2, or 3 has a unique differential structure (up to diffeomorphism); thus the concepts of topological and differentiable manifold are distinct only in higher dimensions. It is known that in each higher dimension, there are some topological manifolds with no smooth structure, and some with multiple non-diffeomorphic structures.
The existence of non-smoothable manifolds was proven by {{harvtxt|Kervaire|1960}}, see Kervaire manifold, and later explained in the context of Donaldson's theorem (compare Hilbert's fifth problem); a good example of a non-smoothable manifold is the E8 manifold.
The classic example of manifolds with multiple incompatible structures are the exotic 7-spheres of John Milnor.
Classification.
Every second-countable 1-manifold without boundary is homeomorphic to a disjoint union of countably many copies of R (the real line) and S (the circle); the only connected examples are R and S, and of these only S is compact. In higher dimensions, classification theory normally focuses only on compact connected manifolds.
For a classification of 2-manifolds, see surface: in particular compact connected oriented 2-manifolds are classified by their genus, which is a nonnegative integer.
A classification of 3-manifolds follows "in principle" from the geometrization of 3-manifolds and various recognition results for geometrizable 3-manifolds, such as Mostow rigidity and Sela's algorithm for the isomorphism problem for hyperbolic groups.
The classification of "n"-manifolds for "n" greater than three is known to be impossible, even up to homotopy equivalence. Given any finitely presented group, one can construct a closed 4-manifold having that group as fundamental group. Since there is no algorithm to decide the isomorphism problem for finitely presented groups, there is no algorithm to decide whether two 4-manifolds have the same fundamental group. Since the previously described construction results in a class of 4-manifolds that are homeomorphic if and only if their groups are isomorphic, the homeomorphism problem for 4-manifolds is undecidable. In addition, since even recognizing the trivial group is undecidable, it is not even possible in general to decide whether a manifold has trivial fundamental group, i.e. is simply connected.
Simply connected 4-manifolds have been classified up to homeomorphism by Freedman using the intersection form and Kirby–Siebenmann invariant. Smooth 4-manifold theory is known to be much more complicated, as the exotic smooth structures on R4 demonstrate.
However, the situation becomes more tractable for simply connected smooth manifolds of dimension ≥ 5, where the h-cobordism theorem can be used to reduce the classification to a classification up to homotopy equivalence, and surgery theory can be applied. This has been carried out to provide an explicit classification of simply connected 5-manifolds by Dennis Barden.
Structures on manifolds.
(Pseudo-)Riemannian manifolds.
A Riemannian manifold is a differentiable manifold on which the tangent spaces are equipped with inner products in a differentiable fashion. The inner product structure is given in the form of a symmetric 2-tensor called the Riemannian metric. This metric can be used to interconvert vectors and covectors, and to define a rank 4 Riemann curvature tensor. On a Riemannian manifold one has notions of length, volume, and angle. Any differentiable manifold can be given a Riemannian structure.
A pseudo-Riemannian manifold is a variant of Riemannian manifold where the metric tensor is allowed to have an indefinite signature (as opposed to a positive-definite one). Pseudo-Riemannian manifolds of signature (3, 1) are important in general relativity. Not every differentiable manifold can be given a pseudo-Riemannian structure; there are topological restrictions on doing so.
A Finsler manifold is a generalization of a Riemannian manifold, in which the inner product is replaced with a vector norm; this allows the definition of length, but not angle.
Symplectic manifolds.
A symplectic manifold is a manifold equipped with a closed, nondegenerate 2-form. This condition forces symplectic manifolds to be even-dimensional. Cotangent bundles, which arise as phase spaces in Hamiltonian mechanics, are the motivating example, but many compact manifolds also have symplectic structure. All orientable surfaces embedded in Euclidean space have a symplectic structure, the signed area form on each tangent space induced by the ambient Euclidean inner product. Every Riemann surface is an example of such a surface, and hence a symplectic manifold, when considered as a real manifold.
Lie groups.
A Lie group is "C"∞ manifold that also carries a group structure whose product and inversion operations are smooth as maps of manifolds. These objects arise naturally in describing symmetries.
Generalizations.
The category of smooth manifolds with smooth maps lacks certain desirable properties, and people have tried to generalize smooth manifolds in order to rectify this. Diffeological spaces use a different notion of chart known as a "plot". Frölicher spaces and orbifolds are other attempts.
A rectifiable set generalizes the idea of a piece-wise smooth or rectifiable curve to higher dimensions; however, rectifiable sets are not in general manifolds.
Banach manifolds and Fréchet manifolds, in particular 
are infinite dimensional differentiable manifolds.
Bibliography.
 |url = http://mathworld.wolfram.com/SmoothManifold.html
 |title = Smooth Manifold
 |accessdate = 2008-03-04
 |author =
 |last = Weisstein
 |first = Eric W.

</doc>
<doc id="1424309" url="https://en.wikipedia.org/wiki?curid=1424309" title="Differential equation">
Differential equation

A differential equation is a mathematical equation that relates some function with its derivatives. In applications, the functions usually represent physical quantities, the derivatives represent their rates of change, and the equation defines a relationship between the two. Because such relations are extremely common, differential equations play a prominent role in many disciplines including engineering, physics, economics, and biology.
In pure mathematics, differential equations are studied from several different perspectives, mostly concerned with their solutions—the set of functions that satisfy the equation. Only the simplest differential equations are solvable by explicit formulas; however, some properties of solutions of a given differential equation may be determined without finding their exact form.
If a self-contained formula for the solution is not available, the solution may be numerically approximated using computers. The theory of dynamical systems puts emphasis on qualitative analysis of systems described by differential equations, while many numerical methods have been developed to determine solutions with a given degree of accuracy.
History.
Differential equations first came into existence with the invention of calculus by Newton and Leibniz. In Chapter 2 of his 1671 work "Methodus fluxionum et Serierum Infinitarum", Isaac Newton listed three kinds of differential equations:
He solves these examples and others using infinite series and discusses the non-uniqueness of solutions.
Jacob Bernoulli proposed the Bernoulli differential equation in 1695. This is an ordinary differential equation of the form
for which the following year Leibniz obtained solutions by simplifying it.
Historically, the problem of a vibrating string such as that of a musical instrument was studied by Jean le Rond d'Alembert, Leonhard Euler, Daniel Bernoulli, and Joseph-Louis Lagrange. In 1746, d’Alembert discovered the one-dimensional wave equation, and within ten years Euler discovered the three-dimensional wave equation.
The Euler–Lagrange equation was developed in the 1750s by Euler and Lagrange in connection with their studies of the tautochrone problem. This is the problem of determining a curve on which a weighted particle will fall to a fixed point in a fixed amount of time, independent of the starting point.
Lagrange solved this problem in 1755 and sent the solution to Euler. Both further developed Lagrange's method and applied it to mechanics, which led to the formulation of Lagrangian mechanics.
Fourier published his work on heat flow in "Théorie analytique de la chaleur" (The Analytic Theory of Heat), in which he based his reasoning on Newton's law of cooling, namely, that the flow of heat between two adjacent molecules is proportional to the extremely small difference of their temperatures. Contained in this book was Fourier's proposal of his heat equation for conductive diffusion of heat. This partial differential equation is now taught to every student of mathematical physics.
Example.
For example, in classical mechanics, the motion of a body is described by its position and velocity as the time value varies. Newton's laws allow (given the position, velocity, acceleration and various forces acting on the body) one to express these variables dynamically as a differential equation for the unknown position of the body as a function of time.
In some cases, this differential equation (called an equation of motion) may be solved explicitly.
An example of modelling a real world problem using differential equations is the determination of the velocity of a ball falling through the air, considering only gravity and air resistance. The ball's acceleration towards the ground is the acceleration due to gravity minus the acceleration due to air resistance.
Gravity is considered constant, and air resistance may be modeled as proportional to the ball's velocity. This means that the ball's acceleration, which is a derivative of its velocity, depends on the velocity (and the velocity depends on time). Finding the velocity as a function of time involves solving a differential equation and verifying its validity.
Main topics.
Ordinary differential equations.
An ordinary differential equation (ODE) is an equation containing a function of one independent variable and its derivatives. The term "ordinary" is used in contrast with the term partial differential equation which may be with respect to "more than" one independent variable.
Linear differential equations, which have solutions that can be added and multiplied by coefficients, are well-defined and understood, and exact closed-form solutions are obtained. By contrast, ODEs that lack additive solutions are nonlinear, and solving them is far more intricate, as one can rarely represent them by elementary functions in closed form: Instead, exact and analytic solutions of ODEs are in series or integral form. Graphical and numerical methods, applied by hand or by computer, may approximate solutions of ODEs and perhaps yield useful information, often sufficing in the absence of exact, analytic solutions.
Partial differential equations.
A partial differential equation (PDE) is a differential equation that contains unknown multivariable functions and their partial derivatives. (This is in contrast to ordinary differential equations, which deal with functions of a single variable and their derivatives.) PDEs are used to formulate problems involving functions of several variables, and are either solved by hand, or used to create a relevant computer model.
PDEs can be used to describe a wide variety of phenomena such as sound, heat, electrostatics, electrodynamics, fluid flow, elasticity, or quantum mechanics. These seemingly distinct physical phenomena can be formalised similarly in terms of PDEs. Just as ordinary differential equations often model one-dimensional dynamical systems, partial differential equations often model multidimensional systems. PDEs find their generalisation in stochastic partial differential equations.
Linear and non-linear.
Both ordinary and partial differential equations are broadly classified as linear and nonlinear.
Linear differential equations frequently appear as approximations to nonlinear equations. These approximations are only valid under restricted conditions. For example, the harmonic oscillator equation is an approximation to the nonlinear pendulum equation that is valid for small amplitude oscillations (see below).
Examples.
In the first group of examples, let "u" be an unknown function of "x", and "c" and "ω" are known constants.
In the next group of examples, the unknown function "u" depends on two variables "x" and "t" or "x" and "y".
Existence of solutions.
Solving differential equations is not like solving algebraic equations. Not only are their solutions oftentimes unclear, but whether solutions are unique or exist at all are also notable subjects of interest.
For first order initial value problems, the Peano existence theorem gives one set of circumstances in which a solution exists. Given any point formula_13 in the xy-plane, define some rectangular region formula_14, such that formula_15 and formula_13 is in the interior of formula_14. If we are given a differential equation formula_18 and the condition that formula_19 when formula_20, then there is locally a solution to this problem if formula_21 and formula_22 are both continuous on formula_14. This solution exists on some interval with its center at formula_24. The solution may not be unique. (See Ordinary differential equation for other results.)
However, this only helps us with first order initial value problems. Suppose we had a linear initial value problem of the nth order:
such that
For any nonzero formula_27, if formula_28 and formula_29 are continuous on some interval containing formula_30, formula_31 is unique and exists.
Connection to difference equations.
The theory of differential equations is closely related to the theory of difference equations, in which the coordinates assume only discrete values, and the relationship involves values of the unknown function or functions and values at nearby coordinates. Many methods to compute numerical solutions of differential equations or study the properties of differential equations involve approximation of the solution of a differential equation by the solution of a corresponding difference equation.
Applications.
The study of differential equations is a wide field in pure and applied mathematics, physics, and engineering. All of these disciplines are concerned with the properties of differential equations of various types. Pure mathematics focuses on the existence and uniqueness of solutions, while applied mathematics emphasizes the rigorous justification of the methods for approximating solutions. Differential equations play an important role in modelling virtually every physical, technical, or biological process, from celestial motion, to bridge design, to interactions between neurons. Differential equations such as those used to solve real-life problems may not necessarily be directly solvable, i.e. do not have closed form solutions. Instead, solutions can be approximated using numerical methods.
Many fundamental laws of physics and chemistry can be formulated as differential equations. In biology and economics, differential equations are used to model the behavior of complex systems. The mathematical theory of differential equations first developed together with the sciences where the equations had originated and where the results found application. However, diverse problems, sometimes originating in quite distinct scientific fields, may give rise to identical differential equations. Whenever this happens, mathematical theory behind the equations can be viewed as a unifying principle behind diverse phenomena. As an example, consider propagation of light and sound in the atmosphere, and of waves on the surface of a pond. All of them may be described by the same second-order partial differential equation, the wave equation, which allows us to think of light and sound as forms of waves, much like familiar waves in the water. Conduction of heat, the theory of which was developed by Joseph Fourier, is governed by another second-order partial differential equation, the heat equation. It turns out that many diffusion processes, while seemingly different, are described by the same equation; the Black–Scholes equation in finance is, for instance, related to the heat equation.
Physics.
Classical mechanics.
So long as the force acting on a particle is known, Newton's second law is sufficient to describe the motion of a particle. Once independent relations for each force acting on a particle are available, they can be substituted into Newton's second law to obtain an ordinary differential equation, which is called the "equation of motion".
Electrodynamics.
Maxwell's equations are a set of partial differential equations that, together with the Lorentz force law, form the foundation of classical electrodynamics, classical optics, and electric circuits. These fields in turn underlie modern electrical and communications technologies. Maxwell's equations describe how electric and magnetic fields are generated and altered by each other and by charges and currents. They are named after the Scottish physicist and mathematician James Clerk Maxwell, who published an early form of those equations between 1861 and 1862.
General relativity.
The Einstein field equations (EFE; also known as "Einstein's equations") are a set of ten partial differential equations in Albert Einstein's general theory of relativity which describe the fundamental interaction of gravitation as a result of spacetime being curved by matter and energy. First published by Einstein in 1915 as a tensor equation, the EFE equate local spacetime curvature (expressed by the Einstein tensor) with the local energy and momentum within that spacetime (expressed by the stress–energy tensor).
Quantum mechanics.
In quantum mechanics, the analogue of Newton's law is Schrödinger's equation (a partial differential equation) for a quantum system (usually atoms, molecules, and subatomic particles whether free, bound, or localized). It is not a simple algebraic equation, but in general a linear partial differential equation, describing the time-evolution of the system's wave function (also called a "state function").
Biology.
Predator-prey equations.
The Lotka–Volterra equations, also known as the predator–prey equations, are a pair of first-order, non-linear, differential equations frequently used to describe the dynamics of biological systems in which two species interact, one as a predator and the other as prey.
Chemistry.
The "rate law" or rate equation for a chemical reaction is a differential equation that links the reaction rate with concentrations or pressures of reactants and constant parameters (normally rate coefficients and partial reaction orders). To determine the rate equation for a particular system one combines the reaction rate with a mass balance for the system.

</doc>
<doc id="8625" url="https://en.wikipedia.org/wiki?curid=8625" title="Differential geometry">
Differential geometry

Differential geometry is a mathematical discipline that uses the techniques of differential calculus, integral calculus, linear algebra and multilinear algebra to study problems in geometry. The theory of plane and space curves and surfaces in the three-dimensional Euclidean space formed the basis for development of differential geometry during the 18th century and the 19th century. 
Since the late 19th century, differential geometry has grown into a field concerned more generally with the geometric structures on differentiable manifolds. Differential geometry is closely related to differential topology and the geometric aspects of the theory of differential equations. The differential geometry of surfaces captures many of the key ideas and techniques characteristic of this field.
History of Development.
Differential geometry arose and developed as a result of and in connection to the mathematical analysis of curves and surfaces. Mathematical analysis of curves and surfaces had been developed to answer some of the nagging and unanswered questions that appeared in Calculus, like the reasons for relationships between complex shapes and curves, series and analytic functions. These unanswered questions indicated greater, hidden relationships and symmetries in nature, which the standard methods of analysis could not address.
When curves, surfaces enclosed by curves, and points on curves were found to be quantitatively, and generally, related by mathematical forms the formal study of the nature of curves and surfaces became a field of study in its own right, with Monge's paper in 1795, and especially, with Gauss's publication of his article, titled 'Disquisitiones Generales Circa Superficies Curvas', in "Commentationes Societatis Regiae Scientiarum Gottingesis Recentiores" in 1827.
Initially applied to the Euclidean space, further explorations led to non-Euclidean space, and metric and topological spaces.
Branches of differential geometry.
Riemannian geometry.
Riemannian geometry studies Riemannian manifolds, smooth manifolds with a "Riemannian metric". This is a concept of distance expressed by means of a smooth positive definite symmetric bilinear form defined on the tangent space at each point. Riemannian geometry generalizes Euclidean geometry to spaces that are not necessarily flat, although they still resemble the Euclidean space at each point infinitesimally, i.e. in the first order of approximation. Various concepts based on length, such as the arc length of curves, area of plane regions, and volume of solids all possess natural analogues in Riemannian geometry. The notion of a directional derivative of a function from multivariable calculus is extended in Riemannian geometry to the notion of a covariant derivative of a tensor. Many concepts and techniques of analysis and differential equations have been generalized to the setting of Riemannian manifolds. 
A distance-preserving diffeomorphism between Riemannian manifolds is called an isometry. This notion can also be defined "locally", i.e. for small neighborhoods of points. Any two regular curves are locally isometric. However, the Theorema Egregium of Carl Friedrich Gauss showed that already for surfaces, the existence of a local isometry imposes strong compatibility conditions on their metrics: the Gaussian curvatures at the corresponding points must be the same. In higher dimensions, the Riemann curvature tensor is an important pointwise invariant associated to a Riemannian manifold that measures how close it is to being flat. An important class of Riemannian manifolds is the Riemannian symmetric spaces, whose curvature is not necessarily constant. These are the closest analogues to the "ordinary" plane and space considered in Euclidean and non-Euclidean geometry.
Pseudo-Riemannian geometry.
Pseudo-Riemannian geometry generalizes Riemannian geometry to the case in which the metric tensor need not be positive-definite. 
A special case of this is a Lorentzian manifold, which is the mathematical basis of Einstein's general relativity theory of gravity.
Finsler geometry.
Finsler geometry has the "Finsler manifold " as the main object of study. This is a differential manifold with a Finsler metric, i.e. a Banach norm defined on each tangent space. A Finsler metric is a much more general structure than a Riemannian metric. A Finsler structure on a manifold "M" is a function "F" : T"M" → [0,∞) such that:
Symplectic geometry.
Symplectic geometry is the study of symplectic manifolds. An almost symplectic manifold is a differentiable manifold equipped with a smoothly varying non-degenerate skew-symmetric bilinear form on each tangent space, i.e., a nondegenerate 2-form "ω", called the "symplectic form". A symplectic manifold is an almost symplectic manifold for which the symplectic form "ω" is closed: d"ω" = 0.
A diffeomorphism between two symplectic manifolds which preserves the symplectic form is called a symplectomorphism. Non-degenerate skew-symmetric bilinear forms can only exist on even-dimensional vector spaces, so symplectic manifolds necessarily have even dimension. In dimension 2, a symplectic manifold is just a surface endowed with an area form and a symplectomorphism is an area-preserving diffeomorphism. The phase space of a mechanical system is a symplectic manifold and they made an implicit appearance already in the work of Joseph Louis Lagrange on analytical mechanics and later in Carl Gustav Jacobi's and William Rowan Hamilton's formulations of classical mechanics.
By contrast with Riemannian geometry, where the curvature provides a local invariant of Riemannian manifolds, Darboux's theorem states that all symplectic manifolds are locally isomorphic. The only invariants of a symplectic manifold are global in nature and topological aspects play a prominent role in symplectic geometry. The first result in symplectic topology is probably the Poincaré-Birkhoff theorem, conjectured by Henri Poincaré and then proved by G.D. Birkhoff in 1912. It claims that if an area preserving map of an annulus twists each boundary component in opposite directions, then the map has at least two fixed points.
Contact geometry.
Contact geometry deals with certain manifolds of odd dimension. It is close to symplectic geometry and like the latter, it originated in questions of classical mechanics. A "contact structure" on a (2n + 1) - dimensional manifold "M" is given by a smooth hyperplane field "H" in the tangent bundle that is as far as possible from being associated with the level sets of a differentiable function on "M" (the technical term is "completely nonintegrable tangent hyperplane distribution"). Near each point "p", a hyperplane distribution is determined by a nowhere vanishing 1-form formula_1, which is unique up to multiplication by a nowhere vanishing function:
A local 1-form on "M" is a "contact form" if the restriction of its exterior derivative to "H" is a non-degenerate two-form and thus induces a symplectic structure on "H""p" at each point. If the distribution "H" can be defined by a global one-form formula_1 then this form is contact if and only if the top-dimensional form
is a volume form on M, i.e. does not vanish anywhere. A contact analogue of the Darboux theorem holds: all contact structures on an odd-dimensional manifold are locally isomorphic and can be brought to a certain local normal form by a suitable choice of the coordinate system.
Complex and Kähler geometry.
"Complex differential geometry" is the study of complex manifolds.
An almost complex manifold is a "real" manifold formula_5, endowed with a tensor of type (1, 1), i.e. a vector bundle endomorphism (called an "almost complex structure")
It follows from this definition that an almost complex manifold is even-dimensional. 
An almost complex manifold is called "complex" if formula_8, where formula_9 is a tensor of type (2, 1) related to formula_10, called the Nijenhuis tensor (or sometimes the "torsion").
An almost complex manifold is complex if and only if it admits a holomorphic coordinate atlas.
An "almost Hermitian structure" is given by an almost complex structure "J", along with a Riemannian metric "g", satisfying the compatibility condition
An almost Hermitian structure defines naturally a differential two-form
The following two conditions are equivalent:
where formula_15 is the Levi-Civita connection of formula_16. In this case, formula_17 is called a "Kähler structure", and a "Kähler manifold" is a manifold endowed with a Kähler structure. In particular, a Kähler manifold is both a complex and a symplectic manifold. A large class of Kähler manifolds (the class of Hodge manifolds) is given by all the smooth complex projective varieties.
CR geometry.
CR geometry is the study of the intrinsic geometry of boundaries of domains in complex manifolds.
Differential topology.
Differential topology is the study of (global) geometric invariants without a metric or symplectic form. It starts from the natural operations such as Lie derivative of natural vector bundles and de Rham differential of forms. Beside Lie algebroids, also Courant algebroids start playing a more important role.
Lie groups.
A Lie group is a group in the category of smooth manifolds. Beside the algebraic properties this enjoys also differential geometric properties. The most obvious construction is that of a Lie algebra which is the tangent space at the unit endowed with the Lie bracket between left-invariant vector fields. Beside the structure theory there is also the wide field of representation theory.
Bundles and connections.
The apparatus of vector bundles, principal bundles, and connections on bundles plays an extraordinarily important role in modern differential geometry. A smooth manifold always carries a natural vector bundle, the tangent bundle. Loosely speaking, this structure by itself is sufficient only for developing analysis on the manifold, while doing geometry requires, in addition, some way to relate the tangent spaces at different points, i.e. a notion of parallel transport. An important example is provided by affine connections. For a surface in R3, tangent planes at different points can be identified using a natural path-wise parallelism induced by the ambient Euclidean space, which has a well-known standard definition of metric and parallelism. In Riemannian geometry, the Levi-Civita connection serves a similar purpose. (The Levi-Civita connection defines path-wise parallelism in terms of a given arbitrary Riemannian metric on a manifold.) More generally, differential geometers consider spaces with a vector bundle and an arbitrary affine connection which is not defined in terms of a metric. In physics, the manifold may be the space-time continuum and the bundles and connections are related to various physical fields.
Intrinsic versus extrinsic.
From the beginning and through the middle of the 18th century, differential geometry was studied from the "extrinsic" point of view: curves and surfaces were considered as lying in a Euclidean space of higher dimension (for example a surface in an ambient space of three dimensions). The simplest results are those in the differential geometry of curves and differential geometry of surfaces. Starting with the work of Riemann, the "intrinsic" point of view was developed, in which one cannot speak of moving "outside" the geometric object because it is considered to be given in a free-standing way. The fundamental result here is Gauss's theorema egregium, to the effect that Gaussian curvature is an intrinsic invariant.
The intrinsic point of view is more flexible. For example, it is useful in relativity where space-time cannot naturally be taken as extrinsic (what would be "outside" of it?). However, there is a price to pay in technical complexity: the intrinsic definitions of curvature and connections become much less visually intuitive.
These two points of view can be reconciled, i.e. the extrinsic geometry can be considered as a structure additional to the intrinsic one. (See the Nash embedding theorem.) In the formalism of geometric calculus both extrinsic and intrinsic geometry of a manifold can be characterized by a single bivector-valued one-form called the shape operator.
Applications.
Below are some examples of how differential geometry is applied to other fields of science and mathematics.

</doc>
<doc id="493403" url="https://en.wikipedia.org/wiki?curid=493403" title="Differential geometry of curves">
Differential geometry of curves

Differential geometry of curves is the branch of geometry that deals 
with smooth curves in the plane and in the Euclidean space by methods of differential and integral calculus. 
Starting in antiquity, many concrete curves have been thoroughly investigated using the synthetic approach. Differential geometry takes another path: curves are represented in a parametrized form, and their geometric properties and various quantities associated with them, such as the curvature and the arc length, are expressed via derivatives and integrals using vector calculus. One of the most important tools used to analyze a curve is the Frenet frame, a moving frame that provides a coordinate system at each point of the curve that is "best adapted" to the curve near that point. 
The theory of curves is much simpler and narrower in scope than the theory of surfaces and its higher-dimensional generalizations, because a regular curve in a Euclidean space has no intrinsic geometry. Any regular curve may be parametrized by the arc length (the "natural parametrization") and from the point of view of a bug on the curve that does not know anything about the ambient space, all curves would appear the same. Different space curves are only distinguished by the way in which they bend and twist. Quantitatively, this is measured by the differential-geometric invariants called the "curvature" and the "torsion" of a curve. The fundamental theorem of curves asserts that the knowledge of these invariants completely determines the curve.
Definitions.
Let "n" be a natural number, "r" a natural number or ∞, "I" be a non-empty interval of real numbers and "t" in "I". A vector-valued function
of class "C""r" (i.e. γ is "r" times continuously differentiable) is called a parametric curve of class Cr or a "C""r" parametrization of the curve γ. "t" is called the parameter of the curve γ. γ("I") is called the image of the curve. It is important to distinguish between a curve γ and the image of a curve γ("I") because a given image can be described by several different "C""r" curves.
One may think of the parameter "t" as representing time and the curve γ("t") as the trajectory of a moving particle in space. 
If "I" is a closed interval ["a", "b"], we call γ("a") the starting point and γ("b") the endpoint of the curve γ.
If γ("a") = γ("b"), we say γ is closed or a loop. Furthermore, we call γ a closed Cr-curve if γ("k")(a) = γ("k")("b") for all "k" ≤ "r".
If γ: ("a","b") → R"n" is injective, we call the curve simple.
If γ is a parametric curve which can be locally described as a power series, we call the curve analytic or of class formula_2.
We write -γ to say the curve is traversed in opposite direction.
A "C""k"-curve
is called regular of order m if for any "t" in interval "I"
are linearly independent in R"n". 
In particular, a "C"1-curve "γ" is regular if 
Reparametrization and equivalence relation.
Given the image of a curve one can define several different parameterizations of the curve. Differential geometry aims to describe properties of curves invariant under certain reparametrizations. So we have to define a suitable equivalence relation on the set of all parametric curves. The differential geometric properties of a curve (length, Frenet frame and generalized curvature) are invariant under reparametrization and therefore properties of the equivalence class.The equivalence classes are called Cr curves and are central objects studied in the differential geometry of curves.
Two parametric curves of class "C""r" 
and
are said to be equivalent if there exists a bijective "C""r" map 
such that
and 
γ2 is said to be a reparametrization of γ1. This reparametrization of γ1 defines the equivalence relation on the set of all parametric "C""r" curves. The equivalence class is called a Cr curve.
We can define an even "finer" equivalence relation of oriented Cr curves by requiring φ to be φ‘("t") > 0.
Equivalent "C""r" curves have the same image. And equivalent oriented "C""r" curves even traverse the image in the same direction.
Length and natural parametrization.
The length "l" of a curve "γ" : ["a", "b"] → R"n" of class "C"1 can be defined as
The length of a curve is invariant under reparametrization and therefore a differential geometric property of the curve.
For each regular "C""r"-curve ("r" at least 1) "γ": ["a", "b"] → R"n" we can define a function
Writing
where "t"("s") is the inverse of "s"("t"), we get a reparametrization formula_15 of γ which is called natural, arc-length or unit speed parametrization. The parameter "s"("t") is called the natural parameter of γ.
This parametrization is preferred because the natural parameter "s"("t") traverses the image of γ at unit speed so that
In practice it is often very difficult to calculate the natural parametrization of a curve, but it is useful for theoretical arguments.
For a given parametrized curve γ("t") the natural parametrization is unique up to shift of parameter.
The quantity
is sometimes called the energy or action of the curve; this name is justified because the geodesic equations are the Euler–Lagrange equations of motion for this action.
Frenet frame.
A Frenet frame is a moving reference frame of "n" orthonormal vectors "e""i"("t") which are used to describe a curve locally at each point γ("t"). It is the main tool in the differential geometric treatment of curves as it is far easier and more natural to describe local properties (e.g. curvature, torsion) in terms of a local reference system than using a global one like the Euclidean coordinates.
Given a "C""n"+1-curve γ in R"n" which is regular of order "n" the Frenet frame for the curve is the set of orthonormal vectors
called Frenet vectors. They are constructed from the derivatives of γ("t") using the Gram–Schmidt orthogonalization algorithm with
The real-valued functions χ"i"("t") are called generalized curvatures and are defined as
The Frenet frame and the generalized curvatures are invariant under reparametrization and are therefore differential geometric properties of the curve.
Special Frenet vectors and generalized curvatures.
The first three Frenet vectors and generalized curvatures can be visualized in three-dimensional space. They have additional names and more semantic information attached to them.
Tangent vector.
If a curve γ represents the path of a particle, then the instantaneous velocity of the particle at a given point "P" is expressed by a vector, called the tangent vector to the curve at "P". Mathematically, given a parametrized "C"1 curve γ = γ("t"), for every value "t" = "t"0 of the parameter, the vector 
is the tangent vector at the point "P" = γ("t"0). Generally speaking, the tangent vector may be zero. The magnitude of the tangent vector,
is the speed at the time "t"0. 
The first Frenet vector "e"1("t") is the unit tangent vector in the same direction, defined at each regular point of γ:
If "t" = "s" is the natural parameter then the tangent vector has unit length, so that the formula simplifies:
The unit tangent vector determines the orientation of the curve, or the forward direction, corresponding to the increasing values of the parameter. The unit tangent vector taken as a curve traces the spherical image of the original curve.
Normal or curvature vector.
The normal vector, sometimes called the curvature vector, indicates the deviance of the curve from being a straight line.
It is defined as
Its normalized form, the unit normal vector, is the second Frenet vector "e"2("t") and defined as
The tangent and the normal vector at point "t" define the osculating plane at point "t".
Curvature.
The first generalized curvature χ1("t") is called curvature and measures the deviance of γ from being a straight line relative to the osculating plane. It is defined as
and is called the curvature of γ at point "t".
The reciprocal of the curvature
is called the radius of curvature.
A circle with radius "r" has a constant curvature of 
whereas a line has a curvature of 0.
Binormal vector.
The unit binormal vector is the third Frenet vector "e"3("t"). 
It is always orthogonal to the unit tangent and normal vectors at "t", and is defined as
In 3-dimensional space the equation simplifies to
or to 
That either sign may occur is illustrated by the examples of a right handed helix and a left handed helix.
Torsion.
The second generalized curvature χ2("t") is called torsion and measures the deviance of γ from being a plane curve. Or, in other words, if the torsion is zero, the curve lies completely in the same osculating plane (there is only one osculating plane for every point "t"). It is defined as
and is called the torsion of γ at point "t".
Main theorem of curve theory.
Given "(n-1)" functions:
formula_36
with formula_37, then there exists a unique (up to transformations using the Euclidean group) "C""n"+1-curve γ which is regular of order "n" and has the following properties
where the set
is the Frenet frame for the curve. 
By additionally providing a start "t"0 in "I", a starting point "p"0 in R"n" and an initial positive orthonormal Frenet frame {"e"1, ..., "e""n"-1} with
we can eliminate the Euclidean transformations and get unique curve γ.
Frenet–Serret formulas.
The Frenet–Serret formulas are a set of ordinary differential equations of first order. The solution is the set of Frenet vectors describing the curve specified by the generalized curvature functions χ"i"

</doc>
<doc id="725272" url="https://en.wikipedia.org/wiki?curid=725272" title="Differential structure">
Differential structure

In mathematics, an "n"-dimensional differential structure (or differentiable structure) on a set "M" makes "M" into an "n"-dimensional differential manifold, which is a topological manifold with some additional structure that allows for differential calculus on the manifold. If "M" is already a topological manifold, it is required that the new topology be identical to the existing one.
Definition.
For a natural number "n" and some "k" which may be a non-negative integer or infinity, an n-dimensional "Ck" differential structure is defined using a Ck-atlas, which is a set of bijections called charts between a collection of subsets of "M" (whose union is the whole of "M"), and a set of open subsets of formula_1:
which are "Ck"-compatible (in the sense defined below):
Each such map provides a way in which certain subsets of the manifold may be viewed as being like open subsets of formula_1 but the usefulness of this notion depends on to what extent these notions agree when the domains of two such maps overlap.
Consider two charts:
The intersection of the domains of these two functions is:
and its map by the two chart maps to the two images: 
The transition map between the two charts is the map between the two images of this intersection under the two chart maps.
Two charts formula_11 are Ck-compatible if
are open, and the transition maps
have continuous derivatives of order "k". If "k = 0", we only require that the transition maps are continuous, consequently a "C0"-atlas is simply another way to define a topological manifold. If "k" = ∞, derivatives of all orders must be continuous. A family of "Ck"-compatible charts covering the whole manifold is a "Ck"-atlas defining a "Ck" differential manifold. Two atlases are "Ck"-equivalent if the union of their sets of charts forms a "Ck"-atlas. In particular, a "Ck"-atlas that is "C0"-compatible with a "C0"-atlas that defines a topological manifold is said to determine a "Ck" differential structure on the topological manifold. The "Ck" equivalence classes of such atlases are the distinct Ck differential structures of the manifold. Each distinct differential structure is determined by a unique maximal atlas, which is simply the union of all atlases in the equivalence class.
For simplification of language, without any loss of precision, one might just call a maximal "C""k"−atlas on a given set a "C""k"−manifold. This maximal atlas then uniquely determines both the topology and the underlying set, the latter being the union of the domains of all charts, and the former having the set of all these domains as a basis.
Existence and uniqueness theorems.
For 0 < "k" < ∞ and any "n"−dimensional "C""k"−manifold, the maximal atlas contains a "C"∞−atlas on the same underlying set by a theorem due to Whitney. However, a given maximal "C""k"−atlas contains "distinct" maximal "C"∞−atlases whenever "n" > 0 but there is a "C"∞−diffeomorphism between any two of these distinct "C"∞−atlases. Thus there is only one class of pairwise smoothly diffeomorphic smooth, i.e. "C"∞−structures in a "C""k"−manifold. A bit loosely, one might express this by saying that the smooth structure is (essentially) unique. The case for "k" = 0 is different. Namely, there exist topological manifolds which admit no "C"1−structure, a result proved by , and later explained in the context of Donaldson's theorem (compare Hilbert's fifth problem).
Smooth structures on an orientable manifold are usually counted modulo orientation-preserving smooth homeomorphisms. There then arises the question whether orientation-reversing diffeomorphisms exist. There is an "essentially unique" smooth structure for any topological manifold of dimension smaller than 4. For compact manifolds of dimension greater than 4, there is a finite number of "smooth types", i.e. equivalence classes of pairwise smoothly diffeomorphic smooth structures. In the case of Rn with "n" ≠ 4, the number of these types is one, whereas for "n" = 4, there are uncountably many such types. One refers to these by exotic R4.
Differential structures on spheres of dimension 1 to 20.
The following table lists the number of smooth types of the topological "m"−sphere S"m" for the values of the dimension "m" from 1 up to 20. Spheres with a smooth, i.e. "C"∞−differential structure not smoothly diffeomorphic to the usual one are known as exotic spheres.
It is not currently known how many smooth types the topological 4-sphere S4 has, except that there is at least one. There may be one, a finite number, or an infinite number. The claim that there is just one is known as the "smooth" Poincaré conjecture (see generalized Poincaré conjecture). Most mathematicians believe that this conjecture is false, i.e. that S4 has more than one smooth type. The problem is connected with the existence of more than one smooth type of the topological 4-disk (or 4-ball).
Differential structures on topological manifolds.
As mentioned above, in dimensions smaller than 4, there is only one differential structure for each topological manifold. That was proved by Johann Radon for dimension 1 and 2, and by Edwin E. Moise in dimension 3. By using obstruction theory, Robion Kirby and Laurent Siebenmann were able to show that the number of PL structures for compact topological manifolds of dimension greater than 4 is finite. John Milnor, Michel Kervaire, and Morris Hirsch proved that the number of smooth structures on a compact PL manifold is finite and agrees with the number of differential structures on the sphere for the same dimension (see the book Asselmeyer-Maluga, Brans chapter 7) By combining these results, the number of smooth structures on a compact topological manifold of dimension not equal to 4 is finite. 
Dimension 4 is more complicated. For compact manifolds, results depend on the complexity of the manifold as measured by the second Betti number formula_14. For large Betti numbers formula_15 in a simply connected 4-manifold, one can use a surgery along a knot or link to produce a new differential structure. With the help of this procedure one can produce countably infinite many differential structures. But even for simple spaces like formula_16 one doesn't know the construction of other differential structures. For non-compact 4-manifolds there are many examples like formula_17 having uncountably many differential structures.

</doc>
<doc id="13146531" url="https://en.wikipedia.org/wiki?curid=13146531" title="Differentiation of trigonometric functions">
Differentiation of trigonometric functions

The differentiation of trigonometric functions is the mathematical process of finding the derivative of a trigonometric function, or its rate of change with respect to a variable. Common trigonometric functions include sin("x"), cos("x") and tan("x"). For example, the derivative of "f"("x") = sin("x") is represented as "f" ′("a") = cos("a"). "f" ′("a") is the rate of change of sin("x") at a particular point "a".
All derivatives of circular trigonometric functions can be found using those of sin("x") and cos("x") since they can all be expressed in terms of sine or cosine. The quotient rule is then implemented to differentiate the resulting expression. Finding the derivatives of the inverse trigonometric functions involves using implicit differentiation and the derivatives of regular trigonometric functions.
Proofs of derivatives of trigonometric functions.
Limit of sin(θ)/θ as θ goes to 0.
The diagram on the right shows a circle, centre "O" and radius "r". Let θ be the angle at "O" made by the two radii "OA" and "OB". Since we are considering the limit as θ tends to zero, we may assume that θ is a very small positive number: .
Consider the following three regions of the diagram: "R"1 is the triangle "OAB", "R"2 is the circular sector "OAB", and "R"3 is the triangle "OAC". Clearly:
Using basic trigonometric formulae, the area of the triangle "OAB" is 
The area of the circular sector "OAB" is formula_15, while the area of the triangle "OAC" is given by
Collecting together these three areas gives:
Since we can divide through by ½·r2; this means that the construction and calculations are all independent of the circle's radius. Moreover, since it follows that and we may divide through by a factor of sin(θ), giving:
In the last step we simply took the reciprocal of each of the three terms. Since all three terms are positive this has the effect of reversing the inequities, e.g. if then .
We have seen that if then sin(θ)/θ is "always" less than 1 and, in addition, is "always" greater than cos(θ). Notice that as θ gets closer to 0, so cos(θ) gets closer to 1. Informally: as θ gets smaller, sin(θ)/θ is "squeezed" between 1 and cos(θ), which itself it heading towards 1. It follows that sin(θ)/θ tends to 1 as θ tends to 0 from the positive side.
For the case where θ is a very small negative number: , we use the fact that sine is an odd function:
Limit of (cos(θ)-1)/θ as θ goes to 0.
The last section enables us to calculate this new limit relatively easily. This is done by employing a simple trick. In this calculation, the sign of θ is unimportant. 
The well-known identity tells us that 
Using this, the fact that the limit of a product is the product of the limits, and the result from the last section, we find that:
Limit of tan(θ)/θ as θ goes to 0.
Using the limit for the sine function, the fact that the tangent function is odd, and the fact that the limit of a product is the product of the limits, we find:
Derivative of the sine function.
To calculate the derivative of the sine function sin "θ", we use first principles. By definition:
Using the well-known angle formula , we have:
Using the limits for the sine and cosine functions:
Derivative of the cosine function.
From the definition of the derivative.
To calculate the derivative of the cosine function cos "θ", we use first principles. By definition:
Using the well-known angle formula , we have:
Using the limits for the sine and cosine functions:
From the chain rule.
To compute the derivative of the cosine function from the chain rule, first observe the following two facts:
The first is a known trigonometric identity, and the second is proven above. Using these two facts, we can write the following,
We can differentiate this using the chain rule:
But, from above, we can rewrite this as
But, this is an identity relating to horizontally translating the sine function, and therefore,
Therefore, we have proven that
Derivative of the tangent function.
From the definition of the derivative.
To calculate the derivative of the tangent function tan "θ", we use first principles. By definition:
Using the well-known angle formula , we have:
Using the fact that the limit of a product is the product of the limits:
Using the limit for the tangent function, and the fact that tan "δ" tends to 0 as δ tends to 0:
We see immediately that:
From the quotient rule.
One can also compute the derivative of the tanget function using the quotient rule.
The numerator can be simplified to 1 by the Pythagorean identity, giving us,
Therefore,
Proofs of derivatives of inverse trigonometric functions.
The following derivatives are found by setting a variable "y" equal to the inverse trigonometric function that we wish to take the derivative of. Using implicit differentiation and then solving for "dy"/"dx", the derivative of the inverse function is found in terms of "y". To convert "dy"/"dx" back into being in terms of "x", we can draw a reference triangle on the unit circle, letting "θ" be y. Using the Pythagorean theorem and the definition of the regular trigonometric functions, we can finally express "dy"/"dx" in terms of "x".
Differentiating the inverse sine function.
We let
Where
Then
Using implicit differentiation and solving for dy/dx:
Substituting formula_50 in from above,
Substituting formula_52 in from above,
Differentiating the inverse cosine function.
We let
Where
Then
Using implicit differentiation and solving for dy/dx:
Substituting formula_60 in from above, we get
Substituting formula_62 in from above, we get
Differentiating the inverse tangent function.
We let
Where
Then
Using implicit differentiation and solving for dy/dx:
Left side:
Right side:
Therefore
Substituting formula_72 in from above, we get
Differentiating the inverse cotangent function.
We let
Where
Then
Using implicit differentiation and solving for dy/dx:
Substituting formula_80 into the above,
Substituting formula_82 in from above, we get

</doc>
<doc id="2375824" url="https://en.wikipedia.org/wiki?curid=2375824" title="Dihedral group of order 6">
Dihedral group of order 6

In mathematics, the smallest non-abelian group has 6 elements. It is a dihedral group with notation D3 (or D6; both are used) and the symmetric group of degree 3, with notation S3.
This page illustrates many group concepts using this group as example.
Symmetry groups.
In two dimensions, the group D3 is the symmetry group of an equilateral triangle. In contrast with the case of a square or other polygon, all permutations of the vertices can be achieved by rotation and flipping over (or reflecting).
In three dimensions, there are two different symmetry groups which are algebraically the group D3:
Permutations of a set of three objects.
Consider three colored blocks (red, green, and blue), initially placed in the order RGB. Let "a" be the action "swap the first block and the second block", and let "b" be the action "swap the second block and the third block".
In multiplicative form, we traditionally write "xy" for the combined action "first do "y", then do "x""; so that "ab" is the action , i.e., "take the last block and move it to the front".
If we write "e" for "leave the blocks as they are" (the identity action), then we can write the six permutations of the set of three blocks as the following actions:
The notation in brackets is the cycle notation.
Note that the action "aa" has the effect , leaving the blocks as they were; so we can write .
Similarly,
so each of the above actions has an inverse.
By inspection, we can also determine associativity and closure; note for example that
The group is non-abelian since, for example, . Since it is built up from the basic actions "a" and "b", we say that the set "generates" it.
The group has presentation
where "a" and "b" are swaps and "r = ab" is a cyclic permutation.
Summary of group operations.
With the generators "a" and "b", we define the additional shorthands , and . In the form of a Cayley table, the group operations now read:
Note that non-equal non-identity elements only commute if they are each other's inverse. Therefore the group is centerless.
Conjugacy classes.
We can easily distinguish three kinds of permutations of the three blocks, called conjugacy classes of the group:
For example (RG) and (RB) are both of the form ("x" "y"); a permutation of the letters R, G, and B (namely (GB)) changes the notation (RG) into (RB). Therefore, if we apply (GB), then (RB), and then the inverse of (GB), which is also (GB), the resulting permutation is (RG).
Note that conjugate group elements always have the same order, but for groups in general group elements that have the same order need not be conjugate.
Subgroups.
From Lagrange's theorem we know that any non-trivial subgroup has order 2 or 3. In fact the two cyclic permutations of all three blocks, with the identity, form a subgroup of order 3, index 2, and the swaps of two blocks, each with the identity, form three subgroups of order 2, index 3. The existence of subgroups of order 2 and 3 is also a consequence of Cauchy's theorem.
The first-mentioned is the alternating group A3.
The left cosets and the right cosets of A3 coincide (as they do for any subgroup of index 2) and consist of A3 and the set of three swaps }.
The left cosets of are:
The right cosets of are:
Thus A3 is normal, and the other three non-trivial subgroups are not. The quotient group is isomorphic with "C"2.
formula_5, a semidirect product, where "H" is a subgroup of two elements: () and one of the three swaps. This decomposition is also a consequence (particular case) of the Schur–Zassenhaus theorem.
In terms of permutations the two group elements of are the set of even permutations and the set of odd permutations.
If the original group is that generated by a 120°-rotation of a plane about a point, and reflection with respect to a line through that point, then the quotient group has the two elements which can be described as the subsets "just rotate (or do nothing)" and "take a mirror image".
Note that for the symmetry group of a "square", an uneven permutation of vertices does "not" correspond to taking a mirror image, but to operations not allowed for "rectangles", i.e. 90° rotation and applying a diagonal axis of reflection.
Semidirect products.
formula_6 is formula_7 if both "φ"(0) and "φ"(1) are the identity.
The semidirect product is isomorphic to the dihedral group of order 6 if "φ"(0) is the identity and "φ"(1) is the non-trivial automorphism of C3, which inverses the elements.
Thus we get:
for all "n"1, "n"2 in C3 and "h"2 in C2.
In a Cayley table:
Note that for the second digit we essentially have a 2×2 table, with 3×3 equal values for each of these 4 cells. For the first digit the left half of the table is the same as the right half, but the top half is different from the bottom half.
For the "direct" product the table is the same except that the first digits of the bottom half of the table are the same as in the top half.
Group action.
Consider "D"3 in the geometrical way, as symmetry group of isometries of the plane, and consider the corresponding group action on a set of 30 evenly spaced points on a circle, numbered 0 to 29, with 0 at one of the reflexion axes.
This section illustrates group action concepts for this case.
The action of "G" on "X" is called 
Orbits and stabilizers.
The orbit of a point "x" in "X" is the set of elements of "X" to which "x" can be moved by the elements of "G". The orbit of "x" is denoted by "Gx":
The orbits are and The points within an orbit are "equivalent". If a symmetry group applies for a pattern, then within each orbit the color is the same.
The set of all orbits of "X" under the action of "G" is written as .
If "Y" is a subset of "X", we write "GY" for the set We call the subset "Y" "invariant under G" if (which is equivalent to . In that case, "G" also operates on "Y". The subset "Y" is called "fixed under G" if for all "g" in "G" and all "y" in "Y". The union of e.g. two orbits is invariant under "G", but not fixed.
For every "x" in "X", we define the stabilizer subgroup of "x" (also called the isotropy group or little group) as the set of all elements in "G" that fix "x":
If "x" is a reflection point , its stabilizer is the group of order two containing the identity and the reflection in "x". In other cases the stabilizer is the trivial group.
For a fixed "x" in "X", consider the map from "G" to "X" given by . The image of this map is the orbit of "x" and the coimage is the set of all left cosets of "Gx". The standard quotient theorem of set theory then gives a natural bijection between and "Gx". Specifically, the bijection is given by . This result is known as the orbit-stabilizer theorem. In the two cases of a small orbit, the stabilizer is non-trivial.
If two elements "x" and "y" belong to the same orbit, then their stabilizer subgroups, "G""x" and "G""y", are isomorphic. More precisely: if "y" = "g" · "x", then "G""y" = "gG""x" "g"−1. In the example this applies e.g. for 5 and 25, both reflection points. Reflection about 25 corresponds to a rotation of 10, reflection about 5, and rotation of −10.
A result closely related to the orbit-stabilizer theorem is Burnside's lemma:
where "X""g" is the set of points fixed by "g". I.e., the number of orbits is equal to the average number of points fixed per group element.
For the identity all 30 points are fixed, for the two rotations none, and for the three reflections two each: and Thus, the average is six, the number of orbits.
Representation theory.
Up to isomorphism, this group has three irreducible complex unitary representations, which we will call formula_11 (the trivial representation), formula_12 and formula_13, where the subscript indicates the dimension. By its definition as a permutation group over the set with three elements, the group has a representation on formula_14 by permuting the entries of the vector, the fundamental representation. This representation is not irreducible, as it decomposes as a direct sum of formula_11 and formula_13. formula_11 appears as the subspace of vectors of the form formula_18 and formula_13 is the representation on its orthogonal complement, which are vectors of the form formula_20.
The nontrivial one-dimensional representation formula_12 arises through the groups formula_22 grading: The action is multiplication by the sign of the permutation of the group element. Every finite group has such a representation since it is a subgroup of a cyclic group by its regular action. Counting the square dimensions of the representations (formula_23, the order of the group), we see these must be all of the irreducible representations.
A 2-dimensional irreducible linear representation yields a 1-dimensional projective representation (i.e., an action on the projective line, an embedding in the Möbius group ), as elliptic transforms. This can be represented by matrices with entries 0 and ±1 (here written as fractional linear transformations), known as the anharmonic group:
and thus descends to a representation over any field, which is always faithful/injective (since no two terms differ only by only a sign). Over the field with two elements, the projective line has only 3 points, and this is thus the exceptional isomorphism formula_27 In characteristic 3, this embedding stabilizes the point formula_28 since formula_29 (in characteristic greater than 3 these points are distinct and permuted, and are the orbit of the harmonic cross-ratio). Over the field with three elements, the projective line has 4 elements, and since is isomorphic to the symmetric group on 4 elements, S4, the resulting embedding formula_30 equals the stabilizer of the point formula_31.

</doc>
<doc id="564961" url="https://en.wikipedia.org/wiki?curid=564961" title="Dilaton">
Dilaton

In particle physics, the dilaton is a hypothetical particle that appears in theories with extra dimensions when the volume of the compactified dimensions is allowed to vary. It appears for instance in Kaluza–Klein theory's compactifications of extra dimensions. It is a particle of a scalar field Φ, a scalar field that always comes with gravity. For comparison, in standard general relativity, Newton's constant, or equivalently the Planck mass is a constant. If this constant is promoted to a dynamical field, the result is the dilaton.
In Kaluza–Klein theories, after dimensional reduction, the effective Planck mass varies as some power of the volume of compactified space. This is why volume can turn out as a dilaton in the lower-dimensional effective theory.
Although string theory naturally incorporates Kaluza–Klein theory (which first introduced the dilaton), perturbative string theories, such as type I string theory, type II string theory and heterotic string theory, already contain the dilaton in the maximal number of 10 dimensions. However, on the other hand, M-theory in 11 dimensions does not include the dilaton in its spectrum unless it is compactified. In fact, the dilaton in type IIA string theory is actually the radion of M-theory compactified over a circle, while the dilaton in string theory is the radion for the Hořava–Witten model. (For more on the M-theory origin of the dilaton, see .)
In string theory, there is also a dilaton in the worldsheet CFT(Conformal field theory). The exponential of its vacuum expectation value determines the coupling constant "g", as for compact worldsheets by the Gauss–Bonnet theorem and the Euler characteristic , where "g" is the genus that counts the number of handles and thus the number of loops or string interactions described by a specific worldsheet.
Therefore the coupling constant is a dynamical variable in string theory, unlike the case of quantum field theory where it is constant. As long as supersymmetry is unbroken, such scalar fields can take arbitrary values (they are moduli). However, supersymmetry breaking usually creates a potential energy for the scalar fields and the scalar fields localize near a minimum whose position should in principle be calculable in string theory.
The dilaton acts like a Brans–Dicke scalar, with the effective Planck scale depending upon "both" the string scale and the dilaton field.
In supersymmetry, the superpartner of the dilaton is called the dilatino, and the dilaton combines with the axion to form a complex scalar field.
Dilaton action.
The dilaton-gravity action is
This is more general than Brans–Dicke in vacuum in that we have a dilaton potential.

</doc>
<doc id="8398" url="https://en.wikipedia.org/wiki?curid=8398" title="Dimension">
Dimension

In physics and mathematics, the dimension of a mathematical space (or object) is informally defined as the minimum number of coordinates needed to specify any point within it. Thus a line has a dimension of one because only one coordinate is needed to specify a point on itfor example, the point at 5 on a number line. A surface such as a plane or the surface of a cylinder or sphere has a dimension of two because two coordinates are needed to specify a point on itfor example, both a latitude and longitude are required to locate a point on the surface of a sphere. The inside of a cube, a cylinder or a sphere is three-dimensional because three coordinates are needed to locate a point within these spaces.
In classical mechanics, space and time are different categories and refer to absolute space and time. That conception of the world is a four-dimensional space but not the one that was found necessary to describe electromagnetism. The four dimensions of spacetime consist of events that are not absolutely defined spatially and temporally, but rather are known relative to the motion of an observer. Minkowski space first approximates the universe without gravity; the pseudo-Riemannian manifolds of general relativity describe spacetime with matter and gravity. Ten dimensions are used to describe string theory, and the state-space of quantum mechanics is an infinite-dimensional function space.
The concept of dimension is not restricted to physical objects. High-dimensional spaces frequently occur in mathematics and the sciences. They may be parameter spaces or configuration spaces such as in Lagrangian or Hamiltonian mechanics; these are abstract spaces, independent of the physical space we live in.
In mathematics.
In mathematics, the dimension of an object is an intrinsic property independent of the space in which the object is embedded. For example, a point on the unit circle in the plane can be specified by two Cartesian coordinates, but a single polar coordinate (the angle) would be sufficient, so the circle is 1-dimensional even though it exists in the 2-dimensional plane. This "intrinsic" notion of dimension is one of the chief ways the mathematical notion of dimension differs from its common usages.
The dimension of Euclidean -space is . When trying to generalize to other types of spaces, one is faced with the question "what makes -dimensional?" One answer is that to cover a fixed ball in by small balls of radius , one needs on the order of such small balls. This observation leads to the definition of the Minkowski dimension and its more sophisticated variant, the Hausdorff dimension, but there are also other answers to that question. For example, the boundary of a ball in looks locally like and this leads to the notion of the inductive dimension. While these notions agree on , they turn out to be different when one looks at more general spaces.
A tesseract is an example of a four-dimensional object. Whereas outside mathematics the use of the term "dimension" is as in: "A tesseract "has four dimensions"", mathematicians usually express this as: "The tesseract "has dimension 4"", or: "The dimension of the tesseract "is" 4".
Although the notion of higher dimensions goes back to René Descartes, substantial development of a higher-dimensional geometry only began in the 19th century, via the work of Arthur Cayley, William Rowan Hamilton, Ludwig Schläfli and Bernhard Riemann. Riemann's 1854 Habilitationsschrift, Schläfli's 1852 "Theorie der vielfachen Kontinuität", Hamilton's 1843 discovery of the quaternions and the construction of the Cayley algebra marked the beginning of higher-dimensional geometry.
The rest of this section examines some of the more important mathematical definitions of the dimensions.
Complex dimension.
A complex number ("x" + "iy") has a real part "x" and an imaginary part "iy" whose magnitude is "y". A single complex coordinate system may be applied to an object having two real dimensions, for example an ordinary two-dimensional spherical surface, when given a complex metric, becomes a Riemann sphere of one complex dimension. Complex dimensions appear in the study of complex manifolds and algebraic varieties.
Dimension of a vector space.
The dimension of a vector space is the number of vectors in any basis for the space, i.e. the number of coordinates necessary to specify any vector. This notion of dimension (the cardinality of a basis) is often referred to as the "Hamel dimension" or "algebraic dimension" to distinguish it from other notions of dimension.
Manifolds.
A connected topological manifold is locally homeomorphic to Euclidean -space, and the number is called the manifold's dimension. One can show that this yields a uniquely defined dimension for every connected topological manifold.
For connected differentiable manifolds, the dimension is also the dimension of the tangent vector space at any point.
In geometric topology, the theory of manifolds is characterized by the way dimensions 1 and 2 are relatively elementary, the high-dimensional cases are simplified by having extra space in which to "work"; and the cases and are in some senses the most difficult. This state of affairs was highly marked in the various cases of the Poincaré conjecture, where four different proof methods are applied.
Varieties.
The dimension of an algebraic variety may be defined in various equivalent ways. The most intuitive way is probably the dimension of the tangent space at any regular point. Another intuitive way is to define the dimension as the number of hyperplanes that are needed in order to have an intersection with the variety that is reduced to a finite number of points (dimension zero). This definition is based on the fact that the intersection of a variety with a hyperplane reduces the dimension by one unless if the hyperplane contains the variety.
An algebraic set being a finite union of algebraic varieties, its dimension is the maximum of the dimensions of its components. It is equal to the maximal length of the chains formula_1 of sub-varieties of the given algebraic set (the length of such a chain is the number of "formula_2").
Each variety can be considered as an algebraic stack, and its dimension as variety agrees with its dimension as stack. There are however many stacks which do not correspond to varieties, and some of these have negative dimension. Specifically, if "V" is a variety of dimension "m" and "G" is an algebraic group of dimension "n" acting on "V", then the quotient stack ["V"/"G"] has dimension "m"−"n".
Krull dimension.
The Krull dimension of a commutative ring is the maximal length of chains of prime ideals in it, a chain of length "n" being a sequence formula_3 of prime ideals related by inclusion. It is strongly related to the dimension of an algebraic variety, because of the natural correspondence between sub-varieties and prime ideals of the ring of the polynomials on the variety.
For an algebra over a field, the dimension as vector space is finite if and only if its Krull dimension is 0.
Lebesgue covering dimension.
For any normal topological space , the Lebesgue covering dimension of is defined to be n if "n" is the smallest integer for which the following holds: any open cover has an open refinement (a second open cover where each element is a subset of an element in the first cover) such that no point is included in more than elements. In this case dim . For a manifold, this coincides with the dimension mentioned above. If no such integer exists, then the dimension of is said to be infinite, and one writes dim . Moreover, has dimension −1, i.e. dim if and only if is empty. This definition of covering dimension can be extended from the class of normal spaces to all Tychonoff spaces merely by replacing the term "open" in the definition by the term "functionally open".
Inductive dimension.
An inductive definition of dimension can be created as follows. Consider a discrete set of points (such as a finite collection of points) to be 0-dimensional. By dragging a 0-dimensional object in some direction, one obtains a 1-dimensional object. By dragging a 1-dimensional object in a "new direction", one obtains a 2-dimensional object. In general one obtains an ()-dimensional object by dragging an -dimensional object in a "new" direction.
The inductive dimension of a topological space may refer to the "small inductive dimension" or the "large inductive dimension", and is based on the analogy that balls have -dimensional boundaries, permitting an inductive definition based on the dimension of the boundaries of open sets.
Hausdorff dimension.
For structurally complicated sets, especially fractals, the Hausdorff dimension is useful. The Hausdorff dimension is defined for all metric spaces and, unlike the dimensions considered above, can also attain non-integer real values. The box dimension or Minkowski dimension is a variant of the same idea. In general, there exist more definitions of fractal dimensions that work for highly irregular sets and attain non-integer positive real values. Fractals have been found useful to describe many natural objects and phenomena.
Hilbert spaces.
Every Hilbert space admits an orthonormal basis, and any two such bases for a particular space have the same cardinality. This cardinality is called the dimension of the Hilbert space. This dimension is finite if and only if the space's Hamel dimension is finite, and in this case the above dimensions coincide.
In physics.
Spatial dimensions.
Classical physics theories describe three physical dimensions: from a particular point in space, the basic directions in which we can move are up/down, left/right, and forward/backward. Movement in any other direction can be expressed in terms of just these three. Moving down is the same as moving up a negative distance. Moving diagonally upward and forward is just as the name of the direction implies; "i.e.", moving in a linear combination of up and forward. In its simplest form: a line describes one dimension, a plane describes two dimensions, and a cube describes three dimensions. (See Space and Cartesian coordinate system.)
Time.
A temporal dimension is a dimension of time. Time is often referred to as the "fourth dimension" for this reason, but that is not to imply that it is a spatial dimension. A temporal dimension is one way to measure physical change. It is perceived differently from the three spatial dimensions in that there is only one of it, and that we cannot move freely in time but subjectively move in one direction.
The equations used in physics to model reality do not treat time in the same way that humans commonly perceive it. The equations of classical mechanics are symmetric with respect to time, and equations of quantum mechanics are typically symmetric if both time and other quantities (such as charge and parity) are reversed. In these models, the perception of time flowing in one direction is an artifact of the laws of thermodynamics (we perceive time as flowing in the direction of increasing entropy).
The best-known treatment of time as a dimension is Poincaré and Einstein's special relativity (and extended to general relativity), which treats perceived space and time as components of a four-dimensional manifold, known as spacetime, and in the special, flat case as Minkowski space.
Additional dimensions.
In physics, three dimensions of space and one of time is the accepted norm. However, there are theories that attempt to unify the four fundamental forces by introducing more dimensions. Most notably, superstring theory requires 10 spacetime dimensions, and originates from a more fundamental 11-dimensional theory tentatively called M-theory which subsumes five previously distinct superstring theories. To date, no experimental or observational evidence is available to confirm the existence of these extra dimensions. If extra dimensions exist, they must be hidden from us by some physical mechanism. One well-studied possibility is that the extra dimensions may be "curled up" at such tiny scales as to be effectively invisible to current experiments. Limits on the size and other properties of extra dimensions are set by particle experiments such as those at the Large Hadron Collider.
At the level of quantum field theory, Kaluza–Klein theory unifies gravity with gauge interactions, based on the realization that gravity propagating in small, compact extra dimensions is equivalent to gauge interactions at long distances. In particular when the geometry of the extra dimensions is trivial, it reproduces electromagnetism. However at sufficiently high energies or short distances, this setup still suffers from the same pathologies that famously obstruct direct attempts to describe quantum gravity. Therefore, these models still require a UV completion, of the kind that string theory is intended to provide. In particular, superstring theory requires six compact dimensions forming a Calabi–Yau manifold. Thus Kaluza-Klein theory may be considered either as an incomplete description on its own, or as a subset of string theory model building.
In addition to small and curled up extra dimensions, there may be extra dimensions that instead aren't apparent because the matter associated with our visible universe is localized on a subspace. Thus the extra dimensions need not be small and compact but may be large extra dimensions. D-branes are dynamical extended objects of various dimensionalities predicted by string theory that could play this role. They have the property that open string excitations, which are associated with gauge interactions, are confined to the brane by their endpoints, whereas the closed strings that mediate the gravitational interaction are free to propagate into the whole spacetime, or "the bulk". This could be related to why gravity is exponentially weaker than the other forces, as it effectively dilutes itself as it propagates into a higher-dimensional volume.
Some aspects of brane physics have been applied to cosmology. For example, brane gas cosmology attempts to explain why there are three dimensions of space using topological and thermodynamic considerations. According to this idea it would be because three is the largest number of spatial dimensions where strings can generically intersect. If initially there are lots of windings of strings around compact dimensions, space could only expand to macroscopic sizes once these windings are eliminated, which requires oppositely wound strings to find each other and annihilate. But strings can only find each other to annihilate at a meaningful rate in three dimensions, so it follows that only three dimensions of space are allowed to grow large given this kind of initial configuration.
Extra dimensions are said to be universal if all fields are equally free to propagate within them.
Networks and dimension.
Some complex networks are characterized by fractal dimensions. The concept of dimension can be generalized to include networks embedded in space. The dimension characterize their spatial constraints.
In literature.
Science fiction texts often mention the concept of "dimension" when referring to parallel or alternate universes or other imagined planes of existence. This usage is derived from the idea that to travel to parallel/alternate universes/planes of existence one must travel in a direction/dimension besides the standard ones. In effect, the other universes/planes are just a small distance away from our own, but the distance is in a fourth (or higher) spatial (or non-spatial) dimension, not the standard ones.
One of the most heralded science fiction stories regarding true geometric dimensionality, and often recommended as a starting point for those just starting to investigate such matters, is the 1884 novella "Flatland" by Edwin A. Abbott. Isaac Asimov, in his foreword to the Signet Classics 1984 edition, described "Flatland" as "The best introduction one can find into the manner of perceiving dimensions."
The idea of other dimensions was incorporated into many early science fiction stories, appearing prominently, for example, in Miles J. Breuer's "The Appendix and the Spectacles" (1928) and Murray Leinster's "The Fifth-Dimension Catapult" (1931); and appeared irregularly in science fiction by the 1940s. Classic stories involving other dimensions include Robert A. Heinlein's "—And He Built a Crooked House" (1941), in which a California architect designs a house based on a three-dimensional projection of a tesseract; and Alan E. Nourse's "Tiger by the Tail" and "The Universe Between" (both 1951). Another reference is Madeleine L'Engle's novel "A Wrinkle In Time" (1962), which uses the fifth dimension as a way for "tesseracting the universe" or "folding" space in order to move across it quickly. The fourth and fifth dimensions were also a key component of the book "The Boy Who Reversed Himself" by William Sleator.
In philosophy.
Immanuel Kant, in 1783, wrote: "That everywhere space (which is not itself the boundary of another space) has three dimensions and that space in general cannot have more dimensions is based on the proposition that not more than three lines can intersect at right angles in one point. This proposition cannot at all be shown from concepts, but rests immediately on intuition and indeed on pure intuition "a priori" because it is apodictically (demonstrably) certain."
"Space has Four Dimensions" is a short story published in 1846 by German philosopher and experimental psychologist Gustav Fechner under the pseudonym "Dr. Mises". The protagonist in the tale is a shadow who is aware of and able to communicate with other shadows, but who is trapped on a two-dimensional surface. According to Fechner, this "shadow-man" would conceive of the third dimension as being one of time. The story bears a strong similarity to the "Allegory of the Cave" presented in Plato's "The Republic" (c. 380 BC).
Simon Newcomb wrote an article for the "Bulletin of the American Mathematical Society" in 1898 entitled "The Philosophy of Hyperspace". Linda Dalrymple Henderson coined the term "hyperspace philosophy", used to describe writing that uses higher dimensions to explore metaphysical themes, in her 1983 thesis about the fourth dimension in early-twentieth-century art. Examples of "hyperspace philosophers" include Charles Howard Hinton, the first writer, in 1888, to use the word "tesseract"; and the Russian esotericist P. D. Ouspensky.
See also.
Topics by dimension.
Zero
One
Two
Three
Four
Higher dimensionsin mathematics
Infinite

</doc>
<doc id="38267" url="https://en.wikipedia.org/wiki?curid=38267" title="Dimension (vector space)">
Dimension (vector space)

In mathematics, the dimension of a vector space "V" is the cardinality (i.e. the number of vectors) of a basis of "V" over its base field.
For every vector space there exists a basis, and all bases of a vector space have equal cardinality; as a result, the dimension of a vector space is uniquely defined. We say "V" is finite-dimensional if the dimension of "V" is finite, and infinite-dimensional if its dimension is infinite.
The dimension of the vector space "V" over the field "F" can be written as dim"F"("V") or as [V : F], read "dimension of "V" over "F"". When "F" can be inferred from context, dim("V") is typically written.
Examples.
The vector space R3 has 
as a basis, and therefore we have dimR(R3) = 3. More generally, dimR(R"n") = "n", and even more generally, dim"F"("F""n") = "n" for any field "F".
The complex numbers C are both a real and complex vector space; we have dimR(C) = 2 and dimC(C) = 1. So the dimension depends on the base field.
The only vector space with dimension 0 is {0}, the vector space consisting only of its zero element.
Facts.
If "W" is a linear subspace of "V", then dim("W") ≤ dim("V").
To show that two finite-dimensional vector spaces are equal, one often uses the following criterion: if "V" is a finite-dimensional vector space and "W" is a linear subspace of "V" with dim("W") = dim("V"), then "W" = "V".
R"n" has the standard basis {e1, ..., e"n"}, where e"i" is the "i"-th column of the corresponding identity matrix. Therefore R"n" 
has dimension "n".
Any two vector spaces over "F" having the same dimension are isomorphic. Any bijective map between their bases can be uniquely extended to a bijective linear map between the vector spaces. If "B" is some set, a vector space with dimension |"B"| over "F" can be constructed as follows: take the set "F"("B") of all functions "f" : "B" → "F" such that "f"("b") = 0 for all but finitely many "b" in "B". These functions can be added and multiplied with elements of "F", and we obtain the desired "F"-vector space. 
An important result about dimensions is given by the rank–nullity theorem for linear maps.
If "F"/"K" is a field extension, then "F" is in particular a vector space over "K". Furthermore, every "F"-vector space "V" is also a "K"-vector space. The dimensions are related by the formula
In particular, every complex vector space of dimension "n" is a real vector space of dimension 2"n".
Some simple formulae relate the dimension of a vector space with the cardinality of the base field and the cardinality of the space itself.
If "V" is a vector space over a field "F" then, denoting the dimension of "V" by dim "V", we have:
Generalizations.
One can see a vector space as a particular case of a matroid, and in the latter there is a well-defined notion of dimension. The length of a module and the rank of an abelian group both have several properties similar to the dimension of vector spaces.
The Krull dimension of a commutative ring, named after Wolfgang Krull (1899–1971), is defined to be the maximal number of strict inclusions in an increasing chain of prime ideals in the ring.
Trace.
The dimension of a vector space may alternatively be characterized as the trace of the identity operator. For instance, formula_2 This appears to be a circular definition, but it allows useful generalizations.
Firstly, it allows one to define a notion of dimension when one has a trace but no natural sense of basis. For example, one may have an algebra "A" with maps formula_3 (the inclusion of scalars, called the "unit") and a map formula_4 (corresponding to trace, called the "counit"). The composition formula_5 is a scalar (being a linear operator on a 1-dimensional space) corresponds to "trace of identity", and gives a notion of dimension for an abstract algebra. In practice, in bialgebras one requires that this map be the identity, which can be obtained by normalizing the counit by dividing by dimension (formula_6), so in these cases the normalizing constant corresponds to dimension.
Alternatively, one may be able to take the trace of operators on an infinite-dimensional space; in this case a (finite) trace is defined, even though no (finite) dimension exists, and gives a notion of "dimension of the operator". These fall under the rubric of "trace class operators" on a Hilbert space, or more generally nuclear operators on a Banach space.
A subtler generalization is to consider the trace of a "family" of operators as a kind of "twisted" dimension. This occurs significantly in representation theory, where the character of a representation is the trace of the representation, hence a scalar-valued function on a group formula_7 whose value on the identity formula_8 is the dimension of the representation, as a representation sends the identity in the group to the identity matrix: formula_9 One can view the other values formula_10 of the character as "twisted" dimensions, and find analogs or generalizations of statements about dimensions to statements about characters or representations. A sophisticated example of this occurs in the theory of monstrous moonshine: the "j"-invariant is the graded dimension of an infinite-dimensional graded representation of the Monster group, and replacing the dimension with the character gives the McKay–Thompson series for each element of the Monster group.

</doc>
<doc id="679369" url="https://en.wikipedia.org/wiki?curid=679369" title="Dimension theory">
Dimension theory

In mathematics, dimension theory is a branch of general topology dealing with dimensional invariants of topological spaces.
Constructions.
Inductive dimension.
The inductive dimension of a topological space "X" is either of two values, the small inductive dimension ind("X") or the large inductive dimension Ind("X"). These are based on the observation that, in "n"-dimensional Euclidean space "R""n", ("n" − 1)-dimensional spheres (that is, the boundaries of "n"-dimensional balls) have dimension "n" − 1. Therefore it should be possible to define the dimension of a space inductively in terms of the dimensions of the boundaries of suitable open sets.
Lebesgue covering dimension.
An open cover of a topological space "X" is a family of open sets whose union is "X". The "ply" of a cover is the smallest number "n" (if it exists) such that each point of the space belongs to at most "n" sets in the cover. A refinement of a cover "C" is another cover, each of whose sets is a subset of a set in "C"; its ply may be smaller than, or possibly larger than, the ply of "C".
The Lebesgue covering dimension of a topological space "X" is defined to be the minimum value of "n", such that every finite open cover "C" of "X" has a refinement with ply at most "n" + 1. If no such minimal "n" exists, the space is said to be of infinite covering dimension.
As a special case, 
a topological space is zero-dimensional with respect to the covering dimension if every open cover of the space has a refinement consisting of disjoint open sets so that any point in the space is contained in exactly one open set of this refinement.

</doc>
<doc id="8267" url="https://en.wikipedia.org/wiki?curid=8267" title="Dimensional analysis">
Dimensional analysis

In engineering and science, dimensional analysis is the analysis of the relationships between different physical quantities by identifying their fundamental dimensions (such as length, mass, time, and electric charge) and units of measure (such as miles vs. kilometers, or pounds vs. kilograms vs. grams) and tracking these dimensions as calculations or comparisons are performed. Converting from one dimensional unit to another is often somewhat complex. Dimensional analysis, or more specifically the factor-label method, also known as the unit-factor method, is a widely used technique for such conversions using the rules of algebra.
The concept of physical dimension was introduced by Joseph Fourier in 1822. Physical quantities that are commensurable have the same dimension; if they have different dimensions, they are incommensurable. For example, it is meaningless to ask whether a kilogram is less, the same, or more than an hour.
Any physically meaningful equation (and likewise any inequality and inequation) will have the same dimensions on the left and right sides, a property known as "dimensional homogeneity". Checking this is a common application of dimensional analysis. Dimensional analysis is also routinely used as a check on the plausibility of derived equations and computations. It is generally used to categorize types of physical quantities and units based on their relationship to or dependence on other units.
Concrete numbers and base units.
Many parameters and measurements in the physical sciences and engineering are expressed as a concrete number – a numerical quantity and a corresponding dimensional unit. Often a quantity is expressed in terms of several other quantities; for example, speed is a combination of length and time, e.g. 60 miles per hour or 1.4 km per second. Compound relations with "per" are expressed with division, e.g. 60 mi/1 h. Other relations can involve multiplication (often shown with · or juxtaposition), powers (like m2 for square meters), or combinations thereof.
A unit of measure that is in a conventionally chosen subset of a given system of units, where no unit in the set can be expressed in terms of the others, is known as a base unit. For example, units for length and time are normally chosen as base units. Units for volume, however, can be factored into the base units of length (m3), thus being derived or compound units.
Sometimes the names of units obscure that they are derived units. For example, an ampere is a unit of electric current, which is equivalent to electric charge per unit time and is measured in coulombs (a unit of electrical charge) per second, so . One newton is 1 kg⋅m/s2.
Percentages and derivatives.
Percentages are dimensionless quantities, since they are ratios of two quantities with the same dimensions. In other words, the % sign can be read as "1/100", since .
Derivatives with respect to a quantity add the dimensions of the variable one is differentiating with respect to on the denominator. Thus:
In economics, one distinguishes between stocks and flows: a stock has units of "units" (say, widgets or dollars), while a flow is a derivative of a stock, and has units of "units/time" (say, dollars/year).
In some contexts, dimensional quantities are expressed as dimensionless quantities or percentages by omitting some dimensions. For example, debt-to-GDP ratios are generally expressed as percentages: total debt outstanding (dimension of currency) divided by annual GDP (dimension of currency) – but one may argue that in comparing a stock to a flow, annual GDP should have dimensions of currency/time (dollars/year, for instance), and thus Debt-to-GDP should have units of years, which indicates that Debt-to-GDP is the number of years needed for a constant GDP to pay the debt, if all GDP is spent on the debt and the debt is otherwise unchanged.
Conversion factor.
In dimensional analysis, a ratio which converts one unit of measure into another without changing the quantity is called a conversion factor. For example, kPa and bar are both units of pressure, and . The rules of algebra allow both sides of an equation to be divided by the same expression, so this is equivalent to . Since any quantity can be multiplied by 1 without changing it, the expression "" can be used to convert from bars to kPa by multiplying it with the quantity to be converted, including units. For example, because , and bar/bar cancels out, so .
Dimensional homogeneity.
The most basic rule of dimensional analysis is that of dimensional homogeneity. Only "commensurable" quantities (quantities with the same dimensions) may be "compared," "equated," "added," or "subtracted."
However, the dimensions form a "multiplicative group" and consequently:
For example, it makes no sense to ask whether 1 hour is more, the same, or less than 1 kilometer, as these have different dimensions, nor to add 1 hour to 1 kilometer. On the other hand, if an object travels 100 km in 2 hours, one may divide these and conclude that the object's average speed was 50 km/h.
The rule implies that in a physically meaningful "expression" only quantities of the same dimension can be added, subtracted, or compared. For example, if "m"man, "m"rat and "L"man denote, respectively, the mass of some man, the mass of a rat and the length of that man, the dimensionally homogeneous expression is meaningful, but the heterogeneous expression is meaningless. However, "m"man/"L"2man is fine. Thus, dimensional analysis may be used as a sanity check of physical equations: the two sides of any equation must be commensurable or have the same dimensions.
Even when two physical quantities have identical dimensions, it may nevertheless be meaningless to compare or add them. For example, although torque and energy share the dimension , they are fundamentally different physical quantities.
To compare, add, or subtract quantities with the same dimensions but expressed in different units, the standard procedure is first to convert them all to the same units. For example, to compare 32 metres with 35 yards, use 1 yard = 0.9144 m to convert 35 yards to 32.004 m.
A related principle is that any physical law that accurately describes the real world must be independent of the units used to measure the physical variables. For example, Newton's laws of motion must hold true whether distance is measured in miles or kilometers. This principle gives rise to the form that conversion factors must take between units that measure the same dimension: multiplication by a simple constant. It also ensures equivalence; for example, if two buildings are the same height in feet, then they must be the same height in meters.
The factor-label method for converting units.
The factor-label method is the sequential application of conversion factors expressed as fractions and arranged so that any dimensional unit appearing in both the numerator and denominator of any of the fractions can be cancelled out until only the desired set of dimensional units is obtained. For example, 10 miles per hour can be converted to meters per second by using a sequence of conversion factors as shown below:
It can be seen that each conversion factor is equivalent to the value of one. For example, starting with 1 mile = 1609 meters and dividing both sides of the equation by 1 mile yields 1 mile / 1 mile = 1609 meters / 1 mile, which when simplified yields 1 = 1609 meters / 1 mile.
So, when the units "mile" and "hour" are cancelled out and the arithmetic is done, 10 miles per hour converts to 4.47 meters per second.
As a more complex example, the concentration of nitrogen oxides (i.e., NOx) in the flue gas from an industrial furnace can be converted to a mass flow rate expressed in grams per hour (i.e., g/h) of NOx by using the following information as shown below:
After cancelling out any dimensional units that appear both in the numerators and denominators of the fractions in the above equation, the NOx concentration of 10 ppmv converts to mass flow rate of 24.63 grams per hour.
Checking equations that involve dimensions.
The factor-label method can also be used on any mathematical equation to check whether or not the dimensional units on the left hand side of the equation are the same as the dimensional units on the right hand side of the equation. Having the same units on both sides of an equation does not guarantee that the equation is correct, but having different units on the two sides of an equation does guarantee that the equation is wrong.
For example, check the Universal Gas Law equation of "P·V = n·R·T", when:
As can be seen, when the dimensional units appearing in the numerator and denominator of the equation's right hand side are cancelled out, both sides of the equation have the same dimensional units.
Limitations.
The factor-label method can convert only unit quantities for which the units are in a linear relationship intersecting at 0. Most units fit this paradigm. An example for which it cannot be used is the conversion between degrees Celsius and kelvins (or Fahrenheit). Between degrees Celsius and kelvins, there is a constant difference rather than a constant ratio, while between Celsius and Fahrenheit there is neither a constant difference nor a constant ratio. There is however an affine transform (formula_4), (rather than a linear transform (formula_5)) between them.
For example, the freezing point of water is 0 °C and 32 °F, and a 5 °C change is the same as a 9 °F change. Thus, to convert from units of Fahrenheit to units of Celsius, one subtracts 32 °F (the offset from the point of reference), divides by 9 °F and multiplies by 5 °C (scales by the ratio of units), and adds 0 °C (the offset from the point of reference). Reversing this yields the formula for obtaining a quantity in units of Celsius from units of Fahrenheit; one could have started with the equivalence between 100 °C and 212 °F, though this would yield the same formula at the end.
Hence, to convert the numerical quantity value of a temperature "F" in Fahrenheit to a numerical quantity value "C" in Celsius, this formula may be used:
To convert "C" in Celsius to "F" in Fahrenheit, this formula may be used:
Applications.
Dimensional analysis is most often used in physics and chemistry- and in the mathematics thereof – but finds some applications outside of those fields as well.
Mathematics.
A simple application of dimensional analysis to mathematics is in computing the form of the volume of an "n"-ball (the solid ball in "n" dimensions), or the area of its surface, the "n"-sphere: being an "n"-dimensional figure, the volume scales as formula_6 while the surface area, being formula_7-dimensional, scales as formula_8 Thus the volume of the "n"-ball in terms of the radius is formula_9 for some constant formula_10 Determining the constant takes more involved mathematics, but the form can be deduced and checked by dimensional analysis alone.
Finance, economics, and accounting.
In finance, economics, and accounting, dimensional analysis is most commonly referred to in terms of the distinction between stocks and flows. More generally, dimensional analysis is used in interpreting various financial ratios, economics ratios, and accounting ratios.
Fluid mechanics.
Common dimensionless groups in fluid mechanics include:
History.
The origins of dimensional analysis have been disputed by historians. The 19th-century French mathematician Joseph Fourier is generally credited with having made important contributions based on the idea that physical laws like should be independent of the units employed to measure the physical variables. This led to the conclusion that meaningful laws must be homogeneous equations in their various units of measurement, a result which was eventually formalized in the Buckingham π theorem. However, the first application of dimensional analysis has been credited to the Italian scholar François Daviet de Foncenex (1734–1799). It was published in 1761, 61 years before the publication of Fourier’s work. 
James Clerk Maxwell played a major role in establishing modern use of dimensional analysis by distinguishing mass, length, and time as fundamental units, while referring to other units as derived. Although Maxwell defined length, time and mass to be "the three fundamental units", he also noted that gravitational mass can be derived from length and time by assuming a form of Newton's law of universal gravitation in which the gravitational constant "G" is taken as unity, giving . By assuming a form of Coulomb's law in which Coulomb's constant "k"e is taken as unity, Maxwell then determined that the dimensions of an electrostatic unit of charge were , which, after substituting his equation for mass, results in charge having the same dimensions as mass, viz. .
Dimensional analysis is also used to derive relationships between the physical quantities that are involved in a particular phenomenon that one wishes to understand and characterize. It was used for the first time in this way in 1872 by Lord Rayleigh, who was trying to understand why the sky is blue. Rayleigh first published the technique in his 1877 book "The Theory of Sound".
Mathematical examples.
The Buckingham π theorem describes how every physically meaningful equation involving "n" variables can be equivalently rewritten as an equation of dimensionless parameters, where "m" is the rank of the dimensional matrix. Furthermore, and most importantly, it provides a method for computing these dimensionless parameters from the given variables.
A dimensional equation can have the dimensions reduced or eliminated through nondimensionalization, which begins with dimensional analysis, and involves scaling quantities by characteristic units of a system or natural units of nature. This gives insight into the fundamental properties of the system, as illustrated in the examples below.
Definition.
The dimension of a physical quantity can be expressed as a product of the basic physical dimensions length, mass, time, electric charge, and absolute temperature, represented by sans-serif roman symbols L, M, T, Q, and Θ, respectively, each raised to a rational power.
The SI standard recommends the usage of the following dimensions and corresponding symbols: length (L), mass (M), time (T), electric current (I), absolute temperature (Θ), amount of substance (N) and luminous intensity (J).
The term "dimension" is more abstract than "scale" unit: "mass" is a dimension, while kilograms are a scale unit (choice of standard) in the mass dimension.
As examples, the dimension of the physical quantity speed is "length"/"time" (L/T or LT−1), and the dimension of the physical quantity force is "mass × acceleration" or "mass×(length/time)/time" (ML/T2 or MLT−2). In principle, other dimensions of physical quantity could be defined as "fundamental" (such as momentum or energy or electric current) in lieu of some of those shown above. Most physicists do not recognize temperature, Θ, as a fundamental dimension of physical quantity since it essentially expresses the energy per degree of freedom, which can be expressed in terms of energy (or mass, length, and time). Still others do not recognize electric current, I, as a separate fundamental dimension of physical quantity, since it has been expressed in terms of mass, length, and time in unit systems such as the cgs system. There are also physicists that have cast doubt on the very existence of incompatible fundamental dimensions of physical quantity, although this does not invalidate the usefulness of dimensional analysis.
The unit of a physical quantity and its dimension are related, but not identical concepts. The units of a physical quantity are defined by convention and related to some standard; e.g. length may have units of metres, feet, inches, miles or micrometres; but any length always has a dimension of L, no matter what units of length are chosen to measure it. Two different units of the same physical quantity have conversion factors that relate them. For example, 1 in = 2.54 cm; in this case (2.54 cm/in) is the conversion factor, and is itself dimensionless. Therefore, multiplying by that conversion factor does not change a quantity. Dimensional symbols do not have conversion factors.
Mathematical properties.
The dimensions that can be formed from a given collection of basic physical dimensions, such as M, L, and T, form an abelian group: The identity is written as 1; , and the inverse to L is 1/L or L−1. L raised to any rational power "p" is a member of the group, having an inverse of L−"p" or 1/Lp. The operation of the group is multiplication, having the usual rules for handling exponents ().
This group can be described as a vector space over the rational numbers, with for example dimensional symbol M"i"L"j"T"k" corresponding to the vector . When physical measured quantities (be they like-dimensioned or unlike-dimensioned) are multiplied or divided by one other, their dimensional units are likewise multiplied or divided; this corresponds to addition or subtraction in the vector space. When measurable quantities are raised to a rational power, the same is done to the dimensional symbols attached to those quantities; this corresponds to scalar multiplication in the vector space.
A basis for a given vector space of dimensional symbols is called a set of fundamental units or fundamental dimensions, and all other vectors are called derived units. As in any vector space, one may choose different bases, which yields different systems of units (e.g., choosing whether the unit for charge is derived from the unit for current, or vice versa).
The group identity 1, the dimension of dimensionless quantities, corresponds to the origin in this vector space.
The set of units of the physical quantities involved in a problem correspond to a set of vectors (or a matrix). The kernel describes some number (e.g., "m") of ways in which these vectors can be combined to produce a zero vector. These correspond to producing (from the measurements) a number of dimensionless quantities, {π1, ..., π"m"}. (In fact these ways completely span the null subspace of another different space, of powers of the measurements.) Every possible way of multiplying (and exponentiating) together the measured quantities to produce something with the same units as some derived quantity "X" can be expressed in the general form
Consequently, every possible commensurate equation for the physics of the system can be rewritten in the form
Knowing this restriction can be a powerful tool for obtaining new insight into the system.
Mechanics.
In mechanics, the dimension of any physical quantity can be expressed in terms of the fundamental dimensions (or "base dimensions") M, L, and T – these form a 3-dimensional vector space. This is not the only possible choice, but it is the one most commonly used. For example, one might choose force, length and mass as the base dimensions (as some have done), with associated dimensions F, L, M; this corresponds to a different basis, and one may convert between these representations by a change of basis. The choice of the base set of dimensions is, thus, partly a convention, resulting in increased utility and familiarity. It is, however, important to note that the choice of the set of dimensions cannot be chosen arbitrarily – it is not "just" a convention – because the dimensions must form a basis: they must span the space, and be linearly independent.
For example, F, L, M form a set of fundamental dimensions because they form an equivalent basis to M, L, T: the former can be expressed as [F = ML/T2], L, M, while the latter can be expressed as M, L, [T = (ML/F)1/2].
On the other hand, using length, velocity and time as base dimensions will not work well (they do not form a set of fundamental dimensions), for two reasons:
Other fields of physics and chemistry.
Depending on the field of physics, it may be advantageous to choose one or another extended set of dimensional symbols. In electromagnetism, for example, it may be useful to use dimensions of M, L, T, and Q, where Q represents the dimension of electric charge. In thermodynamics, the base set of dimensions is often extended to include a dimension for temperature, Θ. In chemistry the number of moles of substance (the number of molecules divided by Avogadro's constant, ≈ 6.02 × 1023) is defined as a base unit as well.
In the interaction of relativistic plasma with strong laser pulses, a dimensionless relativistic similarity parameter, connected with the symmetry properties of the collisionless Vlasov equation, is constructed from the plasma-, electron- and critical-densities in addition to the electromagnetic vector potential. The choice of the dimensions or even the number of dimensions to be used in different fields of physics is to some extent arbitrary, but consistency in use and ease of communications are common and necessary features.
Polynomials and transcendental functions.
Scalar arguments to transcendental functions such as exponential, trigonometric and logarithmic functions, or to inhomogeneous polynomials, must be dimensionless quantities. (Note: this requirement is somewhat relaxed in Siano's orientational analysis described below, in which the square of certain dimensioned quantities are dimensionless.)
While most mathematical identities about dimensionless numbers translate in a straightforward manner to dimensional quantities, care must be taken with logarithms of ratios: the identity log(a/b) = log a − log b, where the logarithm is taken in any base, holds for dimensionless numbers a and b, but it does "not" hold if a and b are dimensional, because in this case the left-hand side is well-defined but the right-hand side is not.
Similarly, while one can evaluate monomials ("x""n") of dimensional quantities, one cannot evaluate polynomials of mixed degree with dimensionless coefficients on dimensional quantities: for "x"2, the expression (3 m)2 = 9 m2 makes sense (as an area), while for "x"2 + "x", the expression (3 m)2 + 3 m = 9 m2 + 3 m does not make sense.
However, polynomials of mixed degree can make sense if the coefficients are suitably chosen physical quantities that are not dimensionless. For example,
This is the height to which an object rises in time "t" if the acceleration of gravity is 32 feet per second per second and the initial upward speed is 500 feet per second. It is not even necessary for "t" to be in "seconds". For example, suppose "t" = 0.01 minutes. Then the first term would be
Incorporating units.
The value of a dimensional physical quantity "Z" is written as the product of a unit ["Z"] within the dimension and a dimensionless numerical factor, "n".
When like-dimensioned quantities are added or subtracted or compared, it is convenient to express them in consistent units so that the numerical values of these quantities may be directly added or subtracted. But, in concept, there is no problem adding quantities of the same dimension expressed in different units. For example, 1 meter added to 1 foot is a length, but one cannot derive that length by simply adding 1 and 1. A conversion factor, which is a ratio of like-dimensioned quantities and is equal to the dimensionless unity, is needed:
The factor formula_18 is identical to the dimensionless 1, so multiplying by this conversion factor changes nothing. Then when adding two quantities of like dimension, but expressed in different units, the appropriate conversion factor, which is essentially the dimensionless 1, is used to convert the quantities to identical units so that their numerical values can be added or subtracted.
Position vs displacement.
Some discussions of dimensional analysis implicitly describe all quantities as mathematical vectors. (In mathematics scalars are considered a special case of vectors; vectors can be added to or subtracted from other vectors, and, inter alia, multiplied or divided by scalars. If a vector is used to define a position, this assumes an implicit point of reference: an origin. While this is useful and often perfectly adequate, allowing many important errors to be caught, it can fail to model certain aspects of physics. A more rigorous approach requires distinguishing between position and displacement (or moment in time versus duration, or absolute temperature versus temperature change).
Consider points on a line, each with a position with respect to a given origin, and distances among them. Positions and displacements all have units of length, but their meaning is not interchangeable:
This illustrates the subtle distinction between "affine" quantities (ones modeled by an affine space, such as position) and "vector" quantities (ones modeled by a vector space, such as displacement).
Properly then, positions have dimension of "affine" length, while displacements have dimension of "vector" length. To assign a number to an "affine" unit, one must not only choose a unit of measurement, but also a point of reference, while to assign a number to a "vector" unit only requires a unit of measurement.
Thus some physical quantities are better modeled by vectorial quantities while others tend to require affine representation, and the distinction is reflected in their dimensional analysis.
This distinction is particularly important in the case of temperature, for which the numeric value of absolute zero is not the origin 0 in some scales. For absolute zero,
but for temperature differences,
(Here °R refers to the Rankine scale, not the Réaumur scale).
Unit conversion for temperature differences is simply a matter of multiplying by, e.g., 1 °F / 1 K (although the ratio is not a constant value). But because some of these scales have origins that do not correspond to absolute zero, conversion from one temperature scale to another requires accounting for that. As a result, simple dimensional analysis can lead to errors if it is ambiguous whether 1 K means the absolute temperature equal to −272.15 °C, or the temperature difference equal to 1 °C.
Orientation and frame of reference.
Similar to the issue of a point of reference is the issue of orientation: a displacement in 2 or 3 dimensions is not just a length, but is a length together with a "direction". (This issue does not arise in 1 dimension, or rather is equivalent to the distinction between positive and negative.) Thus, to compare or combine two dimensional quantities in a multi-dimensional space, one also needs an orientation: they need to be compared to a frame of reference.
This leads to the extensions discussed below, namely Huntley's directed dimensions and Siano's orientational analysis.
Examples.
A simple example: period of a harmonic oscillator.
What is the period of oscillation formula_19 of a mass formula_20 attached to an ideal linear spring with spring constant formula_21 suspended in gravity of strength formula_22? That period is the solution for formula_19 of some dimensionless equation in the variables formula_19, formula_20, formula_21, and formula_22.
The four quantities have the following dimensions: formula_19 [T]; formula_20 [M]; formula_21 [M/T2]; and formula_22 [L/T2]. From these we can form only one dimensionless product of powers of our chosen variables, formula_32 = formula_33 , and putting formula_34 for some dimensionless constant formula_35 gives the dimensionless equation sought. The dimensionless product of powers of variables is sometimes referred to as a dimensionless group of variables; here the term "group" means "collection" rather than mathematical group. They are often called dimensionless numbers as well.
Note that the variable formula_22 does not occur in the group. It is easy to see that it is impossible to form a dimensionless product of powers that combines formula_22 with formula_21, formula_20, and formula_19, because formula_22 is the only quantity that involves the dimension L. This implies that in this problem the formula_22 is irrelevant. Dimensional analysis can sometimes yield strong statements about the "irrelevance" of some quantities in a problem, or the need for additional parameters. If we have chosen enough variables to properly describe the problem, then from this argument we can conclude that the period of the mass on the spring is independent of formula_22: it is the same on the earth or the moon. The equation demonstrating the existence of a product of powers for our problem can be written in an entirely equivalent way: formula_44, for some dimensionless constant κ (equal to formula_45 from the original dimensionless equation).
When faced with a case where dimensional analysis rejects a variable (formula_22, here) that one intuitively expects to belong in a physical description of the situation, another possibility is that the rejected variable is in fact relevant, but that some other relevant variable has been omitted, which might combine with the rejected variable to form a dimensionless quantity. That is, however, not the case here.
When dimensional analysis yields only one dimensionless group, as here, there are no unknown functions, and the solution is said to be "complete" – although it still may involve unknown dimensionless constants, such as κ.
A more complex example: energy of a vibrating wire.
Consider the case of a vibrating wire of length "ℓ" (L) vibrating with an amplitude "A" (L). The wire has a linear density "ρ" (M/L) and is under tension "s" (ML/T2), and we want to know the energy "E" (ML2/T2) in the wire. Let "π"1 and "π"2 be two dimensionless products of powers of the variables chosen, given by
The linear density of the wire is not involved. The two groups found can be combined into an equivalent form as an equation
where "F" is some unknown function, or, equivalently as
where "f" is some other unknown function. Here the unknown function implies that our solution is now incomplete, but dimensional analysis has given us something that may not have been obvious: the energy is proportional to the first power of the tension. Barring further analytical analysis, we might proceed to experiments to discover the form for the unknown function "f". But our experiments are simpler than in the absence of dimensional analysis. We'd perform none to verify that the energy is proportional to the tension. Or perhaps we might guess that the energy is proportional to "ℓ", and so infer that . The power of dimensional analysis as an aid to experiment and forming hypotheses becomes evident.
The power of dimensional analysis really becomes apparent when it is applied to situations, unlike those given above, that are more complicated, the set of variables involved are not apparent, and the underlying equations hopelessly complex. Consider, for example, a small pebble sitting on the bed of a river. If the river flows fast enough, it will actually raise the pebble and cause it to flow along with the water. At what critical velocity will this occur? Sorting out the guessed variables is not so easy as before. But dimensional analysis can be a powerful aid in understanding problems like this, and is usually the very first tool to be applied to complex problems where the underlying equations and constraints are poorly understood. In such cases, the answer may depend on a dimensionless number such as the Reynolds number, which may be interpreted by dimensional analysis.
Extensions.
Huntley's extension: directed dimensions.
Huntley has pointed out that it is sometimes productive to refine our concept of dimension. Two possible refinements are:
As an example of the usefulness of the first refinement, suppose we wish to calculate the distance a cannonball travels when fired with a vertical velocity component formula_50 and a horizontal velocity component formula_51, assuming it is fired on a flat surface. Assuming no use of directed lengths, the quantities of interest are then formula_51, formula_50, both dimensioned as LT−1, "R", the distance travelled, having dimension L, and "g" the downward acceleration of gravity, with dimension LT−2.
With these four quantities, we may conclude that the equation for the range "R" may be written:
Or dimensionally
from which we may deduce that formula_56 and formula_57, which leaves one exponent undetermined. This is to be expected since we have two fundamental dimensions L and T, and four parameters, with one equation.
If, however, we use directed length dimensions, then formula_51 will be dimensioned as LxT−1, formula_50 as LyT−1, "R" as Lx and "g" as LyT−2. The dimensional equation becomes:
and we may solve completely as formula_61, formula_62 and formula_63. The increase in deductive power gained by the use of directed length dimensions is apparent.
In a similar manner, it is sometimes found useful (e.g., in fluid mechanics and thermodynamics) to distinguish between mass as a measure of inertia (inertial mass), and mass as a measure of quantity (substantial mass). For example, consider the derivation of Poiseuille's Law. We wish to find the rate of mass flow of a viscous fluid through a circular pipe. Without drawing distinctions between inertial and substantial mass we may choose as the relevant variables
There are three fundamental variables so the above five equations will yield two dimensionless variables which we may take to be formula_69 and formula_70 and we may express the dimensional equation as
where "C" and "a" are undetermined constants. If we draw a distinction between inertial mass with dimension formula_72 and substantial mass with dimension formula_73, then mass flow rate and density will use substantial mass as the mass parameter, while the pressure gradient and coefficient of viscosity will use inertial mass. We now have four fundamental parameters, and one dimensionless constant, so that the dimensional equation may be written:
where now only "C" is an undetermined constant (found to be equal to formula_75 by methods outside of dimensional analysis). This equation may be solved for the mass flow rate to yield Poiseuille's law.
Siano's extension: orientational analysis.
Huntley's extension has some serious drawbacks:
It also is often quite difficult to assign the L, Lx, Ly, Lz, symbols to the physical variables involved in the problem of interest. He invokes a procedure that involves the "symmetry" of the physical problem. This is often very difficult to apply reliably: It is unclear as to what parts of the problem that the notion of "symmetry" is being invoked. Is it the symmetry of the physical body that forces are acting upon, or to the points, lines or areas at which forces are being applied? What if more than one body is involved with different symmetries? Consider the spherical bubble attached to a cylindrical tube, where one wants the flow rate of air as a function of the pressure difference in the two parts. What are the Huntley extended dimensions of the viscosity of the air contained in the connected parts? What are the extended dimensions of the pressure of the two parts? Are they the same or different? These difficulties are responsible for the limited application of Huntley's addition to real problems.
Angles are, by convention, considered to be dimensionless variables, and so the use of angles as physical variables in dimensional analysis can give less meaningful results. As an example, consider the projectile problem mentioned above. Suppose that, instead of the x- and y-components of the initial velocity, we had chosen the magnitude of the velocity "v" and the angle "θ" at which the projectile was fired. The angle is, by convention, considered to be dimensionless, and the magnitude of a vector has no directional quality, so that no dimensionless variable can be composed of the four variables "g", "v", "R", and "θ". Conventional analysis will correctly give the powers of "g" and "v", but will give no information concerning the dimensionless angle "θ".
 has suggested that the directed dimensions of Huntley be replaced by using "orientational symbols" 1x 1y 1z to denote vector directions, and an orientationless symbol 10. Thus, Huntley's Lx becomes L 1x with L specifying the dimension of length, and 1x specifying the orientation. Siano further shows that the orientational symbols have an algebra of their own. Along with the requirement that , the following multiplication table for the orientation symbols results:
Note that the orientational symbols form a group (the Klein four-group or "Viergruppe"). In this system, scalars always have the same orientation as the identity element, independent of the "symmetry of the problem". Physical quantities that are vectors have the orientation expected: a force or a velocity in the z-direction has the orientation of 1z. For angles, consider an angle "θ" that lies in the z-plane. Form a right triangle in the z-plane with "θ" being one of the acute angles. The side of the right triangle adjacent to the angle then has an orientation 1x and the side opposite has an orientation 1y. Then, since tan("θ") = 1y/1x = "θ" + ... we conclude that an angle in the xy-plane must have an orientation 1y/1x = 1z, which is not unreasonable. Analogous reasoning forces the conclusion that sin("θ") has orientation 1z while cos("θ") has orientation 10. These are different, so one concludes (correctly), for example, that there are no solutions of physical equations that are of the form , where "a" and "b" are real scalars. Note that an expression such as formula_77 is not dimensionally inconsistent since it is a special case of the sum of angles formula and should properly be written:
which for formula_79 and formula_80 yields formula_81. Physical quantities may be expressed as complex numbers (e.g. formula_82) which imply that the complex quantity "i" has an orientation equal to that of the angle it is associated with (1z in the above example).
The assignment of orientational symbols to physical quantities and the requirement that physical equations be orientationally homogeneous can actually be used in a way that is similar to dimensional analysis to derive a little more information about acceptable solutions of physical problems. In this approach one sets up the dimensional equation and solves it as far as one can. If the lowest power of a physical variable is fractional, both sides of the solution is raised to a power such that all powers are integral. This puts it into "normal form". The orientational equation is then solved to give a more restrictive condition on the unknown powers of the orientational symbols, arriving at a solution that is more complete than the one that dimensional analysis alone gives. Often the added information is that one of the powers of a certain variable is even or odd.
As an example, for the projectile problem, using orientational symbols, "θ", being in the xy-plane will thus have dimension 1z and the range of the projectile "R" will be of the form:
Dimensional homogeneity will now correctly yield and , and orientational homogeneity requires that "c" be an odd integer. In fact the required function of theta will be sin("θ")cos("θ") which is a series of odd powers of "θ".
It is seen that the Taylor series of sin("θ") and cos("θ") are orientationally homogeneous using the above multiplication table, while expressions like and exp("θ") are not, and are (correctly) deemed unphysical.
It should be clear that the multiplication rule used for the orientational symbols is not the same as that for the cross product of two vectors. The cross product of two identical vectors is zero, while the product of two identical orientational symbols is the identity element.
Dimensionless concepts.
Constants.
The dimensionless constants that arise in the results obtained, such as the C in the Poiseuille's Law problem and the formula_84 in the spring problems discussed above come from a more detailed analysis of the underlying physics, and often arises from integrating some differential equation. Dimensional analysis itself has little to say about these constants, but it is useful to know that they very often have a magnitude of order unity. This observation can allow one to sometimes make "back of the envelope" calculations about the phenomenon of interest, and therefore be able to more efficiently design experiments to measure it, or to judge whether it is important, etc.
Formalisms.
Paradoxically, dimensional analysis can be a useful tool even if all the parameters in the underlying theory are dimensionless, e.g., lattice models such as the Ising model can be used to study phase transitions and critical phenomena. Such models can be formulated in a purely dimensionless way. As we approach the critical point closer and closer, the distance over which the variables in the lattice model are correlated (the so-called correlation length, formula_85 ) becomes larger and larger. Now, the correlation length is the relevant length scale related to critical phenomena, so one can, e.g., surmise on "dimensional grounds" that the non-analytical part of the free energy per lattice site should be formula_86 where formula_87 is the dimension of the lattice.
It has been argued by some physicists, e.g., M. J. Duff, that the laws of physics are inherently dimensionless. The fact that we have assigned incompatible dimensions to Length, Time and Mass is, according to this point of view, just a matter of convention, borne out of the fact that before the advent of modern physics, there was no way to relate mass, length, and time to each other. The three independent dimensionful constants: "c", "ħ", and "G", in the fundamental equations of physics must then be seen as mere conversion factors to convert Mass, Time and Length into each other.
Just as in the case of critical properties of lattice models, one can recover the results of dimensional analysis in the appropriate scaling limit; e.g., dimensional analysis in mechanics can be derived by reinserting the constants "ħ", "c", and "G" (but we can now consider them to be dimensionless) and demanding that a nonsingular relation between quantities exists in the limit formula_88, formula_89 and formula_90. In problems involving a gravitational field the latter limit should be taken such that the field stays finite.
Dimensional equivalences.
Following are tables of commonly occurring expressions in physics, related to the dimensions of energy, momentum, and force.
Natural units.
If , where "c" is the speed of light and "ħ" is the reduced Planck constant, and a suitable fixed unit of energy is chosen, then all quantities of length "L", mass "M" and time "T" can be expressed (dimensionally) as a power of energy "E", because length, mass and time can be expressed using speed "v", action "S", and energy "E":
though speed and action are dimensionless ( and ) – so the only remaining quantity with dimension is energy. In terms of powers of dimensions:
This is particularly useful in particle physics and high energy physics, in which case the energy unit is the electron volt (eV). Dimensional checks and estimates become very simple in this system.
However, if electric charges and currents are involved, another unit to be fixed is for electric charge, normally the electron charge "e" though other choices are possible.

</doc>
<doc id="51331" url="https://en.wikipedia.org/wiki?curid=51331" title="Dimensionless quantity">
Dimensionless quantity

In dimensional analysis, a dimensionless quantity is a quantity to which no physical dimension is applicable. It is thus a bare number, and is therefore also known as a quantity of dimension one. Dimensionless quantities are widely used in many fields, such as mathematics, physics, engineering, and economics. Numerous well-known quantities, such as , , and , are dimensionless. By contrast, examples of quantities with dimensions are length, time, and speed, which are measured in dimensional units, such as meter, second and meter/second.
Dimensionless quantities are often obtained as products or ratios of quantities that are not dimensionless, but whose dimensions cancel in the mathematical operation. This is the case, for instance, with the engineering strain, a measure of deformation. It is defined as change in length, divided by initial length, but because these quantities both have dimensions "L" (length), the result is a dimensionless quantity.
Properties.
All pure numbers are dimensionless quantities.
A dimensionless quantity may have dimensionless units, even though it has no physical dimension associated with it. For example, to show the quantity being measured (for example mass fraction or mole fraction), it is sometimes helpful to use the same units in both the numerator and denominator (kg/kg or mol/mol). The quantity may also be given as a ratio of two different units that have the same dimension (for instance, light years over meters). This may be the case when calculating slopes in graphs, or when making unit conversions. Such notation does not indicate the presence of physical dimensions, and is purely a notational convention. Other common dimensionless units are  % (= 0.01),  ‰ (= 0.001), ppm (= 10−6), ppb (= 10−9), ppt (= 10−12), angle units (degrees, radians, grad), dalton and mole. Units of number such as the dozen, gross, and googol are also dimensionless.
When a quantity is the ratio of two other quantities, each of the same dimension, the defined quantity is dimensionless and has the same value regardless of the units used to calculate the two composing quantities. For instance, if body A exerts a force of magnitude "F" on body B, and B exerts a force of magnitude "f" on A, then the ratio "F"/"f" is always equal to 1, regardless of the actual units used to measure "F" and "f". This is a fundamental property of dimensionless proportions and follows from the assumption that the laws of physics are independent of the system of units used in their expression. In this case, if the ratio "F"/"f" was not always equal to 1, but changed if one switched from SI to CGS, that would mean that Newton's Third Law's truth or falsity would depend on the system of units used, which would contradict this fundamental hypothesis. This assumption that the laws of physics are not contingent upon a specific unit system is the basis for the Buckingham π theorem, as discussed in a later section.
Examples.
There are many areas where dimensionless quantities are used. Some quantities are given here to illustrate the properties more concretely in their respective areas of application. The list of dimensionless quantities contains an extensive number of other important examples.
Mathematics.
Several examples from mathematics include proportions, angles, and special numbers. A simple problem illustrates how a proportion may be a dimensionless quantity. Sarah says, "Out of every 10 apples I gather, 1 is rotten." The rotten-to-gathered ratio is (1 apple) / (10 apples) = 0.1 = 10%, which is a dimensionless quantity. Similarly, angles may be defined as a proportion. The radian measure of angles is the ratio of the length of a circle's arc subtended by an angle whose vertex is the centre of the circle to some other length. The ratio—i.e., length divided by length—is dimensionless. When using radians as the unit, the length that is compared is the length of the radius of the circle. When using degree as the units, the arc's length is compared to 1/360 of the circumference of the circle. In the case of the dimensionless quantity π, being the ratio of a circle's circumference to its diameter, the number would be constant regardless of what unit is used to measure a circle's circumference and diameter (e.g., centimetres, miles, light-years, etc.), as long as the same unit is used for both. Additionally, the golden ratio, , is simply the ratio of length of the two sides of a golden rectangle. In statistics the coefficient of variation is the ratio of the standard deviation to the mean and is used to measure the dispersion in the data.
Dimensionless physical constants.
Certain fundamental physical constants, such as the speed of light in a vacuum, the universal gravitational constant, Planck's constant and Boltzmann's constant can be normalized to 1 if appropriate units for time, length, mass, charge, and temperature are chosen. The resulting system of units is known as the natural units. However, not all physical constants can be normalized in this fashion. For example, the values of the following constants are independent of the system of units and must be determined experimentally:
Origin and derivation.
History.
Dimensionless quantities are a special result of the field of dimensional analysis. In the nineteenth century, French mathematician Joseph Fourier and Scottish physicist James Clerk Maxwell lead significant developments in the modern concepts of dimension and unit. Later work by British physicists Osborne Reynolds and Lord Rayleigh contributed to the understanding of dimensionless numbers in physics. Building on Rayleigh's method of dimensional analysis, Edgar Buckingham proved the π theorem (independent of French mathematician Joseph Bertrand's previous work) to formalize the nature of dimensionless quantities. Numerous other dimensionless numbers were discovered in the early 1900s, the particularly in the areas of fluid mechanics and heat transfer. In the 2000s, the International Committee for Weights and Measures contemplated defining the unit of 1 as the 'uno', but the idea was dropped.
Buckingham π theorem.
The Buckingham π theorem indicates that validity of the laws of physics does not depend on a specific unit system. A statement of this theorem is that any physical law can be expressed as an identity involving only dimensionless combinations (ratios or products) of the variables linked by the law (e. g., pressure and volume are linked by Boyle's Law – they are inversely proportional). If the dimensionless combinations' values changed with the systems of units, then the equation would not be an identity, and Buckingham's theorem would not hold.
Another consequence of the Buckingham π theorem of dimensional analysis is that the functional dependence between a certain number (say, "n") of variables can be reduced by the number (say, "k") of independent dimensions occurring in those variables to give a set of "p" = "n" − "k" independent, dimensionless quantities. For the purposes of the experimenter, different systems that share the same description by dimensionless quantity are equivalent.
Example.
To demonstrate the application of the π theorem, consider the power consumption of a stirrer with a given shape.
The power, "P", in dimensions [M · L2/T3], is a function of the density, "ρ" [M/L3], and the viscosity of the fluid to be stirred, "μ" [M/(L · T)], as well as the size of the stirrer given by its diameter, "D" [L], and the speed of the stirrer, "n" [1/T]. Therefore, we have a total of "n" = 5 variables representing our example. Those "n" = 5 variables are built up from "k" = 3 fundamental dimensions, the length: L (SI units: m), time: T (s), and mass: M (kg).
According to the π-theorem, the "n" = 5 variables can be reduced by the "k" = 3 dimensions to form "p" = "n" − "k" = 5 − 3 = 2 independent dimensionless numbers. These quantities are formula_1, commonly named the Reynolds number which describes the fluid flow regime, and formula_2, the Power number, which is the dimensionless description of the stirrer.
Nondimensionalization of Differential Equations.
The process of nondimensionalization has significant applications in the analysis of differential equations.

</doc>
<doc id="8378" url="https://en.wikipedia.org/wiki?curid=8378" title="Dipole">
Dipole

In physics, there are several kinds of dipole:
Dipoles can be characterized by their dipole moment, a vector quantity. For the simple electric dipole given above, the electric dipole moment points from the negative charge towards the positive charge, and has a magnitude equal to the strength of each charge times the separation between the charges. (To be precise: for the definition of the dipole moment, one should always consider the "dipole limit", where e.g. the distance of the generating charges should "converge" to 0, while simultaneously the charge strength should "diverge" to infinity in such a way that the product remains a positive constant.)
For the current loop, the magnetic dipole moment points through the loop (according to the right hand grip rule), with a magnitude equal to the current in the loop times the area of the loop.
In addition to current loops, the electron, among other fundamental particles, has a magnetic dipole moment. This is because it generates a magnetic field that is identical to that generated by a very small current loop. However, the electron's magnetic moment is not due to a current loop, but is instead an intrinsic property of the electron. It is also possible that the electron has an "electric" dipole moment, although this has not yet been observed (see electron electric dipole moment for more information).
A permanent magnet, such as a bar magnet, owes its magnetism to the intrinsic magnetic dipole moment of the electron. The two ends of a bar magnet are referred to as poles (not to be confused with monopoles), and may be labeled "north" and "south". In terms of the Earth's magnetic field, these are respectively "north-seeking" and "south-seeking" poles, that is if the magnet were freely suspended in the Earth's magnetic field, the north-seeking pole would point towards the north and the south-seeking pole would point twards the south. The dipole moment of the bar magnet points from its magnetic south to its magnetic north pole. The north pole of a bar magnet in a compass points north. However, this means that Earth's geomagnetic north pole is the "south" pole (south-seeking pole) of its dipole moment, and vice versa.
The only known mechanisms for the creation of magnetic dipoles are by current loops or quantum-mechanical spin since the existence of magnetic monopoles has never been experimentally demonstrated.
The term comes from the Greek δίς ("dis"), "twice" and πόλος ("pòlos"), "axis".
Classification.
A "physical dipole" consists of two equal and opposite point charges: in the literal sense, two poles. Its field at large distances (i.e., distances large in comparison to the separation of the poles) depends almost entirely on the dipole moment as defined above. A "point (electric) dipole" is the limit obtained by letting the separation tend to 0 while keeping the dipole moment fixed. The field of a point dipole has a particularly simple form, and the order-1 term in the multipole expansion is precisely the point dipole field.
Although there are no known magnetic monopoles in nature, there are magnetic dipoles in the form of the quantum-mechanical spin associated with particles such as electrons (although the accurate description of such effects falls outside of classical electromagnetism). A theoretical magnetic "point dipole" has a magnetic field of exactly the same form as the electric field of an electric point dipole. A very small current-carrying loop is approximately a magnetic point dipole; the magnetic dipole moment of such a loop is the product of the current flowing in the loop and the (vector) area of the loop.
Any configuration of charges or currents has a 'dipole moment', which describes the dipole whose field is the best approximation, at large distances, to that of the given configuration. This is simply one term in the multipole expansion when the total charge ("monopole moment") is 0 — as it "always" is for the magnetic case, since there are no magnetic monopoles. The dipole term is the dominant one at large distances: Its field falls off in proportion to 1/"r"3, as compared to 1/"r"4 for the next (quadrupole) term and higher powers of 1/"r" for higher terms, or 1/"r"2 for the monopole term.
Molecular dipoles.
Many molecules have such dipole moments due to non-uniform distributions of positive and negative charges on the various atoms. Such is the case with polar compounds like hydrogen fluoride (HF), where electron density is shared unequally between atoms. Therefore, a molecule's dipole is an electric dipole with an inherent electric field which should not be confused with a magnetic dipole which generates a magnetic field.
The physical chemist Peter J. W. Debye was the first scientist to study molecular dipoles extensively, and, as a consequence, dipole moments are measured in units named "debye" in his honor.
For molecules there are three types of dipoles:
More generally, an induced dipole of "any" polarizable charge distribution "ρ" (remember that a molecule has a charge distribution) is caused by an electric field external to "ρ". This field may, for instance, originate from an ion or polar molecule in the vicinity of "ρ" or may be macroscopic (e.g., a molecule between the plates of a charged capacitor). The size of the induced dipole is equal to the product of the strength of the
external field and the dipole polarizability of "ρ".
Dipole moment values can be obtained from measurement of the dielectric constant. Some typical gas phase values in debye units are:
KBr has one of the highest dipole moments because it is a very ionic molecule (which only exists as a molecule in the gas phase).
 The overall dipole moment of a molecule may be approximated as a vector sum of bond dipole moments. As a vector sum it depends on the relative orientation of the bonds, so that from the dipole moment information can be deduced about the molecular geometry.
For example the zero dipole of CO2 implies that the two C=O bond dipole moments cancel so that the molecule must be linear. For H2O the O-H bond moments do not cancel because the molecule is bent. For ozone (O3) which is also a bent molecule, the bond dipole moments are not zero even though the O-O bonds are between similar atoms. This agrees with the Lewis structures for the resonance forms of ozone which show a positive charge on the central oxygen atom. 
An example in organic chemistry of the role of geometry in determining dipole moment is the "cis" and "trans" isomers of 1,2-dichloroethene. In the cis isomer the two polar C-Cl bonds are on the same side of the C=C double bond and the molecular dipole moment is 1.90 D. In the trans isomer, the dipole moment is zero because the two C-Cl bond are on opposite sides of the C=C and cancel (and the two bond moments for the much less polar C-H bonds also cancel).
Another example of the role of molecular geometry is boron trifluoride, which has three polar bonds with a difference in electronegativity greater than the traditionally cited threshold of 1.7 for ionic bonding. However, due to the equilateral triangular distribution of the fluoride ions about the boron cation center, the molecule as a whole does not exhibit any identifiable pole: one cannot construct a plane that divides the molecule into a net negative part and a net positive part.
Quantum mechanical dipole operator.
Consider a collection of "N" particles with charges "qi" and position vectors r"i". For instance, this collection may be a molecule consisting of electrons, all with charge −"e", and nuclei with charge "eZi", where "Zi" is the atomic number of the "i" th nucleus.
The dipole observable (physical quantity) has the quantum mechanical dipole operator:
Notice that this definition is valid only for non-charged dipoles, i.e. total charge equal to zero. To a charged dipole we have the next equation:
where formula_3 is the center of mass of the molecule/group of particles.
Atomic dipoles.
A non-degenerate (S-state) atom can have only a zero permanent dipole. This fact follows quantum mechanically from the inversion symmetry of atoms. All 3 components of the dipole operator are antisymmetric under inversion with respect to the nucleus,
where formula_5 is the dipole operator and formula_6 is the inversion operator.
The permanent dipole moment of an atom in a non-degenerate state (see degenerate energy level) is given as the expectation (average) value of the dipole operator,
where formula_8 is an S-state, non-degenerate, wavefunction, which
is symmetric or antisymmetric under inversion: formula_9.
Since the product of the wavefunction (in the ket) and its complex conjugate (in the bra) is always symmetric under inversion and its inverse,
it follows that the expectation value changes sign under inversion. We used here the fact that
formula_11, being a symmetry operator, is unitary:
formula_12 and by definition
the Hermitian adjoint formula_13 may be moved from bra to ket and then becomes formula_14.
Since the only quantity that is equal to minus itself is the zero, the expectation value vanishes,
In the case of open-shell atoms with degenerate energy levels, one could define a dipole moment by the aid of the first-order Stark effect. This gives a non-vanishing dipole (by definition proportional to a non-vanishing first-order Stark shift) only if some of the wavefunctions belonging to the degenerate energies have opposite parity; i.e., have different behavior under inversion. This is a rare occurrence, but happens for the excited H-atom, where 2s and 2"p" states are "accidentally" degenerate (see article Laplace–Runge–Lenz vector for the origin of this degeneracy) and have opposite parity (2s is even and 2p is odd).
Field of a static magnetic dipole.
Magnitude.
The far-field strength, "B", of a dipole magnetic field is given by
where
Conversion to cylindrical coordinates is achieved using and
where "ρ" is the perpendicular distance from the "z"-axis. Then,
Vector form.
The field itself is a vector quantity:
where
This is "exactly" the field of a point dipole, "exactly" the dipole term in the multipole expansion of an arbitrary field, and "approximately" the field of any dipole-like configuration at large distances.
Magnetic vector potential.
The vector potential A of a magnetic dipole is
with the same definitions as above.
Field from an electric dipole.
The electrostatic potential at position r due to an electric dipole at the origin is given by:
where
This term appears as the second term in the multipole expansion of an arbitrary electrostatic potential Φ(r). If the source of Φ(r) is a dipole, as it is assumed here, this term is the only non-vanishing term in the multipole expansion of Φ(r). The electric field from a dipole can be found from the gradient of this potential:
where E is the electric field and "δ"3 is the 3-dimensional delta function. This is formally identical to the magnetic H field of a point magnetic dipole with only a few names changed.
Torque on a dipole.
Since the direction of an electric field is defined as the direction of the force on a positive charge, electric field lines point away from a positive charge and toward a negative charge.
When placed in an electric or magnetic field, equal but opposite forces arise on each side of the dipole creating a torque τ:
for an electric dipole moment p (in coulomb-meters), or
for a magnetic dipole moment m (in ampere-square meters).
The resulting torque will tend to align the dipole with the applied field, which in the case of an electric dipole, yields a potential energy of
The energy of a magnetic dipole is similarly
Dipole radiation.
In addition to dipoles in electrostatics, it is also common to consider an electric or magnetic dipole that is oscillating in time. It is an extension, or a more physical next-step, to spherical wave radiation.
In particular, consider a harmonically oscillating electric dipole, with angular frequency ω and a dipole moment formula_29 along the formula_30 direction of the form
In vacuum, the exact field produced by this oscillating dipole can be derived using the retarded potential formulation as:
formula_32
formula_33
For formula_34, the far-field takes the simpler form of a radiating "spherical" wave, but with angular dependence embedded in the cross-product:
The time-averaged Poynting vector
formula_37
is not distributed isotropically, but concentrated around the directions lying perpendicular to the dipole moment, as a result of the non-spherical electric and magnetic waves. In fact, the spherical harmonic function (formula_38) responsible for such "donut-shaped" angular distribution is precisely the formula_39 "p" wave.
The total time-average power radiated by the field can then be derived from the Poynting vector as
Notice that the dependence of the power on the fourth power of the frequency of the radiation is in accordance with the Rayleigh scattering, and the underlying effects why the sky consists of mainly blue colour.
A circular polarized dipole is described as a superposition of two linear dipoles.

</doc>
<doc id="2142913" url="https://en.wikipedia.org/wiki?curid=2142913" title="Diquark">
Diquark

In particle physics, a diquark, or diquark correlation/clustering, is a hypothetical state of two quarks grouped inside a baryon (that consists of three quarks) (Lichtenberg 1982). Corresponding models of baryons are referred to as quark–diquark models. The diquark is often treated as a single subatomic particle with which the third quark interacts via the strong interaction. The existence of diquarks inside the nucleons is a disputed issue, but it helps to explain some nucleon properties and to reproduce experimental data sensitive to the nucleon structure. Diquark–antidiquark pairs have also been advanced for anomalous particles such as the X(3872).
Formation.
The forces between the two quarks in a diquark is attractive when both the colors and spins are antisymmetric. When both quarks are correlated in this way they tend to form a very low energy configuration. This low energy configuration has become known as a diquark.
Controversy.
Many scientists theorize that a diquark should not be considered a particle. Even though they may contain two quarks they are not colour neutral, and therefore cannot exist as isolated bound states. So instead they tend to float freely inside hadrons as composite entities; while free-floating they have a size of about . This also happens to be the same size as the hadron itself.
Uses.
Diquarks are the conceptual building blocks, and as such give scientists an ordering principle for the most important states in the hadronic spectrum. There are many different pieces of evidence that prove diquarks are fundamental in the structure of hadrons. One of the most compelling pieces of evidence comes from a recent study of baryons. In this study the baryon had one heavy and two light quarks. Since the heavy quark is inert, the scientists were able to discern the properties of the different quark configurations in the hadronic spectrum.
Λ and Σ baryon experiment.
An experiment was conducted using diquarks in an attempt to study the Λ and Σ baryons that are produced in the creation of hadrons created by fast-moving quarks. In the experiment the quarks ionized the vacuum area. This produced the quark–antiquark pairs, which then converted themselves into mesons. When generating a baryon by assembling quarks, it is helpful if the quarks first form a stable two-quark state. The Λ and the Σ are created as a result of up, down and strange quarks. Scientists found that the Λ contains the [ud] diquark, however the Σ does not. From this experiment scientists inferred that Λ baryons are more common than Σ baryons, and indeed they are more common by a factor of 10.

</doc>
<doc id="37021" url="https://en.wikipedia.org/wiki?curid=37021" title="Dirac delta function">
Dirac delta function

In mathematics, the Dirac delta function, or function, is a generalized function, or distribution, on the real number line that is zero everywhere except at zero, with an integral of one over the entire real line. The delta function is sometimes thought of as an infinitely high, infinitely thin spike at the origin, with total area one under the spike, and physically represents the density of an idealized point mass or point charge. It was introduced by theoretical physicist Paul Dirac. In the context of signal processing it is often referred to as the unit impulse symbol (or function). Its discrete analog is the Kronecker delta function, which is usually defined on a discrete domain and takes values 0 and 1.
From a purely mathematical viewpoint, the Dirac delta is not strictly a function, because any extended-real function that is equal to zero everywhere but a single point must have total integral zero. The delta function only makes sense as a mathematical object when it appears inside an integral. While from this perspective the Dirac delta can usually be manipulated as though it were a function, formally it must be defined as a distribution that is also a measure. In many applications, the Dirac delta is regarded as a kind of limit (a weak limit) of a sequence of functions having a tall spike at the origin. The approximating functions of the sequence are thus "approximate" or "nascent" delta functions.
Overview.
The graph of the delta function is usually thought of as following the whole "x"-axis and the positive "y"-axis. Despite its name, the delta function is not truly a function, at least not a usual one with range in real numbers. For example, the objects and are equal everywhere except at yet have integrals that are different. According to Lebesgue integration theory, if "f" and "g" are functions such that almost everywhere, then "f" is integrable if and only if "g" is integrable and the integrals of "f" and "g" are identical. Rigorous treatment of the Dirac delta requires measure theory or the theory of distributions.
The Dirac delta is used to model a tall narrow spike function (an "impulse"), and other similar abstractions such as a point charge, point mass or electron point. For example, to calculate the dynamics of a baseball being hit by a bat, one can approximate the force of the bat hitting the baseball by a delta function. In doing so, one not only simplifies the equations, but one also is able to calculate the motion of the baseball by only considering the total impulse of the bat against the ball rather than requiring knowledge of the details of how the bat transferred energy to the ball.
In applied mathematics, the delta function is often manipulated as a kind of limit (a weak limit) of a sequence of functions, each member of which has a tall spike at the origin: for example, a sequence of Gaussian distributions centered at the origin with variance tending to zero.
History.
Joseph Fourier presented what is now called the Fourier integral theorem in his treatise "Théorie analytique de la chaleur" in the form:
which is tantamount to the introduction of the "δ"-function in the form:
Later, Augustin Cauchy expressed the theorem using exponentials:
Cauchy pointed out that in some circumstances the "order" of integration in this result was significant.
As justified using the theory of distributions, the Cauchy equation can be rearranged to resemble Fourier's original formulation and expose the "δ"-function as:
where the "δ"-function is expressed as:
A rigorous interpretation of the exponential form and the various limitations upon the function "f" necessary for its application extended over several centuries. The problems with a classical interpretation are explained as follows:
Further developments included generalization of the Fourier integral, "beginning with Plancherel's pathbreaking "L"2-theory (1910), continuing with Wiener's and Bochner's works (around 1930) and culminating with the amalgamation into L. Schwartz's theory of distributions (1945) ...", and leading to the formal development of the Dirac delta function.
An infinitesimal formula for an infinitely tall, unit impulse delta function (infinitesimal version of Cauchy distribution) explicitly appears in an 1827 text of Augustin Louis Cauchy. Siméon Denis Poisson considered the issue in connection with the study of wave propagation as did Gustav Kirchhoff somewhat later. Kirchhoff and Hermann von Helmholtz also introduced the unit impulse as a limit of Gaussians, which also corresponded to Lord Kelvin's notion of a point heat source. At the end of the 19th century, Oliver Heaviside used formal Fourier series to manipulate the unit impulse. The Dirac delta function as such was introduced as a "convenient notation" by Paul Dirac in his influential 1930 book "The Principles of Quantum Mechanics". He called it the "delta function" since he used it as a continuous analogue of the discrete Kronecker delta.
Definitions.
The Dirac delta can be loosely thought of as a function on the real line which is zero everywhere except at the origin, where it is infinite,
and which is also constrained to satisfy the identity
This is merely a heuristic characterization. The Dirac delta is not a function in the traditional sense as no function defined on the real numbers has these properties. The Dirac delta function can be rigorously defined either as a distribution or as a measure.
As a measure.
One way to rigorously define the delta function is as a measure, which accepts as an argument a subset "A" of the real line R, and returns if , and otherwise. If the delta function is conceptualized as modeling an idealized point mass at 0, then "δ"("A") represents the mass contained in the set "A". One may then define the integral against "δ" as the integral of a function against this mass distribution. Formally, the Lebesgue integral provides the necessary analytic device. The Lebesgue integral with respect to the measure "δ" satisfies
for all continuous compactly supported functions "f". The measure "δ" is not absolutely continuous with respect to the Lebesgue measure — in fact, it is a singular measure. Consequently, the delta measure has no Radon–Nikodym derivative — no true function for which the property
holds. As a result, the latter notation is a convenient abuse of notation, and not a standard (Riemann or Lebesgue) integral.
As a probability measure on R, the delta measure is characterized by its cumulative distribution function, which is the unit step function
This means that "H"("x") is the integral of the cumulative indicator function with respect to the measure "δ"; to wit,
Thus in particular the integral of the delta function against a continuous function can be properly understood as a Stieltjes integral:
All higher moments of "δ" are zero. In particular, characteristic function and moment generating function are both equal to one.
As a distribution.
In the theory of distributions a generalized function is thought of not as a function itself, but only in relation to how it affects other functions when it is "integrated" against them. In keeping with this philosophy, to define the delta function properly, it is enough to say what the "integral" of the delta function against a sufficiently "good" test function is. If the delta function is already understood as a measure, then the Lebesgue integral of a test function against that measure supplies the necessary integral.
A typical space of test functions consists of all smooth functions on R with compact support. As a distribution, the Dirac delta is a linear functional on the space of test functions and is defined by
for every test function "φ".
For "δ" to be properly a distribution, it must be "continuous" in a suitable sense. In general, for a linear functional "S" on the space of test functions to define a distribution, it is necessary and sufficient that, for every positive integer "N" there is an integer "M""N" and a constant "C""N" such that for every test function "φ", one has the inequality
With the "δ" distribution, one has such an inequality (with with for all "N". Thus "δ" is a distribution of order zero. It is, furthermore, a distribution with compact support (the support being {0}).
The delta distribution can also be defined in a number of equivalent ways. For instance, it is the distributional derivative of the Heaviside step function. This means that, for every test function "φ", one has
Intuitively, if integration by parts were permitted, then the latter integral should simplify to
and indeed, a form of integration by parts is permitted for the Stieltjes integral, and in that case one does have
In the context of measure theory, the Dirac measure gives rise to a distribution by integration. Conversely, equation () defines a Daniell integral on the space of all compactly supported continuous functions "φ" which, by the Riesz representation theorem, can be represented as the Lebesgue integral of "φ" with respect to some Radon measure.
Generalizations.
The delta function can be defined in "n"-dimensional Euclidean space R"n" as the measure such that
for every compactly supported continuous function "f". As a measure, the "n"-dimensional delta function is the product measure of the 1-dimensional delta functions in each variable separately. Thus, formally, with , one has
The delta function can also be defined in the sense of distributions exactly as above in the one-dimensional case. However, despite widespread use in engineering contexts, () should be manipulated with care, since the product of distributions can only be defined under quite narrow circumstances.
The notion of a Dirac measure makes sense on any set. Thus if "X" is a set, is a marked point, and Σ is any sigma algebra of subsets of "X", then the measure defined on sets by
is the delta measure or unit mass concentrated at "x"0.
Another common generalization of the delta function is to a differentiable manifold where most of its properties as a distribution can also be exploited because of the differentiable structure. The delta function on a manifold "M" centered at the point is defined as the following distribution:
for all compactly supported smooth real-valued functions "φ" on "M". A common special case of this construction is when "M" is an open set in the Euclidean space R"n".
On a locally compact Hausdorff space "X", the Dirac delta measure concentrated at a point "x" is the Radon measure associated with the Daniell integral () on compactly supported continuous functions "φ". At this level of generality, calculus as such is no longer possible, however a variety of techniques from abstract analysis are available. For instance, the mapping formula_19 is a continuous embedding of "X" into the space of finite Radon measures on "X", equipped with its vague topology. Moreover, the convex hull of the image of "X" under this embedding is dense in the space of probability measures on "X".
Properties.
Scaling and symmetry.
The delta function satisfies the following scaling property for a non-zero scalar α:
Then "δ" is obtained by applying a power of the Laplacian to the integral with respect to the unit sphere measure dω of for "ξ" in the unit sphere "S""n"−1:
The Laplacian here is interpreted as a weak derivative, so that this equation is taken to mean that, for any test function "φ",
The result follows from the formula for the Newtonian potential (the fundamental solution of Poisson's equation). This is essentially a form of the inversion formula for the Radon transform, because it recovers the value of "φ"("x") from its integrals over hyperplanes. For instance, if "n" is odd and , then the integral on the right hand side is
where is the Radon transform of "φ":
An alternative equivalent expression of the plane wave decomposition, from , is
for "n" even, and
for "n" odd.
Fourier kernels.
In the study of Fourier series, a major question consists of determining whether and in what sense the Fourier series associated with a periodic function converges to the function. The "n"th partial sum of the Fourier series of a function "f" of period 2π is defined by convolution (on the interval [−π,π]) with the Dirichlet kernel:
Thus,
where
A fundamental result of elementary Fourier series states that the Dirichlet kernel tends to the a multiple of the delta function as . This is interpreted in the distribution sense, that
for every compactly supported "smooth" function "f". Thus, formally one has
on the interval [−π,π].
In spite of this, the result does not hold for all compactly supported "continuous" functions: that is "DN" does not converge weakly in the sense of measures. The lack of convergence of the Fourier series has led to the introduction of a variety of summability methods in order to produce convergence. The method of Cesàro summation leads to the Fejér kernel
The Fejér kernels tend to the delta function in a stronger sense that
for every compactly supported "continuous" function "f". The implication is that the Fourier series of any continuous function is Cesàro summable to the value of the function at every point.
Hilbert space theory.
The Dirac delta distribution is a densely defined unbounded linear functional on the Hilbert space L2 of square integrable functions. Indeed, smooth compactly support functions are dense in "L"2, and the action of the delta distribution on such functions is well-defined. In many applications, it is possible to identify subspaces of "L"2 and to give a stronger topology on which the delta function defines a bounded linear functional.
The Sobolev embedding theorem for Sobolev spaces on the real line R implies that any square-integrable function "f" such that
is automatically continuous, and satisfies in particular
Thus "δ" is a bounded linear functional on the Sobolev space "H"1. Equivalently "δ" is an element of the continuous dual space "H"−1 of "H"1. More generally, in "n" dimensions, one has provided .
Spaces of holomorphic functions.
In complex analysis, the delta function enters via Cauchy's integral formula which asserts that if "D" is a domain in the complex plane with smooth boundary, then
for all holomorphic functions "f" in "D" that are continuous on the closure of "D". As a result, the delta function "δ""z" is represented on this class of holomorphic functions by the Cauchy integral:
More generally, let "H"2(∂"D") be the Hardy space consisting of the closure in "L"2(∂"D") of all holomorphic functions in "D" continuous up to the boundary of "D". Then functions in "H"2(∂"D") uniquely extend to holomorphic functions in "D", and the Cauchy integral formula continues to hold. In particular for , the delta function "δ""z" is a continuous linear functional on "H"2(∂"D"). This is a special case of the situation in several complex variables in which, for smooth domains "D", the Szegő kernel plays the role of the Cauchy integral.
Resolutions of the identity.
Given a complete orthonormal basis set of functions {"φ""n"} in a separable Hilbert space, for example, the normalized eigenvectors of a compact self-adjoint operator, any vector "f" can be expressed as:
The coefficients {αn} are found as:
which may be represented by the notation:
a form of the bra–ket notation of Dirac. Adopting this notation, the expansion of "f" takes the dyadic form:
Letting "I" denote the identity operator on the Hilbert space, the expression
is called a resolution of the identity. When the Hilbert space is the space "L"2("D") of square-integrable functions on a domain "D", the quantity:
is an integral operator, and the expression for "f" can be rewritten as:
The right-hand side converges to "f" in the "L"2 sense. It need not hold in a pointwise sense, even when "f" is a continuous function. Nevertheless, it is common to abuse notation and write
resulting in the representation of the delta function:
With a suitable rigged Hilbert space where contains all compactly supported smooth functions, this summation may converge in Φ*, depending on the properties of the basis "φ""n". In most cases of practical interest, the orthonormal basis comes from an integral or differential operator, in which case the series converges in the distribution sense.
Infinitesimal delta functions.
Cauchy used an infinitesimal α to write down a unit impulse, infinitely tall and narrow Dirac-type delta function "δα" satisfying formula_47 in a number of articles in 1827. Cauchy defined an infinitesimal in Cours d'Analyse (1827) in terms of a sequence tending to zero. Namely, such a null sequence becomes an infinitesimal in Cauchy's and Lazare Carnot's terminology.
Non-standard analysis allows one to rigorously treat infinitesimals. The article by contains a bibliography on modern Dirac delta functions in the context of an infinitesimal-enriched continuum provided by the hyperreals. Here the Dirac delta can be given by an actual function, having the property that for every real function "F" one has formula_47 as anticipated by Fourier and Cauchy.
Dirac comb.
A so-called uniform "pulse train" of Dirac delta measures, which is known as a Dirac comb, or as the Shah distribution, creates a sampling function, often used in digital signal processing (DSP) and discrete time signal analysis. The Dirac comb is given as the infinite sum, whose limit is understood in the distribution sense,
which is a sequence of point masses at each of the integers.
Up to an overall normalizing constant, the Dirac comb is equal to its own Fourier transform. This is significant because if "f" is any Schwartz function, then the periodization of "f" is given by the convolution
In particular,
is precisely the Poisson summation formula.
Sokhotski–Plemelj theorem.
The Sokhotski–Plemelj theorem, important in quantum mechanics, relates the delta function to the distribution p.v.1/"x", the Cauchy principal value of the function 1/"x", defined by
Sokhotsky's formula states that
Here the limit is understood in the distribution sense, that for all compactly supported smooth functions "f",
Relationship to the Kronecker delta.
The Kronecker delta "δij" is the quantity defined by
for all integers "i", "j". This function then satisfies the following analog of the sifting property: if formula_56 is any doubly infinite sequence, then
Similarly, for any real or complex valued continuous function "f" on R, the Dirac delta satisfies the sifting property
This exhibits the Kronecker delta function as a discrete analog of the Dirac delta function.
Applications.
Probability theory.
In probability theory and statistics, the Dirac delta function is often used to represent a discrete distribution, or a partially discrete, partially continuous distribution, using a probability density function (which is normally used to represent fully continuous distributions). For example, the probability density function "f"("x") of a discrete distribution consisting of points x = {"x"1, ..., "xn"}, with corresponding probabilities "p"1, ..., "pn", can be written as
As another example, consider a distribution which 6/10 of the time returns a standard normal distribution, and 4/10 of the time returns exactly the value 3.5 (i.e. a partly continuous, partly discrete mixture distribution). The density function of this distribution can be written as
The delta function is also used in a completely different way to represent the local time of a diffusion process (like Brownian motion). The local time of a stochastic process "B"("t") is given by
and represents the amount of time that the process spends at the point "x" in the range of the process. More precisely, in one dimension this integral can be written
where is the indicator function of the interval .
Quantum mechanics.
We give an example of how the delta function is expedient in quantum mechanics. The wave function of a particle gives the probability amplitude of finding a particle within a given region of space. Wave functions are assumed to be elements of the Hilbert space "L"2 of square-integrable functions, and the total probability of finding a particle within a given interval is the integral of the magnitude of the wave function squared over the interval. A set {"φ""n"} of wave functions is orthonormal if they are normalized by
where "δ" here refers to the Kronecker delta. A set of orthonormal wave functions is complete in the space of square-integrable functions if any wave function "ψ" can be expressed as a combination of the "φ""n":
with formula_65. Complete orthonormal systems of wave functions appear naturally as the eigenfunctions of the Hamiltonian (of a bound system) in quantum mechanics that measures the energy levels, which are called the eigenvalues. The set of eigenvalues, in this case, is known as the spectrum of the Hamiltonian. In bra–ket notation, as above, this equality implies the resolution of the identity:
Here the eigenvalues are assumed to be discrete, but the set of eigenvalues of an observable may be continuous rather than discrete. An example is the position observable, . The spectrum of the position (in one dimension) is the entire real line, and is called a continuous spectrum. However, unlike the Hamiltonian, the position operator lacks proper eigenfunctions. The conventional way to overcome this shortcoming is to widen the class of available functions by allowing distributions as well: that is, to replace the Hilbert space of quantum mechanics by an appropriate rigged Hilbert space. In this context, the position operator has a complete set of eigen-distributions, labeled by the points "y" of the real line, given by
The eigenfunctions of position are denoted by formula_68 in Dirac notation, and are known as position eigenstates.
Similar considerations apply to the eigenstates of the momentum operator, or indeed any other self-adjoint unbounded operator "P" on the Hilbert space, provided the spectrum of "P" is continuous and there are no degenerate eigenvalues. In that case, there is a set Ω of real numbers (the spectrum), and a collection "φ""y" of distributions indexed by the elements of Ω, such that
That is, "φ""y" are the eigenvectors of "P". If the eigenvectors are normalized so that
in the distribution sense, then for any test function ψ,
where
That is, as in the discrete case, there is a resolution of the identity
where the operator-valued integral is again understood in the weak sense. If the spectrum of "P" has both continuous and discrete parts, then the resolution of the identity involves a summation over the discrete spectrum "and" an integral over the continuous spectrum.
The delta function also has many more specialized applications in quantum mechanics, such as the delta potential models for a single and double potential well.
Structural mechanics.
The delta function can be used in structural mechanics to describe transient loads or point loads acting on structures. The governing equation of a simple mass–spring system excited by a sudden force impulse "I" at time "t" = 0 can be written
where "m" is the mass, ξ the deflection and "k" the spring constant.
As another example, the equation governing the static deflection of a slender beam is, according to Euler–Bernoulli theory,
where "EI" is the bending stiffness of the beam, "w" the deflection, "x" the spatial coordinate and "q"("x") the load distribution. If a beam is loaded by a point force "F" at "x" = "x"0, the load distribution is written
As integration of the delta function results in the Heaviside step function, it follows that the static deflection of a slender beam subject to multiple point loads is described by a set of piecewise polynomials.
Also a point moment acting on a beam can be described by delta functions. Consider two opposing point forces "F" at a distance "d" apart. They then produce a moment "M" = "Fd" acting on the beam. Now, let the distance "d" approach the limit zero, while "M" is kept constant. The load distribution, assuming a clockwise moment acting at "x" = 0, is written
Point moments can thus be represented by the derivative of the delta function. Integration of the beam equation again results in piecewise polynomial deflection.

</doc>
