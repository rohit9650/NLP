{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_assignment_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohit9650/NLP/blob/master/Assignment%202/nlp_assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "4HCE18fPjNo2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "NLP assignment 2\n",
        "Problem statement and dataset : https://github.com/rohit9650/NLP/tree/master/Assignment%201\n",
        "\n",
        "author : Rohit Singh (MCS201812)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vnDpIGGPjrBi",
        "colab_type": "code",
        "outputId": "5071858a-538e-4dbd-a465-ab059c40b2cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import nltk\n",
        "from nltk import bigrams    \n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import os\n",
        "import re\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import random\n",
        "from scipy import spatial"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Mb_GAlIgkNij",
        "colab_type": "code",
        "outputId": "0a1fc413-b22e-4952-c462-e7b7f224b6b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "cell_type": "code",
      "source": [
        "# downloading corpus\n",
        "! wget -O corpus.zip \"https://github.com/rohit9650/NLP/blob/master/Assignment%201/corpus.zip?raw=true\" "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-15 08:56:16--  https://github.com/rohit9650/NLP/blob/master/Assignment%201/corpus.zip?raw=true\n",
            "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/rohit9650/NLP/raw/master/Assignment%201/corpus.zip [following]\n",
            "--2019-03-15 08:56:17--  https://github.com/rohit9650/NLP/raw/master/Assignment%201/corpus.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/rohit9650/NLP/master/Assignment%201/corpus.zip [following]\n",
            "--2019-03-15 08:56:17--  https://raw.githubusercontent.com/rohit9650/NLP/master/Assignment%201/corpus.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 89709 (88K) [application/zip]\n",
            "Saving to: ‘corpus.zip’\n",
            "\n",
            "\rcorpus.zip            0%[                    ]       0  --.-KB/s               \rcorpus.zip          100%[===================>]  87.61K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-03-15 08:56:17 (3.50 MB/s) - ‘corpus.zip’ saved [89709/89709]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nLE7gOVLkQET",
        "colab_type": "code",
        "outputId": "98b9cc08-bf21-4823-95d4-55a2cd092ada",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# listing files in directory \"corpus.zip\" must be there now\n",
        "os.listdir(\"/content/\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'corpus.zip', 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "COiqAIBJkVGl",
        "colab_type": "code",
        "outputId": "034384e7-61f8-4922-e8a3-e6b544032a08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4931
        }
      },
      "cell_type": "code",
      "source": [
        "# unzipping the corpus\n",
        "!unzip '/content/corpus.zip'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/corpus.zip\n",
            "   creating: corpus/\n",
            "  inflating: corpus/filename0000.txt  \n",
            "  inflating: corpus/filename0114.txt  \n",
            "  inflating: corpus/filename0001.txt  \n",
            "  inflating: corpus/filename0002.txt  \n",
            "  inflating: corpus/filename0003.txt  \n",
            "  inflating: corpus/filename0004.txt  \n",
            "  inflating: corpus/filename0005.txt  \n",
            "  inflating: corpus/filename0006.txt  \n",
            "  inflating: corpus/filename0007.txt  \n",
            "  inflating: corpus/filename0008.txt  \n",
            "  inflating: corpus/filename0009.txt  \n",
            "  inflating: corpus/filename0010.txt  \n",
            "  inflating: corpus/filename0011.txt  \n",
            "  inflating: corpus/filename0012.txt  \n",
            "  inflating: corpus/filename0013.txt  \n",
            "  inflating: corpus/filename0014.txt  \n",
            "  inflating: corpus/filename0015.txt  \n",
            "  inflating: corpus/filename0016.txt  \n",
            "  inflating: corpus/filename0017.txt  \n",
            "  inflating: corpus/filename0018.txt  \n",
            "  inflating: corpus/filename0019.txt  \n",
            "  inflating: corpus/filename0020.txt  \n",
            "  inflating: corpus/filename0021.txt  \n",
            "  inflating: corpus/filename0022.txt  \n",
            "  inflating: corpus/filename0023.txt  \n",
            "  inflating: corpus/filename0024.txt  \n",
            "  inflating: corpus/filename0025.txt  \n",
            "  inflating: corpus/filename0026.txt  \n",
            "  inflating: corpus/filename0027.txt  \n",
            "  inflating: corpus/filename0028.txt  \n",
            "  inflating: corpus/filename0029.txt  \n",
            "  inflating: corpus/filename0030.txt  \n",
            "  inflating: corpus/filename0031.txt  \n",
            "  inflating: corpus/filename0032.txt  \n",
            "  inflating: corpus/filename0033.txt  \n",
            "  inflating: corpus/filename0034.txt  \n",
            "  inflating: corpus/filename0035.txt  \n",
            "  inflating: corpus/filename0036.txt  \n",
            "  inflating: corpus/filename0037.txt  \n",
            "  inflating: corpus/filename0038.txt  \n",
            "  inflating: corpus/filename0039.txt  \n",
            "  inflating: corpus/filename0040.txt  \n",
            "  inflating: corpus/filename0041.txt  \n",
            "  inflating: corpus/filename0042.txt  \n",
            "  inflating: corpus/filename0043.txt  \n",
            "  inflating: corpus/filename0044.txt  \n",
            "  inflating: corpus/filename0045.txt  \n",
            "  inflating: corpus/filename0046.txt  \n",
            "  inflating: corpus/filename0047.txt  \n",
            "  inflating: corpus/filename0048.txt  \n",
            "  inflating: corpus/filename0049.txt  \n",
            "  inflating: corpus/filename0050.txt  \n",
            "  inflating: corpus/filename0051.txt  \n",
            "  inflating: corpus/filename0052.txt  \n",
            "  inflating: corpus/filename0053.txt  \n",
            "  inflating: corpus/filename0054.txt  \n",
            "  inflating: corpus/filename0055.txt  \n",
            "  inflating: corpus/filename0056.txt  \n",
            "  inflating: corpus/filename0057.txt  \n",
            "  inflating: corpus/filename0058.txt  \n",
            "  inflating: corpus/filename0059.txt  \n",
            "  inflating: corpus/filename0060.txt  \n",
            "  inflating: corpus/filename0061.txt  \n",
            "  inflating: corpus/filename0062.txt  \n",
            "  inflating: corpus/filename0063.txt  \n",
            "  inflating: corpus/filename0064.txt  \n",
            "  inflating: corpus/filename0065.txt  \n",
            "  inflating: corpus/filename0066.txt  \n",
            "  inflating: corpus/filename0067.txt  \n",
            "  inflating: corpus/filename0068.txt  \n",
            "  inflating: corpus/filename0069.txt  \n",
            "  inflating: corpus/filename0070.txt  \n",
            "  inflating: corpus/filename0071.txt  \n",
            "  inflating: corpus/filename0072.txt  \n",
            "  inflating: corpus/filename0073.txt  \n",
            "  inflating: corpus/filename0074.txt  \n",
            "  inflating: corpus/filename0075.txt  \n",
            "  inflating: corpus/filename0076.txt  \n",
            "  inflating: corpus/filename0077.txt  \n",
            "  inflating: corpus/filename0078.txt  \n",
            "  inflating: corpus/filename0079.txt  \n",
            "  inflating: corpus/filename0080.txt  \n",
            "  inflating: corpus/filename0081.txt  \n",
            "  inflating: corpus/filename0082.txt  \n",
            "  inflating: corpus/filename0083.txt  \n",
            "  inflating: corpus/filename0084.txt  \n",
            "  inflating: corpus/filename0085.txt  \n",
            "  inflating: corpus/filename0086.txt  \n",
            "  inflating: corpus/filename0087.txt  \n",
            "  inflating: corpus/filename0088.txt  \n",
            "  inflating: corpus/filename0089.txt  \n",
            "  inflating: corpus/filename0090.txt  \n",
            "  inflating: corpus/filename0091.txt  \n",
            "  inflating: corpus/filename0092.txt  \n",
            "  inflating: corpus/filename0093.txt  \n",
            "  inflating: corpus/filename0094.txt  \n",
            "  inflating: corpus/filename0095.txt  \n",
            "  inflating: corpus/filename0096.txt  \n",
            "  inflating: corpus/filename0097.txt  \n",
            "  inflating: corpus/filename0098.txt  \n",
            "  inflating: corpus/filename0099.txt  \n",
            "  inflating: corpus/filename0100.txt  \n",
            "  inflating: corpus/filename0101.txt  \n",
            "  inflating: corpus/filename0102.txt  \n",
            "  inflating: corpus/filename0103.txt  \n",
            "  inflating: corpus/filename0104.txt  \n",
            "  inflating: corpus/filename0105.txt  \n",
            "  inflating: corpus/filename0106.txt  \n",
            "  inflating: corpus/filename0107.txt  \n",
            "  inflating: corpus/filename0108.txt  \n",
            "  inflating: corpus/filename0109.txt  \n",
            "  inflating: corpus/filename0110.txt  \n",
            "  inflating: corpus/filename0111.txt  \n",
            "  inflating: corpus/filename0112.txt  \n",
            "  inflating: corpus/filename0113.txt  \n",
            "  inflating: corpus/filename0115.txt  \n",
            "  inflating: corpus/filename0116.txt  \n",
            "  inflating: corpus/filename0117.txt  \n",
            "  inflating: corpus/filename0118.txt  \n",
            "  inflating: corpus/filename0119.txt  \n",
            "  inflating: corpus/filename0120.txt  \n",
            "  inflating: corpus/filename0121.txt  \n",
            "  inflating: corpus/filename0122.txt  \n",
            "  inflating: corpus/filename0123.txt  \n",
            "  inflating: corpus/filename0124.txt  \n",
            "  inflating: corpus/filename0125.txt  \n",
            "  inflating: corpus/filename0126.txt  \n",
            "  inflating: corpus/filename0127.txt  \n",
            "  inflating: corpus/filename0128.txt  \n",
            "  inflating: corpus/filename0129.txt  \n",
            "  inflating: corpus/filename0130.txt  \n",
            "  inflating: corpus/filename0131.txt  \n",
            "  inflating: corpus/filename0132.txt  \n",
            "  inflating: corpus/filename0133.txt  \n",
            "  inflating: corpus/filename0134.txt  \n",
            "  inflating: corpus/filename0135.txt  \n",
            "  inflating: corpus/filename0136.txt  \n",
            "  inflating: corpus/filename0137.txt  \n",
            "  inflating: corpus/filename0138.txt  \n",
            "  inflating: corpus/filename0139.txt  \n",
            "  inflating: corpus/filename0140.txt  \n",
            "  inflating: corpus/filename0141.txt  \n",
            "  inflating: corpus/filename0142.txt  \n",
            "  inflating: corpus/filename0143.txt  \n",
            "  inflating: corpus/filename0144.txt  \n",
            "  inflating: corpus/filename0145.txt  \n",
            "  inflating: corpus/filename0146.txt  \n",
            "  inflating: corpus/filename0147.txt  \n",
            "  inflating: corpus/filename0148.txt  \n",
            "  inflating: corpus/filename0149.txt  \n",
            "  inflating: corpus/filename0150.txt  \n",
            "  inflating: corpus/filename0151.txt  \n",
            "  inflating: corpus/filename0152.txt  \n",
            "  inflating: corpus/filename0153.txt  \n",
            "  inflating: corpus/filename0154.txt  \n",
            "  inflating: corpus/filename0155.txt  \n",
            "  inflating: corpus/filename0156.txt  \n",
            "  inflating: corpus/filename0157.txt  \n",
            "  inflating: corpus/filename0158.txt  \n",
            "  inflating: corpus/filename0159.txt  \n",
            "  inflating: corpus/filename0160.txt  \n",
            "  inflating: corpus/filename0161.txt  \n",
            "  inflating: corpus/filename0162.txt  \n",
            "  inflating: corpus/filename0163.txt  \n",
            "  inflating: corpus/filename0164.txt  \n",
            "  inflating: corpus/filename0165.txt  \n",
            "  inflating: corpus/filename0166.txt  \n",
            "  inflating: corpus/filename0167.txt  \n",
            "  inflating: corpus/filename0168.txt  \n",
            "  inflating: corpus/filename0169.txt  \n",
            "  inflating: corpus/filename0170.txt  \n",
            "  inflating: corpus/filename0171.txt  \n",
            "  inflating: corpus/filename0172.txt  \n",
            "  inflating: corpus/filename0173.txt  \n",
            "  inflating: corpus/filename0174.txt  \n",
            "  inflating: corpus/filename0175.txt  \n",
            "  inflating: corpus/filename0176.txt  \n",
            "  inflating: corpus/filename0177.txt  \n",
            "  inflating: corpus/filename0178.txt  \n",
            "  inflating: corpus/filename0179.txt  \n",
            "  inflating: corpus/filename0180.txt  \n",
            "  inflating: corpus/filename0181.txt  \n",
            "  inflating: corpus/filename0182.txt  \n",
            "  inflating: corpus/filename0183.txt  \n",
            "  inflating: corpus/filename0184.txt  \n",
            "  inflating: corpus/filename0185.txt  \n",
            "  inflating: corpus/filename0186.txt  \n",
            "  inflating: corpus/filename0187.txt  \n",
            "  inflating: corpus/filename0188.txt  \n",
            "  inflating: corpus/filename0189.txt  \n",
            "  inflating: corpus/filename0190.txt  \n",
            "  inflating: corpus/filename0191.txt  \n",
            "  inflating: corpus/filename0192.txt  \n",
            "  inflating: corpus/filename0193.txt  \n",
            "  inflating: corpus/filename0194.txt  \n",
            "  inflating: corpus/filename0195.txt  \n",
            "  inflating: corpus/filename0196.txt  \n",
            "  inflating: corpus/filename0197.txt  \n",
            "  inflating: corpus/filename0198.txt  \n",
            "  inflating: corpus/filename0199.txt  \n",
            "  inflating: corpus/filename0200.txt  \n",
            "  inflating: corpus/filename0201.txt  \n",
            "  inflating: corpus/filename0202.txt  \n",
            "  inflating: corpus/filename0203.txt  \n",
            "  inflating: corpus/filename0204.txt  \n",
            "  inflating: corpus/filename0205.txt  \n",
            "  inflating: corpus/filename0206.txt  \n",
            "  inflating: corpus/filename0207.txt  \n",
            "  inflating: corpus/filename0208.txt  \n",
            "  inflating: corpus/filename0209.txt  \n",
            "  inflating: corpus/filename0210.txt  \n",
            "  inflating: corpus/filename0211.txt  \n",
            "  inflating: corpus/filename0212.txt  \n",
            "  inflating: corpus/filename0213.txt  \n",
            "  inflating: corpus/filename0214.txt  \n",
            "  inflating: corpus/filename0215.txt  \n",
            "  inflating: corpus/filename0216.txt  \n",
            "  inflating: corpus/filename0217.txt  \n",
            "  inflating: corpus/filename0218.txt  \n",
            "  inflating: corpus/filename0219.txt  \n",
            "  inflating: corpus/filename0220.txt  \n",
            "  inflating: corpus/filename0221.txt  \n",
            "  inflating: corpus/filename0222.txt  \n",
            "  inflating: corpus/filename0223.txt  \n",
            "  inflating: corpus/filename0224.txt  \n",
            "  inflating: corpus/filename0225.txt  \n",
            "  inflating: corpus/filename0226.txt  \n",
            "  inflating: corpus/filename0227.txt  \n",
            "  inflating: corpus/filename0228.txt  \n",
            "  inflating: corpus/filename0229.txt  \n",
            "  inflating: corpus/filename0230.txt  \n",
            "  inflating: corpus/filename0231.txt  \n",
            "  inflating: corpus/filename0232.txt  \n",
            "  inflating: corpus/filename0233.txt  \n",
            "  inflating: corpus/filename0234.txt  \n",
            "  inflating: corpus/filename0235.txt  \n",
            "  inflating: corpus/filename0236.txt  \n",
            "  inflating: corpus/filename0237.txt  \n",
            "  inflating: corpus/filename0238.txt  \n",
            "  inflating: corpus/filename0239.txt  \n",
            "  inflating: corpus/filename0240.txt  \n",
            "  inflating: corpus/filename0241.txt  \n",
            "  inflating: corpus/filename0242.txt  \n",
            "  inflating: corpus/filename0243.txt  \n",
            "  inflating: corpus/filename0244.txt  \n",
            "  inflating: corpus/filename0245.txt  \n",
            "  inflating: corpus/filename0246.txt  \n",
            "  inflating: corpus/filename0247.txt  \n",
            "  inflating: corpus/filename0248.txt  \n",
            "  inflating: corpus/filename0249.txt  \n",
            "  inflating: corpus/filename0250.txt  \n",
            "  inflating: corpus/filename0251.txt  \n",
            "  inflating: corpus/filename0252.txt  \n",
            "  inflating: corpus/filename0253.txt  \n",
            "  inflating: corpus/filename0254.txt  \n",
            "  inflating: corpus/filename0255.txt  \n",
            "  inflating: corpus/filename0256.txt  \n",
            "  inflating: corpus/filename0257.txt  \n",
            "  inflating: corpus/filename0258.txt  \n",
            "  inflating: corpus/filename0259.txt  \n",
            "  inflating: corpus/filename0260.txt  \n",
            "  inflating: corpus/filename0261.txt  \n",
            "  inflating: corpus/filename0262.txt  \n",
            "  inflating: corpus/filename0263.txt  \n",
            "  inflating: corpus/filename0264.txt  \n",
            "  inflating: corpus/filename0265.txt  \n",
            "  inflating: corpus/filename0266.txt  \n",
            "  inflating: corpus/filename0267.txt  \n",
            "  inflating: corpus/filename0268.txt  \n",
            "  inflating: corpus/filename0269.txt  \n",
            "  inflating: corpus/filename0270.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "56tHZB7Ljw7w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_files(dir_path):\n",
        "    \"\"\"\n",
        "    return the list of all files present in dir_path\n",
        "    \"\"\"\n",
        "    files = []\n",
        "    for file in sorted(os.listdir(dir_path)):\n",
        "        files.append(file)\n",
        "    return files\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "43IZXB2Qj2Wn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_data(doc_set):\n",
        "    \"\"\"\n",
        "    Input  : document list\n",
        "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
        "    Output : preprocessed text as list and list of list\n",
        "    \"\"\"\n",
        "    # initialize regex tokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    # create English stop words list\n",
        "    en_stop = set(stopwords.words('english'))\n",
        "    # Create p_stemmer of class PorterStemmer\n",
        "    p_stemmer = PorterStemmer()\n",
        "    # list for tokenized documents in loop\n",
        "    text_list = []\n",
        "    text_list_of_list = []\n",
        "    # loop through document list\n",
        "    for file in doc_set:\n",
        "        file_path = '/content/corpus' + \"/\" + file\n",
        "        fp = open(file_path,\"r\")\n",
        "        i = fp.read()\n",
        "        # remove numbers\n",
        "        i = re.sub(r'\\d+', '', i)\n",
        "        # clean and tokenize document string\n",
        "        raw = i.lower()\n",
        "        tokens = tokenizer.tokenize(raw)\n",
        "        # remove stop words from tokens\n",
        "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
        "        # stem tokens\n",
        "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
        "        # add tokens to list\n",
        "        text_list = text_list + stemmed_tokens\n",
        "        text_list_of_list.append(stemmed_tokens)\n",
        "    return text_list, text_list_of_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SoXatHFJj3-m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def co_occurrence_matrix(corpus):\n",
        "    vocab = set(corpus)\n",
        "    vocab = list(vocab)\n",
        "\n",
        "    # Key:Value = Word:Index\n",
        "    vocab_to_index = { word:i for i, word in enumerate(vocab) }\n",
        "\n",
        "    # Create bigrams from all words in corpus\n",
        "    bi_grams = list(bigrams(corpus))\n",
        "    # Frequency distribution of bigrams ((word1, word2), num_occurrences)\n",
        "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
        "    # Initialise co-occurrence matrix\n",
        "\n",
        "    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "\n",
        "    # Loop through the bigrams in the frequency distribution, noting the \n",
        "    # current and previous word, and the number of occurrences of \n",
        "    # the bigram.\n",
        "    # Get the vocab index of the current and previous words.\n",
        "    # Put the number of occurrences into the appropriate element \n",
        "    # of the array.\n",
        "    for bigram in bigram_freq:\n",
        "        current = bigram[0][1]\n",
        "        previous = bigram[0][0]\n",
        "        count = bigram[1]\n",
        "        pos_current = vocab_to_index[current]\n",
        "        pos_previous = vocab_to_index[previous]\n",
        "        co_occurrence_matrix[pos_current][pos_previous] = count\n",
        "        co_occurrence_matrix[pos_previous][pos_current] = count\n",
        "    co_occurrence_matrix = np.matrix(co_occurrence_matrix)\n",
        "    \n",
        "    return co_occurrence_matrix, vocab_to_index\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Lz4E-rXj-9G",
        "colab_type": "code",
        "outputId": "4036b045-c75b-4426-eebd-3a7e2571da4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6803
        }
      },
      "cell_type": "code",
      "source": [
        "path = '/content/corpus'\n",
        "corpus_as_list, corpus_as_list_of_list = preprocess_data(get_files(path))\n",
        "m, vocab_to_index = co_occurrence_matrix(corpus_as_list)\n",
        "\n",
        "#%%\n",
        "word_indices = [random.randint(1,754) for x in range(10)]\n",
        "words = []\n",
        "for x in range(len(word_indices)):\n",
        "    for word, i in vocab_to_index.items():\n",
        "        if i == word_indices[x]:\n",
        "            words.append(word)\n",
        "\n",
        "#for measuring the similarity between these words\n",
        "word1 = 'away'\n",
        "word2 = 'east'\n",
        "\n",
        "print('shape of word co-occurrence matrix before redcution : {}'.\n",
        "      format(m.shape))\n",
        "#word vectors before reduction \n",
        "for x in range(len(words)):\n",
        "    print(words[x] + ' : ')\n",
        "    print(m[word_indices[x]][:])\n",
        "\n",
        "svd = TruncatedSVD(n_components=15, n_iter=7, random_state=42)\n",
        "m = svd.fit_transform(m)\n",
        "print('shape of word co-occurrence matrix after redcution : {}'.\n",
        "      format(m.shape))\n",
        "\n",
        "\n",
        "#word vectors After reduction \n",
        "for x in range(len(words)):\n",
        "    print(words[x] + ' : ')\n",
        "    print(m[word_indices[x]][:])\n",
        "\n",
        "word1_vec = m[vocab_to_index[word1]][:]\n",
        "word2_vec = m[vocab_to_index[word2]][:]\n",
        "similarity = 1 - spatial.distance.cosine(word1_vec, word2_vec)\n",
        "\n",
        "print('similarity between words using lsi : {}'.format(similarity))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of word co-occurrence matrix before redcution : (753, 753)\n",
            "hail : \n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "bicycl : \n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "wherea : \n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "upon : \n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "per : \n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "coast : \n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "everi : \n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "diego : \n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "penetr : \n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "hotel : \n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "shape of word co-occurrence matrix after redcution : (753, 15)\n",
            "hail : \n",
            "[ 0.0929018   0.03392241  0.05896904 -0.07078871  0.08386746  0.01342007\n",
            " -0.02605224  0.07654939 -0.05071615 -0.04882734  0.23663453 -0.01955656\n",
            "  0.0461406   0.0715928  -0.10786063]\n",
            "bicycl : \n",
            "[ 0.66314325 -0.00166714  0.25996346 -0.08549458  0.00114955 -0.04983074\n",
            "  0.75779489 -0.26115851  0.48153701  0.50532283  0.01591168 -0.62852601\n",
            "  0.17385405 -0.12405069 -0.06194116]\n",
            "wherea : \n",
            "[ 0.20637789  0.00731272 -0.04058748  0.02017396  0.1038645  -0.12232417\n",
            "  0.49602203 -0.52685242  0.13213004  0.36138999  0.03224766 -0.08969834\n",
            "  0.00075455 -0.23382852 -0.13216739]\n",
            "upon : \n",
            "[ 0.01055153 -0.0014703   0.00233362  0.00423264  0.0274238   0.02080767\n",
            "  0.0226515   0.00448162 -0.00133604 -0.00467244 -0.00283737  0.00212025\n",
            " -0.00140565 -0.02350799  0.00601004]\n",
            "per : \n",
            "[ 3.21049478e-01 -4.17865876e-02  3.17488978e-01 -1.36471972e-01\n",
            " -4.44149732e-02  5.25267725e-02  1.91949710e-01  2.05935333e-01\n",
            "  2.00593937e-01  7.97710046e-02  3.93176205e-04 -5.04788926e-01\n",
            "  1.79221530e-01  1.60361515e-01 -1.89746360e-01]\n",
            "coast : \n",
            "[ 7.03601157e-06  5.52396770e-06 -1.56451167e-06  8.63025685e-06\n",
            "  3.36155082e-05 -5.26331121e-05 -1.96754697e-05  1.02467872e-04\n",
            " -1.18991285e-05 -1.50838656e-05 -1.08072842e-04 -2.54828466e-05\n",
            "  5.92476700e-05  7.21761408e-06  1.34846176e-04]\n",
            "everi : \n",
            "[ 6.83289319e-04 -7.70196432e-04 -6.75786612e-04 -1.14044869e-04\n",
            "  1.19587349e-03 -9.25914835e-04  7.80627194e-05 -1.79564664e-04\n",
            " -8.01760409e-04  6.07922354e-04  1.86355745e-03  2.47588246e-03\n",
            "  1.37097158e-04  3.81828421e-03 -1.66994170e-03]\n",
            "diego : \n",
            "[ 4.27085543e-04 -6.15315717e-04 -5.38582067e-04  2.07145284e-04\n",
            " -2.96909838e-04 -2.78758687e-04 -4.97385645e-04 -2.07483248e-04\n",
            " -1.78205476e-04 -6.94943032e-05  2.15008287e-03  3.11070914e-04\n",
            "  7.35616735e-04 -3.20243743e-04 -1.15208414e-04]\n",
            "penetr : \n",
            "[ 0.2396541   0.01654062  0.07472833 -0.02445094  0.02914791  0.10424476\n",
            "  0.32489538  0.05350412  0.20752688 -0.2578449  -0.01832795  0.06099775\n",
            " -0.02416105 -0.11846845  0.40028397]\n",
            "hotel : \n",
            "[ 0.17942161  0.0250193   0.21145427  0.11407901  0.09875476  0.0786457\n",
            " -0.06509754 -0.02022684  0.01668313 -0.03389771  0.56371742  0.06057376\n",
            " -0.53600218  0.31395637  0.77799635]\n",
            "similarity between words using lsi : 0.14947796570253913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nPIg3tPnk1hb",
        "colab_type": "code",
        "outputId": "393649b8-36d2-48ce-d3bb-b5ceb424a2d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        }
      },
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "documents = corpus_as_list_of_list\n",
        "\n",
        "model = Word2Vec(\n",
        "        documents,\n",
        "        size=15,\n",
        "        window=10,\n",
        "        min_count=2,\n",
        "        workers=10)\n",
        "model.train(documents, total_examples=len(documents), epochs=10)\n",
        "vocab = list(model.wv.vocab.keys())\n",
        "print(len(vocab))\n",
        "for word in words:\n",
        "    if word in vocab:\n",
        "        print(word + ' : ')\n",
        "        print(model[word])\n",
        "\n",
        "print('similarity between words using word2vec : {}'.\n",
        "      format(model.similarity(word1, word2)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "450\n",
            "hail : \n",
            "[ 0.20906258  0.05950342 -0.07988214 -0.06658026  0.13358952  0.00035428\n",
            " -0.05969271 -0.15774333  0.01821885 -0.13289729 -0.04284043  0.29175216\n",
            "  0.05973857 -0.00637272  0.01252111]\n",
            "bicycl : \n",
            "[ 0.5771764   0.20837347 -0.15113893 -0.13065235  0.3756109  -0.02872476\n",
            " -0.19925728 -0.4080223   0.05446416 -0.29256335 -0.14098185  0.6550672\n",
            "  0.18975034 -0.04173634  0.03107618]\n",
            "diego : \n",
            "[ 0.6371439   0.22904915 -0.17735295 -0.11534565  0.40158758 -0.04777213\n",
            " -0.23109686 -0.42678824  0.0756458  -0.27436277 -0.11867758  0.72777426\n",
            "  0.19806175 -0.04336616  0.0734105 ]\n",
            "penetr : \n",
            "[ 0.30855244  0.10343279 -0.07725293 -0.08205175  0.19311269 -0.04608217\n",
            " -0.10704585 -0.27726462  0.04799226 -0.19077039 -0.07453614  0.39717054\n",
            "  0.11385931  0.03085589  0.02081998]\n",
            "hotel : \n",
            "[ 1.1256202   0.37362963 -0.2911832  -0.21681093  0.7195874  -0.02019183\n",
            " -0.39723948 -0.836242    0.16197456 -0.5240559  -0.24800105  1.3288405\n",
            "  0.31662232 -0.06050669  0.08705126]\n",
            "similarity between words using word2vec : 0.9983783960342407\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "D219vMJOoF4I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**Question 3**\n",
        "\n",
        "##*Is factoring a matrix same as back-propagation error mechanism to update weights. If yes or no, why?*\n",
        "\n",
        "#**Answer  : **\n",
        "A word embedding method which uses back-propagation introduced by Mikolov et al.,  is implicitly factorizing\n",
        "a word-context matrix, whose cells are the pointwise mutual information (PMI) of\n",
        "the respective word and context pairs, shifted by a global constant.\n",
        "\n",
        "###Skip-Gram with Negative Sampling (SGNS)\n",
        "\n",
        "####**Setting and Notation** \n",
        "The skip-gram model assumes a corpus of words *w* ∈ $V_W$ and their\n",
        "contexts *c* ∈ $V_C$ , where $V_W$ and $V_C$ are the word and context vocabularies. In \n",
        "the words come from an unannotated textual corpus of words *$w_1$, $w_2$, . . . , $w_n$* (typically n is in\n",
        "the billions) and the contexts for word $w_i$ are the words surrounding it in an L-sized window\n",
        "$w_L$, . . . , $w_{i-1}$, $w_{i+1}$, . . . , $w_{i+L}$. Other definitions of contexts are possible. We denote the\n",
        "collection of observed words and context pairs as **D**. We use *#(w, c)* to denote the number of times\n",
        "the pair* (w, c)* appears in **D**. Similarly, \n",
        "* \\begin{equation*}\n",
        "\\#(w) = \\sum_{c'\\in V_c} \\#w(w,c')\n",
        "\\end{equation*} \n",
        "* \\begin{equation*}\n",
        "\\#(c) = \\sum_{w'\\in V_w} \\#w(w',c)\n",
        "\\end{equation*} \n",
        "are the number of times *w* and *c* occurred in **D**, respectively.\n",
        "\n",
        "\n",
        "Each word *w* ∈ $V_W$ is associated with a vector $\\vec{w}$ ∈ $R^d$\n",
        "and similarly each context *c* ∈ $V_C$ is\n",
        "represented as a vector $\\vec{c}$ ∈ $R^d$\n",
        ", where *d* is the embedding’s dimensionality. The entries in the\n",
        "vectors are latent, and treated as parameters to be learned. We sometimes refer to the vectors $\\vec{w}$ as\n",
        "rows in a |$V_W$ | × *d* matrix *W*, and to the vectors $\\vec{c}$ as rows in a |$V_C$ | × *d* matrix *C*. In such cases, $W_i$\n",
        "($C_i$) refers to the vector representation of the ith word (context) in the corresponding vocabulary.\n",
        "When referring to embeddings produced by a specific method x, we will usually use $W^x$\n",
        "and $C^x$\n",
        "explicitly, but may use just *W* and *C* when the method is clear from the discussion.\n",
        "\n",
        "####**SGNS’s Objective** \n",
        "\n",
        "\n",
        "\\begin{equation*}\n",
        "l =  \\sum_{w\\in V_w}  \\sum_{c\\in V_c}  \\#(w, c) ~(log ~σ( ~\\vec{w} · \\vec{c}) + k · \\mathop{\\mathbb{E}}_{c_N ∼P_D} [log ~σ(−~\\vec{w} · \\vec{c_N} )]) \n",
        "\\end{equation*}\n",
        "\n",
        "###SGNS as Implicit Matrix Factorization\n",
        "\n",
        "SGNS embeds both words and their contexts into a low-dimensional space $R^d$\n",
        ", resulting in the\n",
        "word and context matrices $W$ and $C$. The rows of matrix $W$ are typically used in NLP tasks (such\n",
        "as computing word similarities) while $C$ is ignored. It is nonetheless instructive to consider the\n",
        "product *$W·C^T$* = $M$. Viewed this way, SGNS can be described as factorizing an implicit matrix\n",
        "$M$ of dimensions $|V_W | × |V_C | $into two smaller matrices.\n",
        "\n",
        "\n",
        "Which matrix is being factorized? A matrix entry $M_{ij}$ corresponds to the dot product $W_i\n",
        "· C_j =\n",
        "\\vec{w_i}\n",
        "· \\vec{c_j}$. Thus, SGNS is factorizing a matrix in which each row corresponds to a word $w ∈ V_W $,\n",
        "each column corresponds to a context $c ∈ V_C$ , and each cell contains a quantity $f(w, c)$ reflecting\n",
        "the strength of association between that particular word-context pair. Such word-context association\n",
        "matrices are very common in the NLP and word-similarity literature. That said, the\n",
        "objective of SGNS  does not explicitly state what this association metric is. What can\n",
        "we say about the association function $f(w, c)$? In other words, which matrix is SGNS factorizing?\n",
        "\n",
        "####**Characterizing the Implicit Matrix** \n",
        "\n",
        "Consider the global objective above. For sufficiently large dimensionality $d$ (i.e. allowing\n",
        "for a perfect reconstruction of $M$), each product $\\vec{w_i}\n",
        "· \\vec{c_j}$ can assume a value independently of the\n",
        "others. Under these conditions, we can treat the objective $l$ as a function of independent $\\vec{w_i}\n",
        "· \\vec{c_j}$terms,\n",
        "and find the values of these terms that maximize it.\n",
        "We begin by rewriting global objective:\n",
        "\n",
        "\\begin{equation*}\n",
        "l =  \\sum_{w\\in V_w}  \\sum_{c\\in V_c}  \\#(w, c) ~(log ~σ( ~\\vec{w} · \\vec{c}) )+ \n",
        "\\sum_{w\\in V_w}  \\sum_{c\\in V_c}  \\#(w, c)~(k · \\mathop{\\mathbb{E}}_{c_N ∼P_D} [log ~σ(−~\\vec{w} · \\vec{c_N} )]) \n",
        "\\end{equation*}\n",
        "\n",
        "\\begin{equation*}\n",
        "  =  \\sum_{w\\in V_w}  \\sum_{c\\in V_c}  \\#(w, c) ~(log ~σ( ~\\vec{w} · \\vec{c}) )+ \n",
        "\\sum_{w\\in V_w}    \\#(w)~(k · \\mathop{\\mathbb{E}}_{c_N ∼P_D} [log ~σ(−~\\vec{w} · \\vec{c_N} )]) \n",
        "\\end{equation*}  \n",
        "\n",
        "and explicitly expressing the expectation term:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\mathop{\\mathbb{E}}_{c_N ∼P_D} [log ~σ(−~\\vec{w} · \\vec{c_N} )]  =   \\sum_{c_N\\in V_c}  {\\frac{\\#(c_N)}{|D|}} ~(log ~σ(- ~\\vec{w} · \\vec{c}) )\n",
        "\\end{equation*}\n",
        "\n",
        "\\begin{equation*}\n",
        "=  {\\frac{\\#(c_N)}{|D|}} ~(log ~σ(- ~\\vec{w} · \\vec{c}) ) +   \\sum_{c_N\\in V_c - c}  {\\frac{\\#(c_N)}{|D|}} ~(log ~σ(- ~\\vec{w} · \\vec{c}) )\n",
        "\\end{equation*}\n",
        "\n",
        "Combining equations reveals the local objective for a specific $(w, c)$ pair : \n",
        "\n",
        "\\begin{equation*}\n",
        "l(w,c) = \\#(w,c)~(log ~σ(~\\vec{w} · \\vec{c}) ) -~k~. \\#(w) {\\frac{\\#(c)}{|D|}} ~(log ~σ(- ~\\vec{w} · \\vec{c}) )\n",
        "\\end{equation*}\n",
        "\n",
        "To optimize the objective, we define $x = \\vec{w} · \\vec{c}$ and find its partial derivative with respect to $x$ :\n",
        "\n",
        "\\begin{equation*}\n",
        "\\frac{\\partial l}{\\partial x} = \\#(w,c)~σ(-x)  -~k~. \\#(w) ~{\\frac{\\#(c)}{|D|}} ~σ(x)\n",
        "\\end{equation*}\n",
        "\n",
        "We compare the derivative to zero, and after some simplification, arrive at:\n",
        "\n",
        "\\begin{equation*}\n",
        "e^{2x} - ~\\left ( ~ \\frac{\\#(w,c)}{~k~. \\#(w) ~{\\frac{\\#(c)}{|D|}}} ~ - ~1 \\right )e^x - \n",
        "~\\left ( ~ \\frac{\\#(w,c)}{~k~. \\#(w) ~{\\frac{\\#(c)}{|D|}}} ~ - ~1 \\right ) = 0\n",
        "\\end{equation*}\n",
        "\n",
        "If we define $y = e^x$\n",
        ", this equation becomes a quadratic equation of $y$, which has two solutions,\n",
        "$y = −1$ (which is invalid given the definition of $y$) and :\n",
        "\n",
        "\\begin{equation*}\n",
        "y = ~\\left ( ~ \\frac{\\#(w,c)}{~k~. \\#(w) ~{\\frac{\\#(c)}{|D|}}} ~ \\right ) = \n",
        "\\frac{\\#(w,c) . |D|}{\\#w.\\#c}.\\frac{1}{k}\n",
        "\\end{equation*}\n",
        "\n",
        "Substituting $y$ with $e^x$\n",
        "and $x$ with $\\vec{w}.\\vec{c}$ reveals :\n",
        "\n",
        "\\begin{equation*}\n",
        " \\vec{w}.\\vec{c} = log \\left (  \\frac{\\#(w,c) . |D|}{\\#w.\\#c}.\\frac{1}{k} \\right) = \n",
        "  log \\left (  \\frac{\\#(w,c) . |D|}{\\#w.\\#c} \\right) - log k\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "Interestingly, the expression $ log \\left (  \\frac{\\#(w,c) . |D|}{\\#w.\\#c} \\right) $\n",
        "is the well-known pointwise mutual information\n",
        "($PMI$) of $(w, c)$, which we discuss in depth below.\n",
        "Finally, we can describe the matrix $M$ that SGNS is factorizing :\n",
        "\n",
        "\\begin{equation*}\n",
        "M_{ij}^{SGNS} = W_i.C_j = \\vec{w_i}.\\vec{c_j} = P M I (w_i, c_j) -log k\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "\n",
        "*so, factoring a matrix is same as back-propagation error mechanism to update weights.*\n"
      ]
    },
    {
      "metadata": {
        "id": "g5_h4ErwoT3R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**Question 4**\n",
        "\n",
        "##*Discuss (about 100 words) how LSI word vectors could be improved to be close to s.t *\n",
        "\n",
        "$||wv_{LSI}^2 - wv_{w2v}^2 ||  {\\simeq0}$\n",
        "\n",
        "#**Answer  : **\n",
        "\n",
        "LSI word vectors are based on the co-occurrence matrix. If we see closely the co-occurrence matrix has entries $M_{ij}$ = \\# $word_i$ occurred with $word_j$. This type of relationship would be good if we are taking only the syntactic aspect of word vectors. On the other hand, word2vec also incorporate the semantic nature of words, therefore,  we need a way to incrporate this relationship that into our co-occurrence matrix. This can be done by using *n-gram* models to build our matrix.\n",
        "The Glove model does this."
      ]
    }
  ]
}