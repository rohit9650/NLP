<doc id="22213" url="https://en.wikipedia.org/wiki?curid=22213" title="Operator (mathematics)">
Operator (mathematics)

An operator is a mapping from one vector space or module to another. Operators are of critical importance to both linear algebra and functional analysis, and they find application in many other fields of pure and applied mathematics. For example, in classical mechanics, the derivative is used ubiquitously, and in quantum mechanics, observables are represented by hermitian operators. Important properties that various operators may exhibit include linearity, continuity, and boundedness.
Definitions.
Let "U, V" be two vector spaces. Any mapping from "U" to "V" is called an operator. Let "V" be a vector space over the field "K". We can define the structure of a vector space on the set of all operators from "U" to "V" ("A" and "B" are operators):
for all "A, B: U → V", for all x in "U" and for all "α" in "K".
Additionally, operators from any vector space to itself form a unital associative algebra:
with the identity mapping (usually denoted "E", "I" or id) being the unit.
Bounded operators and operator norm.
Let "U" and "V" be two vector spaces over the same ordered field (for example, formula_4), and they are equipped with norms. Then a linear operator from "U" to "V" is called bounded if there exists "C > 0" such that
for all x in "U".
Bounded operators form a vector space. On this vector space we can introduce a norm that is compatible with the norms of "U" and "V":
In case of operators from "U" to itself it can be shown that
Any unital normed algebra with this property is called a Banach algebra. It is possible to generalize spectral theory to such algebras. C*-algebras, which are Banach algebras with some additional structure, play an important role in quantum mechanics.
Special cases.
Functionals.
A functional is an operator that maps a vector space to its underlying field. Important applications of functionals are the theories of generalized functions and calculus of variations. Both are of great importance to theoretical physics.
Linear operators.
The most common kind of operator encountered are "linear operators". Let "U" and "V" be vector spaces over a field "K". Operator "A: U → V" is called linear if
for all x, y in "U" and for all "α, β" in "K".
The importance of linear operators is partially because they are morphisms between vector spaces.
In finite-dimensional case linear operators can be represented by matrices in the following way. Let formula_9 be a field, and formula_10 and formula_11 be finite-dimensional vector spaces over formula_9. Let us select a basis formula_13 in formula_10 and formula_15 in formula_11. Then let formula_17 be an arbitrary vector in formula_10 (assuming Einstein convention), and formula_19 be a linear operator. Then
Then formula_21 is the matrix of the operator formula_22 in fixed bases. formula_23 does not depend on the choice of formula_24, and formula_25 iff formula_26. Thus in fixed bases n-by-m matrices are in bijective correspondence to linear operators from formula_10 to formula_11.
The important concepts directly related to operators between finite-dimensional vector spaces are the ones of rank, determinant, inverse operator, and eigenspace.
Linear operators also play a great role in the infinite-dimensional case. The concepts of rank and determinant cannot be extended to infinite-dimensional matrices. This is why very different techniques are employed when studying linear operators (and operators in general) in the infinite-dimensional case. The study of linear operators in the infinite-dimensional case is known as functional analysis (so called because various classes of functions form interesting examples of infinite-dimensional vector spaces).
The space of sequences of real numbers, or more generally sequences of vectors in any vector space, themselves form an infinite-dimensional vector space. The most important cases are sequences of real or complex numbers, and these spaces, together with linear subspaces, are known as sequence spaces. Operators on these spaces are known as sequence transformations.
Bounded linear operators over Banach space form a Banach algebra in respect to the standard operator norm. The theory of Banach algebras develops a very general concept of spectra that elegantly generalizes the theory of eigenspaces.
Examples.
Geometry.
In geometry, additional structures on vector spaces are sometimes studied. Operators that map such vector spaces to themselves bijectively are very useful in these studies, they naturally form groups by composition.
For example, bijective operators preserving the structure of a vector space are precisely the invertible linear operators. They form the general linear group under composition. They "do not" form a vector space under the addition of operators, e.g. both "id" and "-id" are invertible (bijective), but their sum, 0, is not.
Operators preserving the Euclidean metric on such a space form the isometry group, and those that fix the origin form a subgroup known as the orthogonal group. Operators in the orthogonal group that also preserve the orientation of vector tuples form the special orthogonal group, or the group of rotations.
Probability theory.
Operators are also involved in probability theory, such as expectation, variance, covariance, factorials, etc.
Calculus.
From the point of view of functional analysis, calculus is the study of two linear operators: the differential operator formula_29, and the indefinite integral operator formula_30.
Fourier series and Fourier transform.
The Fourier transform is useful in applied mathematics, particularly physics and signal processing. It is another integral operator; it is useful mainly because it converts a function on one (temporal) domain to a function on another (frequency) domain, in a way effectively invertible. Nothing significant is lost, because there is an inverse transform operator. In the simple case of periodic functions, this result is based on the theorem that any continuous periodic function can be represented as the sum of a series of sine waves and cosine waves:
Coefficients "(a0, a1, b1, a2, b2, ...)" are in fact an element of an infinite-dimensional vector space ℓ2, and thus Fourier series is a linear operator.
When dealing with general function R → C, the transform takes on an integral form:
Laplace transform.
The "Laplace transform" is another integral operator and is involved in simplifying the process of solving differential equations.
Given "f" = "f"("s"), it is defined by:
Fundamental operators on scalar and vector fields.
Three operators are key to vector calculus:
As an extension of vector calculus operators to physics, engineering and tensor spaces, Grad, Div and Curl operators also are often associatied with Tensor calculus as well as vector calculus. 

</doc>
<doc id="291928" url="https://en.wikipedia.org/wiki?curid=291928" title="Operator (physics)">
Operator (physics)

In physics, an operator is a function over the space of physical states. As a result
of its application on a physical state, another physical state is obtained, very often along with
some extra relevant information.
The simplest example of the utility of operators is the study of symmetry. Because of this, they
are a very useful tool in classical mechanics. In quantum mechanics, on the other hand, they
are an intrinsic part of the formulation of the theory.
Operators in classical mechanics.
In classical mechanics, the movement of a particle (or system of particles) is completely determined by the Lagrangian formula_1 or equivalently the Hamiltonian formula_2, a function of the generalized coordinates "q", generalized velocities formula_3 and its conjugate momenta:
If either "L" or "H" are independent of a generalized coordinate "q", meaning the "L" and "H" do not change when "q" is changed, which in turn means the dynamics of the particle are still the same even when "q" changes, the corresponding momenta conjugate to those coordinates will be conserved (this is part of Noether's theorem, and the invariance of motion with respect to the coordinate "q" is a symmetry). Operators in classical mechanics are related to these symmetries.
More technically, when "H" is invariant under the action of a certain group of transformations "G":
the elements of "G" are physical operators, which map physical states among themselves.
Table of classical mechanics operators.
where formula_6 is the rotation matrix about an axis defined by the unit vector formula_7 and angle "θ".
Concept of generator.
If the transformation is infinitesimal, the operator action should be of the form
where formula_9 is the identity operator, formula_10 is a parameter with a small value, and formula_11 will depend on the transformation at hand, and is called a generator of the group. Again, as a simple example, we will derive the generator of the space translations on 1D functions.
As it was stated, formula_12. If formula_13 is infinitesimal, then we may write
This formula may be rewritten as
where formula_16 is the generator of the translation group, which in this case happens to be the "derivative" operator. Thus, it is said that the generator of translations is the derivative.
The exponential map.
The whole group may be recovered, under normal circumstances, from the generators, via the exponential map. In the case of the translations the idea works like this.
The translation for a finite value of formula_17 may be obtained by repeated application of the infinitesimal translation:
with the formula_19 standing for the application formula_20 times. If formula_20 is large, each of the factors may be considered to be infinitesimal:
But this limit may be rewritten as an exponential:
To be convinced of the validity of this formal expression, we may expand the exponential in a power series:
The right-hand side may be rewritten as
which is just the Taylor expansion of formula_26, which was our original value for formula_27.
The mathematical properties of physical operators are a topic of great importance in itself. For further information, see C*-algebra and Gelfand-Naimark theorem.
Operators in quantum mechanics.
The mathematical formulation of quantum mechanics (QM) is built upon the concept of an operator.
The wavefunction represents the probability amplitude of finding the system in that state. The terms "wavefunction" and "state" in QM context are usually used interchangeably.
Physical pure states in quantum mechanics are represented as unit-norm vectors (probabilities are normalized to one) in a special complex vector space: a Hilbert space. Time evolution in this vector space is given by the application of the evolution operator.
Any observable, i.e., any quantity which can be measured in a physical experiment, should be associated with a self-adjoint linear operator. The operators must yield real eigenvalues, since they are values which may come up as the result of the experiment. Mathematically this means the operators must be Hermitian. The probability of each eigenvalue is related to the projection of the physical state on the subspace related to that eigenvalue. See below for mathematical details.
In the wave mechanics formulation of QM, the wavefunction varies with space and time, or equivalently momentum and time (see position and momentum space for details), so observables are differential operators.
In the matrix mechanics formulation, the norm of the physical state should stay fixed, so the evolution operator should be unitary, and the operators can be represented as matrices. Any other symmetry, mapping a physical state into another, should keep this restriction.
Wavefunction.
The wavefunction must be square-integrable (see Lp spaces), meaning:
and normalizable, so that:
Two cases of eigenstates (and eigenvalues) are:
Linear operators in wave mechanics.
Let "ψ" be the wavefunction for a quantum system, and formula_36 be any linear operator for some observable "A" (such as position, momentum, energy, angular momentum etc.), then
where:
If "ψ" is an eigenfunction of a given operator A, then a definite quantity (the eigenvalue a) will be observed if a measurement of the observable A is made on the state "ψ". Conversely, if "ψ" is not an eigenfunction of A, then it has no eigenvalue for A, and the observable does not have a single definite value in that case. Instead, measurements of the observable A will yield each eigenvalue with a certain probability (related to the decomposition of "ψ" relative to the orthonormal eigenbasis of A).
In bra–ket notation the above can be written;
in which case formula_40 is an eigenvector, or eigenket.
Due to linearity, vectors can be defined in any number of dimensions, as each component of the vector acts on the function separately. One mathematical example is the del operator, which is itself a vector (useful in momentum-related quantum operators, in the table below).
An operator in "n"-dimensional space can be written:
where e"j" are basis vectors corresponding to each component operator "Aj". Each component will yield a corresponding eigenvalue. Acting this on the wave function "ψ":
in which
In bra–ket notation:
Commutation of operators on "Ψ".
If two observables "A" and "B" have linear operators formula_45 and formula_46, the commutator is defined by,
The commutator is itself a (composite) operator. Acting the commutator on "ψ" gives:
If "ψ" is an eigenfunction with eigenvalues "a" and "b" for observables "A" and "B" respectively, and if the operators commute:
then the observables "A" and "B" can be measured simultaneously with infinite precision i.e. uncertainties formula_50, formula_51 simultaneously. "ψ" is then said to be the simultaneous eigenfunction of A and B. To illustrate this:
It shows that measurement of A and B does not cause any shift of state i.e. initial and final states are same (no disturbance due to measurement). Suppose we measure A to get value a. We then measure B to get the value b. We measure A again. We still get the same value a. Clearly the state ("ψ") of the system is not destroyed and so we are able to measure A and B simultaneously with infinite precision.
If the operators do not commute:
they can't be prepared simultaneously to arbitrary precision, and there is an uncertainty relation between the observables,
even if "ψ" is an eigenfunction the above relation holds.. Notable pairs are position and momentum, and energy and time - uncertainty relations, and the angular momenta (spin, orbital and total) about any two orthogonal axes (such as "Lx" and "Ly", or "sy" and "sz" etc.).
Expectation values of operators on "Ψ".
The expectation value (equivalently the average or mean value) is the average measurement of an observable, for particle in region "R". The expectation value formula_55 of the operator formula_45 is calculated from:
This can be generalized to any function "F" of an operator:
An example of "F" is the 2-fold action of "A" on "ψ", i.e. squaring an operator or doing it twice:
Hermitian operators.
The definition of a Hermitian operator is:
Following from this, in bra–ket notation:
Important properties of Hermitian operators include:
Operators in Matrix mechanics.
An operator can be written in matrix form to map one basis vector to another. Since the operators and basis vectors are linear, the matrix is a linear transformation (aka transition matrix) between bases. Each basis element formula_62 can be connected to another, by the expression:
which is a matrix element:
A further property of a Hermitian operator is that eigenfunctions corresponding to different eigenvalues are orthogonal. In matrix form, operators allow real eigenvalues to be found, corresponding to measurements. Orthogonality allows a suitable basis set of vectors to represent the state of the quantum system. The eigenvalues of the operator are also evaluated in the same way as for the square matrix, by solving the characteristic polynomial:
where "I" is the "n" × "n" identity matrix, as an operator it corresponds to the identity operator. For a discrete basis:
while for a continuous basis:
Inverse of an operator.
A non-singular operator formula_36 has an inverse formula_69 defined by:
If an operator has no inverse, it is a singular operator. In a finite-dimensional space, the determinant of a non-singular operator is non-zero:
and hence it is zero for a singular operator.
Table of QM operators.
The operators used in quantum mechanics are collected in the table below (see for example,). The bold-face vectors with circumflexes are not unit vectors, they are 3-vector operators; all three spatial components taken together.
Examples of applying quantum operators.
The procedure for extracting information from a wave function is as follows. Consider the momentum "p" of a particle as an example. The momentum operator in one dimension is:
Letting this act on "ψ" we obtain:
if "ψ" is an eigenfunction of formula_74, then the momentum eigenvalue "p" is the value of the particle's momentum, found by:
For three dimensions the momentum operator uses the nabla operator to become:
In Cartesian coordinates (using the standard Cartesian basis vectors ex, ey, ez) this can be written;
that is:
The process of finding eigenvalues is the same. Since this is a vector and operator equation, if "ψ" is an eigenfunction, then each component of the momentum operator will have an eigenvalue corresponding to that component of momentum. Acting formula_79 on "ψ" obtains:

</doc>
<doc id="1721981" url="https://en.wikipedia.org/wiki?curid=1721981" title="Orbital angular momentum">
Orbital angular momentum

Orbital angular momentum may refer to:

</doc>
<doc id="170251" url="https://en.wikipedia.org/wiki?curid=170251" title="Ordinate">
Ordinate

In mathematics, ordinate most often refers to that element of an ordered pair which is plotted on the vertical axis of a two-dimensional Cartesian coordinate system, as opposed to the abscissa. The term can also refer to the vertical axis (typically "y"-axis) of a two-dimensional graph (because that axis is used to define and measure the vertical coordinates of points in the space). An ordered pair consists of two terms—the abscissa (horizontal, usually "x") and the ordinate (vertical, usually "y")—which define the location of a point in two-dimensional rectangular space.

</doc>
<doc id="173954" url="https://en.wikipedia.org/wiki?curid=173954" title="Orthogonal group">
Orthogonal group

In mathematics, the orthogonal group in dimension , denoted , is the group of distance-preserving transformations of a Euclidean space of dimension that preserve a fixed point, where the group operation is given by composing transformations. Equivalently, it is the group of orthogonal matrices, where the group operation is given by matrix multiplication, and an orthogonal matrix is a real matrix whose inverse equals its transpose.
The determinant of an orthogonal matrix being either or , an important subgroup of is the special orthogonal group, denoted SO("n"), of the orthogonal matrices of determinant . This group is also called the rotation group, because, in dimensions 2 and 3, its elements are the usual rotations around a point (in dimension 2) or a line (in dimension 3). In low dimension, these groups have been widely studied, see , and .
The term "orthogonal group" may also refer to a generalization of the above case: the group of invertible linear operators that preserve a non-degenerate symmetric bilinear form or quadratic form on a vector space over a field. In particular, when the bilinear form is the scalar product on the vector space of dimension over a field , with quadratic form the sum of squares, then the corresponding orthogonal group, denoted , is the set of orthogonal matrices with entries from , with the group operation of matrix multiplication. This is a subgroup of the general linear group given by
where T is the transpose of and is the identity matrix.
This article mainly discusses the orthogonal groups of quadratic forms that may be expressed over some bases as the dot product; over the reals, they are the positive definite quadratic forms. Over the reals, for any non-degenerate quadratic form, there is a basis, on which the matrix of the form is a diagonal matrix such that the diagonal entries are either or . Thus the orthogonal group depends only on the numbers of and of , and is denoted , where is the number of ones and the number of negative ones. For details, see indefinite orthogonal group.
The derived subgroup of is an often studied object because, when is a finite field, is often a central extension of a finite simple group.
Both and are algebraic groups, because the condition that a matrix be orthogonal, i.e. have its own transpose as inverse, can be expressed as a set of polynomial equations in the entries of the matrix. The Cartan–Dieudonné theorem describes the structure of the orthogonal group for a non-singular form.
Name.
The determinant of any orthogonal matrix is either or . The orthogonal -by- matrices with determinant form a normal subgroup of known as the special orthogonal group , consisting of all proper rotations. (More precisely, is the kernel of the Dickson invariant, discussed below.). By analogy with GL–SL (general linear group, special linear group), the orthogonal group is sometimes called the "general" orthogonal group and denoted , though this term is also sometimes used for "indefinite" orthogonal groups . The term rotation group can be used to describe either the special or general orthogonal group.
In even and odd dimension.
The structure of the orthogonal group differs in certain respects between even and odd dimensions; for example, over ordered fields (such as ) the element is orientation-preserving in even dimensions, but orientation-reversing in odd dimensions. When this distinction is to be emphasized, the groups may be denoted and , reserving for the dimension of the space ( or ). The letters or are also used, indicating the rank of the corresponding Lie algebra; in odd dimension the corresponding Lie algebra is formula_2, while in even dimension the Lie algebra is formula_3.
Difference between O("n") and SO("n") in even dimensions.
In two dimensions, O(2) is the group of all rotations about the origin and all reflections along a line through the origin. SO(2) is the group of all rotations about the origin.
These groups are closely related: SO(2) is a subgroup of O(2), since any two rotations gives a rotation. 
More generally, in any number of dimensions an even number of reflections gives a rotation and a rotation followed by reflection, or vice versa, produces a reflection. Therefore, the rotations define a subgroup of O(2), but the reflections do not define a subgroup.
A "reflection through the origin" may be generated as a combination of one reflection along each of the axes. The 'reflection through the origin' is not a reflection in the usual sense in even dimensions, but rather a rotation. In two dimensions it is the only nontrivial rotation that when applied twice gives the identity. It is its own inverse in any number of dimensions. In 4D it is isoclinic, and if that classification were generalised it would be isoclinic in every even number of dimensions.
Over the real number field.
Over the field of real numbers, the orthogonal group and the special orthogonal group are often simply denoted by and if no confusion is possible. They form real compact Lie groups of dimension . has two connected components, with being the identity component, i.e., the connected component containing the identity matrix.
Geometric interpretation.
The real orthogonal and real special orthogonal groups have the following geometric interpretations:
 is a subgroup of the Euclidean group , the group of isometries of ; it contains those that leave the origin fixed – . It is the symmetry group of the sphere () or -sphere and all objects with spherical symmetry, if the origin is chosen at the center.
 is a subgroup of , which consists of "direct" isometries, i.e., isometries preserving orientation; it contains those that leave the origin fixed – . It is the rotation group of the sphere and all objects with spherical symmetry, if the origin is chosen at the center.
} is a normal subgroup and even a characteristic subgroup of , and, if is even, also of . If is odd, is the internal direct product of and }. For every positive integer the cyclic group of -fold rotations is a normal subgroup of and .
Relative to suitable orthogonal bases, the isometries are of the form:
where the matrices are 2-by-2 rotation matrices in orthogonal planes of rotation. As a special case, known as Euler's rotation theorem, any (non-identity) element of is rotation about a uniquely defined axis.
The orthogonal group is generated by reflections (two reflections give a rotation), as in a Coxeter group, and elements have length at most (require at most reflections to generate; this follows from the above classification, noting that a rotation is generated by 2 reflections, and is true more generally for indefinite orthogonal groups, by the Cartan–Dieudonné theorem). A longest element (element needing the most reflections) is reflection through the origin (the map ), though so are other maximal combinations of rotations (and a reflection, in odd dimension).
The symmetry group of a circle is . 
The orientation preserving subgroup is isomorphic (as a "real" Lie group) to the circle group, also known as . This isomorphism sends the complex number of absolute value  to the special orthogonal matrix
The group , understood as the set of rotations of 3-dimensional space, is of major importance in the sciences and engineering, and there are numerous charts on.
Maximal tori and Weyl groups.
A maximal torus for , of rank "n", is given by the block-diagonal matrices
where the are 2-by-2 rotation matrices. The image } of the same torus
under the block-diagonal inclusion
is a maximal torus for . The Weyl group of is the semidirect product
formula_8 of a normal elementary abelian 2-subgroup and a symmetric group, where the nontrivial element of each } factor of acts on the corresponding circle factor of } by inversion,
and the symmetric group acts on both and } by permuting factors. The elements of the Weyl group are represented by matrices in }.
The factor is represented by block permutation matrices with 2-by-2 blocks, and a final 1 on the diagonal. The component is represented by block-diagonal matrices with 2-by-2 blocks either
with the last component chosen to make the determinant 1.
The Weyl group of is the subgroup formula_10
of that of ,
where is the kernel of the product homomorphism } given by formula_11;
that is is the subgroup with an even number of minus signs. The Weyl group of is represented in by the preimages under the standard injection
 of the representatives for the Weyl group of .
Those matrices with an odd number of formula_12
blocks have no remaining final coordinate to make their determinants positive, and hence cannot be represented in .
Low-dimensional topology.
The low-dimensional (real) orthogonal groups are familiar spaces:
Homotopy groups.
In terms of algebraic topology, for the fundamental group of is cyclic of order 2, and the spin group is its universal cover. For the fundamental group is infinite cyclic and the universal cover corresponds to the real line (the group is the unique connected 2-fold cover).
Generally, the homotopy groups of the real orthogonal group are related to homotopy groups of spheres, and thus are in general hard to compute. However, one can compute the homotopy groups of the stable orthogonal group (aka the infinite orthogonal group), defined as the direct limit of the sequence of inclusions:
Since the inclusions are all closed, hence cofibrations, this can also be interpreted as a union. On the other hand is a homogeneous space for , and one has the following fiber bundle:
which can be understood as "The orthogonal group acts transitively on the unit sphere , and the stabilizer of a point (thought of as a unit vector) is the orthogonal group of the perpendicular complement, which is an orthogonal group one dimension lower. Thus the natural inclusion is -connected, so the homotopy groups stabilize, and for : thus the homotopy groups of the stable space equal the lower homotopy groups of the unstable spaces.
From Bott periodicity we obtain , therefore the homotopy groups of are 8-fold periodic, meaning , and one needs only to list the lower 8 homotopy groups:
Relation to KO-theory.
Via the clutching construction, homotopy groups of the stable space are identified with stable vector bundles on spheres (up to isomorphism), with a dimension shift of 1: . Setting (to make fit into the periodicity), one obtains:
Computation and interpretation of homotopy groups.
Low-dimensional groups.
The first few homotopy groups can be calculated by using the concrete descriptions of low-dimensional groups.
Lie groups.
From general facts about Lie groups, always vanishes, and is free (free abelian).
Vector bundles.
From the vector bundle point of view, is vector bundles over , which is two points. Thus over each point, the bundle is trivial, and the non-triviality of the bundle is the difference between the dimensions of the vector spaces over the two points, so is dimension.
Loop spaces.
Using concrete descriptions of the loop spaces in Bott periodicity, one can interpret higher homotopy of as lower homotopy of simple to analyze spaces. Using π0, and have two components, and have countably many components, and the rest are connected.
Interpretation of homotopy groups.
In a nutshell:
Let be any of the four division algebras , , , , and let be the tautological line bundle over the projective line , and its class in K-theory. Noting that , , , , these yield vector bundles over the corresponding spheres, and
From the point of view of symplectic geometry, can be interpreted as the Maslov index, thinking of it as the fundamental group of the stable Lagrangian Grassmannian as , so .
Over the complex number field.
Over the field of complex numbers, and are complex Lie groups of dimension over (it means the dimension over is twice that). has two connected components, and is the connected component containing the identity matrix. For these groups are noncompact.
Just as in the real case is not simply connected. For the fundamental group of is cyclic of order 2 whereas the fundamental group of is infinite cyclic.
Over finite fields.
Orthogonal groups can also be defined over finite fields , where is a power of a prime .
Over finite fields of characteristic not equal to 2, orthogonal groups come in two types in even dimension: and ; and one type in odd dimension: .
If is the vector space on which the orthogonal group acts, it can be written as a direct orthogonal sum as follows:
where are hyperbolic lines and contains no singular vectors. If is the zero subspace, then is of plus type. If is one-dimensional then has odd dimension. If has dimension 2, is of minus type.
In the special case where , is a dihedral group of order .
We have the following formulas for the order of , when the characteristic is not two:
If is a square in
If is a non-square in 
The Dickson invariant.
For orthogonal groups, the Dickson invariant is a homomorphism from the orthogonal group to the quotient group (integers modulo 2), taking the value in case the element is the product of an even number of reflections, and the value of 1 otherwise.
Algebraically, the Dickson invariant can be defined as , where is the identity . Over fields that are not of characteristic 2 it is equivalent to the determinant: the determinant is −1 to the power of the Dickson invariant.
Over fields of characteristic 2, the determinant is always 1, so the Dickson invariant gives more information than the determinant.
The special orthogonal group is the kernel of the Dickson invariant and usually has index 2 in . When the characteristic of is not 2, the Dickson Invariant is whenever the determinant is . Thus when the characteristic is not 2, is commonly defined to be the elements of with determinant . Each element in has determinant . Thus in characteristic 2, the determinant is always .
The Dickson invariant can also be defined for Clifford groups and Pin groups in a similar way (in all dimensions).
Orthogonal groups of characteristic 2.
Over fields of characteristic 2 orthogonal groups often exhibit special behaviors, some of which are listed in this section. (Formerly these groups were known as the hypoabelian groups but this term is no longer used.)
The spinor norm.
The spinor norm is a homomorphism from an orthogonal group over a field to the quotient group (the multiplicative group of the field up to square elements), that takes reflection in a vector of norm to the image of in .
For the usual orthogonal group over the reals it is trivial, but it is often non-trivial over other fields, or for the orthogonal group of a quadratic form over the reals that is not positive definite.
Galois cohomology and orthogonal groups.
In the theory of Galois cohomology of algebraic groups, some further points of view are introduced. They have explanatory value, in particular in relation with the theory of quadratic forms; but were for the most part "post hoc", as far as the discovery of the phenomena is concerned. The first point is that quadratic forms over a field can be identified as a Galois , or twisted forms (torsors) of an orthogonal group. As an algebraic group, an orthogonal group is in general neither connected nor simply-connected; the latter point brings in the spin phenomena, while the former is related to the discriminant.
The 'spin' name of the spinor norm can be explained by a connection to the spin group (more accurately a pin group). This may now be explained quickly by Galois cohomology (which however postdates the introduction of the term by more direct use of Clifford algebras). The spin covering of the orthogonal group provides a short exact sequence of algebraic groups.
Here is the algebraic group of square roots of 1; over a field of characteristic not 2 it is roughly the same as a two-element group with trivial Galois action. The connecting homomorphism from , which is simply the group of -valued points, to is essentially the spinor norm, because is isomorphic to the multiplicative group of the field modulo squares.
There is also the connecting homomorphism from of the orthogonal group, to the of the kernel of the spin covering. The cohomology is non-abelian, so that this is as far as we can go, at least with the conventional definitions.
Lie algebra.
The Lie algebra corresponding to Lie groups and consists of the skew-symmetric matrices, with the Lie bracket given by the commutator. One Lie algebra corresponds to both groups. It is often denoted by formula_22 or formula_23, and called the orthogonal Lie algebra or special orthogonal Lie algebra. Over real numbers, these Lie algebras for different are the compact real forms of two of the four families of semisimple Lie algebras: in odd dimension , where , while in even dimension , where .
More intrinsically, given a vector space with an inner product, the special orthogonal Lie algebra is given by the bivectors on the space, which are sums of simple bivectors (2-blades) . The correspondence is given by the map formula_24 where is the covector dual to the vector ; in coordinates these are exactly the elementary skew-symmetric matrices.
Over real numbers, this characterization is used in interpreting the curl of a vector field (naturally a 2-vector) as an infinitesimal rotation or "curl", hence the name. Generalizing the inner product with a nondegenerate form yields the indefinite orthogonal Lie algebras formula_25
The representation theory of the orthogonal Lie algebras includes both representations corresponding to linear representations of the orthogonal groups, and representations corresponding to projective representations of the orthogonal groups (linear representations of spin groups), the so-called spin representation, which are important in physics.
Related groups.
The orthogonal groups and special orthogonal groups have a number of important subgroups, supergroups, quotient groups, and covering groups. These are listed below.
The inclusions and are part of a sequence of 8 inclusions used in a geometric proof of the Bott periodicity theorem, and the corresponding quotient spaces are symmetric spaces of independent interest – for example, is the Lagrangian Grassmannian.
Lie subgroups.
In physics, particularly in the areas of Kaluza–Klein compactification, it is important to find out the subgroups of the orthogonal group. The main ones are:
Lie supergroups.
The orthogonal group is also an important subgroup of various Lie groups:
Conformal group.
Being isometries, real orthogonal transforms preserve angles, and are thus conformal maps, though not all conformal linear transforms are orthogonal. In classical terms this is the difference between congruence and similarity, as exemplified by SSS (Side-Side-Side) congruence of triangles and AAA (Angle-Angle-Angle) similarity of triangles. The group of conformal linear maps of is denoted for the conformal orthogonal group, and consists of the product of the orthogonal group with the group of dilations. If is odd, these two subgroups do not intersect, and they are a direct product: , where } is the real multiplicative group, while if is even, these subgroups intersect in , so this is not a direct product, but it is a direct product with the subgroup of dilation by a positive scalar: .
Similarly one can define ; note that this is always: .
Discrete subgroups.
As the orthogonal group is compact, discrete subgroups are equivalent to finite subgroups. These subgroups are known as point group and can be realized as the symmetry groups of polytopes. A very important class of examples are the finite Coxeter groups, which include the symmetry groups of regular polytopes.
Dimension 3 is particularly studied – see point groups in three dimensions, polyhedral groups, and list of spherical symmetry groups. In 2 dimensions, the finite groups are either cyclic or dihedral – see point groups in two dimensions.
Other finite subgroups include:
Covering and quotient groups.
The orthogonal group is neither simply connected nor centerless, and thus has both a covering group and a quotient group, respectively:
These are all 2-to-1 covers.
For the special orthogonal group, the corresponding groups are:
Spin is a 2-to-1 cover, while in even dimension, is a 2-to-1 cover, and in odd dimension is a 1-to-1 cover, i.e., isomorphic to . These groups, , , and are Lie group forms of the compact special orthogonal Lie algebra, formula_37 – Spin is the simply connected form, while PSO is the centerless form, and SO is in general neither.
In dimension 3 and above these are the covers and quotients, while dimension 2 and below are somewhat degenerate; see specific articles for details.
Principal homogeneous space: Stiefel manifold.
The principal homogeneous space for the orthogonal group is the Stiefel manifold of orthonormal bases (orthonormal -frames).
In other words, the space of orthonormal bases is like the orthogonal group, but without a choice of base point: given an orthogonal space, there is no natural choice of orthonormal basis, but once one is given one, there is a one-to-one correspondence between bases and the orthogonal group. Concretely, a linear map is determined by where it sends a basis: just as an invertible map can take any basis to any other basis, an orthogonal map can take any "orthogonal" basis to any other "orthogonal" basis.
The other Stiefel manifolds for of "incomplete" orthonormal bases (orthonormal -frames) are still homogeneous spaces for the orthogonal group, but not "principal" homogeneous spaces: any -frame can be taken to any other -frame by an orthogonal map, but this map is not uniquely determined.

</doc>
<doc id="105620" url="https://en.wikipedia.org/wiki?curid=105620" title="Orthogonal matrix">
Orthogonal matrix

In linear algebra, an orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors (i.e., orthonormal vectors), i.e.
where "I" is the identity matrix.
This leads to the equivalent characterization: a matrix "Q" is orthogonal if its transpose is equal to its inverse:
An orthogonal matrix "Q" is necessarily invertible (with inverse ), unitary () and therefore normal () in the reals. The determinant of any orthogonal matrix is either +1 or −1. As a linear transformation, an orthogonal matrix preserves the dot product of vectors, and therefore acts as an isometry of Euclidean space, such as a rotation or reflection. In other words, it is a unitary transformation.
The set of orthogonal matrices forms a group O("n"), known as the orthogonal group. The subgroup SO("n") consisting of orthogonal matrices with determinant +1 is called the special orthogonal group, and each of its elements is a special orthogonal matrix. As a linear transformation, every special orthogonal matrix acts as a rotation.
The complex analogue of an orthogonal matrix is a unitary matrix.
Overview.
An orthogonal matrix is the real specialization of a unitary matrix, and thus always a normal matrix. Although we consider only real matrices here, the definition can be used for matrices with entries from any field. However, orthogonal matrices arise naturally from dot products, and for matrices of complex numbers that leads instead to the unitary requirement. Orthogonal matrices preserve the dot product, so, for vectors u, v in an "n"-dimensional real Euclidean space
where "Q" is an orthogonal matrix. To see the inner product connection, consider a vector v in an "n"-dimensional real Euclidean space. Written with respect to an orthonormal basis, the squared length of v is vTv. If a linear transformation, in matrix form "Q"v, preserves vector lengths, then
Thus finite-dimensional linear isometries—rotations, reflections, and their combinations—produce orthogonal matrices. The converse is also true: orthogonal matrices imply orthogonal transformations. However, linear algebra includes orthogonal transformations between spaces which may be neither finite-dimensional nor of the same dimension, and these have no orthogonal matrix equivalent.
Orthogonal matrices are important for a number of reasons, both theoretical and practical. The "n"×"n" orthogonal matrices form a group under matrix multiplication, the orthogonal group denoted by O("n"), which—with its subgroups—is widely used in mathematics and the physical sciences. For example, the point group of a molecule is a subgroup of O(3). Because floating point versions of orthogonal matrices have advantageous properties, they are key to many algorithms in numerical linear algebra, such as QR decomposition. As another example, with appropriate normalization the discrete cosine transform (used in MP3 compression) is represented by an orthogonal matrix.
Examples.
Below are a few examples of small orthogonal matrices and possible interpretations.
An instance of a 2×2 rotation matrix:
Elementary constructions.
Lower dimensions.
The simplest orthogonal matrices are the 1×1 matrices [1] and [−1] which we can interpret as the identity and a reflection of the real line across the origin.
The matrices have the form
which orthogonality demands satisfy the three equations
In consideration of the first equation, without loss of generality let , ; then either , or , . We can interpret the first case as a rotation by "θ" (where is the identity), and the second as a reflection across a line at an angle of "θ"/2.
The special case of the reflection matrix with generates a reflection about the line at 45° given by and therefore exchanges "x" and "y"; it is a permutation matrix, with a single 1 in each column and row (and otherwise 0):
The identity is also a permutation matrix.
A reflection is its own inverse, which implies that a reflection matrix is symmetric (equal to its transpose) as well as orthogonal. The product of two rotation matrices is a rotation matrix, and the product of two reflection matrices is also a rotation matrix.
Higher dimensions.
Regardless of the dimension, it is always possible to classify orthogonal matrices as purely rotational or not, but for matrices and larger the non-rotational matrices can be more complicated than reflections. For example,
represent an "inversion" through the origin and
a "rotoinversion" about the "z" axis.
Rotations become more complicated in higher dimensions; they can no longer be completely characterized by one angle, and may affect more than one planar subspace. It is common to describe a rotation matrix in terms of an axis and angle, but this only works in three dimensions. Above three dimensions two or more angles are needed, each associated with a plane of rotation.
However, we have elementary building blocks for permutations, reflections, and rotations that apply in general.
Primitives.
The most elementary permutation is a transposition, obtained from the identity matrix by exchanging two rows. Any "n"×"n" permutation matrix can be constructed as a product of no more than transpositions.
A Householder reflection is constructed from a non-null vector v as
Here the numerator is a symmetric matrix while the denominator is a number, the squared magnitude of v. This is a reflection in the hyperplane perpendicular to v (negating any vector component parallel to v). If v is a unit vector, then suffices. A Householder reflection is typically used to simultaneously zero the lower part of a column. Any orthogonal matrix of size can be constructed as a product of at most "n" such reflections.
A Givens rotation acts on a two-dimensional (planar) subspace spanned by two coordinate axes, rotating by a chosen angle. It is typically used to zero a single subdiagonal entry. Any rotation matrix of size "n"×"n" can be constructed as a product of at most such rotations. In the case of matrices, three such rotations suffice; and by fixing the sequence we can thus describe all rotation matrices (though not uniquely) in terms of the three angles used, often called Euler angles.
A Jacobi rotation has the same form as a Givens rotation, but is used to zero both off-diagonal entries of a symmetric submatrix.
Properties.
Matrix properties.
A real square matrix is orthogonal if and only if its columns form an orthonormal basis of the Euclidean space R"n" with the ordinary Euclidean dot product, which is the case if and only if its rows form an orthonormal basis of R"n". It might be tempting to suppose a matrix with orthogonal (not orthonormal) columns would be called an orthogonal matrix, but such matrices have no special interest and no special name; they only satisfy , with "D" a diagonal matrix.
The determinant of any orthogonal matrix is +1 or −1. This follows from basic facts about determinants, as follows:
The converse is not true; having a determinant of ±1 is no guarantee of orthogonality, even with orthogonal columns, as shown by the following counterexample.
With permutation matrices the determinant matches the signature, being +1 or −1 as the parity of the permutation is even or odd, for the determinant is an alternating function of the rows.
Stronger than the determinant restriction is the fact that an orthogonal matrix can always be diagonalized over the complex numbers to exhibit a full set of eigenvalues, all of which must have (complex) modulus 1.
Group properties.
The inverse of every orthogonal matrix is again orthogonal, as is the matrix product of two orthogonal matrices. In fact, the set of all orthogonal matrices satisfies all the axioms of a group. It is a compact Lie group of dimension , called the orthogonal group and denoted by O("n").
The orthogonal matrices whose determinant is +1 form a path-connected normal subgroup of O("n") of index 2, the special orthogonal group SO("n") of rotations. The quotient group O("n")/SO("n") is isomorphic to O(1), with the projection map choosing [+1] or [−1] according to the determinant. Orthogonal matrices with determinant −1 do not include the identity, and so do not form a subgroup but only a coset; it is also (separately) connected. Thus each orthogonal group falls into two pieces; and because the projection map splits, O("n") is a semidirect product of SO("n") by O(1). In practical terms, a comparable statement is that any orthogonal matrix can be produced by taking a rotation matrix and possibly negating one of its columns, as we saw with 2×2 matrices. If "n" is odd, then the semidirect product is in fact a direct product, and any orthogonal matrix can be produced by taking a rotation matrix and possibly negating all of its columns. This follows from the property of determinants that negating a column negates the determinant, and thus negating an odd (but not even) number of columns negates the determinant.
Now consider orthogonal matrices with bottom right entry equal to 1. The remainder of the last column (and last row) must be zeros, and the product of any two such matrices has the same form. The rest of the matrix is an orthogonal matrix; thus O("n") is a subgroup of (and of all higher groups).
Since an elementary reflection in the form of a Householder matrix can reduce any orthogonal matrix to this constrained form, a series of such reflections can bring any orthogonal matrix to the identity; thus an orthogonal group is a reflection group. The last column can be fixed to any unit vector, and each choice gives a different copy of O("n") in ; in this way is a bundle over the unit sphere "S""n" with fiber O("n").
Similarly, SO("n") is a subgroup of ; and any special orthogonal matrix can be generated by given plane rotations using an analogous procedure. The bundle structure persists: . A single rotation can produce a zero in the first row of the last column, and series of "n"−1 rotations will zero all but the last row of the last column of an rotation matrix. Since the planes are fixed, each rotation has only one degree of freedom, its angle. By induction, SO("n") therefore has
degrees of freedom, and so does O("n").
Permutation matrices are simpler still; they form, not a Lie group, but only a finite group, the order "n"! symmetric group S"n". By the same kind of argument, S"n" is a subgroup of S"n"+1. The even permutations produce the subgroup of permutation matrices of determinant +1, the order "n"!/2 alternating group.
Canonical form.
More broadly, the effect of any orthogonal matrix separates into independent actions on orthogonal two-dimensional subspaces. That is, if "Q" is special orthogonal then one can always find an orthogonal matrix "P", a (rotational) change of basis, that brings "Q" into block diagonal form:
where the matrices "R"1, ..., "R""k" are rotation matrices, and with the remaining entries zero. Exceptionally, a rotation block may be diagonal, ±"I". Thus, negating one column if necessary, and noting that a reflection diagonalizes to a +1 and −1, any orthogonal matrix can be brought to the form
The matrices "R"1, ..., "R""k" give conjugate pairs of eigenvalues lying on the unit circle in the complex plane; so this decomposition confirms that all eigenvalues have absolute value 1. If "n" is odd, there is at least one real eigenvalue, +1 or −1; for a rotation, the eigenvector associated with +1 is the rotation axis.
Lie algebra.
Suppose the entries of "Q" are differentiable functions of "t", and that gives . Differentiating the orthogonality condition
yields
Evaluation at () then implies
In Lie group terms, this means that the Lie algebra of an orthogonal matrix group consists of skew-symmetric matrices. Going the other direction, the matrix exponential of any skew-symmetric matrix is an orthogonal matrix (in fact, special orthogonal).
For example, the three-dimensional object physics calls angular velocity is a differential rotation, thus a vector in the Lie algebra formula_26 tangent to SO(3). Given , with being a unit vector, the correct skew-symmetric matrix form of ω is
The exponential of this is the orthogonal matrix for rotation around axis v by angle "θ"; setting , ,
Numerical linear algebra.
Benefits.
Numerical analysis takes advantage of many of the properties of orthogonal matrices for numerical linear algebra, and they arise naturally. For example, it is often desirable to compute an orthonormal basis for a space, or an orthogonal change of bases; both take the form of orthogonal matrices. Having determinant ±1 and all eigenvalues of magnitude 1 is of great benefit for numeric stability. One implication is that the condition number is 1 (which is the minimum), so errors are not magnified when multiplying with an orthogonal matrix. Many algorithms use orthogonal matrices like Householder reflections and Givens rotations for this reason. It is also helpful that, not only is an orthogonal matrix invertible, but its inverse is available essentially free, by exchanging indices.
Permutations are essential to the success of many algorithms, including the workhorse Gaussian elimination with partial pivoting (where permutations do the pivoting). However, they rarely appear explicitly as matrices; their special form allows more efficient representation, such as a list of "n" indices.
Likewise, algorithms using Householder and Givens matrices typically use specialized methods of multiplication and storage. For example, a Givens rotation affects only two rows of a matrix it multiplies, changing a full multiplication of order "n"3 to a much more efficient order "n". When uses of these reflections and rotations introduce zeros in a matrix, the space vacated is enough to store sufficient data to reproduce the transform, and to do so robustly. (Following , we do "not" store a rotation angle, which is both expensive and badly behaved.)
Decompositions.
A number of important matrix decompositions involve orthogonal matrices, including especially:
Examples.
Consider an overdetermined system of linear equations, as might occur with repeated measurements of a physical phenomenon to compensate for experimental errors. Write , where "A" is , .
A QR decomposition reduces "A" to upper triangular "R". For example, if "A" is then "R" has the form
The linear least squares problem is to find the x that minimizes , which is equivalent to projecting b to the subspace spanned by the columns of "A". Assuming the columns of "A" (and hence "R") are independent, the projection solution is found from . Now "A"T"A" is square () and invertible, and also equal to "R"T"R". But the lower rows of zeros in "R" are superfluous in the product, which is thus already in lower-triangular upper-triangular factored form, as in Gaussian elimination (Cholesky decomposition). Here orthogonality is important not only for reducing to "R"T"R", but also for allowing solution without magnifying numerical problems.
In the case of a linear system which is underdetermined, or an otherwise non-invertible matrix, singular value decomposition (SVD) is equally useful. With "A" factored as "UΣV"T, a satisfactory solution uses the Moore-Penrose pseudoinverse, "VΣ"+"U"T, where "Σ"+ merely replaces each non-zero diagonal entry with its reciprocal. Set x to "VΣ"+"U"Tb.
The case of a square invertible matrix also holds interest. Suppose, for example, that "A" is a rotation matrix which has been computed as the composition of numerous twists and turns. Floating point does not match the mathematical ideal of real numbers, so "A" has gradually lost its true orthogonality. A Gram-Schmidt process could orthogonalize the columns, but it is not the most reliable, nor the most efficient, nor the most invariant method. The polar decomposition factors a matrix into a pair, one of which is the unique "closest" orthogonal matrix to the given matrix, or one of the closest if the given matrix is singular. (Closeness can be measured by any matrix norm invariant under an orthogonal change of basis, such as the spectral norm or the Frobenius norm.) For a near-orthogonal matrix, rapid convergence to the orthogonal factor can be achieved by a "Newton's method" approach due to (1990), repeatedly averaging the matrix with its inverse transpose. has published an accelerated method with a convenient convergence test.
For example, consider a non-orthogonal matrix for which the simple averaging algorithm takes seven steps
and which acceleration trims to two steps (with γ = 0.353553, 0.565685).
Gram-Schmidt yields an inferior solution, shown by a Frobenius distance of 8.28659 instead of the minimum 8.12404.
Randomization.
Some numerical applications, such as Monte Carlo methods and exploration of high-dimensional data spaces, require generation of uniformly distributed random orthogonal matrices. In this context, "uniform" is defined in terms of Haar measure, which essentially requires that the distribution not change if multiplied by any freely chosen orthogonal matrix. Orthogonalizing matrices with independent uniformly distributed random entries does not result in uniformly distributed orthogonal matrices, but the QR decomposition of independent normally distributed random entries does, as long as the diagonal of "R" contains only positive entries. replaced this with a more efficient idea that later generalized as the "subgroup algorithm" (in which form it works just as well for permutations and rotations). To generate an orthogonal matrix, take an one and a uniformly distributed unit vector of dimension . Construct a Householder reflection from the vector, then apply it to the smaller matrix (embedded in the larger size with a 1 at the bottom right corner).
Nearest orthogonal matrix.
The problem of finding the orthogonal matrix formula_33 nearest a given matrix formula_34 is related to the Orthogonal Procrustes problem. There are several different ways to get the unique solution, the simplest of which is taking the singular value decomposition of formula_34 and replacing the singular values with ones. Another method expresses the formula_36 explicitly but requires the use of a matrix square root:
This may be combined with the Babylonian method for extracting the square root of a matrix to give a recurrence which converges to an orthogonal matrix quadratically:
where formula_39. These iterations are stable provided the condition number of formula_34 is less than three.
Spin and pin.
A subtle technical problem afflicts some uses of orthogonal matrices. Not only are the group components with determinant +1 and −1 not connected to each other, even the +1 component, SO("n"), is not simply connected (except for SO(1), which is trivial). Thus it is sometimes advantageous, or even necessary, to work with a covering group of SO("n"), the spin group, Spin("n"). Likewise, O("n") has covering groups, the pin groups, Pin("n"). For , Spin("n") is simply connected and thus the universal covering group for SO("n"). By far the most famous example of a spin group is Spin(3), which is nothing but SU(2), or the group of unit quaternions.
The Pin and Spin groups are found within Clifford algebras, which themselves can be built from orthogonal matrices.
Rectangular matrices.
If "Q" is not a square matrix, then the conditions and are not equivalent. The condition says that the columns of "Q" are orthonormal. This can only happen if "Q" is an matrix with . Similarly, says that the rows of "Q" are orthonormal, which requires .
There is no standard terminology for these matrices. They are sometimes called "orthonormal matrices", sometimes "orthogonal matrices", and sometimes simply "matrices with orthonormal rows/columns".

</doc>
<doc id="102221" url="https://en.wikipedia.org/wiki?curid=102221" title="Orthogonality">
Orthogonality

In mathematics, orthogonality is the relation of two lines at right angles to one another (perpendicularity), and the generalization of this relation into "n" dimensions; and to a variety of mathematical relations thought of as describing non-overlapping, uncorrelated, or independent objects of some kind.
The concept of orthogonality has been broadly generalized in mathematics, science, and engineering, especially since the beginning of the 16th century. Much of the generalizing has taken place in the areas of mathematical functions, calculus and linear algebra.
Etymology.
The word comes from the Greek ' ("orthos"), meaning "upright", and ' ("gonia"), meaning "angle".
The ancient Greek ὀρθογώνιον "orthogōnion" (< ὀρθός "orthos" 'upright' + γωνία "gōnia" 'angle') and classical Latin "orthogonium" originally denoted a rectangle. Later, they came to mean a right triangle. In the 12th century, the post-classical Latin word "orthogonalis" came to mean a right angle or something related to a right angle.
Mathematics.
Definitions.
A set of vectors is called pairwise orthogonal if each pairing of them is orthogonal. Such a set is called an orthogonal set. Nonzero pairwise orthogonal vectors are always linearly independent.
In certain cases, the word "normal" is used to mean "orthogonal", particularly in the geometric sense as in the normal to a surface. For example, the "y"-axis is normal to the curve at the origin. However, "normal" may also refer to the magnitude of a vector. In particular, a set is called orthonormal (orthogonal plus normal) if it is an orthogonal set of unit vectors. As a result, use of the term "normal" to mean "orthogonal" is often avoided. The word "normal" also has a different meaning in probability and statistics.
A vector space with a bilinear form generalizes the case of an inner product. When the bilinear form applied to two vectors results in zero, then they are orthogonal. The case of a pseudo-Euclidean plane uses the term hyperbolic orthogonality. In the diagram, axes x′ and t′ are hyperbolic-orthogonal for any given "ϕ".
Euclidean vector spaces.
In 2-D or higher-dimensional Euclidean space, two vectors are orthogonal if and only if their dot product is zero, i.e. they make an angle of 90°, or π/2 radians. Hence orthogonality of vectors is an extension of the concept of perpendicular vectors into higher-dimensional spaces.
In terms of Euclidean subspaces, a subspace has an "orthogonal complement" such that every vector in the subspace is orthogonal to every vector in the complement. In three-dimensional Euclidean space, the orthogonal complement of a line is the plane perpendicular to it, and vice versa.
Note however that there is no correspondence with regards to perpendicular planes, because vectors in subspaces start from the origin (by the definition of a Linear subspace).
In four-dimensional Euclidean space, the orthogonal complement of a line is a hyperplane and vice versa, and that of a plane is a plane.
Orthogonal functions.
By using integral calculus, it is common to use the following to define the inner product of two functions "f" and "g":
Here we introduce a nonnegative weight function "w"("x") in the definition of this inner product. In simple cases, .
We say that these functions are orthogonal (with respect to this inner product) if the value of this integral is zero:
Note that two functions which are orthogonal with respect to one inner product, aren't necessarily orthogonal with respect to another inner product.
We write the norms with respect to this inner product and the weight function as
The members of a set of functions are:
where
is the "Kronecker delta" function. In other words, any two of them are orthogonal, and the norm of each is 1 in the case of the orthonormal sequence. See in particular the orthogonal polynomials.
Art.
In art, the perspective (imaginary) lines pointing to the vanishing point are referred to as "orthogonal lines".
The term "orthogonal line" often has a quite different meaning in the literature of modern art criticism. Many works by painters such as Piet Mondrian and Burgoyne Diller are noted for their exclusive use of "orthogonal lines" — not, however, with reference to perspective, but rather referring to lines that are straight and exclusively horizontal or vertical, forming right angles where they intersect. For example, an essay at the Web site of the Thyssen-Bornemisza Museum states that "Mondrian ... dedicated his entire oeuvre to the investigation of the balance between orthogonal lines and primary colours." 
Computer science.
Orthogonality in programming language design is the ability to use various language features in arbitrary combinations with consistent results. This usage was introduced by van Wijngaarden in the design of Algol 68:
The number of independent primitive concepts has been minimized in order that the language be easy to describe, to learn, and to implement. On the other hand, these concepts have been applied “orthogonally” in order to maximize the expressive power of the language while trying to avoid deleterious superfluities.
Orthogonality is a system design property which guarantees that modifying the technical effect produced by a component of a system neither creates nor propagates side effects to other components of the system. Typically this is achieved through the separation of concerns and encapsulation, and it is essential for feasible and compact designs of complex systems. The emergent behavior of a system consisting of components should be controlled strictly by formal definitions of its logic and not by side effects resulting from poor integration, i.e., non-orthogonal design of modules and interfaces. Orthogonality reduces testing and development time because it is easier to verify designs that neither cause side effects nor depend on them.
An instruction set is said to be orthogonal if it lacks redundancy (i.e., there is only a single instruction that can be used to accomplish a given task) and is designed such that instructions can use any register in any addressing mode. This terminology results from considering an instruction as a vector whose components are the instruction fields. One field identifies the registers to be operated upon and another specifies the addressing mode. An orthogonal instruction set uniquely encodes all combinations of registers and addressing modes.
Communications.
In communications, multiple-access schemes are orthogonal when an ideal receiver can completely reject arbitrarily strong unwanted signals from the desired signal using different basis functions. One such scheme is TDMA, where the orthogonal basis functions are nonoverlapping rectangular pulses ("time slots").
Another scheme is orthogonal frequency-division multiplexing (OFDM), which refers to the use, by a single transmitter, of a set of frequency multiplexed signals with the exact minimum frequency spacing needed to make them orthogonal so that they do not interfere with each other. Well known examples include (a, g, and n) versions of 802.11 Wi-Fi; WiMAX; ITU-T G.hn, DVB-T, the terrestrial digital TV broadcast system used in most of the world outside North America; and DMT (Discrete Multi Tone), the standard form of ADSL.
In OFDM, the subcarrier frequencies are chosen so that the subcarriers are orthogonal to each other, meaning that crosstalk between the subchannels is eliminated and intercarrier guard bands are not required. This greatly simplifies the design of both the transmitter and the receiver. In conventional FDM, a separate filter for each subchannel is required.
Statistics, econometrics, and economics.
When performing statistical analysis, independent variables that affect a particular dependent variable are said to be orthogonal if they are uncorrelated, since the covariance forms an inner product. In this case the same results are obtained for the effect of any of the independent variables upon the dependent variable, regardless of whether one models the effects of the variables individually with simple regression or simultaneously with multiple regression. If correlation is present, the factors are not orthogonal and different results are obtained by the two methods. This usage arises from the fact that if centered by subtracting the expected value (the mean), uncorrelated variables are orthogonal in the geometric sense discussed above, both as observed data (i.e., vectors) and as random variables (i.e., density functions).
One econometric formalism that is alternative to the maximum likelihood framework, the Generalized Method of Moments, relies on orthogonality conditions. In particular, the Ordinary Least Squares estimator may be easily derived from an orthogonality condition between the explanatory variables and model residuals.
Taxonomy.
In taxonomy, an orthogonal classification is one in which no item is a member of more than one group, that is, the classifications are mutually exclusive.
Combinatorics.
In combinatorics, two "n"×"n" Latin squares are said to be orthogonal if their superimposition yields all possible "n"2 combinations of entries.
Chemistry.
In synthetic organic chemistry orthogonal protection is a strategy allowing the deprotection of functional groups independently of each other. In supramolecular chemistry the notion of orthogonality refers to the possibility of two or more supramolecular, often non-covalent, interactions being compatible; reversibly forming without interference from the other.
In analytical chemistry, analyses are "orthogonal" if they make a measurement or identification in completely different ways, thus increasing the reliability of the measurement. This is often required as a part of a new drug application.
System reliability.
In the field of system reliability orthogonal redundancy is that form of redundancy where the form of backup device or method is completely different from the prone to error device or method. The failure mode of an orthogonally redundant back-up device or method does not intersect with and is completely different from the failure mode of the device or method in need of redundancy to safeguard the total system against catastrophic failure.
Neuroscience.
In neuroscience, a sensory map in the brain which has overlapping stimulus coding (e.g. location and quality) is called an orthogonal map.
Gaming.
In board games such as chess which feature a grid of squares, 'orthogonal' is commonly used to mean "in the same row/'rank' or column/'file'". In this context 'orthogonal' and 'diagonal' are considered opposites.
Other examples.
Stereo vinyl records encode both the left and right stereo channels in a single groove. By making the groove a 90-degree cut into the vinyl, variation in one wall was independent of variations in the other wall. The cartridge senses the resultant motion of the stylus following the groove in two orthogonal directions: 45 degrees from vertical to either side.

</doc>
<doc id="194743" url="https://en.wikipedia.org/wiki?curid=194743" title="Orthonormality">
Orthonormality

In linear algebra, two vectors in an inner product space are orthonormal if they are orthogonal and unit vectors. A set of vectors form an orthonormal set if all vectors in the set are mutually orthogonal and all of unit length. An orthonormal set which forms a basis is called an orthonormal basis.
Intuitive overview.
The construction of orthogonality of vectors is motivated by a desire to extend the intuitive notion of perpendicular vectors to higher-dimensional spaces. In the Cartesian plane, two vectors are said to be "perpendicular" if the angle between them is 90° (i.e. if they form a right angle). This definition can be formalized in Cartesian space by defining the dot product and specifying that two vectors in the plane are orthogonal if their dot product is zero.
Similarly, the construction of the norm of a vector is motivated by a desire to extend the intuitive notion of the length of a vector to higher-dimensional spaces. In Cartesian space, the "norm" of a vector is the square root of the vector dotted with itself. That is,
Many important results in linear algebra deal with collections of two or more orthogonal vectors. But often, it is easier to deal with vectors of unit length. That is, it often simplifies things to only consider vectors whose norm equals 1. The notion of restricting orthogonal pairs of vectors to only those of unit length is important enough to be given a special name. Two vectors which are orthogonal and of length 1 are said to be "orthonormal".
Simple example.
What does a pair of orthonormal vectors in 2-D Euclidean space look like?
Let u = (x1, y1) and v = (x2, y2).
Consider the restrictions on x1, x2, y1, y2 required to make u and v form an orthonormal pair.
Expanding these terms gives 3 equations:
forms an orthonormal set.
However, this is of little consequence, because C[−π,π] is infinite-dimensional, and a finite set of vectors cannot span it. But, removing the restriction that "n" be finite makes the set dense in C[−π,π] and therefore an orthonormal basis of C[−π,π].

</doc>
<doc id="23231" url="https://en.wikipedia.org/wiki?curid=23231" title="Parabola">
Parabola

A parabola (; plural "parabolas" or "parabolae", adjective "parabolic", from ) is a two-dimensional, mirror-symmetrical curve, which is approximately U-shaped when oriented as shown in the diagram below, but which can be in any orientation in its plane. It fits any of several superficially different mathematical descriptions which can all be proved to define curves of exactly the same shape.
One description of a parabola involves a point (the focus) and a line (the directrix). The focus does not lie on the directrix. The parabola is the locus of points in that plane that are equidistant from both the directrix and the focus. Another description of a parabola is as a conic section, created from the intersection of a right circular conical surface and a plane which is parallel to another plane which is tangential to the conical surface. A third description is algebraic. A parabola is a graph of a quadratic function, formula_1, for example.
The line perpendicular to the directrix and passing through the focus (that is, the line that splits the parabola through the middle) is called the "axis of symmetry". The point on the parabola that intersects the axis of symmetry is called the "vertex", and is the point where the parabola is most sharply curved. The distance between the vertex and the focus, measured along the axis of symmetry, is the "focal length". The "latus rectum" is the chord of the parabola which is parallel to the directrix and passes through the focus. Parabolas can open up, down, left, right, or in some other arbitrary direction. Any parabola can be repositioned and rescaled to fit exactly on any other parabola — that is, all parabolas are geometrically similar.
Parabolas have the property that, if they are made of material that reflects light, then light which travels parallel to the axis of symmetry of a parabola and strikes its concave side is reflected to its focus, regardless of where on the parabola the reflection occurs. Conversely, light that originates from a point source at the focus is reflected into a parallel ("collimated") beam, leaving the parabola parallel to the axis of symmetry. The same effects occur with sound and other forms of energy. This reflective property is the basis of many practical uses of parabolas.
The parabola has many important applications, from a parabolic antenna or parabolic microphone to automobile headlight reflectors to the design of ballistic missiles. They are frequently used in physics, engineering, and many other areas.
Strictly, the adjective "parabolic" should be applied only to things that are shaped as a parabola, which is a two-dimensional shape. However, as shown in the last paragraph, the same adjective is commonly used for three-dimensional objects, such as parabolic reflectors, which are really paraboloids. Sometimes, the noun "parabola" is also used to refer to these objects. Though not perfectly correct, this usage is generally understood.
History.
The earliest known work on conic sections was by Menaechmus in the fourth century BC. He discovered a way to solve the problem of doubling the cube using parabolas. (The solution, however, does not meet the requirements imposed by compass and straightedge construction). The area enclosed by a parabola and a line segment, the so-called "parabola segment", was computed by Archimedes via the method of exhaustion in the third century BC, in his "The Quadrature of the Parabola." The name "parabola" is due to Apollonius who discovered many properties of conic sections. It means "application", referring to "application of areas" concept, that has a connection with this curve, as Apollonius had proved. The focus–directrix property of the parabola and other conics is due to Pappus.
Galileo showed that the path of a projectile follows a parabola, a consequence of uniform acceleration due to gravity.
The idea that a parabolic reflector could produce an image was already well known before the invention of the reflecting telescope. Designs were proposed in the early to mid seventeenth century by many mathematicians including René Descartes, Marin Mersenne, and James Gregory. When Isaac Newton built the first reflecting telescope in 1668, he skipped using a parabolic mirror because of the difficulty of fabrication, opting for a spherical mirror. Parabolic mirrors are used in most modern reflecting telescopes and in satellite dishes and radar receivers.
Equation in Cartesian coordinates.
Let the directrix be the line "x" = −"p" and let the focus be the point ("p", 0). If ("x", "y") is a point on the parabola then, by 
definition of a parabola, it is the same distance from the directrix as the focus; in other words:
Squaring both sides and simplifying produces
as the equation of the parabola. By interchanging the roles of "x" and "y" one obtains the corresponding equation of a parabola with a vertical axis as
The equation can be generalized to allow the vertex to be at a point other than the origin by defining the vertex as the point ("h", "k"). The equation of a parabola with a vertical axis then becomes
The last equation can be rewritten
so the graph of any function which is a polynomial of degree 2 in "x" is a parabola with a vertical axis.
More generally, a parabola is a curve in the Cartesian plane defined by an irreducible equation — one that does not factor as a product of two not necessarily distinct linear equations — of the general conic form
with the parabola restriction that
where all of the coefficients are real and where "A" and "C" are not both zero. The equation is irreducible if and only if the determinant of the 3×3 matrix
is non-zero: that is, if ("AC" − "B"2/4)"F" + "BED"/4 − "CD"2/4 − "AE"2/4 ≠ 0. The reducible case, also called the degenerate case, gives a pair of parallel lines, possibly real, possibly imaginary, and possibly coinciding with each other.
Conic section and quadratic form.
The diagram represents a cone with its axis vertical. The point A is its apex. A horizontal cross-section of the cone passes through the points B, E, C, and D. This cross-section is circular, but appears elliptical when viewed obliquely, as is shown in the diagram. An inclined cross-section of the cone, shown in pink, is inclined from the vertical by the same angle, θ, as the side of the cone. According to the definition of a parabola as a conic section, the boundary of this pink cross-section, EPD, is a parabola. The cone also has another horizontal cross-section, which passes through the vertex, P, of the parabola, and is also circular, with a radius which we will call r. Its centre is V, and is a diameter. The chord is a diameter of the lower circle, and passes through the point M, which is the midpoint of the chord . Let us call the lengths of the line segments and x, and the length of y.
Thus:
Using the intersecting chords theorem on the chords BC and DE, we get:
Substituting:
Rearranging:
For any given cone and parabola, "r" and "θ" are constants, but "x" and "y" are variables which depend on the arbitrary height at which the horizontal cross-section BECD is made. This last equation is a simple quadratic one which describes how "x" and "y" are related to each other, and therefore defines the shape of the parabolic curve. This shows that the definition of a parabola as a conic section implies its definition as the graph of a quadratic function. Both definitions produce curves of exactly the same shape.
Focal length.
It is proved in the preceding section that if a parabola has its vertex at the origin, and if it opens in the positive–y direction, then its equation is formula_15, where formula_16 is its focal length. Comparing this with the last equation above shows that the focal length of the parabola in the cone is formula_17.
Position of the focus.
In the diagram above, the point F is the foot of the perpendicular from the point V to the plane of the parabola. By symmetry, F is on the axis of symmetry of the parabola. Angle VPF is complementary to "θ", and angle PVF is complementary to angle VPF, therefore angle PVF is "θ". Since the length of is "r", the distance of F from the vertex of the parabola is "r" sin "θ". It is shown above that this distance equals the focal length of the parabola, which is the distance from the vertex to the focus. The focus and the point F are therefore equally distant from the vertex, along the same line, which implies that they are the same point. Therefore the position of the focus is at the point F, defined above.
Other geometric definitions.
A parabola may also be characterized as a conic section with an eccentricity of 1. As a consequence of this, all parabolas are similar, meaning that while they can be different sizes, they are all the same shape. Another consequence is that the universal parabolic constant is the same for all parabolas. A parabola can also be obtained as the limit of the shape of an ellipse where one focus is kept fixed as the other is allowed to move arbitrarily far away in one direction. In this sense, a parabola may be considered an ellipse that has one focus at infinity. The parabola is an inverse transform of a cardioid.
A parabola has a single axis of reflective symmetry, which passes through its focus and is perpendicular to its directrix. The point of intersection of this axis and the parabola is called the vertex. A parabola spun about this axis in three dimensions traces out a shape known as a paraboloid of revolution.
The parabola is found in numerous situations in the physical world (see below).
Equations.
Cartesian.
In the following equations formula_18 and formula_19 are the coordinates of the vertex formula_20 of the parabola and formula_21 is the distance from the vertex to the focus and the vertex to the directrix.
Vertical axis of symmetry.
where
Parametric form:
Horizontal axis of symmetry.
where
Parametric form:
General parabola.
The general form for a parabola is
This result is derived from the general conic equation given below:
and the fact that, for a parabola,
The equation for a general parabola with a focus point "F"("u", "v"), and a directrix in the form
is
Latus rectum, semilatus rectum, and polar coordinates.
In the polar coordinate system, a parabola with the focus at the origin and the directrix parallel to the "y"-axis, and opening to the left, is given by the equation
where "l" is the "semilatus rectum": the distance from the focus to the parabola itself, measured along a line perpendicular to the axis of symmetry. Note that this equals the perpendicular distance from the focus to the directrix, and is twice the focal length, which is the distance from the focus to the vertex of the parabola.
The latus rectum is the chord that passes through the focus and is perpendicular to the axis of symmetry. It has a length of 2"l".
Dimensions of parabolas with axes of symmetry parallel to the y-axis.
These parabolas have equations of the form formula_40 . By interchanging formula_41 and formula_42 the parabolas' axes of symmetry become parallel to the x-axis.
Coordinates of the vertex.
The "x"-coordinate at the vertex can be found by completing the square to put the equation formula_43 in vertex form, or by differentiating the original equation, setting the resulting formula_44 equal to zero (a critical point), and solving for formula_41. Both methods yield formula_46.
Substituting this into the original equation yields
These terms can be combined over a common denominator:
Thus, the vertex is at the point formula_51.
Coordinates of the focus.
Since the axis of symmetry of this parabola is parallel with the y-axis, the x-coordinates of the focus and the vertex are equal. The coordinates of the vertex are calculated in the preceding section. The x-coordinate of the focus is therefore also formula_52
To find the y-coordinate of the focus, consider the point, "P", located on the parabola where the slope is 1, so the tangent to the parabola at "P" is inclined at 45 degrees to the axis of symmetry. Using the reflective property of a parabola, we know that light which is initially travelling parallel to the axis of symmetry is reflected at "P" toward the focus. The 45-degree inclination causes the light to be turned 90 degrees by the reflection, so it travels from "P" to the focus along a line that is perpendicular to the axis of symmetry and to the y-axis. This means that the y-coordinate of "P" must equal that of the focus.
By differentiating the equation of the parabola and setting the slope to 1, we find the x-coordinate of "P":
Substituting this value of formula_41 in the equation of the parabola, we find the y-coordinate of "P", and also of the focus:
where formula_50 is the discriminant, as used in the "Coordinates of the vertex" section.
The focus is therefore the point
Axis of symmetry, focal length, latus rectum, and directrix.
The above coordinates of the focus of a parabola of the form
can be compared with the coordinates of its vertex, which are derived in the section "Coordinates of the vertex", above, and are
where formula_65
The axis of symmetry is the line which passes through both the focus and the vertex. In this case, it is vertical, with equation
The focal length of the parabola is the difference between the y-coordinates of the focus and the vertex:
It is sometimes useful to invert this equation and use it in the form: formula_69 See the section "Conic section and quadratic form", above.
The point where the slope of the parabola is 1 lies at one end of the latus rectum. The length of the semilatus rectum (half of the latus rectum) is the difference between the x-coordinates of this point, which is considered as "P" in the above derivation of the coordinates of the focus, and of the focus itself. Thus, the length of the semilatus rectum is
The total length of the latus rectum is therefore four times the focal length.
Measured along the axis of symmetry, the vertex is the midpoint between the focus and the directrix. Therefore, the equation of the directrix is
Proof of the reflective property.
The reflective property states that, if a parabola can reflect light, then light which enters it travelling parallel to the axis of symmetry is reflected to the focus. This is derived from the wave nature of light in the paragraph "description of final diagram", which describes a diagram just above it, at the end of this article. This derivation is valid, but may not be satisfying to readers who would prefer a mathematical approach. In the following proof, the fact that every point on the parabola is equidistant from the focus and from the directrix is taken as axiomatic.
Consider the parabola formula_75 Since all parabolas are similar, this simple case represents all others. The right-hand side of the diagram shows part of this parabola.
Construction and definitions
The point "E" is an arbitrary point on the parabola, with coordinates formula_76 The focus is "F", the vertex is "A" (the origin), and the line "FA" (the y-axis) is the axis of symmetry. The line "EC" is parallel to the axis of symmetry, and intersects the x-axis at "D". The point "C" is located on the directrix (which is not shown, to minimize clutter). The point "B" is the midpoint of the line segment "FC".
Deductions
Measured along the axis of symmetry, the vertex, "A", is equidistant from the focus, "F", and from the directrix. Correspondingly, since "C" is on the directrix, the y-coordinates of "F" and "C" are equal in absolute value and opposite in sign. "B" is the midpoint of "FC", so its y-coordinate is zero, so it lies on the x-axis. Its x-coordinate is half that of "E", "D", and "C", i.e. formula_77 The slope of the line "BE" is the quotient of the lengths of "ED" and "BD", which is formula_78 which comes to formula_79
But formula_80 is also the slope (first derivative) of the parabola at "E". Therefore the line "BE" is the tangent to the parabola at "E".
The distances "EF" and "EC" are equal because "E" is on the parabola, "F" is the focus and "C" is on the directrix. Therefore, since "B" is the midpoint of "FC", triangles "FEB" and "CEB" are congruent (three sides), which implies that the angles marked formula_81 are congruent. (The angle above "E" is vertically opposite angle "BEC".) This means that a ray of light which enters the parabola and arrives at "E" travelling parallel to the axis of symmetry will be reflected by the line "BE" so it travels along the line "EF", as shown in red in the diagram (assuming that the lines can somehow reflect light). Since "BE" is the tangent to the parabola at "E", the same reflection will be done by an infinitesimal arc of the parabola at "E". Therefore, light that enters the parabola and arrives at "E" travelling parallel to the axis of symmetry of the parabola is reflected by the parabola toward its focus.
The point "E" has no special characteristics. This conclusion about reflected light applies to all points on the parabola, as is shown on the left side of the diagram. This is the reflective property.
Other consequences.
There are other theorems that can be deduced simply from the above argument.
Tangent bisection property.
The above proof, and the accompanying diagram, show that the tangent "BE" bisects the angle "FEC". In other words, the tangent to the parabola at any point bisects the angle between the lines joining the point to the focus, and perpendicularly to the directrix.
Intersection of a tangent and perpendicular from focus.
Since triangles "FBE" and "CBE" are congruent, "FB" is perpendicular to the tangent "BE". Since "B" is on the x-axis, which is the tangent to the parabola at its vertex, it follows that the point of intersection between any tangent to a parabola and the perpendicular from the focus to that tangent lies on the line that is tangential to the parabola at its vertex. See animated diagram.
Reflection of light striking the convex side.
If light travels along the line "CE", it moves parallel to the axis of symmetry and strikes the convex side of the parabola at "E". It is clear from the above diagram that this light will be reflected directly away from the focus, along an extension of the segment "FE".
Alternative proofs.
The above proofs of the reflective and tangent bisection properties use a line of calculus. For readers who are not comfortable with calculus, the following alternative is presented.
In this diagram, "F" is the focus of the parabola, and "T" and "U" lie on its directrix. "P" is an arbitrary point on the parabola. "PT" is perpendicular to the directrix, and the line "MP" bisects angle "FPT". "Q" is another point on the parabola, with "QU" perpendicular to the directrix. We know that "FP"="PT" and "FQ"="QU". Clearly, "QT">"QU", so "QT">"FQ". All points on the bisector "MP" are equidistant from "F" and "T", but "Q" is closer to "F" than to "T". This means that "Q" is to the "left" of "MP", i.e. on the same side of it as the focus. The same would be true if "Q" were located anywhere else on the parabola (except at the point "P"), so the entire parabola, except the point "P", is on the focus side of "MP". Therefore "MP" is the tangent to the parabola at "P". Since it bisects the angle "FPT", this proves the tangent bisection property.
The logic of the last paragraph can be applied to modify the above proof of the reflective property. It effectively proves the line "BE" to be the tangent to the parabola at "E" if the angles formula_81 are equal. The reflective property follows as shown previously.
Tangent properties.
Two tangent properties related to the latus rectum.
Let the line of symmetry intersect the parabola at point "Q", and denote the focus as point "F" and its distance from point "Q" as "f". Let the perpendicular to the line of symmetry, through the focus, intersect the parabola at a point "T". Then (1) the distance from "F" to "T" is 2"f", and (2) a tangent to the parabola at point "T" intersects the line of symmetry at a 45° angle.
Orthoptic property.
If two tangents to a parabola are perpendicular to each other, then they intersect on the directrix. Conversely, two tangents which intersect on the directrix are perpendicular.
Proof
Without loss of generality, consider the parabola formula_83 Suppose that two tangents contact this parabola at the points formula_84 and formula_85 Their slopes are formula_86 and formula_87 respectively. Thus the equation of the first tangent is of the form formula_88 where formula_89 is a constant. In order to make the line pass through formula_90 the value of formula_89 must be formula_92 so the equation of this tangent is formula_93 Likewise, the equation of the other tangent is formula_94 At the intersection point of the two tangents, formula_95 Thus formula_96 Factoring the difference of squares, cancelling, and dividing by 2 gives formula_97 Substituting this into one of the equations of the tangents gives an expression for the y-coordinate of the intersection point: formula_98 Simplifying this gives formula_99
We now use the fact that these tangents are perpendicular. The product of the slopes of perpendicular lines is −1, assuming that both of the slopes are finite. The slopes of our tangents are formula_86 and formula_87, so formula_102 so formula_103 Thus the y-coordinate of the intersection point of the tangents is given by formula_104 This is also the equation of the directrix of this parabola, so the two perpendicular tangents intersect on the directrix.
Lambert's theorem.
Let three tangents to a parabola form a triangle. Then Lambert's theorem states that the focus of the parabola lies on the circumcircle of the triangle. 
Tsukerman's converse to Lambert's theorem states that, given three lines that bound a triangle, if two of the lines are tangent to a parabola whose focus lies on the circumcircle of the triangle, then the third line is also tangent to the parabola.
Properties proved elsewhere in this article.
Click on link to find description and proof.
Facts related to chords.
Focal length calculated from parameters of a chord.
Suppose a chord crosses a parabola perpendicular to its axis of symmetry. Let the length of the chord between the points where it intersects the parabola be formula_105 and the distance from the vertex of the parabola to the chord, measured along the axis of symmetry, be formula_106 The focal length, formula_107 of the parabola is given by:
Proof
Suppose a system of Cartesian coordinates is used such that the vertex of the parabola is at the origin, and the axis of symmetry is the y-axis. The parabola opens upward. It is shown elsewhere in this article that the equation of the parabola is 4fy=x2, where f is the focal length. At the positive-x end of the chord, x=c/2 and y=d. Since this point is on the parabola, these coordinates must satisfy the equation above. Therefore, by substitution, 4fd=(c/2)2. From this, f=c2/(16d).
Area enclosed between a parabola and a chord.
The area enclosed between a parabola and a chord (see diagram) is two-thirds of the area of a parallelogram which surrounds it. One side of the parallelogram is the chord, and the opposite side is a tangent to the parabola. The slope of the other parallel sides is irrelevant to the area. Often, as here, they are drawn parallel with the parabola's axis of symmetry, but this is arbitrary.
A theorem equivalent to this one, but different in details, was derived by Archimedes in the 3rd Century BCE. He used the areas of triangles, rather than that of the parallelogram. See the article "The Quadrature of the Parabola".
If the chord has length b, and is perpendicular to the parabola's axis of symmetry, and if the perpendicular distance from the parabola's vertex to the chord is h, the parallelogram is a rectangle, with sides of b and h. The area, A, of the parabolic segment enclosed by the parabola and the chord is therefore:
This formula can be compared with the area of a triangle: formula_110.
In general, the enclosed area can be calculated as follows. First, locate the point on the parabola where its slope equals that of the chord. This can be done with calculus, or by using a line that is parallel with the axis of symmetry of the parabola and passes through the midpoint of the chord. The required point is where this line intersects the parabola. Then, using the formula given in the article "Distance from a point to a line", calculate the perpendicular distance from this point to the chord. Multiply this by the length of the chord to get the area of the parallelogram, then by formula_111 to get the required enclosed area.
Corollary concerning midpoints and endpoints of chords.
A corollary of the above discussion is that if a parabola has several parallel chords, their midpoints all lie on a line which is parallel to the axis of symmetry. If tangents to the parabola are drawn through the endpoints of any of these chords, the two tangents intersect on this same line parallel to the axis of symmetry.
Length of an arc of a parabola.
If a point X is located on a parabola which has focal length formula_16, and if formula_21 is the perpendicular distance from X to the axis of symmetry of the parabola, then the lengths of arcs of the parabola which terminate at X can be calculated from formula_16 and formula_21 as follows, assuming they are all expressed in the same units.
This quantity, formula_119, is the length of the arc between X and the vertex of the parabola.
The length of the arc between X and the symmetrically opposite point on the other side of the parabola is formula_120
The perpendicular distance, formula_21, can be given a positive or negative sign to indicate on which side of the axis of symmetry X is situated. Reversing the sign of formula_21 reverses the signs of formula_18 and formula_119 without changing their absolute values. If these quantities are signed, the length of the arc between "any" two points on the parabola is always shown by the difference between their values of formula_125 The calculation can be simplified by using the properties of logarithms:
This can be useful, for example, in calculating the size of the material needed to make a parabolic reflector or parabolic trough.
This calculation can be used for a parabola in any orientation. It is not restricted to the situation where the axis of symmetry is parallel to the y-axis.
Focal length and radius of curvature at the vertex.
The focal length of a parabola is half of its radius of curvature at its vertex.
Proof
Consider a point formula_127 on a circle of radius formula_128 and with centre at the point formula_129 The circle passes through the origin. If the point is near the origin, the Pythagorean Theorem shows that:
formula_131
formula_132
But, if formula_127 is extremely close to the origin, since the x-axis is a tangent to the circle, formula_42 is very small compared with formula_135 so formula_136 is negligible compared with the other terms. Therefore, extremely close to the origin:
Compare this with the parabola:
which has its vertex at the origin, opens upward, and has focal length formula_139. (See preceding sections of this article.)
Equations 1 and 2 are equivalent if formula_140 Therefore this is the condition for the circle and parabola to coincide at and extremely close to the origin. The radius of curvature at the origin, which is the vertex of the parabola, is twice the focal length.
Corollary
A concave mirror which is a small segment of a sphere behaves approximately like a parabolic mirror, focusing parallel light to a point which is midway between the centre and the surface of the sphere.
Mathematical generalizations.
In algebraic geometry, the parabola is generalized by the rational normal curves, which have coordinates formula_141 the standard parabola is the case formula_142 and the case formula_143 is known as the twisted cubic. A further generalization is given by the Veronese variety, when there is more than one input variable.
In the theory of quadratic forms, the parabola is the graph of the quadratic form formula_144 (or other scalings), while the elliptic paraboloid is the graph of the positive-definite quadratic form formula_145 (or scalings) and the hyperbolic paraboloid is the graph of the indefinite quadratic form formula_146 Generalizations to more variables yield further such objects.
The curves formula_147 for other values of "p" are traditionally referred to as the higher parabolas, and were originally treated implicitly, in the form formula_148 for "p" and "q" both positive integers, in which form they are seen to be algebraic curves. These correspond to the explicit formula formula_149 for a positive fractional power of "x." Negative fractional powers correspond to the implicit equation formula_150 and are traditionally referred to as higher hyperbolas. Analytically, "x" can also be raised to an irrational power (for positive values of "x"); the analytic properties are analogous to when "x" is raised to rational powers, but the resulting curve is no longer algebraic, and cannot be analyzed via algebraic geometry.
Parabolas in the physical world.
In nature, approximations of parabolas and paraboloids are found in many diverse situations. The best-known instance of the parabola in the history of physics is the trajectory of a particle or body in motion under the influence of a uniform gravitational field without air resistance (for instance, a ball flying through the air, neglecting air friction).
The parabolic trajectory of projectiles was discovered experimentally by Galileo in the early 17th century, who performed experiments with balls rolling on inclined planes. He also later proved this mathematically in his book "Dialogue Concerning Two New Sciences". For objects extended in space, such as a diver jumping from a diving board, the object itself follows a complex motion as it rotates, but the center of mass of the object nevertheless forms a parabola. As in all cases in the physical world, the trajectory is always an approximation of a parabola. The presence of air resistance, for example, always distorts the shape, although at low speeds, the shape is a good approximation of a parabola. At higher speeds, such as in ballistics, the shape is highly distorted and does not resemble a parabola.
Another hypothetical situation in which parabolas might arise, according to the theories of physics described in the 17th and 18th centuries by Sir Isaac Newton, is in two-body orbits; for example the path of a small planetoid or other object under the influence of the gravitation of the Sun. Parabolic orbits do not occur in nature; simple orbits most commonly resemble hyperbolas or ellipses. The parabolic orbit is the degenerate intermediate case between those two types of ideal orbit. An object following a parabolic orbit would travel at the exact escape velocity of the object it orbits; objects in elliptical or hyperbolic orbits travel at less or greater than escape velocity, respectively. Long-period comets travel close to the Sun's escape velocity while they are moving through the inner solar system, so their paths are close to being parabolic.
Approximations of parabolas are also found in the shape of the main cables on a simple suspension bridge. The curve of the chains of a suspension bridge is always an intermediate curve between a parabola and a catenary, but in practice the curve is generally nearer to a parabola, and in calculations the second degree parabola is used. Under the influence of a uniform load (such as a horizontal suspended deck), the otherwise catenary-shaped cable is deformed toward a parabola. Unlike an inelastic chain, a freely hanging spring of zero unstressed length takes the shape of a parabola. Suspension-bridge cables are, ideally, purely in tension, without having to carry other, e.g. bending, forces. Similarly, the structures of parabolic arches are purely in compression.
Paraboloids arise in several physical situations as well. The best-known instance is the parabolic reflector, which is a mirror or similar reflective device that concentrates light or other forms of electromagnetic radiation to a common focal point, or conversely, collimates light from a point source at the focus into a parallel beam. The principle of the parabolic reflector may have been discovered in the 3rd century BC by the geometer Archimedes, who, according to a legend of debatable veracity, constructed parabolic mirrors to defend Syracuse against the Roman fleet, by concentrating the sun's rays to set fire to the decks of the Roman ships. The principle was applied to telescopes in the 17th century. Today, paraboloid reflectors can be commonly observed throughout much of the world in microwave and satellite-dish receiving and transmitting antennas.
In parabolic microphones, a parabolic reflector that reflects sound, but not necessarily electromagnetic radiation, is used to focus sound onto a microphone, giving it highly directional performance.
Paraboloids are also observed in the surface of a liquid confined to a container and rotated around the central axis. In this case, the centrifugal force causes the liquid to climb the walls of the container, forming a parabolic surface. This is the principle behind the liquid mirror telescope.
Aircraft used to create a weightless state for purposes of experimentation, such as NASA's "Vomit Comet," follow a vertically parabolic trajectory for brief periods in order to trace the course of an object in free fall, which produces the same effect as zero gravity for most purposes.
In the United States, vertical curves in roads are usually parabolic by design.
Gallery.
Click on any image to enlarge it.
Gallery of diagrams.
Click on any image to enlarge it. To shrink back, return to previous page.
Description of final diagram.
Parabolic curve showing chord (L), focus (F), and vertex (V). L is an arbitrary chord of the parabola perpendicular to its axis of symmetry, which passes through V and F. (The ends of the chord are not shown here.) The lengths of all paths Qn - Pn - F are the same, equalling the distance between the chord L and the directrix. (See previous image.) This is similar to saying that a parabola is an ellipse, but with one focal point at infinity. It also directly implies, by the wave nature of light, that parallel light arriving along the lines Qn - Pn will be reflected to converge at F. A linear wavefront along L is concentrated, after reflection, to the one point where all parts of it have travelled equal distances and are in phase, namely F. No consideration of angles is required

</doc>
<doc id="145845" url="https://en.wikipedia.org/wiki?curid=145845" title="Paraboloid">
Paraboloid

In mathematics, a paraboloid is a quadric surface of special kind. There are two kinds of paraboloids: elliptic and hyperbolic.
The "elliptic paraboloid" is shaped like an oval cup and can have a maximum or minimum point. In a suitable coordinate system with three axes formula_1, formula_2, and formula_3, it can be represented by the equation
where formula_5 and formula_6 are constants that dictate the level of curvature in the formula_1-formula_3 and formula_2-formula_3 planes respectively. This is an elliptic paraboloid which opens upward for c>0 and downward for c<0.
The "hyperbolic paraboloid" (not to be confused with a hyperboloid) is a doubly ruled surface shaped like a saddle. In a suitable coordinate system, a hyperbolic paraboloid can be represented by the equation
For c>0, this is a hyperbolic paraboloid that opens down along the x-axis and up along the y-axis (i.e., the parabola in the plane x=0 opens upward and the parabola in the plane y=0 opens downward).
Properties.
With "a = b" an elliptic paraboloid is a "paraboloid of revolution": a surface obtained by revolving a parabola around its axis. It is the shape of the parabolic reflectors used in mirrors, antenna dishes, and the like; and is also the shape of the surface of a rotating liquid, a principle used in liquid mirror telescopes and in making solid telescope mirrors (see Rotating furnace). This shape is also called a "circular paraboloid".
There is a point called the "focus" (or "focal point") on the axis of a circular paraboloid such that, if the paraboloid is a mirror, light from a point source at the focus is reflected into a parallel beam, parallel to the axis of the paraboloid. This also works the other way around: a parallel beam of light incident on the paraboloid parallel to its axis is concentrated at the focal point. This applies also for other waves, hence parabolic antennas. For a geometrical proof, click here.
The hyperbolic paraboloid is a doubly ruled surface: it contains two families of mutually skew lines. The lines in each family are parallel to a common plane, but not to each other.
Curvature.
The elliptic paraboloid, parametrized simply as
has Gaussian curvature
and mean curvature
which are both always positive, have their maximum at the origin, become smaller as a point on the surface moves further away from the origin, and tend asymptotically to zero as the said point moves infinitely away from the origin.
The hyperbolic paraboloid, when parametrized as
has Gaussian curvature
and mean curvature
Multiplication table.
If the hyperbolic paraboloid
is rotated by an angle of π/4 in the +"z" direction (according to the right hand rule), the result is the surface
and if formula_20 then this simplifies to
Finally, letting formula_22, we see that the hyperbolic paraboloid
is congruent to the surface
which can be thought of as the geometric representation (a three-dimensional nomograph, as it were) of a multiplication table.
The two paraboloidal formula_25 functions
and
are harmonic conjugates, and together form the analytic function
which is the analytic continuation of the formula_29 parabolic function formula_30
Dimensions of a paraboloidal dish.
The dimensions of a symmetrical paraboloidal dish are related by the equation: formula_31 where formula_32 is the focal length, formula_33 is the depth of the dish (measured along the axis of symmetry from the vertex to the plane of the rim), and formula_34 is the radius of the rim. Of course, they must all be in the same units. If two of these three quantities are known, this equation can be used to calculate the third.
A more complex calculation is needed to find the diameter of the dish "measured along its surface". This is sometimes called the "linear diameter", and equals the diameter of a flat, circular sheet of material, usually metal, which is the right size to be cut and bent to make the dish. Two intermediate results are useful in the calculation: formula_35 (or the equivalent: formula_36 and formula_37 where formula_38 formula_39 and formula_34 are defined as above. The diameter of the dish, measured along the surface, is then given by: formula_41 where formula_42 means the natural logarithm of formula_43, i.e. its logarithm to base "e".
The volume of the dish, the amount of liquid it could hold if the rim were horizontal and the vertex at the bottom (e.g. the capacity of a paraboloidal wok), is given by formula_44 where the symbols are defined as above. This can be compared with the formulae for the volumes of a cylinder formula_45 a hemisphere formula_46 where formula_47 and a cone formula_48 Of course, formula_49 is the aperture area of the dish, the area enclosed by the rim, which is proportional to the amount of sunlight a reflector dish can intercept. The surface area of a parabolic dish can be found using the area formula for a surface of revolution which gives formula_50.
Applications.
Paraboloidal mirrors are frequently used to bring parallel light to a point focus, e.g. in astronomical telescopes, or to collimate light that has originated from a source at the focus into a parallel beam, e.g. in a searchlight.
The top surface of a fluid in an open-topped rotating container will form a paraboloid. This property can be used to make a liquid mirror telescope with a rotating pool of a reflective liquid, such as mercury, for the primary mirror. The same technique is used to make solid paraboloids, in rotating furnaces.
The widely-sold fried snack food Pringles potato crisps resemble a truncated hyperbolic paraboloid. The distinctive shape of these crisps allows them to be stacked in sturdy tubular containers, fulfilling a design goal that they break less easily than other types of crisp.

</doc>
<doc id="133496" url="https://en.wikipedia.org/wiki?curid=133496" title="Parallelogram">
Parallelogram

In Euclidean geometry, a parallelogram is a (non self-intersecting) quadrilateral with two pairs of parallel sides. The opposite or facing sides of a parallelogram are of equal length and the opposite angles of a parallelogram are of equal measure. The congruence of opposite sides and opposite angles is a direct consequence of the Euclidean parallel postulate and neither condition can be proven without appealing to the Euclidean parallel postulate or one of its equivalent formulations. The three-dimensional counterpart of a parallelogram is a parallelepiped. A quadrilateral with one pair of parallel sides is a trapezoid in American English or a trapezium in British English.
The etymology (in Greek παραλληλ-όγραμμον, a shape "of parallel lines") reflects the definition.
Characterizations.
A simple (non self-intersecting) quadrilateral is a parallelogram if and only if any one of the following statements is true:
Note that this means that all parallelograms have all the properties listed above, and conversely, if just one of these statements is true in a convex quadrilateral, then it is a parallelogram.
Area formula.
formula_1
Another parallelogram with the same base and height.
Given any parallelogram divided in half through one of the diagonals, one can move one of the triangles to the other side of the other triangle to get another parallelogram with the same base and height, and thus the same area.
Area in terms of Cartesian coordinates of vertices.
Let vectors formula_10 and let formula_11 denote the matrix with elements of a and b. Then the area of the parallelogram generated by a and b is equal to formula_12.
Let vectors formula_13 and let formula_14. Then the area of the parallelogram generated by a and b is equal to formula_15.
Let points formula_16. Then the area of the parallelogram with vertices at "a", "b" and "c" is equivalent to the absolute value of the determinant of a matrix built using "a", "b" and "c" as rows with the last column padded using ones as follows:
Proof that diagonals bisect each other.
To prove that the diagonals of a parallelogram bisect each other, we will use congruent triangles:
(since these are angles that a transversal makes with parallel lines "AB" and "DC").
Also, side "AB" is equal in length to side "DC", since opposite sides of a parallelogram are equal in length.
Therefore triangles "ABE" and "CDE" are congruent (ASA postulate, "two corresponding angles and the included side").
Therefore,
Since the diagonals "AC" and "BD" divide each other into segments of equal length, the diagonals bisect each other.
Separately, since the diagonals "AC" and "BD" bisect each other at point "E", point "E" is the midpoint of each diagonal.
Parallelograms arising from other figures.
Automedian triangle.
An automedian triangle is one whose medians are in the same proportions as its sides (though in a different order). If "ABC" is an automedian triangle in which vertex "A" stands opposite the side "a", "G" is the centroid (where the three medians of "ABC" intersect), and "AL" is one of the extended medians of "ABC" with "L" lying on the circumcircle of "ABC", then "BGCL" is a parallelogram.
Varignon parallelogram.
The midpoints of the sides of an arbitrary quadrilateral are the vertices of a parallelogram, called its Varignon parallelogram. If the quadrilateral is convex or concave (that is, not self-intersecting), then the area of the Varignon parallelogram is half the area of the quadrilateral.
Tangent parallelogram of an ellipse.
For an ellipse, two diameters are said to be conjugate if and only if the tangent line to the ellipse at an endpoint of one diameter is parallel to the other diameter. Each pair of conjugate diameters of an ellipse has a corresponding tangent parallelogram, sometimes called a bounding parallelogram, formed by the tangent lines to the ellipse at the four endpoints of the conjugate diameters. All tangent parallelograms for a given ellipse have the same area.
It is possible to reconstruct an ellipse from any pair of conjugate diameters, or from any tangent parallelogram.
Faces of a parallelepiped.
A parallelepiped is a three-dimensional figure whose six faces are parallelograms.

</doc>
<doc id="1240666" url="https://en.wikipedia.org/wiki?curid=1240666" title="Parity (physics)">
Parity (physics)

In quantum mechanics, a parity transformation (also called parity inversion) is the flip in the sign of "one" spatial coordinate. In three dimensions, it is also often described by the simultaneous flip in the sign of all three spatial coordinates (a point reflection):
It can also be thought of as a test for chirality of a physical phenomenon, in that a parity inversion transforms a phenomenon into its mirror image. A parity transformation on something achiral, on the other hand, can be viewed as an identity transformation. All fundamental interactions of elementary particles, with the exception of the weak interaction, are symmetric under parity. The weak interaction is chiral and thus provides a means for probing chirality in physics. In interactions that are symmetric under parity, such as electromagnetism in atomic and molecular physics, parity serves as a powerful controlling principle underlying quantum transitions.
A matrix representation of P (in any number of dimensions) has determinant equal to −1, and hence is distinct from a rotation, which has a determinant equal to 1. In a two-dimensional plane, a simultaneous flip of all coordinates in sign is "not" a parity transformation; it is the same as a 180°-rotation.
Simple symmetry relations.
Under rotations, classical geometrical objects can be classified into scalars, vectors, and tensors of higher rank. In classical physics, physical configurations need to transform under representations of every symmetry group.
Quantum theory predicts that states in a Hilbert space do not need to transform under representations of the group of rotations, but only under projective representations. The word "projective" refers to the fact that if one projects out the phase of each state, where we recall that the overall phase of a quantum state is not an observable, then a projective representation reduces to an ordinary representation. All representations are also projective representations, but the converse is not true, therefore the projective representation condition on quantum states is weaker than the representation condition on classical states.
The projective representations of any group are isomorphic to the ordinary representations of a central extension of the group. For example, projective representations of the 3-dimensional rotation group, which is the special orthogonal group SO(3), are ordinary representations of the special unitary group SU(2) (see Representation theory of SU(2)). Projective representations of the rotation group that are not representations are called spinors, and so quantum states may transform not only as tensors but also as spinors.
If one adds to this a classification by parity, these can be extended, for example, into notions of
One can define reflections such as
which also have negative determinant and form a valid parity transformation. Then, combining them with rotations (or successively performing "x"-, "y"-, and "z"-reflections) one can recover the particular parity transformation defined earlier. The first parity transformation given does not work in an even number of dimensions, though, because it results in a positive determinant. In odd number of dimensions only the latter example of a parity transformation (or any reflection of an odd number of coordinates) can be used.
Parity forms the abelian group Z2 due to the relation . All Abelian groups have only one-dimensional irreducible representations. For Z2, there are two irreducible representations: one is even under parity (), the other is odd (). These are useful in quantum mechanics. However, as is elaborated below, in quantum mechanics states need not transform under actual representations of parity but only under projective representations and so in principle a parity transformation may rotate a state by any phase.
Classical mechanics.
Newton's equation of motion F = "m"a (if the mass is constant) equates two vectors, and hence is invariant under parity. The law of gravity also involves only vectors and is also, therefore, invariant under parity. 
However, angular momentum L is an axial vector,
In classical electrodynamics, the charge density "ρ" is a scalar, the electric field, E, and current j are vectors, but the magnetic field, H is an axial vector. However, Maxwell's equations are invariant under parity because the curl of an axial vector is a vector.
Effect of spatial inversion on some variables of classical physics.
Even.
Classical variables, predominantly scalar quantities, which do not change upon spatial inversion include:
Odd.
Classical variables, predominantly vector quantities, which have their sign flipped by spatial inversion include:
Quantum mechanics.
Possible eigenvalues.
In quantum mechanics, spacetime transformations act on quantum states. The parity transformation, P, is a unitary operator, in general acting on a state ψ as follows: Pψ("r") = "e""iφ"/2ψ(−"r"). 
One must then have P2ψ("r") = "e""iφ"ψ("r"), since an overall phase is unobservable. The operator P2, which reverses the parity of a state twice, leaves the spacetime invariant, and so is an internal symmetry which rotates its eigenstates by phases "e""iφ". If P2 is an element "e""iQ" of a continuous U(1) symmetry group of phase rotations, then "e"−"iQ"/2 is part of this U(1) and so is also a symmetry. In particular, we can define P′ = P"e"−"iQ"/2, which is also a symmetry, and so we can choose to call P′ our parity operator, instead of P. Note that P′2 = 1 and so P′ has eigenvalues ±1. However, when no such symmetry group exists, it may be that all parity transformations have some eigenvalues which are phases other than ±1.
For electronic wavefunctions, even states are usually indicated by a subscript g for "gerade" (German: even) and odd states by a subscript u for "ungerade" (German: odd). For example, the lowest energy level of the hydrogen molecule ion (H2+) is labelled 1σg and the next-lowest 1σu.
Consequences of parity symmetry.
When parity generates the Abelian group ℤ2, one can always take linear combinations of quantum states such that they are either even or odd under parity (see the figure). Thus the parity of such states is ±1. The parity of a multiparticle state is the product of the parities of each state; in other words parity is a multiplicative quantum number
In quantum mechanics, Hamiltonians are invariant (symmetric) under a parity transformation if P commutes with the Hamiltonian. In non-relativistic quantum mechanics, this happens for any potential which is scalar, i.e., "V" = "V"("r"), hence the potential is spherically symmetric. The following facts can be easily proven:
Some of the non-degenerate eigenfunctions of H are unaffected (invariant) by parity P and the others will be merely reversed in sign when the Hamiltonian operator and the parity operator commute:
where "c" is a constant, the eigenvalue of P,
Quantum field theory.
If we can show that the vacuum state is invariant under parity (P|0 = |0), the Hamiltonian is parity invariant ([H, P] = 0) and the quantization conditions remain unchanged under parity, then it follows that every state has good parity, and this parity is conserved in any reaction.
To show that quantum electrodynamics is invariant under parity, we have to prove that the action is invariant and the quantization is also invariant. For simplicity we will assume that canonical quantization is used; the vacuum state is then invariant under parity by construction. The invariance of the action follows from the classical invariance of Maxwell's equations. The invariance of the canonical quantization procedure can be worked out, and turns out to depend on the transformation of the annihilation operator:
where p denotes the momentum of a photon and ± refers to its polarization state. This is equivalent to the statement that the photon has odd intrinsic parity. Similarly all vector bosons can be shown to have odd intrinsic parity, and all axial-vectors to have even intrinsic parity.
There is a straightforward extension of these arguments to scalar field theories which shows that scalars have even parity, since
This is true even for a complex scalar field. ("Details of spinors are dealt with in the article on the "Dirac equation", where it is shown that fermions and antifermions have opposite intrinsic parity.")
With fermions, there is a slight complication because there is more than one spin group.
Parity in the standard model.
Fixing the global symmetries.
In the Standard Model of fundamental interactions there are precisely three global internal U(1) symmetry groups available, with charges equal to the baryon number "B", the lepton number "L" and the electric charge "Q". The product of the parity operator with any combination of these rotations is another parity operator. It is conventional to choose one specific combination of these rotations to define a standard parity operator, and other parity operators are related to the standard one by internal rotations. One way to fix a standard parity operator is to assign the parities of three particles with linearly independent charges "B", "L" and "Q". In general one assigns the parity of the most common massive particles, the proton, the neutron and the electron, to be +1.
Steven Weinberg has shown that if , where "F" is the fermion number operator, then, since the fermion number is the sum of the lepton number plus the baryon number, , for all particles in the Standard Model and since lepton number and baryon number are charges "Q" of continuous symmetries "e""iQ", it is possible to redefine the parity operator so that . However, if there exist Majorana neutrinos, which experimentalists today believe is possible, their fermion number is equal to one because they are neutrinos while their baryon and lepton numbers are zero because they are Majorana, and so (−1)"F" would not be embedded in a continuous symmetry group. Thus Majorana neutrinos would have parity ±"i".
Parity of the pion.
In 1954, a paper by William Chinowsky and Jack Steinberger demonstrated that the pion has negative parity. They studied the decay of an "atom" made from a deuteron () and a negatively charged pion () in a state with zero orbital angular momentum "L" = 0 into two neutrons (n).
Neutrons are fermions and so obey Fermi–Dirac statistics, which implies that the final state is antisymmetric. Using the fact that the deuteron has spin one and the pion spin zero together with the antisymmetry of the final state they concluded that the two neutrons must have orbital angular momentum "L" = 1. The total parity is the product of the intrinsic parities of the particles and the extrinsic parity of the spherical harmonic function (−1)"L". Since the orbital momentum changes from zero to one in this process, if the process is to conserve the total parity then the products of the intrinsic parities of the initial and final particles must have opposite sign. A deuteron nucleus is made from a proton and a neutron, and so using the aforementioned convention that protons and neutrons have intrinsic parities equal to +1 they argued that the parity of the pion is equal to minus the product of the parities of the two neutrons divided by that of the proton and neutron in the deuteron, (−1)(1)2/(1)2, which is equal to minus one. Thus they concluded that the pion is a pseudoscalar particle.
Parity violation.
Although parity is conserved in electromagnetism, strong interactions and gravity, it turns out to be violated in weak interactions. The Standard Model incorporates parity violation by expressing the weak interaction as a chiral gauge interaction. Only the left-handed components of particles and right-handed components of antiparticles participate in weak interactions in the Standard Model. This implies that parity is not a symmetry of our universe, unless a hidden mirror sector exists in which parity is violated in the opposite way.
By the mid-20th Century, it had been suggested by several scientists that parity might not be conserved (in different contexts), but without solid evidence these suggestions were not considered important. Then, in 1956, a careful review and analysis by theoretical physicists Tsung Dao Lee and Chen Ning Yang went further, showing that while parity conservation had been verified in decays by the strong or electromagnetic interactions, it was untested in the weak interaction. They proposed several possible direct experimental tests. They were mostly ignored, but Lee was able to convince his Columbia colleague Chien-Shiung Wu to try it. She needed special cryogenic facilities and expertise, so the experiment was done at the National Bureau of Standards.
In 1957 C. S. Wu, E. Ambler, R. W. Hayward, D. D. Hoppes, and R. P. Hudson found a clear violation of parity conservation in the beta decay of cobalt-60. As the experiment was winding down, with double-checking in progress, Wu informed Lee and Yang of their positive results, and saying the results need further examination, she asked them not to publicize the results first. However, Lee revealed the results to his Columbia colleagues on 4 January 1957 at a "Friday Lunch" gathering of the Physics Department of Columbia. Three of them, R. L. Garwin, Leon Lederman, and R. Weinrich modified an existing cyclotron experiment, and they immediately verified the parity violation. They delayed publication of their results until after Wu's group was ready, and the two papers appeared back to back in the same physics journal.
After the fact, it was noted that an obscure 1928 experiment had in effect reported parity violation in weak decays, but since the appropriate concepts had not yet been developed, those results had no impact. The discovery of parity violation immediately explained the outstanding τ–θ puzzle in the physics of kaons.
In 2010, it was reported that physicists working with the Relativistic Heavy Ion Collider (RHIC) had created a short-lived parity symmetry-breaking bubble in quark-gluon plasmas. An experiment conducted by several physicists including Yale's Jack Sandweiss as part of the STAR collaboration, suggested that parity may also be violated in the strong interaction.
Intrinsic parity of hadrons.
To every particle one can assign an intrinsic parity as long as nature preserves parity. Although weak interactions do not, one can still assign a parity to any hadron by examining the strong interaction reaction that produces it, or through decays not involving the weak interaction, such as rho meson decay to pions.

</doc>
<doc id="52565" url="https://en.wikipedia.org/wiki?curid=52565" title="Partial derivative">
Partial derivative

In mathematics, a partial derivative of a function of several variables is its derivative with respect to one of those variables, with the others held constant (as opposed to the total derivative, in which all variables are allowed to vary). Partial derivatives are used in vector calculus and differential geometry.
The partial derivative of a function "f"("x", "y", ...) with respect to the variable "x" is variously denoted by
Since in general a partial derivative is a function of the same arguments as was the original function, this functional dependence is sometimes explicitly included in the notation, as in
The partial-derivative symbol is ∂. One of the first known uses of the symbol in mathematics is by Marquis de Condorcet from 1770, who used it for partial differences. The modern partial derivative notation is by Adrien-Marie Legendre (1786), though he later abandoned it; Carl Gustav Jacob Jacobi re-introduced the symbol in 1841.
Introduction.
Suppose that "ƒ" is a function of more than one variable. For instance,
The graph of this function defines a surface in Euclidean space. To every point on this surface, there are an infinite number of tangent lines. Partial differentiation is the act of choosing one of these lines and finding its slope. Usually, the lines of most interest are those that are parallel to the "xz"-plane, and those that are parallel to the "yz"-plane (which result from holding either y or x constant, respectively.)
To find the slope of the line tangent to the function at P that is parallel to the "xz"-plane, the "y" variable is treated as constant. The graph and this plane are shown on the right. On the graph below it, we see the way the function looks on the plane . By finding the derivative of the equation while assuming that "y" is a constant, the slope of "ƒ" at the point is found to be:
So at , by substitution, the slope is 3. Therefore
at the point . That is, the partial derivative of "z" with respect to "x" at is 3, as shown in the graph.
Definition.
Basic definition.
The function "f" can be reinterpreted as a family of functions of one variable indexed by the other variables:
In other words, every value of "y" defines a function, denoted "fy", which is a function of one variable "x". That is,
Once a value of "y" is chosen, say "a", then "f"("x","y") determines a function "fa" which traces a curve "x"2 + "ax" + "a"2 on the xz plane:
In this expression, "a" is a "constant", not a "variable", so "fa" is a function of only one real variable, that being "x". Consequently, the definition of the derivative for a function of one variable applies:
The above procedure can be performed for any choice of "a". Assembling the derivatives together into a function gives a function which describes the variation of "f" in the "x" direction:
This is the partial derivative of "f" with respect to "x". Here ∂ is a rounded "d" called the partial derivative symbol. To distinguish it from the letter "d", ∂ is sometimes pronounced "tho" or "partial" 
In general, the partial derivative of a function "f"("x"1...,"x""n") in the direction "xi" at the point ("a"1...,"an") is defined to be:
In the above difference quotient, all the variables except "xi" are held fixed. That choice of fixed values determines a function of one variable formula_12, and by definition,
In other words, the different choices of "a" index a family of one-variable functions just as in the example above. This expression also shows that the computation of partial derivatives reduces to the computation of one-variable derivatives.
An important example of a function of several variables is the case of a scalar-valued function "f"("x"1..."x""n") on a domain in Euclidean space formula_14 (e.g., on formula_15 or formula_16). In this case "f" has a partial derivative ∂"f"/∂"x""j" with respect to each variable "x""j". At the point "a", these partial derivatives define the vector
This vector is called the gradient of "f" at "a". If "f" is differentiable at every point in some domain, then the gradient is a vector-valued function ∇"f" which takes the point "a" to the vector ∇"f"("a"). Consequently, the gradient produces a vector field.
A common abuse of notation is to define the del operator (∇) as follows in three-dimensional Euclidean space formula_16 with unit vectors formula_19:
Or, more generally, for "n"-dimensional Euclidean space formula_14 with coordinates (x1, x2, x3...,x"n") and unit vectors (formula_22):
Formal definition.
Like ordinary derivatives, the partial derivative is defined as a limit. Let "U" be an open subset of R"n" and "f" : "U" → R a function. The partial derivative of "f" at the point a = ("a"1, ..., "a""n") ∈ "U" with respect to the "i"-th variable "a""i" is defined as
Even if all partial derivatives ∂"f"/∂"a""i"("a") exist at a given point "a", the function need not be continuous there. However, if all partial derivatives exist in a neighborhood of "a" and are continuous there, then "f" is totally differentiable in that neighborhood and the total derivative is continuous. In this case, it is said that "f" is a C1 function. This can be used to generalize for vector valued functions ("f" : "U" → "R"'"m") by carefully using a componentwise argument.
The partial derivative formula_25 can be seen as another function defined on "U" and can again be partially differentiated. If all mixed second order partial derivatives are continuous at a point (or on a set), "f" is termed a C2 function at that point (or on that set); in this case, the partial derivatives can be exchanged by Clairaut's theorem:
Examples.
Geometry.
The volume "V" of a cone depends on the cone's height "h" and its radius "r" according to the formula
The partial derivative of "V" with respect to "r" is
which represents the rate with which a cone's volume changes if its radius is varied and its height is kept constant. The partial derivative with respect to "h" is
which represents the rate with which the volume changes if its height is varied and its radius is kept constant.
By contrast, the "total" derivative of "V" with respect to "r" and "h" are respectively
and
The difference between the total and partial derivative is the elimination of indirect dependencies between variables in partial derivatives.
If (for some arbitrary reason) the cone's proportions have to stay the same, and the height and radius are in a fixed ratio "k",
This gives the total derivative with respect to "r":
which simplifies to:
Similarly, the total derivative with respect to "h" is:
The total derivative with respect to "both" r and h of the volume intended as scalar function of these two variables is given by the gradient vector formula_36.
Optimization.
Partial derivatives appear in any calculus-based optimization problem with more than one choice variable. For example, in economics a firm may wish to maximize profit π("x", "y") with respect to the choice of the quantities "x" and "y" of two different types of output. The first order conditions for this optimization are π"x" = 0 = π"y". Since both partial derivatives π"x" and π"y" will generally themselves be functions of both arguments "x" and "y", these two first order conditions form a system of two equations in two unknowns.
Science and engineering.
Equations involving an unknown function's partial derivatives are called partial differential equations. These equations are used to mathematically approximate many physical phenomena like fluid flows, force in a spring, nerve conduction and are frequently encountered in physics, engineering, and other sciences and applied disciplines.
Economics.
Partial derivatives play a prominent role in economics, in which most functions describing economic behaviour posit that the behaviour depends on more than one variable. For example a societal consumption function may describe the amount spent on consumer goods as depending on both income and wealth; the marginal propensity to consume is then the partial derivative of the consumption function with respect to income.
Notation.
For the following examples, let "f" be a function in "x", "y" and "z".
First-order partial derivatives:
Second-order partial derivatives:
Second-order mixed derivatives:
Higher-order partial and mixed derivatives:
When dealing with functions of multiple variables, some of these variables may be related to each other, and it may be necessary to specify explicitly which variables are being held constant. In fields such as statistical mechanics, the partial derivative of "f" with respect to "x", holding "y" and "z" constant, is often expressed as
Antiderivative analogue.
There is a concept for partial derivatives that is analogous to antiderivatives for regular derivatives. Given a partial derivative, it allows for the partial recovery of the original function.
Consider the example of formula_4. The "partial" integral can be taken with respect to "x" (treating "y" as constant, in a similar manner to partial differentiation):
Here, the "constant" of integration is no longer a constant, but instead a function of all the variables of the original function except "x". The reason for this is that all the other variables are treated as constant when taking the partial derivative, so any function which does not involve formula_44 will disappear when taking the partial derivative, and we have to account for this when we take the antiderivative. The most general way to represent this is to have the "constant" represent an unknown function of all the other variables.
Thus the set of functions formula_45, where "g" is any one-argument function, represents the entire set of functions in variables "x","y" that could have produced the "x"-partial derivative formula_46.
If all the partial derivatives of a function are known (for example, with the gradient), then the antiderivatives can be matched via the above process to reconstruct the original function up to a constant.
Higher order partial derivatives.
Second and higher order partial derivatives are defined analogously to the higher order derivatives of univariate functions. For the function formula_47 the "own" second partial derivative with respect to "x" is simply the partial derivative of the partial derivative (both with respect to "x"):
The cross partial derivative with respect to "x" and "y" is obtained by taking the partial derivative of "f" with respect to "x", and then taking the partial derivative of the result with respect to "y", to obtain
Schwarz' theorem states that if the second derivatives are continuous the expression for the cross partial derivative is unaffected by which variable the partial derivative is taken with respect to first and which is taken second. That is,
or equivalently formula_51
Own and cross partial derivatives appear in the Hessian matrix which is used in the second order conditions in optimization problems.

</doc>
<doc id="52564" url="https://en.wikipedia.org/wiki?curid=52564" title="Partial differential equation">
Partial differential equation

In mathematics, a partial differential equation (PDE) is a differential equation that contains unknown multivariable functions and their partial derivatives. (A special case are ordinary differential equations (ODEs), which deal with functions of a single variable and their derivatives.) PDEs are used to formulate problems involving functions of several variables, and are either solved by hand, or used to create a relevant computer model.
PDEs can be used to describe a wide variety of phenomena such as sound, heat, electrostatics, electrodynamics, fluid flow, elasticity, or quantum mechanics. These seemingly distinct physical phenomena can be formalised similarly in terms of PDEs. Just as ordinary differential equations often model one-dimensional dynamical systems, partial differential equations often model multidimensional systems. PDEs find their generalisation in stochastic partial differential equations.
Introduction.
Partial differential equations (PDEs) are equations that involve rates of change with respect to continuous variables. The position of a rigid body is specified by six numbers, but the configuration of a fluid is given by the continuous distribution of several parameters, such as the temperature, pressure, and so forth. The dynamics for the rigid body take place in a finite-dimensional configuration space; the dynamics for the ﬂuid occur in an infinite-dimensional conﬁguration space. This distinction usually makes PDEs much harder to solve than ordinary differential equations (ODEs), but here again there will be simple solutions for linear problems. Classic domains where PDEs are used include acoustics, fluid flow, electrodynamics, and heat transfer.
A partial differential equation (PDE) for the function formula_1 is an equation of the form
If "f" is a linear function of "u" and its derivatives, then the PDE is called linear. Common examples of linear PDEs include the heat equation, the wave equation, Laplace's equation, Helmholtz equation, Klein–Gordon equation, and Poisson's equation.
A relatively simple PDE is
This relation implies that the function "u"("x","y") is independent of "x". However, the equation gives no information on the function's dependence on the variable "y". Hence the general solution of this equation is
where "f" is an arbitrary function of "y". The analogous ordinary differential equation is
which has the solution
where "c" is any constant value. These two examples illustrate that general solutions of ordinary differential equations (ODEs) involve arbitrary constants, but solutions of PDEs involve arbitrary functions. A solution of a PDE is generally not unique; additional conditions must generally be specified on the boundary of the region where the solution is defined. For instance, in the simple example above, the function "f(y)" can be determined if "u" is specified on the line "x" = 0.
Existence and uniqueness.
Although the issue of existence and uniqueness of solutions of ordinary differential equations has a very satisfactory answer with the Picard–Lindelöf theorem, that is far from the case for partial differential equations. The Cauchy–Kowalevski theorem states that the Cauchy problem for any partial differential equation whose coefficients are analytic in the unknown function and its derivatives, has a locally unique analytic solution. Although this result might appear to settle the existence and uniqueness of solutions, there are examples of linear partial differential equations whose coefficients have derivatives of all orders (which are nevertheless not analytic) but which have no solutions at all: see Lewy (1957). Even if the solution of a partial differential equation exists and is unique, it may nevertheless have undesirable properties. The mathematical study of these questions is usually in the more powerful context of weak solutions.
An example of pathological behavior is the sequence (depending upon "n") of Cauchy problems for the Laplace equation
with boundary conditions
where "n" is an integer. The derivative of "u" with respect to "y" approaches 0 uniformly in "x" as "n" increases, but the solution is
This solution approaches infinity if "nx" is not an integer multiple of π for any non-zero value of "y". The Cauchy problem for the Laplace equation is called "ill-posed" or "not well posed", since the solution does not depend continuously upon the data of the problem. Such ill-posed problems are not usually satisfactory for physical applications.
Notation.
In PDEs, it is common to denote partial derivatives using subscripts. That is:
Especially in physics, del or Nabla (∇) is often used to denote spatial derivatives, and formula_14 for time derivatives. For example, the wave equation (described below) can be written as
or
where Δ is the Laplace operator.
Examples.
Heat equation in one space dimension.
The equation for conduction of heat in one dimension for a homogeneous body has
where "u"("t","x") is temperature, and α is a positive constant that describes the rate of diffusion. The Cauchy problem for this equation consists in specifying "u"(0, "x")= "f"("x"), where "f"("x") is an arbitrary function.
General solutions of the heat equation can be found by the method of separation of variables. Some examples appear in the heat equation article. They are examples of Fourier series for periodic "f" and Fourier transforms for non-periodic "f". Using the Fourier transform, a general solution of the heat equation has the form
where "F" is an arbitrary function. To satisfy the initial condition, "F" is given by the Fourier transform of "f", that is
If "f" represents a very small but intense source of heat, then the preceding integral can be approximated by the delta distribution, multiplied by the strength of the source. For a source whose strength is normalized to 1, the result is
and the resulting solution of the heat equation is
This is a Gaussian integral. It may be evaluated to obtain
This result corresponds to the normal probability density for "x" with mean 0 and variance 2α"t". The heat equation and similar diffusion equations are useful tools to study random phenomena.
Wave equation in one spatial dimension.
The wave equation is an equation for an unknown function "u"("k", "x") of the form
Here "u" might describe the displacement of a stretched string from equilibrium, or the difference in air pressure in a tube, or the magnitude of an electromagnetic field in a tube, and "m" is a number that corresponds to the velocity of the wave. The Cauchy problem for this equation consists in prescribing the initial displacement and velocity of a string or other medium:
where "f" and "g" are arbitrary given functions. The solution of this problem is given by d'Alembert's formula:
This formula implies that the solution at ("k","x") depends only upon the data on the segment of the initial line that is cut out by the characteristic curves
that are drawn backwards from that point. These curves correspond to signals that propagate with velocity "m" forward and backward. Conversely, the influence of the data at any given point on the initial line propagates with the finite velocity "m": there is no effect outside a triangle through that point whose sides are characteristic curves. This behavior is very different from the solution for the heat equation, where the effect of a point source appears (with small amplitude) instantaneously at every point in space. The solution given above is also valid if "k" < 0, and the explicit formula shows that the solution depends smoothly upon the data: both the forward and backward Cauchy problems for the wave equation are well-posed.
Generalised heat-like equation in one space dimension.
Where heat-like equation means equations of the form:
where formula_29 is a Sturm–Liouville operator subject to the boundary conditions:
Then:
If:
where
Spherical waves.
Spherical waves are waves whose amplitude depends only upon the radial distance "r" from a central point source. For such waves, the three-dimensional wave equation takes the form
This is equivalent to
and hence the quantity "ru" satisfies the one-dimensional wave equation. Therefore a general solution for spherical waves has the form
where "F" and "G" are completely arbitrary functions. Radiation from an antenna corresponds to the case where "G" is identically zero. Thus the wave form transmitted from an antenna has no distortion in time: the only distorting factor is 1/"r". This feature of undistorted propagation of waves is not present if there are two spatial dimensions.
Laplace equation in two dimensions.
The Laplace equation for an unknown function of two variables φ has the form
Solutions of Laplace's equation are called harmonic functions.
Connection with holomorphic functions.
Solutions of the Laplace equation in two dimensions are intimately connected with analytic functions of a complex variable (a.k.a. holomorphic functions): the real and imaginary parts of any analytic function are conjugate harmonic functions: they both satisfy the Laplace equation, and their gradients are orthogonal. If "f"="u"+"iv", then the Cauchy–Riemann equations state that
and it follows that
Conversely, given any harmonic function in two dimensions, it is the real part of an analytic function, at least locally. Details are given in Laplace equation.
A typical boundary value problem.
A typical problem for Laplace's equation is to find a solution that satisfies arbitrary values on the boundary of a domain. For example, we may seek a harmonic function that takes on the values "u"(θ) on a circle of radius one. The solution was given by Poisson:
Petrovsky (1967, p. 248) shows how this formula can be obtained by summing a Fourier series for φ. If "r" < 1, the derivatives of φ may be computed by differentiating under the integral sign, and one can verify that φ is analytic, even if "u" is continuous but not necessarily differentiable. This behavior is typical for solutions of elliptic partial differential equations: the solutions may be much more smooth than the boundary data. This is in contrast to solutions of the wave equation, and more general hyperbolic partial differential equations, which typically have no more derivatives than the data.
Euler–Tricomi equation.
The Euler–Tricomi equation is used in the investigation of transonic flow.
Advection equation.
The advection equation describes the transport of a conserved scalar ψ in a velocity field u = ("u", "v", "w"). It is:
If the velocity field is solenoidal (that is, ∇⋅u = 0), then the equation may be simplified to
In the one-dimensional case where "u" is not constant and is equal to ψ, the equation is referred to as Burgers' equation.
Ginzburg–Landau equation.
The Ginzburg–Landau equation is used in modelling superconductivity. It is
where "p","q" ∈ C and γ ∈ R are constants and "i" is the imaginary unit.
The Dym equation.
The Dym equation is named for Harry Dym and occurs in the study of solitons. It is
Initial-boundary value problems.
Many problems of mathematical physics are formulated as initial-boundary value problems.
Vibrating string.
If the string is stretched between two points where "x"=0 and "x"="L" and "u" denotes the amplitude of the displacement of the string, then "u" satisfies the one-dimensional wave equation in the region where 0 < "x" < "L" and "t" is unlimited. Since the string is tied down at the ends, "u" must also satisfy the boundary conditions
as well as the initial conditions
The method of separation of variables for the wave equation
leads to solutions of the form
where
where the constant "k" must be determined. The boundary conditions then imply that "X" is a multiple of sin "kx", and "k" must have the form
where "n" is an integer. Each term in the sum corresponds to a mode of vibration of the string. The mode with "n" = 1 is called the fundamental mode, and the frequencies of the other modes are all multiples of this frequency. They form the overtone series of the string, and they are the basis for musical acoustics. The initial conditions may then be satisfied by representing "f" and "g" as infinite sums of these modes. Wind instruments typically correspond to vibrations of an air column with one end open and one end closed. The corresponding boundary conditions are
The method of separation of variables can also be applied in this case, and it leads to a series of odd overtones.
The general problem of this type is solved in Sturm–Liouville theory.
Vibrating membrane.
If a membrane is stretched over a curve "C" that forms the boundary of a domain "D" in the plane, its vibrations are governed by the wave equation
if "t">0 and ("x","y") is in "D". The boundary condition is "u(t,x,y)" = 0 if "(x,y)" is on "C". The method of separation of variables leads to the form
which in turn must satisfy
The latter equation is called the Helmholtz Equation. The constant "k" must be determined to allow a non-trivial "v" to satisfy the boundary condition on "C". Such values of "k"2 are called the eigenvalues of the Laplacian in "D", and the associated solutions are the eigenfunctions of the Laplacian in "D". The Sturm–Liouville theory may be extended to this elliptic eigenvalue problem (Jost, 2002).
Other examples.
The Schrödinger equation is a PDE at the heart of non-relativistic quantum mechanics. In the WKB approximation it is the Hamilton–Jacobi equation.
Except for the Dym equation and the Ginzburg–Landau equation, the above equations are linear in the sense that they can be written in the form "Au" = "f" for a given linear operator "A" and a given function "f". Other important non-linear equations include the Navier–Stokes equations describing the flow of fluids, and Einstein's field equations of general relativity.
Also see the list of non-linear partial differential equations.
Classification.
Some linear, second-order partial differential equations can be classified as parabolic, hyperbolic and elliptic. Others such as the Euler–Tricomi equation have different types in different regions. The classification provides a guide to appropriate initial and boundary conditions, and to smoothness of the solutions.
Equations of second order.
Assuming formula_60, the general second-order PDE in two independent variables has the form
where the coefficients "A", "B", "C" etc. may depend upon "x" and "y". If formula_62 over a region of the xy plane, the PDE is second-order in that region. This form is analogous to the equation for a conic section:
More precisely, replacing ∂"x" by "X", and likewise for other variables (formally this is done by a Fourier transform), converts a constant-coefficient PDE into a polynomial of the same degree, with the top degree (a homogeneous polynomial, here a quadratic form) being most significant for the classification.
Just as one classifies conic sections and quadratic forms into parabolic, hyperbolic, and elliptic based on the discriminant formula_64, the same can be done for a second-order PDE at a given point. However, the discriminant in a PDE is given by formula_65 due to the convention of the "xy" term being 2"B" rather than "B"; formally, the discriminant (of the associated quadratic form) is formula_66 with the factor of 4 dropped for simplicity.
If there are "n" independent variables "x"1, "x"2 , ..., "x""n", a general linear partial differential equation of second order has the form
The classification depends upon the signature of the eigenvalues of the coefficient matrix "ai,j"..
Systems of first-order equations and characteristic surfaces.
The classification of partial differential equations can be extended to systems of first-order equations, where the unknown "u" is now a vector with "m" components, and the coefficient matrices "A"ν are "m" by "m" matrices for ν = 1, ..., "n". The partial differential equation takes the form
where the coefficient matrices "A"ν and the vector "B" may depend upon "x" and "u". If a hypersurface "S" is given in the implicit form
where φ has a non-zero gradient, then "S" is a characteristic surface for the operator "L" at a given point if the characteristic form vanishes:
The geometric interpretation of this condition is as follows: if data for "u" are prescribed on the surface "S", then it may be possible to determine the normal derivative of "u" on "S" from the differential equation. If the data on "S" and the differential equation determine the normal derivative of "u" on "S", then "S" is non-characteristic. If the data on "S" and the differential equation "do not" determine the normal derivative of "u" on "S", then the surface is characteristic, and the differential equation restricts the data on "S": the differential equation is "internal" to "S".
Equations of mixed type.
If a PDE has coefficients that are not constant, it is possible that it will not belong to any of these categories but rather be of mixed type. A simple but important example is the Euler–Tricomi equation
which is called elliptic-hyperbolic because it is elliptic in the region "x" < 0, hyperbolic in the region "x" > 0, and degenerate parabolic on the line "x" = 0.
Infinite-order PDEs in quantum mechanics.
In the phase space formulation of quantum mechanics, one may consider the quantum Hamilton's equations for trajectories of quantum particles. These equations are infinite-order PDEs. However, in the semiclassical expansion, one has a finite system of ODEs at any fixed order of ħ. The evolution equation of the Wigner function is also an infinite-order PDE. The quantum trajectories are quantum characteristics, with the use of which one could calculate the evolution of the Wigner function.
Analytical methods to solve PDEs.
Separation of variables.
Linear PDEs can be reduced to systems of ordinary differential equations by the important technique of separation of variables. This technique rests on a characteristic of solutions to differential equations: if one can ﬁnd any solution that solves the equation and satisﬁes the boundary conditions, then it is "the" solution (this also applies to ODEs). We assume as an ansatz that the dependence of a solution on the parameters space and time can be written as a product of terms that each depend on a single parameter, and then see if this can be made to solve the problem.
In the method of separation of variables, one reduces a PDE to a PDE in fewer variables, which is an ordinary differential equation if in one variable – these are in turn easier to solve.
This is possible for simple PDEs, which are called separable partial differential equations, and the domain is generally a rectangle (a product of intervals). Separable PDEs correspond to diagonal matrices – thinking of "the value for fixed "x"" as a coordinate, each coordinate can be understood separately.
This generalizes to the method of characteristics, and is also used in integral transforms.
Method of characteristics.
In special cases, one can find characteristic curves on which the equation reduces to an ODE – changing coordinates in the domain to straighten these curves allows separation of variables, and is called the method of characteristics.
More generally, one may find characteristic surfaces.
Integral transform.
An integral transform may transform the PDE to a simpler one, in particular a separable PDE. This corresponds to diagonalizing an operator.
An important example of this is Fourier analysis, which diagonalizes the heat equation using the eigenbasis of sinusoidal waves.
If the domain is finite or periodic, an infinite sum of solutions such as a Fourier series is appropriate, but an integral of solutions such as a Fourier integral is generally required for infinite domains. The solution for a point source for the heat equation given above is an example for use of a Fourier integral.
Change of variables.
Often a PDE can be reduced to a simpler form with a known solution by a suitable change of variables. For example the Black–Scholes PDE
is reducible to the heat equation
by the change of variables (for complete details see )
Fundamental solution.
Inhomogeneous equations can often be solved (for constant coefficient PDEs, always be solved) by finding the fundamental solution (the solution for a point source), then taking the convolution with the boundary conditions to get the solution.
This is analogous in signal processing to understanding a filter by its impulse response.
Superposition principle.
Because any superposition of solutions of a linear, homogeneous PDE is again a solution, the particular solutions may then be combined to obtain more general solutions.
if u1 and u2 are solutions of a homogeneous linear pde in same region R, then u= c1u1+c2u2
with any constants c1 and c2 is also a solution of that pde in that same region...
Methods for non-linear equations.
There are no generally applicable methods to solve non-linear PDEs. Still, existence and uniqueness results (such as the Cauchy–Kowalevski theorem) are often possible, as are proofs of important qualitative and quantitative properties of solutions (getting these results is a major part of analysis). Computational solution to the nonlinear PDEs, the split-step method, exist for specific equations like nonlinear Schrödinger equation.
Nevertheless, some techniques can be used for several types of equations. The h-principle is the most powerful method to solve underdetermined equations. The Riquier–Janet theory is an effective method for obtaining information about many analytic overdetermined systems.
The method of characteristics (similarity transformation method) can be used in some very special cases to solve partial differential equations.
In some cases, a PDE can be solved via perturbation analysis in which the solution is considered to be a correction to an equation with a known solution. Alternatives are numerical analysis techniques from simple finite difference schemes to the more mature multigrid and finite element methods. Many interesting problems in science and engineering are solved in this way using computers, sometimes high performance supercomputers.
Lie group method.
From 1870 Sophus Lie's work put the theory of differential equations on a more satisfactory foundation. He showed that the integration theories of the older mathematicians can, by the introduction of what are now called Lie groups, be referred to a common source; and that ordinary differential equations which admit the same infinitesimal transformations present comparable difficulties of integration. He also emphasized the subject of transformations of contact.
A general approach to solve PDE's uses the symmetry property of differential equations, the continuous infinitesimal transformations of solutions to solutions (Lie theory). Continuous group theory, Lie algebras and differential geometry are used to understand the structure of linear and nonlinear partial differential equations for generating integrable equations, to find its Lax pairs, recursion operators, Bäcklund transform and finally finding exact analytic solutions to the PDE.
Symmetry methods have been recognized to study differential equations arising in mathematics, physics, engineering, and many other disciplines.
Semianalytical methods.
The adomian decomposition method, the Lyapunov artificial small parameter method, and He's homotopy perturbation method are all special cases of the more general homotopy analysis method. These are series expansion methods, and except for the Lyapunov method, are independent of small physical parameters as compared to the well known perturbation theory, thus giving these methods greater flexibility and solution generality.
Numerical methods to solve PDEs.
The three most widely used numerical methods to solve PDEs are the finite element method (FEM), finite volume methods (FVM) and finite difference methods (FDM). The FEM has a prominent position among these methods and especially its exceptionally efficient higher-order version hp-FEM. Other versions of FEM include the generalized finite element method (GFEM), extended finite element method (XFEM), spectral finite element method (SFEM), meshfree finite element method, discontinuous Galerkin finite element method (DGFEM), Element-Free Galerkin Method (EFGM), Interpolating Element-Free Galerkin Method (IEFGM), etc.
Finite element method.
The finite element method (FEM) (its practical application often known as finite element analysis (FEA)) is a numerical technique for finding approximate solutions of partial differential equations (PDE) as well as of integral equations. The solution approach is based either on eliminating the differential equation completely (steady state problems), or rendering the PDE into an approximating system of ordinary differential equations, which are then numerically integrated using standard techniques such as Euler's method, Runge–Kutta, etc.
Finite difference method.
Finite-difference methods are numerical methods for approximating the solutions to differential equations using finite difference equations to approximate derivatives.
Finite volume method.
Similar to the finite difference method or finite element method, values are calculated at discrete places on a meshed geometry. "Finite volume" refers to the small volume surrounding each node point on a mesh. In the finite volume method, surface integrals in a partial differential equation that contain a divergence term are converted to volume integrals, using the Divergence theorem. These terms are then evaluated as fluxes at the surfaces of each finite volume. Because the flux entering a given volume is identical to that leaving the adjacent volume, these methods are conservative.

</doc>
<doc id="23259" url="https://en.wikipedia.org/wiki?curid=23259" title="Particle physics">
Particle physics

Particle physics is the branch of physics that studies the nature of the particles that constitute "matter" (particles with mass) and "radiation" (massless particles). Although the word "particle" can refer to various types of very small objects (e.g. protons, gas particles, or even household dust), "particle physics" usually investigates the irreducibly smallest detectable particles and the irreducibly fundamental force fields necessary to explain them. By our current understanding, these elementary particles are excitations of the quantum fields that also govern their interactions. The currently dominant theory explaining these fundamental particles and fields, along with their dynamics, is called the Standard Model. Thus, modern particle physics generally investigates the Standard Model and its various possible extensions, e.g. to the newest "known" particle, the Higgs boson, or even to the oldest known force field, gravity.
Subatomic particles.
Modern particle physics research is focused on subatomic particles, including atomic constituents such as electrons, protons, and neutrons (protons and neutrons are composite particles called baryons, made of quarks), produced by radioactive and scattering processes, such as photons, neutrinos, and muons, as well as a wide range of exotic particles.
Dynamics of particles is also governed by quantum mechanics; they exhibit wave–particle duality, displaying particle-like behaviour under certain experimental conditions and wave-like behaviour in others. In more technical terms, they are described by quantum state vectors in a Hilbert space, which is also treated in quantum field theory. Following the convention of particle physicists, the term "elementary particles" is applied to those particles that are, according to current understanding, presumed to be indivisible and not composed of other particles.
All particles, and their interactions observed to date, can be described almost entirely by a quantum field theory called the Standard Model. The Standard Model, as currently formulated, has 61 elementary particles.
Those elementary particles can combine to form composite particles, accounting for the hundreds of other species of particles that have been discovered since the 1960s. The Standard Model has been found to agree with almost all the experimental tests conducted to date. However, most particle physicists believe that it is an incomplete description of nature, and that a more fundamental theory awaits discovery (See Theory of Everything). In recent years, measurements of neutrino mass have provided the first experimental deviations from the Standard Model.
History.
The idea that all matter is composed of elementary particles dates to at least the 6th century BC. In the 19th century, John Dalton, through his work on stoichiometry, concluded that each element of nature was composed of a single, unique type of particle. The word "atom", after the Greek word "atomos" meaning "indivisible", denotes the smallest particle of a chemical element since then, but physicists soon discovered that "atoms" are not, in fact, the fundamental particles of nature, but conglomerates of even smaller particles, such as the electron. The early 20th-century explorations of nuclear physics and quantum physics culminated in proofs of nuclear fission in 1939 by Lise Meitner (based on experiments by Otto Hahn), and nuclear fusion by Hans Bethe in that same year; both discoveries also led to the development of nuclear weapons. Throughout the 1950s and 1960s, a bewildering variety of particles were found in scattering experiments. It was referred to as the "particle zoo". That term was deprecated after the formulation of the Standard Model during the 1970s in which the large number of particles was explained as combinations of a (relatively) small number of fundamental particles.
Standard Model.
The current state of the classification of all elementary particles is explained by the Standard Model. It describes the strong, weak, and electromagnetic fundamental interactions, using mediating gauge bosons. The species of gauge bosons are the gluons, , and bosons, and the photons. The Standard Model also contains 24 fundamental particles, (12 particles and their associated anti-particles), which are the constituents of all matter. Finally, the Standard Model also predicted the existence of a type of boson known as the Higgs boson. Early in the morning on 4 July 2012, physicists with the Large Hadron Collider at CERN announced they had found a new particle that behaves similarly to what is expected from the Higgs boson.
Experimental laboratories.
In particle physics, the major international laboratories are located at the:
Many other particle accelerators do exist.
The techniques required to do modern, experimental, particle physics are quite varied and complex, constituting a sub-specialty nearly completely distinct from the theoretical side of the field.
Theory.
Theoretical particle physics attempts to develop the models, theoretical framework, and mathematical tools to understand current experiments and make predictions for future experiments. See also theoretical physics. There are several major interrelated efforts being made in theoretical particle physics today.
One important branch attempts to better understand the Standard Model and its tests. By extracting the parameters of the Standard Model, from experiments with less uncertainty, this work probes the limits of the Standard Model and therefore expands our understanding of nature's building blocks. Those efforts are made challenging by the difficulty of calculating quantities in quantum chromodynamics. Some theorists working in this area refer to themselves as phenomenologists and they may use the tools of quantum field theory and effective field theory. Others make use of lattice field theory and call themselves "lattice theorists".
Another major effort is in model building where model builders develop ideas for what physics may lie beyond the Standard Model (at higher energies or smaller distances). This work is often motivated by the hierarchy problem and is constrained by existing experimental data. It may involve work on supersymmetry, alternatives to the Higgs mechanism, extra spatial dimensions (such as the Randall-Sundrum models), Preon theory, combinations of these, or other ideas.
A third major effort in theoretical particle physics is string theory. "String theorists" attempt to construct a unified description of quantum mechanics and general relativity by building a theory based on small strings, and branes rather than particles. If the theory is successful, it may be considered a "Theory of Everything", or "TOE".
There are also other areas of work in theoretical particle physics ranging from particle cosmology to loop quantum gravity.
This division of efforts in particle physics is reflected in the names of categories on the arXiv, a preprint archive: hep-th (theory), hep-ph (phenomenology), hep-ex (experiments), hep-lat (lattice gauge theory).
Practical applications.
In principle, all physics (and practical applications developed therefrom) can be derived from the study of fundamental particles. In practice, even if "particle physics" is taken to mean only "high-energy atom smashers", many technologies have been developed during these pioneering investigations that later find wide uses in society. Cyclotrons are used to produce medical isotopes for research and treatment (for example, isotopes used in PET imaging), or used directly for certain cancer treatments. The development of Superconductors has been pushed forward by their use in particle physics. The World Wide Web and touchscreen technology were initially developed at CERN.
Additional applications are found in medicine, national security, industry, computing, science, and workforce development, illustrating a long and growing list of beneficial practical applications with contributions from particle physics.
Future.
The primary goal, which is pursued in several distinct ways, is to find and understand what physics may lie beyond the standard model. There are several powerful experimental reasons to expect new physics, including dark matter and neutrino mass. There are also theoretical hints that this new physics should be found at accessible energy scales.
Much of the effort to find this new physics are focused on new collider experiments. The Large Hadron Collider (LHC) was completed in 2008 to help continue the search for the Higgs boson, supersymmetric particles, and other new physics. An intermediate goal is the construction of the International Linear Collider (ILC), which will complement the LHC by allowing more precise measurements of the properties of newly found particles. In August 2004, a decision for the technology of the ILC was taken but the site has still to be agreed upon.
In addition, there are important non-collider experiments that also attempt to find and understand physics beyond the Standard Model. One important non-collider effort is the determination of the neutrino masses, since these masses may arise from neutrinos mixing with very heavy particles. In addition, cosmological observations provide many useful constraints on the dark matter, although it may be impossible to determine the exact nature of the dark matter without the colliders. Finally, lower bounds on the very long lifetime of the proton put constraints on Grand Unified Theories at energy scales much higher than collider experiments will be able to probe any time soon.
In May 2014, the Particle Physics Project Prioritization Panel released its report on particle physics funding priorities for the United States over the next decade. This report emphasized continued U.S. participation in the LHC and ILC, and expansion of the Long Baseline Neutrino Experiment, among other recommendations.

</doc>
<doc id="438476" url="https://en.wikipedia.org/wiki?curid=438476" title="Path integral formulation">
Path integral formulation

The path integral formulation of quantum mechanics is a description of quantum theory which generalizes the action principle of classical mechanics. It replaces the classical notion of a single, unique trajectory for a system with a sum, or functional integral, over an infinity of possible trajectories to compute a quantum amplitude.
The basic idea of the path integral formulation can be traced back to Norbert Wiener, who introduced the Wiener integral for solving problems in diffusion and Brownian motion. This idea was extended to the use of the Lagrangian in quantum mechanics by P. A. M. Dirac in his 1933 paper. The complete method was developed in 1948 by Richard Feynman. Some preliminaries were worked out earlier, in the course of his doctoral thesis work by John Archibald Wheeler. The original motivation stemmed from the desire to obtain a quantum-mechanical formulation for the Wheeler–Feynman absorber theory using a Lagrangian (rather than a Hamiltonian) as a starting point.
This formulation has proven crucial to the subsequent development of theoretical physics, because it is manifestly symmetric between time and space. Unlike previous methods, the path-integral allows a physicist to easily change coordinates between very different canonical descriptions of the same quantum system.
The path integral also relates quantum and stochastic processes, and this provided the basis for the grand synthesis of the 1970s which unified quantum field theory with the statistical field theory of a fluctuating field near a second-order phase transition. The Schrödinger equation is a diffusion equation with an imaginary diffusion constant, and the path integral is an analytic continuation of a method for summing up all possible random walks. For this reason path integrals were used in the study of Brownian motion and diffusion a while before they were introduced in quantum mechanics.
Quantum action principle.
In quantum mechanics, as in classical mechanics, the Hamiltonian is the generator of time-translations. This means that the state at a slightly later time differs from the state at the current time by the result of acting with the Hamiltonian operator (multiplied by the negative imaginary unit, −"i"). For states with a definite energy, this is a statement of the De Broglie relation between frequency and energy, and the general relation is consistent with that plus the superposition principle.
But the Hamiltonian in classical mechanics is derived from a Lagrangian, which is a more fundamental quantity relative to special relativity. The Hamiltonian tells you how to march forward in time, but the time is different in different reference frames. So the Hamiltonian is different in different frames, and this type of symmetry is not apparent in the original formulation of quantum mechanics.
The Hamiltonian is a function of the position and momentum at one time, and it tells you the position and momentum a little later. The Lagrangian is a function of the position now and the position a little later (or, equivalently for infinitesimal time separations, it is a function of the position and velocity). The relation between the two is by a Legendre transform, and the condition that determines the classical equations of motion (the Euler–Lagrange equations) is that the action is an extremum.
In quantum mechanics, the Legendre transform is hard to interpret, because the motion is not over a definite trajectory. So what does the Legendre transform mean? In classical mechanics, with discretization in time,
and
where the partial derivative with respect to formula_3 holds "q"("t" + ε) fixed. The inverse Legendre transform is:
where
and the partial derivative now is with respect to "p" at fixed "q".
In quantum mechanics, the state is a superposition of different states with different values of "q", or different values of "p", and the quantities "p" and "q" can be interpreted as noncommuting operators. The operator "p" is only definite on states that are indefinite with respect to "q". So consider two states separated in time and act with the operator corresponding to the Lagrangian:
If the multiplications implicit in this formula are reinterpreted as matrix multiplications, what does this mean?
It can be given a meaning as follows: The first factor is
If this is interpreted as doing a "matrix" multiplication, the sum over all states integrates over all "q"("t"), and so it takes the Fourier transform in "q"("t"), to change basis to "p"("t"). That is the action on the Hilbert space – change basis to p at time t.
Next comes:
or evolve an infinitesimal time into the future.
Finally, the last factor in this interpretation is
which means change basis back to q at a later time.
This is not very different from just ordinary time evolution: the "H" factor contains all the dynamical information – it pushes the state forward in time. The first part and the last part are just doing Fourier transforms to change to a pure "q" basis from an intermediate "p" basis.
Another way of saying this is that since the Hamiltonian is naturally a function of "p" and "q", exponentiating this quantity and changing basis from "p" to "q" at each step allows the matrix element of "H" to be expressed as a simple function along each path. This function is the quantum analog of the classical action. This observation is due to Paul Dirac.
Dirac further noted that one could square the time-evolution operator in the S representation
and this gives the time evolution operator between time "t" and time "t" + 2ε. While in the "H" representation the quantity that is being summed over the intermediate states is an obscure matrix element, in the S representation it is reinterpreted as a quantity associated to the path. In the limit that one takes a large power of this operator, one reconstructs the full quantum evolution between two states, the early one with a fixed value of "q"(0) and the later one with a fixed value of "q"("t"). The result is a sum over paths with a phase which is the quantum action. Crucially, Dirac identified in this paper the deep quantum mechanical reason for the principle of least action controlling the classical limit (see quotation box).
Feynman's interpretation.
Dirac's work did not provide a precise prescription to calculate the sum over paths, and he did not show that one could recover the Schrödinger equation or the canonical commutation relations from this rule. This was done by Feynman.
Feynman showed that Dirac's quantum action was, for most cases of interest, simply equal to the classical action, appropriately discretized. This means that the classical action is the phase acquired by quantum evolution between two fixed endpoints. He proposed to recover all of quantum mechanics from the following postulates:
In order to find the overall probability amplitude for a given process, then, one adds up, or integrates, the amplitude of postulate 3 over the space of "all" possible paths of the system in between the initial and final states, including those that are absurd by classical standards. In calculating the amplitude for a single particle to go from one place to another in a given time, it is correct to include paths in which the particle describes elaborate curlicues, curves in which the particle shoots off into outer space and flies back again, and so forth. The path integral assigns to all these amplitudes "equal weight" but varying phase, or argument of the complex number. Contributions from paths wildly different from the classical trajectory may be suppressed by interference (see below).
Feynman showed that this formulation of quantum mechanics is equivalent to the canonical approach to quantum mechanics when the Hamiltonian is at most quadratic in the momentum. An amplitude computed according to Feynman's principles will also obey the Schrödinger equation for the Hamiltonian corresponding to the given action.
The path integral formulation of quantum field theory represents the transition amplitude (corresponding to the classical correlation function) as a weighted sum of all possible histories of the system from the initial to the final state. A Feynman diagram is a graphical representation of a perturbative contribution to the transition amplitude.
Concrete formulation.
Feynman's postulates can be interpreted as follows:
Time-slicing definition.
For a particle in a smooth potential, the path integral is approximated by zig-zag paths, which in one dimension is a product of ordinary integrals. For the motion of the particle from position "xa" at time "ta" to "xb" at time "tb", the time sequence
can be divided up into "n" + 1 little segments "tj" − "tj − 1", where "j" = 1...,"n" + 1, of fixed duration
This process is called "time-slicing".
An approximation for the path integral can be computed as proportional to
where formula_15 is the Lagrangian of the 1d system with position variable "x"("t") and velocity "v" = "ẋ"("t") considered (see below), and "dxj" corresponds to the position at the "j"th time step, if the time integral is approximated by a sum of "n" terms.
In the limit "n" → ∞, this becomes a functional integral, which, apart from a nonessential factor, is directly the product of the probability amplitudes formula_16 (more precisely, since one must work with a continuous spectrum, the respective densities) to find the quantum mechanical particle at "ta" in the initial state "xa" and at "tb" in the final state "xb".
Actually formula_17 is the classical Lagrangian of the one-dimensional system considered, also
where formula_19 is the Hamiltonian,
In the Riemannian sum approximating the time integral, which are finally integrated over "x1" to "xn" with the integration measure "dx1...dxn", "x̃j" is an arbitrary value of the interval corresponding to "j", e.g. its center, ("xj + "x""j" − 1)/2.
Thus, in contrast to classical mechanics, not only does the stationary path contribute, but actually all virtual paths between the initial and the final point also contribute.
Feynman's time-sliced approximation does not, however, exist for the most important quantum-mechanical path integrals of atoms, due to the singularity of the Coulomb potential "e"2/"r" at the origin. Only after replacing the time "t" by another path-dependent pseudo-time parameter
the singularity is removed and a time-sliced approximation exists, that is exactly integrable, since it can be made harmonic by a simple coordinate transformation, as discovered in 1979 by İsmail Hakkı Duru and Hagen Kleinert. The combination of a path-dependent time transformation and a coordinate transformation is an important tool to solve many path integrals and is called generically the Duru–Kleinert transformation.
Free particle.
The path integral representation gives the quantum amplitude to go from point x to point y as an integral over all paths. For a free particle action ("m" = 1, "ħ" = 1):
the integral can be evaluated explicitly.
To do this, it is convenient to start without the factor "i" in the exponential, so that large deviations are suppressed by small numbers, not by cancelling oscillatory contributions.
Splitting the integral into time slices:
where the "Dx" is interpreted as a finite collection of integrations at each integer multiple of ε. Each factor in the product is a Gaussian as a function of "x"("t" + ε) centered at "x"("t") with variance ε. The multiple integrals are a repeated convolution of this Gaussian "G"ε with copies of itself at adjacent times.
Where the number of convolutions is "T"/ε. The result is easy to evaluate by taking the Fourier transform of both sides, so that the convolutions become multiplications.
The Fourier transform of the Gaussian "G" is another Gaussian of reciprocal variance:
and the result is:
The Fourier transform gives "K", and it is a Gaussian again with reciprocal variance:
The proportionality constant is not really determined by the time slicing approach, only the ratio of values for different endpoint choices is determined. The proportionality constant should be chosen to ensure that between each two time-slices the time-evolution is quantum-mechanically unitary, but a more illuminating way to fix the normalization is to consider the path integral as a description of a stochastic process.
The result has a probability interpretation. The sum over all paths of the exponential factor can be seen as the sum over each path of the probability of selecting that path. The probability is the product over each segment of the probability of selecting that segment, so that each segment is probabilistically independently chosen. The fact that the answer is a Gaussian spreading linearly in time is the central limit theorem, which can be interpreted as the first historical evaluation of a statistical path integral.
The probability interpretation gives a natural normalization choice. The path integral should be defined so that:
This condition normalizes the Gaussian, and produces a Kernel which obeys the diffusion equation:
For oscillatory path integrals, ones with an "i" in the numerator, the time-slicing produces convolved Gaussians, just as before. Now, however, the convolution product is marginally singular since it requires careful limits to evaluate the oscillating integrals. To make the factors well defined, the easiest way is to add a small imaginary part to the time increment formula_33. This is closely related to Wick rotation. Then the same convolution argument as before gives the propagation kernel:
Which, with the same normalization as before (not the sum-squares normalization – this function has a divergent norm), obeys a free Schrödinger equation
This means that any superposition of "K"'s will also obey the same equation, by linearity. Defining
then ψ"t" obeys the free Schrödinger equation just as "K" does:
The Schrödinger equation.
The path integral reproduces the Schrödinger equation for the initial and final state even when a potential is present. This is easiest to see by taking a path-integral over infinitesimally separated times.
Basically, if formula_39 is viewed as a functional distribution (this shouldn't be taken too literally as an interpretation of QFT, unlike its Wick rotated statistical mechanics analogue, because we have time ordering complications here!), then formula_40 are its moments and Z is its Fourier transform.
If "F" is a functional of φ, then for an operator "K", "F"["K"] is defined to be the operator which substitutes "K" for φ. For example, if
and "G" is a functional of "J", then
Then, from the properties of the functional integrals
we get the "master" Schwinger–Dyson equation:
or
If the functional measure is not translationally invariant, it might be possible to express it as the product formula_46 where "M" is a functional and formula_47 is a translationally invariant measure. This is true, for example, for nonlinear sigma models where the target space is diffeomorphic to R"n". However, if the target manifold is some topologically nontrivial space, the concept of a translation does not even make any sense.
In that case, we would have to replace the formula_48 in this equation by another functional formula_49
If we expand this equation as a Taylor series about "J" = 0, we get the entire set of Schwinger–Dyson equations.
Localization.
The path integrals are usually thought of as being the sum of all paths through an infinite space–time. However, in Local quantum field theory we would restrict everything to lie within a finite "causally complete" region, for example inside a double light-cone. This gives a more mathematically precise and physically rigorous definition of quantum field theory.
Ward–Takahashi identities.
Now how about the on shell Noether's theorem for the classical case? Does it have a quantum analog as well? Yes, but with a caveat. The functional measure would have to be invariant under the one parameter group of symmetry transformation as well.
Let's just assume for simplicity here that the symmetry in question is local (not local in the sense of a gauge symmetry, but in the sense that the transformed value of the field at any given point under an infinitesimal transformation would only depend on the field configuration over an arbitrarily small neighborhood of the point in question). Let's also assume that the action is local in the sense that it is the integral over spacetime of a Lagrangian, and that formula_50 for some function f where f only depends locally on φ (and possibly the spacetime position).
If we don't assume any special boundary conditions, this would not be a "true" symmetry in the true sense of the term in general unless f=0 or something. Here, Q is a derivation which generates the one parameter group in question. We could have antiderivations as well, such as BRST and supersymmetry.
Let's also assume formula_51 for any polynomially bounded functional F. This property is called the invariance of the measure. And this does not hold in general. See anomaly (physics) for more details.
Then,
which implies
where the integral is over the boundary. This is the quantum analog of Noether's theorem.
Now, let's assume even further that Q is a local integral
where
so that
where
(this is assuming the Lagrangian only depends on φ and its first partial derivatives! More general Lagrangians would require a modification to this definition!). Note that we're NOT insisting that q(x) is the generator of a symmetry (i.e. we are "not" insisting upon the gauge principle), but just that "Q" is. And we also assume the even stronger assumption that the functional measure is locally invariant:
Then, we would have
Alternatively,
The above two equations are the Ward–Takahashi identities.
Now for the case where f=0, we can forget about all the boundary conditions and locality assumptions. We'd simply have
Alternatively,
The need for regulators and renormalization.
Path integrals as they are defined here require the introduction of regulators. Changing the scale of the regulator leads to the renormalization group. In fact, renormalization is the major obstruction to making path integrals well-defined.
The path integral in quantum-mechanical interpretation.
In one philosophical interpretation of quantum mechanics, the "sum over histories" interpretation, the path integral is taken to be fundamental and reality is viewed as a single indistinguishable "class" of paths which all share the same events. For this interpretation, it is crucial to understand what exactly an event is. The sum over histories method gives identical results to canonical quantum mechanics, and Sinha and Sorkin claim the interpretation explains the Einstein–Podolsky–Rosen paradox without resorting to nonlocality. (Note that the Copenhagen/pragmatism interpretation claims there is no paradox—only a sloppy materialism motivated question on the part of EPR—Joseph Wienberg a lecture. On the other hand, the fact that the EPR thought experiment (and its result) does represent the results of a QM experiment says that (despite the path dependence of parallelness/anti-parallelness in curved space) all contributions of paths close to black holes cancel in the action for an EPR style experiment here on earth.)
Some advocates of interpretations of quantum mechanics emphasizing decoherence have attempted to make more rigorous the notion of extracting a classical-like "coarse-grained" history from the space of all possible histories.
Quantum gravity.
Whereas in quantum mechanics the path integral formulation is fully equivalent to other formulations, it may be that it can be extended to quantum gravity, which would make it different from the Hilbert space model. Feynman had some success in this direction and his work has been extended by Hawking and others. Approaches that use this method include causal dynamical triangulations and spinfoam models.
Quantum tunneling.
Quantum tunnelling can be modeled by using the path integral formation to determine the action of the trajectory through a potential barrier. Using the WKB approximation, the tunneling rate (formula_63) can be determined to be of the form
with the effective action formula_65 and pre-exponential factor formula_66. This form is specifically useful in a dissipative system, in which the systems and surroundings must be modeled together. Using the Langevin equation to model Brownian motion, the path integral formation can be used to determine an effective action and pre-exponential model to see the effect of dissipation on tunnelling. From this model, tunneling rates of macroscopic systems (at finite temperatures) can be predicted.

</doc>
<doc id="24868" url="https://en.wikipedia.org/wiki?curid=24868" title="Pauli matrices">
Pauli matrices

In mathematical physics and mathematics, the Pauli matrices are a set of three complex matrices which are Hermitian and unitary. Usually indicated by the Greek letter sigma (), they are occasionally denoted by tau () when used in connection with isospin symmetries. They are
These matrices are named after the physicist Wolfgang Pauli. In quantum mechanics, they occur in the Pauli equation which takes into account the interaction of the spin of a particle with an external electromagnetic field.
Each Pauli matrix is Hermitian, and together with the identity matrix (sometimes considered as the zeroth Pauli matrix ), the Pauli matrices (multiplied by "real" coefficients) span the full vector space of Hermitian matrices.
In the language of quantum mechanics, Hermitian matrices are observables, so the Pauli matrices span the space of observables of the -dimensional complex Hilbert space. In the context of Pauli's work, is the observable corresponding to spin along the th coordinate axis in three-dimensional Euclidean space .
The Pauli matrices (after multiplication by to make them anti-Hermitian), also generate transformations in the sense of Lie algebras: the matrices form a basis for , which exponentiates to the special unitary group SU(2). The algebra generated by the three matrices is isomorphic to the Clifford algebra of , called the algebra of physical space.
Algebraic properties.
All three of the Pauli matrices can be compacted into a single expression:
where is the imaginary unit, and is the Kronecker delta, which equals +1 if and 0 otherwise. This expression is useful for "selecting" any one of the matrices numerically by substituting values of , in turn useful when any of the matrices (but no particular one) is to be used in algebraic manipulations.
The matrices are involutory:
where is the identity matrix.
From above we can deduce that the eigenvalues of each are .
Eigenvectors and eigenvalues.
Each of the (Hermitian) Pauli matrices has two eigenvalues, and . The corresponding normalized eigenvectors are:
Pauli vector.
The Pauli vector is defined by
and provides a mapping mechanism from a vector basis to a Pauli matrix basis as follows,
using the summation convention. Further,
and also (see completeness, below) 
Commutation relations.
The Pauli matrices obey the following commutation relations:
and anticommutation relations:
where is the Levi-Civita symbol, Einstein summation notation is used, is the Kronecker delta, and is the identity matrix.
For example,
Relation to dot and cross product.
Pauli vectors elegantly map these commutation and anticommutation relations to corresponding vector products. Adding the commutator to the anticommutator gives
so that, cancelling the factors of 2,
Contracting each side of the equation with components of two -vectors and (which commute with the Pauli matrices, i.e., for each matrix and vector component (and likewise with ), and relabeling indices , to prevent notational conflicts, yields
Finally, translating the index notation for the dot product and cross product results in 
Exponential of a Pauli vector.
For 
one has, for even powers,
which can be shown first for the case using the anticommutation relations.
Thus, for odd powers,
Matrix exponentiating, and using the Taylor series for sine and cosine,
and, in the last line, the first sum is the cosine, while the second sum is the sine; so, finally,
which is analogous to Euler's formula. Note 
while the determinant of the exponential itself is just , which makes it the generic group element of SU(2).
A more abstract version of formula for a general matrix can be found in the article on matrix exponentials. A general version of for an analytic (at "a" and −"a") function is provided by application of Sylvester's formula,
The group composition law of.
A straightforward application of formula provides a parameterization of the composition law of the group . One may directly solve for in 
which specifies the generic group multiplication, where, manifestly, 
the spherical law of cosines. Given , then, 
Consequently, the composite rotation parameters in this group element (a closed form of the respective BCH expansion in this case) simply amount to 
The fact that any complex Hermitian matrices can be expressed in terms of the identity matrix and the Pauli matrices also leads to the Bloch sphere representation of mixed states' density matrix, ( positive semidefinite matrices with trace ). This can be seen by simply first writing an arbitrary Hermitian matrix as a real linear combination of as above, and then imposing the positive-semidefinite and trace conditions.
Completeness relation.
An alternative notation that is commonly used for the Pauli matrices is to write the vector index in the superscript, and the matrix indices as subscripts, so that the element in row and column of the -th Pauli matrix is .
In this notation, the completeness relation for the Pauli matrices can be written
Proof
The fact that the Pauli matrices, along with the identity matrix "I", form an orthogonal basis for the complex Hilbert space of all 2 × 2 matrices means that we can express any matrix "M" as
where "c" is a complex number, and "a" is a 3-component complex vector. It is straightforward to show, using the properties listed above, that
where "tr" denotes the trace, and hence that formula_28 and formula_29.
This therefore gives
which can be rewritten in terms of matrix indices as
where summation is implied over the repeated indices "γ" and "δ". Since this is true for any choice of the matrix "M", the completeness relation follows as stated above.
As noted above, it is common to denote the 2 × 2 unit matrix by "σ"0, so "σ"0"αβ" = "δ""αβ". The completeness relation can therefore alternatively be expressed as
Relation with the permutation operator.
Let be the transposition (also known as a permutation) between two spins and living in the tensor product space ,
This operator can also be written more explicitly as Dirac's spin exchange operator,
Its eigenvalues are therefore 1 or −1. It may thus be utilized as an interaction term in a Hamiltonian, splitting the energy eigenvalues of its symmetric versus antisymmetric eigenstates.
SU(2).
The group SU(2) is the Lie group of unitary 2×2 matrices with unit determinant; its Lie algebra is the set of all 2×2 anti-Hermitian matrices with trace 0. Direct calculation, as above, shows that the Lie algebra formula_35 is the 3-dimensional real algebra spanned by the set }. In compact notation,
As a result, each can be seen as an infinitesimal generator of SU(2). The elements of SU(2) are exponentials of linear combinations of these three generators, and multiply as indicated above in discussing the Pauli vector. Although this suffices to generate SU(2), it is not a proper representation of, as the Pauli eigenvalues are scaled unconventionally. The conventional normalization is  , so that
As SU(2) is a compact group, its Cartan decomposition is trivial.
SO(3).
The Lie algebra is isomorphic to the Lie algebra , which corresponds to the Lie group SO(3), the group of rotations in three-dimensional space. In other words, one can say that the are a realization (and, in fact, the lowest-dimensional realization) of "infinitesimal" rotations in three-dimensional space. However, even though and are isomorphic as Lie algebras, and are not isomorphic as Lie groups. is actually a double cover of , meaning that there is a two-to-one group homomorphism from to , see relationship between SO(3) and SU(2).
Quaternions.
The real linear span of is isomorphic to the real algebra of quaternions . The isomorphism from to this set is given by the following map (notice the reversed signs for the Pauli matrices):
Alternatively, the isomorphism can be achieved by a map using the Pauli matrices in reversed order,
As the quaternions of unit norm is group-isomorphic to , this gives yet another way of describing via the Pauli matrices. The two-to-one homomorphism from to can also be explicitly given in terms of the Pauli matrices in this formulation.
Quaternions form a division algebra—every non-zero element has an inverse—whereas Pauli matrices do not. For a quaternionic version of the algebra generated by Pauli matrices see biquaternions, which is a venerable algebra of eight real dimensions.
Physics.
Quantum mechanics.
In quantum mechanics, each Pauli matrix is related to an angular momentum operator that corresponds to an observable describing the spin of a spin ½ particle, in each of the three spatial directions. As an immediate consequence of the Cartan decomposition mentioned above, are the generators of a projective representation (spin representation) of the rotation group SO(3) acting on non-relativistic particles with spin ½. The states of the particles are represented as two-component spinors. In the same way, the Pauli matrices are related to the isospin operator
An interesting property of spin ½ particles is that they must be rotated by an angle of 4 in order to return to their original configuration. This is due to the two-to-one correspondence between SU(2) and SO(3) mentioned above, and the fact that, although one visualizes spin up/down as the north/south pole on the 2-sphere 2, they are actually represented by orthogonal vectors in the two dimensional complex Hilbert space.
For a spin ½ particle, the spin operator is given by , the fundamental representation of "SU(2)". By taking Kronecker products of this representation with itself repeatedly, one may construct all higher irreducible representations. That is, the resulting spin operators for higher spin systems in three spatial dimensions, for arbitrarily large "j", can be calculated using this spin operator and ladder operators. They can be found in Rotation group SO(3)#A note on representations. The analog formula to the above generalization of Euler's formula for Pauli matrices, the group element in terms of spin matrices, is tractable, but less simple.
Also useful in the quantum mechanics of multiparticle systems, the general Pauli group is defined to consist of all -fold tensor products of Pauli matrices.

</doc>
<doc id="76848" url="https://en.wikipedia.org/wiki?curid=76848" title="Periodic function">
Periodic function

In mathematics, a periodic function is a function that repeats its values in regular intervals or periods. The most important examples are the trigonometric functions, which repeat over intervals of 2"π" radians. Periodic functions are used throughout science to describe oscillations, waves, and other phenomena that exhibit periodicity. Any function which is not periodic is called aperiodic.
Definition.
A function "f" is said to be periodic with period "P" ("P" being a nonzero constant) if we have
for all values of "x" in the domain. If there exists a least positive
constant "P" with this property, it is called the fundamental period (also primitive period, basic period, or prime period.) A function with period "P" will repeat on intervals of length "P", and these intervals
are referred to as periods.
Geometrically, a periodic function can be defined as a function whose graph exhibits translational symmetry. Specifically, a function "f" is periodic with period "P" if the graph of "f" is invariant under translation in the "x"-direction by a distance of "P". This definition of periodic can be extended to other geometric shapes and patterns, such as periodic tessellations of the plane.
A function that is not periodic is called aperiodic.
Examples.
For example, the sine function is periodic with period 2"π", since
for all values of "x". This function repeats on intervals of length 2"π" (see the graph to the right).
Everyday examples are seen when the variable is "time"; for instance the hands of a clock or the phases of the moon show periodic behaviour. Periodic motion is motion in which the position(s) of the system are expressible as periodic functions, all with the "same" period.
For a function on the real numbers or on the integers, that means that the entire graph can be formed from copies of one particular portion, repeated at regular intervals.
A simple example of a periodic function is the function "f" that gives the "fractional part" of its argument. Its period is 1. In particular,
The graph of the function "f" is the sawtooth wave.
The trigonometric functions sine and cosine are common periodic functions, with period 2π (see the figure on the right). The subject of Fourier series investigates the idea that an 'arbitrary' periodic function is a sum of trigonometric functions with matching periods.
According to the definition above, some exotic functions, for example the Dirichlet function, are also periodic; in the case of Dirichlet function, any nonzero rational number is a period.
Properties.
If a function "f" is periodic with period "P", then for all "x" in the domain of "f" and all integers "n",
If "f"("x") is a function with period "P", then "f"("ax+b"), where "a" is a positive constant, is periodic with period "P/|a|". For example, "f"("x")=sin"x" has period 2π, therefore sin(5"x") will have period 2π/5.
Double-periodic functions.
A function whose domain is the complex numbers can have two incommensurate periods without being constant. The elliptic functions are such functions.
Complex example.
Using complex variables we have the common period function:
As you can see, since the cosine and sine functions are periodic, and the complex exponential above is made up of cosine/sine waves, then the above (actually Euler's formula) has the following property. If "L" is the period of the function then:
Generalizations.
Antiperiodic functions.
One common generalization of periodic functions is that of antiperiodic functions. This is a function "f" such that "f"("x" + "P") = −"f"("x") for all "x". (Thus, a "P"-antiperiodic function is a 2"P"-periodic function.) For example, the sine or cosine function is π-antiperiodic and 2π-periodic
Bloch-periodic functions.
A further generalization appears in the context of Bloch waves and Floquet theory, which govern the solution of various periodic differential equations. In this context, the solution (in one dimension) is typically a function of the form:
where "k" is a real or complex number (the "Bloch wavevector" or "Floquet exponent"). Functions of this form are sometimes called Bloch-periodic in this context. A periodic function is the special case "k" = 0, and an antiperiodic function is the special case "k" = π/"P".
Quotient spaces as domain.
In signal processing you encounter the problem, that Fourier series represent periodic functions
and that Fourier series satisfy convolution theorems
(i.e. convolution of Fourier series corresponds to multiplication of represented periodic function and vice versa),
but periodic functions cannot be convolved with the usual definition,
since the involved integrals diverge.
A possible way out is to define a periodic function on a bounded but periodic domain.
To this end you can use the notion of a quotient space:
That is, each element in formula_7 is an equivalence class
of real numbers that share the same fractional part.
Thus a function like formula_8
is a representation of a 1-periodic function.

</doc>
<doc id="23053" url="https://en.wikipedia.org/wiki?curid=23053" title="Periodic table">
Periodic table

The periodic table is a tabular arrangement of the chemical elements, ordered by their atomic number (number of protons in the nucleus), electron configurations, and recurring chemical properties. The table also shows four rectangular blocks: s-, p- d- and f-block. In general, within one row (period) the elements are metals on the lefthand side, and non-metals on the righthand side.
The rows of the table are called periods; the columns are called groups. Six groups (columns) have names as well as numbers: for example, group 17 elements are the halogens; and group 18, the noble gases. The periodic table can be used to derive relationships between the properties of the elements, and predict the properties of new elements yet to be discovered or synthesized. The periodic table provides a useful framework for analyzing chemical behavior, and is widely used in chemistry and other sciences.
Although precursors exist, Dmitri Mendeleev is generally credited with the publication, in 1869, of the first widely recognized periodic table. He developed his table to illustrate periodic trends in the properties of the then-known elements. Mendeleev also predicted some properties of then-unknown elements that would be expected to fill gaps in this table. Most of his predictions were proved correct when the elements in question were subsequently discovered. Mendeleev's periodic table has since been expanded and refined with the discovery or synthesis of further new elements and the development of new theoretical models to explain chemical behavior.
All elements from atomic numbers 1 (hydrogen) to 118 (ununoctium) have been discovered or reportedly synthesized, with elements 113, 115, 117, and 118 having yet to be confirmed. The first 94 elements exist naturally, although some are found only in trace amounts and were synthesized in laboratories before being found in nature. Elements with atomic numbers from 95 to 118 have only been synthesized in laboratories. It has been shown that elements 95 to 100 once occurred in nature but currently do not. Synthesis of elements having higher atomic numbers is being pursued. Numerous synthetic radionuclides of naturally occurring elements have also been produced in laboratories.
Overview.
Some presentations include an element zero (i.e. a substance composed purely of neutrons), although this is uncommon. See, for example. Philip Stewart's Chemical Galaxy. Each chemical element has a unique atomic number representing the number of protons in its nucleus. Most elements have differing numbers of neutrons among different atoms, with these variants being referred to as isotopes. For example, carbon has three naturally occurring isotopes: all of its atoms have six protons and most have six neutrons as well, but about one per cent have seven neutrons, and a very small fraction have eight neutrons. Isotopes are never separated in the periodic table; they are always grouped together under a single element. Elements with no stable isotopes have the atomic masses of their most stable isotopes, where such masses are shown, listed in parentheses.
In the standard periodic table, the elements are listed in order of increasing atomic number (the number of protons in the nucleus of an atom). A new row ("period") is started when a new electron shell has its first electron. Columns ("groups") are determined by the electron configuration of the atom; elements with the same number of electrons in a particular subshell fall into the same columns (e.g. oxygen and selenium are in the same column because they both have four electrons in the outermost p-subshell). Elements with similar chemical properties generally fall into the same group in the periodic table, although in the f-block, and to some respect in the d-block, the elements in the same period tend to have similar properties, as well. Thus, it is relatively easy to predict the chemical properties of an element if one knows the properties of the elements around it.
As of 2014, the periodic table has 114 confirmed elements, comprising elements 1 (hydrogen) to 112 (copernicium), 114 (flerovium) and 116 (livermorium). Elements 113, 115, 117 and 118 have reportedly been synthesised in laboratories, but none of these claims have been officially confirmed by the International Union of Pure and Applied Chemistry (IUPAC), nor are they named. As such these elements are currently identified by their atomic number (e.g., "element 113"), or by their provisional systematic name ("ununtrium", symbol "Uut").
A total of 94 elements occur naturally; the remaining 20 elements, from americium to copernicium, and flerovium and livermorium, occur only when synthesised in laboratories. Of the 94 elements that occur naturally, 84 are primordial. The other 10 naturally occurring elements occur only in decay chains of primordial elements. No element heavier than einsteinium (element 99) has ever been observed in macroscopic quantities in its pure form, nor has astatine (element 85); francium (element 87) has been only photographed in the form of light emitted from microscopic quantities (300,000 atoms).
Layout variants.
In the most common graphic presentation of the periodic table, the main table has 18 columns and the lanthanides and the actinides are shown as two additional rows below the main body of the table, with two placeholders shown in the main table, between barium and hafnium, and radium and rutherfordium, respectively. These placeholders can be asterisk-like markers, or a contracted range description of elements ("57–71"). This convention is entirely a matter of formatting practicality. The same table structure can be shown in a 32-column format, with the lanthanides and actinides in the main table's row 6 and 7.
However, based on the chemical and physical properties of elements, many alternative table "structures" have been constructed.
Grouping methods.
Groups.
A "group" or "family" is a vertical column in the periodic table. Groups usually have more significant periodic trends than periods and blocks, explained below. Modern quantum mechanical theories of atomic structure explain group trends by proposing that elements within the same group generally have the same electron configurations in their valence shell. Consequently, elements in the same group tend to have a shared chemistry and exhibit a clear trend in properties with increasing atomic number. However, in some parts of the periodic table, such as the d-block and the f-block, horizontal similarities can be as important as, or more pronounced than, vertical similarities.
Under an international naming convention, the groups are numbered numerically from 1 to 18 from the leftmost column (the alkali metals) to the rightmost column (the noble gases). Previously, they were known by roman numerals. In America, the roman numerals were followed by either an "A" if the group was in the s- or p-block, or a "B" if the group was in the d-block. The roman numerals used correspond to the last digit of today's naming convention (e.g. the group 4 elements were group IVB, and the group 14 elements was group IVA). In Europe, the lettering was similar, except that "A" was used if the group was before group 10, and "B" was used for groups including and after group 10. In addition, groups 8, 9 and 10 used to be treated as one triple-sized group, known collectively in both notations as group VIII. In 1988, the new IUPAC naming system was put into use, and the old group names were deprecated.
Some of these groups have been given trivial (unsystematic) names, as seen in the table below, although some are rarely used. Groups 3–10 have no trivial names and are referred to simply by their group numbers or by the name of the first member of their group (such as 'the scandium group' for Group 3), since they display fewer similarities and/or vertical trends.
Elements in the same group tend to show patterns in atomic radius, ionization energy, and electronegativity. From top to bottom in a group, the atomic radii of the elements increase. Since there are more filled energy levels, valence electrons are found farther from the nucleus. From the top, each successive element has a lower ionization energy because it is easier to remove an electron since the atoms are less tightly bound. Similarly, a group has a top to bottom decrease in electronegativity due to an increasing distance between valence electrons and the nucleus. There are exceptions to these trends, however, an example of which occurs in group 11 where electronegativity increases farther down the group.
Periods.
A "period" is a horizontal row in the periodic table. Although groups generally have more significant periodic trends, there are regions where horizontal trends are more significant than vertical group trends, such as the f-block, where the lanthanides and actinides form two substantial horizontal series of elements.
Elements in the same period show trends in atomic radius, ionization energy, electron affinity, and electronegativity. Moving left to right across a period, atomic radius usually decreases. This occurs because each successive element has an added proton and electron, which causes the electron to be drawn closer to the nucleus. This decrease in atomic radius also causes the ionization energy to increase when moving from left to right across a period. The more tightly bound an element is, the more energy is required to remove an electron. Electronegativity increases in the same manner as ionization energy because of the pull exerted on the electrons by the nucleus. Electron affinity also shows a slight trend across a period. Metals (left side of a period) generally have a lower electron affinity than nonmetals (right side of a period), with the exception of the noble gases.
Blocks.
Specific regions of the periodic table can be referred to as "blocks" in recognition of the sequence in which the electron shells of the elements are filled. Each block is named according to the subshell in which the "last" electron notionally resides. The s-block comprises the first two groups (alkali metals and alkaline earth metals) as well as hydrogen and helium. The p-block comprises the last six groups, which are groups 13 to 18 in IUPAC (3A to 8A in American) and contains, among other elements, all of the metalloids. The d-block comprises groups 3 to 12 (or 3B to 2B in American group numbering) and contains all of the transition metals. The f-block, often offset below the rest of the periodic table, has no group numbers and comprises lanthanides and actinides.
Metals, metalloids and nonmetals.
According to their shared physical and chemical properties, the elements can be classified into the major categories of metals, metalloids and nonmetals. Metals are generally shiny, highly conducting solids that form alloys with one another and salt-like ionic compounds with nonmetals (other than the noble gases). The majority of nonmetals are coloured or colourless insulating gases; nonmetals that form compounds with other nonmetals feature covalent bonding. In between metals and nonmetals are metalloids, which have intermediate or mixed properties.
Metal and nonmetals can be further classified into subcategories that show a gradation from metallic to non-metallic properties, when going left to right in the rows. The metals are subdivided into the highly reactive alkali metals, through the less reactive alkaline earth metals, lanthanides and actinides, via the archetypal transition metals, and ending in the physically and chemically weak post-transition metals. The nonmetals are simply subdivided into the polyatomic nonmetals, which, being nearest to the metalloids, show some incipient metallic character; the diatomic nonmetals, which are essentially nonmetallic; and the monatomic noble gases, which are nonmetallic and almost completely inert. Specialized groupings such as the refractory metals and the noble metals, which are subsets (in this example) of the transition metals, are also known and occasionally denoted.
Placing the elements into categories and subcategories based on shared properties is imperfect. There is a spectrum of properties within each category and it is not hard to find overlaps at the boundaries, as is the case with most classification schemes. Beryllium, for example, is classified as an alkaline earth metal although its amphoteric chemistry and tendency to mostly form covalent compounds are both attributes of a chemically weak or post transition metal. Radon is classified as a nonmetal and a noble gas yet has some cationic chemistry that is more characteristic of a metal. Other classification schemes are possible such as the division of the elements into mineralogical occurrence categories, or crystalline structures. Categorising the elements in this fashion dates back to at least 1869 when Hinrichs wrote that simple boundary lines could be drawn on the periodic table to show elements having like properties, such as the metals and the nonmetals, or the gaseous elements.
Periodic trends.
Electron configuration.
The electron configuration or organisation of electrons orbiting neutral atoms shows a recurring pattern or periodicity. The electrons occupy a series of electron shells (numbered shell 1, shell 2, and so on). Each shell consists of one or more subshells (named s, p, d, f and g). As atomic number increases, electrons progressively fill these shells and subshells more or less according to the Madelung rule or energy ordering rule, as shown in the diagram. The electron configuration for neon, for example, is 1s2 2s2 2p6. With an atomic number of ten, neon has two electrons in the first shell, and eight electrons in the second shell—two in the s subshell and six in the p subshell. In periodic table terms, the first time an electron occupies a new shell corresponds to the start of each new period, these positions being occupied by hydrogen and the alkali metals.
Since the properties of an element are mostly determined by its electron configuration, the properties of the elements likewise show recurring patterns or periodic behaviour, some examples of which are shown in the diagrams below for atomic radii, ionization energy and electron affinity. It is this periodicity of properties, manifestations of which were noticed well before the underlying theory was developed, that led to the establishment of the periodic law (the properties of the elements recur at varying intervals) and the formulation of the first periodic tables.
Atomic radii.
Atomic radii vary in a predictable and explainable manner across the periodic table. For instance, the radii generally decrease along each period of the table, from the alkali metals to the noble gases; and increase down each group. The radius increases sharply between the noble gas at the end of each period and the alkali metal at the beginning of the next period. These trends of the atomic radii (and of various other chemical and physical properties of the elements) can be explained by the electron shell theory of the atom; they provided important evidence for the development and confirmation of quantum theory.
The electrons in the 4f-subshell, which is progressively filled from cerium (element 58) to ytterbium (element 70), are not particularly effective at shielding the increasing nuclear charge from the sub-shells further out. The elements immediately following the lanthanides have atomic radii that are smaller than would be expected and that are almost identical to the atomic radii of the elements immediately above them. Hence hafnium has virtually the same atomic radius (and chemistry) as zirconium, and tantalum has an atomic radius similar to niobium, and so forth. This is known as the lanthanide contraction. The effect of the lanthanide contraction is noticeable up to platinum (element 78), after which it is masked by a relativistic effect known as the inert pair effect. The d-block contraction, which is a similar effect between the d-block and p-block, is less pronounced than the lanthanide contraction but arises from a similar cause.
Ionization energy.
The first ionization energy is the energy it takes to remove one electron from an atom, the second ionization energy is the energy it takes to remove a second electron from the atom, and so on. For a given atom, successive ionization energies increase with the degree of ionization. For magnesium as an example, the first ionization energy is 738 kJ/mol and the second is 1450 kJ/mol. Electrons in the closer orbitals experience greater forces of electrostatic attraction; thus, their removal requires increasingly more energy. Ionization energy becomes greater up and to the right of the periodic table.
Large jumps in the successive molar ionization energies occur when removing an electron from a noble gas (complete electron shell) configuration. For magnesium again, the first two molar ionization energies of magnesium given above correspond to removing the two 3s electrons, and the third ionization energy is a much larger 7730 kJ/mol, for the removal of a 2p electron from the very stable neon-like configuration of Mg2+. Similar jumps occur in the ionization energies of other third-row atoms.
Electronegativity.
Electronegativity is the tendency of an atom to attract electrons. An atom's electronegativity is affected by both its atomic number and the distance between the valence electrons and the nucleus. The higher its electronegativity, the more an element attracts electrons. It was first proposed by Linus Pauling in 1932. In general, electronegativity increases on passing from left to right along a period, and decreases on descending a group. Hence, fluorine is the most electronegative of the elements, while caesium is the least, at least of those elements for which substantial data is available.
There are some exceptions to this general rule. Gallium and germanium have higher electronegativities than aluminium and silicon respectively because of the d-block contraction. Elements of the fourth period immediately after the first row of the transition metals have unusually small atomic radii because the 3d-electrons are not effective at shielding the increased nuclear charge, and smaller atomic size correlates with higher electronegativity. The anomalously high electronegativity of lead, particularly when compared to thallium and bismuth, appears to be an artifact of data selection (and data availability)—methods of calculation other than the Pauling method show the normal periodic trends for these elements.
Electron affinity.
The electron affinity of an atom is the amount of energy released when an electron is added to a neutral atom to form a negative ion. Although electron affinity varies greatly, some patterns emerge. Generally, nonmetals have more positive electron affinity values than metals. Chlorine most strongly attracts an extra electron. The electron affinities of the noble gases have not been measured conclusively, so they may or may not have slightly negative values.
Electron affinity generally increases across a period. This is caused by the filling of the valence shell of the atom; a group 17 atom releases more energy than a group 1 atom on gaining an electron because it obtains a filled valence shell and is therefore more stable.
A trend of decreasing electron affinity going down groups would be expected. The additional electron will be entering an orbital farther away from the nucleus. As such this electron would be less attracted to the nucleus and would release less energy when added. However, in going down a group, around one-third of elements are anomalous, with heavier elements having higher electron affinities than their next lighter congenors. Largely, this is due to the poor shielding by d and f electrons. A uniform decrease in electron affinity only applies to group 1 atoms.
Metallic character.
The lower the values of ionization energy, electronegativity and electron affinity, the more metallic character the element has. Conversely, nonmetallic character increases with higher values of these properties. Given the periodic trends of these three properties, metallic character tends to decrease going across a period (or row) and, with some irregularities (mostly) due to poor screening of the nucleus by d and f electrons, and relativistic effects, tends to increase going down a group (or column or family). Thus, the most metallic elements (such as caesium and francium) are found at the bottom left of traditional periodic tables and the most nonmetallic elements (oxygen, fluorine, chlorine) at the top right. The combination of horizontal and vertical trends in metallic character explains the stair-shaped dividing line between metals and nonmetals found on some periodic tables, and the practice of sometimes categorizing several elements adjacent to that line, or elements adjacent to those elements, as metalloids.
History.
First systemization attempts.
In 1789, Antoine Lavoisier published a list of 33 chemical elements, grouping them into gases, metals, nonmetals, and earths. Chemists spent the following century searching for a more precise classification scheme. In 1829, Johann Wolfgang Döbereiner observed that many of the elements could be grouped into triads based on their chemical properties. Lithium, sodium, and potassium, for example, were grouped together in a triad as soft, reactive metals. Döbereiner also observed that, when arranged by atomic weight, the second member of each triad was roughly the average of the first and the third; this became known as the Law of Triads. German chemist Leopold Gmelin worked with this system, and by 1843 he had identified ten triads, three groups of four, and one group of five. Jean-Baptiste Dumas published work in 1857 describing relationships between various groups of metals. Although various chemists were able to identify relationships between small groups of elements, they had yet to build one scheme that encompassed them all.
In 1858, German chemist August Kekulé observed that carbon often has four other atoms bonded to it. Methane, for example, has one carbon atom and four hydrogen atoms. This concept eventually became known as valency; different elements bond with different numbers of atoms.
In 1862, Alexandre-Emile Béguyer de Chancourtois, a French geologist, published an early form of periodic table, which he called the telluric helix or screw. He was the first person to notice the periodicity of the elements. With the elements arranged in a spiral on a cylinder by order of increasing atomic weight, de Chancourtois showed that elements with similar properties seemed to occur at regular intervals. His chart included some ions and compounds in addition to elements. His paper also used geological rather than chemical terms and did not include a diagram; as a result, it received little attention until the work of Dmitri Mendeleev.
In 1864, Julius Lothar Meyer, a German chemist, published a table with 44 elements arranged by valency. The table showed that elements with similar properties often shared the same valency. Concurrently, William Odling (an English chemist) published an arrangement of 57 elements, ordered on the basis of their atomic weights. With some irregularities and gaps, he noticed what appeared to be a periodicity of atomic weights among the elements and that this accorded with 'their usually received groupings.' Odling alluded to the idea of a periodic law but did not pursue it. He subsequently proposed (in 1870) a valence-based classification of the elements.
English chemist John Newlands produced a series of papers from 1863 to 1866 noting that when the elements were listed in order of increasing atomic weight, similar physical and chemical properties recurred at intervals of eight; he likened such periodicity to the octaves of music. This so termed Law of Octaves, however, was ridiculed by Newlands' contemporaries, and the Chemical Society refused to publish his work. Newlands was nonetheless able to draft a table of the elements and used it to predict the existence of missing elements, such as germanium. The Chemical Society only acknowledged the significance of his discoveries five years after they credited Mendeleev.
In 1867, Gustavus Hinrichs, a Danish born academic chemist based in America, published a spiral periodic system based on atomic spectra and weights, and chemical similarities. His work was regarded as idiosyncratic, ostentatious and labyrinthine and this may have militated against its recognition and acceptance.
Mendeleev's table.
Russian chemistry professor Dmitri Mendeleev and German chemist Julius Lothar Meyer independently published their periodic tables in 1869 and 1870, respectively. Mendeleev's table was his first published version; that of Meyer was an expanded version of his (Meyer's) table of 1864. They both constructed their tables by listing the elements in rows or columns in order of atomic weight and starting a new row or column when the characteristics of the elements began to repeat.
The recognition and acceptance afforded to Mendeleev's table came from two decisions he made. The first was to leave gaps in the table when it seemed that the corresponding element had not yet been discovered. Mendeleev was not the first chemist to do so, but he was the first to be recognized as using the trends in his periodic table to predict the properties of those missing elements, such as gallium and germanium. The second decision was to occasionally ignore the order suggested by the atomic weights and switch adjacent elements, such as tellurium and iodine, to better classify them into chemical families. Later in 1913, Henry Moseley determined experimental values of the nuclear charge or atomic number of each element, and showed that Mendeleev's ordering actually corresponds to the order of increasing atomic number.
The significance of atomic numbers to the organization of the periodic table was not appreciated until the existence and properties of protons and neutrons became understood. Mendeleev's periodic tables used atomic weight instead of atomic number to organize the elements, information determinable to fair precision in his time. Atomic weight worked well enough in most cases to (as noted) give a presentation that was able to predict the properties of missing elements more accurately than any other method then known. Substitution of atomic numbers, once understood, gave a definitive, integer-based sequence for the elements, and Moseley predicted that the only missing elements (in 1913) between aluminum (Z=13) and gold (Z=79) (in 1913) were Z = 43, 61, 72 and 75, which were all later discovered. The sequence of atomic numbers is still used today even as new synthetic elements are being produced and studied.
Second version and further development.
In 1871, Mendeleev published his periodic table in a new form, with groups of similar elements arranged in columns rather than in rows, and those columns numbered I to VIII corresponding with the element's oxidation state. He also gave detailed predictions for the properties of elements he had earlier noted were missing, but should exist. These gaps were subsequently filled as chemists discovered additional naturally occurring elements. It is often stated that the last naturally occurring element to be discovered was francium (referred to by Mendeleev as "eka-caesium") in 1939. However, plutonium, produced synthetically in 1940, was identified in trace quantities as a naturally occurring primordial element in 1971.
The popular periodic table layout, also known as the common or standard form (as shown at various other points in this article), is attributable to Horace Groves Deming. In 1923, Deming, an American chemist, published short (Mendeleev style) and medium (18-column) form periodic tables. Merck and Company prepared a handout form of Deming's 18-column medium table, in 1928, which was widely circulated in American schools. By the 1930s Deming's table was appearing in handbooks and encyclopaedias of chemistry. It was also distributed for many years by the Sargent-Welch Scientific Company.
With the development of modern quantum mechanical theories of electron configurations within atoms, it became apparent that each period (row) in the table corresponded to the filling of a quantum shell of electrons. Larger atoms have more electron sub-shells, so later tables have required progressively longer periods.
In 1945, Glenn Seaborg, an American scientist, made the suggestion that the actinide elements, like the lanthanides, were filling an f sub-level. Before this time the actinides were thought to be forming a fourth d-block row. Seaborg's colleagues advised him not to publish such a radical suggestion as it would most likely ruin his career. As Seaborg considered he did not then have a career to bring into disrepute, he published anyway. Seaborg's suggestion was found to be correct and he subsequently went on to win the 1951 Nobel Prize in chemistry for his work in synthesizing actinide elements.
Although minute quantities of some transuranic elements occur naturally, they were all first discovered in laboratories. Their production has expanded the periodic table significantly, the first of these being neptunium, synthesized in 1939. Because many of the transuranic elements are highly unstable and decay quickly, they are challenging to detect and characterize when produced. There have been controversies concerning the acceptance of competing discovery claims for some elements, requiring independent review to determine which party has priority, and hence naming rights. The most recently accepted and named elements are flerovium (element 114) and livermorium (element 116), both named on 31 May 2012. In 2010, a joint Russia–US collaboration at Dubna, Moscow Oblast, Russia, claimed to have synthesized six atoms of ununseptium (element 117), making it the most recently claimed discovery.
Alternative structures.
There are many periodic tables with structures other than that of the standard form. Within 100 years of the appearance of Mendeleev's table in 1869 it has been estimated that around 700 different periodic table versions were published. As well as numerous rectangular variations, other periodic table formats have included, for example, circular, cubic, cylindrical, edificial (building-like), helical, lemniscate, octagonal prismatic, pyramidal, separated, spherical, spiral, and triangular forms. Such alternatives are often developed to highlight or emphasize chemical or physical properties of the elements that are not as apparent in traditional periodic tables.
A popular alternative structure is that of Theodor Benfey (1960). The elements are arranged in a continuous spiral, with hydrogen at the center and the transition metals, lanthanides, and actinides occupying peninsulas.
Most periodic tables are two-dimensional; however, three-dimensional tables are known to as far back as at least 1862 (pre-dating Mendeleev's two-dimensional table of 1869). More recent examples include Courtines' Periodic Classification (1925), Wringley's Lamina System (1949),
Giguère's Periodic helix (1965) and Dufour's Periodic Tree (1996). Going one better, Stowe's Physicist's Periodic Table (1989) has been described as being four-dimensional (having three spatial dimensions and one colour dimension).
The various forms of periodic tables can be thought of as lying on a chemistry–physics continuum. Towards the chemistry end of the continuum can be found, as an example, Rayner-Canham's 'unruly' Inorganic Chemist's Periodic Table (2002), which emphasizes trends and patterns, and unusual chemical relationships and properties. Near the physics end of the continuum is Janet's Left-Step Periodic Table (1928). This has a structure that shows a closer connection to the order of electron-shell filling and, by association, quantum mechanics. Somewhere in the middle of the continuum is the ubiquitous common or standard form of periodic table. This is regarded as better expressing empirical trends in physical state, electrical and thermal conductivity, and oxidation numbers, and other properties easily inferred from traditional techniques of the chemical laboratory.
Open questions and controversies.
Elements with unknown chemical properties.
Although all elements up to ununoctium have been discovered, of the elements above hassium (element 108), only copernicium (element 112) and flerovium (element 114) have known chemical properties. The other elements may behave differently from what would be predicted by extrapolation, due to relativistic effects; for example, flerovium has been predicted to possibly exhibit some noble-gas-like properties, even though it is currently placed in the carbon group. More recent experiments have suggested, however, that flerovium behaves chemically like lead, as expected from its periodic table position.
Further periodic table extensions.
It is unclear whether new elements will continue the pattern of the current periodic table as period 8, or require further adaptations or adjustments. Seaborg expected the eighth period to follow the previously established pattern exactly, so that it would include a two-element s-block for elements 119 and 120, a new g-block for the next 18 elements, and 30 additional elements continuing the current f-, d-, and p-blocks. More recently, physicists such as Pekka Pyykkö have theorized that these additional elements do not follow the Madelung rule, which predicts how electron shells are filled and thus affects the appearance of the present periodic table.
Element with the highest possible atomic number.
The number of possible elements is not known. A very early suggestion made by Elliot Adams in 1911, and based on the arrangement of elements in each horizontal periodic table row, was that elements of atomic weight greater than 256± (which would equate to between elements 99 and 100 in modern-day terms) did not exist. A higher—more recent—estimate is that the periodic table may end soon after the island of stability, which is expected to center around element 126, as the extension of the periodic and nuclides tables is restricted by proton and neutron drip lines. Other predictions of an end to the periodic table include at element 128 by John Emsley, at element 137 by Richard Feynman, and at element 155 by Albert Khazan.
The Bohr model exhibits difficulty for atoms with atomic number greater than 137, as any element with an atomic number greater than 137 would require 1s electrons to be traveling faster than "c", the speed of light. Hence the non-relativistic Bohr model is inaccurate when applied to such an element.
The relativistic Dirac equation has problems for elements with more than 137 protons. For such elements, the wave function of the Dirac ground state is oscillatory rather than bound, and there is no gap between the positive and negative energy spectra, as in the Klein paradox. More accurate calculations taking into account the effects of the finite size of the nucleus indicate that the binding energy first exceeds the limit for elements with more than 173 protons. For heavier elements, if the innermost orbital (1s) is not filled, the electric field of the nucleus will pull an electron out of the vacuum, resulting in the spontaneous emission of a positron; however, this does not happen if the innermost orbital is filled, so that element 173 is not necessarily the end of the periodic table.
Placement of hydrogen and helium.
Simply following electron configurations, hydrogen (electronic configuration 1s1) and helium (1s2) should be placed in groups 1 and 2, above lithium ([He]2s1) and beryllium ([He]2s2). However, such placing is rarely used outside of the context of electron configurations: When the noble gases (then called "inert gases") were first discovered around 1900, they were known as "group 0," reflecting no chemical reactivity of these elements known at that point, and helium was placed on the top that group, as it did share the extreme chemical inertness seen throughout the group. As the group changed its formal number, many authors continued to assign helium directly above neon, in the group 18; one of the examples of such placing is the current IUPAC table.
Hydrogen's chemical properties are not very close to those of the alkali metals, which occupy the group 1, and on that basis hydrogen is sometimes placed elsewhere: one of the most common alternatives is in group 17; one of the factors behind it is the strictly univalent predominantly non-metallic chemistry of hydrogen, and that of fluorine (the element placed on the top of the group 17) is strictly univalent and non-metallic. Sometimes, to show how hydrogen has properties both corresponding to those of the alkali metals and the halogens, it may be shown in two columns simultaneously. Another suggestion is above carbon in group 14: placed that way, it fits well into the trend of increasing trends of ionization potential values and electron affinity values, and is not too stray from the electronegativity trend. Finally, hydrogen is sometimes placed separately from any group; this is based on how general properties of hydrogen differ from that of any group: unlike hydrogen, the other group 1 elements show extremely metallic behavior; the group 17 elements commonly form salts (hence the term "halogen"); elements of any other group show some multivalent chemistry. The other period 1 element, helium, is sometimes placed separately from any group as well. The property that distinguishes helium from the rest of the noble gases (even though the extraordinary inertness of helium is extremely close to that of neon and argon) is that in its closed electron shell, helium has only two electrons in the outermost electron orbital, while the rest of the noble gases have eight.
Groups included in the transition metals.
The definition of a transition metal, as given by IUPAC, is an element whose atom has an incomplete d sub-shell, or which can give rise to cations with an incomplete d sub-shell. By this definition all of the elements in groups 3–11 are transition metals. The IUPAC definition therefore excludes group 12, comprising zinc, cadmium and mercury, from the transition metals category.
Some chemists treat the categories "d-block elements" and "transition metals" interchangeably, thereby including groups 3–12 among the transition metals. In this instance the group 12 elements are treated as a special case of transition metal in which the d electrons are not ordinarily involved in chemical bonding. The recent discovery that mercury can use its d electrons in the formation of mercury(IV) fluoride (HgF4) has prompted some commentators to suggest that mercury can be regarded as a transition metal. Other commentators, such as Jensen, have argued that the formation of a compound like HgF4 can occur only under highly abnormal conditions. As such, mercury could not be regarded as a transition metal by any reasonable interpretation of the ordinary meaning of the term.
Still other chemists further exclude the group 3 elements from the definition of a transition metal. They do so on the basis that the group 3 elements do not form any ions having a partially occupied d shell and do not therefore exhibit any properties characteristic of transition metal chemistry. In this case, only groups 4–11 are regarded as transition metals.
Period 6 and 7 elements in group 3.
Although scandium and yttrium are always the first two elements in group 3 the identity of the next two elements is not settled. They are either lanthanum and actinium; or lutetium and lawrencium. There are strong chemical and physical arguments supporting the latter arrangement but not all authors have been convinced. Most working chemists are not aware there is any controversy.
Lanthanum and actinium are traditionally depicted as the remaining group 3 members. It has been suggested that this layout originated in the 1940s, with the appearance of periodic tables relying on the electron configurations of the elements and the notion of the differentiating electron. The configurations of caesium, barium and lanthanum are [Xe]6s1, [Xe]6s2 and [Xe]5d16s2. Lanthanum thus has a 5"d" differentiating electron and this establishes "it in group 3 as the first member of the "d"-block for period 6." A consistent set of electron configurations is then seen in group 3: scandium [Ar]3d14s2, yttrium [Kr]4d15s2 and lanthanum [Xe]5d16s2. Still in period 6, ytterbium was assigned an electron configuration of [Xe]4f135d16s2 and lutetium [Xe]4f145d16s2, "resulting in a 4"f" differentiating electron for lutetium and firmly establishing it as the last member of the "f"-block for period 6."
In other tables, lutetium and lawrencium are the remaining group 3 members. It has been known since the early 20th century that, "yttrium and (to a lesser degree) scandium are closer in their chemical properties to lutetium and the other heavy rare earths [i.e. lanthanides] than they are to lanthanum." Accordingly, lutetium rather than lanthanum was assigned to group 3 by some chemists in the 1920s and 30s. Later spectroscopic work found that the electron configuration of ytterbium was in fact [Xe]4f146s2. This meant that ytterbium and lutetium—the latter with [Xe]4f145d16s2—both had 14 "f" electrons, "resulting in a "d" rather than an "f" differentiating electron" for lutetium and making it an "equally valid candidate" with [Xe]5d16s2 lanthanum, for the group 3 periodic table position below yttrium. Several physicists in the 1950s and 60s opted for lutetium, in light of a comparison of several of its physical properties with those of lanthanum. This arrangement, in which lanthanum is the first member of the "f"-block, is disputed by some authors since lanthanum lacks any "f" electrons. However, it has been argued that this is not valid concern given other periodic table anomalies—thorium, for example, has no "f" electrons yet is part of the "f"-block. As for lawrencium, its electron configuration was confirmed in 2015 as [Rn]5f147s27p1. Such a configuration represents another periodic table anomaly, regardless of whether lawrencium is located in the "f"-block or the "d"-block, as the only potentially applicable "p"-block position has been reserved for ununtrium with its predicted electron configuration of [Rn]5f146d107s27p1.
Some tables, including the table on the IUPAC site, place footnote markers in the two positions below scandium and yttrium, and show both lanthanum and lutetium, and actinium and lawrencium as being part of, respectively, the lanthanide series and the actinide series of elements. This arrangement emphasizes similarities in the chemistry of the 15 lanthanide elements (La–Lu) over electron configuration arguments. The actinides are more diverse in their behavior. Most early members show some similarities to transition metals; actinium and the later members are more like lanthanides.
Optimal form.
The many different forms of periodic table have prompted the question of whether there is an optimal or definitive form of periodic table. The answer to this question is thought to depend on whether the chemical periodicity seen to occur among the elements has an underlying truth, effectively hard-wired into the universe, or if any such periodicity is instead the product of subjective human interpretation, contingent upon the circumstances, beliefs and predilections of human observers. An objective basis for chemical periodicity would settle the questions about the location of hydrogen and helium, and the composition of group 3. Such an underlying truth, if it exists, is thought to have not yet been discovered. In its absence, the many different forms of periodic table can be regarded as variations on the theme of chemical periodicity, each of which explores and emphasizes different aspects, properties, perspectives and relationships of and among the elements. The ubiquity of the standard or medium-long periodic table is thought to be a result of this layout having a good balance of features in terms of ease of construction and size, and its depiction of atomic order and periodic trends.

</doc>
<doc id="754487" url="https://en.wikipedia.org/wiki?curid=754487" title="Permeability (electromagnetism)">
Permeability (electromagnetism)

In electromagnetism, permeability is the measure of the ability of a material to support the formation of a magnetic field within itself. Hence, it is the degree of magnetization that a material obtains in response to an applied magnetic field. Magnetic permeability is typically represented by the Greek letter "μ". The term was coined in September 1885 by Oliver Heaviside. The reciprocal of magnetic permeability is "magnetic reluctivity".
In SI units, permeability is measured in henries per meter (H·m−1), or newtons per ampere squared (N·A−2). The permeability constant ("μ"0), also known as the magnetic constant or the permeability of free space, is a measure of the amount of resistance encountered when forming a magnetic field in a classical vacuum. The magnetic constant has the exact (defined) value "µ"0 = 4"π" × 10−7 H·m−1≈ 1.2566370614…×10−6 H·m−1 or N·A−2).
A closely related property of materials is magnetic susceptibility, which is a dimensionless proportionality factor that indicates the degree of magnetization of a material in response to an applied magnetic field.
Explanation.
In electromagnetism, the auxiliary magnetic field H represents how a magnetic field B influences the organization of magnetic dipoles in a given medium, including dipole migration and magnetic dipole reorientation. Its relation to permeability is
where the permeability, "μ", is a scalar if the medium is isotropic or a second rank tensor for an anisotropic medium.
In general, permeability is not a constant, as it can vary with the position in the medium, the frequency of the field applied, humidity, temperature, and other parameters. In a nonlinear medium, the permeability can depend on the strength of the magnetic field. Permeability as a function of frequency can take on real or complex values. In ferromagnetic materials, the relationship between B and H exhibits both non-linearity and hysteresis: B is not a single-valued function of H, but depends also on the history of the material. For these materials it is sometimes useful to consider the "incremental permeability" defined as
This definition is useful in local linearizations of non-linear material behavior, for example in a Newton–Raphson iterative solution scheme that computes the changing saturation of a magnetic circuit.
Permeability is the inductance per unit length. In SI units, permeability is measured in henries per metre (H·m−1 = J/(A2·m) = N A−2). The auxiliary magnetic field H has dimensions current per unit length and is measured in units of amperes per metre (A m−1). The product "μ" H thus has dimensions inductance times current per unit area (H·A/m2). But inductance is magnetic flux per unit current, so the product has dimensions magnetic flux per unit area. This is just the magnetic field B, which is measured in webers (volt-seconds) per square-metre (V·s/m2), or teslas (T).
B is related to the Lorentz force on a moving charge "q":
The charge "q" is given in coulombs (C), the velocity "v" in meters per second (m/s), so that the force "F" is in newtons (N):
H is related to the magnetic dipole density. A magnetic dipole is a closed circulation of electric current. The dipole moment has dimensions current times area, units ampere square-metre (A·m2), and magnitude equal to the current around the loop times the area of the loop. The H field at a distance from a dipole has magnitude proportional to the dipole moment divided by distance cubed, which has dimensions current per unit length.
Relative permeability and magnetic susceptibility.
Relative permeability, sometimes denoted by the symbol "μ"r, is the ratio of the permeability of a specific medium to the permeability of free space "μ"0:
where "μ"0 = 4"π" × 10−7 N A−2. In terms of relative permeability, the magnetic susceptibility is
The number "χ"m is a dimensionless quantity, sometimes called "volumetric" or "bulk" susceptibility, to distinguish it from "χ"p ("magnetic mass" or "specific" susceptibility) and "χ"M ("molar" or "molar mass" susceptibility).
Diamagnetism.
"Diamagnetism" is the property of an object which causes it to create a magnetic field in opposition of an externally applied magnetic field, thus causing a repulsive effect. Specifically, an external magnetic field alters the orbital velocity of electrons around their nuclei, thus changing the magnetic dipole moment in the direction opposing the external field. Diamagnets are materials with a magnetic permeability less than "μ"0 (a relative permeability less than 1).
Consequently, diamagnetism is a form of magnetism that a substance exhibits only in the presence of an externally applied magnetic field. It is generally a quite weak effect in most materials, although superconductors exhibit a strong effect.
Paramagnetism.
"Paramagnetism" is a form of magnetism which occurs only in the presence of an externally applied magnetic field. Paramagnetic materials are attracted to magnetic fields, hence have a relative magnetic permeability greater than one (or, equivalently, a positive magnetic susceptibility).
The magnetic moment induced by the applied field is "linear" in the field strength and rather "weak". It typically requires a sensitive analytical balance to detect the effect. Unlike ferromagnets, paramagnets do not retain any magnetization in the absence of an externally applied magnetic field, because thermal motion causes the spins to become "randomly oriented" without it. Thus the total magnetization will drop to zero when the applied field is removed. Even in the presence of the field there is only a small "induced" magnetization because only a small fraction of the spins will be oriented by the field. This fraction is proportional to the field strength and this explains the linear dependency. The attraction experienced by ferromagnets is non-linear and much stronger, so that it is easily observed, for instance, in magnets on one's refrigerator.
Gyromagnetism.
For gyromagnetic media (see Faraday rotation) the magnetic permeability response to an alternating electromagnetic field in the microwave frequency domain is treated as a non-diagonal tensor expressed by:
Values for some common materials.
The following table should be used with caution as the permeability of ferromagnetic materials varies greatly with field strength. For example 4% Si steel has an initial relative permeability (at or near 0T) of 2,000 and a maximum of 35,000 and, indeed, the relative permeability of any material at a sufficiently high field strength trends toward 1.
A good magnetic core material must have high permeability.
For "passive" magnetic levitation a relative permeability below 1 is needed (corresponding to a negative susceptibility).
Permeability varies with magnetic field. Values shown above are approximate and valid only at the magnetic fields shown. They are given for a zero frequency; in practice, the permeability is generally a function of the frequency. When frequency is considered, the permeability can be complex, corresponding to the in phase and out of phase response.
Note that the magnetic constant "μ"0 has an exact value in SI units (that is, there is no uncertainty in its value) because the definition of the ampere fixes its value to 4"π" × 10−7 H/m exactly.
Complex permeability.
A useful tool for dealing with high frequency magnetic effects is the complex permeability. While at low frequencies in a linear material the magnetic field and the auxiliary magnetic field are simply proportional to each other through some scalar permeability, at high frequencies these quantities will react to each other with some lag time. These fields can be written as phasors, such that
where formula_9 is the phase delay of formula_10 from formula_11. Understanding permeability as the ratio of the magnetic flux density to the magnetic field, the ratio of the phasors can be written and simplified as
so that the permeability becomes a complex number.
By Euler's formula, the complex permeability can be translated from polar to rectangular form,
The ratio of the imaginary to the real part of the complex permeability is called the loss tangent,
which provides a measure of how much power is lost in a material versus how much is stored.

</doc>
<doc id="53933" url="https://en.wikipedia.org/wiki?curid=53933" title="Permittivity">
Permittivity

In electromagnetism, absolute permittivity is the measure of resistance that is encountered when forming an electric field in a medium. In other words, permittivity is a measure of how an electric field affects, and is affected by, a dielectric medium. The permittivity of a medium describes how much electric field (more correctly, flux) is 'generated' per unit charge in that medium. More electric flux exists in a medium with a low permittivity (per unit charge) because of polarization effects. Permittivity is directly related to electric susceptibility, which is a measure of how easily a dielectric polarizes in response to an electric field. Thus, permittivity relates to a material's ability to resist an electric field (while unfortunately the word stem "permit" suggests the inverse quantity).
In SI units, permittivity "ε" is measured in farads per meter (F/m); electric susceptibility "χ" is dimensionless. They are related to each other through
where "ε"r is the relative permittivity of the material, and "ε"0 = 8.8541878176.. × 10−12 F/m is the vacuum permittivity.
Explanation.
In electromagnetism, the electric displacement field D represents how an electric field E influences the organization of electric charges in a given medium, including charge migration and electric dipole reorientation. Its relation to permittivity in the very simple case of "linear, homogeneous, isotropic" materials with ""instantaneous" response" to changes in electric field is
where the permittivity "ε" is a scalar. If the medium is anisotropic, the permittivity is a second rank tensor.
In general, permittivity is not a constant, as it can vary with the position in the medium, the frequency of the field applied, humidity, temperature, and other parameters. In a nonlinear medium, the permittivity can depend on the strength of the electric field. Permittivity as a function of frequency can take on real or complex values.
In SI units, permittivity is measured in farads per meter (F/m or A2·s4·kg−1·m−3). The displacement field D is measured in units of coulombs per square meter (C/m2), while the electric field E is measured in volts per meter (V/m). D and E describe the interaction between charged objects. D is related to the "charge densities" associated with this interaction, while E is related to the "forces" and "potential differences".
Vacuum permittivity.
The vacuum permittivity "ε"0 (also called permittivity of free space or the electric constant) is the ratio D/E in free space. It also appears in the Coulomb force constant, "k"e = 1/(4"πε"0).
Its value is
where
Constants "c"0 and "μ"0 are defined in SI units to have exact numerical values, shifting responsibility of experiment to the determination of the meter and the ampere. (The approximation in the second value of "ε"0 above stems from "π" being an irrational number.)
Relative permittivity.
The linear permittivity of a homogeneous material is usually given relative to that of free space, as a relative permittivity "ε"r (also called dielectric constant, although this sometimes only refers to the static, zero-frequency relative permittivity). In an anisotropic material, the relative permittivity may be a tensor, causing birefringence. The actual permittivity is then calculated by multiplying the relative permittivity by "ε"0:
where χ (frequently written χe) is the electric susceptibility of the material.
The susceptibility is defined as the constant of proportionality (which may be a tensor) relating an electric field E to the induced dielectric polarization density P such that
where "ε"0 is the electric permittivity of free space.
The susceptibility of a medium is related to its relative permittivity "ε"r by
So in the case of a vacuum,
The susceptibility is also related to the polarizability of individual particles in the medium by the Clausius-Mossotti relation.
The electric displacement D is related to the polarization density P by
The permittivity "ε" and permeability "µ" of a medium together determine the phase velocity "v" = "c"/"n" of electromagnetic radiation through that medium:
Dispersion and causality.
In general, a material cannot polarize instantaneously in response to an applied field, and so the more general formulation as a function of time is
That is, the polarization is a convolution of the electric field at previous times with time-dependent susceptibility given by "χ"(Δ"t"). The upper limit of this integral can be extended to infinity as well if one defines "χ"(Δ"t") = 0 for Δ"t" < 0. An instantaneous response would correspond to a Dirac delta function susceptibility "χ"(Δ"t") = "χ δ"(Δ"t").
It is convenient to take the Fourier transform with respect to time and write this relationship as a function of frequency. Because of the convolution theorem, the integral becomes a simple product,
This frequency dependence of the susceptibility leads to frequency dependence of the permittivity. The shape of the susceptibility with respect to frequency characterizes the dispersion properties of the material.
Moreover, the fact that the polarization can only depend on the electric field at previous times (i.e. effectively "χ"(Δ"t") = 0 for Δ"t" < 0), a consequence of causality, imposes Kramers–Kronig constraints on the susceptibility "χ"(0).
Complex permittivity.
As opposed to the response of a vacuum, the response of normal materials to external fields generally depends on the frequency of the field. This frequency dependence reflects the fact that a material's polarization does not respond instantaneously to an applied field. The response must always be "causal" (arising after the applied field) which can be represented by a phase difference. For this reason, permittivity is often treated as a complex function of the (angular) frequency of the applied field "ω": formula_12 (since complex numbers allow specification of magnitude and phase). The definition of permittivity therefore becomes
where
The response of a medium to static electric fields is described by the low-frequency limit of permittivity, also called the static permittivity "ε"s (also "ε"DC
At the high-frequency limit, the complex permittivity is commonly referred to as "ε"∞. At the plasma frequency and above, dielectrics behave as ideal metals, with electron gas behavior. The static permittivity is a good approximation for alternating fields of low frequencies, and as the frequency increases a measurable phase difference "δ" emerges between D and E. The frequency at which the phase shift becomes noticeable depends on temperature and the details of the medium. For moderate fields strength ("E"0), D and E remain proportional, and
Since the response of materials to alternating fields is characterized by a complex permittivity, it is natural to separate its real and imaginary parts, which is done by convention in the following way:
where
The choice of sign for time-dependence, exp(-"iωt"), dictates the sign convention for the imaginary part of permittivity. The signs used here correspond to those commonly used in physics, whereas for the engineering convention one should reverse all imaginary quantities.
The complex permittivity is usually a complicated function of frequency "ω", since it is a superimposed description of dispersion phenomena occurring at multiple frequencies. The dielectric function "ε"("ω") must have poles only for frequencies with positive imaginary parts, and therefore satisfies the Kramers–Kronig relations. However, in the narrow frequency ranges that are often studied in practice, the permittivity can be approximated as frequency-independent or by model functions.
At a given frequency, the imaginary part of formula_17 leads to absorption loss if it is positive (in the above sign convention) and gain if it is negative. More generally, the imaginary parts of the eigenvalues of the anisotropic dielectric tensor should be considered.
In the case of solids, the complex dielectric function is intimately connected to band structure. The primary quantity that characterizes the electronic structure of any crystalline material is the probability of photon absorption, which is directly related to the imaginary part of the optical dielectric function "ε"("ω"). The optical dielectric function is given by the fundamental expression:
In this expression, "Wc,v"("E") represents the product of the Brillouin zone-averaged transition probability at the energy "E" with the joint density of states, "Jc,v"("E"); "φ" is a broadening function, representing the role of scattering in smearing out the energy levels. In general, the broadening is intermediate between Lorentzian and Gaussian; for an alloy it is somewhat closer to Gaussian because of strong scattering from statistical fluctuations in the local composition on a nanometer scale.
Tensorial permittivity.
According to the Drude model of magnetized plasma, a more general expression which takes into account the interaction of the carriers with an alternating electric field at millimeter and microwave frequencies in an axially magnetized semiconductor requires the expression of the permittivity as a non-diagonal tensor. (see also Electro-gyration).
If formula_20 vanishes, then the tensor is diagonal but not proportional to the identity and the medium is said to be a uniaxial medium, which has similar properties to a uniaxial crystal.
Classification of materials.
Materials can be classified according to their complex-valued permittivity "ε", upon comparison of its real "ε"’ and imaginary "ε"" components (or, equivalently, conductivity, "σ", when it's accounted for in the latter). A "perfect conductor" has infinite conductivity, "σ"=∞, while a "perfect dielectric" is a material that has no conductivity at all, "σ"=0; this latter case, of real-valued permittivity (or complex-valued permittivity with zero imaginary component) is also associated with the name "lossless media". Generally, when "σ"/("ωε"’) ≪ 1 we consider the material to be a "low-loss dielectric" (nearly though not exactly lossless), whereas "σ"/("ωε"’) ≫ 1 is associated with a "good conductor"; such materials with non-negligible conductivity yield a large amount of loss that inhibit the propagation of electromagnetic waves, thus are also said to be "lossy media". Those materials that do not fall under either limit are considered to be general media.
Lossy medium.
In the case of a lossy medium, i.e. when the conduction current is not negligible, the total current density flowing is:
where
The size of the displacement current is dependent on the frequency ω of the applied field "E"; there is no displacement current in a constant field.
In this formalism, the complex permittivity is defined as:
In general, the absorption of electromagnetic energy by dielectrics is covered by a few different mechanisms that influence the shape of the permittivity as a function of frequency:
The above effects often combine to cause non-linear effects within capacitors. For example, dielectric absorption refers to the inability of a capacitor that has been charged for a long time to completely discharge when briefly discharged. Although an ideal capacitor would remain at zero volts after being discharged, real capacitors will develop a small voltage, a phenomenon that is also called "soakage" or "battery action". For some dielectrics, such as many polymer films, the resulting voltage may be less than 1-2% of the original voltage. However, it can be as much as 15 - 25% in the case of electrolytic capacitors or supercapacitors.
Quantum-mechanical interpretation.
In terms of quantum mechanics, permittivity is explained by atomic and molecular interactions.
At low frequencies, molecules in polar dielectrics are polarized by an applied electric field, which induces periodic rotations. For example, at the microwave frequency, the microwave field causes the periodic rotation of water molecules, sufficient to break hydrogen bonds. The field does work against the bonds and the energy is absorbed by the material as heat. This is why microwave ovens work very well for materials containing water. There are two maxima of the imaginary component (the absorptive index) of water, one at the microwave frequency, and the other at far ultraviolet (UV) frequency. Both of these resonances are at higher frequencies than the operating frequency of microwave ovens.
At moderate frequencies, the energy is too high to cause rotation, yet too low to affect electrons directly, and is absorbed in the form of resonant molecular vibrations. In water, this is where the absorptive index starts to drop sharply, and the minimum of the imaginary permittivity is at the frequency of blue light (optical regime).
At high frequencies (such as UV and above), molecules cannot relax, and the energy is purely absorbed by atoms, exciting electron energy levels. Thus, these frequencies are classified as ionizing radiation.
While carrying out a complete "ab initio" (that is, first-principles) modelling is now computationally possible, it has not been widely applied yet. Thus, a phenomenological model is accepted as being an adequate method of capturing experimental behaviors. The Debye model and the Lorentz model use a 1st-order and 2nd-order (respectively) lumped system parameter linear representation (such as an RC and an LRC resonant circuit).
Measurement.
The dielectric constant of a material can be found by a variety of static electrical measurements. The complex permittivity is evaluated over a wide range of frequencies by using different variants of dielectric spectroscopy, covering nearly 21 orders of magnitude from 10−6 to 1015 Hz. Also, by using cryostats and ovens, the dielectric properties of a medium can be characterized over an array of temperatures. In order to study systems for such diverse excitation fields, a number of measurement setups are used, each adequate for a special frequency range.
Various microwave measurement techniques are outlined in Chen "et al.". Typical errors for the Hakki-Coleman method employing a puck of material between conducting planes are about 0.3%.
At infrared and optical frequencies, a common technique is ellipsometry. Dual polarisation interferometry is also used to measure the complex refractive index for very thin films at optical frequencies.

</doc>
<doc id="297069" url="https://en.wikipedia.org/wiki?curid=297069" title="Perturbation theory (quantum mechanics)">
Perturbation theory (quantum mechanics)

In quantum mechanics, perturbation theory is a set of approximation schemes directly related to mathematical perturbation for describing a complicated quantum system in terms of a simpler one. The idea is to start with a simple system for which a mathematical solution is known, and add an additional "perturbing" Hamiltonian representing a weak disturbance to the system. If the disturbance is not too large, the various physical quantities associated with the perturbed system (e.g. its energy levels and eigenstates) can be expressed as "corrections" to those of the simple system. These corrections, being small compared to the size of the quantities themselves, can be calculated using approximate methods such as asymptotic series. The complicated system can therefore be studied based on knowledge of the simpler one.
Applications of perturbation theory.
Perturbation theory is an important tool for describing real quantum systems, as it turns out to be very difficult to find exact solutions to the Schrödinger equation for Hamiltonians of even moderate complexity. The Hamiltonians to which we know exact solutions, such as the hydrogen atom, the quantum harmonic oscillator and the particle in a box, are too idealized to adequately describe most systems. Using perturbation theory, we can use the known solutions of these simple Hamiltonians to generate solutions for a range of more complicated systems.
For example, by adding a perturbative electric potential to the quantum mechanical model of the hydrogen atom, we can calculate the tiny shifts in the spectral lines of hydrogen caused by the presence of an electric field (the Stark effect). This is only approximate because the sum of a Coulomb potential with a linear potential is unstable (has no true bound states) although the tunneling time (decay rate) is very long. This instability shows up as a broadening of the energy spectrum lines, which perturbation theory fails to reproduce entirely.
The expressions produced by perturbation theory are not exact, but they can lead to accurate results as long as the expansion parameter, say , is very small. Typically, the results are expressed in terms of finite power series in that seem to converge to the exact values when summed to higher order. After a certain order however, the results become increasingly worse since the series are usually divergent (being asymptotic series). There exist ways to convert them into convergent series, which can be evaluated for large-expansion parameters, most efficiently by Variational method.
In the theory of quantum electrodynamics (QED), in which the electron–photon interaction is treated perturbatively, the calculation of the electron's magnetic moment has been found to agree with experiment to eleven decimal places. In QED and other quantum field theories, special calculation techniques known as Feynman diagrams are used to systematically sum the power series terms.
Under some circumstances, perturbation theory is an invalid approach to take. This happens when the system we wish to describe cannot be described by a small perturbation imposed on some simple system. In quantum chromodynamics, for instance, the interaction of quarks with the gluon field cannot be treated perturbatively at low energies because the coupling constant (the expansion parameter) becomes too large. Perturbation theory also fails to describe states that are not generated adiabatically from the "free model", including bound states and various collective phenomena such as solitons. Imagine, for example, that we have a system of free (i.e. non-interacting) particles, to which an attractive interaction is introduced. Depending on the form of the interaction, this may create an entirely new set of eigenstates corresponding to groups of particles bound to one another. An example of this phenomenon may be found in conventional superconductivity, in which the phonon-mediated attraction between conduction electrons leads to the formation of correlated electron pairs known as Cooper pairs. When faced with such systems, one usually turns to other approximation schemes, such as the variational method and the WKB approximation. This is because there is no analogue of a bound particle in the unperturbed model and the energy of a soliton typically goes as the "inverse" of the expansion parameter. However, if we "integrate" over the solitonic phenomena, the nonperturbative corrections in this case will be tiny; of the order of exp(−1/) or exp(−1/2) in the perturbation parameter . Perturbation theory can only detect solutions "close" to the unperturbed solution, even if there are other solutions for which the perturbative expansion is not valid.
The problem of non-perturbative systems has been somewhat alleviated by the advent of modern computers. It has become practical to obtain numerical non-perturbative solutions for certain problems, using methods such as density functional theory. These advances have been of particular benefit to the field of quantum chemistry. Computers have also been used to carry out perturbation theory calculations to extraordinarily high levels of precision, which has proven important in particle physics for generating theoretical results that can be compared with experiment.
Time-independent perturbation theory.
Time-independent perturbation theory is one of two categories of perturbation theory, the other being time-dependent perturbation (see next section). In time-independent perturbation theory the perturbation Hamiltonian is static (i.e., possesses no time dependence). Time-independent perturbation theory was presented by Erwin Schrödinger in a 1926 paper, shortly after he produced his theories in wave mechanics. In this paper Schrödinger referred to earlier work of Lord Rayleigh, who investigated harmonic vibrations of a string perturbed by small inhomogeneities. This is why this perturbation theory is often referred to as Rayleigh–Schrödinger perturbation theory.
First order corrections.
We begin with an unperturbed Hamiltonian , which is also assumed to have no time dependence. It has known energy levels and eigenstates, arising from the time-independent Schrödinger equation:
For simplicity, we have assumed that the energies are discrete. The superscripts denote that these quantities are associated with the unperturbed system. Note the use of bra–ket notation.
We now introduce a perturbation to the Hamiltonian. Let be a Hamiltonian representing a weak physical disturbance, such as a potential energy produced by an external field. (Thus, is formally a Hermitian operator.) Let be a dimensionless parameter that can take on values ranging continuously from 0 (no perturbation) to 1 (the full perturbation). The perturbed Hamiltonian is
The energy levels and eigenstates of the perturbed Hamiltonian are again given by the Schrödinger equation:
Our goal is to express and formula_4 in terms of the energy levels and eigenstates of the old Hamiltonian. If the perturbation is sufficiently weak, we can write them as power series in :
where
When , these reduce to the unperturbed values, which are the first term in each series. Since the perturbation is weak, the energy levels and eigenstates should not deviate too much from their unperturbed values, and the terms should rapidly become smaller as we go to higher order.
Substituting the power series expansion into the Schrödinger equation, we obtain
Expanding this equation and comparing coefficients of each power of "λ" results in an infinite series of simultaneous equations. The zeroth-order equation is simply the Schrödinger equation for the unperturbed system. The first-order equation is
Operating through by formula_9. The first term on the left-hand side cancels with the first term on the right-hand side. (Recall, the unperturbed Hamiltonian is Hermitian). This leads to the first-order energy shift:
This is simply the expectation value of the perturbation Hamiltonian while the system is in the unperturbed state. This result can be interpreted in the following way: suppose the perturbation is applied, but we keep the system in the quantum state formula_11, which is a valid quantum state though no longer an energy eigenstate. The perturbation causes the average energy of this state to increase by formula_12. However, the true energy shift is slightly different, because the perturbed eigenstate is not exactly the same as "formula_11". These further shifts are given by the second and higher order corrections to the energy.
Before we compute the corrections to the energy eigenstate, we need to address the issue of normalization. We may suppose
but perturbation theory assumes we also have formula_15. It follows that at first order in , we must have
Since the overall phase is not determined in quantum mechanics, without loss of generality, we may assume formula_19 is purely real. Therefore,
and we deduce
To obtain the first-order correction to the energy eigenstate, we insert our expression for the first-order energy correction back into the result shown above of equating the first-order coefficients of . We then make use of the resolution of the identity,
where the formula_23 are in the orthogonal complement of formula_24. The result is
For the moment, suppose that the zeroth-order energy level is not degenerate, i.e. there is no eigenstate of in the orthogonal complement of formula_24 with the energy formula_27. We multiply through by formula_28, which gives
and hence the component of the first-order correction along formula_30 since by assumption formula_31. In total we get
The first-order change in the -th energy eigenket has a contribution from each of the energy eigenstates . Each term is proportional to the matrix element formula_33, which is a measure of how much the perturbation mixes eigenstate with eigenstate ; it is also inversely proportional to the energy difference between eigenstates and , which means that the perturbation deforms the eigenstate to a greater extent if there are more eigenstates at nearby energies. We see also that the expression is singular if any of these states have the same energy as state , which is why we assumed that there is no degeneracy.
Second-order and higher corrections.
We can find the higher-order deviations by a similar procedure, though the calculations become quite tedious with our current formulation. Our normalization prescription gives that
Up to second order, the expressions for the energies and (normalized) eigenstates are:
Extending the process further, the third-order energy correction can be shown to be 
E_n^{(3)} &= \frac{V_{nk_3}V_{k_3k_2}V_{k_2n}}{E_{nk_2}E_{nk_3}}-V_{nn}\frac{|V_{nk_3}|^2}{E_{nk_3}^2} \\
E_n^{(4)} &= \frac{V_{nk_4}V_{k_4k_3}V_{k_3k_2}V_{k_2n}}{E_{nk_2}E_{nk_3}E_{nk_4}}-\frac{|V_{nk_4}|^2}{E_{nk_4}^2}\frac{|V_{nk_2}|^2}{E_{nk_2}}-V_{nn}\frac{V_{nk_4}V_{k_4k_3}V_{k_3n}}{E_{nk_3}^2E_{nk_4}}-V_{nn}\frac{V_{nk_4}V_{k_4k_2}V_{k_2n}}{E_{nk_2}E_{nk_4}^2}+V_{nn}^2\frac{|V_{nk_4}|^2}{E_{nk_4}^3} \\
&=\frac{V_{nk_4}V_{k_4k_3}V_{k_3k_2}V_{k_2n}}{E_{nk_2}E_{nk_3}E_{nk_4}}-E_{n}^{(2)}\frac{|V_{nk_4}|^2}{E_{nk_4}^2}-2V_{nn}\frac{V_{nk_4}V_{k_4k_3}V_{k_3n}}{E_{nk_3}^2E_{nk_4}} +V_{nn}^2\frac{|V_{nk_4}|^2}{E_{nk_4}^3} \\
E_n^{(5)} &=\frac{V_{nk_5}V_{k_5k_4}V_{k_4k_3}V_{k_3k_2}V_{k_2n}}{E_{nk_2}E_{nk_3}E_{nk_4}E_{nk_5}}-\frac{V_{nk_5}V_{k_5k_4}V_{k_4n}}{E_{nk_4}^2E_{nk_5}}\frac{|V_{nk_2}|^2}{E_{nk_2}}-\frac{V_{nk_5}V_{k_5k_2}V_{k_2n}}{E_{nk_2}E_{nk_5}^2}\frac{|V_{nk_2}|^2}{E_{nk_2}}-\frac{|V_{nk_5}|^2}{E_{nk_5}^2}\frac{V_{nk_3}V_{k_3k_2}V_{k_2n}}{E_{nk_2}E_{nk_3}} \\
&\quad -V_{nn}\frac{V_{nk_5}V_{k_5k_4}V_{k_4k_3}V_{k_3n}}{E_{nk_3}^2E_{nk_4}E_{nk_5}}-V_{nn}\frac{V_{nk_5}V_{k_5k_4}V_{k_4k_2}V_{k_2n}}{E_{nk_2}E_{nk_4}^2E_{nk_5}}-V_{nn}\frac{V_{nk_5}V_{k_5k_3}V_{k_3k_2}V_{k_2n}}{E_{nk_2}E_{nk_3}E_{nk_5}^2}+V_{nn}\frac{|V_{nk_5}|^2}{E_{nk_5}^2}\frac{|V_{nk_3}|^2}{E_{nk_3}^2}+2V_{nn}\frac{|V_{nk_5}|^2}{E_{nk_5}^3}\frac{|V_{nk_2}|^2}{E_{nk_2}} \\
&\quad +V_{nn}^2\frac{V_{nk_5}V_{k_5k_4}V_{k_4n}}{E_{nk_4}^3E_{nk_5}}+V_{nn}^2\frac{V_{nk_5}V_{k_5k_3}V_{k_3n}}{E_{nk_3}^2E_{nk_5}^2}+V_{nn}^2\frac{V_{nk_5}V_{k_5k_2}V_{k_2n}}{E_{nk_2}E_{nk_5}^3}-V_{nn}^3\frac{|V_{nk_5}|^2}{E_{nk_5}^4} \\
&=\frac{V_{nk_5}V_{k_5k_4}V_{k_4k_3}V_{k_3k_2}V_{k_2n}}{E_{nk_2}E_{nk_3}E_{nk_4}E_{nk_5}}-2E_n^{(2)}\frac{V_{nk_5}V_{k_5k_4}V_{k_4n}}{E_{nk_4}^2E_{nk_5}}-\frac{|V_{nk_5}|^2}{E_{nk_5}^2}\frac{V_{nk_3}V_{k_3k_2}V_{k_2n}}{E_{nk_2}E_{nk_3}} \\
&\quad -2V_{nn}\left(\frac{V_{nk_5}V_{k_5k_4}V_{k_4k_3}V_{k_3n}}{E_{nk_3}^2E_{nk_4}E_{nk_5}}-\frac{V_{nk_5}V_{k_5k_4}V_{k_4k_2}V_{k_2n}}{E_{nk_2}E_{nk_4}^2E_{nk_5}}+\frac{|V_{nk_5}|^2}{E_{nk_5}^2}\frac{|V_{nk_3}|^2}{E_{nk_3}^2}+2E_n^{(2)}\frac{|V_{nk_5}|^2}{E_{nk_5}^3}\right) \\
\end{align}</math>
and the states to fourth order can be written
All terms involved should be summed over such that the denominator does not vanish.
Effects of degeneracy.
Suppose that two or more energy eigenstates are degenerate. The first-order energy shift is not well defined, since there is no unique way to choose a basis of eigenstates for the unperturbed system. The calculation of the change in the eigenstate is problematic as well, because the operator
does not have a well-defined inverse.
Let denote the subspace spanned by these degenerate eigenstates. No matter how small the perturbation is, in the degenerate subspace the energy differences between the eigenstates are zero, so complete mixing of at least some of these states is assured. Thus the perturbation can not be considered small in the subspace and in that subspace the new Hamiltonian must be diagonalized first. These correct perturbed eigenstates in are now the basis for the perturbation expansion:
where only eigenstates outside of the subspace are considered to be small. For the first-order perturbation we need to solve the perturbed Hamiltonian restricted to the degenerate subspace 
simultaneously for all the degenerate eigenstates, where formula_42 are first-order corrections to the degenerate energy levels. This is equivalent to diagonalizing the matrix
This procedure is approximate, since we neglected states outside the subspace. The splitting of degenerate energies formula_42 is generally observed. Although the splitting may be small compared to the range of energies found in the system, it is crucial in understanding certain details, such as spectral lines in Electron Spin Resonance experiments.
Higher-order corrections due to other eigenstates can be found in the same way as for the non-degenerate case
The operator on the left hand side is not singular when applied to eigenstates outside , so we can write
but the effect on the degenerate states is minuscule, proportional to the square of the first-order correction formula_42.
Near-degenerate states should also be treated in the above manner, since the original Hamiltonian won't be larger than the perturbation in the near-degenerate subspace. An application is found in the nearly free electron model, where near-degeneracy treated properly gives rise to an energy gap even for small perturbations. Other eigenstates will only shift the absolute energy of all near-degenerate states simultaneously.
Generalization to multi-parameter case.
The generalization of the time-independent perturbation theory to the case where there are multiple small parameters formula_48 in place of λ can be formulated more systematically using the language of differential geometry, which basically defines the derivatives of the quantum states and calculate the perturbative corrections by taking derivatives iteratively at the unperturbed point.
Hamiltonian and force operator.
From the differential geometric point of view, a parameterized Hamiltonian is considered as a function defined on the parameter manifold that maps each particular set of parameters formula_49 to an Hermitian operator that acts on the Hilbert space. The parameters here can be external field, interaction strength, or driving parameters in the quantum phase transition. Let and formula_50 be the -th eigenenergy and eigenstate of respectively. In the language of differential geometry, the states formula_50 form a vector bundle over the parameter manifold, on which derivatives of these states can be defined. The perturbation theory is to answer the following question: given formula_52 and formula_53 at an unperturbed reference point formula_54, how to estimate the and formula_50 at close to that reference point.
Without loss of generality, the coordinate system can be shifted, such that the reference point formula_56 is set to be the origin. The following linearly parameterized Hamiltonian is frequently used
If the parameters are considered as generalized coordinates, then should be identified as the generalized force operators related to those coordinates. Different indices label the different forces along different directions in the parameter manifold. For example, if denotes the external magnetic field in the -direction, then should be the magnetization in the same direction.
Perturbation theory as power series expansion.
The validity of the perturbation theory lies on the adiabatic assumption, which assumes the eigenenergies and eigenstates of the Hamiltonian are smooth functions of parameters such that their values in the vicinity region can be calculated in power series (like Taylor expansion) of the parameters:
Here denotes the derivative with respect to . When applying to the state formula_59, it should be understood as the covariant derivative if the vector bundle is equipped with non-vanishing connection. All the terms on the right-hand-side of the series are evaluated at , e.g. and formula_60. This convention will be adopted throughout this subsection, that all functions without the parameter dependence explicitly stated are assumed to be evaluated at the origin. The power series may converge slowly or even not converge when the energy levels are close to each other. The adiabatic assumption breaks down when there is energy level degeneracy, and hence the perturbation theory is not applicable in that case.
Hellman–Feynman theorems.
The above power series expansion can be readily evaluated if there is a systematic approach to calculate the derivates to any order. Using the chain rule, the derivatives can be broken down to the single derivative on either the energy or the state. The Hellmann–Feynman theorems are used to calculate these single derivatives. The first Hellmann–Feynman theorem gives the derivative of the energy,
The second Hellmann–Feynman theorem gives the derivative of the state (resolved by the complete basis with m ≠ n),
For the linearly parameterized Hamiltonian, simply stands for the generalized force operator .
The theorems can be simply derived by applying the differential operator to both sides of the Schrödinger equation formula_63 which reads
Then overlap with the state formula_65 from left and make use of the Schrödinger equation formula_66 again,
Given that the eigenstates of the Hamiltonian always form an orthonormal basis formula_68, the cases of and can be discussed separately. The first case will lead to the first theorem and the second case to the second theorem, which can be shown immediately by rearranging the terms. With the differential rules given by the Hellmann–Feynman theorems, the perturbative correction to the energies and states can be calculated systematically.
Correction of energy and state.
To the second order, the energy correction reads
The first order derivative is given by the first Hellmann–Feynman theorem directly. To obtain the second order derivative , simply applying the differential operator to the result of the first order derivative formula_70, which reads
Note that for linearly parameterized Hamiltonian, there is no second derivative on the operator level. Resolve the derivative of state by inserting the complete set of basis,
then all parts can be calculated using the Hellmann–Feynman theorems. In terms of Lie derivatives, formula_73 according to the definition of the connection for the vector bundle. Therefore the case can be excluded from the summation, which avoids the singularity of the energy denominator. The same procedure can be carried on for higher order derivatives, from which higher order corrections are obtained.
The same computational scheme is applicable for the correction of states. The result to the second order is as follows
Both energy derivatives and state derivatives will be involved in deduction. Whenever a state derivative is encountered, resolve it by inserting the complete set of basis, then the Hellmann-Feynman theorem is applicable. Because differentiation can be calculated systematically, the series expansion approach to the perturbative corrections can be coded on computers with symbolic processing software like Mathematica.
Effective Hamiltonian.
Let be the Hamiltonian completely restricted either in the low-energy subspace formula_75 or in the high-energy subspace formula_76, such that there is no matrix element in connecting the low- and the high-energy subspaces, i.e. formula_77 if formula_78. Let be the coupling terms connecting the subspaces. Then when the high energy degrees of freedoms are integrated out, the effective Hamiltonian in the low energy subspace reads
Here formula_80 are restricted in the low energy subspace. The above result can be derived by power series expansion of formula_81.
In a formal way it is possible to define an effective Hamiltonian that gives exactly the low-lying energy states and wavefunctions. In practice, some kind of approximation (perturbation theory) is generally required.
Time-dependent perturbation theory.
Method of variation of constants.
Time-dependent perturbation theory, developed by Paul Dirac, studies the effect of a time-dependent perturbation applied to a time-independent Hamiltonian 0.
Since the perturbed Hamiltonian is time-dependent, so are its energy levels and eigenstates. Thus, the goals of time-dependent perturbation theory are slightly different from time-independent perturbation theory. One is interested in the following quantities:
The first quantity is important because it gives rise to the classical result of an measurement performed on a macroscopic number of copies of the perturbed system. For example, we could take to be the displacement in the -direction of the electron in a hydrogen atom, in which case the expected value, when multiplied by an appropriate coefficient, gives the time-dependent dielectric polarization of a hydrogen gas. With an appropriate choice of perturbation (i.e. an oscillating electric potential), this allows one to calculate the AC permittivity of the gas.
The second quantity looks at the time-dependent probability of occupation for each eigenstate. This is particularly useful in laser physics, where one is interested in the populations of different atomic states in a gas when a time-dependent electric field is applied. These probabilities are also useful for calculating the "quantum broadening" of spectral lines (see line broadening) and particle decay in particle physics and nuclear physics.
We will briefly examine the method behind Dirac's formulation of time-dependent perturbation theory. Choose an energy basis formula_82 for the unperturbed system. (We drop the (0) superscripts for the eigenstates, because it is not useful to speak of energy levels and eigenstates for the perturbed system.)
If the unperturbed system is in eigenstate formula_83 at time = 0, its state at subsequent times varies only by a phase (in the Schrödinger picture, where state vectors evolve in time and operators are constant),
Now, introduce a time-dependent perturbing Hamiltonian . The Hamiltonian of the perturbed system is
Let formula_86 denote the quantum state of the perturbed system at time . It obeys the time-dependent Schrödinger equation,
The quantum state at each instant can be expressed as a linear combination of the complete eigenbasis of formula_4:
where the s are to be determined complex functions of which we will refer to as amplitudes (strictly speaking, they are the amplitudes in the Dirac picture).
We have explicitly extracted the exponential phase factors formula_90 on the right hand side. This is only a matter of convention, and may be done without loss of generality. The reason we go to this trouble is that when the system starts in the state formula_91 and no perturbation is present, the amplitudes have the convenient property that, for all , 
 = 1 and = 0 if .
The square of the absolute amplitude is the probability that the system is in state at time , since
Plugging into the Schrödinger equation and using the fact that ∂/∂"t" acts by a chain rule, one obtains
By resolving the identity in front of , this can be reduced to a set of partial differential equations for the amplitudes,
The matrix elements of play a similar role as in time-independent perturbation theory, being proportional to the rate at which amplitudes are shifted between states. Note, however, that the direction of the shift is modified by the exponential phase factor. Over times much longer than the energy difference , the phase winds around 0 several times. If the time-dependence of is sufficiently slow, this may cause the state amplitudes to oscillate. ( E.g., such oscillations are useful for managing radiative transitions in a laser.)
Up to this point, we have made no approximations, so this set of differential equations is exact. By supplying appropriate initial values , we could in principle find an exact (i.e., non-perturbative) solution. This is easily done when there are only two energy levels ( = 1, 2), and this solution is useful for modelling systems like the ammonia molecule.
However, exact solutions are difficult to find when there are many energy levels, and one instead looks for perturbative solutions. These may be obtained by expressing the equations in an integral form,
Repeatedly substituting this expression for back into right hand side, yields an iterative solution,
where, for example, the first-order term is
Several further results follow from this, such as Fermi's golden rule, which relates the rate of transitions between quantum states to the density of states at particular energies; or the Dyson series, obtained by applying the iterative method to the time evolution operator, which is one of the starting points for the method of Feynman diagrams.
Method of Dyson series.
Time-dependent perturbations can be reorganized through the technique of the Dyson series. The Schrödinger equation
has the formal solution
where is the time ordering operator,
Thus, the exponential represents the following Dyson series,
Consider the following perturbation problem
assuming that the parameter is small and that the problem formula_103 has been solved.
Perform the following unitary transformation to the interaction picture (or Dirac picture),
Consequently, the Schrödinger equation simplifies to
so it is solved through the above Dyson series,
as a perturbation series with small .
Using the solution of the unperturbed problem formula_107 and formula_108 (for the sake of simplicity assume a pure discrete spectrum), yields, to first order,
Thus, the system, initially in the unperturbed state formula_110, by dint of the perturbation can go into the state formula_111. The corresponding transition probability amplitude to first order is
as detailed in the previous section——while the corresponding transition probability to a continuum is furnished by Fermi's golden rule.
As an aside, note that time-independent perturbation theory is also organized inside this time-dependent perturbation theory Dyson series. To see this, write the unitary evolution operator, obtained from the above Dyson series, as
and take the perturbation to be time-independent.
Using the identity resolution
with formula_107 for a pure discrete spectrum, write
It is evident that, at second order, one must sum on all the intermediate states. Assume formula_117 and the asymptotic limit of larger times. This means that, at each contribution of the perturbation series, one has to add a multiplicative factor formula_118 in the integrands for arbitrarily small. Thus the limit gives back the final state of the system by eliminating all oscillating terms, but keeping the secular ones. The integrals are thus computable, and, separating the diagonal terms from the others yields
where the time secular series yields the eigenvalues of the perturbed problem specified above, recursively; whereas the remaining time-constant part yields the corrections to the stationary eigenfunctions also given above (formula_120.)
The unitary evolution operator is applicable to arbitrary eigenstates of the unperturbed problem and, in this case, yields a secular series that holds at small times.
Strong perturbation theory.
In a similar way as for small perturbations, it is possible to develop a strong perturbation theory. Let us consider as usual the Schrödinger equation
and we consider the question if a dual Dyson series exists that applies in the limit of a perturbation increasingly large. This question can be answered in an affirmative way and the series is the well-known adiabatic series. This approach is quite general and can be shown in the following way. Let us consider the perturbation problem
being . Our aim is to find a solution in the form
but a direct substitution into the above equation fails to produce useful results. This situation can be adjusted making a rescaling of the time variable as formula_124 producing the following meaningful equations
that can be solved once we know the solution of the leading order equation. But we know that in this case we can use the adiabatic approximation. When formula_128 does not depend on time one gets the Wigner-Kirkwood series that is often used in statistical mechanics. Indeed, in this case we introduce the unitary transformation
that defines a free picture as we are trying to eliminate the interaction term. Now, in dual way with respect to the small perturbations, we have to solve the Schrödinger equation
and we see that the expansion parameter appears only into the exponential and so, the corresponding Dyson series, a dual Dyson series, is meaningful at large s and is
After the rescaling in time formula_124 we can see that this is indeed a series in formula_133 justifying in this way the name of dual Dyson series. The reason is that we have obtained this series simply interchanging and and we can go from one to another applying this exchange. This is called duality principle in perturbation theory. The choice formula_134 yields, as already said, a Wigner-Kirkwood series that is a gradient expansion. The Wigner-Kirkwood series is a semiclassical series with eigenvalues given exactly as for WKB approximation.
Examples.
Example of first order perturbation theory – ground state energy of the quartic oscillator.
Let us consider the quantum harmonic oscillator with the quartic potential perturbation and
the Hamiltonian
The ground state of the harmonic oscillator is
(formula_137) and the energy of unperturbed ground state is
Using the first order correction formula we get
or
Example of first and second order perturbation theory – quantum pendulum.
Consider the quantum mathematical pendulum with the Hamiltonian
with the potential energy formula_142 taken as the perturbation i.e.
The unperturbed normalized quantum wave functions are those of the rigid rotor and are given by
and the energies
The first order energy correction to the rotor due to the potential energy is
Using the formula for the second order correction one gets
or
or

</doc>
<doc id="25098" url="https://en.wikipedia.org/wiki?curid=25098" title="Phase velocity">
Phase velocity

The phase velocity of a wave is the rate at which the phase of the wave propagates in space. This is the velocity at which the phase of any one frequency component of the wave travels. For such a component, any given phase of the wave (for example, the crest) will appear to travel at the phase velocity. The phase velocity is given in terms of the wavelength "λ" (lambda) and period "T" as
Or, equivalently, in terms of the wave's angular frequency "ω", which specifies angular change per unit of time, and wavenumber (or angular wave number) "k", which represents the proportionality between the angular frequency ω and the linear speed (speed of propagation) νp:
To understand where this equation comes from, imagine a basic sine wave, "A" cos ("kx"−"ωt"). Given time "t", the source produces "ωt/2π = ft" oscillations. At the same time, the initial wave front propagates away from the source through the space to the distance "x" to fit the same amount of oscillations, "kx" = "ωt". So that the propagation velocity "v" is "v" = "x"/"t" = "ω"/"k". The wave propagates faster when higher frequency oscillations are distributed less densely in space. Formally, "Φ" = "kx"−"ωt" is the phase. Since "ω" = −d"Φ"/d"t" and "k" = +d"Φ"/d"x", the wave velocity is "v" = d"x"/d"t" = "ω"/"k".
Relation to group velocity, refractive index and transmission speed.
Since a pure sine wave cannot convey any information, some change in amplitude or frequency, known as modulation, is required. By combining two sines with slightly different frequencies and wavelengths,
the amplitude becomes a sinusoid with "phase" speed Δ"ω"/Δ"k". It is this modulation that represents the signal content. Since each amplitude "envelope" contains a group of internal waves, this speed is usually called the group velocity, "v"g.
In a given medium, the frequency is some function "ω"("k") of the wave number, so in general, the phase velocity "v"p = "ω"/"k" and the group velocity "v"g = d"ω"/d"k" depend on the frequency and on the medium. The ratio between the phase speed "v"p and the speed of light "c" is known as the refractive index, "n" = "c"/"v"p = "ck"/"ω". Taking the derivative of "ω" = "ck"/"n" with respect to "k", we recover the group speed,
Noting that "c"/"n" = "v"p, this shows that the group speed is equal to the phase speed only when the refractive index is a constant: d"n"/d"k" = 0, and in this case the phase speed and group speed are independent of frequency: "ω"/"k"=d"ω"/d"k"="c"/"n".
 Otherwise, both the phase velocity and the group velocity vary with frequency, and the medium is called dispersive; the relation "ω"="ω"("k") is known as the dispersion relation of the medium.
The phase velocity of electromagnetic radiation may – under certain circumstances (for example anomalous dispersion) – exceed the speed of light in a vacuum, but this does not indicate any superluminal information or energy transfer. It was theoretically described by physicists such as Arnold Sommerfeld and Léon Brillouin. See dispersion for a full discussion of wave velocities.

</doc>
<doc id="19889751" url="https://en.wikipedia.org/wiki?curid=19889751" title="Phi meson">
Phi meson

In particle physics, the phi meson is a vector meson formed of a strange quark and a strange antiquark. It has a mass of .

</doc>
<doc id="85754" url="https://en.wikipedia.org/wiki?curid=85754" title="Phonon">
Phonon

In physics, a phonon is a collective excitation in a periodic, elastic arrangement of atoms or molecules in condensed matter, like solids and some liquids. Often designated a quasiparticle, it represents an excited state in the quantum mechanical quantization of the modes of vibrations of elastic structures of interacting particles.
Phonons play a major role in many of the physical properties of condensed matter, like thermal conductivity and electrical conductivity. The study of phonons is an important part of condensed matter physics.
The concept of phonons was introduced in 1932 by Soviet physicist Igor Tamm. The name "phonon" comes from the Greek word "φωνή" (phonē), which translates to "sound" or "voice" because long-wavelength phonons give rise to sound.
Shorter-wavelength higher-frequency phonons give rise to heat.
Definition.
A phonon is a quantum mechanical description of an elementary vibrational motion in which a lattice of atoms or molecules uniformly oscillates at a single frequency. In classical mechanics this is designated a normal mode. Normal modes are important because any arbitrary lattice vibration can be considered to be a superposition of these "elementary" vibrations (cf. Fourier analysis). While normal modes are wave-like phenomena in classical mechanics, phonons have particle-like properties too, in a way related to the wave–particle duality of quantum mechanics.
Lattice dynamics.
The equations in this section either do not use axioms of quantum mechanics or use relations for which there exists a direct correspondence in classical mechanics.
For example, a rigid regular, crystalline, i.e. not amorphous, lattice is composed of "N" particles. These particles may be atoms, but they may be molecules as well. "N" is a large number, say ~1023, and on the order of Avogadro's number, for a typical sample of solid. If the lattice is rigid, the atoms must be exerting forces on one another to keep each atom near its equilibrium position. These forces may be Van der Waals forces, covalent bonds, electrostatic attractions, and others, all of which are ultimately due to the electric force. Magnetic and gravitational forces are generally negligible. The forces between each pair of atoms may be characterized by a potential energy function formula_1 that depends on the distance of separation of the atoms. The potential energy of the entire lattice is the sum of all pairwise potential energies:
where formula_3 is the position of the formula_4th atom, and formula_1 is the potential energy between two atoms.
It is difficult to solve this many-body problem in full generality, in either classical or quantum mechanics. In order to simplify the task, two important approximations are usually imposed. First, the sum is only performed over neighboring atoms. Although the electric forces in real solids extend to infinity, this approximation is nevertheless valid because the fields produced by distant atoms are effectively screened. Secondly, the potentials formula_1 are treated as harmonic potentials. This is permissible as long as the atoms remain close to their equilibrium positions. Formally, this is accomplished by Taylor expanding 
formula_1 about its equilibrium value to quadratic order, giving formula_1 proportional to the displacement formula_9 and the elastic force simply proportional to formula_10. The error in ignoring higher order terms remains small if formula_10 remains close to the equilibrium position.
The resulting lattice may be visualized as a system of balls connected by springs. The following figure shows a cubic lattice, which is a good model for many types of crystalline solid. Other lattices include a linear chain, which is a very simple lattice which we will shortly use for modeling phonons. Other common lattices may be found under "crystal structure".
The potential energy of the lattice may now be written as
Here, formula_13 is the natural frequency of the harmonic potentials, which are assumed to be the same since the lattice is regular. formula_14 is the position coordinate of the formula_4th atom, which we now measure from its equilibrium position. The sum over nearest neighbors is denoted as "(nn)".
Lattice waves.
Due to the connections between atoms, the displacement of one or more atoms from their equilibrium positions give rise to a set of vibration waves propagating through the lattice. One such wave is shown in the figure to the right. The amplitude of the wave is given by the displacements of the atoms from their equilibrium positions. The wavelength formula_16 is marked.
There is a minimum possible wavelength, given by twice the equilibrium separation "a" between atoms. Any wavelength shorter than this can be mapped onto a wavelength longer than 2"a", due to the periodicity of the lattice.
Not every possible lattice vibration has a well-defined wavelength and frequency. However, the normal modes do possess well-defined wavelengths and frequencies.
One-dimensional lattice.
In order to simplify the analysis needed for a 3-dimensional lattice of atoms it is convenient to model a 1-dimensional lattice or linear chain. This model is complex enough to display the salient features of phonons.
Classical treatment.
The forces between the atoms are assumed to be linear and nearest-neighbour, 
and they are represented by an elastic spring. Each atom is assumed to be a point particle and the nucleus and electrons move in step (adiabatic approximation).
formula_17o++++++o++++++o++++++o++++++o++++++o++++++o++++++o++++++o++++++oformula_17
Where formula_20 labels the formula_20-th atom, formula_22 is the distance between atoms when the chain is in equilibrium and formula_23 the displacement of the formula_20-th atom from its equilibrium position.
If formula_25 is the elastic constant of the spring and formula_26 the mass of the atom then the equation of motion of the formula_20-th atom is :
This is a set of coupled equations and since the solutions are expected to be oscillatory, new coordinates can be defined by a discrete Fourier transform, in order to de-couple them.
Put
Here formula_30 replaces the usual continuous variable formula_31. The formula_32 are known as the normal coordinates. Substitution into the equation of motion produces the following decoupled equations.(This requires a significant manipulation using the orthonormality and completeness relations of the discrete Fourier transform )
These are the equations for harmonic oscillators which have the solution:
Each normal coordinate formula_32 represents an independent vibrational mode of the lattice with wavenumber formula_36 which is known as a normal mode. The second equation for formula_37 is known as the dispersion relation between the angular frequency and the wavenumber.
Quantum treatment.
A one-dimensional quantum mechanical harmonic chain consists of "N" identical atoms. This is the simplest quantum mechanical model of a lattice that allows phonons to arise from it. The formalism for this model is readily generalizable to two and three dimensions.
As in the previous section, the positions of the masses are denoted by formula_38, as measured from their equilibrium positions (i.e. formula_39 if particle formula_40 is at its equilibrium position.) In two or more dimensions, the formula_41 are vector quantities. The Hamiltonian for this system is
where formula_43 is the mass of each atom (assuming is equal for all), and formula_44 and formula_45 are the position and momentum operators, respectively, for the formula_4th atom and the sum is made over the nearest neighbors (nn). However one expects that in a lattice there could also appear waves that behave like particles. It is customary to deal with waves in Fourier space which uses normal modes of the wavevector as variables instead coordinates of particles. The number of normal modes is same as the number of particles. However, the Fourier space is very useful given the periodicity of the system.
A set of formula_47 "normal coordinates" formula_48 may be introduced, defined as the discrete Fourier transforms of the formula_10's and formula_47 "conjugate momenta" formula_51 defined as the Fourier transforms of the formula_52's:
The quantity formula_55 turns out to be the wave number of the phonon, i.e. formula_56 divided by the wavelength.
This choice retains the desired commutation relations in either real space or wave vector space
From the general result
The potential energy term is
where
The couplings between the position variables have been transformed away; if the formula_61's and formula_62's were hermitian (which they are not), the transformed Hamiltonian would describe formula_47 "uncoupled" harmonic oscillators.
The form of the quantization depends on the choice of boundary conditions; for simplicity, "periodic" boundary conditions are imposed, defining the formula_64th atom as equivalent to the first atom. Physically, this corresponds to joining the chain at its ends. The resulting quantization is
The upper bound to formula_66 comes from the minimum wavelength, which is twice the lattice spacing formula_67, as discussed above.
The harmonic oscillator eigenvalues or energy levels for the mode formula_37 are :
The levels are evenly spaced at:
Where formula_71 is the zero-point energy of a quantum harmonic oscillator.
An exact amount of energy formula_72 must be supplied to the harmonic oscillator lattice to push it to the next energy level. In comparison to the photon case when the electromagnetic field is quantized, the quantum of vibrational energy is called a phonon.
All quantum systems show wave-like and particle-like properties simultaneously. The particle-like properties of the phonon are best understood using the methods of second quantization and operator techniques described later.
Three-dimensional lattice.
This may be generalized to a three-dimensional lattice. The wave number "k" is replaced by a three-dimensional wave vector k. Furthermore, each k is now associated with three normal coordinates.
The new indices "s = 1, 2, 3" label the polarization of the phonons. In the one-dimensional model, the atoms were restricted to moving along the line, so the phonons corresponded to longitudinal waves. In three dimensions, vibration is not restricted to the direction of propagation, and can also occur in the perpendicular planes, like transverse waves. This gives rise to the additional normal coordinates, which, as the form of the Hamiltonian indicates, we may view as independent species of phonons.
Dispersion relation.
For a one-dimensional alternating array of two types of ion or atom of mass "m1, m2" repeated periodically at a distance "a", connected by springs of spring constant "K", two modes of vibration result:
where "k" is the wave-vector of the vibration related to its wavelength by "k"=2π/λ.
The connection between frequency and wave-vector, ω=ω("k"), is known as a dispersion relation. The plus sign results in the so-called "optical" mode, and the minus sign to the "acoustic" mode. In the optical mode two adjacent different atoms move against each other, while in the acoustic mode they move together.
The speed of propagation of an acoustic phonon, which is also the speed of sound in the lattice, is given by the slope of the acoustic dispersion relation, formula_74 (see group velocity.) At low values of formula_75 (i.e. long wavelengths), the dispersion relation is almost linear, and the speed of sound is approximately formula_76, independent of the phonon frequency. As a result, packets of phonons with different (but long) wavelengths can propagate for large distances across the lattice without breaking apart. This is the reason that sound propagates through solids without significant distortion. This behavior fails at large values of formula_75, i.e. short wavelengths, due to the microscopic details of the lattice.
For a crystal that has at least two atoms in its primitive cell, the dispersion relations exhibit two types of phonons, namely, optical and acoustic modes corresponding to the upper blue and lower red of curve in the diagram, respectively. The vertical axis is the energy or frequency of phonon, while the horizontal axis is the wave-vector. The boundaries at -π/a and π/a are those of the first Brillouin zone. A crystal with "N" ≥ 2 different atoms in the primitive cell exhibits three acoustic modes: one longitudinal acoustic mode and two transverse acoustic modes. The number of optical modes is 3"N" – 3. The lower figure shows the dispersion relations for several phonon modes in GaAs as a function of wavevector k in the principal directions of its Brillouin zone.
Many phonon dispersion curves have been measured by neutron scattering.
The physics of sound in fluids differs from the physics of sound in solids, although both are density waves: sound waves in fluids only have longitudinal components, whereas sound waves in solids have longitudinal and transverse components. This is because fluids can't support shear stresses (but see viscoelastic fluids, which only apply to high frequencies, though).
Interpretation of phonons using second quantization techniques.
In fact, the above-derived Hamiltonian looks like the classical Hamiltonian function, but if it is interpreted as an operator, then it describes a quantum field theory of non-interacting bosons.
The energy spectrum of this Hamiltonian is easily obtained by the method of ladder operators, similar to the quantum harmonic oscillator problem. We introduce a set of ladder operators defined by:
By direct insertion on the Hamiltonian, it is readily verified that
As with the quantum harmonic oscillator, one can show that formula_83 and formula_84 respectively create and destroy one excitation of energy formula_85. These excitations are phonons.
Two important properties of phonons may be deduced. Firstly, phonons are bosons, since any number of identical excitations can be created by repeated application of the creation operator formula_83. Secondly, each phonon is a "collective mode" caused by the motion of every atom in the lattice. This may be seen from the fact that the ladder operators contain sums over the position and momentum operators of every atom.
It is not "a priori" obvious that these excitations generated by the formula_87 operators are literally waves of lattice displacement, but one may convince oneself of this by calculating the "position-position correlation function". Let formula_88 denote a state with a single quantum of mode formula_75 excited, i.e.
One can show that, for any two atoms formula_91 and formula_92,
which has the form of a lattice wave with frequency formula_94 and wave number formula_75.
In three dimensions the Hamiltonian has the form
Acoustic and optical phonons.
Solids with more than one atom in the smallest unit cell, exhibit two types of phonons: acoustic phonons and optical phonons.
Acoustic phonons are coherent movements of atoms of the lattice out of their equilibrium positions. If the displacement is in the direction of propagation, then in some areas the atoms will be closer, in others farther apart, as in a sound wave in air (hence the name acoustic). Displacement perpendicular to the propagation direction is comparable to waves in water. If the wavelength of acoustic phonons goes to infinity, this corresponds to a simple displacement of the whole crystal, and this costs zero energy. Acoustic phonons exhibit a linear relationship between frequency and phonon wavevector for long wavelengths. The frequencies of acoustic phonons tend to zero with longer wavelength. Longitudinal and transverse acoustic phonons are often abbreviated as LA and TA phonons, respectively.
Optical phonons are out-of-phase movement of the atoms in the lattice, one atom moving to the left, and its neighbour to the right. This occurs if the lattice basis consists of two or more atoms. They are called "optical" because in ionic crystals, like sodium chloride, they are excited by infrared radiation. The electric field of the light will move every positive sodium ion in the direction of the field, and every negative chloride ion in the other direction, sending the crystal vibrating. 
Optical phonons have a non-zero frequency at the Brillouin zone center and show no dispersion near that long wavelength limit. This is because they correspond to a mode of vibration where positive and negative ions at adjacent lattice sites swing against each other, creating a time-varying electrical dipole moment. Optical phonons that interact in this way with light are called "infrared active". Optical phonons that are "Raman active" can also interact indirectly with light, through Raman scattering. Optical phonons are often abbreviated as LO and TO phonons, for the longitudinal and transverse modes respectively; the splitting between LO and TO frequencies is often described accurately by the Lyddane–Sachs–Teller relation.
When measuring optical phonon energy by experiment, optical phonon frequencies are sometimes given in spectroscopic wavenumber notation, where the symbol formula_97 represents ordinary frequency (not angular frequency), and is expressed in units of cm−1. The value is obtained by dividing the frequency by the speed of light in vacuum. In other words, the frequency in cm−1 units corresponds to the inverse of the wavelength of a photon in vacuum, that has the same frequency as the measured phonon. The cm−1 is a unit of energy used frequently in the dispersion relations of both acoustic and optical phonons, see units of energy for more details and uses.
Crystal momentum.
By analogy to photons and matter waves, phonons have been treated with wave vector formula_75 as though it has a momentum formula_99, however, this is not strictly correct, because formula_99 is not actually a physical momentum; it is called the "crystal momentum" or "pseudomomentum". This is because formula_75 is only determined up to addition of constant vectors (the reciprocal lattice vectors and integer multiples thereof). For example, in the one-dimensional model, the normal coordinates formula_61 and formula_62 are defined so that
where
for any integer formula_66. A phonon with wave number formula_75 is thus equivalent to an infinite "family" of phonons with wave numbers formula_108, formula_109, and so forth. Physically, the reciprocal lattice vectors act as additional "chunks" of momentum which the lattice can impart to the phonon. Bloch electrons obey a similar set of restrictions.
It is usually convenient to consider phonon wave vectors formula_75 which have the smallest magnitude formula_111 in their "family". The set of all such wave vectors defines the "first Brillouin zone". Additional Brillouin zones may be defined as copies of the first zone, shifted by some reciprocal lattice vector.
Thermodynamics.
The thermodynamic properties of a solid are directly related to its phonon structure. The entire set of all possible phonons that are described by the above phonon dispersion relations combine in what is known as the phonon density of states which determines the heat capacity of a crystal.
At absolute zero temperature, a crystal lattice lies in its ground state, and contains no phonons. A lattice at a non-zero temperature has an energy that is not constant, but fluctuates randomly about some mean value. These energy fluctuations are caused by random lattice vibrations, which can be viewed as a gas of phonons. (The random motion of the atoms in the lattice is what we usually think of as heat.) Because these phonons are generated by the temperature of the lattice, they are sometimes designated thermal phonons.
Unlike the atoms which make up an ordinary gas, thermal phonons can be created and destroyed by random energy fluctuations. In the language of statistical mechanics this means that the chemical potential for adding a phonon is zero. This behavior is an extension of the harmonic potential, mentioned earlier, into the anharmonic regime. The behavior of thermal phonons is similar to the photon gas produced by an electromagnetic cavity, wherein photons may be emitted or absorbed by the cavity walls. This similarity is not coincidental, for it turns out that the electromagnetic field behaves like a set of harmonic oscillators; see Black-body radiation. Both gases obey the Bose–Einstein statistics: in thermal equilibrium and within the harmonic regime, the probability of finding phonons (or photons) in a given state with a given angular frequency is:
where formula_113 is the frequency of the phonons (or photons) in the state, formula_114 is Boltzmann's constant, and formula_115 is the temperature.
Operator formalism.
The phonon Hamiltonian is given by
In terms of the operators, these are given by
Here, in expressing the Hamiltonian in operator formalism, we have not taken into account the formula_118 term, since if we take an infinite lattice or, for that matter a continuum, the formula_119 terms will add up giving an infinity. Hence, it is "renormalized" by putting the factor of formula_119 to 0 arguing that the difference in energy is what we measure and not the absolute value of it. Hence, the formula_119 factor is absent in the operator formalised expression for the Hamiltonian.<br>
The ground state also called the "vacuum state" is the state composed of no phonons. Hence, the energy of the ground state is 0. When, a system is in state formula_122, we say there are formula_123 phonons of type formula_124. The formula_123 are called the occupation number of the phonons. Energy of a single phonon of type formula_124 being formula_127, the total energy of a general phonon system is given by formula_128. In other words, the phonons are non-interacting. The action of creation and annihilation operators are given by
and,
i.e. formula_131 creates a phonon of type formula_124 while formula_133 annihilates. Hence, they are respectively the creation and annihilation operator for phonons. Analogous to the Quantum harmonic oscillator case, we can define particle number operator as formula_134. The number operator commutes with a string of products of the creation and annihilation operators if, the number of formula_135's are equal to number of formula_136's.<br>
Phonons are bosons since, formula_137 i.e. they are symmetric under exchange.
Nonlinearity.
As well as photons, phonons can interact via parametric down conversion and form squeezed coherent states.
Phononic computing.
As phonons carry information, it is theoretically possible to build a quantum computer using phonons.

</doc>
<doc id="23579" url="https://en.wikipedia.org/wiki?curid=23579" title="Photoelectric effect">
Photoelectric effect

The photoelectric effect is the observation that many metals emit electrons when light shines upon them. Electrons emitted in this manner can be called "photoelectrons". The phenomenon is commonly studied in electronic physics, as well as in fields of chemistry, such as quantum chemistry or electrochemistry.
According to classical electromagnetic theory, this effect can be attributed to the transfer of energy from the light to an electron in the metal. From this perspective, an alteration in either the intensity or wavelength of light would induce changes in the rate of emission of electrons from the metal. Furthermore, according to this theory, a sufficiently dim light would be expected to show a time lag between the initial shining of its light and the subsequent emission of an electron. However, the experimental results did not correlate with either of the two predictions made by classical theory.
Instead, electrons are only dislodged by the impingement of photons when those photons reach or exceed a threshold frequency. Below that threshold, no electrons are emitted from the metal regardless of the light intensity or the length of time of exposure to the light. To make sense of the fact that light can eject electrons even if its intensity is low, Albert Einstein proposed that a beam of light is not a wave propagating through space, but rather a collection of discrete wave packets (photons), each with energy "hf". This shed light on Max Planck's previous discovery of the Planck relation () linking energy ("E") and frequency ("f") as arising from quantization of energy. The factor "h" is known as the Planck constant.
In 1887, Heinrich Hertz discovered that electrodes illuminated with ultraviolet light create electric sparks more easily. In 1905 Albert Einstein published a paper that explained experimental data from the photoelectric effect as the result of light energy being carried in discrete quantized packets. This discovery led to the quantum revolution. In 1914, Robert Millikan's experiment confirmed Einstein's law on photoelectric effect. Einstein was awarded the Nobel Prize in 1921 for "his discovery of the law of the photoelectric effect", and Millikan was awarded the Nobel Prize in 1923 for "his work on the elementary charge of electricity and on the photoelectric effect".
The photoelectric effect requires photons with energies from a few electronvolts to over 1 MeV in elements with a high atomic number. Study of the photoelectric effect led to important steps in understanding the quantum nature of light and electrons and influenced the formation of the concept of wave–particle duality. Other phenomena where light affects the movement of electric charges include the photoconductive effect (also known as photoconductivity or photoresistivity), the photovoltaic effect, and the photoelectrochemical effect.
Emission mechanism.
The photons of a light beam have a characteristic energy proportional to the frequency of the light. In the photoemission process, if an electron within some material absorbs the energy of one photon and acquires more energy than the work function (the electron binding energy) of the material, it is ejected. If the photon energy is too low, the electron is unable to escape the material. Since an increase in the intensity of low-frequency light will only increase the number of low-energy photons sent over a given interval of time, this change in intensity will not create any single photon with enough energy to dislodge an electron. Thus, the energy of the emitted electrons does not depend on the intensity of the incoming light, but only on the energy (equivalently frequency) of the individual photons. It is an interaction between the incident photon and the outermost electrons.
Electrons can absorb energy from photons when irradiated, but they usually follow an "all or nothing" principle. All of the energy from one photon must be absorbed and used to liberate one electron from atomic binding, or else the energy is re-emitted. If the photon energy is absorbed, some of the energy liberates the electron from the atom, and the rest contributes to the electron's kinetic energy as a free particle.
Experimental observations of photoelectric emission.
The theory of the photoelectric effect must explain the experimental observations of the emission of electrons from an illuminated metal surface.
For a given metal, there exists a certain minimum frequency of incident radiation below which no photoelectrons are emitted. This frequency is called the "threshold frequency". Increasing the frequency of the incident beam, keeping the number of incident photons fixed (this would result in a proportionate increase in energy) increases the maximum kinetic energy of the photoelectrons emitted. Thus the stopping voltage increases. The number of electrons also changes because the probability that each photon results in an emitted electron is a function of photon energy. If the intensity of the incident radiation of a given frequency is increased, there is no effect on the kinetic energy of each photoelectron.
Above the threshold frequency, the maximum kinetic energy of the emitted photoelectron depends on the frequency of the incident light, but is independent of the intensity of the incident light so long as the latter is not too high.
For a given metal and frequency of incident radiation, the rate at which photoelectrons are ejected is directly proportional to the intensity of the incident light. An increase in the intensity of the incident beam (keeping the frequency fixed) increases the magnitude of the photoelectric current, although the stopping voltage remains the same.
The time lag between the incidence of radiation and the emission of a photoelectron is very small, less than 10−9 second.
The direction of distribution of emitted electrons peaks in the direction of polarization (the direction of the electric field) of the incident light, if it is linearly polarized.
Mathematical description.
The maximum kinetic energy formula_1 of an ejected electron is given by
formula_2
where formula_3 is the Planck constant and formula_4 is the frequency of the incident photon. The term formula_5 is the work function (sometimes denoted formula_6, or formula_7), which gives the minimum energy required to remove a delocalised electron from the surface of the metal. The work function satisfies
formula_8
where formula_9 is the threshold frequency for the metal. The maximum kinetic energy of an ejected electron is then
formula_10
Kinetic energy is positive, so we must have formula_11 for the photoelectric effect to occur.
Stopping potential.
The relation between current and applied voltage illustrates the nature of the photoelectric effect. For discussion, a light source illuminates a plate P, and another plate electrode Q collects any emitted electrons. We vary the potential between P and Q and measure the current flowing in the external circuit between the two plates.
If the frequency and the intensity of the incident radiation are fixed, the photoelectric current increases gradually with an increase in the positive potential on the collector electrode until all the photoelectrons emitted are collected. The photoelectric current attains a saturation value and does not increase further for any increase in the positive potential. The saturation current increases with the increase of the light intensity. It also increases with greater frequencies due to a greater probability of electron emission when collisions happen with higher energy photons.
If we apply a negative potential to the collector plate Q with respect to the plate P and gradually increase it, the photoelectric current decreases, becoming zero at a certain negative potential. The negative potential on the collector at which the photoelectric current becomes zero is called the "stopping potential" or "cut off" potential
i. For a given frequency of incident radiation, the stopping potential is independent of its intensity.
ii. For a given frequency of incident radiation, the stopping potential is determined by the maximum kinetic energy formula_1of the photoelectrons that are emitted. If "qe" is the charge on the electron and formula_13 is the stopping potential, then the work done by the retarding potential in stopping the electron is formula_14, so we have
formula_15
Recalling
formula_16
we see that the stopping voltage varies linearly with frequency of light, but depends on the type of material. For any particular material, there is a threshold frequency that must be exceeded, independent of light intensity, to observe any electron emission.
Three-step model.
In the X-ray regime, the photoelectric effect in crystalline material is often decomposed into three steps:
In the three-step model, an electron can take multiple paths through these three steps. All paths can interfere in the sense of the path integral formulation.
For surface states and molecules the three-step model does still make some sense as even most atoms have multiple electrons which can scatter the one electron leaving.
History.
When a surface is exposed to electromagnetic radiation above a certain threshold frequency (typically visible light for alkali metals, near ultraviolet for other metals, and extreme ultraviolet for non-metals), the radiation is absorbed and electrons are emitted.
Light, and especially ultra-violet light, discharges negatively electrified bodies with the production of rays of the same nature as cathode rays. Under certain circumstances it can directly ionize gases. The first of these phenomena was discovered by Hertz and Hallwachs in 1887. The second was announced first by Philipp Lenard in 1900.
The ultra-violet light to produce these effects may be obtained from an arc lamp, or by burning magnesium, or by sparking with an induction coil between zinc or cadmium terminals, the light from which is very rich in ultra-violet rays. Sunlight is not rich in ultra-violet rays, as these have been absorbed by the atmosphere, and it does not produce nearly so large an effect as the arc-light. Many substances besides metals discharge negative electricity under the action of ultraviolet light: lists of these substances will be found in papers by G. C. Schmidt and O. Knoblauch.
19th century.
In 1839, Alexandre Edmond Becquerel discovered the photovoltaic effect while studying the effect of light on electrolytic cells. Though not equivalent to the photoelectric effect, his work on photovoltaics was instrumental in showing a strong relationship between light and electronic properties of materials. In 1873, Willoughby Smith discovered photoconductivity in selenium while testing the metal for its high resistance properties in conjunction with his work involving submarine telegraph cables.
Johann Elster (1854–1920) and Hans Geitel (1855–1923), students in Heidelberg, developed the first practical photoelectric cells that could be used to measure the intensity of light. Elster and Geitel had investigated with great success the effects produced by light on electrified bodies.
In 1887, Heinrich Hertz observed the photoelectric effect and the production and reception of electromagnetic waves. He published these observations in the journal Annalen der Physik. His receiver consisted of a coil with a spark gap, where a spark would be seen upon detection of electromagnetic waves. He placed the apparatus in a darkened box to see the spark better. However, he noticed that the maximum spark length was reduced when in the box. A glass panel placed between the source of electromagnetic waves and the receiver absorbed ultraviolet radiation that assisted the electrons in jumping across the gap. When removed, the spark length would increase. He observed no decrease in spark length when he replaced glass with quartz, as quartz does not absorb UV radiation. Hertz concluded his months of investigation and reported the results obtained. He did not further pursue investigation of this effect.
The discovery by Hertz in 1887 that the incidence of ultra-violet light on a spark gap facilitated the passage of the spark, led immediately to a series of investigations by Hallwachs, Hoor, Righi and Stoletow. on the effect of light, and especially of ultra-violet light, on charged bodies. It was proved by these investigations that a newly cleaned surface of zinc, if charged with negative electricity, rapidly loses this charge however small it may be when ultra-violet light falls upon the surface; while if the surface is uncharged to begin with, it acquires a positive charge when exposed to the light, the negative electrification going out into the gas by which the metal is surrounded; this positive electrification can be much increased by directing a strong airblast against the surface. If however the zinc surface is positively electrified it suffers no loss of charge when exposed to the light: this result has been questioned, but a very careful examination of the phenomenon by Elster and Geitel has shown that the loss observed under certain circumstances is due to the discharge by the light reflected from the zinc surface of negative electrification on neighbouring conductors induced by the positive charge, the negative electricity under the influence of the electric field moving up to the positively electrified surface.
With regard to the "Hertz effect", the researches from the start showed a great complexity of the phenomenon of photoelectric fatigue — that is, the progressive diminution of the effect observed upon fresh metallic surfaces. According to an important research by Wilhelm Hallwachs, ozone played an important part in the phenomenon. However, other elements enter such as oxidation, the humidity, the mode of polish of the surface, etc. It was at the time not even sure that the fatigue is absent in a vacuum. 
In the period from February 1888 and until 1891, a detailed analysis of photoeffect was performed by Aleksandr Stoletov with results published in 6 works; four of them in "Comptes Rendus", one review in "Physikalische Revue" (translated from Russian), and the last work in "Journal de Physique". First, in these works Stoletov invented a new experimental setup which was more suitable for a quantitative analysis of photoeffect. Using this setup, he discovered the direct proportionality between the intensity of light and the induced photo electric current (the first law of photoeffect or Stoletov's law). One of his other findings resulted from measurements of the dependence of the intensity of the electric photo current on the gas pressure, where he found the existence of an optimal gas pressure Pm corresponding to a maximum photocurrent; this property was used for a creation of solar cells.
In 1899, J. J. Thomson investigated ultraviolet light in Crookes tubes. Thomson deduced that the ejected particles were the same as those previously found in the cathode ray, later called electrons, which he called "corpuscles". In the research, Thomson enclosed a metal plate (a cathode) in a vacuum tube, and exposed it to high frequency radiation. It was thought that the oscillating electromagnetic fields caused the atoms' field to resonate and, after reaching a certain amplitude, caused a subatomic "corpuscle" to be emitted, and current to be detected. The amount of this current varied with the intensity and colour of the radiation. Larger radiation intensity or frequency would produce more current.
20th century.
The discovery of the ionization of gases by ultra-violet light was made by Philipp Lenard in 1900. As the effect was produced across several centimeters of air and made very great positive and small negative ions, it was natural to interpret the phenomenon, as did J. J. Thomson, as a "Hertz effect" upon the solid or liquid particles present in the gas.
In 1902, Lenard observed that the energy of individual emitted electrons increased with the frequency (which is related to the color) of the light.
This appeared to be at odds with Maxwell's wave theory of light, which predicted that the electron energy would be proportional to the intensity of the radiation.
Lenard observed the variation in electron energy with light frequency using a powerful electric arc lamp which enabled him to investigate large changes in intensity, and that had sufficient power to enable him to investigate the variation of potential with light frequency. His experiment directly measured potentials, not electron kinetic energy: he found the electron energy by relating it to the maximum stopping potential (voltage) in a phototube. He found that the calculated maximum electron kinetic energy is determined by the frequency of the light. For example, an increase in frequency results in an increase in the maximum kinetic energy calculated for an electron upon liberation – ultraviolet radiation would require a higher applied stopping potential to stop current in a phototube than blue light. However Lenard's results were qualitative rather than quantitative because of the difficulty in performing the experiments: the experiments needed to be done on freshly cut metal so that the pure metal was observed, but it oxidised in a matter of minutes even in the partial vacuums he used. The current emitted by the surface was determined by the light's intensity, or brightness: doubling the intensity of the light doubled the number of electrons emitted from the surface.
The researches of Langevin and those of Eugene Bloch have shown that the greater part of the Lenard effect is certainly due to this 'Hertz effect'. The Lenard effect upon the gas itself nevertheless does exist. Refound by J. J. Thomson and then more decisively by Frederic Palmer, Jr., it was studied and showed very different characteristics than those at first attributed to it by Lenard.
In 1905, Albert Einstein solved this apparent paradox by describing light as composed of discrete quanta, now called photons, rather than continuous waves. Based upon Max Planck's theory of black-body radiation, Einstein theorized that the energy in each quantum of light was equal to the frequency multiplied by a constant, later called Planck's constant. A photon above a threshold frequency has the required energy to eject a single electron, creating the observed effect. This discovery led to the quantum revolution in physics and earned Einstein the Nobel Prize in Physics in 1921. By wave-particle duality the effect can be analyzed purely in terms of waves though not as conveniently.
Albert Einstein's mathematical description of how the photoelectric effect was caused by absorption of quanta of light was in one of his 1905 papers, named "On a Heuristic Viewpoint Concerning the Production and Transformation of Light". This paper proposed the simple description of "light quanta", or photons, and showed how they explained such phenomena as the photoelectric effect. His simple explanation in terms of absorption of discrete quanta of light explained the features of the phenomenon and the characteristic frequency.
The idea of light quanta began with Max Planck's published law of black-body radiation ("On the Law of Distribution of Energy in the Normal Spectrum") by assuming that Hertzian oscillators could only exist at energies "E" proportional to the frequency "f" of the oscillator by "E" = "hf", where "h" is Planck's constant. By assuming that light actually consisted of discrete energy packets, Einstein wrote an equation for the photoelectric effect that agreed with experimental results. It explained why the energy of photoelectrons was dependent only on the "frequency" of the incident light and not on its "intensity": a low-intensity, high-frequency source could supply a few high energy photons, whereas a high-intensity, low-frequency source would supply no photons of sufficient individual energy to dislodge any electrons. This was an enormous theoretical leap, but the concept was strongly resisted at first because it contradicted the wave theory of light that followed naturally from James Clerk Maxwell's equations for electromagnetic behavior, and more generally, the assumption of infinite divisibility of energy in physical systems. Even after experiments showed that Einstein's equations for the photoelectric effect were accurate, resistance to the idea of photons continued, since it appeared to contradict Maxwell's equations, which were well-understood and verified.
Einstein's work predicted that the energy of individual ejected electrons increases linearly with the frequency of the light. Perhaps surprisingly, the precise relationship had not at that time been tested. By 1905 it was known that the energy of photoelectrons increases with increasing "frequency" of incident light and is independent of the "intensity" of the light. However, the manner of the increase was not experimentally determined until 1914 when Robert Andrews Millikan showed that Einstein's prediction was correct.
The photoelectric effect helped to propel the then-emerging concept of wave–particle duality in the nature of light. Light simultaneously possesses the characteristics of both waves and particles, each being manifested according to the circumstances. The effect was impossible to understand in terms of the classical wave description of light, as the energy of the emitted electrons did not depend on the intensity of the incident radiation. Classical theory predicted that the electrons would 'gather up' energy over a period of time, and then be emitted.
Uses and effects.
Photomultipliers.
These are extremely light-sensitive vacuum tubes with a photocathode coated onto part (an end or side) of the inside of the envelope. The photocathode contains combinations of materials such as caesium, rubidium and antimony specially selected to provide a low work function, so when illuminated even by very low levels of light, the photocathode readily releases electrons. By means of a series of electrodes (dynodes) at ever-higher potentials, these electrons are accelerated and substantially increased in number through secondary emission to provide a readily detectable output current. Photomultipliers are still commonly used wherever low levels of light must be detected.
Image sensors.
Video camera tubes in the early days of television used the photoelectric effect, for example, Philo Farnsworth's "Image dissector" used a screen charged by the photoelectric effect to transform an optical image into a scanned electronic signal.
Gold-leaf electroscope.
Gold-leaf electroscopes are designed to detect static electricity. Charge placed on the metal cap spreads to the stem and the gold leaf of the electroscope. Because they then have the same charge, the stem and leaf repel each other. This will cause the leaf to bend away from the stem.
The electroscope is an important tool in illustrating the photoelectric effect. For example, if the electroscope is negatively charged throughout, there is an excess of electrons and the leaf is separated from the stem. If high-frequency light shines on the cap, the electroscope discharges and the leaf will fall limp. This is because the frequency of the light shining on the cap is above the cap's threshold frequency. The photons in the light have enough energy to liberate electrons from the cap, reducing its negative charge. This will discharge a negatively charged electroscope and further charge a positive electroscope. However, if the electromagnetic radiation hitting the metal cap does not have a high enough frequency (its frequency is below the threshold value for the cap), then the leaf will never discharge, no matter how long one shines the low-frequency light at the cap.
Photoelectron spectroscopy.
Since the energy of the photoelectrons emitted is exactly the energy of the incident photon minus the material's work function or binding energy, the work function of a sample can be determined by bombarding it with a monochromatic X-ray source or UV source, and measuring the kinetic energy distribution of the electrons emitted.
Photoelectron spectroscopy is usually done in a high-vacuum environment, since the electrons would be scattered by gas molecules if they were present. However, some companies are now selling products that allow photoemission in air. The light source can be a laser, a discharge tube, or a synchrotron radiation source.
The concentric hemispherical analyser (CHA) is a typical electron energy analyzer, and uses an electric field to change the directions of incident electrons, depending on their kinetic energies. For every element and core (atomic orbital) there will be a different binding energy. The many electrons created from each of these combinations will show up as spikes in the analyzer output, and these can be used to determine the elemental composition of the sample.
Spacecraft.
The photoelectric effect will cause spacecraft exposed to sunlight to develop a positive charge. This can be a major problem, as other parts of the spacecraft in shadow develop a negative charge from nearby plasma, and the imbalance can discharge through delicate electrical components. The static charge created by the photoelectric effect is self-limiting, though, because a more highly charged object gives up its electrons less easily.
Moon dust.
Light from the sun hitting lunar dust causes it to become charged through the photoelectric effect. The charged dust then repels itself and lifts off the surface of the Moon by electrostatic levitation. This manifests itself almost like an "atmosphere of dust", visible as a thin haze and blurring of distant features, and visible as a dim glow after the sun has set. This was first photographed by the Surveyor program probes in the 1960s. It is thought that the smallest particles are repelled up to kilometers high, and that the particles move in "fountains" as they charge and discharge.
Night vision devices.
Photons hitting a thin film of alkali metal or semiconductor material such as gallium arsenide in an image intensifier tube cause the ejection of photoelectrons due to the photoelectric effect. These are accelerated by an electrostatic field where they strike a phosphor coated screen, converting the electrons back into photons. Intensification of the signal is achieved either through acceleration of the electrons or by increasing the number of electrons through secondary emissions, such as with a micro-channel plate. Sometimes a combination of both methods is used. Additional kinetic energy is required to move an electron out of the conduction band and into the vacuum level. This is known as the electron affinity of the photocathode and is another barrier to photoemission other than the forbidden band, explained by the band gap model. Some materials such as Gallium Arsenide have an effective electron affinity that is below the level of the conduction band. In these materials, electrons that move to the conduction band are all of sufficient energy to be emitted from the material and as such, the film that absorbs photons can be quite thick. These materials are known as negative electron affinity materials.
Cross section.
The photoelectric effect is one interaction mechanism between photons and atoms. It is one of 12 theoretically possible interactions.
At the high photon energies comparable to the electron rest energy of , Compton scattering, another process, may take place. Above twice this () pair production may take place. Compton scattering and pair production are examples of two other competing mechanisms.
Indeed, even if the photoelectric effect is the favoured reaction for a particular single-photon bound-electron interaction, the result is also subject to statistical processes and is not guaranteed, albeit the photon has certainly disappeared and a bound electron has been excited (usually K or L shell electrons at gamma ray energies). The probability of the photoelectric effect occurring is measured by the cross section of interaction, σ. This has been found to be a function of the atomic number of the target atom and photon energy. A crude approximation, for photon energies above the highest atomic binding energy, is given by:
Here "Z" is atomic number and "n" is a number which varies between 4 and 5. (At lower photon energies a characteristic structure with edges appears, K edge, L edges, M edges, etc.) The obvious interpretation follows that the photoelectric effect rapidly decreases in significance, in the gamma ray region of the spectrum, with increasing photon energy, and that photoelectric effect increases steeply with atomic number. The corollary is that high-"Z" materials make good gamma-ray shields, which is the principal reason that lead ("Z" = 82) is a preferred and ubiquitous gamma radiation shield.
See also.
"Electronics:"
"Physics:"
"Chemistry:"
"Lists":
External links.
"Applets"

</doc>
<doc id="23535" url="https://en.wikipedia.org/wiki?curid=23535" title="Photon">
Photon

A photon is an elementary particle, the quantum of light and all other forms of electromagnetic radiation. It is the force carrier for the electromagnetic force, even when static via virtual photons. The effects of this force are easily observable at the microscopic and at the macroscopic level, because the photon has zero rest mass; this allows long distance interactions. Like all elementary particles, photons are currently best explained by quantum mechanics and exhibit wave–particle duality, exhibiting properties of waves and of particles. For example, a single photon may be refracted by a lens or exhibit wave interference with itself, but also act as a particle giving a definite result when its position is measured. Waves and quanta, being two observable aspects of a single phenomenon cannot have their true nature described in terms of any mechanical model. A representation of this dual property of light, which assumes certain points on the wave front to be the seat of the energy is also impossible. Thus, the quanta in a light wave cannot be spatially localized. Some defined physical parameters of a photon are listed.
The modern photon concept was developed gradually by Albert Einstein in the first years of the 20th century to explain experimental observations that did not fit the classical wave model of light. In particular, the photon model accounted for the frequency dependence of light's energy, and explained the ability of matter and radiation to be in thermal equilibrium. It also accounted for anomalous observations, including the properties of black-body radiation, that other physicists, most notably Max Planck, had sought to explain using "semiclassical models", in which light is still described by Maxwell's equations, but the material objects that emit and absorb light do so in amounts of energy that are "quantized" (i.e., they change energy only by certain particular discrete amounts and cannot change energy in any arbitrary way). Although these semiclassical models contributed to the development of quantum mechanics, many further experiments starting with Compton scattering of single photons by electrons, first observed in 1923, validated Einstein's hypothesis that "light itself" is quantized. In 1926 the optical physicist Frithiof Wolfers and the chemist Gilbert N. Lewis coined the name "photon" for these particles, and after 1927, when Arthur H. Compton won the Nobel Prize for his scattering studies, most scientists accepted the validity that quanta of light have an independent existence, and the term "photon" for light quanta was accepted.
In the Standard Model of particle physics, photons and other elementary particles are described as a necessary consequence of physical laws having a certain symmetry at every point in spacetime. The intrinsic properties of particles, such as charge, mass and spin, are determined by the properties of this gauge symmetry.
The photon concept has led to momentous advances in experimental and theoretical physics, such as lasers, Bose–Einstein condensation, quantum field theory, and the probabilistic interpretation of quantum mechanics. It has been applied to photochemistry, high-resolution microscopy, and measurements of molecular distances. Recently, photons have been studied as elements of quantum computers and for applications in optical imaging and optical communication such as quantum cryptography.
Nomenclature.
In 1900, the German physicist Max Planck was working on black-body radiation and suggested that the energy in electromagnetic waves could only be released in "packets" of energy. In his 1901 article in Annalen der Physik he called these packets "energy elements". The word "quanta" (singular "quantum") was used even before 1900 to mean particles or amounts of different quantities, including electricity. Later, in 1905, Albert Einstein went further by suggesting that electromagnetic waves could only exist in these discrete wave-packets. He called such a wave-packet "the light quantum" (German: "das Lichtquant"). The name "photon" derives from the Greek word for light, "" (transliterated "phôs"). Arthur Compton used "photon" in 1928, referring to Gilbert N. Lewis. The same name was used earlier, by the American physicist and psychologist Leonard T. Troland, who coined the word in 1916, in 1921 by the Irish physicist John Joly, in 1924 by the French physiologist René Wurmser (1890-1993) and in 1926 by the French physicist Frithiof Wolfers (1891-1971). The name was suggested initially as a unit related to the illumination of the eye and the resulting sensation of light and was used later on in a physiological context. Although Wolfers's and Lewis's theories were never accepted, as they were contradicted by many experiments, the new name was adopted very soon by most physicists after Compton used it.
In physics, a photon is usually denoted by the symbol "γ" (the Greek letter gamma). This symbol for the photon probably derives from gamma rays, which were discovered in 1900 by Paul Villard, named by Ernest Rutherford in 1903, and shown to be a form of electromagnetic radiation in 1914 by Rutherford and Edward Andrade. In chemistry and optical engineering, photons are usually symbolized by "hν", the energy of a photon, where "h" is Planck's constant and the Greek letter "ν" (nu) is the photon's frequency. Much less commonly, the photon can be symbolized by "hf", where its frequency is denoted by "f".
Physical properties.
A photon is massless, has no electric charge, and is stable. A photon has two possible polarization states. In the momentum representation, which is preferred in quantum field theory, a photon is described by its wave vector, which determines its wavelength "λ" and its direction of propagation. A photon's wave vector may not be zero and can be represented either as a spatial 3-vector or as a (relativistic) four-vector; in the latter case it belongs to the light cone (pictured). Different signs of the four-vector denote different circular polarizations, but in the 3-vector representation one should account for the polarization state separately; it actually is a spin quantum number. In both cases the space of possible wave vectors is three-dimensional.
The photon is the gauge boson for electromagnetism, and therefore all other quantum numbers of the photon (such as lepton number, baryon number, and flavour quantum numbers) are zero. Also, the photon does not obey the Pauli exclusion principle.
Photons are emitted in many natural processes. For example, when a charge is accelerated it emits synchrotron radiation. During a molecular, atomic or nuclear transition to a lower energy level, photons of various energy will be emitted, from radio waves to gamma rays. A photon can also be emitted when a particle and its corresponding antiparticle are annihilated (for example, electron–positron annihilation).
In empty space, the photon moves at "c" (the speed of light) and its energy and momentum are related by , where "p" is the magnitude of the momentum vector p. This derives from the following relativistic relation, with :
The energy and momentum of a photon depend only on its frequency ("ν") or inversely, its wavelength ("λ"):
where k is the wave vector (where the wave number ), is the angular frequency, and is the reduced Planck constant.
Since p points in the direction of the photon's propagation, the magnitude of the momentum is
The photon also carries spin angular momentum that does not depend on its frequency. The magnitude of its spin is formula_5 and the component measured along its direction of motion, its helicity, must be ±ħ. These two possible helicities, called right-handed and left-handed, correspond to the two possible circular polarization states of the photon.
To illustrate the significance of these formulae, the annihilation of a particle with its antiparticle in free space must result in the creation of at least "two" photons for the following reason. In the center of momentum frame, the colliding antiparticles have no net momentum, whereas a single photon always has momentum (since, as we have seen, it is determined by the photon's frequency or wavelength, which cannot be zero). Hence, conservation of momentum (or equivalently, translational invariance) requires that at least two photons are created, with zero net momentum. (However, it is possible if the system interacts with another particle or field for annihilation to produce one photon, as when a positron annihilates with a bound atomic electron, it is possible for only one photon to be emitted, as the nuclear Coulomb field breaks translational symmetry.) The energy of the two photons, or, equivalently, their frequency, may be determined from conservation of four-momentum. Seen another way, the photon can be considered as its own antiparticle. The reverse process, pair production, is the dominant mechanism by which high-energy photons such as gamma rays lose energy while passing through matter. That process is the reverse of "annihilation to one photon" allowed in the electric field of an atomic nucleus.
The classical formulae for the energy and momentum of electromagnetic radiation can be re-expressed in terms of photon events. For example, the pressure of electromagnetic radiation on an object derives from the transfer of photon momentum per unit time and unit area to that object, since pressure is force per unit area and force is the change in momentum per unit time.
Experimental checks on photon mass.
Current commonly accepted physical theories imply or assume the photon to be strictly massless. If the photon is not a strictly massless particle, it would not move at the exact speed of light in vacuum, "c". Its speed would be lower and depend on its frequency. Relativity would be unaffected by this; the so-called speed of light, "c", would then not be the actual speed at which light moves, but a constant of nature which is the maximum speed that any object could theoretically attain in space-time. Thus, it would still be the speed of space-time ripples (gravitational waves and gravitons), but it would not be the speed of photons.
If a photon did have non-zero mass, there would be other effects as well. Coulomb's law would be modified and the electromagnetic field would have an extra physical degree of freedom. These effects yield more sensitive experimental probes of the photon mass than the frequency dependence of the speed of light. If Coulomb's law is not exactly valid, then that would cause the presence of an electric field inside a hollow conductor when it is subjected to an external electric field. This thus allows one to test Coulomb's law to very high precision. A null result of such an experiment has set a limit of "m" ≲ 10−14 eV/c2.
Sharper upper limits have been obtained in experiments designed to detect effects caused by the galactic vector potential. Although the galactic vector potential is very large because the galactic magnetic field exists on very long length scales, only the magnetic field is observable if the photon is massless. In case of a massive photon, the mass term formula_6 would affect the galactic plasma. The fact that no such effects are seen implies an upper bound on the photon mass of "m" < . The galactic vector potential can also be probed directly by measuring the torque exerted on a magnetized ring. Such methods were used to obtain the sharper upper limit of 10−18eV/c2 (the equivalent of ) given by the Particle Data Group.
These sharp limits from the non-observation of the effects caused by the galactic vector potential have been shown to be model dependent. If the photon mass is generated via the Higgs mechanism then the upper limit of "m"≲10−14 eV/c2 from the test of Coulomb's law is valid.
Photons inside superconductors do develop a nonzero effective rest mass; as a result, electromagnetic forces become short-range inside superconductors.
Historical development.
In most theories up to the eighteenth century, light was pictured as being made up of particles. Since particle models cannot easily account for the refraction, diffraction and birefringence of light, wave theories of light were proposed by René Descartes (1637), Robert Hooke (1665), and Christiaan Huygens (1678); however, particle models remained dominant, chiefly due to the influence of Isaac Newton. In the early nineteenth century, Thomas Young and August Fresnel clearly demonstrated the interference and diffraction of light and by 1850 wave models were generally accepted. In 1865, James Clerk Maxwell's prediction that light was an electromagnetic wave—which was confirmed experimentally in 1888 by Heinrich Hertz's detection of radio waves—seemed to be the final blow to particle models of light.
The Maxwell wave theory, however, does not account for "all" properties of light. The Maxwell theory predicts that the energy of a light wave depends only on its intensity, not on its frequency; nevertheless, several independent types of experiments show that the energy imparted by light to atoms depends only on the light's frequency, not on its intensity. For example, some chemical reactions are provoked only by light of frequency higher than a certain threshold; light of frequency lower than the threshold, no matter how intense, does not initiate the reaction. Similarly, electrons can be ejected from a metal plate by shining light of sufficiently high frequency on it (the photoelectric effect); the energy of the ejected electron is related only to the light's frequency, not to its intensity.
At the same time, investigations of blackbody radiation carried out over four decades (1860–1900) by various researchers culminated in Max Planck's hypothesis that the energy of "any" system that absorbs or emits electromagnetic radiation of frequency "ν" is an integer multiple of an energy quantum . As shown by Albert Einstein, some form of energy quantization "must" be assumed to account for the thermal equilibrium observed between matter and electromagnetic radiation; for this explanation of the photoelectric effect, Einstein received the 1921 Nobel Prize in physics.
Since the Maxwell theory of light allows for all possible energies of electromagnetic radiation, most physicists assumed initially that the energy quantization resulted from some unknown constraint on the matter that absorbs or emits the radiation. In 1905, Einstein was the first to propose that energy quantization was a property of electromagnetic radiation itself. Although he accepted the validity of Maxwell's theory, Einstein pointed out that many anomalous experiments could be explained if the "energy" of a Maxwellian light wave were localized into point-like quanta that move independently of one another, even if the wave itself is spread continuously over space. In 1909 and 1916, Einstein showed that, if Planck's law of black-body radiation is accepted, the energy quanta must also carry momentum , making them full-fledged particles. This photon momentum was observed experimentally by Arthur Compton, for which he received the Nobel Prize in 1927. The pivotal question was then: how to unify Maxwell's wave theory of light with its experimentally observed particle nature? The answer to this question occupied Albert Einstein for the rest of his life, and was solved in quantum electrodynamics and its successor, the Standard Model (see Second quantization and The photon as a gauge boson, below).
Einstein's light quantum.
Unlike Planck, Einstein entertained the possibility that there might be actual physical quanta of light—what we now call photons. He noticed that a light quantum with energy proportional to its frequency would explain a number of troubling puzzles and paradoxes, including an unpublished law by Stokes, the ultraviolet catastrophe, and of course the photoelectric effect. Stokes's law said simply that the frequency of fluorescent light cannot be greater than the frequency of the light (usually ultraviolet) inducing it. Einstein eliminated the ultraviolet catastrophe by imagining a gas of photons behaving like a gas of electrons that he had previously considered. He was advised by a colleague to be careful how he wrote up this paper, in order to not challenge Planck too directly, as he was a powerful figure, and indeed the warning was justified, as Planck never forgave him for writing it.
Early objections.
Einstein's 1905 predictions were verified experimentally in several ways in the first two decades of the 20th century, as recounted in Robert Millikan's Nobel lecture. However, before Compton's experiment showing that photons carried momentum proportional to their wave number (or frequency) (1922), most physicists were reluctant to believe that electromagnetic radiation itself might be particulate. (See, for example, the Nobel lectures of Wien, Planck and Millikan.) Instead, there was a widespread belief that energy quantization resulted from some unknown constraint on the matter that absorbs or emits radiation. Attitudes changed over time. In part, the change can be traced to experiments such as Compton scattering, where it was much more difficult not to ascribe quantization to light itself to explain the observed results.
Even after Compton's experiment, Niels Bohr, Hendrik Kramers and John Slater made one last attempt to preserve the Maxwellian continuous electromagnetic field model of light, the so-called BKS model. To account for the data then available, two drastic hypotheses had to be made:
However, refined Compton experiments showed that energy–momentum is conserved extraordinarily well in elementary processes; and also that the jolting of the electron and the generation of a new photon in Compton scattering obey causality to within 10 ps. Accordingly, Bohr and his co-workers gave their model "as honorable a funeral as possible". Nevertheless, the failures of the BKS model inspired Werner Heisenberg in his development of matrix mechanics.
A few physicists persisted in developing semiclassical models in which electromagnetic radiation is not quantized, but matter appears to obey the laws of quantum mechanics. Although the evidence for photons from chemical and physical experiments was overwhelming by the 1970s, this evidence could not be considered as "absolutely" definitive; since it relied on the interaction of light with matter, a sufficiently complicated theory of matter could in principle account for the evidence. Nevertheless, "all" semiclassical theories were refuted definitively in the 1970s and 1980s by photon-correlation experiments. Hence, Einstein's hypothesis that quantization is a property of light itself is considered to be proven.
Wave–particle duality and uncertainty principles.
Photons, like all quantum objects, exhibit wave-like and particle-like properties. Their dual wave–particle nature can be difficult to visualize. The photon displays clearly wave-like phenomena such as diffraction and interference on the length scale of its wavelength. For example, a single photon passing through a double-slit experiment lands on the screen exhibiting interference phenomena but only if no measure was made on the actual slit being run across. To account for the particle interpretation that phenomenon is called probability distribution but behaves according to Maxwell's equations. However, experiments confirm that the photon is "not" a short pulse of electromagnetic radiation; it does not spread out as it propagates, nor does it divide when it encounters a beam splitter. Rather, the photon seems to be a point-like particle since it is absorbed or emitted "as a whole" by arbitrarily small systems, systems much smaller than its wavelength, such as an atomic nucleus (≈10−15 m across) or even the point-like electron. Nevertheless, the photon is "not" a point-like particle whose trajectory is shaped probabilistically by the electromagnetic field, as conceived by Einstein and others; that hypothesis was also refuted by the photon-correlation experiments cited above. According to our present understanding, the electromagnetic field itself is produced by photons, which in turn result from a local gauge symmetry and the laws of quantum field theory (see the Second quantization and Gauge boson sections below).
A key element of quantum mechanics is Heisenberg's uncertainty principle, which forbids the simultaneous measurement of the position and momentum of a particle along the same direction. Remarkably, the uncertainty principle for charged, material particles "requires" the quantization of light into photons, and even the frequency dependence of the photon's energy and momentum. An elegant illustration is Heisenberg's thought experiment for locating an electron with an ideal microscope. The position of the electron can be determined to within the resolving power of the microscope, which is given by a formula from classical optics
where formula_8 is the aperture angle of the microscope. Thus, the position uncertainty formula_9 can be made arbitrarily small by reducing the wavelength λ. The momentum of the electron is uncertain, since it received a "kick" formula_10 from the light scattering from it into the microscope. If light were "not" quantized into photons, the uncertainty formula_10 could be made arbitrarily small by reducing the light's intensity. In that case, since the wavelength and intensity of light can be varied independently, one could simultaneously determine the position and momentum to arbitrarily high accuracy, violating the uncertainty principle. By contrast, Einstein's formula for photon momentum preserves the uncertainty principle; since the photon is scattered anywhere within the aperture, the uncertainty of momentum transferred equals
giving the product formula_13, which is Heisenberg's uncertainty principle. Thus, the entire world is quantized; both matter and fields must obey a consistent set of quantum laws, if either one is to be quantized.
The analogous uncertainty principle for photons forbids the simultaneous measurement of the number formula_14 of photons (see Fock state and the Second quantization section below) in an electromagnetic wave and the phase formula_15 of that wave
See coherent state and squeezed coherent state for more details.
Both (photons and material) particles such as electrons create analogous interference patterns when passing through a double-slit experiment. For photons, this corresponds to the interference of a Maxwell light wave whereas, for material particles, this corresponds to the interference of the Schrödinger wave equation. Although this similarity might suggest that Maxwell's equations are simply Schrödinger's equation for photons, most physicists do not agree. For one thing, they are mathematically different; most obviously, Schrödinger's one equation solves for a complex field, whereas Maxwell's four equations solve for real fields. More generally, the normal concept of a Schrödinger probability wave function cannot be applied to photons. Being massless, they cannot be localized without being destroyed; technically, photons cannot have a position eigenstate formula_17, and, thus, the normal Heisenberg uncertainty principle formula_18 does not pertain to photons. A few substitute wave functions have been suggested for the photon, but they have not come into general use. Instead, physicists generally accept the second-quantized theory of photons described below, quantum electrodynamics, in which photons are quantized excitations of electromagnetic modes.
Another interpretation, that avoids duality, is the De Broglie–Bohm theory: known also as the "pilot-wave model", the photon in this theory is both, wave and particle. "This idea seems to me so natural and simple, to resolve the wave-particle dilemma in such a clear and ordinary way, that it is a great mystery to me that it was so generally ignored", J.S.Bell.
Bose–Einstein model of a photon gas.
In 1924, Satyendra Nath Bose derived Planck's law of black-body radiation without using any electromagnetism, but rather a modification of coarse-grained counting of phase space. Einstein showed that this modification is equivalent to assuming that photons are rigorously identical and that it implied a "mysterious non-local interaction", now understood as the requirement for a symmetric quantum mechanical state. This work led to the concept of coherent states and the development of the laser. In the same papers, Einstein extended Bose's formalism to material particles (bosons) and predicted that they would condense into their lowest quantum state at low enough temperatures; this Bose–Einstein condensation was observed experimentally in 1995. It was later used by Lene Hau to slow, and then completely stop, light in 1999 and 2001.
The modern view on this is that photons are, by virtue of their integer spin, bosons (as opposed to fermions with half-integer spin). By the spin-statistics theorem, all bosons obey Bose–Einstein statistics (whereas all fermions obey Fermi–Dirac statistics).
Stimulated and spontaneous emission.
In 1916, Einstein showed that Planck's radiation law could be derived from a semi-classical, statistical treatment of photons and atoms, which implies a relation between the rates at which atoms emit and absorb photons. The condition follows from the assumption that light is emitted and absorbed by atoms independently, and that the thermal equilibrium is preserved by interaction with atoms. Consider a cavity in thermal equilibrium and filled with electromagnetic radiation and atoms that can emit and absorb that radiation. Thermal equilibrium requires that the energy density formula_19 of photons with frequency formula_20 (which is proportional to their number density) is, on average, constant in time; hence, the rate at which photons of any particular frequency are "emitted" must equal the rate of "absorbing" them.
Einstein began by postulating simple proportionality relations for the different reaction rates involved. In his model, the rate formula_21 for a system to "absorb" a photon of frequency formula_20 and transition from a lower energy formula_23 to a higher energy formula_24 is proportional to the number formula_25 of atoms with energy formula_23 and to the energy density formula_19 of ambient photons with that frequency,
where formula_29 is the rate constant for absorption. For the reverse process, there are two possibilities: spontaneous emission of a photon, and a return to the lower-energy state that is initiated by the interaction with a passing photon. Following Einstein's approach, the corresponding rate formula_30 for the emission of photons of frequency formula_20 and transition from a higher energy formula_24 to a lower energy formula_23 is
where formula_35 is the rate constant for emitting a photon spontaneously, and formula_36 is the rate constant for emitting it in response to ambient photons (induced or stimulated emission). In thermodynamic equilibrium, the number of atoms in state i and that of atoms in state j must, on average, be constant; hence, the rates formula_21 and formula_30 must be equal. Also, by arguments analogous to the derivation of Boltzmann statistics, the ratio of formula_39 and formula_25 is formula_41 where formula_42 are the degeneracy of the state i and that of j, respectively, formula_43 their energies, k the Boltzmann constant and T the system's temperature. From this, it is readily derived that
formula_44 and
The A and Bs are collectively known as the "Einstein coefficients".
Einstein could not fully justify his rate equations, but claimed that it should be possible to calculate the coefficients formula_35, formula_29 and formula_36 once physicists had obtained "mechanics and electrodynamics modified to accommodate the quantum hypothesis". In fact, in 1926, Paul Dirac derived the formula_36 rate constants in using a semiclassical approach, and, in 1927, succeeded in deriving "all" the rate constants from first principles within the framework of quantum theory. Dirac's work was the foundation of quantum electrodynamics, i.e., the quantization of the electromagnetic field itself. Dirac's approach is also called "second quantization" or quantum field theory; earlier quantum mechanical treatments only treat material particles as quantum mechanical, not the electromagnetic field.
Einstein was troubled by the fact that his theory seemed incomplete, since it did not determine the "direction" of a spontaneously emitted photon. A probabilistic nature of light-particle motion was first considered by Newton in his treatment of birefringence and, more generally, of the splitting of light beams at interfaces into a transmitted beam and a reflected beam. Newton hypothesized that hidden variables in the light particle determined which path it would follow. Similarly, Einstein hoped for a more complete theory that would leave nothing to chance, beginning his separation from quantum mechanics. Ironically, Max Born's probabilistic interpretation of the wave function was inspired by Einstein's later work searching for a more complete theory.
Second quantization and high energy photon interactions.
In 1910, Peter Debye derived Planck's law of black-body radiation from a relatively simple assumption. He correctly decomposed the electromagnetic field in a cavity into its Fourier modes, and assumed that the energy in any mode was an integer multiple of formula_50, where formula_20 is the frequency of the electromagnetic mode. Planck's law of black-body radiation follows immediately as a geometric sum. However, Debye's approach failed to give the correct formula for the energy fluctuations of blackbody radiation, which were derived by Einstein in 1909.
In 1925, Born, Heisenberg and Jordan reinterpreted Debye's concept in a key way. As may be shown classically, the Fourier modes of the electromagnetic field—a complete set of electromagnetic plane waves indexed by their wave vector k and polarization state—are equivalent to a set of uncoupled simple harmonic oscillators. Treated quantum mechanically, the energy levels of such oscillators are known to be formula_52, where formula_20 is the oscillator frequency. The key new step was to identify an electromagnetic mode with energy formula_52 as a state with formula_14 photons, each of energy formula_50. This approach gives the correct energy fluctuation formula.
Dirac took this one step further. He treated the interaction between a charge and an electromagnetic field as a small perturbation that induces transitions in the photon states, changing the numbers of photons in the modes, while conserving energy and momentum overall. Dirac was able to derive Einstein's formula_35 and formula_36 coefficients from first principles, and showed that the Bose–Einstein statistics of photons is a natural consequence of quantizing the electromagnetic field correctly (Bose's reasoning went in the opposite direction; he derived Planck's law of black-body radiation by "assuming" B–E statistics). In Dirac's time, it was not yet known that all bosons, including photons, must obey Bose–Einstein statistics.
Dirac's second-order perturbation theory can involve virtual photons, transient intermediate states of the electromagnetic field; the static electric and magnetic interactions are mediated by such virtual photons. In such quantum field theories, the probability amplitude of observable events is calculated by summing over "all" possible intermediate steps, even ones that are unphysical; hence, virtual photons are not constrained to satisfy formula_59, and may have extra polarization states; depending on the gauge used, virtual photons may have three or four polarization states, instead of the two states of real photons. Although these transient virtual photons can never be observed, they contribute measurably to the probabilities of observable events. Indeed, such second-order and higher-order perturbation calculations can give apparently infinite contributions to the sum. Such unphysical results are corrected for using the technique of renormalization.
Other virtual particles may contribute to the summation as well; for example, two photons may interact indirectly through virtual electron–positron pairs. In fact, such photon-photon scattering (see two-photon physics), as well as electron-photon scattering, is meant to be one of the modes of operations of the planned particle accelerator, the International Linear Collider.
In modern physics notation, the quantum state of the electromagnetic field is written as a Fock state, a tensor product of the states for each electromagnetic mode
where formula_61 represents the state in which formula_62 photons are in the mode formula_63. In this notation, the creation of a new photon in mode formula_63 (e.g., emitted from an atomic transition) is written as formula_65. This notation merely expresses the concept of Born, Heisenberg and Jordan described above, and does not add any physics.
The hadronic properties of the photon.
Measurements of the interaction between energetic photons and hadrons show that the interaction is much more intense than expected by the interaction of merely photons with the hadron's electric charge. Furthermore, the interaction of energetic photons with protons is similar to the interaction of photons with neutrons in spite of the fact that the electric charge structures of protons and neutrons are substantially different.
A theory called Vector Meson Dominance (VMD) was developed to explain this effect. According to VMD, the photon is a superposition of the pure electromagnetic photon (which interacts only with electric charges) and vector meson.
However, if experimentally probed at very short distances, the intrinsic structure of the photon is recognized as a flux of quark and gluon components, quasi-free according to asymptotic freedom in QCD and described by the photon structure function. A comprehensive comparison of data with theoretical predictions is presented in a recent review.
The photon as a gauge boson.
The electromagnetic field can be understood as a gauge field, i.e., as a field that results from requiring that a gauge symmetry holds independently at every position in spacetime. For the electromagnetic field, this gauge symmetry is the Abelian U(1) symmetry of complex numbers of absolute value 1, which reflects the ability to vary the phase of a complex number without affecting observables or real valued functions made from it, such as the energy or the Lagrangian.
The quanta of an Abelian gauge field must be massless, uncharged bosons, as long as the symmetry is not broken; hence, the photon is predicted to be massless, and to have zero electric charge and integer spin. The particular form of the electromagnetic interaction specifies that the photon must have spin ±1; thus, its helicity must be formula_66. These two spin components correspond to the classical concepts of right-handed and left-handed circularly polarized light. However, the transient virtual photons of quantum electrodynamics may also adopt unphysical polarization states.
In the prevailing Standard Model of physics, the photon is one of four gauge bosons in the electroweak interaction; the other three are denoted W+, W− and Z0 and are responsible for the weak interaction. Unlike the photon, these gauge bosons have mass, owing to a mechanism that breaks their SU(2) gauge symmetry. The unification of the photon with W and Z gauge bosons in the electroweak interaction was accomplished by Sheldon Glashow, Abdus Salam and Steven Weinberg, for which they were awarded the 1979 Nobel Prize in physics. Physicists continue to hypothesize grand unified theories that connect these four gauge bosons with the eight gluon gauge bosons of quantum chromodynamics; however, key predictions of these theories, such as proton decay, have not been observed experimentally.
Contributions to the mass of a system.
The energy of a system that emits a photon is "decreased" by the energy formula_67 of the photon as measured in the rest frame of the emitting system, which may result in a reduction in mass in the amount formula_68. Similarly, the mass of a system that absorbs a photon is "increased" by a corresponding amount. As an application, the energy balance of nuclear reactions involving photons is commonly written in terms of the masses of the nuclei involved, and terms of the form formula_68 for the gamma photons (and for other relevant energies, such as the recoil energy of nuclei).
This concept is applied in key predictions of quantum electrodynamics (QED, see above). In that theory, the mass of electrons (or, more generally, leptons) is modified by including the mass contributions of virtual photons, in a technique known as renormalization. Such "radiative corrections" contribute to a number of predictions of QED, such as the magnetic dipole moment of leptons, the Lamb shift, and the hyperfine structure of bound lepton pairs, such as muonium and positronium.
Since photons contribute to the stress–energy tensor, they exert a gravitational attraction on other objects, according to the theory of general relativity. Conversely, photons are themselves affected by gravity; their normally straight trajectories may be bent by warped spacetime, as in gravitational lensing, and their frequencies may be lowered by moving to a higher gravitational potential, as in the Pound–Rebka experiment. However, these effects are not specific to photons; exactly the same effects would be predicted for classical electromagnetic waves.
Photons in matter.
Any 'explanation' of how photons travel through matter has to explain why different arrangements of matter are transparent or opaque at different wavelengths (light through carbon as diamond or not, as graphite) and why individual photons behave in the same way as large groups. Explanations that invoke 'absorption' and 're-emission' have to provide an explanation for the directionality of the photons (diffraction, reflection) and further explain how entangled photon pairs can travel through matter without their quantum state collapsing.
The simplest explanation is that light that travels through transparent matter does so at a lower speed than "c", the speed of light in a vacuum. In addition, light can also undergo scattering and absorption. There are circumstances in which heat transfer through a material is mostly radiative, involving emission and absorption of photons within it. An example would be in the core of the Sun. Energy can take about a million years to reach the surface. However, this phenomenon is distinct from scattered radiation passing diffusely through matter, as it involves local equilibrium between the radiation and the temperature. Thus, the time is how long it takes the "energy" to be transferred, not the "photons" themselves. Once in open space, a photon from the Sun takes only 8.3 minutes to reach Earth. The factor by which the speed of light is decreased in a material is called the refractive index of the material. In a classical wave picture, the slowing can be explained by the light inducing electric polarization in the matter, the polarized matter radiating new light, and the new light interfering with the original light wave to form a delayed wave. In a particle picture, the slowing can instead be described as a blending of the photon with quantum excitation of the matter (quasi-particles such as phonons and excitons) to form a polariton; this polariton has a nonzero effective mass, which means that it cannot travel at "c".
Alternatively, photons may be viewed as "always" traveling at "c", even in matter, but they have their phase shifted (delayed or advanced) upon interaction with atomic scatters: this modifies their wavelength and momentum, but not speed. A light wave made up of these photons does travel slower than the speed of light. In this view the photons are "bare", and are scattered and phase shifted, while in the view of the preceding paragraph the photons are "dressed" by their interaction with matter, and move without scattering or phase shifting, but at a lower speed.
Light of different frequencies may travel through matter at different speeds; this is called dispersion. In some cases, it can result in extremely slow speeds of light in matter. The effects of photon interactions with other quasi-particles may be observed directly in Raman scattering and Brillouin scattering.
Photons can also be absorbed by nuclei, atoms or molecules, provoking transitions between their energy levels. A classic example is the molecular transition of retinal (C20H28O), which is responsible for vision, as discovered in 1958 by Nobel laureate biochemist George Wald and co-workers. The absorption provokes a cis-trans isomerization that, in combination with other such transitions, is transduced into nerve impulses. The absorption of photons can even break chemical bonds, as in the photodissociation of chlorine; this is the subject of photochemistry. Analogously, gamma rays can in some circumstances dissociate atomic nuclei in a process called photodisintegration.
Technological applications.
Photons have many applications in technology. These examples are chosen to illustrate applications of photons "per se", rather than general optical devices such as lenses, etc. that could operate under a classical theory of light. The laser is an extremely important application and is discussed above under stimulated emission.
Individual photons can be detected by several methods. The classic photomultiplier tube exploits the photoelectric effect: a photon landing on a metal plate ejects an electron, initiating an ever-amplifying avalanche of electrons. Charge-coupled device chips use a similar effect in semiconductors: an incident photon generates a charge on a microscopic capacitor that can be detected. Other detectors such as Geiger counters use the ability of photons to ionize gas molecules, causing a detectable change in conductivity.
Planck's energy formula formula_70 is often used by engineers and chemists in design, both to compute the change in energy resulting from a photon absorption and to predict the frequency of the light emitted for a given energy transition. For example, the emission spectrum of a gas-discharge lamp can be altered by filling it with (mixtures of) gasses with different electronic energy level configurations.
Under some conditions, an energy transition can be excited by "two" photons that individually would be insufficient. This allows for higher resolution microscopy, because the sample absorbs energy only in the region where two beams of different colors overlap significantly, which can be made much smaller than the excitation volume of a single beam (see two-photon excitation microscopy). Moreover, these photons cause less damage to the sample, since they are of lower energy.
In some cases, two energy transitions can be coupled so that, as one system absorbs a photon, another nearby system "steals" its energy and re-emits a photon of a different frequency. This is the basis of fluorescence resonance energy transfer, a technique that is used in molecular biology to study the interaction of suitable proteins.
Several different kinds of hardware random number generator involve the detection of single photons. In one example, for each bit in the random sequence that is to be produced, a photon is sent to a beam-splitter. In such a situation, there are two possible outcomes of equal probability. The actual outcome is used to determine whether the next bit in the sequence is "0" or "1".
Recent research.
Much research has been devoted to applications of photons in the field of quantum optics. Photons seem well-suited to be elements of an extremely fast quantum computer, and the quantum entanglement of photons is a focus of research. Nonlinear optical processes are another active research area, with topics such as two-photon absorption, self-phase modulation, modulational instability and optical parametric oscillators. However, such processes generally do not require the assumption of photons "per se"; they may often be modeled by treating atoms as nonlinear oscillators. The nonlinear process of spontaneous parametric down conversion is often used to produce single-photon states. Finally, photons are essential in some aspects of optical communication, especially for quantum cryptography.
Additional references.
By date of publication:
Education with single photons:

</doc>
<doc id="4043742" url="https://en.wikipedia.org/wiki?curid=4043742" title="Physics beyond the Standard Model">
Physics beyond the Standard Model

Physics beyond the Standard Model (BSM) refers to the theoretical developments needed to explain the deficiencies of the Standard Model, such as the origin of mass, the strong CP problem, neutrino oscillations, matter–antimatter asymmetry, and the nature of dark matter and dark energy. Another problem lies within the mathematical framework of the Standard Model itself—the Standard Model is inconsistent with that of general relativity, to the point that one or both theories break down under certain conditions (for example within known spacetime singularities like the Big Bang and black hole event horizons).
Theories that lie beyond the Standard Model include various extensions of the standard model through supersymmetry, such as the Minimal Supersymmetric Standard Model (MSSM) and Next-to-Minimal Supersymmetric Standard Model (NMSSM), or entirely novel explanations, such as string theory, M-theory, and extra dimensions. As these theories tend to reproduce the entirety of current phenomena, the question of which theory is the right one, or at least the "best step" towards a Theory of Everything, can only be settled via experiments, and is one of the most active areas of research in both theoretical and experimental physics.
Problems with the Standard Model.
Despite being the most successful theory of particle physics to date, the Standard Model is not perfect. A large share of the published output of theoretical physicists consists of proposals for various forms of "Beyond the Standard Model" new physics proposals that would modify the Standard Model in ways subtle enough to be consistent with existing data, yet address its imperfections materially enough to predict non-Standard Model outcomes of new experiments that can be proposed.
Phenomena not explained.
The Standard Model is inherently an incomplete theory. There are fundamental physical phenomena in nature that the Standard Model does not adequately explain:
Experimental results not explained.
No experimental result is widely accepted as definitively contradicting the Standard Model at the "five sigma" level, widely considered to be the threshold of a "discovery" in particle physics. But because every experiment contains some degree of statistical and systemic uncertainty, and the theoretical predictions themselves are also almost never calculated exactly and are subject to uncertainties in measurements of the fundamental constants of the Standard Model (some of which are tiny and others of which are substantial), it is mathematically expected that some of the hundreds of experimental tests of the Standard Model will deviate to some extent from it, even if there were no "new physics" to be discovered.
At any given time there are a number of experimental results that are significantly different from the Standard Model expectation, although many of these have been found to be statistical flukes or experimental errors as more data has been collected. On the other hand, any "beyond the Standard Model" physics would necessarily first manifest experimentally as a statistically significant difference between an experiment and the theoretical prediction.
In each case, physicists seek to determine if a result is a mere statistical fluke or experimental error on the one hand, or a sign of new physics on the other. More statistically significant results cannot be mere statistical flukes but can still result from experimental error or inaccurate estimates of experimental precision. Frequently, experiments are tailored to be more sensitive to experimental results that would distinguish the Standard Model from theoretical alternatives.
Some of the most notable examples include the following:
Theoretical predictions not observed.
Observation at particle colliders of all of the fundamental particles predicted by the Standard Model has been confirmed. The Higgs boson is predicted by the Standard Model's explanation of the Higgs mechanism, which describes how the weak SU(2) gauge symmetry is broken and how fundamental particles obtain mass; it was the last particle predicted by the Standard Model to be observed. On July 4, 2012, CERN scientists using the Large Hadron Collider announced the discovery of a particle consistent with the Higgs boson, with a mass of about . A Higgs boson was confirmed to exist on March 14, 2013, although efforts to confirm that it has all of the properties predicted by the Standard Model are ongoing.
A few hadrons (i.e. composite particles made of quarks) whose existence is predicted by the Standard Model, which can be produced only at very high energies in very low frequencies have not yet been definitively observed, and "glueballs" (i.e. composite particles made of gluons) have also not yet been definitively observed. Some very low frequency particle decays predicted by the Standard Model have also not yet been definitively observed because insufficient data is available to make a statistically significant observation.
Theoretical problems.
Some features of the standard model are added in an ad hoc way. These are not problems per se (i.e. the theory works fine with these ad hoc features), but they imply a lack of understanding. These ad hoc features have motivated theorists to look for more fundamental theories with fewer parameters. Some of the ad hoc features are:
Grand unified theories.
The standard model has three gauge symmetries; the colour SU(3), the weak isospin SU(2), and the hypercharge U(1) symmetry, corresponding to the three fundamental forces. Due to renormalization the coupling constants of each of these symmetries vary with the energy at which they are measured. Around these couplings become approximately equal. This has led to speculation that above this energy the three gauge symmetries of the standard model are unified in one single gauge symmetry with a simple group gauge group, and just one coupling constant. Below this energy the symmetry is spontaneously broken to the standard model symmetries. Popular choices for the unifying group are the special unitary group in five dimensions SU(5) and the special orthogonal group in ten dimensions SO(10).
Theories that unify the standard model symmetries in this way are called Grand Unified Theories (or GUTs), and the energy scale at which the unified symmetry is broken is called the GUT scale. Generically, grand unified theories predict the creation of magnetic monopoles in the early universe, and instability of the proton. Neither of these have been observed, and this absence of observation puts limits on the possible GUTs.
Supersymmetry.
Supersymmetry extends the Standard Model by adding another class of symmetries to the Lagrangian. These symmetries exchange fermionic particles with bosonic ones. Such a symmetry predicts the existence of "supersymmetric particles", abbreviated as "sparticles", which include the sleptons, squarks, neutralinos and charginos. Each particle in the Standard Model would have a superpartner whose spin differs by 1/2 from the ordinary particle. Due to the breaking of supersymmetry, the sparticles are much heavier than their ordinary counterparts; they are so heavy that existing particle colliders may not be powerful enough to produce them.
Neutrinos.
In the standard model, neutrinos have exactly zero mass. This is a consequence of the standard model containing only left-handed neutrinos. With no suitable right-handed partner, it is impossible to add a renormalizable mass term to the standard model. Measurements however indicated that neutrinos spontaneously change flavour, which implies that neutrinos have a mass. These measurements only give the relative masses of the different flavours. The best constraint on the absolute mass of the neutrinos comes from precision measurements of tritium decay, providing an upper limit 2 eV, which makes them at least five orders of magnitude lighter than the other particles in the standard model. This necessitates an extension of the standard model, which not only needs to explain how neutrinos get their mass, but also why the mass is so small.
One approach to add masses to the neutrinos, the so-called seesaw mechanism, is to add right-handed neutrinos and have these couple to left-handed neutrinos with a Dirac mass term. The right-handed neutrinos have to be sterile, meaning that they do not participate in any of the standard model interactions. Because they have no charges, the right-handed neutrinos can act as their own anti-particles, and have a Majorana mass term. Like the other Dirac masses in the standard model, the neutrino Dirac mass is expected to be generated through the Higgs mechanism, and is therefore unpredictable. The standard model fermion masses differ by many orders of magnitude; the Dirac neutrino mass has at least the same uncertainty. On the other hand, the Majorana mass for the right-handed neutrinos does not arise from the Higgs mechanism, and is therefore expected to be tied to some energy scale of new physics beyond the standard model, for example the Planck scale. Therefore, any process involving right-handed neutrinos will be suppressed at low energies. The correction due to these suppressed processes effectively gives the left-handed neutrinos a mass that is inversely proportional to the right-handed Majorana mass, a mechanism known as the see-saw. The presence of heavy right-handed neutrinos thereby explains both the small mass of the left-handed neutrinos and the absence of the right-handed neutrinos in observations. 
However, due to the uncertainty in the Dirac neutrino masses, the right-handed neutrino masses can lie anywhere. For example, they could be as light as keV and be dark matter, they can have a mass in the LHC energy range and lead to observable lepton number violation, or they can be near the GUT scale, linking the right-handed neutrinos to the possibility of a grand unified theory.
The mass terms mix neutrinos of different generations. This mixing is parameterized by the PMNS matrix, which is the neutrino analogue of the CKM quark mixing matrix. Unlike the quark mixing, which is almost minimal, the mixing of the neutrinos appears to be almost maximal. This has led to various speculations of symmetries between the various generations that could explain the mixing patterns. The mixing matrix could also contain several complex phases that break CP invariance, although there has been no experimental probe of these. These phases could potentially create a surplus of leptons over anti-leptons in the early universe, a process known as leptogenesis. This asymmetry could then at a later stage be converted in an excess of baryons over anti-baryons, and explain the matter-antimatter asymmetry in the universe.
The light neutrinos are disfavored as an explanation for the observation of dark matter, due to considerations of large-scale structure formation in the early universe. Simulations of structure formation show that they are too hot—i.e. their kinetic energy is large compared to their mass—while formation of structures similar to the galaxies in our universe requires cold dark matter. The simulations show that neutrinos can at best explain a few percent of the missing dark matter. The heavy sterile right-handed neutrinos are however a possible candidate for a dark matter WIMP.
Preon Models.
Several preon models have been proposed to address the unsolved problem concerning the fact that there are three generations of quarks and leptons. Preon models generally postulate some additional new particles which are further postulated to be able to combine to form the quarks and leptons of the standard model. One of the earliest preon models was the Rishon model.
To date, no preon model is widely accepted or fully verified.
Theories of everything.
Theory of everything.
Theoretical physics continues to strive toward a theory of everything, a theory that fully explains and links together all known physical phenomena, and predicts the outcome of any experiment that could be carried out in principle. In practical terms the immediate goal in this regard is to develop a theory which would unify the Standard Model with General Relativity in a theory of quantum gravity. Additional features, such as overcoming conceptual flaws in either theory or accurate prediction of particle masses, would be desired.
The challenges in putting together such a theory are not just conceptual - they include the experimental aspects of the very high energies needed to probe exotic realms.
Several notable attempts in this direction are supersymmetry, string theory, and loop quantum gravity.
String theory.
Extensions, revisions, replacements, and reorganizations of the Standard Model exist in attempt to correct for these and other issues. String theory is one such reinvention, and many theoretical physicists think that such theories are the next theoretical step toward a true Theory of Everything. Theories of quantum gravity such as loop quantum gravity and others are thought by some to be promising candidates to the mathematical unification of quantum field theory and general relativity, requiring less drastic changes to existing theories. However recent work places stringent limits on the putative effects of quantum gravity on the speed of light, and disfavours some current models of quantum gravity.
Among the numerous variants of string theory, M-theory, whose mathematical existence was first proposed at a String Conference in 1995, is believed by many to be a proper "ToE" candidate, notably by physicists Brian Greene and Stephen Hawking. Though a full mathematical description is not yet known, solutions to the theory exist for specific cases. Recent works have also proposed alternate string models, some of which lack the various harder-to-test features of M-theory (e.g. the existence of Calabi–Yau manifolds, many extra dimensions, etc.) including works by well-published physicists such as Lisa Randall.

</doc>
<doc id="36961" url="https://en.wikipedia.org/wiki?curid=36961" title="Pion">
Pion

In particle physics, a pion (or a pi meson, denoted with the Greek letter pi: ) is any of three subatomic particles: , , and . Each pion consists of a quark and an antiquark and is therefore a meson. Pions are the lightest mesons (and, more generally, the lightest hadrons), because they are composed of the lightest quarks (the u and d quarks). They are unstable, with the charged pions and decaying with a mean lifetime of 26 nanoseconds ( seconds), and the neutral pion decaying with a much shorter lifetime of seconds. Charged pions most often decay into muons and muon neutrinos, and neutral pions into gamma rays.
The exchange of virtual pions, along with the vector, rho and omega mesons, provides an explanation for the residual strong force between nucleons. Pions are not produced in radioactive decay, but are produced commonly in high energy accelerators in collisions between hadrons. All types of pions are also produced in natural processes when high energy cosmic ray protons and other hadronic cosmic ray components interact with matter in the Earth's atmosphere. Recently, detection of characteristic gamma rays originating from decay of neutral pions in two supernova remnant stars has shown that pions are produced copiously in supernovas, most probably in conjunction with production of high energy protons that are detected on Earth as cosmic rays.
The concept of mesons as the carrier particles of the nuclear force was first proposed in 1935 by Hideki Yukawa. While the muon was first proposed to be this particle after its discovery in 1936, later work found that it did not participate in the strong nuclear interaction. The pions, which turned out to be examples of Yukawa's proposed mesons, were discovered later: the charged pions in 1947, and the neutral pion in 1950.
History.
Theoretical work by Hideki Yukawa in 1935 had predicted the existence of mesons as the carrier particles of the strong nuclear force. From the range of the strong nuclear force (inferred from the radius of the atomic nucleus), Yukawa predicted the existence of a particle having a mass of about 100 MeV. Initially after its discovery in 1936, the muon (initially called the "mu meson") was thought to be this particle, since it has a mass of 106 MeV. However, later particle physics experiments showed that the muon did not participate in the strong nuclear interaction. In modern terminology, this makes the muon a lepton, and not a true meson. However, some communities of nuclear physicists, continue to call the muon a "mu-meson."
In 1947, the first true mesons, the charged pions, were found by the collaboration of Cecil Powell, César Lattes, Giuseppe Occhialini, "et al.", at the University of Bristol, in England. Since the advent of particle accelerators had not yet come, high-energy subatomic particles were only obtainable from atmospheric cosmic rays. Photographic emulsions, which used the gelatin-silver process, were placed for long periods of time in sites located at high altitude mountains, first at Pic du Midi de Bigorre in the Pyrenees, and later at Chacaltaya in the Andes Mountains, where they were impacted by cosmic rays.
After the development of the photographic plates, microscopic inspection of the emulsions revealed the tracks of charged subatomic particles. Pions were first identified by their unusual "double meson" tracks, which were left by their decay into another "meson". (It was actually the muon, which is not classified as a meson in modern particle physics.) In 1948, Lattes, Eugene Gardner, and their team first artificially produced pions at the University of California's cyclotron in Berkeley, California, by bombarding carbon atoms with high-speed alpha particles. Further advanced theoretical work was carried out by Riazuddin, who in 1959, used the dispersion relation for Compton scattering of virtual photons on pions to analyze their charge radius.
Nobel Prizes in Physics were awarded to Yukawa in 1949 for his theoretical prediction of the existence of mesons, and to Cecil Powell in 1950 for developing and applying the technique of particle detection using photographic emulsions.
Since the neutral pion is not electrically charged, it is more difficult to detect and observe than the charged pions are. Neutral pions do not leave tracks in photographic emulsions, and neither do they in Wilson cloud chambers. The existence of the neutral pion was inferred from observing its decay products from cosmic rays, a so-called "soft component" of slow electrons with photons. The was identified definitively at the University of California's cyclotron in 1950 by observing its decay into two photons. Later in the same year, they were also observed in cosmic-ray balloon experiments at Bristol University.
The pion also plays a crucial role in cosmology, by imposing an upper limit on the energies of cosmic rays surviving collisions with the cosmic microwave background, through the Greisen–Zatsepin–Kuzmin limit.
In the standard understanding of the strong force interaction (called QCD, "quantum chromodynamics"), pions are understood to be the pseudo-Nambu-Goldstone bosons of spontaneously broken chiral symmetry. This explains why the three kinds of pions' masses are considerably less than the masses of the other mesons, such as the scalar or vector mesons. If their current quarks were massless particles, hypothetically, making the chiral symmetry exact, then the Goldstone theorem would dictate that all pions have zero masses. In reality, since the light quarks actually have minuscule nonzero masses, the pions also have nonzero rest masses, albeit "almost an order of magnitude smaller" than that of the nucleons, roughly "m"π ≈ √q / "f"π ≈ √q 45 MeV, where "m" are the relevant current quark masses in MeV, 5−10 MeVs.
The use of pions in medical radiation therapy, such as for cancer, was explored at a number of research institutions, including the Los Alamos National Laboratory's Meson Physics Facility, which treated 228 patients between 1974 and 1981 in New Mexico, and the TRIUMF laboratory in Vancouver, British Columbia.
Theoretical overview.
The pion can be thought of as one of the particles that mediate the interaction between a pair of nucleons. This interaction is attractive: it pulls the nucleons together. Written in a non-relativistic form, it is called the Yukawa potential. The pion, being spinless, has kinematics described by the Klein–Gordon equation. In the terms of quantum field theory, the effective field theory Lagrangian describing the pion-nucleon interaction is called the Yukawa interaction.
The nearly identical masses of and imply that there must be a symmetry at play; this symmetry is called the SU(2) flavour symmetry or isospin. The reason that there are three pions, , and , is that these are understood to belong to the triplet representation or the adjoint representation 3 of SU(2). By contrast, the up and down quarks transform according to the fundamental representation 2 of SU(2), whereas the anti-quarks transform according to the conjugate representation 2*.
With the addition of the strange quark, one can say that the pions participate in an SU(3) flavour symmetry, belonging to the adjoint representation 8 of SU(3). The other members of this octet are the four kaons and the eta meson.
Pions are pseudoscalars under a parity transformation. Pion currents thus couple to the axial vector current and pions participate in the chiral anomaly.
Basic properties.
Pions are mesons with zero spin, and they are composed of first-generation quarks. In the quark model, an up quark and an anti-down quark make up a , whereas a down quark and an anti-up quark make up the , and these are the antiparticles of one another. The neutral pion is a combination of an up quark with an anti-up quark or a down quark with an anti-down quark. The two combinations have identical quantum numbers, and hence they are only found in superpositions. The lowest-energy superposition of these is the , which is its own antiparticle. Together, the pions form a triplet of isospin. Each pion has isospin ("I" = 1) and third-component isospin equal to its charge ("I"z = +1, 0 or −1).
Charged pion decays.
The mesons have a mass of and a mean lifetime of . They decay due to the weak interaction. The primary decay mode of a pion, with probability 0.999877, is a purely leptonic decay into an anti-muon and a muon neutrino:
The second most common decay mode of a pion, with probability 0.000123, is also a leptonic decay into an electron and the corresponding electron antineutrino. This "electronic mode" was discovered at CERN in 1958:
The suppression of the electronic mode, with respect to the muonic one, is given approximately (to within radiative corrections) by the ratio of the half-widths of the pion–electron and the pion–muon decay reactions:
and is a spin effect known as the helicity suppression. Its mechanism is as follows: The negative pion has spin zero, therefore the lepton and antineutrino must be emitted with opposite spins (and opposite linear momenta) to preserve net zero spin (and conserve linear momentum). However, the antineutrino, due to very high speed, is always right-handed, so this implies that the lepton must be emitted with spin in the direction of its linear momentum (i.e., also right-handed). If, however, leptons were massless, they would only exist in the left-handed form, just as the neutrino does (due to parity violation), and this decay mode would be prohibited. Therefore, suppression of the electron decay channel comes from the fact that the electron's mass is much smaller than the muon's. The electron is thus relatively massless compared with the muon, and thus the electronic mode is "almost" prohibited.
Hence, electronic mode decay favors the left-handed symmetry and inhibits this decay channel. Measurements of the above ratio have been considered for decades to be tests of the "V − A structure" (vector minus axial vector or left-handed lagrangian) of the charged weak current and of lepton universality. Experimentally this ratio is .
Besides the purely leptonic decays of pions, some structure-dependent radiative leptonic decays (that is, decay to the usual leptons plus a gamma ray) have also been observed.
Also observed, for charged pions only, is the very rare "pion beta decay" (with probability of about 10−8) into a neutral pion plus an electron and electron antineutrino (or for positive pions, a neutral pion, positron, and electron neutrino).
The rate at which pions decay is a prominent quantity in many sub-fields of particle physics, such as chiral perturbation theory. This rate is parametrized by the pion decay constant ("ƒ"π), related to the wave function overlap of the quark and antiquark, which is about .
Neutral pion decays.
The meson has a mass of and a mean lifetime of . It decays via the electromagnetic force, which explains why its mean lifetime is much smaller than that of the charged pion (which can only decay via the weak force). The main π0 decay mode, with a branching ratio of BR=0.98823, is into two photons:
The decay π0→3γ (as well as decays into any odd number of photons) is forbidden by the C-symmetry of the electromagnetic interaction. The intrinsic C-parity of the π0 is +1, while the C-parity of a system of n photons is (−1)n.
The second largest π0 decay mode (BR=0.01174) is the Dalitz decay (named after Richard Dalitz), which is a two-photon decay with an internal photon conversion resulting a photon and an electron-positron pair in the final state:
The third largest established decay mode (BR=) is the double Dalitz decay, with both photons undergoing internal conversion which leads to further suppression of the rate:
The fourth largest established decay mode is the loop-induced and therefore suppressed (and additionally helicity-suppressed) leptonic decay mode (BR=):
The neutral pion has also been observed to decay into positronium with a branching fraction of the order of 10−9. No other decay modes have been established experimentally. The branching fractions above are the PDG central values, and their uncertainties are not quoted.
[a] Make-up inexact due to non-zero quark masses.

</doc>
<doc id="1062060" url="https://en.wikipedia.org/wiki?curid=1062060" title="Planck charge">
Planck charge

In physics, the Planck charge, denoted by formula_1, is one of the base units in the system of natural units called Planck units. It is a quantity of electric charge defined in terms of fundamental physical constants.
The Planck charge is defined as:
where:
The Planck charge is formula_8 times larger than the elementary charge "e" carried by an electron.
The Gaussian cgs units are defined so that formula_9, in which case formula_1 has the following simple form:
It is customary in theoretical physics to adopt the Lorentz–Heaviside units (also known as rationalized cgs). When made natural (formula_12) they are like the SI system with formula_13. Therefore it is more appropriate to define the Planck charge as
When charges are measured in units of formula_15, i.e., when formula_15 is set equal to 1, we obtain formula_17, which is commonly used in theoretical physics. In contrast, in (non-rationalized) natural cgs units where formula_18 we have formula_19.

</doc>
<doc id="19594213" url="https://en.wikipedia.org/wiki?curid=19594213" title="Planck constant">
Planck constant

The Planck constant (denoted , also called Planck's constant) is a physical constant that is the quantum of action, central in quantum mechanics.
First recognized in 1900 by Max Planck, it was originally the proportionality constant between the minimal increment of energy, , of a hypothetical electrically charged oscillator in a cavity that contained black body radiation, and the frequency, , of its associated electromagnetic wave. In 1905 the value , the minimal energy increment of a hypothetical oscillator, was theoretically associated by Einstein with a "quantum" or minimal element of the energy of the electromagnetic wave itself. The light quantum behaved in some respects as an electrically neutral particle, as opposed to an electromagnetic wave. It was eventually called the photon.
The Planck–Einstein relation connects the particulate photon energy with its associated wave frequency :
This energy is extremely small in terms of ordinary experience. Since the frequency , wavelength , and speed of light are related by formula_2, the relation can also be expressed as
This leads to another relationship involving the Planck constant. With denoting the linear momentum of a particle (not only a photon, but other fine particles as well), the de Broglie wavelength of the particle is given by
In applications where it is natural to use the angular frequency (i.e. where the frequency is expressed in terms of radians per second instead of cycles per second or hertz) it is often useful to absorb a factor of into the Planck constant. The resulting constant is called the reduced Planck constant or Dirac constant. It is equal to the Planck constant divided by , and is denoted (pronounced "h-bar"):
The energy of a photon with angular frequency , where , is given by
while its linear momentum relates to
where "k" is a wavenumber. In 1923, Louis de Broglie generalized the Planck–Einstein relation by postulating that the Planck constant represents the proportionality between the momentum and the quantum wavelength of not just the photon, but the quantum wavelength of any particle. This was confirmed by experiments soon afterwards. This holds throughout quantum theory, including electrodynamics.
These two relations are the temporal and spatial component parts of the special relativistic expression using 4-Vectors.
Classical statistical mechanics requires the existence of (but does not define its value). Eventually, following upon Planck's discovery, it was recognized that physical action cannot take on an arbitrary value. Instead, it must be some multiple of a very small quantity, the "quantum of action", now called the Planck constant. Classical physics cannot explain this fact. In many cases, such as for monochromatic light or for atoms, this quantum of action also implies that only certain energy levels are allowed, and values in between are forbidden.
Value.
The Planck constant has dimensions of physical action; these are the same as those of angular momentum, i.e., energy multiplied by time, or momentum multiplied by distance. In SI units, the Planck constant is expressed in joule seconds (J⋅s) or (N⋅m⋅s) or (kg⋅m2⋅s−1).
The value of the Planck constant is:
The value of the reduced Planck constant is:
The two digits inside the parentheses denote the standard uncertainty in the last two digits of the value. The figures cited here are the 2014 CODATA recommended values for the constants and their uncertainties. The 2014 CODATA results were made available in June 2015 and represent the best-known, internationally accepted values for these constants, based on all data published as of 31 December 2014. New CODATA figures are normally produced every four years. 
Significance of the value.
The Planck constant is related to the quantization of light and matter. It can be seen as a subatomic-scale constant. In a unit system adapted to subatomic scales, the electronvolt is the appropriate unit of energy and the petahertz the appropriate unit of frequency. Atomic unit systems are based (in part) on the Planck constant.
The numerical value of the Planck constant depends entirely on the system of units used to measure it. When it is expressed in SI units, it is one of the smallest constants used in physics. This reflects the fact that "on a scale adapted to humans", where energies are typically of the order of kilojoules and times are typically of the order of seconds or minutes, the Planck constant (the quantum of action) is very small.
Equivalently, the smallness of the Planck constant reflects the fact that everyday objects and systems are made of a "large" number of particles. For example, green light with a wavelength of 555 nanometres (the approximate wavelength to which human eyes are most sensitive) has a frequency of (). Each photon has an energy . That is a very small amount of energy in terms of everyday experience, but everyday experience is not concerned with individual photons any more than with individual atoms or molecules. An amount of light compatible with everyday experience is the energy of one mole of photons; its energy can be computed by multiplying the photon energy by the Avogadro constant, . The result is that green light of wavelength 555 nm has an energy of , a typical energy of everyday life.
Origins.
Black-body radiation.
In the last years of the nineteenth century, Planck was investigating the problem of black-body radiation first posed by Kirchhoff some forty years earlier. It is well known that hot objects glow, and that hotter objects glow brighter than cooler ones. The reason is that the electromagnetic field obeys laws of motion just like a mass on a spring, and can come to thermal equilibrium with hot atoms. When a hot object is in equilibrium with light, the amount of light it absorbs is equal to the amount of light it emits. If the object is black, meaning it absorbs all the light that hits it, then it emits the maximum amount of thermal light too.
The assumption that blackbody radiation is thermal leads to an accurate prediction: the total amount of emitted energy goes up with the temperature according to a definite rule, the Stefan–Boltzmann law (1879–84). But it was also known that the colour of the light given off by a hot object changes with the temperature, so that "white hot" is hotter than "red hot". Nevertheless, Wilhelm Wien discovered the mathematical relationship between the peaks of the curves at different temperatures, by using the principle of adiabatic invariance. At each different temperature, the curve is moved over by Wien's displacement law (1893). Wien also proposed an approximation for the spectrum of the object, which was correct at high frequencies (short wavelength) but not at low frequencies (long wavelength). It still was not clear "why" the spectrum of a hot object had the form that it has (see diagram).
Planck hypothesized that the equations of motion for light describe a set of harmonic oscillators, one for each possible frequency. He examined how the entropy of the oscillators varied with the temperature of the body, trying to match Wien's law, and was able to derive an approximate mathematical function for black-body spectrum.
However, Planck soon realized that his solution was not unique. There were several different solutions, each of which gave a different value for the entropy of the oscillators. To save his theory, Planck had to resort to using the then controversial theory of statistical mechanics, which he described as "an act of despair … I was ready to sacrifice any of my previous convictions about physics." One of his new boundary conditions was
With this new condition, Planck had imposed the quantization of the energy of the oscillators, "a purely formal assumption … actually I did not think much about it…" in his own words, but one which would revolutionize physics. Applying this new approach to Wien's displacement law showed that the "energy element" must be proportional to the frequency of the oscillator, the first version of what is now sometimes termed the "Planck–Einstein relation":
Planck was able to calculate the value of "h" from experimental data on black-body radiation: his result, , is within 1.2% of the currently accepted value. He was also able to make the first determination of the Boltzmann constant "k"B from the same data and theory.
Prior to Planck's work, it had been assumed that the energy of a body could take on any value whatsoever – that it was a continuous variable. The Rayleigh–Jeans law makes close predictions for a narrow range of values at one limit of temperatures, but the results diverge more and more strongly as temperatures increase. To make Planck's law, which correctly predicts blackbody emissions, it was necessary to multiply the classical expression by a complex factor that involves h in both the numerator and the denominator. The influence of h in this complex factor would not disappear if it were set to zero or to any other value. Making an equation out of Planck's law that would reproduce the Rayleigh–Jeans law could not be done by changing the values of h, of the Boltzmann constant, or of any other constant or variable in the equation. In this case the picture given by classical physics is not duplicated by a range of results in the quantum picture.
The black-body problem was revisited in 1905, when Rayleigh and Jeans (on the one hand) and Einstein (on the other hand) independently proved that classical electromagnetism could "never" account for the observed spectrum. These proofs are commonly known as the "ultraviolet catastrophe", a name coined by Paul Ehrenfest in 1911. They contributed greatly (along with Einstein's work on the photoelectric effect) to convincing physicists that Planck's postulate of quantized energy levels was more than a mere mathematical formalism. The very first Solvay Conference in 1911 was devoted to "the theory of radiation and quanta". Max Planck received the 1918 Nobel Prize in Physics "in recognition of the services he rendered to the advancement of Physics by his discovery of energy quanta".
Photoelectric effect.
The photoelectric effect is the emission of electrons (called "photoelectrons") from a surface when light is shone on it. It was first observed by Alexandre Edmond Becquerel in 1839, although credit is usually reserved for Heinrich Hertz, who published the first thorough investigation in 1887. Another particularly thorough investigation was published by Philipp Lenard in 1902. Einstein's 1905 paper discussing the effect in terms of light quanta would earn him the Nobel Prize in 1921, when his predictions had been confirmed by the experimental work of Robert Andrews Millikan. The Nobel committee awarded the prize for his work on the photo-electric effect, rather than relativity, both because of a bias against purely theoretical physics not grounded in discovery or experiment, and dissent amongst its members as to the actual proof that relativity was real.
Prior to Einstein's paper, electromagnetic radiation such as visible light was considered to behave as a wave: hence the use of the terms "frequency" and "wavelength" to characterise different types of radiation. The energy transferred by a wave in a given time is called its intensity. The light from a theatre spotlight is more "intense" than the light from a domestic lightbulb; that is to say that the spotlight gives out more energy per unit time and per unit space(and hence consumes more electricity) than the ordinary bulb, even though the colour of the light might be very similar. Other waves, such as sound or the waves crashing against a seafront, also have their own intensity. However, the energy account of the photoelectric effect didn't seem to agree with the wave description of light.
The "photoelectrons" emitted as a result of the photoelectric effect have a certain kinetic energy, which can be measured. This kinetic energy (for each photoelectron) is "independent" of the intensity of the light, but depends linearly on the frequency; and if the frequency is too low (corresponding to a photon energy that is less than the work function of the material), no photoelectrons are emitted at all, unless a plurality of photons, whose energetic sum is greater than the energy of the photoelectrons, acts virtually simultaneously (multiphoton effect) Assuming the frequency is high enough to cause the photoelectric effect, a rise in intensity of the light source causes more photoelectrons to be emitted with the same kinetic energy, rather than the same number of photoelectrons to be emitted with higher kinetic energy.
Einstein's explanation for these observations was that light itself is quantized; that the energy of light is not transferred continuously as in a classical wave, but only in small "packets" or quanta. The size of these "packets" of energy, which would later be named photons, was to be the same as Planck's "energy element", giving the modern version of the Planck–Einstein relation:
Einstein's postulate was later proven experimentally: the constant of proportionality between the frequency of incident light ("f") and the kinetic energy of photoelectrons ("E") was shown to be equal to the Planck constant ("h").
Atomic structure.
Niels Bohr introduced the first quantized model of the atom in 1913, in an attempt to overcome a major shortcoming of Rutherford's classical model. In classical electrodynamics, a charge moving in a circle should radiate electromagnetic radiation. If that charge were to be an electron orbiting a nucleus, the radiation would cause it to lose energy and spiral down into the nucleus. Bohr solved this paradox with explicit reference to Planck's work: an electron in a Bohr atom could only have certain defined energies "En"
where c0 is the speed of light in vacuum, "R"∞ is an experimentally determined constant (the Rydberg constant) and "n" is any integer ("n" = 1, 2, 3, …). Once the electron reached the lowest energy level (), it could not get any closer to the nucleus (lower energy). This approach also allowed Bohr to account for the Rydberg formula, an empirical description of the atomic spectrum of hydrogen, and to account for the value of the Rydberg constant "R"∞ in terms of other fundamental constants.
Bohr also introduced the quantity formula_14, now known as the reduced Planck constant, as the quantum of angular momentum. At first, Bohr thought that this was the angular momentum of each electron in an atom: this proved incorrect and, despite developments by Sommerfeld and others, an accurate description of the electron angular momentum proved beyond the Bohr model. The correct quantization rules for electrons – in which the energy reduces to the Bohr model equation in the case of the hydrogen atom – were given by Heisenberg's matrix mechanics in 1925 and the Schrödinger wave equation in 1926: the reduced Planck constant remains the fundamental quantum of angular momentum. In modern terms, if "J" is the total angular momentum of a system with rotational invariance, and "Jz" the angular momentum measured along any given direction, these quantities can only take on the values
Uncertainty principle.
The Planck constant also occurs in statements of Werner Heisenberg's uncertainty principle. Given a large number of particles prepared in the same state, the uncertainty in their position, Δ"x", and the uncertainty in their momentum (in the same direction), Δ"p", obey
where the uncertainty is given as the standard deviation of the measured value from its expected value. There are a number of other such pairs of physically measurable values which obey a similar rule. One example is time vs. energy. The either-or nature of uncertainty forces measurement attempts to choose between trade offs, and given that they are quanta, the trade offs often take the form of either-or (as in Fourier analysis), rather than the compromises and gray areas of time series analysis.
In addition to some assumptions underlying the interpretation of certain values in the quantum mechanical formulation, one of the fundamental cornerstones to the entire theory lies in the commutator relationship between the position operator formula_17 and the momentum operator formula_18:
where "δij" is the Kronecker delta.
Dependent physical constants.
There are several related constants for which more than 99% of the uncertainty in the 2014 CODATA values is due to the uncertainty in the value of the Planck constant, as indicated by the square of the correlation coefficient (). The Planck constant is (with one or two exceptions) the fundamental physical constant which is known to the lowest level of precision, with a 1"σ" relative uncertainty "u"r of .
Rest mass of the electron.
The normal textbook derivation of the Rydberg constant "R"∞ defines it in terms of the electron mass "m"e and a variety of other physical constants.
However, the Rydberg constant can be determined very accurately () from the atomic spectrum of hydrogen, whereas there is no direct method to measure the mass of a stationary electron in SI units. Hence the equation for the computation of "m"e becomes
where "c"0 is the speed of light and "α" is the fine-structure constant. The speed of light has an exactly defined value in SI units, and the fine-structure constant can be determined more accurately () than the Planck constant. Thus, the uncertainty in the value of the electron rest mass is due entirely to the uncertainty in the value of the Planck constant ().
Avogadro constant.
The Avogadro constant "N"A is determined as the ratio of the mass of one mole of electrons to the mass of a single electron; the mass of one mole of electrons is the "relative atomic mass" of an electron "A"r(e), which can be measured in a Penning trap (), multiplied by the molar mass constant "M"u, which is defined as .
The dependence of the Avogadro constant on the Planck constant () also holds for the physical constants which are related to amount of substance, such as the atomic mass constant. The uncertainty in the value of the Planck constant limits the knowledge of the masses of atoms and subatomic particles when expressed in SI units. It is possible to measure the masses more precisely in atomic mass units, but not to convert them more precisely into kilograms.
Elementary charge.
Sommerfeld originally defined the fine-structure constant "α" as:
where "e" is the elementary charge, "ε"0 is the electric constant (also called the permittivity of free space), and "μ"0 is the magnetic constant (also called the permeability of free space). The latter two constants have fixed values in the International System of Units. However, "α" can also be determined experimentally, notably by measuring the electron spin g-factor "g"e, then comparing the result with the value predicted by quantum electrodynamics.
At present, the most precise value for the elementary charge is obtained by rearranging the definition of "α" to obtain the following definition of "e" in terms of "α" and "h":
Bohr magneton and nuclear magneton.
The Bohr magneton and the nuclear magneton are units which are used to describe the magnetic properties of the electron and atomic nuclei respectively. The Bohr magneton is the magnetic moment which would be expected for an electron if it behaved as a spinning charge according to classical electrodynamics. It is defined in terms of the reduced Planck constant, the elementary charge and the electron mass, all of which depend on the Planck constant: the final dependence on "h"1/2 () can be found by expanding the variables.
The nuclear magneton has a similar definition, but corrected for the fact that the proton is much more massive than the electron. The ratio of the electron relative atomic mass to the proton relative atomic mass can be determined experimentally to a high level of precision ().
Determination.
In principle, the Planck constant could be determined by examining the spectrum of a black-body radiator or the kinetic energy of photoelectrons, and this is how its value was first calculated in the early twentieth century. In practice, these are no longer the most accurate methods. The CODATA value quoted here is based on three watt-balance measurements of "K"J2"R"K and one inter-laboratory determination of the molar volume of silicon, but is mostly determined by a 2007 watt-balance measurement made at the U.S. National Institute of Standards and Technology (NIST). Five other measurements by three different methods were initially considered, but not included in the final refinement as they were too imprecise to affect the result.
There are both practical and theoretical difficulties in determining "h". The practical difficulties can be illustrated by the fact that the two most accurate methods, the watt balance and the X-ray crystal density method, do not appear to agree with one another. The most likely reason is that the measurement uncertainty for one (or both) of the methods has been estimated too low – it is (or they are) not as precise as is currently believed – but for the time being there is no indication which method is at fault.
The theoretical difficulties arise from the fact that all of the methods "except" the X-ray crystal density method rely on the theoretical basis of the Josephson effect and the quantum Hall effect. If these theories are slightly inaccurate – though there is no evidence at present to suggest they are – the methods would not give accurate values for the Planck constant. More importantly, the values of the Planck constant obtained in this way cannot be used as tests of the theories without falling into a circular argument. Fortunately, there are other statistical ways of testing the theories, and the theories have yet to be refuted.
Josephson constant.
The Josephson constant "K"J relates the potential difference "U" generated by the Josephson effect at a "Josephson junction" with the frequency "ν" of the microwave radiation. The theoretical treatment of Josephson effect suggests very strongly that "K"J = 2"e"/"h".
The Josephson constant may be measured by comparing the potential difference generated by an array of Josephson junctions with a potential difference which is known in SI volts. The measurement of the potential difference in SI units is done by allowing an electrostatic force to cancel out a measurable gravitational force. Assuming the validity of the theoretical treatment of the Josephson effect, "K"J is related to the Planck constant by
Watt balance.
A watt balance is an instrument for comparing two powers, one of which is measured in SI watts and the other of which is measured in conventional electrical units. From the definition of the "conventional" watt "W"90, this gives a measure of the product "K"J2"R"K in SI units, where "R"K is the von Klitzing constant which appears in the quantum Hall effect. If the theoretical treatments of the Josephson effect and the quantum Hall effect are valid, and in particular assuming that "R"K = "h"/"e"2, the measurement of "K"J2"R"K is a direct determination of the Planck constant.
Magnetic resonance.
The gyromagnetic ratio "γ" is the constant of proportionality between the frequency "ν" of nuclear magnetic resonance (or electron paramagnetic resonance for electrons) and the applied magnetic field . It is difficult to measure gyromagnetic ratios precisely because of the difficulties in precisely measuring "B", but the value for protons in water at is known to better than one part per million. The protons are said to be "shielded" from the applied magnetic field by the electrons in the water molecule, the same effect that gives rise to chemical shift in NMR spectroscopy, and this is indicated by a prime on the symbol for the gyromagnetic ratio, "γ"′p. The gyromagnetic ratio is related to the shielded proton magnetic moment "μ"′p, the spin number "I" ( for protons) and the reduced Planck constant.
The ratio of the shielded proton magnetic moment "μ"′p to the electron magnetic moment "μ"e can be measured separately and to high precision, as the imprecisely known value of the applied magnetic field cancels itself out in taking the ratio. The value of "μ"e in Bohr magnetons is also known: it is half the electron g-factor "g"e. Hence
A further complication is that the measurement of "γ"′p involves the measurement of an electric current: this is invariably measured in "conventional" amperes rather than in SI amperes, so a conversion factor is required. The symbol "Γ"′p-90 is used for the measured gyromagnetic ratio using conventional electrical units. In addition, there are two methods of measuring the value, a "low-field" method and a "high-field" method, and the conversion factors are different in the two cases. Only the high-field value "Γ"′p-90(hi) is of interest in determining the Planck constant.
Substitution gives the expression for the Planck constant in terms of "Γ"′p-90(hi):
Faraday constant.
The Faraday constant "F" is the charge of one mole of electrons, equal to the Avogadro constant "N"A multiplied by the elementary charge "e". It can be determined by careful electrolysis experiments, measuring the amount of silver dissolved from an electrode in a given time and for a given electric current. In practice, it is measured in conventional electrical units, and so given the symbol "F"90. Substituting the definitions of "N"A and "e", and converting from conventional electrical units to SI units, gives the relation to the Planck constant.
X-ray crystal density.
The X-ray crystal density method is primarily a method for determining the Avogadro constant "N"A but as the Avogadro constant is related to the Planck constant it also determines a value for "h". The principle behind the method is to determine "N"A as the ratio between the volume of the unit cell of a crystal, measured by X-ray crystallography, and the molar volume of the substance. Crystals of silicon are used, as they are available in high quality and purity by the technology developed for the semiconductor industry. The unit cell volume is calculated from the spacing between two crystal planes referred to as "d"220. The molar volume "V"m(Si) requires a knowledge of the density of the crystal and the atomic weight of the silicon used. The Planck constant is given by
Particle accelerator.
The experimental measurement of the Planck constant in the Large Hadron Collider laboratory was carried out in 2011. The study called PCC using a giant particle accelerator helped to better understand the relationships between the Planck constant and measuring distances in space.
Fixation.
As mentioned above, the numerical value of the Planck constant depends on the system of units used to describe it. Its value in SI units is known to 50 parts per billion but its value in atomic units is known "exactly", because of the way the scale of atomic units is defined. The same is true of conventional electrical units, where the Planck constant (denoted "h"90 to distinguish it from its value in SI units) is given by
with "K"J–90 and "R"K–90 being exactly defined constants. Atomic units and conventional electrical units are very useful in their respective fields, because the uncertainty in the final result does not depend on an uncertain conversion factor, only on the uncertainty of the measurement itself.
There are a number of proposals to redefine certain of the SI base units in terms of fundamental physical constants. This has already been done for the metre, which is defined in terms of a fixed value of the speed of light. The most urgent unit on the list for redefinition is the kilogram, whose value has been fixed for all science (since 1889) by the mass of a small cylinder of platinum–iridium alloy kept in a vault just outside Paris. While nobody knows if the mass of the International Prototype Kilogram has changed since 1889 – the value 1 kg of its mass expressed in kilograms is by definition unchanged and therein lies one of the problems – it is known that over such a timescale the many similar Pt–Ir alloy cylinders kept in national laboratories around the world, have changed their relative mass by several tens of parts per million, however carefully they are stored, and the more so the more they have been taken out and used as mass standards. A change of several tens of micrograms in one kilogram is equivalent to the current uncertainty in the value of the Planck constant in SI units.
The legal process to change the definition of the kilogram is already underway, but it had been decided that no final decision would be made before the next meeting of the General Conference on Weights and Measures in 2011. (For more detailed information, see kilogram definitions.) The Planck constant is a leading contender to form the basis of the new definition, although not the only one. Possible new definitions include "the mass of a body at rest whose equivalent energy equals the energy of photons whose frequencies sum to ", or simply "the kilogram is defined so that the Planck constant equals ".
The BIPM provided "Draft Resolution A" in anticipation of the 24th General Conference on Weights and Measures meeting (2011-10-17 through 2011-10-21), detailing the considerations "On the possible future revision of the International System of Units, the SI".
Watt balances already measure mass in terms of the Planck constant: at present, standard mass is taken as fixed and the measurement is performed to determine the Planck constant but, were the Planck constant to be fixed in SI units, the same experiment would be a measurement of the mass. The relative uncertainty in the measurement would remain the same.
Mass standards could also be constructed from silicon crystals or by other atom-counting methods. Such methods require a knowledge of the Avogadro constant, which fixes the proportionality between atomic mass and macroscopic mass but, with a defined value of the Planck constant, "N"A would be known to the same level of uncertainty (if not better) than current methods of comparing macroscopic mass.

</doc>
<doc id="701107" url="https://en.wikipedia.org/wiki?curid=701107" title="Planck energy">
Planck energy

In physics, Planck energy, denoted by , is the unit of energy in the system of natural units known as Planck units.
where is the speed of light in a vacuum, is the reduced Planck's constant, and is the gravitational constant. is a "derived", as opposed to "basic", Planck unit.
An equivalent definition is:
where is the Planck time.
Also:
where is the Planck mass.
The ultra-high-energy cosmic rays observed in 1991 had a measured energy of about 50 joules, equivalent to about 2.5×10−8 . Most Planck units are fantastically small and thus are unrelated to "macroscopic" phenomena (or fantastically large, as in the case of Planck density). One , on the other hand, is definitely macroscopic, approximately equaling the energy stored in an automobile gas tank (57.2 L of gasoline at 34.2 MJ/L of chemical energy).
Planck units are designed to normalize the physical constants , and to 1. Hence given Planck units, the mass-energy equivalence simplifies to , so that the Planck energy and mass are numerically identical. In the equations of general relativity, is often multiplied by 8π. Hence writings in particle physics and physical cosmology often normalize to 1. This normalization results in the reduced Planck energy, defined as:

</doc>
<doc id="2196884" url="https://en.wikipedia.org/wiki?curid=2196884" title="Planck force">
Planck force

Planck force is the derived unit of force resulting from the definition of the base Planck units for time, length, and mass. It is equal to the natural unit of momentum divided by the natural unit of time.
Other derivations.
The Planck force is also associated with the equivalence of gravitational potential energy and electromagnetic energy and in this context it can be understood as the force that confines a self-gravitating mass to half its Schwarzschild radius:
where "G" is the gravitational constant, "c" is the speed of light, "m" is any mass and "r"G is half the Schwarzschild radius, "r"s, of the given mass.
Since the dimension of force is also a ratio of energy per length, the Planck force can be calculated as energy divided by half the Schwarzschild radius:
As mentioned above, Planck force has a unique association with the Planck mass. This unique association also manifests itself when force is calculated as any energy divided by the reduced Compton wavelength (reduced by 2π) of that same energy:
Here the force is different for every mass (for the electron, for example, the force is responsible for the Schwinger effect (see page 3 here ). It is Planck force only for the Planck mass (approximately 2.18 × 10−8 kg). This follows from the fact that the Planck length is a reduced Compton wavelength equal to half the Schwarzschild radius of the Planck mass:
which in turn follows from another relation of fundamental significance:
General relativity.
Planck force is often useful in scientific calculations as a ratio of electromagnetic energy per gravitational length. Thus for example it appears in the Einstein field equations, describing the properties of a gravitational field surrounding any given mass:
where formula_9 is the Einstein tensor and formula_10 is the energy–momentum tensor.

</doc>
<doc id="748908" url="https://en.wikipedia.org/wiki?curid=748908" title="Planck length">
Planck length

In physics, the Planck length, denoted , is a unit of length, equal to metres. It is a base unit in the system of Planck units, developed by physicist Max Planck. The Planck length can be defined from three fundamental physical constants: the speed of light in a vacuum, the Planck constant, and the gravitational constant.
Value.
The Planck length is defined as
where formula_2 is the speed of light in a vacuum, is the gravitational constant, and is the reduced Planck constant. The two digits enclosed by parentheses are the estimated standard error associated with the reported numerical value.
The Planck length is about 10−20 times the diameter of a proton.
Theoretical significance.
There is currently no proven physical significance of the Planck length; it is, however, a topic of theoretical research. Since the Planck length is so many orders of magnitude smaller than any current instrument could possibly measure, there is no way of examining it directly. According to the generalized uncertainty principle (a concept from speculative models of quantum gravity), the Planck length is, in principle, within a factor of 10, the shortest measurable length – and no theoretically known improvement in measurement instruments could change that.
In some forms of quantum gravity, the Planck length is the length scale at which the structure of spacetime becomes dominated by quantum effects, and it is impossible to determine the difference between two locations less than one Planck length apart. The precise effects of quantum gravity are unknown; it is often guessed that spacetime might have a discrete or foamy structure at a Planck length scale.
The Planck area, equal to the square of the Planck length, plays a role in black hole entropy. The value of this entropy, in units of the Boltzmann constant, is known to be given by formula_3, where is the area of the event horizon. The Planck area is the area by which the surface of a spherical black hole increases when the black hole swallows one bit of information, as was proven by Jacob Bekenstein.
If large extra dimensions exist, the measured strength of gravity may be much smaller than its true (small-scale) value. In this case the Planck length would have no fundamental physical significance, and quantum gravitational effects would appear at other scales.
In string theory, the Planck length is the order of magnitude of the oscillating strings that form elementary particles, and shorter lengths do not make physical sense. The string scale is related to the Planck scale by , where is the string coupling constant. Contrary to what the name suggests, the string coupling constant is not constant, but depends on the value of a scalar field known as the dilaton.
In loop quantum gravity, area is quantized, and the Planck area is, within a factor of 10, the smallest possible area value.
In doubly special relativity, the Planck length is observer-invariant.
The search for the laws of physics valid at the Planck length is a part of the search for the theory of everything.
Visualization.
The size of the Planck length can be visualized as follows: if a particle or dot about 0.1 mm in size (which is approximately the smallest the unaided human eye can see) were magnified in size to be as large as the observable universe, then inside that universe-sized "dot", the Planck length would be roughly the size of an actual 0.1 mm dot. In other words, a 0.1 mm dot is halfway between the Planck length and the size of the observable universe on a logarithmic scale.

</doc>
<doc id="171541" url="https://en.wikipedia.org/wiki?curid=171541" title="Planck mass">
Planck mass

In physics, the Planck mass, denoted by "m"P, is the unit of mass in the system of natural units known as Planck units. It is defined so that
where "c" is the speed of light in a vacuum, "G" is the gravitational constant, and "ħ" is the reduced Planck constant.
Particle physicists and cosmologists often use an alternative normalization with the reduced Planck mass, which is
The factor of formula_3 simplifies a number of equations in general relativity.
Significance.
The Planck mass is nature’s maximum allowed mass for point-masses (quanta) – in other words, a mass capable of holding a single elementary charge. If two quanta of the Planck mass or greater met, they could spontaneously form a black hole whose Schwarzschild radius equals their Compton wavelength. Once such a hole formed, other particles would fall in, and the black hole would experience runaway, explosive growth (assuming it did not evaporate via Hawking radiation). Nature’s stable point-mass particles, such as electrons and quarks, are many, many orders of magnitude lighter than the Planck mass and cannot form black holes in this manner. On the other hand, extended objects (as opposed to point-masses) can have any mass.
Unlike all other Planck base units and most Planck derived units, the Planck mass has a scale more or less conceivable to humans. It is traditionally said to be about the mass of a flea, but more accurately it is about the mass of a flea egg.
Derivations.
Dimensional analysis.
The formula for the Planck mass can be derived by dimensional analysis. In this approach, one starts with the three physical constants "ħ", "c", and "G", and attempt to combine them to get a quantity with units of mass. The expected formula is of the form
where formula_5 are constants to be determined by matching the dimensions of both sides. Using the symbol L for length, T for time, M for mass, and writing "[x]" for the dimensions of some physical quantity x, we have the following:
Therefore,
If one wants dimensions of mass, the following equations must hold:
The solution of this system is:
Thus, the Planck mass is:
Elimination of a coupling constant.
Equivalently, the Planck mass is defined such that the gravitational potential energy between two masses "m"P of separation "r" is equal to the energy of a photon (or graviton) of angular wavelength "r" (see the Planck relation), or that their ratio equals one.
Isolating "m"P, we get that
Note that if, instead of Planck masses, the electron mass were used, the equation would require a gravitational coupling constant, analogous to how the equation of the fine-structure constant relates the elementary charge and the Planck charge. Thus, the Planck mass can be viewed as resulting from absorbing the gravitational coupling constant into the unit of mass (and those of distance/time as well), like the Planck charge does for the fine-structure constant.
Compton wavelength and Schwarzschild radius.
The Planck mass can be derived approximately by setting it as the mass whose Compton wavelength and Schwarzschild radius are equal. The Compton wavelength is, loosely speaking, the length-scale where quantum effects start to become important for a particle; the heavier the particle, the smaller the Compton wavelength. The Schwarzschild radius is the radius in which a mass, if it were a black hole, would have its event horizon located; the heavier the particle, the larger the Schwarzschild radius. If a particle were massive enough that its Compton wavelength and Schwarzschild radius were approximately equal, its dynamics would be strongly affected by quantum gravity. This mass is (approximately) the Planck mass.
The Compton wavelength is
and the Schwarzschild radius is
Setting them equal:
This is not quite the Planck mass: It is a factor of formula_20 larger. However, this heuristic derivation gives the right order of magnitude.

</doc>
<doc id="9130962" url="https://en.wikipedia.org/wiki?curid=9130962" title="Planck momentum">
Planck momentum

Planck momentum is the unit of momentum in the system of natural units known as Planck units. It has no commonly used symbol of its own, but can be denoted by formula_1, where formula_2 is the Planck mass and formula_3 is the speed of light in a vacuum. Then
where
In SI units Planck momentum is approximately 6.5 kg·m/s. It is equal to the Planck mass multiplied by the speed of light, usually associated with the momentum of primordial photons in some prevailing Big Bang models. Unlike most of the other Planck units, Planck momentum occurs on a human scale. By comparison, running with a five-pound object (108×Planck mass) at an average running speed (10−8×speed of light in a vacuum) would give the object Planck momentum. A 70 kg human moving at an average walking speed of would have a momentum of about 15 formula_1. A baseball, which has mass formula_9 0.145 kg, travelling at would have a Planck momentum.

</doc>
<doc id="1600996" url="https://en.wikipedia.org/wiki?curid=1600996" title="Planck particle">
Planck particle

A Planck particle, named after physicist Max Planck, is a hypothetical particle defined as a tiny black hole whose Compton wavelength is equal to its Schwarzschild radius. Its mass is thus approximately the Planck mass, and its Compton wavelength and Schwarzschild radius are about the Planck length. Planck particles are sometimes used as an exercise to define the Planck mass and Planck length. They play a role in some models of the evolution of the universe during the Planck epoch.
Compared to a proton, for example, the Planck particle would be extremely small (its radius being equal to the Planck length, which is about 10−20 times the proton's radius) and heavy (the Planck mass being 1019 times the proton's mass).
It is thought that such a particle would vanish in Hawking radiation.
Derivation.
While opinions vary as to its proper definition, the most common definition of a Planck particle is a particle whose Compton wavelength is equal to its Schwarzschild radius. This sets the relationship:
Thus making the mass of such a particle:
This mass will be formula_3 times larger than the Planck mass, making a Planck particle 1.772 times more massive than the Planck unit mass.
Its radius will be the Compton wavelength:
Dimensions.
Using the above derivations we can substitute the universal constants "h", "G", and "c", and determine physical values for the particle's mass and radius. Assuming this radius represents a sphere of uniform density we can further determine the particle's volume and density.
It should be noted that the above dimensions do not correspond to any known physical entity or material.

</doc>
<doc id="171537" url="https://en.wikipedia.org/wiki?curid=171537" title="Planck temperature">
Planck temperature

Planck temperature, denoted by "T"P, is the unit of temperature in the system of natural units known as Planck units.
It serves as the defining unit of the Planck temperature scale. In this scale the magnitude of the Planck temperature is equal to 1, while that of absolute zero is 0. Unlike Planck length and Planck time, which are defined as the smallest possible measurable units of length and time, Planck temperature is defined as the "largest" possible measurable unit of temperature.
Other temperatures can be converted to Planck temperature units. For example, 0 °C = 273.15 K = 1.9279 × 10−30"T"P.
The Planck temperature in more common units of temperature is about 2.550297534712×1032 degrees Fahrenheit (255 nonillion, 29 octillion, 753 septillion, 471.2 sextillion degrees Fahrenheit), and 1.416831963729×1032 degrees Celsius (141 nonillion, 683 octillion, 196 septillion, 372.9 sextillion degrees Celsius).
Definition.
The Planck temperature is defined as:
formula_1 = 
1.416833(85) × 1032 K
where:
The two digits between the parentheses are used to denote the standard error of the last two digits of the estimated value.
Significance.
As for most of Planck units, a Planck temperature of 1 (unity) is a fundamental limit of quantum theory, in combination with gravitation, as presently understood. In other words, the wavelength of an object can be calculated by its temperature. If an object were to reach the temperature of 1.41 x 1032 Kelvin ("T"P), the radiation it would emit would have a wavelength of 1.616 x 10−26 nanometers (Planck length), at which point quantum gravitational effects become relevant. At temperatures greater than or equal to "T"P, current physical theory breaks down because we lack a theory of quantum gravity.

</doc>
<doc id="174386" url="https://en.wikipedia.org/wiki?curid=174386" title="Planck time">
Planck time

In physics, the Planck time () is the unit of time in the system of natural units known as Planck units. It is the time required for light to travel, in a vacuum, a distance of 1 Planck length. The unit is named after Max Planck, who was the first to propose it.
The Planck time is defined as:
where:
The two digits between parentheses denote the standard error of the estimated value.
Physical significance.
The Planck time is the unique combination of the gravitational constant , the special-relativistic constant , and the quantum constant , to produce a constant with units of time. Because the Planck time comes from dimensional analysis, which ignores constant factors, there is no reason to believe that exactly one unit of Planck time has any special physical significance. Rather, the Planck time represents a rough time scale at which quantum gravitational effects are likely to become important. The nature of those effects, and the exact time scale at which they would occur, would need to be derived from an actual theory of quantum gravity. All scientific experiments and human experiences happen over billions of billions of Planck times, making any events happening at the Planck scale hard to detect. , the smallest time interval uncertainty in direct measurements is on the order of 12 attoseconds (1.2 × 10−17 seconds), about 2.2 × 1026 Planck times.

</doc>
<doc id="33710707" url="https://en.wikipedia.org/wiki?curid=33710707" title="Planck units">
Planck units

In physics, Planck units are physical units of measurement defined exclusively in terms of five universal physical constants listed below, in such a manner that these five physical constants take on the numerical value of 1 when expressed in terms of these units. Planck units have profound significance for theoretical physics since they elegantly simplify several recurring algebraic expressions of physical law by nondimensionalization. They are particularly relevant in research on unified theories such as quantum gravity.
Overview.
Originally proposed in 1899 by German physicist Max Planck, these units are also known as natural units because the origin of their definition comes only from properties of nature and not from any human construct. Planck units are only one system of several systems of natural units, but these units are not based on properties of any prototype object or particle (that would be arbitrarily chosen), but rather on only the properties of free space.
The universal constants that Planck units, by definition, normalize to 1 are:
Each of these constants can be associated with at least one fundamental physical theory: "c" with special relativity and electromagnetism, "G" with general relativity and Newtonian gravity, "ħ" with quantum mechanics, "ε"0 with electrostatics, and "k"B with statistical mechanics and thermodynamics.
Planck units are free of anthropocentric arbitrariness. Some physicists argue that communication with extraterrestrial intelligence would have to employ such a system of units in order to be understood. Unlike the metre and second, which exist as base units in the SI system for historical reasons, the Planck length and Planck time are conceptually linked at a fundamental physical level.
Natural units help physicists to reframe questions. Frank Wilczek puts it succinctly:
While it is true that the electrostatic repulsive force between two protons (alone in free space) greatly exceeds the gravitational attractive force between the same two protons, this is not about the relative strengths of the two fundamental forces. From the point of view of Planck units, this is comparing apples to oranges, because mass and electric charge are incommensurable quantities. Rather, the disparity of magnitude of force is a manifestation of the fact that the charge on the protons is approximately the unit charge but the mass of the protons is far less than the unit mass.
Base units.
All systems of measurement feature base units: in the International System of Units (SI), for example, the base unit of length is the metre. In the system of Planck units, the Planck base unit of length is known simply as the Planck length, the base unit of time is the Planck time, and so on. These units are derived from the five dimensional universal physical constants of Table 1, in such a manner that these constants are eliminated from fundamental equations of physical law when physical quantities are expressed in terms of Planck units. For example, Newton's law of universal gravitation,
can be expressed as
Both equations are dimensionally consistent and equally valid in "any" system of units, but the second equation, with "G" missing, is relating only dimensionless quantities since any ratio of two like-dimensioned quantities is a dimensionless quantity. If, by a shorthand convention, it is axiomatically understood that all physical quantities are expressed in terms of Planck units, the ratios above may be expressed simply with the symbols of physical quantity, without being scaled by their corresponding unit:
In order for this last equation to be valid (without "G" present), "F", "m"1, "m"2, and "r" are understood to be the dimensionless numerical values of these quantities measured in terms of Planck units. This is why Planck units or any other use of natural units should be employed with care; referring to , Paul S. Wesson wrote that, "Mathematically it is an acceptable trick which saves labour. Physically it represents a loss of information and can lead to confusion."
Key: L = length, M = mass, T = time, Q = electric charge, Θ = temperature.
As can be seen above, the gravitational attractive force of two bodies of 1 Planck mass each, set apart by 1 Planck length is 1 Planck force. Likewise, the distance traveled by light during 1 Planck time is 1 Planck length. To determine, in terms of SI or another existing system of units, the quantitative values of the five base Planck units, those two equations and three others must be satisfied to determine the five unknown quantities that define the base Planck units:
Solving the five equations above for the five unknowns results in a unique set of values for the five base Planck units:
Table 2: Base Planck units
Derived units.
In any system of measurement, units for many physical quantities can be derived from base units. Table 3 offers a sample of derived Planck units, some of which in fact are seldom used. As with the base units, their use is mostly confined to theoretical physics because most of them are too large or too small for empirical or practical use and there are large uncertainties in their values (see "Discussion" and "Uncertainties in values" below).
Simplification of physical equations.
Physical quantities that have different dimensions (such as time and length) cannot be equated even if they are numerically equal (1 second is not the same as 1 metre). In theoretical physics, however, this scruple can be set aside, by a process called nondimensionalization. Table 4 shows how the use of Planck units simplifies many fundamental equations of physics, because this gives each of the five fundamental constants, and products of them, a simple numeric value of 1. In the SI form, the units should be accounted for. In the nondimensionalized form, the units, which are now Planck units, need not be written if their use is understood.
Other possible normalizations.
As already stated above, Planck units are derived by "normalizing" the numerical values of certain fundamental constants to 1. These normalizations are neither the only ones possible nor necessarily the best. Moreover, the choice of what factors to normalize, among the factors appearing in the fundamental equations of physics, is not evident, and the values of the Planck units are sensitive to this choice.
There are several possible alternative normalizations.
Gravity.
In 1899, Newton's law of universal gravitation was still seen as exact, rather than as a convenient approximation holding for "small" velocities and masses (the approximate nature of Newton's law was shown following the development of general relativity in 1915). Hence Planck normalized to 1 the gravitational constant "G" in Newton's law. In theories emerging after 1899, "G" nearly always appears in formulae multiplied by 4π or a small integer multiple thereof. Hence, a choice to be made when designing a system of natural units is which, if any, instances of 4π appearing in the equations of physics are to be eliminated via the normalization.
Electromagnetism.
Planck normalized to 1 the Coulomb force constant 1/(4π"ε"0) (as does the cgs system of units). This sets the Planck impedance, "Z"P equal to "Z"0/4π, where "Z"0 is the characteristic impedance of free space.
Temperature.
Planck normalized to 1 the Boltzmann constant "k"B.
The factor 4π is ubiquitous in theoretical physics because the surface area of a sphere is 4π"r"2. This, along with the concept of flux is the basis for the inverse-square law. For example, gravitational and electrostatic fields produced by point charges have spherical symmetry (Barrow 2002: 214–15). The 4π"r"2 appearing in the denominator of Coulomb's law, for example, follows from the flux of an electrostatic field being distributed uniformly on the surface of a sphere. If space had more than three spacial dimensions, the factor 4π would have to be changed according to the geometry of the sphere in higher dimensions. Likewise for Newton's law of universal gravitation.
Hence a substantial body of physical theory discovered since Planck (1899) suggests normalizing to 1 not "G" but 4"n"π"G", for one of "n" = 1, 2, or 4. Doing so would introduce a factor of 1/(4"n"π) into the nondimensionalized form of the law of universal gravitation, consistent with the modern formulation of Coulomb's law in terms of the vacuum permittivity. In fact, alternative normalizations frequently preserve the factor of 1/(4π) in the nondimensionalized form of Coulomb's law as well, so that the nondimensionalized Maxwell's equations for electromagnetism and gravitomagnetism both take the same form as those for electromagnetism in SI, which does not have any factors of 4π.
Uncertainties in measured values.
Table 2 clearly defines Planck units in terms of the fundamental constants. Yet relative to other units of measurement such as SI, the values of the Planck units are only known "approximately." This is mostly due to uncertainty in the value of the gravitational constant "G".
Today the value of the speed of light "c" in SI units is not subject to measurement error, because the SI base unit of length, the metre, is now "defined" as the length of the path travelled by light in vacuum during a time interval of of a second. Hence the value of "c" is now exact by definition, and contributes no uncertainty to the SI equivalents of the Planck units. The same is true of the value of the vacuum permittivity "ε"0, due to the definition of ampere which sets the vacuum permeability "μ"0 to and the fact that "μ"0"ε"0 = 1/"c"2. The numerical value of the reduced Planck constant ℏ has been determined experimentally to 44 parts per billion, while that of "G" has been determined experimentally to no better than 1 part in 8300 (or 120000 parts per billion). "G" appears in the definition of almost every Planck unit in Tables 2 and 3. Hence the uncertainty in the values of the Table 2 and 3 SI equivalents of the Planck units derives almost entirely from uncertainty in the value of "G". (The propagation of the error in "G" is a function of the exponent of "G" in the algebraic expression for a unit. Since that exponent is ± for every base unit other than Planck charge, the relative uncertainty of each base unit is about one half that of "G". This is indeed the case; according to CODATA, the experimental values of the SI equivalents of the base Planck units are known to about 1 part in 16600, or 60000 parts per billion.)
Discussion.
Some Planck units are suitable for measuring quantities that are familiar from daily experience. For example:
The charge, as other Planck units, was not originally defined by Planck. It is a unit of charge that is a natural addition to the other units of Planck, and is used in some publications. The elementary charge, measured in terms of the Planck charge, is
where formula_10 is the fine-structure constant
However, most Planck units are many orders of magnitude too large or too small to be of practical use, so that Planck units as a system are really only relevant to theoretical physics. In fact, 1 Planck unit is often the largest or smallest value of a physical quantity that makes sense according to our current understanding. For example:
Relative to the Planck Epoch, the universe today looks extreme when expressed in Planck units, as in this set of approximations:
The recurrence of large numbers close or related to 1060 in the above table is a coincidence that intrigues some theorists. It is an example of the kind of large numbers coincidence that led theorists such as Eddington and Dirac to develop alternative physical theories. Theories derived from such coincidences have sometimes been dismissed by mainstream physicists as "numerology".
History.
Natural units began in 1881, when George Johnstone Stoney, noting that electric charge is quantized, derived units of length, time, and mass, now named Stoney units in his honor, by normalizing "G", "c", and the electron charge, "e", to 1. In 1898, Max Planck discovered that action is quantized, and published the result in a paper presented to the Prussian Academy of Sciences in May 1899. At the end of the paper, Planck introduced, as a consequence of his discovery, the base units later named in his honor. The Planck units are based on the quantum of action, now usually known as Planck's constant. Planck called the constant "b" in his paper, though "h" is now common. Planck underlined the universality of the new unit system, writing:
Planck considered only the units based on the universal constants "G", "ħ", "c", and "k"B to arrive at natural units for length, time, mass, and temperature. Planck did not adopt any electromagnetic units. However, since the non-rationalized gravitational constant, "G", is set to 1, a natural extension of Planck units to a unit of electric charge is to also set the non-rationalized Coulomb constant, "k"e, to 1 as well.
 Another convention is to use the elementary charge as the basic unit of electric charge in Planck system. Such system is convenient for black hole physics. The two conventions for unit charge differ by a factor of the square root of the Fine-structure constant. Planck's paper also gave numerical values for the base units that were close to modern values.
Planck units and the invariant scaling of nature.
Some theorists (such as Dirac and Milne) have proposed cosmologies that conjecture that physical "constants" might actually change over time (e.g. a variable speed of light or Dirac varying-"G" theory). Such cosmologies have not gained mainstream acceptance and yet there is still considerable scientific interest in the possibility that physical "constants" might change, although such propositions introduce difficult questions. Perhaps the first question to address is: How would such a change make a noticeable operational difference in physical measurement or, more fundamentally, our perception of reality? If some particular physical constant had changed, how would we notice it, how would physical reality be different? Which changed constants result in a meaningful and measurable difference in physical reality? If a physical constant that is not dimensionless, such as the speed of light, "did" in fact change, would we be able to notice it or measure it unambiguously? – a question examined by Michael Duff in his paper "Comment on time-variation of fundamental constants".
George Gamow argued in his book "Mr Tompkins in Wonderland" that a sufficient change in a dimensionful physical constant, such as the speed of light in a vacuum, would result in obvious perceptible changes. But this idea is challenged:
Referring to Duff's "Comment on time-variation of fundamental constants" and Duff, Okun, and Veneziano's paper "Trialogue on the number of fundamental constants", particularly the section entitled "The operationally indistinguishable world of Mr. Tompkins", if all physical quantities (masses and other properties of particles) were expressed in terms of Planck units, those quantities would be dimensionless numbers (mass divided by the Planck mass, length divided by the Planck length, etc.) and the only quantities that we ultimately measure in physical experiments or in our perception of reality are dimensionless numbers. When one commonly measures a length with a ruler or tape-measure, that person is actually counting tick marks on a given standard or is measuring the length relative to that given standard, which is a dimensionless value. It is no different for physical experiments, as all physical quantities are measured relative to some other like-dimensioned quantity.
We can notice a difference if some dimensionless physical quantity such as fine-structure constant, "α", changes or the proton-to-electron mass ratio, "m"p/"m"e, changes (atomic structures would change) but if all dimensionless physical quantities remained unchanged (this includes all possible ratios of identically dimensioned physical quantity), we can not tell if a dimensionful quantity, such as the speed of light, "c", has changed. And, indeed, the Tompkins concept becomes meaningless in our perception of reality if a dimensional quantity such as "c" has changed, even drastically.
If the speed of light "c", were somehow suddenly cut in half and changed to "c"/2, (but with the axiom that "all" dimensionless physical quantities remain the same), then the Planck length would "increase" by a factor of formula_12 from the point of view of some unaffected observer on the outside. Measured by "mortal" observers in terms of Planck units, the new speed of light would remain as 1 new Planck length per 1 new Planck time – which is no different from the old measurement. But, since by axiom, the size of atoms (approximately the Bohr radius) are related to the Planck length by an unchanging dimensionless constant of proportionality:
Then atoms would be bigger (in one dimension) by formula_12, each of us would be taller by formula_12, and so would our metre sticks be taller (and wider and thicker) by a factor of formula_12. Our perception of distance and lengths relative to the Planck length is, by axiom, an unchanging dimensionless constant.
Our clocks would tick slower by a factor of formula_17 (from the point of view of this unaffected observer) because the Planck time has increased by formula_17 but we would not know the difference (our perception of durations of time relative to the Planck time is, by axiom, an unchanging dimensionless constant). This hypothetical observer on the outside might observe that light now propagates at half the speed that it previously did (as well as all other observed velocities) but it would still travel 299792458 of our "new" metres in the time elapsed by one of our "new" seconds (formula_19 continues to equal 299792458 m/s). "We" would not notice any difference.
This contradicts what George Gamow writes in his book "Mr. Tompkins"; there, Gamow suggests that if a dimension-dependent universal constant such as "c" changed, we "would" easily notice the difference. The disagreement is better thought of as the ambiguity in the phrase "changing a physical constant"; what would happen depends on whether (1) all other dimensionless constants were kept the same, or whether (2) all other dimension-dependent constants are kept the same. The second choice is a somewhat confusing possibility, since most of our units of measurement are defined in relation to the outcomes of physical experiments, and the experimental results depend on the constants. (The only exception is the kilogram.) Gamow does not address this subtlety; the thought experiments he conducts in his popular works assume the second choice for "changing a physical constant". And Duff or Barrow would point out that ascribing a change in measurable reality, i.e. α, to a specific dimensional component quantity, such as "c", is unjustified. The very same operational difference in measurement or perceived reality could just as well be caused by a change in "h" or "e".
This unvarying aspect of the Planck-relative scale, or that of any other system of natural units, leads many theorists to conclude that a hypothetical change in dimensionful physical constants can only be manifest as a change in dimensionless physical constants. One such dimensionless physical constant is the fine-structure constant. There are some experimental physicists who assert they have in fact measured a change in the fine structure constant and this has intensified the debate about the measurement of physical constants. According to some theorists there are some very special circumstances in which changes in the fine-structure constant "can" be measured as a change in "dimensionful" physical constants. Others however reject the possibility of measuring a change in dimensionful physical constants under any circumstance. The difficulty or even the impossibility of measuring changes in dimensionful physical constants has led some theorists to debate with each other whether or not a dimensionful physical constant has any practical significance at all and that in turn leads to questions about which dimensionful physical constants are meaningful.

</doc>
<doc id="230487" url="https://en.wikipedia.org/wiki?curid=230487" title="Poincaré group">
Poincaré group

The Poincaré group, named after Henri Poincaré, is the group of Minkowski spacetime isometries. It is a ten-generator non-abelian Lie group of fundamental importance in physics.
Overview.
A Minkowski spacetime isometry has the property that the interval between events is left invariant. For example, if everything was postponed by two hours including two events and the path you took to go from one to the other, then the time interval between the events recorded by a stop-watch you carried with you would be the same. Or if everything was shifted five miles to the west, or turned 60 degrees to the right, you would also see no change in the interval. It turns out that the proper length of an object is also unaffected by such a shift. A time or space reversal (a reflection) is also an isometry of this group.
In Minkowski space (i.e. ignoring the effects of gravity), there are ten degrees of freedom of the isometries, which may be thought of as translation through time or space (four degrees, one per dimension); reflection through a plane (three degrees, the freedom in orientation of this plane); or a "boost" in any of the three spatial directions (three degrees). Composition of transformations is the operator of the Poincaré group, with proper rotations being produced as the composition of an even number of reflections.
In classical physics, the Galilean group is a comparable ten-parameter group that acts on absolute time and space. Instead of boosts, it features shear mappings to relate co-moving frames of reference.
Details.
The Poincaré group is the group of Minkowski spacetime isometries. It is a ten-dimensional noncompact Lie group. The abelian group of translations is a normal subgroup, while the Lorentz group is also a subgroup, the stabilizer of the origin. The Poincaré group itself is the minimal subgroup of the affine group which includes all translations and Lorentz transformations. More precisely, it is a semidirect product of the translations and the Lorentz group, 
Another way of putting this is that the Poincaré group is a group extension of the Lorentz group by a vector representation of it; it is sometimes dubbed, informally, as the "inhomogeneous Lorentz group". In turn, it can also be obtained as a group contraction of the de Sitter group SO(4,1) ~ Sp(2,2), as the de Sitter radius goes to infinity.
Its positive energy unitary irreducible representations are indexed by mass (nonnegative number) and spin (integer or half integer) and are associated with particles in quantum mechanics (see Wigner's classification).
In accordance with the Erlangen program, the geometry of Minkowski space is defined by the Poincaré group: Minkowski space is considered as a homogeneous space for the group.
The Poincaré algebra is the Lie algebra of the Poincaré group. It is a Lie algebra extension of the Lie algebra of the Lorentz group. More specifically, the proper (det"Λ"=1), orthochronous () part of the Lorentz subgroup (its identity component), SO+(1, 3), is connected to the identity and is thus provided by the exponentiation of this Lie algebra. In component form, the Poincaré algebra is given by the commutation relations:
where is the generator of translations, is the generator of Lorentz transformations, and is the (+,−,−,−) Minkowski metric (see Sign convention).
The bottom commutation relation is the ("homogeneous") Lorentz group, consisting of rotations, , and boosts, . In this notation, the entire Poincaré algebra is expressible in noncovariant (but more practical) language as
where the bottom line commutator of two boosts is often referred to as a "Wigner rotation". Note the important simplification , which permits reduction of the Lorentz subalgebra to su(2)⊕su(2) and efficient treatment of its associated representations.
The Casimir invariants of this algebra are and where is the Pauli–Lubanski pseudovector; they serve as labels for the representations of the group.
The Poincaré group is the full symmetry group of any relativistic field theory. As a result, all elementary particles fall in representations of this group. These are usually specified by the "four-momentum" squared of each particle (i.e. its mass squared) and the intrinsic quantum numbers , where is the spin quantum number, is the parity and is the charge-conjugation quantum number. In practice, charge conjugation and parity are violated by many quantum field theories; where this occurs, and are forfeited. Since CPT symmetry is invariant in quantum field theory, a time-reversal quantum number may be constructed from those given.
As a topological space, the group has four connected components: the component of the identity; the time reversed component; the spatial inversion component; and the component which is both time-reversed and spatially inverted.
Poincaré symmetry.
Poincaré symmetry is the full symmetry of special relativity. It includes:
The last two symmetries, J and K, together make the Lorentz group (see also Lorentz invariance); the semi-direct product of the translations group and the Lorentz group then produce the Poincaré group. Objects which are invariant under this group are then said to possess Poincaré invariance or relativistic invariance.

</doc>
<doc id="7622892" url="https://en.wikipedia.org/wiki?curid=7622892" title="Point source">
Point source

A point source is a single identifiable "localised" source of something. A point source has negligible extent, distinguishing it from other source geometries. Sources are called point sources because in mathematical modeling, these sources can usually be approximated as a mathematical point to simplify analysis.
The actual source need not be physically small, if its size is negligible relative to other length scales in the problem. For example, in astronomy, stars are routinely treated as point sources, even though they are in actuality much larger than the Earth.
In three dimensions, the density of something leaving a point source decreases in proportion to the inverse square of the distance from the source, if the distribution is isotropic, and there is no absorption or other loss.
Mathematics.
In mathematics, a point source is a singularity from which flux or flow is emanating. Although singularities such as this do not exist in the observable universe, mathematical point sources are often used as approximations to reality in physics and other fields.
Light.
Generally a source of light can be considered a point source if the resolution of the imaging instrument is too low to resolve its apparent size.
Mathematically an object may be considered a point source if its angular size, formula_1, is much smaller than the resolving power of the telescope:
formula_2,
where formula_3 is the wavelength of light and formula_4 is the telescope diameter.
Examples:
Electromagnetic radiation.
Radio wave sources which are smaller than one radio wavelength are also generally treated as point sources. Radio emissions generated by a fixed electrical circuit are usually polarized, producing anisotropic radiation. If the propagating medium is lossless, however, the radiant power in the radio waves at a given distance will still vary as the inverse square of the distance if the angle remains constant to the source polarization.
Gamma ray and X-ray sources may be treated as a point source if sufficiently small. Radiological contamination and nuclear sources are often point sources. This has significance in health physics and radiation protection.
Examples:
Sound.
Sound is an oscillating pressure wave. As the pressure oscillates up and down, an audio point source acts in turn as a fluid point source and then a fluid point sink. (Such an object does not exist physically, but is often a good simplified model for calculations.)
Examples:
Ionizing radiation.
Point sources are used as a means of calibrating ionizing radiation instruments. They are usually a sealed capsule and are most commonly used for gamma, x-ray and beta measuring instruments.
Heat.
In vacuum, heat escapes as radiation isotropically. If the source remains stationary in a compressible fluid such as air, flow patterns can form around the source due to convection, leading to an anisotropic pattern of heat loss. The most common form of anisotropy is the formation of a thermal plume above the heat source.
Examples:
Fluid.
Fluid point sources are commonly used in fluid dynamics and aerodynamics. A point source of fluid is the inverse of a fluid point sink (a point where fluid is removed). Whereas fluid sinks exhibit complex rapidly changing behaviour such as is seen in vortices (for example water running into a plug-hole or tornadoes generated at points where air is rising), fluid sources generally produce simple flow patterns, with stationary isotropic point sources generating an expanding sphere of new fluid. If the fluid is moving (such as wind in air or currents in water) a plume is generated from the point source.
Examples:
Pollution.
Sources of various types of pollution are often considered as point sources in large-scale studies of pollution.

</doc>
<doc id="294340" url="https://en.wikipedia.org/wiki?curid=294340" title="Poisson bracket">
Poisson bracket

In mathematics and classical mechanics, the Poisson bracket is an important binary operation in Hamiltonian mechanics, playing a central role in Hamilton's equations of motion, which govern the time-evolution of a Hamiltonian dynamical system. The Poisson bracket also distinguishes a certain class of coordinate-transformations, called "canonical transformations", which maps canonical coordinate systems into canonical coordinate systems. (A "canonical coordinate system" consists of canonical position and momentum variables (here symbolized by qi and pi respectively) that satisfy canonical Poisson-bracket relations.) The set of possible canonical transformations is always very rich. For instance, often it is possible to choose the Hamiltonian itself "H" = "H"("q", "p"; "t") as one of the new canonical momentum coordinates.
In a more general sense: the Poisson bracket is used to define a Poisson algebra, of which the algebra of functions on a Poisson manifold is a special case. These are all named in honour of Siméon Denis Poisson.
Properties.
For any functions formula_1 of phase space and time:
Also, if a function formula_6 is time-dependent but constant over phase space, then formula_7 for any formula_8.
Definition in canonical coordinates.
In canonical coordinates (also known as Darboux coordinates) formula_9 on the phase space, given two functions formula_10 and formula_11, the Poisson bracket takes the form
A few basic properties result for brackets of the canonical coordinates:
Hamilton's equations of motion.
Hamilton's equations of motion have an equivalent expression in terms of the Poisson bracket. This may be most directly demonstrated in an explicit coordinate frame. Suppose that formula_16 is a function on the manifold. Then from the multivariable chain rule, one has
Further, one may take and to be solutions to Hamilton's equations; that is,
Then, one has
Thus, the time evolution of a function on a symplectic manifold can be given as a one-parameter family of symplectomorphisms (i.e. canonical transformations, area-preserving diffeomorphisms), with the time "t" being the parameter: Hamiltonian motion is a canonical transformation generated by the Hamiltonian. That is, Poisson brackets are preserved in it, so that "any time t" in the solution to Hamilton's equations, , can serve as the bracket coordinates. "Poisson brackets are canonical invariants".
Dropping the coordinates, one has
The operator in the convective part of the derivative, "L̂" = , is sometimes referred to as the Liouvillian (see Liouville's theorem (Hamiltonian)).
Constants of motion.
An integrable dynamical system will have constants of motion in addition to the energy. Such constants of motion will commute with the Hamiltonian under the Poisson bracket. Suppose some function "f(p,q)" is a constant of motion. This implies that if "p(t)", "q(t)" is a trajectory or solution to the Hamilton's equations of motion, then one has that
along that trajectory. Then one has
where, as above, the intermediate step follows by applying the equations of motion. This equation is known as the Liouville equation. The content of Liouville's theorem is that the time evolution of a measure (or "distribution function" on the phase space) is given by the above.
If the Poisson bracket of "f" and "g" vanishes ("{f,g}" = 0), then "f" and "g" are said to be in involution. In order for a Hamiltonian system to be completely integrable, all of the constants of motion must be in mutual involution.
Furthermore, according to the Poisson's Theorem, if two quantities formula_23 and formula_24 are constants of motion, so is their Poisson bracket formula_25. This does not always supply a useful result, however, since the number of possible constants of motion is limited (formula_26 for a system with "n" degrees of freedom), and so the result may be trivial (a constant, or a function of formula_23 and formula_24.)
The Poisson bracket in coordinate-free language.
Let "M" be symplectic manifold, that is, a manifold equipped with a symplectic form: a 2-form ω which is both closed (i.e. its exterior derivative "d"ω = 0) and non-degenerate. For example, in the treatment above, take "M" to be formula_29 and take
If formula_31 is the interior product or contraction operation defined by formula_32, then non-degeneracy is equivalent to saying that for every one-form α there is a unique vector field formula_33 such that formula_34. Alternatively, formula_35. Then if "H" is a smooth function on "M", the Hamiltonian vector field "XH" can be defined to be formula_36. It is easy to see that
The Poisson bracket formula_39 on ("M", ω) is a bilinear operation on differentiable functions, defined by formula_40; the Poisson bracket of two functions on "M" is itself a function on "M". The Poisson bracket is antisymmetric because:
Furthermore,
Here "Xgf" denotes the vector field "Xg" applied to the function "f" as a directional derivative, and formula_42 denotes the (entirely equivalent) Lie derivative of the function "f".
If α is an arbitrary one-form on "M", the vector field Ωα generates (at least locally) a flow formula_43 satisfying the boundary condition formula_44 and the first-order differential equation
The formula_43 will be symplectomorphisms (canonical transformations) for every "t" as a function of "x" if and only if formula_47; when this is true, Ωα is called a symplectic vector field. Recalling Cartan's identity formula_48 and "d"ω = 0, it follows that formula_49. Therefore Ωα is a symplectic vector field if and only if α is a closed form. Since formula_50, it follows that every Hamiltonian vector field "Xf" is a symplectic vector field, and that the Hamiltonian flow consists of canonical transformations. From above, under the Hamiltonian flow "XH",
This is a fundamental result in Hamiltonian mechanics, governing the time evolution of functions defined on phase space. As noted above, when "{f,H} = 0", "f" is a constant of motion of the system. In addition, in canonical coordinates (with formula_52 and formula_15), Hamilton's equations for the time evolution of the system follow immediately from this formula.
It also follows from that the Poisson bracket is a derivation; that is, it satisfies a non-commutative version of Leibniz's product rule:
The Poisson bracket is intimately connected to the Lie bracket of the Hamiltonian vector fields. Because the Lie derivative is a derivation,
Thus if "v" and "w" are symplectic, using formula_55, Cartan's identity, and the fact that formula_56 is a closed form,
It follows that formula_58, so that
Thus, the Poisson bracket on functions corresponds to the Lie bracket of the associated Hamiltonian vector fields. We have also shown that the Lie bracket of two symplectic vector fields is a Hamiltonian vector field and hence is also symplectic. In the language of abstract algebra, the symplectic vector fields form a subalgebra of the Lie algebra of smooth vector fields on "M", and the Hamiltonian vector fields form an ideal of this subalgebra. The symplectic vector fields are the Lie algebra of the (infinite-dimensional) Lie group of symplectomorphisms of "M".
It is widely asserted that the Jacobi identity for the Poisson bracket,
follows from the corresponding identity for the Lie bracket of vector fields, but this is true only up to a locally constant function. However, to prove the Jacobi identity for the Poisson bracket, it is sufficient to show that:
where the operator formula_61 on smooth functions on "M" is defined by formula_62 and the bracket on the right-hand side is the commutator of operators, formula_63. By , the operator formula_61 is equal to the operator "Xg". The proof of the Jacobi identity follows from because the Lie bracket of vector fields is just their commutator as differential operators.
The algebra of smooth functions on M, together with the Poisson bracket forms a Poisson algebra, because it is a Lie algebra under the Poisson bracket, which additionally satisfies Leibniz's rule . We have shown that every symplectic manifold is a Poisson manifold, that is a manifold with a "curly-bracket" operator on smooth functions such that the smooth functions form a Poisson algebra. However, not every Poisson manifold arises in this way, because Poisson manifolds allow for degeneracy which cannot arise in the symplectic case.
A result on conjugate momenta.
Given a smooth vector field "X" on the configuration space, let "PX" be its conjugate momentum. The conjugate momentum mapping is a Lie algebra anti-homomorphism from the Poisson bracket to the Lie bracket:
This important result is worth a short proof. Write a vector field "X" at point "q" in the configuration space as
where the formula_67 is the local coordinate frame. The conjugate momentum to "X" has the expression
where the "pi" are the momentum functions conjugate to the coordinates. One then has, for a point "(q,p)" in the phase space,
The above holds for all "(q,p)", giving the desired result.
Quantization.
Poisson brackets deform to Moyal brackets upon quantization, that is, they generalize to a different Lie algebra, the Moyal algebra, or, equivalently in Hilbert space, quantum commutators. The Wigner-İnönü group contraction of these (the classical limit, "ħ"→0) yields the above Lie algebra.

</doc>
<doc id="193735" url="https://en.wikipedia.org/wiki?curid=193735" title="Poisson's equation">
Poisson's equation

In mathematics, Poisson's equation is a partial differential equation of elliptic type with broad utility in electrostatics, mechanical engineering and theoretical physics. It is used, for instance, to describe the potential energy field caused by a given charge or mass density distribution. The equation is named after the French mathematician, geometer, and physicist Siméon Denis Poisson.
Statement of the equation.
Poisson's equation is
where formula_2 is the Laplace operator, and "f" and "φ" are real or complex-valued functions on a manifold. Usually, "f" is given and "φ" is sought. When the manifold is Euclidean space, the Laplace operator is often denoted as ∇2 and so Poisson's equation is frequently written as
In three-dimensional Cartesian coordinates, it takes the form
When formula_5 we retrieve Laplace's equation.
Poisson's equation may be solved using a Green's function; a general exposition of the Green's function for Poisson's equation is given in the article on the screened Poisson equation. There are various methods for numerical solution. The relaxation method, an iterative algorithm, is one example.
Newtonian gravity.
In the case of a gravitational field g due to an attracting massive object of density "ρ", Gauss's law for gravity in differential form can be used to obtain the corresponding Poisson equation for gravity:
Since the gravitational field is conservative, it can be expressed in terms of a scalar potential "Φ":
Substituting into Gauss's law
obtains Poisson's equation for gravity:
Using Green's Function, the potential at distance "r" from a central point mass "m" (ie: the Fundamental Solution) is:
Electrostatics.
One of the cornerstones of electrostatics is setting up and solving problems described by the Poisson equation. Solving the Poisson equation amounts to finding the electric potential φ for a given charge distribution "formula_11".
The mathematical details behind Poisson's equation in electrostatics are as follows (SI units are used rather than Gaussian units, which are also frequently used in electromagnetism).
Starting with Gauss's law for electricity (also one of Maxwell's equations) in differential form, we have:
where formula_13 is the divergence operator, D = electric displacement field, and "ρf" = free charge density (describing charges brought from outside). Assuming the medium is linear, isotropic, and homogeneous (see polarization density), we have the constitutive equation:
where "ε" = permittivity of the medium and E = electric field. Substituting this into Gauss's law and assuming "ε" is spatially constant in the region of interest obtains:
In the absence of a changing magnetic field, B, Faraday's law of induction gives:
where formula_17 is the curl operator and "t" is time. Since the curl of the electric field is zero, it is defined by a scalar electric potential field, formula_18 (see Helmholtz decomposition).
The derivation of Poisson's equation under these circumstances is straightforward. Substituting the potential gradient for the electric field
directly obtains Poisson's equation for electrostatics, which is:
Solving Poisson's equation for the potential requires knowing the charge density distribution. If the charge density is zero, then Laplace's equation results. If the charge density follows a Boltzmann distribution, then the Poisson-Boltzmann equation results. The Poisson–Boltzmann equation plays a role in the development of the Debye–Hückel theory of dilute electrolyte solutions.
Using Green's Function, the potential at distance "r" from a central point charge "Q" (ie: the Fundamental Solution) is:
The above discussion assumes that the magnetic field is not varying in time. The same Poisson equation arises even if it does vary in time, as long as the Coulomb gauge is used. In this more general context, computing "φ" is no longer sufficient to calculate E, since E also depends on the magnetic vector potential A, which must be independently computed. See Maxwell's equation in potential formulation for more on "φ" and A in Maxwell's equations and how Poisson's equation is obtained in this case.
Potential of a Gaussian charge density.
If there is a static spherically symmetric Gaussian charge density
where "Q" is the total charge, then the solution "φ"("r") of Poisson's equation,
is given by
where erf("x") is the error function.
This solution can be checked explicitly by evaluating formula_27. Note that, for "r" much greater than "σ", the erf function approaches unity and the potential φ ("r") approaches the point charge potential
as one would expect. Furthermore the erf function approaches 1 extremely quickly as its argument increases; in practice for r > 3"σ" the relative error is smaller than one part in a thousand.
Surface reconstruction.
Poisson's equation is also used to reconstruct a smooth 3D surface based on a large number of points "pi" (a point cloud) where each point also carries an estimate of the local surface normal n"i".
This technique reconstructs the implicit function "f" whose value is zero at the points "pi" and whose gradient at the points "pi" equals the normal vectors n"i". The set of ("pi", n"i") is thus a sampling of a continuous vector ﬁeld V. The implicit function "f" is found by integrating the vector ﬁeld V. Since not every vector ﬁeld is the gradient of a function, the problem may or may not have a solution: the necessary and sufﬁcient condition for a smooth vector ﬁeld V to be the gradient of a function "f" is that the curl of V must be identically zero. In case this condition is difﬁcult to impose, it is still possible to perform a least-squares fit to minimize the difference between V and the gradient of "f".

</doc>
<doc id="25120" url="https://en.wikipedia.org/wiki?curid=25120" title="Polar coordinate system">
Polar coordinate system

In mathematics, the polar coordinate system is a two-dimensional coordinate system in which each point on a plane is determined by a distance from a reference point and an angle from a reference direction.
The reference point (analogous to the origin of a Cartesian system) is called the "pole", and the ray from the pole in the reference direction is the "polar axis". The distance from the pole is called the "radial coordinate" or "radius", and the angle is called the "angular coordinate", "polar angle", or "azimuth".
History.
The concepts of angle and radius were already used by ancient peoples of the first millennium BC. The Greek astronomer and astrologer Hipparchus (190–120 BC) created a table of chord functions giving the length of the chord for each angle, and there are references to his using polar coordinates in establishing stellar positions.
In "On Spirals", Archimedes describes the Archimedean spiral, a function whose radius depends on the angle. The Greek work, however, did not extend to a full coordinate system.
From the 8th century AD onward, astronomers developed methods for approximating and calculating the direction to Mecca (qibla)—and its distance—from any location on the Earth. From the 9th century onward they were using spherical trigonometry and map projection methods to determine these quantities accurately. The calculation is essentially the conversion of the equatorial polar coordinates of Mecca (i.e. its longitude and latitude) to its polar coordinates (i.e. its qibla and distance) relative to a system whose reference meridian is the great circle through the given location and the Earth's poles, and whose polar axis is the line through the location and its antipodal point.
There are various accounts of the introduction of polar coordinates as part of a formal coordinate system. The full history of the subject is described in Harvard professor Julian Lowell Coolidge's "Origin of Polar Coordinates." Grégoire de Saint-Vincent and Bonaventura Cavalieri independently introduced the concepts in the mid-seventeenth century. Saint-Vincent wrote about them privately in 1625 and published his work in 1647, while Cavalieri published his in 1635 with a corrected version appearing in 1653. Cavalieri first used polar coordinates to solve a problem relating to the area within an Archimedean spiral. Blaise Pascal subsequently used polar coordinates to calculate the length of parabolic arcs.
In "Method of Fluxions" (written 1671, published 1736), Sir Isaac Newton examined the transformations between polar coordinates, which he referred to as the "Seventh Manner; For Spirals", and nine other coordinate systems. In the journal "Acta Eruditorum" (1691), Jacob Bernoulli used a system with a point on a line, called the "pole" and "polar axis" respectively. Coordinates were specified by the distance from the pole and the angle from the "polar axis". Bernoulli's work extended to finding the radius of curvature of curves expressed in these coordinates.
The actual term "polar coordinates" has been attributed to Gregorio Fontana and was used by 18th-century Italian writers. The term appeared in English in George Peacock's 1816 translation of Lacroix's "Differential and Integral Calculus". Alexis Clairaut was the first to think of polar coordinates in three dimensions, and Leonhard Euler was the first to actually develop them.
Conventions.
The radial coordinate is often denoted by "r" or "ρ", and the angular coordinate by "ϕ", "θ", or "t". The angular coordinate is specified as "ϕ" by ISO standard 31-11.
Angles in polar notation are generally expressed in either degrees or radians (2π rad being equal to 360°). Degrees are traditionally used in navigation, surveying, and many applied disciplines, while radians are more common in mathematics and mathematical physics.
In many contexts, a positive angular coordinate means that the angle "ϕ" is measured counterclockwise from the axis.
In mathematical literature, the polar axis is often drawn horizontal and pointing to the right.
Uniqueness of polar coordinates.
Adding any number of full turns (360°) to the angular coordinate does not change the corresponding direction. Also, a negative radial coordinate is best interpreted as the corresponding positive distance measured in the opposite direction. Therefore, the same point can be expressed with an infinite number of different polar coordinates or , where "n" is any integer. Moreover, the pole itself can be expressed as (0, "ϕ") for any angle "ϕ".
Where a unique representation is needed for any point, it is usual to limit "r" to non-negative numbers () and "ϕ" to the interval [0, 360°) or (−180°, 180°] (in radians, [0, 2π) or (−π, π]). One must also choose a unique azimuth for the pole, e.g., "ϕ" = 0.
Converting between polar and Cartesian coordinates.
The polar coordinates "r" and "ϕ" can be converted to the Cartesian coordinates "x" and "y" by using the trigonometric functions sine and cosine:
The Cartesian coordinates "x" and "y" can be converted to polar coordinates "r" and "ϕ" with "r" ≥ 0 and "ϕ" in the interval (−π, π] by:
where atan2 is a common variation on the arctangent function defined as
The value of "ϕ" above is the principal value of the complex number function arg applied to "x"+"iy". An angle in the range [0, 2π) may be obtained by adding 2π to the value in case it is negative.
Polar equation of a curve.
The equation defining an algebraic curve expressed in polar coordinates is known as a "polar equation". In many cases, such an equation can simply be specified by defining "r" as a function of "ϕ". The resulting curve then consists of points of the form ("r"("ϕ"), "ϕ") and can be regarded as the graph of the polar function "r".
Different forms of symmetry can be deduced from the equation of a polar function "r". If the curve will be symmetrical about the horizontal (0°/180°) ray, if it will be symmetric about the vertical (90°/270°) ray, and if it will be rotationally symmetric by α clockwise and counterclockwise about the pole.
Because of the circular nature of the polar coordinate system, many curves can be described by a rather simple polar equation, whereas their Cartesian form is much more intricate. Among the best known of these curves are the polar rose, Archimedean spiral, lemniscate, limaçon, and cardioid.
For the circle, line, and polar rose below, it is understood that there are no restrictions on the domain and range of the curve.
Circle.
The general equation for a circle with a center at and radius "a" is
This can be simplified in various ways, to conform to more specific cases, such as the equation
for a circle with a center at the pole and radius "a".
When 0 = , or when the origin lies on the circle, the equation becomes
In the general case, the equation can be solved for , giving
the solution with a minus sign in front of the square root gives the same curve.
Line.
"Radial" lines (those running through the pole) are represented by the equation
where ɣ is the angle of elevation of the line; that is, where "m" is the slope of the line in the Cartesian coordinate system. The non-radial line that crosses the radial line perpendicularly at the point ("r0", ɣ) has the equation
Otherwise stated ("r0", ɣ) is the point in which the tangent intersects the imaginary circle of radius "r0".
Polar rose.
A polar rose is a famous mathematical curve that looks like a petaled flower, and that can be expressed as a simple polar equation,
for any constant ɣ0 (including 0). If "k" is an integer, these equations will produce a "k"-petaled rose if "k" is odd, or a 2"k"-petaled rose if "k" is even. If "k" is rational but not an integer, a rose-like shape may form but with overlapping petals. Note that these equations never define a rose with 2, 6, 10, 14, etc. petals. The variable "a" represents the length of the petals of the rose.
Archimedean spiral.
The Archimedean spiral is a famous spiral that was discovered by Archimedes, which can also be expressed as a simple polar equation. It is represented by the equation
Changing the parameter "a" will turn the spiral, while "b" controls the distance between the arms, which for a given spiral is always constant. The Archimedean spiral has two arms, one for and one for . The two arms are smoothly connected at the pole. Taking the mirror image of one arm across the 90°/270° line will yield the other arm. This curve is notable as one of the first curves, after the conic sections, to be described in a mathematical treatise, and as being a prime example of a curve that is best defined by a polar equation.
Conic sections.
A conic section with one focus on the pole and the other somewhere on the 0° ray (so that the conic's major axis lies along the polar axis) is given by:
where "e" is the eccentricity and formula_15 is the semi-latus rectum (the perpendicular distance at a focus from the major axis to the curve). If , this equation defines a hyperbola; if , it defines a parabola; and if , it defines an ellipse. The special case of the latter results in a circle of radius formula_15.
Intersection of two polar curves.
The graphs of two polar functions formula_17 and formula_18 have possible intersections in 3 cases:
Complex numbers.
Every complex number can be represented as a point in the complex plane, and can therefore be expressed by specifying either the point's Cartesian coordinates (called rectangular or Cartesian form) or the point's polar coordinates (called polar form). The complex number "z" can be represented in rectangular form as
where "i" is the imaginary unit, or can alternatively be written in polar form (via the conversion formulae given above) as
and from there as
where "e" is Euler's number, which are equivalent as shown by Euler's formula. (Note that this formula, like all those involving exponentials of angles, assumes that the angle "ϕ" is expressed in radians.) To convert between the rectangular and polar forms of a complex number, the conversion formulae given above can be used.
For the operations of multiplication, division, and exponentiation of complex numbers, it is generally much simpler to work with complex numbers expressed in polar form rather than rectangular form. From the laws of exponentiation:
Calculus.
Calculus can be applied to equations expressed in polar coordinates.
The angular coordinate "ϕ" is expressed in radians throughout this section, which is the conventional choice when doing calculus.
Differential calculus.
Using and , one can derive a relationship between derivatives in Cartesian and polar coordinates. For a given function, "u"("x","y"), it follows that (by computing its total derivatives)
or
Hence, we have the following formulae:
Using the inverse coordinates transformation, an analogous reciprocal relationship can be derived between the derivatives. Given a function "u"("r","ϕ"), it follows that
or
Hence, we have the following formulae:
To find the Cartesian slope of the tangent line to a polar curve "r"("ϕ") at any given point, the curve is first expressed as a system of parametric equations.
Differentiating both equations with respect to "ϕ" yields
Dividing the second equation by the first yields the Cartesian slope of the tangent line to the curve at the point :
For other useful formulas including divergence, gradient, and Laplacian in polar coordinates, see curvilinear coordinates.
Integral calculus (arc length).
The arc length (length of a line segment) defined by a polar function is found by the integration over the curve "r"("ϕ"). Let "L" denote this length along the curve starting from points "A" through to point "B", where these points correspond to "ϕ" = "a" and "ϕ" = "b" such that . The length of "L" is given by the following integral
Integral calculus (area).
Let "R" denote the region enclosed by a curve "r"("ϕ") and the rays "ϕ" = "a" and "ϕ" = "b", where . Then, the area of "R" is
This result can be found as follows. First, the interval is divided into "n" subintervals, where "n" is an arbitrary positive integer. Thus Δ"ϕ", the length of each subinterval, is equal to (the total length of the interval), divided by "n", the number of subintervals. For each subinterval "i" = 1, 2, …, "n", let "ϕ""i" be the midpoint of the subinterval, and construct a sector with the center at the pole, radius "r"("ϕ""i"), central angle Δ"ϕ" and arc length "r"("ϕ""i")Δ"ϕ". The area of each constructed sector is therefore equal to
Hence, the total area of all of the sectors is
As the number of subintervals "n" is increased, the approximation of the area continues to improve. In the limit as , the sum becomes the Riemann sum for the above integral.
A mechanical device that computes area integrals is the planimeter, which measures the area of plane figures by tracing them out: this replicates integration in polar coordinates by adding a joint so that the 2-element linkage effects Green's theorem, converting the quadratic polar integral to a linear integral.
Generalization.
Using Cartesian coordinates, an infinitesimal area element can be calculated as "dA" = "dx" "dy". The substitution rule for multiple integrals states that, when using other coordinates, the Jacobian determinant of the coordinate conversion formula has to be considered:
Hence, an area element in polar coordinates can be written as
Now, a function, that is given in polar coordinates, can be integrated as follows:
Here, "R" is the same region as above, namely, the region enclosed by a curve "r"("ϕ") and the rays "ϕ" = "a" and "ϕ" = "b".
The formula for the area of "R" mentioned above is retrieved by taking "f" identically equal to 1. A more surprising application of this result yields the Gaussian integral
Vector calculus.
Vector calculus can also be applied to polar coordinates. For a planar motion, let formula_59 be the position vector , with "r" and "ϕ" depending on time "t".
We define the unit vectors
in the direction of r and
in the plane of the motion perpendicular to the radial direction, where formula_62 is a unit vector normal to the plane of the motion.
Then
Centrifugal and Coriolis terms.
The term formula_67 is sometimes referred to as the "centrifugal term", and the term formula_68 as the "Coriolis term". For example, see Shankar. Although these equations bear some resemblance in form to the centrifugal and Coriolis effects found in rotating reference frames, nonetheless these are not the same things. For example, the physical centrifugal and Coriolis forces appear only in non-inertial frames of reference. In contrast, these terms, that appear when acceleration is expressed in polar coordinates, are a mathematical consequence of differentiation; these terms appear wherever polar coordinates are used. In particular, these terms appear even when polar coordinates are used in inertial frames of reference, where the physical centrifugal and Coriolis forces never appear.
Co-rotating frame.
For a particle in planar motion, one approach to attaching physical significance to these terms is based on the concept of an instantaneous "co-rotating frame of reference". To define a co-rotating frame, first an origin is selected from which the distance "r"("t") to the particle is defined. An axis of rotation is set up that is perpendicular to the plane of motion of the particle, and passing through this origin. Then, at the selected moment "t", the rate of rotation of the co-rotating frame Ω is made to match the rate of rotation of the particle about this axis, "dϕ"/"dt". Next, the terms in the acceleration in the inertial frame are related to those in the co-rotating frame. Let the location of the particle in the inertial frame be ("r("t"), "ϕ"("t")), and in the co-rotating frame be ("r(t), "ϕ"′(t)"). Because the co-rotating frame rotates at the same rate as the particle, "dϕ"′/"dt" = 0. The fictitious centrifugal force in the co-rotating frame is "mrΩ2, radially outward. The velocity of the particle in the co-rotating frame also is radially outward, because "dϕ"′/"dt" = 0. The "fictitious Coriolis force" therefore has a value −2"m"("dr"/"dt")Ω, pointed in the direction of increasing "ϕ" only. Thus, using these forces in Newton's second law we find:
where over dots represent time differentiations, and F is the net real force (as opposed to the fictitious forces). In terms of components, this vector equation becomes:
which can be compared to the equations for the inertial frame:
This comparison, plus the recognition that by the definition of the co-rotating frame at time "t" it has a rate of rotation Ω = "dϕ"/"dt", shows that we can interpret the terms in the acceleration (multiplied by the mass of the particle) as found in the inertial frame as the negative of the centrifugal and Coriolis forces that would be seen in the instantaneous, non-inertial co-rotating frame.
For general motion of a particle (as opposed to simple circular motion), the centrifugal and Coriolis forces in a particle's frame of reference commonly are referred to the instantaneous osculating circle of its motion, not to a fixed center of polar coordinates. For more detail, see centripetal force.
Connection to spherical and cylindrical coordinates.
The polar coordinate system is extended into three dimensions with two different coordinate systems, the cylindrical and spherical coordinate system.
Applications.
Polar coordinates are two-dimensional and thus they can be used only where point positions lie on a single two-dimensional plane. They are most appropriate in any context where the phenomenon being considered is inherently tied to direction and length from a center point. For instance, the examples above show how elementary polar equations suffice to define curves—such as the Archimedean spiral—whose equation in the Cartesian coordinate system would be much more intricate. Moreover, many physical systems—such as those concerned with bodies moving around a central point or with phenomena originating from a central point—are simpler and more intuitive to model using polar coordinates. The initial motivation for the introduction of the polar system was the study of circular and orbital motion.
Position and navigation.
Polar coordinates are used often in navigation as the destination or direction of travel can be given as an angle and distance from the object being considered. For instance, aircraft use a slightly modified version of the polar coordinates for navigation. In this system, the one generally used for any sort of navigation, the 0° ray is generally called heading 360, and the angles continue in a clockwise direction, rather than counterclockwise, as in the mathematical system. Heading 360 corresponds to magnetic north, while headings 90, 180, and 270 correspond to magnetic east, south, and west, respectively. Thus, an aircraft traveling 5 nautical miles due east will be traveling 5 units at heading 90 (read zero-niner-zero by air traffic control).
Modeling.
Systems displaying radial symmetry provide natural settings for the polar coordinate system, with the central point acting as the pole. A prime example of this usage is the groundwater flow equation when applied to radially symmetric wells. Systems with a radial force are also good candidates for the use of the polar coordinate system. These systems include gravitational fields, which obey the inverse-square law, as well as systems with point sources, such as radio antennas.
Radially asymmetric systems may also be modeled with polar coordinates. For example, a microphone's pickup pattern illustrates its proportional response to an incoming sound from a given direction, and these patterns can be represented as polar curves. The curve for a standard cardioid microphone, the most common unidirectional microphone, can be represented as at its target design frequency. The pattern shifts toward omnidirectionality at lower frequencies.

</doc>
<doc id="41564" url="https://en.wikipedia.org/wiki?curid=41564" title="Polarization (waves)">
Polarization (waves)

Polarization (also polarisation) is a property of waves that can oscillate with more than one orientation. Electromagnetic waves such as light exhibit polarization, as do some other types of wave, such as gravitational waves. Sound waves in a gas or liquid do not exhibit polarization, since the oscillation is always in the direction the wave travels.
In an electromagnetic wave, both the electric field and magnetic field are oscillating but in different directions; by convention the "polarization" of light refers to the polarization of the electric field. Light which can be approximated as a plane wave in free space or in an isotropic medium propagates as a transverse wave—both the electric and magnetic fields are perpendicular to the wave's direction of travel. The oscillation of these fields may be in a single direction (linear polarization), or the field may rotate at the optical frequency (circular or elliptical polarization). In that case the direction of the fields' rotation, and thus the specified polarization, may be either clockwise or counter clockwise; this is referred to as the wave's chirality or "handedness".
The most common optical materials (such as glass) are isotropic and simply preserve the polarization of a wave but do not differentiate between polarization states. However, there are important classes of materials classified as birefringent or optically active in which this is not the case and a wave's polarization will generally be modified or will affect propagation through it. A polarizer is an optical filter that transmits only one polarization.
Polarization is an important parameter in areas of science dealing with transverse wave propagation, such as optics, seismology, radio, and microwaves. Especially impacted are technologies such as lasers, wireless and optical fiber telecommunications, and radar.
Introduction.
Wave propagation and polarization.
Most sources of light are classified as incoherent and unpolarized (or only "partially polarized") because they consist of a random mixture of waves having different spatial characteristics, frequencies (wavelengths), phases, and polarization states. However, for understanding electromagnetic waves and polarization in particular, it is easiest to just consider coherent plane waves; these are sinusoidal waves of one particular direction (or wavevector), frequency, phase, and polarization state. Characterizing an optical system in relation to a plane wave with those given parameters can then be used to predict its response to a more general case, since a wave with any specified spatial structure can be decomposed into a combination of plane waves (its so-called angular spectrum). And incoherent states can be modeled stochastically as a weighted combination of such uncorrelated waves with some distribution of frequencies (its "spectrum"), phases, and polarizations.
Transverse electromagnetic waves.
Electromagnetic waves (such as light), traveling in free space or another homogeneous isotropic non-attenuating medium, are properly described as transverse waves, meaning that a plane wave's electric field vector E and magnetic field H are in directions perpendicular to (or "transverse" to) the direction of wave propagation; E and H are also perpendicular to each other. Considering a monochromatic plane wave of optical frequency "f" (light of vacuum wavelength λ has a frequency of "f = c/λ" where "c" is the speed of light), let us take the direction of propagation as the "z" axis. Being a transverse wave the E and H fields must then contain components only in the "x" and "y" directions whereas "Ez=Hz=0". Using complex (or phasor) notation, we understand the instantaneous physical electric and magnetic fields to be given by the real parts of the complex quantities occurring in the following equations. As a function of time "t" and spatial position "z" (since for a plane wave in the +"z" direction the fields have no dependence on "x" or "y") these complex fields can be written as:
and
where λ/"n" is the wavelength "in the medium" (whose refractive index is "n") and is the period of the wave. Here 
"ex", "ey", "hx", and "hy" are complex numbers.
In the second more compact form, as these equations are customarily expressed, these factors are described using the wavenumber formula_3 and angular frequency (or "radian frequency") formula_4. In a more general formulation with propagation "not" restricted to the "+z" direction, then the spatial dependence "kz" is replaced by formula_5 where formula_6 is called the wave vector, the magnitude of which is the wavenumber.
Thus the leading vectors e and h each contain up to two nonzero (complex) components describing the amplitude and phase of the wave's "x" and "y" polarization components (again, there can be no "z" polarization component for a transverse wave in the +"z" direction). For a given medium with a characteristic impedance formula_7, h is related to e by:
and
In a dielectric, "η" is real and has the value "η"0/"n", where "n" is the refractive index and "η"0 is the impedance of free space. The impedance will be complex in a conducting medium. Note that given that relationship, the dot product of E and H must be zero:
indicating that these vectors are orthogonal (at right angles to each other), as expected.
So knowing the propagation direction (+"z" in this case) and η, one can just as well specify the wave in terms of just "ex" and "ey" describing the electric field. The vector containing "ex" and "ey" (but without the "z" component which is necessarily zero for a transverse wave) is known as a Jones vector. In addition to specifying the polarization state of the wave, a general Jones vector also specifies the overall magnitude and phase of that wave. Specifically, the intensity of the light wave is proportional to the sum of the squared magnitudes of the two electric field components:
however the wave's "state of polarization" is only dependent on the (complex) "ratio" of "ey" to "ex". So let us just consider waves whose "|ex|2 + |ey|2 = 1"; this happens to correspond to an intensity of about .00133 watts per square meter in free space (where formula_12 formula_13). And since the absolute phase of a wave is unimportant in discussing its polarization state, let us stipulate that the phase of "ex" is zero, in other words "ex" is a real number while "ey" may be complex. Under these restrictions, "ex" and "ey" can be represented as follows:
where the polarization state is now totally parameterized by the value of "Q" (such that -1 < "Q" < 1) and the relative phase formula_16. By convention when one speaks of a wave's "polarization," if not otherwise specified, reference is being made to the polarization of the electric field. The polarization of the magnetic field always follows that of the electric field but with a 90 degree rotation, as detailed above.
Non-transverse polarization.
In addition to transverse waves, there are many wave motions where the oscillation is not limited to directions perpendicular to the direction of propagation. These cases are beyond the scope of the current article which concentrates on transverse waves (such as most electromagnetic waves in bulk media), however one should be aware of cases where the polarization of a coherent wave cannot be described simply using a Jones vector, as we have just done.
Just considering electromagnetic waves, we note that the preceding discussion strictly applies to plane waves in a homogeneous isotropic non-attenuating medium, whereas in an anisotropic medium (such as birefringent crystals as discussed below) the electric or magnetic field may have longitudinal as well as transverse components. In those cases the electric displacement "D" and magnetic flux density "B" still obey the above geometry but due to anisotropy in the electric susceptibility (or in the magnetic permeability), now given by a tensor, the direction of "E" (or "H") may differ from that of "D" (or "B"). Even in isotropic media, so-called inhomogeneous waves can be launched into a medium whose refractive index has a significant imaginary part (or "extinction coefficient") such as metals; these fields are also not strictly transverse. Surface waves or waves propagating in a waveguide (such as an optical fiber) are generally "not" transverse waves, but might be described as an electric or magnetic transverse mode, or a hybrid mode.
Even in free space, longitudinal field components can be generated in focal regions, where the plane wave approximation breaks down. An extreme example is radially or tangentially polarized light, at the focus of which the electric or magnetic field respectively is "entirely" longitudinal (along the direction of propagation).
For longitudinal waves such as sound waves in fluids, the direction of oscillation is by definition along the direction of travel, so the issue of polarization is not normally even mentioned. On the other hand, sound waves in a bulk solid can be transverse as well as longitudinal, for a total of three polarization components. In this case, the transverse polarization is associated with the direction of the shear stress and displacement in directions perpendicular to the propagation direction, while the longitudinal polarization describes compression of the solid and vibration along the direction of propagation. The differential propagation of transverse and longitudinal polarizations is important in seismology.
Polarization state.
Polarization is best understood by initially considering only pure polarization states, and only a coherent sinusoidal wave at some optical frequency. The vector on the right might describe the oscillation of the electric field emitted by a single-mode laser (whose oscillation frequency would be typically 1015 times faster). The field oscillates in the "x-y" plane, along the page, with the wave propagating in the "z" direction, perpendicular to the page.
The first two diagrams below trace the electric field vector over a complete cycle for linear polarization at two different orientations; these are each considered a distinct "State Of Polarization" (SOP). Note that the linear polarization at 45° can also be viewed as the addition of a horizontally linearly polarized wave (as in the leftmost figure) and a vertically polarized wave of the same amplitude "in the same phase".
Now if one were to introduce a phase shift in between those horizontal and vertical polarization components, one would generally obtain elliptical polarization as is shown in the third figure. When the phase shift is exactly ±90°, then "circular polarization" is produced (fourth and fifth figures). Thus is circular polarization created in practice, starting with linearly polarized light and employing a quarter-wave plate to introduce such a phase shift. The result of two such phase-shifted components in causing a rotating electric field vector is depicted in the animation on the right. Note that circular or elliptical polarization can involve either a clockwise or counterclockwise rotation of the field. These correspond to distinct polarization states, such as the two circular polarizations shown above.
Of course the orientation of the "x" and "y" axes used in this description is arbitrary. The choice of such a coordinate system and viewing the polarization ellipse in terms of the "x" and "y" polarization components, corresponds to the definition of the Jones vector (below) in terms of those basis polarizations. One would typically choose axes to suit a particular problem such as "x" being in the plane of incidence. Since there are separate reflection coefficients for the linear polarizations in and orthogonal to the plane of incidence ("p" and "s" polarizations, see below), that choice greatly simplifies the calculation of a wave's reflection from a surface.
Moreover, one can use as basis functions "any" pair of orthogonal polarization states, not just linear polarizations. For instance, choosing right and left circular polarizations as basis functions simplifies the solution of problems involving circular birefringence (optical activity) or circular dichroism.
Polarization ellipse.
Consider a purely polarized monochromatic wave. If one were to plot the electric field vector over one cycle of oscillation, an ellipse would generally be obtained, as is shown in the figure, corresponding to a particular state of elliptical polarization. Note that linear polarization and circular polarization can be seen as special cases of elliptical polarization.
A polarization state can then be described in relation to the geometrical parameters of the ellipse, and its "handedness", that is, whether the rotation around the ellipse is clockwise or counter clockwise. One parameterization of the elliptical figure specifies the orientation angle ψ, defined as the angle between the major axis of the ellipse and the "x"-axis along with the ellipticity ε=a/b, the ratio of the ellipse's major to minor axis. (also known as the axial ratio). The ellipticity parameter is an alternative parameterization of an ellipse's eccentricity formula_17, or the ellipticity angle, χ = arctan b/a= arctan 1/ε as is shown in the figure. The angle χ is also significant in that the latitude (angle from the equator) of the polarization state as represented on the Poincaré sphere (see below) is equal to ±2χ. The special cases of linear and circular polarization correspond to an ellipticity ε of infinity and unity (or χ of zero and 45°) respectively.
Jones vector.
Full information on a completely polarized state is also provided by the amplitude and phase of oscillations in two components of the electric field vector in the plane of polarization. This representation was used above to show how different states of polarization are possible. The amplitude and phase information can be conveniently represented as a two-dimensional complex vector (the Jones vector):
Here formula_19 and formula_20 denote the amplitude of the wave in the two components of the electric field vector, while formula_21 and formula_22 represent the phases. The product of a Jones vector with a complex number of unit modulus gives a different Jones vector representing the same ellipse, and thus the same state of polarization. The physical electric field, as the real part of the Jones vector, would be altered but the polarization state itself is independent of absolute phase. The basis vectors used to represent the Jones vector need not represent linear polarization states (i.e. be real). In general any two orthogonal states can be used, where an orthogonal vector pair is formally defined as one having a zero inner product. A common choice is left and right circular polarizations, for example to model the different propagation of waves in two such components in circularly birefringent media (see below) or signal paths of coherent detectors sensitive to circular polarization.
Coordinate frame.
Regardless of whether polarization state is represented using geometric parameters or Jones vectors, implicit in the parameterization is the orientation of the coordinate frame. This permits a degree of freedom, namely rotation about the propagation direction. When considering light that is propagating parallel to the surface of the Earth, the terms "horizontal" and "vertical" polarization are often used, with the former being associated with the first component of the Jones vector, or zero azimuth angle. On the other hand, in astronomy the equatorial coordinate system is generally used instead, with the zero azimuth (or position angle, as it is more commonly called in astronomy to avoid confusion with the horizontal coordinate system) corresponding to due north.
"s" and "p" designations.
Another coordinate system frequently used relates to the plane made by the propagation direction and a vector perpendicular to the plane of a reflecting surface. This is known as the "plane of incidence". The component of the electric field parallel to this plane is termed "p-like" (parallel) and the component perpendicular to this plane is termed "s-like" (from "senkrecht", German for perpendicular). Polarized light with its electric field along the plane of incidence is thus denoted "p-polarized", while light whose electric field is normal to the plane of incidence is called "s-polarized". "P" polarization is commonly referred to as "transverse-magnetic" (TM), and has also been termed "pi-polarized" or "tangential plane polarized". "S" polarization is also called "transverse-electric" (TE), as well as "sigma-polarized" or "sagittal plane polarized".
Unpolarized and partially polarized light.
Definition.
Most common sources of visible light, including thermal (black body) radiation and fluorescence (but "not" lasers), produce light described as "incoherent". Radiation is produced independently by a large number of atoms or molecules whose emissions are uncorrelated and generally of random polarizations. In this case the light is said to be "unpolarized". This term is somewhat inexact, since at any instant of time at one location there is a definite direction to the electric and magnetic fields, however it implies that the polarization changes so quickly in time that it will not be measured or relevant to the outcome of an experiment. A so-called depolarizer acts on a polarized beam to create one which is actually "fully" polarized at every point, but in which the polarization varies so rapidly across the beam that it may be ignored in the intended applications.
Light is said to be "partially polarized" when there is more power in one polarization mode than another. At any particular wavelength, partially polarized light can be statistically described as the superposition of a completely unpolarized component, and a completely polarized one. One may then describe the light in terms of the degree of polarization, and the parameters of the polarized component. That polarized component can be described in terms of a Jones vector or polarization ellipse, as is detailed above. However, in order to also describe the degree of polarization, one normally employs Stokes parameters (see below) to specify a state of partial polarization.
Motivation.
The transmission of plane waves through a homogeneous medium are fully described in terms of Jones vectors and 2×2 Jones matrices. However, in practice there are cases in which all of the light cannot be viewed in such a simple manner due to spatial inhomogeneities or the presence of mutually incoherent waves. So-called depolarization, for instance, cannot be described using Jones matrices. For these cases it is usual instead to use a 4×4 matrix that acts upon the Stokes 4-vector. Such matrices were first used by Paul Soleillet in 1929, although they have come to be known as Mueller matrices. While every Jones matrix has a Mueller matrix, the reverse is not true. Mueller matrices are then used to describe the observed polarization effects of the scattering of waves from complex surfaces or ensembles of particles, as shall now be presented.
Coherency matrix.
The Jones vector perfectly describes the state of polarization "and phase" of a single monochromatic wave, representing a pure state of polarization as described above. However any mixture of waves of different polarizations (or even of different frequencies) do "not" correspond to a Jones vector. In so-called partially polarized radiation the fields are stochastic, and the variations and correlations between components of the electric field can only be described statistically. One such representation is the coherency matrix:
where angular brackets denote averaging over many wave cycles. Several variants of the coherency matrix have been proposed: the Wiener coherency matrix and the spectral coherency matrix of Richard Barakat measure the coherence of a spectral decomposition of the signal, while the Wolf coherency matrix averages over all time/frequencies.
The coherency matrix contains all second order statistical information about the polarization. This matrix can be decomposed into the sum of two idempotent matrices, corresponding to the eigenvectors of the coherency matrix, each representing a polarization state that is orthogonal to the other. An alternative decomposition is into completely polarized (zero determinant) and unpolarized (scaled identity matrix) components. In either case, the operation of summing the components corresponds to the incoherent superposition of waves from the two components. The latter case gives rise to the concept of the "degree of polarization"; i.e., the fraction of the total intensity contributed by the completely polarized component.
Stokes parameters.
The coherency matrix is not easy to visualize, and it is therefore common to describe incoherent or partially polarized radiation in terms of its total intensity ("I"), (fractional) degree of polarization ("p"), and the shape parameters of the polarization ellipse. An alternative and mathematically convenient description is given by the Stokes parameters, introduced by George Gabriel Stokes in 1852. The relationship of the Stokes parameters to intensity and polarization ellipse parameters is shown in the equations and figure below.
Here "Ip", 2ψ and 2χ are the spherical coordinates of the polarization state in the three-dimensional space of the last three Stokes parameters. Note the factors of two before ψ and χ corresponding respectively to the facts that any polarization ellipse is indistinguishable from one rotated by 180°, or one with the semi-axis lengths swapped accompanied by a 90° rotation. The Stokes parameters are sometimes denoted "I", "Q", "U" and "V".
Poincaré sphere.
Neglecting the first Stokes parameter "S"0 (or "I"), the three other Stokes parameters can be plotted directly in three dimensional Cartesian coordinates. For a given power in the polarized component given by:
the set of all polarization states are then mapped to points on the surface of the so-called "Poincaré sphere" (but of radius "P"), as shown in the accompanying diagram.
Often the total beam power is not of interest, in which case a normalized Stokes vector is used by dividing the Stokes vector by the total intensity "S"0: 
The normalized Stokes vector formula_32 then has unity power (formula_33) and the three significant Stokes parameters plotted in three dimensions will lie on the unity-radius Poincaré sphere for pure polarization states (where formula_34). Partially polarized states will lie "inside" the Poincaré sphere at a distance of formula_35 from the origin. When the non-polarized component is not of interest, the Stokes vector can be further normalized to obtain 
When plotted, that point will lie on the surface of the unity-radius Poincaré sphere and indicate the state of polarization of the polarized component.
Any two antipodal points on the Poincaré sphere refer to orthogonal polarization states. The overlap between any two polarization states is dependent solely on the distance between their locations along the sphere. This property, which can only be true when pure polarization states are mapped onto a sphere, is the motivation for the invention of the Poincaré sphere and the use of Stokes parameters which are thus plotted on (or beneath) it.
Implications for reflection and propagation.
Polarization in wave propagation.
In a vacuum, the components of the electric field propagate at the speed of light, so that the phase of the wave varies in space and time while the polarization state does not. That is, the electric field vector e of a plane wave in the +"z" direction follows:
where "k" is the wavenumber. As noted above, the instantaneous electric field is the real part of the product of the Jones vector times the phase factor formula_38. When an electromagnetic wave interacts with matter, its propagation is altered according to the material's (complex) index of refraction. When the real or imaginary part of that refractive index is dependent on the polarization state of a wave, properties known as birefringence and polarization dichroism (or diattenuation) respectively, then the polarization state of a wave will generally be altered.
In such media, an electromagnetic wave with any given state of polarization may be decomposed into two orthogonally polarized components that encounter different propagation constants. The effect of propagation over a given path on those two components is most easily characterized in the form of a complex 2×2 transformation matrix J known as a Jones matrix:
The Jones matrix due to passage through a transparent material is dependent on the propagation distance as well as the birefringence. The birefringence (as well as the average refractive index) will generally be dispersive, that is, it will vary as a function of optical frequency (wavelength). In the case of non-birefringent materials, however, the 2×2 Jones matrix is the identity matrix (multiplied by a scalar phase factor and attenuation factor), implying no change in polarization during propagation.
For propagation effects in two orthogonal modes, the Jones matrix can be written as
where "g"1 and "g"2 are complex numbers
describing the phase delay and possibly the amplitude attenuation due to propagation in each of the two polarization eigenmodes. T is a unitary matrix representing a change of basis from these propagation modes to the linear system used for the Jones vectors; in the case of linear birefringence or diattenuation the modes are themselves linear polarization states so T and T−1 can be omitted if the coordinate axes have been chosen appropriately.
Birefringence.
In media termed birefringent, in which the amplitudes are unchanged but a differential phase delay occurs, the Jones matrix is a unitary matrix: |"g"1| = |"g"2| = 1. Media termed diattenuating (or "dichroic" in the sense of polarization), in which only the amplitudes of the two polarizations are affected differentially, may be described using a Hermitian matrix (generally multiplied by a common phase factor). In fact, since "any" matrix may be written as the product of unitary and positive Hermitian matrices, light propagation through any sequence of polarization-dependent optical components can be written as the product of these two basic types of transformations.
In birefringent media there is no attenuation but two modes accrue a differential phase delay. Well known manifestations of linear birefringence (that is, in which the basis polarizations are orthogonal linear polarizations) appear in optical wave plates/retarders and many crystals. If linearly polarized light passes through a birefringent material, its state of polarization will generally change "unless" its polarization direction is identical to one of those basis polarizations. Since the phase shift, and thus the change in polarization state, is usually wavelength dependent, such objects viewed under white light in between two polarizers may give rise to colorful effects, as seen in the accompanying photograph.
Circular birefringence is also termed optical activity especially in chiral fluids, or Faraday rotation when due to the presence of a magnetic field along the direction of propagation. When linearly polarized light is passed through such an object, it will exit still linearly polarized but with the axis of polarization rotated. A combination of linear and circular birefringence will have as basis polarizations two orthogonal elliptical polarizations; the term "elliptical birefringence" however is rarely used.
One can visualize the case of linear birefringence (with two orthogonal linear propagation modes) with an incoming wave linearly polarized at a 45° angle to those modes. As a differential phase starts to accrue, the polarization becomes elliptical, eventually changing to purely circular polarization (90° phase difference), then to elliptical and eventually linear polarization (180° phase) perpendicular to the original polarization, then through circular again (270° phase), then elliptical with the original azimuth angle, and finally back to the original linearly polarized state (360° phase) where the cycle begins anew. In general the situation is more complicated and can be characterized as a rotation in the Poincaré sphere about the axis defined by the propagation modes. Examples for linear (blue), circular (red), and elliptical (yellow) birefringence are shown in the figure on the left. The total intensity and degree of polarization are unaffected. If the path length in the birefringent medium is sufficient, the two polarization components of a collimated beam (or ray) can exit the material with a positional offset, even though their final propagation directions will be the same (assuming the entrance face and exit face are parallel). This is commonly viewed using calcite crystals, which present the viewer with two slightly offset images, in opposite polarizations, of an object behind the crystal. It was this effect that provided the first discovery of polarization, by Erasmus Bartholinus in 1669.
Dichroism.
Media in which transmission of one polarization mode is preferentially reduced are called "dichroic" or "diattenuating". Like birefringence, diattenuation can be with respect to linear polarization modes (in a crystal) or circular polarization modes (usually in a liquid).
Devices that block nearly all of the radiation in one mode are known as "polarizing filters" or simply "polarizers". This corresponds to "g"2=0 in the above representation of the Jones matrix. The output of an ideal polarizer is a specific polarization state (usually linear polarization) with an amplitude equal to the input wave's original amplitude in that polarization mode. Power in the other polarization mode is eliminated. Thus if unpolarized light is passed through an ideal polarizer (where "g"1=1 and "g"2=0) exactly half of its initial power is retained. Practical polarizers, especially inexpensive sheet polarizers, have additional loss so that
"g"1 < 1. However, in many instances the more relevant figure of merit is the polarizer's degree of polarization or extinction ratio, which involve a comparison of "g"1 to "g"2. Since Jone's vectors refer to waves' amplitudes (rather than intensity), when illuminated by unpolarized light the remaining power in the unwanted polarization will be ("g"2/"g"1)2 of the power in the intended polarization.
Specular reflection.
In addition to birefringence and dichroism in extended media, polarization effects describable using Jones matrices can also occur at (reflective) interface between two materials of different refractive index. These effects are treated by the Fresnel equations. Part of the wave is transmitted and part is reflected; for a given material those proportions (and also the phase of reflection) are dependent on the angle of incidence and are different for the "s" and "p" polarizations. Therefore, the polarization state of reflected light (even if initially unpolarized) is generally changed.
Any light striking a surface at a special angle of incidence known as Brewster's angle, where the reflection coefficient for "p" polarization is zero, will be reflected with only the "s"-polarization remaining. This principle is employed in the so-called "pile of plates polarizer" (see figure) in which part of the "s" polarization is removed by reflection at each Brewster angle surface, leaving only the "p" polarization after transmission through many such surfaces. The generally smaller reflection coefficient of the "p" polarization is also the basis of polarized sunglasses; by blocking the "s" (horizontal) polarization, most of the glare due to reflection from a wet street, for instance, is removed.
In the important special case of reflection at normal incidence (not involving anisotropic materials) there is no particular "s" or "p" polarization. Both the "x" and "y" polarization components are reflected identically, and therefore the polarization of the reflected wave is identical to that of the incident wave. However, in the case of circular (or elliptical) polarization, the handedness of the polarization state is thereby reversed, since by convention this is specified relative to the direction of propagation. The circular rotation of the electric field around the "x-y" axes called "right-handed" for a wave in the "+z" direction is "left-handed" for a wave in the "-z" direction. But in the general case of reflection at a nonzero angle of incidence, no such generalization can be made. For instance, right-circularly polarized light reflected from a dielectric surface at a grazing angle, will still be right-handed (but elliptically) polarized. Linear polarized light reflected from a metal at non-normal incidence will generally become elliptically polarized. These cases are handled using Jones vectors acted upon by the different Fresnel coefficients for the "s" and "p" polarization components.
Measurement techniques involving polarization.
Some optical measurement techniques are based on polarization. In many other optical techniques polarization is crucial or at least must be taken into account and controlled; such examples are too numerous to mention.
Measurement of stress.
In engineering, the phenomenon of stress induced birefringence allows for stresses in transparent materials to be readily observed. As noted above and seen in the accompanying photograph, the chromaticity of birefringence typically creates colored patterns when viewed in between two polarizers. As external forces are applied, internal stress induced in the material is thereby observed. Additionally, birefringence is frequently observed due to stresses "frozen in" at the time of manufacture. This is famously observed in cellophane tape whose birefringence is due to the stretching of the material during the manufacturing process.
Ellipsometry.
Ellipsometry is a powerful technique for the measurement of the optical properties of a uniform surface. It involves measuring the polarization state of light following specular reflection from such a surface. This is typically done as a function of incidence angle or wavelength (or both). Since ellipsometry relies on reflection, it is not required for the sample to be transparent to light or for its back side to be accessible.
Ellipsometry can be used to model the (complex) refractive index of a surface of a bulk material. It is also very useful in determining parameters of one or more thin film layers deposited on a substrate. Due to their reflection properties, not only are the predicted magnitude of the "p" and "s" polarization components, but their relative phase shifts upon reflection, compared to measurements using an ellipsometer. A normal ellipsometer does not measure the actual reflection coefficient (which requires careful photometric calibration of the illuminating beam) but the ratio of the "p" and "s" reflections, as well as change of polarization ellipticity (hence the name) induced upon reflection by the surface being studied. In addition to use in science and research, ellipsometers are used in situ to control production processes for instance.
Geology.
The property of (linear) birefringence is widespread in crystalline minerals, and indeed was pivotal in the initial discovery of polarization. In mineralogy, this property is frequently exploited using polarization microscopes, for the purpose of identifying minerals. See optical mineralogy for more details.
Sound waves in solid materials exhibit polarization. Differential propagation of the three polarizations through the earth is a crucial in the field of seismology. Horizontally and vertically polarized seismic waves (shear waves)are termed SH and SV, while waves with longitudinal polarization (compressional waves) are termed P-waves.
Chemistry.
We have seen (above) that the birefringence of a type of crystal is useful in identifying it, and thus detection of linear birefringence is especially useful in geology and mineralogy. Linearly polarized light generally has its polarization state altered upon transmission through such a crystal, making it stand out when viewed in between two crossed polarizers, as seen in the photograph, above. Likewise, in chemistry, rotation of polarization axes in a liquid solution can be a useful measurement. In a liquid, linear birefringence is impossible, however there may be circular birefringence when a chiral molecule is in solution. When the right and left handed enantiomers of such a molecule are present in equal numbers (a so-called racemic mixture) then their effects cancel out. However, when there is only one (or a preponderance of one), as is more often the case for organic molecules, a net circular birefringence (or "optical activity") is observed, revealing the magnitude of that imbalance (or the concentration of the molecule itself, when it can be assumed that only one enantiomer is present). This is measured using a polarimeter in which polarized light is passed through a tube of the liquid, at the end of which is another polarizer which is rotated in order to null the transmission of light through it.
Astronomy.
In many areas of astronomy, the study of polarized electromagnetic radiation from outer space is of great importance. Although not usually a factor in the thermal radiation of stars, polarization is also present in radiation from coherent astronomical sources (e.g. hydroxyl or methanol masers), and incoherent sources such as the large radio lobes in active galaxies, and pulsar radio radiation (which may, it is speculated, sometimes be coherent), and is also imposed upon starlight by scattering from interstellar dust. Apart from providing information on sources of radiation and scattering, polarization also probes the interstellar magnetic field via Faraday rotation. The polarization of the cosmic microwave background is being used to study the physics of the very early universe. Synchrotron radiation is inherently polarised. It has been suggested that astronomical sources caused the chirality of biological molecules on Earth.
Applications and examples.
Polarized sunglasses.
Unpolarized light, after reflection at a specular (shiny) surface, generally obtains a degree of polarization. This phenomenon was observed in 1808 by the mathematician Étienne-Louis Malus after whom Malus' law is named. Polarizing sunglasses exploit this effect to reduce glare from reflections by horizontal surfaces, notably the road ahead viewed at a grazing angle.
Wearers of polarized sunglasses will occasionally observe inadvertent polarization effects such as color-dependent birefringent effects, for example in toughened glass (e.g., car windows) or items made from transparent plastics, in conjunction with natural polarization by reflection or scattering. The polarized light from LCD monitors (see below) is very conspicuous when these are worn.
Sky polarization and photography.
Polarization is observed in the light of the sky, as this is due to sunlight scattered by aerosols as it passes through the earth's atmosphere. The scattered light produces the brightness and color in clear skies. This partial polarization of scattered light can be used to darken the sky in photographs, increasing the contrast. This effect is most strongly observed at points on the sky making a 90° angle to the sun. Polarizing filters use these effects to optimize the results of photographing scenes in which reflection or scattering by the sky is involved.
Sky polarization has been used for orientation in navigation. The "sky compass", was used in the 1950s when navigating near the poles of the Earth's magnetic field when neither the sun nor stars were visible (e.g., under daytime cloud or twilight). It has been suggested, controversially, that the Vikings exploited a similar device (the "sunstone") in their extensive expeditions across the North Atlantic in the 9th–11th centuries, before the arrival of the magnetic compass in Europe in the 12th century. Related to the sky compass is the "polar clock", invented by Charles Wheatstone in the late 19th century.
Display technologies.
The principle of liquid-crystal display (LCD) technology relies on the rotation of the axis of linear polarization by the liquid crystal array. Light from the backlight (or the back reflective layer, in devices not including or requiring a backlight) first passes through a linear polarizing sheet. That polarized light passes through the actual liquid crystal layer which may be organized in pixels (for a TV or computer monitor) or in another format such as a seven-segment display or one with custom symbols for a particular product. The liquid crystal layer is produced with a consistent right (or left) handed chirality, essentially consisting of tiny helices. This causes circular birefringence, and is engineered so that there is a 90 degree rotation of the linear polarization state. However, when a voltage is applied across a cell, the molecules straighten out, lessening or totally losing the circular birefringence. On the viewing side of the display is another linear polarizing sheet, usually oriented at 90 degrees from the one behind the active layer. Therefore, when the circular birefringence is removed by the application of a sufficient voltage, the polarization of the transmitted light remains at right angles to the front polarizer, and the pixel appears dark. With no voltage, however, the 90 degree rotation of the polarization causes it to exactly match the axis of the front polarizer, allowing the light through. Intermediate voltages create intermediate rotation of the polarization axis and the pixel has an intermediate intensity. Displays based on this principle are widespread, and now are used in the vast majority of televisions, computer monitors and video projectors, rendering the previous CRT technology essentially obsolete. The use of polarization in the operation of LCD displays is immediately apparent to someone wearing polarized sunglasses, often making the display unreadable.
In a totally different sense, polarization encoding has become the leading (but not sole) method for delivering separate images to the left and right eye in stereoscopic displays used for 3D movies. This involves separate images intended for each eye either projected from two different projectors with orthogonally oriented polarizing filters or, more typically, from a single projector with time multiplexed polarization (a fast alternating polarization device for successive frames). Polarized 3D glasses with suitable polarizing filters ensure that each eye receives only the intended image. Historically such systems used linear polarization encoding because it was inexpensive and offered good separation. However circular polarization makes separation of the two images insensitive to tilting of the head, and is widely used in 3-D movie exhibition today, such as the system from RealD. Projecting such images requires screens that maintain the polarization of the projected light when viewed in reflection (such as silver screens); a normal diffuse white projection screen causes depolarization of the projected images, making it unsuitable for this application.
Although now obsolete, CRT computer displays suffered from reflection by the glass envelope, causing glare from room lights and consequently poor contrast. Several anti-reflection solutions were employed to ameliorate this problem. One solution utilized the principle of reflection of circularly polarized light. A circular polarizing filter in front of the screen allows for the transmission of (say) only right circularly polarized room light. Now, right circularly polarized light (depending on the convention used) has its electric (and magnetic) field direction rotating clockwise while propagating in the +z direction. Upon reflection, the field still has the same direction of rotation, but now propagation is in the −z direction making the reflected wave "left" circularly polarized. With the right circular polarization filter placed in front of the reflecting glass, the unwanted light reflected from the glass will thus be in very polarization state that is "blocked" by that filter, eliminating the reflection problem. The reversal of circular polarization on reflection and elimination of reflections in this manner can be easily observed by looking in a mirror while wearing 3-D movie glasses which employ left and right handed circular polarization in the two lenses. Closing one eye, the other eye will see a reflection in which it cannot see itself; that lens appears black! However the other lens (of the closed eye) will have the correct circular polarization allowing the closed eye to be easily seen by the open one.
Radio transmission and reception.
All radio (and microwave) antennas used for transmitting or receiving are intrinsically polarized. They transmit in (or receive signals from) a particular polarization, being totally insensitive to the opposite polarization; in certain cases that polarization is a function of direction. As is the convention in optics, the "polarization" of a radio wave is understood to refer to the polarization of its electric field, with the magnetic field being at a 90 degree rotation with respect to it for a linearly polarized wave.
The vast majority of antennas are linearly polarized. In fact it can be shown from considerations of symmetry that an antenna that lies entirely in a plane which also includes the observer, can "only" have its polarization in the direction of that plane. This applies to many cases, allowing one to easily infer such an antenna's polarization at an intended direction of propagation. So a typical rooftop Yagi or log-periodic antenna with horizontal conductors, as viewed from a second station toward the horizon, is necessarily horizontally polarized. But a vertical "whip antenna" or AM broadcast tower used as an antenna element (again, for observers horizontally displaced from it) will transmit in the vertical polarization. A turnstile antenna with its four arms in the horizontal plane, likewise transmits horizontally polarized radiation toward the horizon. However, when that same turnstile antenna is used in the "axial mode" (upwards, for the same horizontally-oriented structure) its radiation is circularly polarized. At intermediate elevations it is elliptically polarized.
Polarization is important in radio communications because, for instance, if one attempts to use a horizontally polarized antenna to receive a vertically polarized transmission, the signal strength will be substantially reduced (or under very controlled conditions, reduced to nothing). This principle is used in satellite television in order to double the channel capacity over a fixed frequency band. The same frequency channel can be used for two signals broadcast in opposite polarizations. By adjusting the receiving antenna for one or the other polarization, either signal can be selected without interference from the other.
Especially due to the presence of the ground, there are some differences in propagation (and also in reflections responsible for TV ghosting) between horizontal and vertical polarizations. AM and FM broadcast radio usually use vertical polarization, while television uses horizontal polarization. At low frequencies especially, horizontal polarization is avoided. That is because the phase of a horizontally polarized wave is reversed upon reflection by the ground. A distant station in the horizontal direction will receive both the direct and reflected wave, which thus tend to cancel each other. This problem is avoided with vertical polarization. Polarization is also important in the transmission of radar pulses and reception of radar reflections by the same or a different antenna. For instance, back scattering of radar pulses by rain drops can be avoided by using circular polarization. Just as specular reflection of circularly polarized light reverses the handedness of the polarization, as discussed above, the same principle applies to scattering by objects much smaller than a wavelength such as rain drops. On the other hand, reflection of that wave by an irregular metal object (such as an airplane) will typically introduce a change in polarization and (partial) reception of the return wave by the same antenna.
The effect of free electrons in the ionosphere, in conjunction with the earth's magnetic field, causes Faraday rotation, a sort of circular birefringence. This is the same mechanism which can rotate the axis of linear polarization by electrons in interstellar space as mentioned below. The magnitude of Faraday rotation caused by such a plasma is greatly exaggerated at lower frequencies, so at the higher microwave frequencies used by satellites the effect is minimal. However medium or short wave transmissions received following refraction by the ionosphere are strongly affected. Since a wave's path through the ionosphere and the earth's magnetic field vector along such a path are rather unpredictable, a wave transmitted with vertical (or horizontal) polarization will generally have a resulting polarization in an arbitrary orientation at the receiver.
Polarization and vision.
Many animals are capable of perceiving some of the components of the polarization of light, e.g., linear horizontally polarized light. This is generally used for navigational purposes, since the linear polarization of sky light is always perpendicular to the direction of the sun. This ability is very common among the insects, including bees, which use this information to orient their communicative dances. Polarization sensitivity has also been observed in species of octopus, squid, cuttlefish, and mantis shrimp. In the latter case, one species measures all six orthogonal components of polarization, and is believed to have optimal polarization vision. The rapidly changing, vividly colored skin patterns of cuttlefish, used for communication, also incorporate polarization patterns, and mantis shrimp are known to have polarization selective reflective tissue. Sky polarization was thought to be perceived by pigeons, which was assumed to be one of their aids in homing, but research indicates this is a popular myth.
The naked human eye is weakly sensitive to polarization, without the need for intervening filters. Polarized light creates a very faint pattern near the center of the visual field, called Haidinger's brush. This pattern is very difficult to see, but with practice one can learn to detect polarized light with the naked eye.
Angular momentum using circular polarization.
It is well known that electromagnetic radiation carries a certain linear momentum in the direction of propagation. In addition, however, light carries a certain angular momentum if it is circularly polarized (or partially so). In comparison with lower frequencies such as microwaves, the amount of angular momentum in light, even of pure circular polarization, compared to the same wave's linear momentum (or radiation pressure) is very small and difficult to even measure. However it was utilized in an experiment to achieve speeds of up to 600 million revolutions per minute.

</doc>
<doc id="698648" url="https://en.wikipedia.org/wiki?curid=698648" title="Polarization density">
Polarization density

In classical electromagnetism, polarization density (or electric polarization, or simply polarization) is the vector field that expresses the density of permanent or induced electric dipole moments in a dielectric material. When a dielectric is placed in an external electric field, its molecules gain electric dipole moment and the dielectric is said to be polarized. The electric dipole moment induced per unit volume of the dielectric material is called the electric polarization of the dielectric.
Polarization density also describes how a material responds to an applied electric field as well as the way the material changes the electric field, and can be used to calculate the forces that result from those interactions. It can be compared to magnetization, which is the measure of the corresponding response of a material to a magnetic field in magnetism. The SI unit of measure is coulombs per square meter, and polarization density is represented by a vector P.
Definition.
An external electric field that is applied to a dielectric material, causes a displacement of bound charged elements. These are elements which are bound to molecules and are not free to move around the material. Positive charged elements are displaced in the direction of the field, and negative charged elements are displaced opposite to the direction of the field. The molecules may remain neutral in charge, yet an electric dipole moment forms.
For a certain volume element in the material formula_1, which carries a dipole moment formula_2, we define the polarization vector P:
In general, the dipole moment formula_2 changes from point to point within the dielectric. Hence, the polarization density P of an infinitesimal change dp in the dipole moment for a given change d"V" in the volume is:
The net charge appearing as a result of polarization is called bound charge and denoted formula_6.
Other Expressions.
Let a volume d"V" be isolated inside the dielectric. Due to polarization the positive bound charge formula_7 will be displaced a distance formula_8 relative to the negative bound charge formula_9, giving rise to a dipole moment formula_10. Replacing this expression into (1) we get:
Since the charge formula_12 bounded in the volume d"V" is equal to formula_13 the equation for P becomes:
where formula_15 is the density of the bound charge in the volume under consideration.
Gauss's Law for the Field of "P".
For a given volume "V" enclosed by a surface "S", the bound charge formula_6 inside it is equal to the flux of P through "S" taken with the negative sign, or
Differential Form.
By the divergence theorem, Gauss's law for the field P can be stated in "differential form" as:
where is the divergence of the field P through a given surface containing the bound charge density formula_18.
Relationship between the fields of "P" and "E".
Homogeneous, Isotropic Dielectrics.
In a homogeneous linear and isotropic dielectric medium, the polarization is aligned with and proportional to the electric field E:
where ε0 is the electric constant, and χ is the electric susceptibility of the medium. Note that χ is just a scalar. This is a particular case due to the "isotropy" of the dielectric.
Taking into account this relation between P and E, equation (3) becomes:
The expression in the integral is Gauss's law for the field E which yields the total charge, both free formula_20 and bound formula_21, in the volume "V" enclosed by "S". Therefore
which can be written in terms of free charge and bound charge densities (by considering the relationship between the charges, their volume charge densities and the given volume):
Since within a homogeneous dielectric there can be no free charges formula_24, by the last equation it follows that there is no bulk bound charge in the material formula_25. And since free charges can get as close to the dielectric as to its topmost surface, it follows that polarization only gives rise to surface bound charge density (denoted formula_26 to avoid ambiguity with the volume bound charge density formula_18).
formula_26 may be related to P by the following equation:
where formula_30 is the normal vector to the surface "S" pointing outwards.
Anisotropic Dielectrics.
The class of dielectrics where the polarization density and the electric field are not in the same direction are known as "anisotropic" materials.
In such materials, the ith component of the polarization is related to the jth component of the electric field according to:
This relation shows, for example, that a material can polarize in the x direction by applying a field in the z direction, and so on. The case of an anisotropic dielectric medium is described by the field of crystal optics.
As in most electromagnetism, this relation deals with macroscopic averages of the fields and dipole density, so that one has a continuum approximation of the dielectric materials that neglects atomic-scale behaviors. The polarizability of individual particles in the medium can be related to the average susceptibility and polarization density by the Clausius-Mossotti relation.
In general, the susceptibility is a function of the frequency ω of the applied field. When the field is an arbitrary function of time "t", the polarization is a convolution of the Fourier transform of χ(ω) with the E("t"). This reflects the fact that the dipoles in the material cannot respond instantaneously to the applied field, and causality considerations lead to the Kramers–Kronig relations.
If the polarization P is not linearly proportional to the electric field E, the medium is termed "nonlinear" and is described by the field of nonlinear optics. To a good approximation (for sufficiently weak fields, assuming no permanent dipole moments are present), P is usually given by a Taylor series in E whose coefficients are the nonlinear susceptibilities:
where formula_33 is the linear susceptibility, formula_34 is the second-order susceptibility (describing phenomena such as the Pockels effect, optical rectification and second-harmonic generation), and formula_35 is the third-order susceptibility (describing third-order effects such as the Kerr effect and electric field-induced optical rectification).
In ferroelectric materials, there is no one-to-one correspondence between P and E at all because of hysteresis.
Polarization density in Maxwell's equations.
The behavior of electric fields (E and D), magnetic fields (B, H), charge density (ρ) and current density (J) are described by Maxwell's equations in matter.
Relations between E, D and P.
In terms of volume charge densities, the free charge density formula_36 is given by
where formula_38 is the total charge density. By considering the relationship of each of the terms of the above equation to the divergence of their corresponding fields (of the electric displacement field D, E and P in that order), this can be written as:
Here "ε0" is the electric permittivity of empty space. In this equation, P is the (negative of the) field induced in the material when the "fixed" charges, the dipoles, shift in response to the total underlying field E, whereas D is the field due to the remaining charges, known as "free" charges. In general, P varies as a function of E depending on the medium, as described later in the article. In many problems, it is more convenient to work with D and the free charges than with E and the total charge.
Time-varying Polarization Density.
When the polarization density changes with time, the time-dependent bound-charge density creates a "polarization current density" of
so that the total current density that enters Maxwell's equations is given by
where Jf is the free-charge current density, and the second term is the magnetization current density (also called the "bound current density"), a contribution from atomic-scale (when they are present).
Polarization ambiguity.
The polarization inside a solid is not, in general, uniquely defined: It depends on which electrons are paired up with which nuclei. (See figure.) In other words, two people, Alice and Bob, looking at the same solid, may calculate different values of P, and neither of them will be wrong. Alice and Bob will agree on the microscopic electric field E in the solid, but disagree on the value of the displacement field formula_42. They will both find that Gauss's law is correct (formula_43), but they will disagree on the value of formula_36 at the surfaces of the crystal. For example, if Alice interprets the bulk solid to consist of dipoles with positive ions above and negative ions below, but the real crystal has negative ions as the topmost surface, then Alice will say that there is a negative free charge at the topmost surface. (She might view this as a type of surface reconstruction).
On the other hand, even though the value of P is not uniquely defined in a bulk solid, "variations" in P "are" uniquely defined. If the crystal is gradually changed from one structure to another, there will be a current inside each unit cell, due to the motion of nuclei and electrons. This current results in a macroscopic transfer of charge from one side of the crystal to the other, and therefore it can be measured with an ammeter (like any other current) when wires are attached to the opposite sides of the crystal. The time-integral of the current is proportional to the change in P. The current can be calculated in computer simulations (such as density functional theory); the formula for the integrated current turns out to be a type of Berry's phase.
The non-uniqueness of P is not problematic, because every measurable consequence of P is in fact a consequence of a continuous change in P. For example, when a material is put in an electric field E, which ramps up from zero to a finite value, the material's electronic and ionic positions slightly shift. This changes P, and the result is electric susceptibility (and hence permittivity). As another example, when some crystals are heated, their electronic and ionic positions slightly shift, changing P. The result is pyroelectricity. In all cases, the properties of interest are associated with a "change" in P.
Even though the polarization is "in principle" non-unique, in practice it is often (not always) defined by convention in a specific, unique way. For example, in a perfectly centrosymmetric crystal, P is usually defined by convention to be exactly zero. As another example, in a ferroelectric crystal, there is typically a centrosymmetric configuration above the Curie temperature, and P is defined there by convention to be zero. As the crystal is cooled below the Curie temperature, it shifts gradually into a more and more non-centrosymmetric configuration. Since gradual changes in P are uniquely defined, this convention gives a unique value of P for the ferroelectric crystal, even below its Curie temperature.

</doc>
<doc id="23000" url="https://en.wikipedia.org/wiki?curid=23000" title="Polynomial">
Polynomial

In mathematics, a polynomial is an expression consisting of variables (or indeterminates) and coefficients, that involves only the operations of addition, subtraction, multiplication, and non-negative integer exponents. An example of a polynomial of a single indeterminate (or variable), , is , which is a quadratic polynomial. An example in three variables is .
Polynomials appear in a wide variety of areas of mathematics and science. For example, they are used to form polynomial equations, which encode a wide range of problems, from elementary word problems to complicated problems in the sciences; they are used to define polynomial functions, which appear in settings ranging from basic chemistry and physics to economics and social science; they are used in calculus and numerical analysis to approximate other functions. In advanced mathematics, polynomials are used to construct polynomial rings and algebraic varieties, central concepts in algebra and algebraic geometry.
Etymology.
According to the Oxford English Dictionary, "polynomial" succeeded the term "binomial", and was made simply by replacing the Latin root "bi-" with the Greek "poly-", which comes from the Greek word for "many". The word "polynomial" was first used in the 17th century.
Notation and terminology.
The "x" occurring in a polynomial is commonly called either a "variable" or an "indeterminate". When the polynomial is considered for itself, "x" is a fixed symbol which does not have any value (its value is "indeterminate"). It is thus more correct to call it an "indeterminate". However, when one considers the function defined by the polynomial, then "x" represents the argument of the function, and is therefore called a "variable". Many authors use these two words interchangeably, but this may be sometimes confusing and is not done in this article.
It is a common convention to use uppercase letters for the indeterminates and the corresponding lowercase letters for the variables (arguments) of the associated function.
It may be confusing that a polynomial "P" in the indeterminate "X" may appear in the formulas either as "P" or as "P"("X").
Normally, the name of the polynomial is "P", not "P"("X"). However, if "a" denotes a number, a variable, another polynomial, or, more generally any expression, then "P"("a") denotes, by convention, the result of substituting "X" by "a" in "P". For example, the polynomial "P" defines the function
In particular, if "a" = "X", then the definition of "P"("a") implies
This equality allows writing "let "P"("X") be a polynomial" as a shorthand for "let "P" be a polynomial in the indeterminate "X"". On the other hand, when it is not necessary to emphasize the name of the indeterminate, many formulas are much simpler and easier to read if the name(s) of the indeterminate(s) do not appear at each occurrence of the polynomial.
Definition.
A polynomial in a single indeterminate can be written in the form
where formula_4 are numbers, or more generally elements of a ring, and formula_5 is a symbol which is called an indeterminate or, for historical reasons, a variable. The symbol formula_5 does not represent any value, although the usual (commutative, distributive) laws valid for arithmetic operations also apply to it.
This can be expressed more concisely by using summation notation:
That is, a polynomial can either be zero or can be written as the sum of a finite number of non-zero terms. Each term consists of the product of a number—called the coefficient of the term—and a finite number of indeterminates, raised to nonnegative integer powers. The exponent on an indeterminate in a term is called the degree of that indeterminate in that term; the degree of the term is the sum of the degrees of the indeterminates in that term, and the degree of a polynomial is the largest degree of any one term with nonzero coefficient. Because , the degree of an indeterminate without a written exponent is one. A term and a polynomial with no indeterminates are called respectively a constant term and a constant polynomial; the degree of a constant term and of a nonzero constant polynomial is 0. The degree of the zero polynomial (which has no term) is generally treated as not defined (but see below).
For example:
is a term. The coefficient is , the indeterminates are and , the degree of is two, while the degree of is one. The degree of the entire term is the sum of the degrees of each indeterminate in it, so in this example the degree is .
Forming a sum of several terms produces a polynomial. For example, the following is a polynomial:
It consists of three terms: the first is degree two, the second is degree one, and the third is degree zero.
Polynomials of small degree have been given specific names. A polynomial of degree zero is a "constant polynomial" or simply a "constant". Polynomials of degree one, two or three are respectively "linear polynomials," "quadratic polynomials" and "cubic polynomials". For higher degrees the specific names are not commonly used, although "quartic polynomial" (for degree four) and "quintic polynomial" (for degree five) are sometimes used. The names for the degrees may be applied to the polynomial or to its terms. For example, in the term is a linear term in a quadratic polynomial.
The polynomial 0, which may be considered to have no terms at all, is called the zero polynomial. Unlike other constant polynomials, its degree is not zero. Rather the degree of the zero polynomial is either left explicitly undefined, or defined as negative (either −1 or −∞). These conventions are useful when defining Euclidean division of polynomials. The zero polynomial is also unique in that it is the only polynomial having an infinite number of roots. In the case of polynomials in more than one indeterminate, a polynomial is called "homogeneous" of if "all" its non-zero terms have . The zero polynomial is homogeneous, and, as homogeneous polynomial, its degree is undefined. For example, is homogeneous of degree 5. For more details, see homogeneous polynomial.
The commutative law of addition can be used to rearrange terms into any preferred order. In polynomials with one indeterminate, the terms are usually ordered according to degree, either in "descending powers of ", with the term of largest degree first, or in "ascending powers of ". The polynomial in the example above is written in descending powers of . The first term has coefficient , indeterminate , and exponent . In the second term, the coefficient . The third term is a constant. Because the "degree" of a non-zero polynomial is the largest degree of any one term, this polynomial has degree two.
Two terms with the same indeterminates raised to the same powers are called "similar terms" or "like terms", and they can be combined, using the distributive law, into a single term whose coefficient is the sum of the coefficients of the terms that were combined. It may happen that this makes the coefficient 0. Polynomials can be classified by the number of terms with nonzero coefficients, so that a one-term polynomial is called a monomial, a two-term polynomial is called a binomial, and a three-term polynomial is called a "trinomial". The term "quadrinomial" is occasionally used for a four-term polynomial.
A polynomial in one indeterminate is called a "univariate polynomial", a polynomial in more than one indeterminate is called a multivariate polynomial. A polynomial with two indeterminates is called a bivariate polynomial. These notions refer more to the kind of polynomials one is generally working with than to individual polynomials; for instance when working with univariate polynomials one does not exclude constant polynomials (which may result, for instance, from the subtraction of non-constant polynomials), although strictly speaking constant polynomials do not contain any indeterminates at all. It is possible to further classify multivariate polynomials as "bivariate", "trivariate", and so on, according to the maximum number of indeterminates allowed. Again, so that the set of objects under consideration be closed under subtraction, a study of trivariate polynomials usually allows bivariate polynomials, and so on. It is common, also, to say simply "polynomials in , and ", listing the indeterminates allowed.
The "evaluation of a polynomial" consists of substituting a numerical value to each indeterminate and carrying out the indicated multiplications and additions. For polynomials in one indeterminate, the evaluation is usually more efficient (lower number of arithmetic operations to perform) using Horner's method:
Arithmetic of polynomials.
Polynomials can be added using the associative law of addition (grouping all their terms together into a single sum), possibly followed by reordering, and combining of like terms. For example, if
then
which can be simplified to
To work out the product of two polynomials into a sum of terms, the distributive law is repeatedly applied, which results in each term of one polynomial being multiplied by every term of the other. For example, if
then
which can be simplified to
Polynomial evaluation can be used to compute the remainder of polynomial division by a polynomial of degree one, because the remainder of the division of by is ; see the polynomial remainder theorem. This is more efficient than the usual algorithm of division when the quotient is not needed.
As for the integers, two kinds of divisions are considered for the polynomials. The "Euclidean division of polynomials" that generalizes the Euclidean division of the integers. It results in two polynomials, a "quotient" and a "remainder" that are characterized by the following property of the polynomials: given two polynomials "a" and "b" such that "b" ≠ 0, there exists a unique pair of polynomials, "q", the quotient, and "r", the remainder, such that and (here the polynomial zero is supposed to have a negative degree). By hand as well as with a computer, this division can be computed by the polynomial long division algorithm.
All polynomials with coefficients in a unique factorization domain (for example, the integers or a field) also have a factored form in which the polynomial is written as a product of irreducible polynomials and a constant. This factored form is unique up to the order of the factors and their multiplication by an invertible constant. In the case of the field of complex numbers, the irreducible factors are linear. Over the real numbers, they have the degree either one or two. Over the integers and the rational numbers the irreducible factors may have any degree. For example, the factored form of
is
over the integers and the reals and
over the complex numbers.
The computation of the factored form, called "factorization" is, in general, too difficult to be done by hand-written computation. However, efficient polynomial factorization algorithms are available in most computer algebra systems.
A formal quotient of polynomials, that is, an algebraic fraction wherein the numerator and denominator are polynomials, is called a "rational expression" or "rational fraction" and is not, in general, a polynomial. Division of a polynomial by a number, however, yields another polynomial. For example, is considered a valid term in a polynomial (and a polynomial by itself) because it is equivalent to and is just a constant. When this expression is used as a term, its coefficient is therefore . For similar reasons, if complex coefficients are allowed, one may have a single term like ; even though it looks like it should be expanded to two terms, the complex number is one complex number, and is the coefficient of that term. The expression is not a polynomial because it includes division by a non-constant polynomial. The expression is not a polynomial, because it contains an indeterminate used as exponent.
Because subtraction can be replaced by addition of the opposite quantity, and because positive integer exponents can be replaced by repeated multiplication, all polynomials can be constructed from constants and indeterminates using only addition and multiplication.
Polynomial functions.
A "polynomial function" is a function that can be defined by evaluating a polynomial. A function of one argument is called a polynomial function if it satisfies
for all arguments , where is a non-negative integer and are constant coefficients.
For example, the function , taking real numbers to real numbers, defined by
is a polynomial function of one variable. Polynomial functions of multiple variables can also be defined, using polynomials in multiple indeterminates, as in
An example is also the function formula_23 which, although it doesn't look like a polynomial, is a polynomial function on formula_24 because for every formula_5 from formula_24 it is true that formula_27 (see Chebyshev polynomials).
Polynomial functions are a class of functions having many important properties. They are all continuous, smooth, entire, computable, etc.
Graphs of polynomial functions.
A polynomial function in one real variable can be represented by a graph.
The graph of a non-constant (univariate) polynomial always tends to infinity when the variable increases indefinitely (in absolute value).
Polynomial graphs are analyzed in calculus using intercepts, slopes, concavity, and end behavior.
Polynomial equations.
A "polynomial equation", also called "algebraic equation", is an equation of the form
For example,
is a polynomial equation.
In case of a univariate polynomial equation, the variable is considered an unknown, and one seeks to find the possible values for which both members of the equation evaluate to the same value (in general more than one solution may exist). A polynomial equation stands in contrast to a "polynomial identity" like , where both expressions represent the same polynomial in different forms, and as a consequence any evaluation of both members gives a valid equality.
In elementary algebra, methods such as the quadratic formula are given for solving all first degree and second degree polynomial equations in one variable. There are also formulas for the cubic and quartic equations. For higher degrees, the Abel–Ruffini theorem asserts that there can not exist a general formula in radicals. However, root-finding algorithms may be used to find numerical approximations of the roots of a polynomial expression of any degree.
The number of real solutions of a polynomial equation with real coefficients may not exceed the degree, and equals the degree when the complex solutions are counted with their multiplicity. This fact is called the fundamental theorem of algebra.
Solving polynomial equations.
Every polynomial in corresponds to a function, (where the occurrences of in are interpreted as the argument of ), called the "polynomial function" of ; the equation in setting is the "polynomial equation" corresponding to . The solutions of this equation are called the "roots" of the polynomial; they are the "zeroes" of the function (corresponding to the points where the graph of meets the -axis). A number is a root of if and only if the polynomial (of degree one in ) divides . It may happen that divides more than once: if divides then is called a "multiple root" of , and otherwise is called a "simple root" of . If is a nonzero polynomial, there is a highest power such that divides , which is called the "multiplicity" of the root in . When is the zero polynomial, the corresponding polynomial equation is trivial, and this case is usually excluded when considering roots: with the above definitions every number would be a root of the zero polynomial, with undefined (or infinite) multiplicity. With this exception made, the number of roots of , even counted with their respective multiplicities, cannot exceed the degree of . The relation between the roots of a polynomial and its coefficients is described by Viète's formulas.
Some polynomials, such as , do not have any roots among the real numbers. If, however, the set of allowed candidates is expanded to the complex numbers, every non-constant polynomial has at least one root; this is the fundamental theorem of algebra. By successively dividing out factors , one sees that any polynomial with complex coefficients can be written as a constant (its leading coefficient) times a product of such polynomial factors of degree 1; as a consequence, the number of (complex) roots counted with their multiplicities is exactly equal to the degree of the polynomial.
There is a difference between approximating roots and finding exact expressions for roots. Formulas for expressing the roots of polynomials of degree 2 in terms of square roots have been known since ancient times (see quadratic equation), and for polynomials of degree 3 or 4 similar formulas (using cube roots in addition to square roots) were found in the 16th century (see cubic function and quartic function for the formulas and Niccolò Fontana Tartaglia, Lodovico Ferrari, Gerolamo Cardano, and Vieta for historical details). But formulas for degree 5 eluded researchers. In 1824, Niels Henrik Abel proved the striking result that there can be no general (finite) formula, involving only arithmetic operations and radicals, that expresses the roots of a polynomial of degree 5 or greater in terms of its coefficients (see Abel–Ruffini theorem). In 1830, Évariste Galois, studying the permutations of the roots of a polynomial, extended the Abel–Ruffini theorem by showing that, given a polynomial equation, one may decide whether it is solvable by radicals, and, if it is, solve it. This result marked the start of Galois theory and group theory, two important branches of modern mathematics. Galois himself noted that the computations implied by his method were impracticable. Nevertheless, formulas for solvable equations of degrees 5 and 6 have been published (see quintic function and sextic equation).
Numerical approximation of roots of polynomials in one unknown is easily done on a computer by the Jenkins–Traub method, Laguerre's method, Durand–Kerner method, or by some other root-finding algorithm.
For polynomials in more than one indeterminate the notion of root does not exist, and there are usually infinitely many combinations of values for the variables for which the polynomial function takes the value zero. However, for certain "sets" of such polynomials it may happen that for only finitely many combinations all polynomial functions take the value zero.
For a set of polynomial equations in several unknowns, there are algorithms to decide whether they have a finite number of complex solutions. If the number of solutions is finite, there are algorithms to compute the solutions. The methods underlying these algorithms are described in the article systems of polynomial equations.
The special case where all the polynomials are of degree one is called a system of linear equations, for which another range of different solution methods exist, including the classical Gaussian elimination.
Generalizations of polynomials.
There are at least two ways to generalize polynomials:
Trigonometric polynomials.
A trigonometric polynomial is a finite linear combination of functions sin("nx") and cos("nx") with "n" taking on the values of one or more natural numbers. The coefficients may be taken as real numbers, for real-valued functions. For complex coefficients, there is no difference between such a function and a finite Fourier series.
Trigonometric polynomials are widely used, for example in trigonometric interpolation applied to the interpolation of periodic functions. They are used also in the discrete Fourier transform.
The term "trigonometric polynomial" for the real-valued case can be seen as using the analogy: the functions sin("nx") and cos("nx") are similar to the monomial basis for polynomials. In the complex case the trigonometric polynomials are spanned by the positive and negative powers of "e""ix".
Matrix polynomials.
A matrix polynomial is a polynomial with matrices as variables. Given an ordinary, scalar-valued polynomial
this polynomial evaluated at a matrix "A" is
where "I" is the identity matrix.
A matrix polynomial equation is an equality between two matrix polynomials, which holds for the specific matrices in question. A matrix polynomial identity is a matrix polynomial equation which holds for all matrices "A" in a specified matrix ring "Mn"("R").
Laurent polynomials.
Laurent polynomials are like polynomials, but allow negative powers of the variable(s) to occur.
Rational functions.
A rational fraction is the quotient (algebraic fraction) of two polynomials. Any algebraic expression that can be rewritten as a rational fraction is a rational function.
While polynomial functions are defined for all values of the variables, a rational function is defined only for the values of the variables for which the denominator is not zero.
The rational fractions include the Laurent polynomials, but do not limit denominators to powers of an indeterminate.
Power series.
Formal power series are like polynomials, but allow infinitely many non-zero terms to occur, so that they do not have finite degree. Unlike polynomials they cannot in general be explicitly and fully written down (just like real numbers cannot), but the rules for manipulating their terms are the same as for polynomials. Non-formal power series also generalize polynomials, but the multiplication of two power series may not converge.
Applications of polynomials.
Calculus.
The simple structure of polynomial functions makes them quite useful in analyzing general functions using polynomial approximations. An important example in calculus is Taylor's theorem, which roughly states that every differentiable function locally looks like a polynomial function, and the Stone–Weierstrass theorem, which states that every continuous function defined on a compact interval of the real axis can be approximated on the whole interval as closely as desired by a polynomial function.
Calculating derivatives and integrals of polynomial functions is particularly simple. For the polynomial function
the derivative with respect to "x" is
and the indefinite integral is
Abstract algebra.
In abstract algebra, one distinguishes between "polynomials" and "polynomial functions". A "polynomial" in one indeterminate over a ring is defined as a formal expression of the form
where is a natural number, the coefficients are elements of , and is a formal symbol, whose powers are just placeholders for the corresponding coefficients , so that the given formal expression is just a way to encode the sequence , where there is an such that for all . Two polynomials sharing the same value of "n" are considered equal if and only if the sequences of their coefficients are equal; furthermore any polynomial is equal to any polynomial with greater value of obtained from it by adding terms in front whose coefficient is zero. These polynomials can be added by simply adding corresponding coefficients (the rule for extending by terms with zero coefficients can be used to make sure such coefficients exist). Thus each polynomial is actually equal to the sum of the terms used in its formal expression, if such a term is interpreted as a polynomial that has zero coefficients at all powers of other than . Then to define multiplication, it suffices by the distributive law to describe the product of any two such terms, which is given by the rule
Thus the set of all polynomials with coefficients in the ring forms itself a ring, the "ring of polynomials" over , which is denoted by . The map from to sending to is an injective homomorphism of rings, by which is viewed as a subring of . If is commutative, then is an algebra over .
One can think of the ring as arising from by adding one new element "X" to "R", and extending in a minimal way to a ring in which satisfies no other relations than the obligatory ones, plus commutation with all elements of (that is ). To do this, one must add all powers of and their linear combinations as well.
Formation of the polynomial ring, together with forming factor rings by factoring out ideals, are important tools for constructing new rings out of known ones. For instance, the ring (in fact field) of complex numbers, which can be constructed from the polynomial ring over the real numbers by factoring out the ideal of multiples of the polynomial . Another example is the construction of finite fields, which proceeds similarly, starting out with the field of integers modulo some prime number as the coefficient ring (see modular arithmetic).
If is commutative, then one can associate to every polynomial in , a "polynomial function" with domain and range equal to (more generally one can take domain and range to be the same unital associative algebra over ). One obtains the value by substitution of the value for the symbol in . One reason to distinguish between polynomials and polynomial functions is that over some rings different polynomials may give rise to the same polynomial function (see Fermat's little theorem for an example where is the integers modulo ). This is not the case when is the real or complex numbers, whence the two concepts are not always distinguished in analysis. An even more important reason to distinguish between polynomials and polynomial functions is that many operations on polynomials (like Euclidean division) require looking at what a polynomial is composed of as an expression rather than evaluating it at some constant value for .
Divisibility.
In commutative algebra, one major focus of study is "divisibility" among polynomials. If is an integral domain and and are polynomials in , it is said that "divides" or is a divisor of if there exists a polynomial in such that . One can show that every zero gives rise to a linear divisor, or more formally, if is a polynomial in and is an element of such that , then the polynomial () divides . The converse is also true. The quotient can be computed using the polynomial long division.
If is a field and and are polynomials in with , then there exist unique polynomials and in with
and such that the degree of is smaller than the degree of (using the convention that the polynomial 0 has a negative degree). The polynomials and are uniquely determined by and . This is called "Euclidean division, division with remainder" or "polynomial long division" and shows that the ring is a Euclidean domain.
Analogously, "prime polynomials" (more correctly, "irreducible polynomials") can be defined as "non-zero polynomials which cannot be factorized into the product of two non constant polynomials". In the case of coefficients in a ring, "non constant" must be replaced by "non constant or non unit" (both definitions agree in the case of coefficients in a field). Any polynomial may be decomposed into the product of an invertible constant by a product of irreducible polynomials. If the coefficients belong to a field or a unique factorization domain this decomposition is unique up to the order of the factors and the multiplication of any non unit factor by a unit (and division of the unit factor by the same unit). When the coefficients belong to integers, rational numbers or a finite field, there are algorithms to test irreducibility and to compute the factorization into irreducible polynomials (see Factorization of polynomials). These algorithms are not practicable for hand written computation, but are available in any computer algebra system. Eisenstein's criterion can also be used in some cases to determine irreducibility.
Other applications.
Polynomials serve to approximate other functions, such as the use of splines.
Polynomials are frequently used to encode information about some other object. The characteristic polynomial of a matrix or linear operator contains information about the operator's eigenvalues. The minimal polynomial of an algebraic element records the simplest algebraic relation satisfied by that element. The chromatic polynomial of a graph counts the number of proper colourings of that graph.
The term "polynomial", as an adjective, can also be used for quantities or functions that can be written in polynomial form. For example, in computational complexity theory the phrase "polynomial time" means that the time it takes to complete an algorithm is bounded by a polynomial function of some variable, such as the size of the input.
History.
Determining the roots of polynomials, or "solving algebraic equations", is among the oldest problems in mathematics. However, the elegant and practical notation we use today only developed beginning in the 15th century. Before that, equations were written out in words. For example, an algebra problem from the Chinese Arithmetic in Nine Sections, circa 200 BCE, begins "Three sheafs of good crop, two sheafs of mediocre crop, and one sheaf of bad crop are sold for 29 dou." We would write .
History of the notation.
The earliest known use of the equal sign is in Robert Recorde's "The Whetstone of Witte", 1557. The signs + for addition, − for subtraction, and the use of a letter for an unknown appear in Michael Stifel's "Arithemetica integra", 1544. René Descartes, in "La géometrie", 1637, introduced the concept of the graph of a polynomial equation. He popularized the use of letters from the beginning of the alphabet to denote constants and letters from the end of the alphabet to denote variables, as can be seen above, in the general formula for a polynomial in one variable, where the 's denote constants and denotes a variable. Descartes introduced the use of superscripts to denote exponents as well.

</doc>
<doc id="1345771" url="https://en.wikipedia.org/wiki?curid=1345771" title="Position (vector)">
Position (vector)

In geometry, a position or position vector, also known as location vector or radius vector, is a Euclidean vector that represents the position of a point "P" in space in relation to an arbitrary reference origin "O". Usually denoted x, r, or s, it corresponds to the straight-line distance from "O" to "P":
The term "position vector" is used mostly in the fields of differential geometry, mechanics and occasionally in vector calculus.
Frequently this is used in two-dimensional or three-dimensional space, but can be easily generalized to Euclidean spaces in any number of dimensions.
Definition.
Three dimensions.
In three dimensions, any set of three dimensional coordinates and their corresponding basis vectors can be used to define the location of a point in space—whichever is the simplest for the task at hand may be used.
Commonly, one uses the familiar Cartesian coordinate system, or sometimes spherical polar coordinates, or cylindrical coordinates;
where "t" is a parameter, owing to their rectangular or circular symmetry. These different coordinates and corresponding basis vectors represent the same position vector. More general curvilinear coordinates could be used instead, and are in contexts like continuum mechanics and general relativity (in the latter case one needs an additional time coordinate).
"n" dimensions.
Linear algebra allows for the abstraction of an "n"-dimensional position vector. A position vector can be expressed as a linear combination of basis vectors:
The set of all position vectors form position space (a vector space whose elements are the position vectors), since positions can be added (vector addition) and scaled in length (scalar multiplication) to obtain another position vector in the space. The notion of "space" is intuitive since each "xi" ("i" = 1, 2, …, "n") can be any value, the collection of values defines a point in space.
The "dimension" of the position space is "n" (also denoted dim("R") = "n"). The "coordinates" of the vector r with respect to the basis vectors e"i" are "x""i". The vector of coordinates forms the coordinate vector or "n"-tuple ("x"1, "x"2, …, "xn").
Each coordinate "xi" may be parameterized a number of parameters "t". One parameter "xi"("t") would describe a curved 1D path, two parameters "xi"("t"1, "t"2) describes a curved 2D surface, three "xi"("t"1, "t"2, "t"3) describes a curved 3D volume of space, and so on.
The linear span of a basis set "B" = {e1, e2, …, e"n"} equals the position space "R", denoted span("B") = "R".
Applications.
Differential geometry.
Position vector fields are used to describe continuous and differentiable space curves, in which case the independent parameter needs not be time, but can be (e.g.) arc length of the curve.
Mechanics.
In any equation of motion, the position vector r("t") is usually the most sought-after quantity because this function defines the motion of a particle (i.e. a point mass) - its location relative to a given coordinate system at some time "t".
To define motion in terms of position, each coordinate may be parametrized by time; since each successive value of time corresponds to a sequence of successive spatial locations given by the coordinates, so the continuum limit of many successive locations is a path the particle traces.
In the case of one dimension, the position has only one component, so it effectively degenerates to a scalar coordinate. It could be, say, a vector in the "x"-direction, or the radial "r"-direction. Equivalent notations include:
Derivatives of position.
For a position vector r that is a function of time "t", the time derivatives can be computed with respect to "t". These derivatives have common utility in the study of kinematics, control theory, engineering and other sciences.
where dr is an infinitesimally small displacement (vector).
These names for the first, second and third derivative of position are commonly used in basic kinematics. By extension, the higher order derivatives can be computed in a similar fashion. Study of these higher order derivatives can improve approximations of the original displacement function. Such higher-order terms are required in order to accurately represent the displacement function as a sum of an infinite sequence, enabling several analytical techniques in engineering and physics.
Relationship to displacement vectors.
A displacement vector can be defined as the "action" of uniformly translating spatial points in a given direction over a given distance. Thus the addition of displacement vectors expresses the composition of these displacement actions and scalar multiplication as scaling of the distance. With this in mind we may then define a "position vector" of a point in space as the displacement vector mapping a given origin to that point. Note thus position vectors depend on a choice of origin for the space, as well as displacement vectors depend on the choice of an initial point.

</doc>
<doc id="4838571" url="https://en.wikipedia.org/wiki?curid=4838571" title="Position operator">
Position operator

In quantum mechanics, the position operator is the operator that corresponds to the position observable of a particle. The eigenvalue of the operator is the position vector of the particle.
Introduction.
In one dimension, the square modulus of the wave function, formula_1, represents the probability density of finding the particle at position formula_2. Hence the expected value of a measurement of the position of the particle is
Accordingly, the quantum mechanical operator corresponding to position is formula_4, where
The circumflex over the x on the left side indicates an operator, so that this equation may be read "The result of the operator x acting on any function ψ(x) equals x multiplied by ψ(x)." Or more simply, "the operator x multiplies any function ψ(x) by x."
Eigenstates.
The eigenfunctions of the position operator, represented in position space, are Dirac delta functions.
To show this, suppose that formula_6 is an eigenstate of the position operator with eigenvalue formula_7. We write the eigenvalue equation in position coordinates,
recalling that formula_4 simply multiplies the function by formula_2 in the position representation. Since formula_2 is a variable while formula_7 is a constant, formula_6 must be zero everywhere except at formula_14. The normalized solution to this is
Although such a state is physically unrealizable and, strictly speaking, not a function, it can be thought of as an "ideal state" whose position is known exactly (any measurement of the position always returns the eigenvalue formula_7). Hence, by the uncertainty principle, nothing is known about the momentum of such a state.
Three dimensions.
The generalisation to three dimensions is straightforward. The wavefunction is now formula_17 and the expectation value of the position is
where the integral is taken over all space. The position operator is
Momentum space.
In momentum space, the position operator in one dimension is
Formalism.
Consider, for example, the case of a spinless particle moving in one spatial dimension (i.e. in a line). The state space for such a particle is "L"2(R), the Hilbert space of complex-valued and square-integrable (with respect to the Lebesgue measure) functions on the real line. The position operator, "Q", is then defined by:
with domain 
Since all continuous functions with compact support lie in "D(Q)", "Q" is densely defined. "Q", being simply multiplication by "x", is a self adjoint operator, thus satisfying the requirement of a quantum mechanical observable. Immediately from the definition we can deduce that the spectrum consists of the entire real line and that "Q" has purely continuous spectrum, therefore no discrete eigenvalues. The three-dimensional case is defined analogously. We shall keep the one-dimensional assumption in the following discussion.
Measurement.
As with any quantum mechanical observable, in order to discuss measurement, we need to calculate the spectral resolution of "Q":
Since "Q" is just multiplication by "x", its spectral resolution is simple. For a Borel subset "B" of the real line, let formula_24 denote the indicator function of "B". We see that the projection-valued measure Ω"Q" is given by
i.e. Ω"Q" is multiplication by the indicator function of "B". Therefore, if the system is prepared in state "ψ", then the probability of the measured position of the particle being in a Borel set "B" is 
where "μ" is the Lebesgue measure. After the measurement, the wave function collapses to either 
formula_27 
or 
formula_28, where formula_29 is the Hilbert space norm on "L"2(R).

</doc>
<doc id="40326" url="https://en.wikipedia.org/wiki?curid=40326" title="Positive-definite matrix">
Positive-definite matrix

In linear algebra, a symmetric real matrix formula_1 is said to be positive definite if the scalar formula_2 is positive for every non-zero column vector formula_3 of formula_4 real numbers. Here formula_5 denotes the transpose of formula_3.
More generally, an Hermitian matrix formula_1 is said to be positive definite if the scalar formula_8 is real and positive for all non-zero column vectors formula_3 of formula_4 complex numbers. Here formula_11 denotes the conjugate transpose of formula_3.
The negative definite, positive semi-definite, and negative semi-definite matrices are defined in the same way, except that the expression formula_2 or formula_8 is required to be always negative, non-negative, and non-positive, respectively.
Positive definite matrices are closely related to positive-definite symmetric bilinear forms (or sesquilinear forms in the complex case), and to inner products of vector spaces.
Some authors use more general definitions of "positive definite" that include some non-symmetric real matrices, or non-Hermitian complex ones.
Examples.
The examples "M" and "N" above show that a matrix in which some elements are negative may still be positive-definite, and conversely a matrix whose entries are all positive may not be positive definite.
Connections.
The general purely quadratic real function "f"("z") on "n" real variables "z"1, ..., "zn" can always be written as "z"T"Mz" where "z" is the column vector with those variables, and "M" is a symmetric real matrix. Therefore, the matrix being positive definite means that "f" has a unique minimum (zero) when "z" is zero, and is strictly positive for any other "z".
More generally, a twice-differentiable real function "f" on "n" real variables has an isolated local minimum at arguments "z"1, ..., "zn" if its gradient is zero and its Hessian (the matrix of all second derivatives) is positive definite at that point. Similar statements can be made for negative definite and semi-definite matrices.
In statistics, the covariance matrix of a multivariate probability distribution is always positive semi-definite; and it is positive definite unless one variable is an exact linear combination of the others. Conversely, every positive semi-definite matrix is the covariance matrix of some multivariate distribution.
Characterizations.
Let "M" be an "n" × "n" Hermitian matrix. The following properties are equivalent to "M" being positive definite:
Quadratic forms.
The (purely) quadratic form associated with a real matrix "M" is the function "Q" : R"n" → R such that "Q"("x") = "x"T"Mx" for all "x". It turns out that the matrix "M" is positive definite if and only if it is symmetric and its quadratic form is a strictly convex function.
More generally, any quadratic function from R"n" to R can be written as "x"T"Mx" + "x"T"b" + "c" where "M" is a symmetric "n" × "n" matrix, "b" is a real "n"-vector, and "c" a real constant. This quadratic function is strictly convex when "M" is positive definite, and hence has a unique finite global minimum, if and only if "M" is positive definite. For this reason, positive definite matrices play an important role in optimization problems.
Simultaneous diagonalization.
A symmetric matrix and another symmetric and positive-definite matrix can be simultaneously diagonalized, although not necessarily via a similarity transformation. This result does not extend to the case of three or more matrices. In this section we write for the real case. Extension to the complex case is immediate.
Let "M" be a symmetric and "N" a symmetric and positive-definite matrix. Write the generalized eigenvalue equation as ("M"−λ"N")"x" = 0 where we impose that "x" be normalized, i.e. "x"T"Nx" = 1. Now we use Cholesky decomposition to write the inverse of "N" as "Q"T"Q". Multiplying by "Q" and "Q"T, we get "Q"("M"−λ"N")"Q"T"x" = 0, which can be rewritten as ("QMQ"T)"y" = λ"y" where "y"T"y" = 1. Manipulation now yields "MX" = "NX"Λ where "X" is a matrix having as columns the generalized eigenvectors and Λ is a diagonal matrix with the generalized eigenvalues. Now premultiplication with "X"T gives the final result: "X"T"MX" = Λ and "X"T"NX" = "I", but note that this is no longer an orthogonal diagonalization.
Note that this result does not contradict what is said on simultaneous diagonalization in the article Diagonalizable matrix, which refers to simultaneous diagonalization by a similarity transformation. Our result here is more akin to a simultaneous diagonalization of two quadratic forms, and is useful for optimization of one form under conditions on the other. For this result see Horn&Johnson, 1985, page 218 and following.
Negative-definite, semidefinite and indefinite matrices.
A Hermitian matrix is negative-definite, negative-semidefinite, or positive-semidefinite if and only if all of its eigenvalues are negative, non-positive, or non-negative, respectively.
Negative-definite.
The Hermitian matrix "M" is said to be "negative-definite" if
for all non-zero "x" in C"n" (or, all non-zero "x" in R"n" for the real matrix), where "x*" is the conjugate transpose of "x".
A matrix is negative definite if its "k-"th order leading principal minor is negative when "k" is odd, and positive when "k" is even.
Positive-semidefinite.
M is called "positive-semidefinite" (or sometimes "nonnegative-definite") if
for all "x" in C"n" (or, all "x" in R"n" for the real matrix).
A matrix "M" is positive-semidefinite if and only if it arises as the Gram matrix of some set of vectors. In contrast to the positive-definite case, these vectors need not be linearly independent.
For any matrix "A", the matrix "A*A" is positive semidefinite, and rank("A") = rank("A*A"). 
Conversely, any Hermitian positive semi-definite matrix "M" can be written as "M" = "LL*", where "L" is lower triangular; this is the Cholesky decomposition. If "M" is not positive definite, then some of the diagonal elements of "L" may be zero.
A Hermitian matrix is positive semidefinite if and only if all of its principal minors are nonnegative. It is however not enough to consider the leading principal minors only, as is checked on the diagonal matrix with entries 0 and -1.
Negative-semidefinite.
It is called "negative-semidefinite" if
for all "x" in C"n" (or, all "x" in R"n" for the real matrix).
Indefinite.
A Hermitian matrix which is neither positive definite, negative definite, positive-semidefinite, nor negative-semidefinite is called "indefinite". Indefinite matrices are also characterized by having both positive and negative eigenvalues.
Further properties.
If "M" is a Hermitian positive-semidefinite matrix, one sometimes writes "M" ≥ 0 and if "M" is positive-definite one writes "M" > 0. The notion comes from functional analysis where positive-semidefinite matrices define positive operators.
For arbitrary square matrices "M", "N" we write "M" ≥ "N" if "M" − "N" ≥ 0; i.e., "M" − "N" is positive semi-definite. This defines a partial ordering on the set of all square matrices. One can similarly define a strict partial ordering "M" > "N".
Block matrices.
A positive 2"n" × 2"n" matrix may also be defined by blocks:
where each block is "n" × "n". By applying the positivity condition, it immediately follows that "A" and "D" are hermitian, and "C" = "B*".
We have that "z*Mz" ≥ 0 for all complex "z", and in particular for "z" = ( "v", 0)T. Then
A similar argument can be applied to "D", and thus we conclude that both "A" and "D" must be positive definite matrices, as well.
Converse results can be proved with stronger conditions on the blocks, for instance using the Schur complement.
On the definition.
Consistency between real and complex definitions.
Since every real matrix is also a complex matrix, the definitions of "positive definite" for the two classes must agree.
For complex matrices, the most common definition says that ""M" is positive definite if and only if "z*Mz" is real and positive for all non-zero "complex" column vectors "z". This condition implies that "M" is Hermitian, that is, its transpose is equal to its conjugate. To see this, consider the matrices "A" = ("M"+"M*")/2 and "B" = ("M"−"M*")/(2"i"), so that "M" = "A"+"iB" and "z*Mz" = "z*Az" + "iz*Bz". The matrices "A" and "B" are Hermitian, therefore "z*Az" and "z*Bz" are individually real. If "z*Mz" is real, then "z*Bz" must be zero for all "z". Then "B" is the zero matrix and "M" = "A", proving that "M" is Hermitian.
By this definition, a positive definite "real" matrix "M" is Hermitian, hence symmetric; and "z"T"Mz" is positive for all non-zero "real" column vectors "z". However the last condition alone is not sufficient for "M" to be positive definite. For example, if
then for any real vector "z" with entries "a" and "b" we have "z"T"Mz" = ("a"−"b")"a" + ("a"+"b")"b" = "a"2 + "b"2, which is always positive if "z" is not zero. However, if "z" is the complex vector with entries 1 and "i", one gets
which is not real. Therefore, "M" is not positive definite.
On the other hand, for a "symmetric" real matrix "M", the condition "z"T"Mz" > 0 for all nonzero real vectors "z"" "does" imply that "M" is positive definite in the complex sense.
Extension for non symmetric matrices.
Some authors choose to say that a complex matrix "M" is positive definite if Re("z*Mz") > 0 for all non-zero complex vectors "z", where Re("c") denotes the real part of a complex number "c". This weaker definition encompasses some non-Hermitian complex matrices, including some non-symmetric real ones, such as formula_52.
Indeed, with this definition, a real matrix is positive definite if and only if "z"T"Mz" > 0 for all nonzero real vectors "z", even if "M" is not symmetric.
In general, we have Re("z*Mz") > 0 for all complex nonzero vectors "z" if and only if the Hermitian part ("M" + "M*")/2 of "M" is positive definite in the narrower sense. Similarly, we have "x"T"Mx" > 0 for all real nonzero vectors "x" if and only if the symmetric part ("M" + "M"T)/2 of "M" is positive definite in the narrower sense.
In summary, the distinguishing feature between the real and complex case is that, a bounded positive operator on a complex Hilbert space is necessarily Hermitian, or self adjoint. The general claim can be argued using the polarization identity. That is no longer true in the real case.

</doc>
<doc id="24731" url="https://en.wikipedia.org/wiki?curid=24731" title="Positron">
Positron

The positron or antielectron is the antiparticle or the antimatter counterpart of the electron. The positron has an electric charge of +1 "e", a spin of ½, and has the same mass as an electron. When a low-energy positron collides with a low-energy electron, annihilation occurs, resulting in the production of two or more gamma ray photons (see electron–positron annihilation).
Positrons may be generated by positron emission radioactive decay (through weak interactions), or by pair production from a sufficiently energetic photon which is interacting with an atom in a material.
History.
Theory.
In 1928, Paul Dirac published a paper proposing that electrons can have both a positive charge and negative energy. This paper introduced the Dirac equation, a unification of quantum mechanics, special relativity, and the then-new concept of electron spin to explain the Zeeman effect. The paper did not explicitly predict a new particle, but did allow for electrons having either positive or negative energy as solutions. Hermann Weyl then published "Gravitation and the Electron" (Proceedings of the National Academy of Sciences of the United States of America, Vol. 15, No. 4-Apr. 15, 1929, pp. 323–334) discussing the mathematical implications of the negative energy solution. The positive-energy solution explained experimental results, but Dirac was puzzled by the equally valid negative-energy solution that the mathematical model allowed. Quantum mechanics did not allow the negative energy solution to simply be ignored, as classical mechanics often did in such equations; the dual solution implied the possibility of an electron spontaneously jumping between positive and negative energy states. However, no such transition had yet been observed experimentally. He referred to the issues raised by this conflict between theory and observation as "difficulties" that were "unresolved".
Dirac wrote a follow-up paper in December 1929 that attempted to explain the unavoidable negative-energy solution for the relativistic electron. He argued that "... an electron with negative energy moves in an external [electromagnetic] field as though it carries a positive charge." He further asserted that all of space could be regarded as a "sea" of negative energy states that were filled, so as to prevent electrons jumping between positive energy states (negative electric charge) and negative energy states (positive charge). The paper also explored the possibility of the proton being an island in this sea, and that it might actually be a negative-energy electron. Dirac acknowledged that the proton having a much greater mass than the electron was a problem, but expressed "hope" that a future theory would resolve the issue.
Robert Oppenheimer argued strongly against the proton being the negative-energy electron solution to Dirac's equation. He asserted that if it were, the hydrogen atom would rapidly self-destruct. Persuaded by Oppenheimer's argument, Dirac published a paper in 1931 that predicted the existence of an as-yet unobserved particle that he called an "anti-electron" that would have the same mass as an electron and that would mutually annihilate upon contact with an electron.
Feynman, and earlier Stueckelberg, proposed an interpretation of the positron as an electron moving backward in time, reinterpreting the negative-energy solutions of the Dirac equation. Electrons moving backward in time would have a positive electric charge. Wheeler invoked this concept to explain the identical properties shared by all electrons, suggesting that "they are all the same electron" with a complex, self-intersecting worldline. Yoichiro Nambu later applied it to all production and annihilation of particle-antiparticle pairs, stating that "the eventual creation and annihilation of pairs that may occur now and then is no creation or annihilation, but only a change of direction of moving particles, from past to future, or from future to past." The backwards in time point of view is nowadays accepted as completely equivalent to other pictures, but it does not have anything to do with the macroscopic terms "cause" and "effect", which do not appear in a microscopic physical description.
Experimental clues and discovery.
Dmitri Skobeltsyn first observed the positron in 1929. While using a Wilson cloud chamber to try to detect gamma radiation in cosmic rays, Skobeltsyn detected particles that acted like electrons but curved in the opposite direction in an applied magnetic field.
Likewise, in 1929 Chung-Yao Chao, a graduate student at Caltech, noticed some anomalous results that indicated particles behaving like electrons, but with a positive charge, though the results were inconclusive and the phenomenon was not pursued.
Carl David Anderson discovered the positron on August 2, 1932, for which he won the Nobel Prize for Physics in 1936. Anderson did not coin the term "positron", but allowed it at the suggestion of the Physical Review journal editor to which he submitted his discovery paper in late 1932. The positron was the first evidence of antimatter and was discovered when Anderson allowed cosmic rays to pass through a cloud chamber and a lead plate. A magnet surrounded this apparatus, causing particles to bend in different directions based on their electric charge. The ion trail left by each positron appeared on the photographic plate with a curvature matching the mass-to-charge ratio of an electron, but in a direction that showed its charge was positive.
Anderson wrote in retrospect that the positron could have been discovered earlier based on Chung-Yao Chao's work, if only it had been followed up. Frédéric and Irène Joliot-Curie in Paris had evidence of positrons in old photographs when Anderson's results came out, but they had dismissed them as protons.
Natural production.
Positrons are produced naturally in β+ decays of naturally occurring radioactive isotopes (for example, potassium-40) and in interactions of gamma quanta (emitted by radioactive nuclei) with matter. Antineutrinos are another kind of antiparticle created by natural radioactivity (β− decay). Many different kinds of antiparticles are also produced by (and contained in) cosmic rays. Recent (as of January 2011) research by the American Astronomical Society has discovered antimatter (positrons) originating above thunderstorm clouds; positrons are produced in gamma-ray flashes created by electrons accelerated by strong electric fields in the clouds. Antiprotons have also been found to exist in the Van Allen Belts around the Earth by the PAMELA module.
Antiparticles, of which the most common are positrons due to their low mass, are also produced in any environment with a sufficiently high temperature (mean particle energy greater than the pair production threshold). During the period of baryogenesis, when the universe was extremely hot and dense, matter and antimatter were continually produced and annihilated. The presence of remaining matter, and absence of detectable remaining antimatter, also called baryon asymmetry, is attributed to CP-violation: a violation of the CP-symmetry relating matter to antimatter. The exact mechanism of this violation during baryogenesis remains a mystery.
Positrons production from radioactive decay, can be considered both artificial and natural production, as the generation of the radioisotope can be natural or artificial. Perhaps the best known naturally-occurring radioisotope which produces positrons is potassium-40, a long-lived isotope of potassium which occurs as a primordial isotope of potassium, and even though a small percent of potassium, (0.0117%) is the single most abundant radioisotope in the human body. In a human body of 70 kg mass, about 4,400 nuclei of 40K decay per second. The activity of natural potassium is 31 Bq/g. About 0.001% of these 40K decays produce about 4000 natural positrons per day in the human body. These positrons soon find an electron, undergo annihilation, and produce pairs of 511 keV gamma rays, in a process similar (but much lower intensity) to that which happens during a PET scan nuclear medicine procedure.
Observation in cosmic rays.
Satellite experiments have found evidence of positrons (as well as a few antiprotons) in primary cosmic rays, amounting to less than 1% of the particles in primary cosmic rays. These do not appear to be the products of large amounts of antimatter from the Big Bang, or indeed complex antimatter in the universe (evidence for which is lacking, see below). Rather, the antimatter in cosmic rays appear to consist of only these two elementary particles, probably made in energetic processes long after the Big Bang.
Preliminary results from the presently operating Alpha Magnetic Spectrometer ("AMS-02") on board the International Space Station show that positrons in the cosmic rays arrive with no directionality, and with energies that range from 10 to 250 GeV. In September, 2014, new results with almost twice as much data were presented in a talk at CERN and published in Physical Review Letters. A new measurement of positron fraction up to 500 GeV was reported, showing that positron fraction peaks at a maximum of about 16% of total electron+positron events, around an energy of 275 ± 32 GeV. At higher energies, up to 500 GeV, the ratio of positrons to electrons begins to fall again. The absolute flux of positrons also begins to fall before 500 GeV, but peaks at energies far higher than electron energies, which peak about 10 GeV. These results on interpretation have been suggested to be due to positron production in annihilation events of massive dark matter particles.
Positrons, like anti-protons, do not appear to originate from any hypothetical "antimatter" regions of the universe. On the contrary, there is no evidence of complex antimatter atomic nuclei, such as antihelium nuclei (i.e., anti-alpha particles), in cosmic rays. These are actively being searched for. A prototype of the "AMS-02" designated "AMS-01", was flown into space aboard the on STS-91 in June 1998. By not detecting any antihelium at all, the "AMS-01" established an upper limit of 1.1×10−6 for the antihelium to helium flux ratio.
Artificial production.
New research has dramatically increased the quantity of positrons that experimentalists can produce. Physicists at the Lawrence Livermore National Laboratory in California have used a short, ultra-intense laser to irradiate a millimetre-thick gold target and produce more than 100 billion positrons.
Applications.
Certain kinds of particle accelerator experiments involve colliding positrons and electrons at relativistic speeds. The high impact energy and the mutual annihilation of these matter/antimatter opposites create a fountain of diverse subatomic particles. Physicists study the results of these collisions to test theoretical predictions and to search for new kinds of particles.
Gamma rays, emitted indirectly by a positron-emitting radionuclide (tracer), are detected in positron emission tomography (PET) scanners used in hospitals. PET scanners create detailed three-dimensional images of metabolic activity within the human body.
An experimental tool called positron annihilation spectroscopy (PAS) is used in materials research to detect variations in density, defects, displacements, or even voids, within a solid material.

</doc>
<doc id="391267" url="https://en.wikipedia.org/wiki?curid=391267" title="Positron emission">
Positron emission

Positron emission or beta plus decay (β+ decay) is a particular type of radioactive decay and a subtype of beta decay, in which a proton inside a radionuclide nucleus is converted into a neutron while releasing a positron and an electron neutrino ("ν"e). Positron emission is mediated by the weak force. The positron is a type of beta particle (β+), the other beta particle being the electron (β−) emitted from the β− decay of a nucleus.
An example of positron emission (β+ decay) is shown with magnesium-23 decaying into sodium-23:
Because positron emission decreases proton number relative to neutron number, positron decay happens typically in large "proton-rich" radionuclides. Positron decay results in nuclear transmutation, changing an atom of a chemical element into an atom of an element with an atomic number that is less by one unit.
Positron emission should not be confused with electron emission or beta minus decay (β− decay), which occurs when a neutron turns into a proton and the nucleus emits an electron and an antineutrino.
Electron capture (sometimes called inverse beta decay) is also occasionally classified as a type of beta decay. In some ways, electron capture can be regarded as an equivalent to positron emission, since capture of an electron results in the same transmutation as emission of a positron. Electron capture occurs when electrons are available and requires less energy difference between parent and daughter, so occurs much more often in smaller atoms than positron emission does. Electron capture always competes with positron emission where the latter is seen, and in addition, occurs as the only type of beta decay in proton-rich nuclei when there is not enough decay energy to support positron emission.
Discovery of positron emission.
In 1934 Frédéric and Irène Joliot-Curie bombarded aluminium with alpha particles to effect the nuclear reaction  +  →  + , and observed that the product isotope emits a positron identical to those found in cosmic rays by Carl David Anderson in 1932. This was the first example of  decay (positron emission). The Curies termed the phenomenon "artificial radioactivity," since is a short-lived nuclide which does not exist in nature. The discovery of artificial radioactivity would be cited when the husband and wife team won the Nobel Prize.
Positron-emitting isotopes.
Isotopes which undergo this decay and thereby emit positrons include carbon-11, potassium-40, nitrogen-13, oxygen-15, aluminium-26, sodium-22, fluorine-18, and iodine-121. As an example, the following equation describes the beta plus decay of carbon-11 to boron-11, emitting a positron and a neutrino:
Emission mechanism.
Inside protons and neutrons, there are fundamental particles called quarks. The two most common types of quarks are up quarks, which have a charge of +2/3, and down quarks, with a −1/3 charge. Quarks arrange themselves in sets of three such that they make protons and neutrons. In a proton, whose charge is +1, there are two up quarks and one down quark. Neutrons, with no charge, have one up quark and two down quarks. Via the weak interaction, quarks can change flavor from down to up, resulting in electron emission. Positron emission happens when an up quark changes into a down quark.
Nuclei which decay by positron emission may also decay by electron capture. For low-energy decays, electron capture is energetically favored by 2"m"e"c"2 = 1.022 MeV, since the final state has an electron removed rather than a positron added. As the energy of the decay goes up, so does the branching ratio towards positron emission. However, if the energy difference is less than 2"m"e"c"2, then positron emission cannot occur and electron capture is the sole decay mode. Certain isotopes (for instance, ) are stable in galactic cosmic rays, because the electrons are stripped away and the decay energy is too small for positron emission.
Application.
These isotopes are used in positron emission tomography, a technique used for medical imaging. Note that the energy emitted depends on the isotope that is decaying; the figure of 0.96 MeV applies only to the decay of carbon-11. Isotopes which increase in mass under the conversion of a proton to a neutron, or which decrease in mass by less than 2"m"e, cannot spontaneously decay by positron emission.
The short-lived positron emitting isotopes 11C, 13N, 15O and 18F used for positron emission tomography are typically produced by proton irradiation of natural or enriched targets.

</doc>
<doc id="44596" url="https://en.wikipedia.org/wiki?curid=44596" title="Positronium">
Positronium

"This article is about the exotic atom. For the hydrogen isotope, see Protium."
Positronium (Ps) is a system consisting of an electron and its anti-particle, a positron, bound together into an "exotic atom", specifically an "onium". The system is unstable: the two particles annihilate each other to predominantly produce two or three gamma-rays, depending on the relative spin states. The orbit and energy levels of the two particles are similar to that of the hydrogen atom (which is a bound state of a proton and an electron). However, because of the reduced mass, the frequencies of the spectral lines are less than half of the corresponding hydrogen lines.
States.
The ground state of positronium, like that of hydrogen, has two possible configurations depending on the relative orientations of the spins of the electron and the positron.
The "singlet" state, , with antiparallel spins ("S" = 0, "Ms" = 0) is known as "para-positronium" ("p"-Ps). It has a mean lifetime of 125 picoseconds and decays preferentially into two gamma rays with energy of each (in the center-of-mass frame). Detection of these photons allows to reconstruct the vertex of the decay and is used in the positron-emission tomography. Para-positronium can decay into any even number of photons (2, 4, 6, ...), but the probability quickly decreases with the number: the branching ratio for decay into 4 photons is .
Para-positronium lifetime in vacuum is approximately
The "triplet" state, 3S1, with parallel spins ("S" = 1, "Ms" = −1, 0, 1) is known as "ortho-positronium" ("o"-Ps). It has a mean lifetime of , and the leading decay is three gammas. Other modes of decay are negligible; for instance, the five-photons mode has branching ratio of ~.
Ortho-positronium lifetime in vacuum can be calculated approximately as:
However more accurate calculations with corrections to order O(α²) yield a value of 7.04 μs⁻¹ for the decay rate, corresponding to a lifetime of s.
Positronium in the 2S state is metastable having a lifetime of against annihilation. The positronium created in such an excited state will quickly cascade down to the ground state, where annihilation will occur more quickly.
Measurements.
Measurements of these lifetimes and energy levels have been used in precision tests of quantum electrodynamics, confirming QED predictions to high precision. Annihilation can proceed via a number of channels, each producing gamma rays with total energy of (sum of the electron and positron mass-energy), usually 2 or 3, with up to 5 recorded.
The annihilation into a neutrino–antineutrino pair is also possible, but the probability is predicted to be negligible. The branching ratio for "o"-Ps decay for this channel is (electron neutrino–antineutrino pair) and (for other flavour) in predictions based on the Standard Model, but it can be increased by non-standard neutrino properties, like relatively high magnetic moment. The experimental upper limits on branching ratio for this decay (as well as for a decay into any "invisible" particles) are < for "p"-Ps and < for "o"-Ps.
Energy levels.
While precise calculation of positronium energy levels uses the Bethe–Salpeter equation or the Breit equation, the similarity between positronium and hydrogen allows a rough estimate. In this approximation, the energy levels are different because of a different effective mass, "m"*, in the energy equation (see electron energy levels for a derivation):
where:
Thus, for positronium, its reduced mass only differs from the electron by a factor of 2. This causes the energy levels to also roughly be half of what they are for the hydrogen atom.
So finally, the energy levels of positronium are given by
The lowest energy level of positronium () is −6.8 electronvolts (eV). The next level is . The negative sign implies a bound state. Positronium can also be considered by a particular form of the two-body Dirac equation; Two point particles with a Coulomb interaction can be exactly separated in the (relativistic) center-of-momentum frame and the resulting ground-state energy has been obtained very accurately using finite element methods of J. Shertzer. The Dirac equation whose Hamiltonian comprises two Dirac particles and a static Coulomb potential is not relativistically invariant. But if one adds the (or , where is the fine-structure constant) terms, where , then the result is relativistically invariant. Only the leading term is included. The contribution is the Breit term; workers rarely go to because at one has the Lamb shift, which requires quantum electrodynamics.
History.
Croatian scientist Stjepan Mohorovičić predicted the existence of positronium in a 1934 article published in "Astronomische Nachrichten", in which he called it the "electrum". Other sources credit Carl Anderson as having predicted its existence in 1932 while at Caltech. It was experimentally discovered by Martin Deutsch at MIT in 1951 and became known as positronium. Many subsequent experiments have precisely measured its properties and verified predictions of quantum electrodynamics (QED). There was a discrepancy known as the ortho-positronium lifetime puzzle that persisted for some time, but was eventually resolved with further calculations and measurements. Measurements were in error because of the lifetime measurement of unthermalised positronium, which was only produced at a small rate. This had yielded lifetimes that were too long. Also calculations using relativistic QED are difficult to perform, so they had been done to only the first order. Corrections that involved higher orders were then calculated in a non-relativistic QED.
Exotic compounds.
Molecular bonding was predicted for positronium. Molecules of positronium hydride (PsH) can be made. Positronium can also form a cyanide and can form bonds with halogens or lithium.
The first observation of di-positronium molecules—molecules consisting of two positronium atoms—was reported on 12 September 2007 by David Cassidy and Allen Mills from University of California, Riverside.
Natural occurrence.
Positronium in high energy states has been predicted to be the dominant form of atomic matter in the universe in the far future, if proton decay is a reality.

</doc>
<doc id="23703" url="https://en.wikipedia.org/wiki?curid=23703" title="Potential energy">
Potential energy

In physics, potential energy is the energy that an object has due to its position in a force field or that a system has due to the configuration of its shield. 
Common types include the gravitational potential energy of an object that depends on its mass and its distance from the center of mass of another object, the elastic potential energy of an extended spring, and the electric potential energy of a electric charge in an electric field. The unit for energy in the International System of Units (SI) is the joule, which has the symbol "J".
The term "potential energy" was introduced by the 19th century Scottish engineer and physicist William Rankine, although it has links to Greek philosopher Aristotle's concept of potentiality.
Potential energy is associated with forces that act on a body in a way that depends only on the body's position in space. These forces can be represented by a vector at every point in space forming a vector field of forces, or a force field.
If the work of a force field acting on a body that moves from a start to an end position is determined only by these two positions, and does not depend on the trajectory of the body, then there is a function known as "potential energy" that can be evaluated at the two positions to determine this work. Furthermore, the force field is determined by this potential energy and is described as derivable from a potential.
Overview.
Potential energy is the stored energy of an object. It is the energy by virtue of an object's position relative to other objects. Potential energy is often associated with restoring forces such as a spring or the force of gravity. The action of stretching the spring or lifting the mass is performed by an external force that works against the force field of the potential. This work is stored in the force field, which is said to be stored as potential energy. If the external force is removed the force field acts on the body to perform the work as it moves the body back to the initial position, reducing the stretch of the spring or causing a body to fall.
The more formal definition is that potential energy is the energy difference between the energy of an object in a given position and its energy at a reference position.
There are various types of potential energy, each associated with a particular type of force. For example, the work of an elastic force is called elastic potential energy; work of the gravitational force is called gravitational potential energy; work of the Coulomb force is called electric potential energy; work of the strong nuclear force or weak nuclear force acting on the baryon charge is called nuclear potential energy; work of intermolecular forces is called intermolecular potential energy. Chemical potential energy, such as the energy stored in fossil fuels, is the work of the Coulomb force during rearrangement of mutual positions of electrons and nuclei in atoms and molecules. Thermal energy usually has two components: the kinetic energy of random motions of particles and the potential energy of their mutual positions.
Forces derivable from a potential are also called conservative forces. The work done by a conservative force is
where formula_2 is the change in the potential energy associated with the force. The negative sign provides the convention that work done against a force field increases potential energy, while work done by the force field decreases potential energy. Common notations for potential energy are "U", "V", and "Ep".
Work and potential energy.
Potential energy is closely linked with forces. If the work done by a force on a body that moves from "A" to "B" does not depend on the path between these points, then the work of this force measured from "A" assigns a scalar value to every other point in space and defines a scalar potential field. In this case, the force can be defined as the negative of the vector gradient of the potential field.
If the work for an applied force is independent of the path, then the work done by the force is evaluated at the start and end of the trajectory of the point of application. This means that there is a function "U" (x), called a "potential," that can be evaluated at the two points xA and xB to obtain the work over any trajectory between these two points. It is tradition to define this function with a negative sign so that positive work is a reduction in the potential, that is 
where "C" is the trajectory taken from A to B. Because the work done is independent of the path taken, then this expression is true for any trajectory, "C", from A to B.
The function "U"(x) is called the potential energy associated with the applied force. Examples of forces that have potential energies are gravity and spring forces.
Derivable from a potential.
In this section the relationship between work and potential energy is presented in more detail. The line integral that defines work along curve "C" takes a special form if the force F is related to a scalar field φ(x) so that
In this case, work along the curve is given by
which can be evaluated using the gradient theorem to obtain
This shows that when forces are derivable from a scalar field, the work of those forces along a curve "C" is computed by evaluating the scalar field at the start point "A" and the end point "B" of the curve. This means the work integral does not depend on the path between "A" and "B" and is said to be independent of the path.
Potential energy "U"=-φ(x) is traditionally defined as the negative of this scalar field so that work by the force field decreases potential energy, that is
In this case, the application of the del operator to the work function yields,
and the force F is said to be "derivable from a potential." This also necessarily implies that F must be a conservative vector field. The potential "U" defines a force F at every point x in space, so the set of forces is called a force field.
Computing potential energy.
Given a force field F(x), evaluation of the work integral using the gradient theorem can be used to find the scalar function associated with potential energy. This is done by introducing a parameterized curve γ(t)=r(t) from γ(a)=A to γ(b)=B, and computing,
For the force field F, let v= dr/dt, then the gradient theorem yields,
The power applied to a body by a force field is obtained from the gradient of the work, or potential, in the direction of the velocity v of the point of application, that is
Examples of work that can be computed from potential functions are gravity and spring forces.
Potential energy for near Earth gravity.
In classical physics, gravity exerts a constant downward force F=(0, 0, "Fz") on the center of mass of a body moving near the surface of the Earth. The work of gravity on a body moving along a trajectory r(t) = ("x"(t), "y"(t), "z"(t)), such as the track of a roller coaster is calculated using its velocity, v=("v"x, "v"y, "v"z), to obtain
where the integral of the vertical component of velocity is the vertical distance. Notice that the work of gravity depends only on the vertical movement of the curve r(t).
The function 
is called the potential energy of a near earth gravity field.
Potential energy for a linear spring.
A horizontal spring exerts a force F = (−"kx", 0, 0) that is proportional to its deflection in the "x" direction. The work of this spring on a body moving along the space curve s("t") = ("x"("t"), "y"("t"), "z"("t")), is calculated using its velocity, v = ("v"x, "v"y, "v"z), to obtain
For convenience, consider contact with the spring occurs at "t" = 0, then the integral of the product of the distance "x" and the "x"-velocity, "xvx", is "x"2/2.
The function 
is called the potential energy of a linear spring.
Elastic potential energy is the potential energy of an elastic object (for example a bow or a catapult) that is deformed under tension or compression (or stressed in formal terminology). It arises as a consequence of a force that tries to restore the object to its original shape, which is most often the electromagnetic force between the atoms and molecules that constitute the object. If the stretch is released, the energy is transformed into kinetic energy.
Potential energy for gravitational forces between two bodies.
Gravitational potential energy between two bodies in space is obtained from the force exerted by a mass "M" on another mass "m" is given by
where r is the position vector from "M" to "m".
This can also be expressed as
where formula_18 is a vector of length 1 pointing from "M" to "m".
Let the mass "m" move at the velocity v then the work of gravity on this mass as it moves from position r(t1) to r(t2) is given by
Notice that the position and velocity of the mass "m" are given by
where e"r" and e"t" are the radial and tangential unit vectors directed relative to the vector from "M" to "m". Use this to simplify the formula for work of gravity to,
This calculation uses the fact that
The function
is the gravitational potential function, also known as gravitational potential energy. The negative sign follows the convention that work is gained from a loss of potential energy.
Potential energy for electrostatic forces between two bodies.
The electrostatic force exerted by a charge "Q" on another charge "q" is given by
where r is the position vector from "Q" to "q" and "ε"0 is the vacuum permittivity. This may also be written using Coulomb's constant .
The work "W" required to move "q" from "A" to any point "B" in the electrostatic force field is given by the potential function
Reference level.
The potential energy is a function of the state a system is in, and is defined relative to that for a particular state. This reference state is not always a real state, it may also be a limit, such as with the distances between all bodies tending to infinity, provided that the energy involved in tending to that limit is finite, such as in the case of inverse-square law forces. Any arbitrary reference state could be used, therefore it can be chosen based on convenience.
Typically the potential energy of a system depends on the "relative" positions of its components only, so the reference state can also be expressed in terms of relative positions.
Gravitational potential energy.
Gravitational energy is the potential energy associated with gravitational force, as work is required to elevate objects against Earth's gravity. The potential energy due to elevated positions is called gravitational potential energy, and is evidenced by water in an elevated reservoir or kept behind a dam. If an object falls from one point to another point inside a gravitational field, the force of gravity will do positive work on the object, and the gravitational potential energy will decrease by the same amount.
Consider a book placed on top of a table. As the book is raised from the floor, to the table, some external force works against the gravitational force. If the book falls back to the floor, the "falling" energy the book receives is provided by the gravitational force. Thus, if the book falls off the table, this potential energy goes to accelerate the mass of the book and is converted into kinetic energy. When the book hits the floor this kinetic energy is converted into heat, deformation and sound by the impact.
The factors that affect an object's gravitational potential energy are its height relative to some reference point, its mass, and the strength of the gravitational field it is in. Thus, a book lying on a table has less gravitational potential energy than the same book on top of a taller cupboard, and less gravitational potential energy than a heavier book lying on the same table. An object at a certain height above the Moon's surface has less gravitational potential energy than at the same height above the Earth's surface because the Moon's gravity is weaker. Note that "height" in the common sense of the term cannot be used for gravitational potential energy calculations when gravity is not assumed to be a constant. The following sections provide more detail.
Local approximation.
The strength of a gravitational field varies with location. However, when the change of distance is small in relation to the distances from the center of the source of the gravitational field, this variation in field strength is negligible and we can assume that the force of gravity on a particular object is constant. Near the surface of the Earth, for example, we assume that the acceleration due to gravity is a constant ("standard gravity"). In this case, a simple expression for gravitational potential energy can be derived using the "W" = "Fd" equation for work, and the equation
The amount of gravitational potential energy possessed by an elevated object is equal to the work done against gravity in lifting it. The work done equals the force required to move it upward multiplied with the vertical distance it is moved (remember "W = Fd"). The upward force required while moving at a constant velocity is equal to the weight, "mg", of an object, so the work done in lifting it through a height "h" is the product "mgh". Thus, when accounting only for mass, gravity, and altitude, the equation is:
where "U" is the potential energy of the object relative to its being on the Earth's surface, "m" is the mass of the object, "g" is the acceleration due to gravity, and "h" is the altitude of the object. If "m" is expressed in kilograms, "g" in m/s2 and "h" in metres then "U" will be calculated in joules.
Hence, the potential difference is
General formula.
However, over large variations in distance, the approximation that "g" is constant is no longer valid, and we have to use calculus and the general mathematical definition of work to determine gravitational potential energy. For the computation of the potential energy we can integrate the gravitational force, whose magnitude is given by Newton's law of gravitation, with respect to the distance "r" between the two bodies. Using that definition, the gravitational potential energy of a system of masses "m"1 and "M"2 at a distance "r" using gravitational constant "G" is
where "K" is an arbitrary constant dependent on the choice of datum from which potential is measured. Choosing the convention that "K"=0 (i.e. in relation to a point at infinity) makes calculations simpler, albeit at the cost of making "U" negative; for why this is physically reasonable, see below.
Given this formula for "U", the total potential energy of a system of "n" bodies is found by summing, for all formula_30 pairs of two bodies, the potential energy of the system of those two bodies.
Considering the system of bodies as the combined set of small particles the bodies consist of, and applying the previous on the particle level we get the negative gravitational binding energy. This potential energy is more strongly negative than the total potential energy of the system of bodies as such since it also includes the negative gravitational binding energy of each body. The potential energy of the system of bodies as such is the negative of the energy needed to separate the bodies from each other to infinity, while the gravitational binding energy is the energy needed to separate all particles from each other to infinity.
therefore,
Why choose a convention where gravitational energy is negative?
As with all potential energies, only differences in gravitational potential energy matter for most physical purposes, and the choice of zero point is arbitrary. Given that there is no reasonable criterion for preferring one particular finite "r" over another, there seem to be only two reasonable choices for the distance at which "U" becomes zero: formula_33 and formula_34. The choice of formula_35 at infinity may seem peculiar, and the consequence that gravitational energy is always negative may seem counterintuitive, but this choice allows gravitational potential energy values to be finite, albeit negative.
The singularity at formula_33 in the formula for gravitational potential energy means that the only other apparently reasonable alternative choice of convention, with formula_35 for formula_33, would result in potential energy being positive, but infinitely large for all nonzero values of "r", and would make calculations involving sums or differences of potential energies beyond what is possible with the real number system. Since physicists abhor infinities in their calculations, and "r" is always non-zero in practice, the choice of formula_35 at infinity is by far the more preferable choice, even if the idea of negative energy in a gravity well appears to be peculiar at first.
The negative value for gravitational energy also has deeper implications that make it seem more reasonable in cosmological calculations where the total energy of the universe can meaningfully be considered; see inflation theory for more on this.
Uses.
Gravitational potential energy has a number of practical uses, notably the generation of pumped-storage hydroelectricity. For example, in Dinorwig, Wales, there are two lakes, one at a higher elevation than the other. At times when surplus electricity is not required (and so is comparatively cheap), water is pumped up to the higher lake, thus converting the electrical energy (running the pump) to gravitational potential energy. At times of peak demand for electricity, the water flows back down through electrical generator turbines, converting the potential energy into kinetic energy and then back into electricity. The process is not completely efficient and some of the original energy from the surplus electricity is in fact lost to friction.
Gravitational potential energy is also used to power clocks in which falling weights operate the mechanism. It's also used by counterweights for lifting up an elevator, crane, or sash window.
Roller coasters are an entertaining way to utilize potential energy - chains are used to move a car up an incline (building up gravitational potential energy), to then have that energy converted into kinetic energy as it falls.
Another practical use is utilizing gravitational potential energy to descend (perhaps coast) downhill in transportation such as the descent of an automobile, truck, railroad train, bicycle, airplane, or fluid in a pipeline. In some cases the kinetic energy obtained from potential energy of descent may be used to start ascending the next grade such as what happens when a road is undulating and has frequent dips. The commercialization of stored energy (in the form of rail cars raised to higher elevations) that is then converted to electrical energy when needed by an electrical grid, is being undertaken in the United States in a system called Advanced Rail Energy Storage (ARES).
Chemical potential energy.
Chemical potential energy is a form of potential energy related to the structural arrangement of atoms or molecules. This arrangement may be the result of chemical bonds within a molecule or otherwise. Chemical energy of a chemical substance can be transformed to other forms of energy by a chemical reaction. As an example, when a fuel is burned the chemical energy is converted to heat, same is the case with digestion of food metabolized in a biological organism. Green plants transform solar energy to chemical energy through the process known as photosynthesis, and electrical energy can be converted to chemical energy through electrochemical reactions.
The similar term chemical potential is used to indicate the potential of a substance to undergo a change of configuration, be it in the form of a chemical reaction, spatial transport, particle exchange with a reservoir, etc.
Electric potential energy.
An object can have potential energy by virtue of its electric charge and several forces related to their presence. There are two main types of this kind of potential energy: electrostatic potential energy, electrodynamic potential energy (also sometimes called magnetic potential energy).
Electrostatic potential energy.
Electrostatic potential energy between two bodies in space is obtained from the force exerted by a charge "Q" on another charge "q" which is given by
where r is the position vector from "Q" to "q" and "ε"0 is the vacuum permittivity. This may also be written using Coulomb's constant .
If the electric charge of an object can be assumed to be at rest, then it has potential energy due to its position relative to other charged objects. The electrostatic potential energy is the energy of an electrically charged particle (at rest) in an electric field. It is defined as the work that must be done to move it from an infinite distance away to its present location, adjusted for non-electrical forces on the object. This energy will generally be non-zero if there is another electrically charged object nearby.
The work "W" required to move "q" from "A" to any point "B" in the electrostatic force field is given by the potential function
A related quantity called "electric potential" (commonly denoted with a "V" for voltage) is equal to the electric potential energy per unit charge.
Magnetic potential energy.
The energy of a magnetic moment in an externally produced magnetic B-field has potential energy
The magnetization in a field is
where the integral can be over all space or, equivalently, where is nonzero.
Magnetic potential energy is the form of energy related not only to the distance between magnetic materials, but also to the orientation, or alignment, of those materials within the field. For example, the needle of a compass has the lowest magnetic potential energy when it is aligned with the north and south poles of the Earth's magnetic field. If the needle is moved by an outside force, torque is exerted on the magnetic dipole of the needle by the Earth's magnetic field, causing it to move back into alignment. The magnetic potential energy of the needle is highest when its field is in the same direction as the Earth's magnetic field. Two magnets will have potential energy in relation to each other and the distance between them, but this also depends on their orientation. If the opposite poles are held apart, the potential energy will be the highest when they are near the edge of their attraction, and the lowest when they pull together. Conversely, like poles will have the highest potential energy when forced together, and the lowest when they spring apart.
Nuclear potential energy.
Nuclear potential energy is the potential energy of the particles inside an atomic nucleus. The nuclear particles are bound together by the strong nuclear force. Weak nuclear forces provide the potential energy for certain kinds of radioactive decay, such as beta decay.
Nuclear particles like protons and neutrons are not destroyed in fission and fusion processes, but collections of them have less mass than if they were individually free, and this mass difference is liberated as heat and radiation in nuclear reactions (the heat and radiation have the missing mass, but it often escapes from the system, where it is not measured). The energy from the Sun is an example of this form of energy conversion. In the Sun, the process of hydrogen fusion converts about 4 million tonnes of solar matter per second into electromagnetic energy, which is radiated into space.
Forces, potential and potential energy.
Potential energy is closely linked with forces. If the work done by a force on a body that moves from "A" to "B" does not depend on the path between these points, then the work of this force measured from "A" assigns a scalar value to every other point in space and defines a scalar potential field. In this case, the force can be defined as the negative of the vector gradient of the potential field.
For example, gravity is a conservative force. The associated potential is the gravitational potential, often denoted by formula_44 or formula_45, corresponding to the energy per unit mass as a function of position. The gravitational potential energy of two particles of mass "M" and "m" separated by a distance "r" is
The gravitational potential (specific energy) of the two bodies is
where formula_48 is the reduced mass.
The work done against gravity by moving an infinitesimal mass from point A with formula_49 to point B with formula_50 is formula_51 and the work done going back the other way is formula_52 so that the total work done in moving from A to B and returning to A is
If the potential is redefined at A to be formula_54 and the potential at B to be formula_55, where formula_56 is a constant (i.e. formula_56 can be any number, positive or negative, but it must be the same at A as it is at B) then the work done going from A to B is
as before.
In practical terms, this means that one can set the zero of formula_59 and formula_44 anywhere one likes. One may set it to be zero at the surface of the Earth, or may find it more convenient to set zero at infinity (as in the expressions given earlier in this section).
A conservative force can be expressed in the language of differential geometry as a closed form. As Euclidean space is contractible, its de Rham cohomology vanishes, so every closed form is also an exact form, and can be expressed as the gradient of a scalar field. This gives a mathematical justification of the fact that all conservative forces are gradients of a potential field.

</doc>
<doc id="24236" url="https://en.wikipedia.org/wiki?curid=24236" title="Power (physics)">
Power (physics)

In physics, power is the rate of doing work. It is equivalent to an amount of energy consumed per unit time. In the SI system, the unit of power is the joule per second (J/s), known as the watt in honour of James Watt, the eighteenth-century developer of the steam engine.
The integral of power over time defines the work performed. Because this integral depends on the trajectory of the point of application of the force and torque, this calculation of work is said to be path dependent.
As a physical concept, power requires both a change in the physical universe and a specified time in which the change occurs. This is distinct from the concept of work, which is only measured in terms of a net change in the state of the physical universe. The same amount of work is done when carrying a load up a flight of stairs whether the person carrying it walks or runs, but more power is needed for running because the work is done in a shorter amount of time. 
The output power of an electric motor is the product of the torque that the motor generates and the angular velocity of its output shaft. The power involved in moving a vehicle is the product of the traction force of the wheels and the velocity of the vehicle. The rate at which a light bulb converts electrical energy into light and heat is measured in watts—the higher the wattage, the more power, or equivalently the more electrical energy is used per unit time.
Units.
The dimension of power is energy divided by time. The SI unit of power is the watt (W), which is equal to one joule per second. Other units of power include ergs per second (erg/s), horsepower (hp), metric horsepower (Pferdestärke (PS) or cheval vapeur (CV)), and foot-pounds per minute. One horsepower is equivalent to 33,000 foot-pounds per minute, or the power required to lift 550 pounds by one foot in one second, and is equivalent to about 746 watts. Other units include dBm, a relative logarithmic measure with 1 milliwatt as reference; food calories per hour (often referred to as kilocalories per hour); Btu per hour (Btu/h); and tons of refrigeration (12,000 Btu/h).
Average power.
As a simple example, burning a kilogram of coal releases much more energy than does detonating a kilogram of TNT, but because the TNT reaction releases energy much more quickly, it delivers far more power than the coal.
If Δ"W" is the amount of work performed during a period of time of duration Δ"t", the average power "P"avg over that period is given by the formula
It is the average amount of work done or energy converted per unit of time. The average power is often simply called "power" when the context makes it clear.
The instantaneous power is then the limiting value of the average power as the time interval Δ"t" approaches zero.
In the case of constant power "P", the amount of work performed during a period of duration "T" is given by:
In the context of energy conversion, it is more customary to use the symbol "E" rather than "W".
Mechanical power.
Power in mechanical systems is the combination of forces and movement. In particular, power is the product of a force on an object and the object's velocity, or the product of a torque on a shaft and the shaft's angular velocity.
Mechanical power is also described as the time derivative of work. In mechanics, the work done by a force F on an object that travels along a curve "C" is given by the line integral:
where x defines the path "C" and v is the velocity along this path.
If the force F is derivable from a potential (conservative), then applying the gradient theorem (and remembering that force is the negative of the gradient of the potential energy) yields:
where "A" and "B" are the beginning and end of the path along which the work was done.
The power at any point along the curve "C" is the time derivative
In one dimension, this can be simplified to:
In rotational systems, power is the product of the torque τ and angular velocity ω,
where ω measured in radians per second. The formula_9 represents scalar product.
In fluid power systems such as hydraulic actuators, power is given by
where "p" is pressure in pascals, or N/m2 and "Q" is volumetric flow rate in m3/s in SI units.
Mechanical advantage.
If a mechanical system has no losses, then the input power must equal the output power. This provides a simple formula for the mechanical advantage of the system.
Let the input power to a device be a force "FA" acting on a point that moves with velocity "vA" and the output power be a force "FB" acts on a point that moves with velocity "vB". If there are no losses in the system, then
and the mechanical advantage of the system (output force per input force) is given by
The similar relationship is obtained for rotating systems, where "TA" and "ωA" are the torque and angular velocity of the input and "TB" and "ωB" are the torque and angular velocity of the output. If there are no losses in the system, then
which yields the mechanical advantage
These relations are important because they define the maximum performance of a device in terms of velocity ratios determined by its physical dimensions. See for example gear ratios.
Electrical power.
The instantaneous electrical power "P" delivered to a component is given by
where 
If the component is a resistor with time-invariant voltage to current ratio, then:
where
is the resistance, measured in ohms.
Peak power and duty cycle.
In the case of a periodic signal formula_18 of period formula_19, like a train of identical pulses, the instantaneous power formula_20 is also a periodic function of period formula_19. The "peak power" is simply defined by:
The peak power is not always readily measurable, however, and the measurement of the average power formula_23 is more commonly performed by an instrument. If one defines the energy per pulse as:
then the average power is:
One may define the pulse length formula_26 such that formula_27 so that the ratios
are equal. These ratios are called the "duty cycle" of the pulse train.

</doc>
<doc id="49718" url="https://en.wikipedia.org/wiki?curid=49718" title="Poynting vector">
Poynting vector

In physics, the Poynting vector represents the directional energy flux density (the rate of energy transfer per unit area) of an electromagnetic field. The SI unit of the Poynting vector is the watt per square metre (W/m2). It is named after its inventor John Henry Poynting who first derived it in 1884. Oliver Heaviside and Nikolay Umov also independently discovered the Poynting vector.
Definition.
In Poynting's original paper and in many textbooks, the Poynting vector is defined as
where bold letters represent vectors and
This expression is often called the "Abraham form". The Poynting vector is usually denoted by S or N.
In the "microscopic" version of Maxwell's equations, this definition must be replaced by a definition in terms of the electric field E and the magnetic flux density B (it is described later in the article). 
It is also possible to combine the electric displacement field D with the magnetic flux density B to get the "Minkowski form" of the Poynting vector, or use D and H to construct yet another version. The choice has been controversial: Pfeifer et al. summarize and to a certain extent resolve the century-long dispute between proponents of the Abraham and Minkowski forms.
The Poynting vector represents the particular case of an energy flux vector for electromagnetic energy. However, any type of energy has its direction of movement in space, as well as its density, so energy flux vectors can be defined for other types of energy as well, e.g., for mechanical energy. The Umov–Poynting vector discovered by Nikolay Umov in 1874 describes energy flux in liquid and elastic media in a completely generalized view.
Interpretation.
The Poynting vector appears in Poynting's theorem (see that article for the derivation), an energy-conservation law:
where Jf is the current density of free charges and "u" is the electromagnetic energy density for linear, nondispersive materials, given by
where
The first term in the right-hand side represents the electromagnetic energy flow into a small volume, while the second term subtracts the work done by the field on free electrical currents, which thereby exits from electromagnetic energy as dissipation, heat, etc. In this definition, bound electrical currents are not included in this term, and instead contribute to S and "u".
For linear, nondispersive and isotropic(for simplicity) materials, the constitutive relations can be written as
where
Here "ε" and "μ" are scalar, real-valued constants independent of position, direction, and frequency.
In principle, this limits Poynting's theorem in this form to fields in vacuum and nondispersive linear materials. A generalization to dispersive materials is possible under certain circumstances at the cost of additional terms.
Adding the curl of a vector field.
Since the Poynting vector occurs in Poynting's theorem only through its divergence , and since the divergence of any curl is zero, one can add the curl of any vector field to the Poynting vector and the resulting vector field S' will still satisfy Poynting's theorem. The theory of special relativity, however, in which energy and momentum are defined locally and invariantly via the stress–energy tensor, shows that the given expression for the Poynting vector is unique.
It is generally argued that Maxwell equations are manifestly Lorentz covariant while the electromagnetic (EM) stress-energy tensor follows from the Maxwell equations; thus the EM momentum defined from the EM tensor certainly respects the principle of relativity. However a recent article indicates that “such an argument is based on an incomplete understanding of the relativity principle”, and states that the EM stress-energy tensor is not sufficient to define EM momentum correctly. The study also claims that Poynting vector does not represent the EM power flow (energy flux density vector) for a plane wave in an anisotropic medium if the Poynting vector is not parallel to the wave vector, and states “this conclusion is clearly supported by Fermat’s principle and special theory of relativity”.
Formulation in terms of microscopic fields.
The "microscopic" version of Maxwell's equations admits only the fundamental fields E and B, without a built-in model of material media. Only the vacuum permittivity and permeability are used, and there is no D or H. When this model is used, the Poynting vector is defined as
where
The corresponding form of Poynting's theorem is
where J is the "total" current density and the energy density "u" is given by
where ε0 is the vacuum permittivity.
It can be derived directly from Maxwell's equations in terms of "total" charge and current and the Lorentz force law only.
The two alternative definitions of the Poynting "vector" are equal in vacuum or in non-magnetic materials, where . In all other cases, they differ in that and the corresponding "u" are purely radiative, since the dissipation term covers the total current, while the E × H definition has contributions from bound currents which are then excluded from the dissipation term.
Since only the microscopic fields E and B occur in the derivation of , assumptions about any material present are completely avoided, and Poynting vector and theorem are universally valid, in vacuum as in all kinds of material. This is especially true for the electromagnetic energy density, in contrast to the "macroscopic" form "E" x "H".
Time-averaged Poynting vector.
For time-periodic sinusoidal electromagnetic fields, the average power flow per unit time is often more useful, and can be found by using the analytic representation of the electric and magnetic fields as follows (the subscript "a" denotes an analytic signal, the underbar with the subscript "m" a complex amplitude, and the superscript " * " a complex conjugate):
The average over time is given by
The second term is a sinusoidal curve
and its average is zero (for large "T"), giving
Examples and applications.
Coaxial cable.
For example, the Poynting vector within the dielectric insulator of a coaxial cable is nearly parallel to the wire axis (assuming no fields outside the cable and a wavelength longer than the diameter of the cable, including DC). Electrical energy delivered to the load is flowing entirely through the dielectric between the conductors. Very little energy flows in the conductors themselves, since the electric field strength is nearly zero. The energy flowing in the conductors flows radially into the conductors and accounts for energy lost to resistive heating of the conductor. No energy flows outside the cable, either, since there the magnetic fields of inner and outer conductors cancel to zero.
Resistive dissipation.
If a conductor has significant resistance, then, near the surface of that conductor, the Poynting vector would be tilted toward and impinge upon the conductor. Once the Poynting vector enters the conductor, it is bent to a direction that is almost perpendicular to the surface. This is a consequence of Snell's law and the very slow speed of light inside a conductor. The definition and computation of the speed of light in a conductor can be given. Inside the conductor, the Poynting vector represents energy flow from the electromagnetic field into the wire, producing resistive Joule heating in the wire. For a derivation that starts with Snell's law see Reitz page 454.
Plane waves.
In a propagating "sinusoidal" linearly polarized electromagnetic plane wave of a fixed frequency, the Poynting vector always points in the direction of propagation while oscillating in magnitude. The time-averaged magnitude of the Poynting vector is
where "E"m is the amplitude of the electric field and c is the speed of light in free space. This time-averaged value is called "irradiance" and denoted "E"e in radiometry, or is called "intensity" and denoted "I" in other fields.
Derivation.
In an electromagnetic plane wave, E and B are always perpendicular to each other and the direction of propagation. Moreover, their amplitudes are related according to
and their time and position dependences are
where "ω" is the angular frequency of the wave and k is wave vector.
The time-dependent and position magnitude of the Poynting vector is then
In the last step, we used the equality . Since the time- or space-average of is 1/2, it follows that
It will be appreciated that quantitatively the Poynting vector is evaluated only from a prior knowledge of the distribution of electric and magnetic fields, which are calculated by applying boundary conditions to a particular set of physical circumstances, for example a dipole antenna. Therefore the E and H field distributions form the primary object of any analysis, while the Poynting vector remains an interesting by-product.
Radiation pressure.
The density of the linear momentum of the electromagnetic field is "S"/c2 where "S" is the magnitude of the Poynting vector and c is the speed of light in free space. The radiation pressure exerted by an electromagnetic wave on the surface of a target is given by
Static fields.
The consideration of the Poynting vector in static fields shows the relativistic nature of the Maxwell equations and allows a better understanding of the magnetic component of the Lorentz force, . To illustrate, the accompanying picture is considered, which describes the Poynting vector in a cylindrical capacitor, which is located in an H field (pointing into the page) generated by a permanent magnet. Although there are only static electric and magnetic fields, the calculation of the Poynting vector produces a clockwise circular flow of electromagnetic energy, with no beginning or end.
While the circulating energy flow may seem nonsensical or paradoxical, it is necessary to maintain conservation of momentum. Momentum density is proportional to energy flow density, so the circulating flow of energy contains an "angular" momentum. This is the cause of the magnetic component of the Lorentz force which occurs when the capacitor is discharged. During discharge, the angular momentum contained in the energy flow is depleted as it is transferred to the charges of the discharge current crossing the magnetic field.

</doc>
<doc id="1042263" url="https://en.wikipedia.org/wiki?curid=1042263" title="Poynting's theorem">
Poynting's theorem

In electrodynamics, Poynting's theorem is a statement of conservation of energy for the electromagnetic field, in the form of a partial differential equation, due to the British physicist John Henry Poynting. Poynting's theorem is analogous to the work-energy theorem in classical mechanics, and mathematically similar to the continuity equation, because it relates the energy stored in the electromagnetic field to the work done on a charge distribution (i.e. an electrically charged object), through energy flux.
Statement.
General.
In words, the theorem is an energy balance:
A second statement can also explain the theorem - "The decrease in the electromagnetic energy per unit time in a certain volume is equal to the sum of work done by the field forces and the net outward flux per unit time".
Mathematically, this is summarised in differential form as:
where ∇•S is the divergence of the Poynting vector (energy flow) and Jf•E is the rate at which the fields do work on a charged object (Jf is the "free" current density corresponding to the motion of charge, E is the electric field, and • is the dot product). The energy density "u" is given by:
in which D is the electric displacement field, B is the magnetic flux density and H the magnetic field strength. Since only some of the charges are free to move, and the D and H fields exclude the "bound" charges and currents in the charge distribution (by their definition), one obtains the free current density Jf in the Poynting theorem, rather than the total current density J.
Using the divergence theorem, Poynting's theorem can be rewritten in integral form:
where formula_2 is the boundary of a volume "V". The shape of the volume is arbitrary but fixed for the calculation.
Electrical engineering.
In electrical engineering context the theorem is usually written with the energy density term "u" expanded in the following way, which resembles the continuity equation:
where
Derivation.
While conservation of energy and the Lorentz force law can derive the general form of the theorem, Maxwell's equations are additionally required to derive the expression for the Poynting vector and hence complete the statement.
Poynting's theorem.
Considering the statement in words above - there are three elements to the theorem, which involve writing energy transfer (per unit time) as volume integrals:
So by conservation of energy, the balance equation for the energy flow per unit time is the integral form of the theorem:
and since the volume "V" is arbitrary, this is true for all volumes, implying
which is Poynting's theorem in differential form.
Poynting vector.
From the theorem, the actual form of the Poynting vector S can be found. The time derivative of the energy density (using the product rule for vector dot products) is
using the constitutive relations
The partial time derivatives suggest using two of Maxwell's Equations. Taking the dot product of the Maxwell–Faraday equation with H:
next taking the dot product of the with E:
Collecting the results so far gives:
then, using the vector calculus identity:
gives an expression for the Poynting vector:
which physically means the energy transfer due to time-varying electric and magnetic fields is perpendicular to the fields.
Alternative forms.
It is possible to derive alternative versions of Poynting's theorem. Instead of the flux vector E × B as above, it is possible to follow the same style of derivation, but instead choose the Abraham form E × H, the Minkowski form D × B, or perhaps D × H. Each choice represents the response of the propagation medium in its own way: the E × B form above has the property that the response happens only due to electric currents, while the D × H form uses only (fictitious) magnetic monopole currents. The other two forms (Abraham and Minkowski) use complementary combinations of electric and magnetic currents to represent the polarization and magnetization responses of the medium.
Generalization.
The "mechanical" energy counterpart of the above theorem for the "electromagnetic" energy continuity equation is
where "um" is the (mechanical) kinetic energy density in the system. It can be described as the sum of kinetic energies of particles "α" (e.g., electrons in a wire), whose trajectory is given by "rα"("t"):
where S"m" is the flux of their energies, or a "mechanical Poynting vector":
Both can be combined via the Lorentz force, which the electromagnetic fields exert on the moving charged particles (see above), to the following energy continuity equation or energy conservation law:
covering both types of energy and the conversion of one into the other.

</doc>
<doc id="24714" url="https://en.wikipedia.org/wiki?curid=24714" title="Precession">
Precession

Precession is a change in the orientation of the rotational axis of a rotating body. In an appropriate reference frame it can be defined as a change in the first Euler angle, whereas the third Euler angle defines the rotation itself. In other words, the axis of rotation of a precessing body itself rotates around another axis. A motion in which the second Euler angle changes is called "nutation". In physics, there are two types of precession: torque-free and torque-induced.
In astronomy, "precession" refers to any of several slow changes in an astronomical body's rotational or orbital parameters, and especially to Earth's precession of the equinoxes. "(See section Astronomy below.)"
Torque-free.
In torque-free precession, the angular momentum remains fixed, but the angular velocity vector changes. What makes this possible is a time-varying moment of inertia, or more precisely, a time-varying inertia matrix. The inertia matrix is composed of moments of inertia calculated with respect to separate coordinate axes (e.g. x, y, z), or basis sets. If an object is asymmetric around its principal axis of rotation, the moment of inertia with respect to each basis will change with time, while preserving angular momentum. The result is that the component angular velocities around each axis will vary inversely to each axis' moment of inertia. Poinsot's ellipsoid is a geometrical analog of the functions that govern torque-free motion of a rotating rigid body.
The torque-free precession rate of an object with an axis of symmetry, such as a disk, spinning about an axis not aligned with that axis of symmetry can be calculated as follows:
where formula_2 is the precession rate, formula_3 is the spin rate about the axis of symmetry, formula_4 is the moment of inertia about the axis of symmetry, formula_5 is moment of inertia about either of the other two equal perpendicular principal axes, and formula_6 is the angle between the moment of inertia direction and the symmetry axis.
When an object is not perfectly solid, internal vortices will tend to damp torque-free precession, and the rotation axis will align itself with one of the inertia axes of the body.
For a generic solid object without any axis of symmetry, the evolution of the object's orientation, represented (for example) by a rotation matrix formula_7 that transforms internal to external coordinates, may be numerically simulated. Given the object's fixed internal moment of inertia tensor formula_8 and fixed external angular momentum formula_9, the instantaneous angular velocity is formula_10. Precession occurs by repeatedly recalculating formula_11 and applying a small rotation vector formula_12 for the short time formula_13; e.g., formula_14 for the skew-symmetric matrix formula_15. The errors induced by finite time steps tend to increase the rotational kinetic energy, formula_16; this unphysical tendency can be counter-acted by repeatedly applying a small rotation vector formula_17 perpendicular to both formula_18 and formula_9, noting that formula_20.
Another type of torque-free precession can occur when there are multiple reference frames at work. For example, Earth is subject to local torque induced precession due to the gravity of the sun and moon acting on Earth's axis, but at the same time the solar system is moving around the galactic center. As a consequence, an accurate measurement of Earth's axial reorientation relative to objects outside the frame of the moving galaxy (such as distant quasars commonly used as precession measurement reference points) must account for a minor amount of non-local torque-free precession, due to the solar system’s motion.
Torque-induced.
Torque-induced precession (gyroscopic precession) is the phenomenon in which the axis of a spinning object (e.g.,a gyroscope) describes a cone in space when an external torque is applied to it. The phenomenon is commonly seen in a spinning toy top, but all rotating objects can undergo precession. If the speed of the rotation and the magnitude of the external torque are constant, the spin axis will move at right angles to the direction that would intuitively result from the external torque. In the case of a toy top, its weight is acting downwards from its center of mass and the normal force (reaction) of the ground is pushing up on it at the point of contact with the support. These two opposite forces produce a torque which causes the top to precess.
The device depicted on the right is gimbal mounted. From inside to outside there are three axes of rotation: the hub of the wheel, the gimbal axis, and the vertical pivot.
To distinguish between the two horizontal axes, rotation around the wheel hub will be called "spinning", and rotation around the gimbal axis will be called "pitching". Rotation around the vertical pivot axis is called "rotation".
First, imagine that the entire device is rotating around the (vertical) pivot axis. Then, spinning of the wheel (around the wheelhub) is added. Imagine the gimbal axis to be locked, so that the wheel cannot pitch. The gimbal axis has sensors, that measure whether there is a torque around the gimbal axis.
In the picture, a section of the wheel has been named "dm"1. At the depicted moment in time, section "dm"1 is at the perimeter of the rotating motion around the (vertical) pivot axis. Section "dm"1, therefore, has a lot of angular rotating velocity with respect to the rotation around the pivot axis, and as "dm"1 is forced closer to the pivot axis of the rotation (by the wheel spinning further), because of the Coriolis effect, with respect to the vertical pivot axis, "dm"1 tends to move in the direction of the top-left arrow in the diagram (shown at 45°) in the direction of rotation around the pivot axis. Section "dm"2 of the wheel is moving away from the pivot axis, and so a force (again, a Coriolis force) acts in the same direction as in the case of "dm"1. Note that both arrows point in the same direction.
The same reasoning applies for the bottom half of the wheel, but there the arrows point in the opposite direction to that of the top arrows. Combined over the entire wheel, there is a torque around the gimbal axis when some spinning is added to rotation around a vertical axis.
It is important to note that the torque around the gimbal axis arises without any delay; the response is instantaneous.
In the discussion above, the setup was kept unchanging by preventing pitching around the gimbal axis. In the case of a spinning toy top, when the spinning top starts tilting, gravity exerts a torque. However, instead of rolling over, the spinning top just pitches a little. This pitching motion reorients the spinning top with respect to the torque that is being exerted. The result is that the torque exerted by gravity – via the pitching motion – elicits gyroscopic precession (which in turn yields a counter torque against the gravity torque) rather than causing the spinning top to fall to its side.
Precession or gyroscopic considerations have an effect on bicycle performance at high speed. Precession is also the mechanism behind gyrocompasses.
Classical (Newtonian).
Precession is the result of the angular velocity of rotation and the angular velocity produced by the torque. It is an angular velocity about a line that makes an angle with the permanent rotation axis, and this angle lies in a plane at right angles to the plane of the couple producing the torque. The permanent axis must turn towards this line, because the body cannot continue to rotate about any line that is not a principal axis of maximum moment of inertia; that is, the permanent axis turns in a direction at right angles to that in which the torque might be expected to turn it. If the rotating body is symmetrical and its motion unconstrained, and, if the torque on the spin axis is at right angles to that axis, the axis of precession will be perpendicular to both the spin axis and torque axis.
Under these circumstances the angular velocity of precession is given by:
Where "Is" is the moment of inertia, formula_3 is the angular velocity of spin about the spin axis, "m" is the mass, "g" is the acceleration due to gravity and "r" is the perpendicular distance of the spin axis about the axis of precession. The torque vector originates at the center of mass. Using formula_23 = formula_24, we find that the period of precession is given by:
Where "Is" is the moment of inertia, "Ts" is the period of spin about the spin axis, and formula_26 is the torque. In general, the problem is more complicated than this, however.
There is a non-mathematical way of understanding the cause of gyroscopic precession. The behavior of spinning objects simply obeys the law of inertia by resisting any change in direction. If a force is applied to a spinning object to induce a change the direction of the spin axis, the object behaves as if that force was applied at a location exactly 90 degrees ahead, in the direction of rotation. This is why: A solid object can be thought of as an assembly of individual molecules. If the object is spinning, each molecule's direction of travel constantly changes as that molecule revolves around the object's spin axis. When a force is applied that is parallel to the axis, molecules are being forced to move in new directions at certain places during their path around the axis. These new changes in direction are resisted by inertia.
Imagine the object to be a spinning bicycle wheel, held at both ends of its axle in the hands of a subject. The wheel is spinning clock-wise as seen from a viewer to the subject’s right. Clock positions on the wheel are given relative to this viewer. As the wheel spins, the molecules comprising it are travelling vertically downward the instant they pass the 3-o'clock position, horizontally to the left the instant they pass 6 o'clock, vertically upward at 9 o'clock, and horizontally to the right at 12 o'clock. Between these positions, each molecule travels components of these directions, which should be kept in mind as you read ahead. The viewer then applies a force to the wheel at the 3-o'clock position in a direction away from himself. The molecules at the 3-o'clock position are not being forced to change their direction when this happens; they still travel vertically downward. Actually, the force attempts to displace them some amount horizontally at that moment, but the ostensible component of that motion, attributed to the horizontal force, never occurs, as it would if the wheel was not spinning. Therefore, neither the horizontal nor downward components of travel are affected by the horizontally-applied force. The horizontal component started at zero and remains at zero, and the downward component is at its maximum and remains at maximum. The same holds true for the molecules located at 9 o'clock; they still travel vertically upward and not at all horizontally, thus are unaffected by the force that was applied. However, molecules at 6 and 12 o'clock are being forced to change direction. At 6 o'clock, molecules are forced to veer toward the viewer. At the same time, molecules that are passing 12 o'clock are being forced to veer away from the viewer. The inertia of those molecules resists this change in direction. The result is that they apply an equal and opposite reactive force in response. At 6 o'clock, molecules exert a push directly away from the viewer, while molecules at 12 o'clock push directly toward the viewer. This all happens instantaneously as the force is applied at 3 o'clock. Since no physical force was actually applied at 6 or 12 o’clock, there is nothing to oppose these reactive forces; therefore, the reaction is free to take place. This makes the wheel as a whole tilt toward the viewer. Thus, when the force was applied at 3 o'clock, the wheel behaved as if that force was applied at 6 o'clock, which is 90 degrees ahead in the direction of rotation. 
This principle is demonstrated in helicopters. Helicopter controls are rigged so that inputs to them are transmitted to the rotor blades at points 90 degrees prior to the point where the change in aircraft attitude is desired. 
Precession causes another phenomenon for spinning objects such as the bicycle wheel in this scenario. If the subject holding the wheel removes one hand from the end of the axle, the wheel will not topple over, but will remain upright, supported at just the other end of its axle. However, it will immediately take on an additional motion; it will begin to rotate about a vertical axis, pivoting at the point of support as it continues spinning. If the wheel was not spinning, it would topple over and fall when one hand is removed. The ostensible action of the wheel beginning to topple over is equivalent to applying a force to it at 12 o'clock in the direction of the unsupported side (or a force at 6 o’clock toward the supported side). When the wheel is spinning, the sudden lack of support at one end of its axle is equivalent to this same force. So, instead of toppling over, the wheel behaves as if a continuous force is being applied to it at 3 or 9 o’clock, depending on the direction of spin and which hand was removed. This causes the wheel to begin pivoting at the point of support while remaining upright. It should be noted that although it pivots at the point of support, it does so only because of the fact that it is supported there; the actual axis of precessional rotation is located vertically through the wheel, passing through its center of mass. Also, this explanation does not account for the effect of variation in the speed of the spinning object; it only describes how the spin axis behaves due to precession. More correctly, the object behaves according to the balance of all forces based on the magnitude of the applied force, mass and rotational speed of the object.
Relativistic.
The special and general theories of relativity give three types of corrections to the Newtonian precession, of a gyroscope near a large mass such as Earth, described above. They are:
Astronomy.
In astronomy, precession refers to any of several gravity-induced, slow and continuous changes in an astronomical body's rotational axis or orbital path. Precession of the equinoxes, perihelion precession, changes in the tilt of Earth's axis to its orbit, and the eccentricity of its orbit over tens of thousands of years are all important parts of the astronomical theory of ice ages. "(See Milankovitch cycles.)"
Axial precession (precession of the equinoxes).
Axial precession is the movement of the rotational axis of an astronomical body, whereby the axis slowly traces out a cone. In the case of Earth, this type of precession is also known as the "precession of the equinoxes", "lunisolar precession", or "precession of the equator". Earth goes through one such complete precessional cycle in a period of approximately 26,000 years or 1° every 72 years, during which the positions of stars will slowly change in both equatorial coordinates and ecliptic longitude. Over this cycle, Earth's north axial pole moves from where it is now, within 1° of Polaris, in a circle around the ecliptic pole, with an angular radius of about 23.5 degrees.
Hipparchus is the earliest known astronomer to recognize and assess the precession of the equinoxes at about 1° per century (which is not far from the actual value for antiquity, 1.38°). The precession of Earth's axis was later explained by Newtonian physics. Being an oblate spheroid, Earth has a non-spherical shape, bulging outward at the equator. The gravitational tidal forces of the Moon and Sun apply torque to the equator, attempting to pull the equatorial bulge into the plane of the ecliptic, but instead causing it to precess. The torque exerted by the planets, particularly Jupiter, also plays a role.
Perihelion precession.
The orbits of a planet around the Sun do not really follow an identical ellipse each time, but actually trace out a flower-petal shape because the major axis of each planet's elliptical orbit also precesses within its orbital plane, partly in response to perturbations in the form of the changing gravitational forces exerted by other planets. This is called perihelion precession or apsidal precession.
In the adjunct image, the Earth apsidal precession is illustrated. As the Earth travels around the Sun, its elliptical orbit rotates gradually over time. The eccentricity of its ellipse and the precession rate of its orbit are exaggerated for visualization. Most orbits in the Solar System have a much smaller eccentricity and precess at a much slower rate, making them nearly circular and stationary.
Discrepancies between the observed perihelion precession rate of the planet Mercury and that predicted by classical mechanics were prominent among the forms of experimental evidence leading to the acceptance of Einstein's Theory of Relativity (in particular, his General Theory of Relativity), which accurately predicted the anomalies. Deviating from Newton's law, Einstein's theory of gravitation predicts an extra term of A/r4, which accurately gives the observed excess turning rate of 43 arcseconds every 100 years.
The gravitational force between the Sun and moon induces the precession in Earth's orbit, which is the major cause of the widely known climate oscillation of Earth that has a period of 19,000 to 23,000 years. It follows that changes in Earth's orbital parameters (e.g., orbital inclination, the angle between Earth's rotation axis and its plane of orbit) is important to the study of Earth's climate, in particular to the study of past ice ages. "(See also nodal precession. For precession of the lunar orbit see lunar precession)."

</doc>
<doc id="310950" url="https://en.wikipedia.org/wiki?curid=310950" title="Principal bundle">
Principal bundle

In mathematics, a principal bundle is a mathematical object which formalizes some of the essential features of the Cartesian product of a space with a group . In the same way as with the Cartesian product, a principal bundle is equipped with
Unlike a product space, principal bundles lack a preferred choice of identity cross-section; they have no preferred analog of . Likewise, there is not generally a projection onto generalizing the projection onto the second factor, which exists for the Cartesian product. They may also have a complicated topology, which prevents them from being realized as a product space even if a number of arbitrary choices are made to try to define such a structure by defining it on smaller pieces of the space.
A common example of a principal bundle is the frame bundle of a vector bundle , which consists of all ordered bases of the vector space attached to each point. The group in this case is the general linear group, which acts on the right in the usual way: by changes of basis. Since there is no preferred way to choose an ordered basis of a vector space, a frame bundle lacks a canonical choice of identity cross-section.
Principal bundles have important applications in topology and differential geometry. They have also found application in physics where they form part of the foundational framework of gauge theories. Principal bundles provide a unifying framework for the theory of fiber bundles in the sense that all fiber bundles with structure group determine a unique principal from which the original bundle can be reconstructed.
Formal definition.
A principal -bundle, where denotes any topological group, is a fiber bundle together with a continuous right action such that preserves the fibers of (i.e. if then for all ) and acts freely and transitively on them. This implies that each fiber of the bundle is homeomorphic to the group itself. Frequently, one requires the base space to be Hausdorff and possibly paracompact.
Since the group action preserves the fibers of and acts transitively, it follows that the orbits of the -action are precisely these fibers and the orbit space is homeomorphic to the base space . Because the action is free, the fibers have the structure of "G"-torsors. A -torsor is a space which is homeomorphic to but lacks a group structure since there is no preferred choice of an identity element.
An equivalent definition of a principal -bundle is as a -bundle with fiber where the structure group acts on the fiber by left multiplication. Since right multiplication by on the fiber commutes with the action of the structure group, there exists an invariant notion of right multiplication by on . The fibers of then become right -torsors for this action.
The definitions above are for arbitrary topological spaces. One can also define principal -bundles in the category of smooth manifolds. Here is required to be a smooth map between smooth manifolds, is required to be a Lie group, and the corresponding action on should be smooth.
Examples.
The prototypical example of a smooth principal bundle is the frame bundle of a smooth manifold , often denoted or . Here the fiber over a point is the set of all frames (i.e. ordered bases) for the tangent space . The general linear group acts freely and transitively on these frames. These fibers can be glued together in a natural way so as to obtain a principal -bundle over .
Variations on the above example include the orthonormal frame bundle of a Riemannian manifold. Here the frames are required to be orthonormal with respect to the metric. The structure group is the orthogonal group . The example also works for bundles other than the tangent bundle; if is any vector bundle of rank over , then the bundle of frames of is a principal -bundle, sometimes denoted .
A normal (regular) covering space is a principal bundle where the structure group
acts on the fibres of via the monodromy action. In particular, the universal cover of is a principal bundle over with structure group (since the universal cover is simply connected and thus is trivial).
Let be a Lie group and let be a closed subgroup (not necessarily normal). Then is a principal -bundle over the (left) coset space . Here the action of on is just right multiplication. The fibers are the left cosets of (in this case there is a distinguished fiber, the one containing the identity, which is naturally isomorphic to ).
Consider the projection given by . This principal -bundle is the associated bundle of the Möbius strip. Besides the trivial bundle, this is the only principal -bundle over .
Projective spaces provide some more interesting examples of principal bundles. Recall that the -sphere is a two-fold covering space of real projective space . The natural action of on gives it the structure of a principal -bundle over . Likewise, is a principal -bundle over complex projective space and is a principal -bundle over quaternionic projective space . We then have a series of principal bundles for each positive :
Here denotes the unit sphere in (equipped with the Euclidean metric). For all of these examples the cases give the so-called Hopf bundles.
Basic properties.
Trivializations and cross sections.
One of the most important questions regarding any fiber bundle is whether or not it is trivial, "i.e." isomorphic to a product bundle. For principal bundles there is a convenient characterization of triviality:
The same is not true for other fiber bundles. For instance, Vector bundles always have a zero section whether they are trivial or not and sphere bundles may admit many global sections without being trivial.
The same fact applies to local trivializations of principal bundles. Let be a principal -bundle. An open set in admits a local trivialization if and only if there exists a local section on . Given a local trivialization 
one can define an associated local section 
where is the identity in . Conversely, given a section one defines a trivialization by
The simple transitivity of the action on the fibers of guarantees that this map is a bijection, it is also a homeomorphism. The local trivializations defined by local sections are -equivariant in the following sense. If we write
in the form 
then the map
satisfies
Equivariant trivializations therefore preserve the -torsor structure of the fibers. In terms of the associated local section the map is given by
The local version of the cross section theorem then states that the equivariant local trivializations of a principal bundle are in one-to-one correspondence with local sections.
Given an equivariant local trivialization of , we have local sections on each . On overlaps these must be related by the action of the structure group . In fact, the relationship is provided by the transition functions
For any we have
Characterization of smooth principal bundles.
If "π" : "P" → "X" is a smooth principal "G"-bundle then "G" acts freely and properly on "P" so that the orbit space "P"/"G" is diffeomorphic to the base space "X". It turns out that these properties completely characterize smooth principal bundles. That is, if "P" is a smooth manifold, "G" a Lie group and "μ" : "P" × "G" → "P" a smooth, free, and proper right action then
Use of the notion.
Reduction of the structure group.
Given a subgroup "H" of "G" one may consider the bundle formula_15 whose fibers are homeomorphic to the coset space formula_16. If the new bundle admits a global section, then one says that the section is a reduction of the structure group from "G" to "H" . The reason for this name is that the (fiberwise) inverse image of the values of this section form a subbundle of "P" which is a principal "H"-bundle. If "H" is the identity, then a section of "P" itself is a reduction of the structure group to the identity. Reductions of the structure group do not in general exist.
Many topological questions about the structure of a manifold or the structure of bundles over it that are associated to a principal "G"-bundle may be rephrased as questions about the admissibility of the reduction of the structure group (from "G" to "H"). For example:
Also note: an "n"-dimensional manifold admits "n" vector fields that are linearly independent at each point if and only if its frame bundle admits a global section. In this case, the manifold is called parallelizable.
Associated vector bundles and frames.
If "P" is a principal "G"-bundle and "V" is a linear representation of "G", then one can construct a vector bundle formula_24 with fibre "V", as the quotient of the product "P"×"V" by the diagonal action of "G". This is a special case of the associated bundle construction, and "E" is called an associated vector bundle to "P". If the representation of "G" on "V" is faithful, so that "G" is a subgroup of the general linear group GL("V"), then "E" is a "G"-bundle and "P" provides a reduction of structure group of the frame bundle of "E" from GL("V") to "G". This is the sense in which principal bundles provide an abstract formulation of the theory of frame bundles.
Classification of principal bundles.
Any topological group admits a classifying space : the quotient by the action of of some weakly contractible space , "i.e." a topological space with vanishing homotopy groups. The classifying space has the property that any principal bundle over a paracompact manifold "B" is isomorphic to a pullback of the principal bundle . In fact, more is true, as the set of isomorphism classes of principal bundles over the base identifies with the set of homotopy classes of maps .

</doc>
<doc id="532481" url="https://en.wikipedia.org/wiki?curid=532481" title="Principal quantum number">
Principal quantum number

In quantum mechanics, the principal quantum number (symbolized n) is one of four quantum numbers which are assigned to each electron in an atom to describe that electron's state. As a discrete variable, the principal quantum number is always an integer. As "n" increases, the orbital becomes larger and the electron spends more time farther from the nucleus. As "n" increases, the electron is also at a higher potential energy and is therefore less tightly bound to the nucleus.
The principle quantum number was first created for use in the semiclassical Bohr model of the atom, distinguishing between different energy levels. With the development of modern quantum mechanics, the simple Bohr model was replaced with a more complex theory of atomic orbitals. However, modern theory still requires the principle quantum number. Apart from the principal quantum number, the other quantum numbers for bound electrons are the azimuthal quantum number, the magnetic quantum number, and the spin quantum number.
For an analogy, one could imagine a multistoried building with an elevator structure. The building has an integer number of floors, and a (well-functioning) elevator which can only stop at a particular floor. Furthermore, the elevator can only travel an integer number of levels. As with the principal quantum number, higher numbers are associated with higher potential energy.
Beyond this point the analogy breaks down; in the case of elevators the potential energy is gravitational but with the quantum number it is electromagnetic. The gains and losses in energy are approximate with the elevator, but precise with quantum state. The elevator ride from floor to floor is continuous whereas quantum transitions are discontinuous. Finally the constraints of elevator design are imposed by the requirements of architecture, but quantum behavior reflects fundamental laws of physics.
Derivation.
There are a set of quantum numbers associated with the energy states of the atom. The four quantum numbers "n", "ℓ", "m", and "s" specify the complete and unique quantum state of a single electron in an atom, called its wave function or orbital. Two electrons belonging to the same atom can not have the same four quantum numbers, due to the Pauli exclusion principle. The wave function of the Schrödinger wave equation reduces to the three equations that when solved lead to the first three quantum numbers. Therefore, the equations for the first three quantum numbers are all interrelated. The principal quantum number arose in the solution of the radial part of the wave equation as shown below.
The Schrödinger wave equation describes energy eigenstates having corresponding real numbers "En" with a definite total energy which the value of "En" defines. The bound state energies of the electron in the hydrogen atom are given by:
The parameter "n" can take only positive integer values. The concept of energy levels and notation was utilized from the earlier Bohr model of the atom. Schrödinger's equation developed the idea from a flat two-dimensional Bohr atom to the three-dimensional wave function model.
In the Bohr model, the allowed orbits were derived from quantized (discrete) values of orbital angular momentum, "L" according to the equation
where "n" = 1, 2, 3, … and is called the principal quantum number, and "h" is Planck's constant. This formula is not correct in quantum mechanics as the angular momentum magnitude is described by the azimuthal quantum number, but the energy levels are accurate and classically they correspond to the sum of potential and kinetic energy of the electron.
The principal quantum number "n" represents the relative overall energy of each orbital, and the energy of each orbital increases as the distance from the nucleus increases. The sets of orbitals with the same "n" value are often referred to as electron shells or energy levels.
The minimum energy exchanged during any wave-matter interaction is the wave frequency multiplied by Planck's constant. This causes the wave to display particle-like packets of energy called quanta. The difference between energy levels that have different "n" determine the Emission spectrum of the element.
In the notation of the periodic table, the main shells of electrons are labeled:
based on the principal quantum number.
The principal quantum number is related to the radial quantum number, "n""r", by:
where "ℓ" is the azimuthal quantum number and "n""r" is equal to the number of nodes in the radial wavefunction.

</doc>
<doc id="249438" url="https://en.wikipedia.org/wiki?curid=249438" title="Principle of least action">
Principle of least action

The principle of least action – or, more accurately, the principle of stationary action – is a variational principle that, when applied to the action of a mechanical system, can be used to obtain the equations of motion for that system. In relativity, a different action must be minimized or maximized. The principle can be used to derive Newtonian, Lagrangian, Hamiltonian equations of motion, and even General Relativity. It was historically called "least" because its solution requires finding the path that has the least change from nearby paths. Its classical mechanics and electromagnetic expressions are a consequence of quantum mechanics, but the stationary action method helped in the development of quantum mechanics.
The principle remains central in modern physics and mathematics, being applied in the theory of relativity, quantum mechanics and quantum field theory, and a focus of modern mathematical investigation in Morse theory. Maupertuis' principle and Hamilton's principle exemplify the principle of stationary action.
The action principle is preceded by earlier ideas in surveying and optics. Rope stretchers in ancient Egypt stretched corded ropes to measure the distance between two points. Ptolemy, in his "Geography" (Bk 1, Ch 2), emphasized that one must correct for "deviations from a straight course". In ancient Greece, Euclid wrote in his "Catoptrica" that, for the path of light reflecting from a mirror, the angle of incidence equals the angle of reflection. Hero of Alexandria later showed that this path was the shortest length and least time.
Scholars often credit Pierre Louis Maupertuis for formulating the principle of least action because he wrote about it in 1744 and 1746. However, Leonhard Euler discussed the principle in 1744, and evidence shows that Gottfried Leibniz preceded both by 39 years.
In 1932, Paul Dirac discerned the quantum mechanical underpinning of the principle in the quantum interference of amplitudes:For macroscopic systems, the dominant contribution to the apparent path is the classical path (the stationary, action-extremizing one), while any other path is possible in the quantum realm.
General statement.
The starting point is the "action", denoted formula_1 (calligraphic S), of a physical system. It is defined as the integral of the Lagrangian "L" between two instants of time "t"1 and "t"2 - technically a functional of the "N" generalized coordinates q = ("q"1, "q"2 ... "qN") which define the configuration of the system:
where the dot denotes the time derivative, and "t" is time.
Mathematically the principle is
where δ (Greek lowercase delta) means a "small" change. In words this reads:
In applications the statement and definition of action are taken together:
The action and Lagrangian both contain the dynamics of the system for all times. The term "path" simply refers to a curve traced out by the system in terms of the coordinates in the configuration space, i.e. the curve q("t"), parameterized by time (see also parametric equation for this concept).
Origins, statements, and controversy.
Fermat.
In the 1600s, Pierre de Fermat postulated that ""light travels between two given points along the path of shortest time"," which is known as the principle of least time or Fermat's principle.
Maupertuis.
Credit for the formulation of the principle of least action is commonly given to Pierre Louis Maupertuis, who felt that "Nature is thrifty in all its actions", and applied the principle broadly:
This notion of Maupertuis, although somewhat deterministic today, does capture much of the essence of mechanics.
In application to physics, Maupertuis suggested that the quantity to be minimized was the product of the duration (time) of movement within a system by the "vis viva",
which is the integral of twice what we now call the kinetic energy "T" of the system.
Euler.
Leonhard Euler gave a formulation of the action principle in 1744, in very recognizable terms, in the "Additamentum 2" to his "Methodus Inveniendi Lineas Curvas Maximi Minive Proprietate Gaudentes". Beginning with the second paragraph:
As Euler states, ∫"Mv"d"s" is the integral of the momentum over distance travelled, which, in modern notation, equals the reduced action
Thus, Euler made an equivalent and (apparently) independent statement of the variational principle in the same year as Maupertuis, albeit slightly later. Curiously, Euler did not claim any priority, as the following episode shows.
Disputed priority.
Maupertuis' priority was disputed in 1751 by the mathematician Samuel König, who claimed that it had been invented by Gottfried Leibniz in 1707. Although similar to many of Leibniz's arguments, the principle itself has not been documented in Leibniz's works. König himself showed a "copy" of a 1707 letter from Leibniz to Jacob Hermann with the principle, but the "original" letter has been lost. In contentious proceedings, König was accused of forgery, and even the King of Prussia entered the debate, defending Maupertuis (the head of his Academy), while Voltaire defended König.
Euler, rather than claiming priority, was a staunch defender of Maupertuis, and Euler himself prosecuted König for forgery before the Berlin Academy on 13 April 1752. The claims of forgery were re-examined 150 years later, and archival work by C.I. Gerhardt in 1898 and W. Kabitz in 1913 uncovered other copies of the letter, and three others cited by König, in the Bernoulli archives.
Further development.
Euler continued to write on the topic; in his "Reflexions sur quelques loix generales de la nature" (1748), he called the quantity "effort". His expression corresponds to what we would now call potential energy, so that his statement of least action in statics is equivalent to the principle that a system of bodies at rest will adopt a configuration that minimizes total potential energy.
Lagrange and Hamilton.
Much of the calculus of variations was stated by Joseph-Louis Lagrange in 1760 and he proceeded to apply this to problems in dynamics. In "Méchanique Analytique" (1788) Lagrange derived the general equations of motion of a mechanical body. William Rowan Hamilton in 1834 and 1835 applied the variational principle to the classical Lagrangian function
to obtain the Euler–Lagrange equations in their present form.
Jacobi and Morse.
In 1842, Carl Gustav Jacobi tackled the problem of whether the variational principle always found minima as opposed to other stationary points (maxima or stationary saddle points); most of his work focused on geodesics on two-dimensional surfaces. The first clear general statements were given by Marston Morse in the 1920s and 1930s, leading to what is now known as Morse theory. For example, Morse showed that the number of conjugate points in a trajectory equalled the number of negative eigenvalues in the second variation of the Lagrangian.
Gauss and Hertz.
Other extremal principles of classical mechanics have been formulated, such as Gauss's principle of least constraint and its corollary, Hertz's principle of least curvature.
Disputes about possible teleological aspects.
The mathematical equivalence of the differential equations of motion and their integral
counterpart has important philosophical implications. The differential equations are statements about quantities localized to a single point in space or single moment of time. For example, Newton's second law
states that the "instantaneous" force F applied to a mass "m" produces an acceleration a at the same "instant". By contrast, the action principle is not localized to a point; rather, it involves integrals over an interval of time and (for fields) an extended region of space. Moreover, in the usual formulation of classical action principles, the initial and final states of the system are fixed, e.g.,
In particular, the fixing of the "final" state has been interpreted as giving the action principle a teleological character which has been controversial historically. However, according to W. Yourgrau and S. Mandelstam, "the teleological approach... presupposes that the variational principles themselves have mathematical characteristics which they "de facto" do not possess" In addition, some critics maintain this apparent teleology occurs because of the way in which the question was asked. By specifying some but not all aspects of both the initial and final conditions (the positions but not the velocities) we are making some inferences about the initial conditions from the final conditions, and it is this "backward" inference that can be seen as a teleological explanation. Teleology can also be overcome if we consider the classical description as a limiting case of the quantum formalism of path integration, in which stationary paths are obtained as a result of interference of amplitudes along all possible paths.
The short story "Story of Your Life" by the speculative fiction writer Ted Chiang contains visual depictions of Fermat's Principle along with a discussion of its teleological dimension. Keith Devlin's "The Math Instinct" contains a chapter, "Elvis the Welsh Corgi Who Can Do Calculus" that discusses the calculus "embedded" in some animals as they solve the "least time" problem in actual situations.
More Fundamental Than Newton's 2nd Law.
According to Richard Feynman, the principle of least action is mathematically more specific than Newton's 2nd law and more fundamental in theoretical physics because it explains a wider range of physical law. You can derive Newton's 2nd law from least action, but the converse is not true without also applying Newton's 1st and 3rd laws and disallowing non-conservative forces like friction. By being more specific and thereby explaining only conservative forces, the principle of least action is able to solve problems Newton's 2nd law can't, but the converse is not true. The principle of least action can be used to derive the conservation of momentum and energy if its symmetry in space and time are assumed. It correctly does not allow non-conservative potential fields, but Newton's 2nd law allows for them by allowing for non-conservative momenta and forces (such as friction) which are not fundamental forces. The mathematical basis for the difference is that Newton's 2nd law (correctly stated as F=dp/dt instead of F=ma) allows for momentums p(t)=q(t)+C where q(t) are conserved momenta allowed by least action and C is a constant that can be non-zero in Newton's 2nd law but not in least action. The constant allows for non-conservative momenta and therefore non-conservative forces and potentials in Newton's 2nd law. Newton's 2nd law explains conservation of energy and momentum and can be used to show equivalency with least action when forces are properly conserved, e.g. when forces are summed to zero in accordance with Newton's 1st and 3rd laws and when accounting for heat generated by friction. Derivations of Lagrangian and Hamiltonian methods do not begin with Newton's 2nd law, but with a more modern mathematical formulation of it that requires forces to be conservative.

</doc>
<doc id="22934" url="https://en.wikipedia.org/wiki?curid=22934" title="Probability">
Probability

Probability is the measure of the likeliness that an event will occur. Probability is quantified as a number between 0 and 1 (where 0 indicates impossibility and 1 indicates certainty). The higher the probability of an event, the more certain we are that the event will occur. A simple example is the toss of a fair (unbiased) coin. Since the two outcomes are equally probable, the probability of "heads" equals the probability of "tails", so the probability is 1/2 (or 50%) chance of either "heads" or "tails".
These concepts have been given an axiomatic mathematical formalization in probability theory (see probability axioms), which is used widely in such areas of study as mathematics, statistics, finance, gambling, science (in particular physics), artificial intelligence/machine learning, computer science, game theory, and philosophy to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems.
Interpretations.
When dealing with experiments that are random and well-defined in a purely theoretical setting (like tossing a fair coin), probabilities can be numerically described by the number of desired outcomes divided by the total number of all outcomes (tossing a fair coin twice will yield head-head with probability 1/4, because the four outcomes head-head, head-tails, tails-head and tails-tails are equally likely to occur). When it comes to practical application however, there are two major competing categories of probability interpretations, whose adherents possess different views about the fundamental nature of probability:
Etymology.
The word "probability" derives from the Latin "probabilitas", which can also mean "probity", a measure of the authority of a witness in a legal case in Europe, and often correlated with the witness's nobility. In a sense, this differs much from the modern meaning of "probability", which, in contrast, is a measure of the weight of empirical evidence, and is arrived at from inductive reasoning and statistical inference.
History.
The scientific study of probability is a modern development. Gambling shows that there has been an interest in quantifying the ideas of probability for millennia, but exact mathematical descriptions arose much later. There are reasons of course, for the slow development of the mathematics of probability. Whereas games of chance provided the impetus for the mathematical study of probability, are still obscured by the superstitions of gamblers.
According to Richard Jeffrey, "Before the middle of the seventeenth century, the term 'probable' (Latin "probabilis") meant "approvable", and was applied in that sense, univocally, to opinion and to action. A probable action or opinion was one such as sensible people would undertake or hold, in the circumstances." However, in legal contexts especially, 'probable' could also apply to propositions for which there was good evidence.
The sixteenth century Italian polymath Gerolamo Cardano demonstrated the efficacy of defining odds as the ratio of favourable to unfavourable outcomes (which implies that the probability of an event is given by the ratio of favourable outcomes to the total number of possible outcomes).
Aside from the elementary work by Cardano, the doctrine of probabilities dates to the correspondence of Pierre de Fermat and Blaise Pascal (1654). Christiaan Huygens (1657) gave the earliest known scientific treatment of the subject. Jakob Bernoulli's "Ars Conjectandi" (posthumous, 1713) and Abraham de Moivre's "Doctrine of Chances" (1718) treated the subject as a branch of mathematics. See Ian Hacking's "The Emergence of Probability" and James Franklin's "The Science of Conjecture" for histories of the early development of the very concept of mathematical probability.
The theory of errors may be traced back to Roger Cotes's "Opera Miscellanea" (posthumous, 1722), but a memoir prepared by Thomas Simpson in 1755 (printed 1756) first applied the theory to the discussion of errors of observation. The reprint (1757) of this memoir lays down the axioms that positive and negative errors are equally probable, and that certain assignable limits define the range of all errors. Simpson also discusses continuous errors and describes a probability curve.
The first two laws of error that were proposed both originated with Pierre-Simon Laplace. The first law was published in 1774 and stated that the frequency of an error could be expressed as an exponential function of the numerical magnitude of the error, disregarding sign. The second law of error was proposed in 1778 by Laplace and stated that the frequency of the error is an exponential function of the square of the error. The second law of error is called the normal distribution or the Gauss law. "It is difficult historically to attribute that law to Gauss, who in spite of his well-known precocity had probably not made this discovery before he was two years old."
Daniel Bernoulli (1778) introduced the principle of the maximum product of the probabilities of a system of concurrent errors.
Adrien-Marie Legendre (1805) developed the method of least squares, and introduced it in his "Nouvelles méthodes pour la détermination des orbites des comètes" ("New Methods for Determining the Orbits of Comets"). In ignorance of Legendre's contribution, an Irish-American writer, Robert Adrain, editor of "The Analyst" (1808), first deduced the law of facility of error,
where formula_2 is a constant depending on precision of observation, and formula_3 is a scale factor ensuring that the area under the curve equals 1. He gave two proofs, the second being essentially the same as John Herschel's (1850). Gauss gave the first proof that seems to have been known in Europe (the third after Adrain's) in 1809. Further proofs were given by Laplace (1810, 1812), Gauss (1823), James Ivory (1825, 1826), Hagen (1837), Friedrich Bessel (1838), W. F. Donkin (1844, 1856), and Morgan Crofton (1870). Other contributors were Ellis (1844), De Morgan (1864), Glaisher (1872), and Giovanni Schiaparelli (1875). Peters's (1856) formula for "r", the probable error of a single observation, is well known.
In the nineteenth century authors on the general theory included Laplace, Sylvestre Lacroix (1816), Littrow (1833), Adolphe Quetelet (1853), Richard Dedekind (1860), Helmert (1872), Hermann Laurent (1873), Liagre, Didion, and Karl Pearson. Augustus De Morgan and George Boole improved the exposition of the theory.
Andrey Markov introduced the notion of Markov chains (1906), which played an important role in stochastic processes theory and its applications. The modern theory of probability based on the measure theory was developed by Andrey Kolmogorov (1931).
On the geometric side (see integral geometry) contributors to "The Educational Times" were influential (Miller, Crofton, McColl, Wolstenholme, Watson, and Artemas Martin).
Theory.
Like other theories, the theory of probability is a representation of probabilistic concepts in formal terms—that is, in terms that can be considered separately from their meaning. These formal terms are manipulated by the rules of mathematics and logic, and any results are interpreted or translated back into the problem domain.
There have been at least two successful attempts to formalize probability, namely the Kolmogorov formulation and the Cox formulation. In Kolmogorov's formulation (see probability space), sets are interpreted as events and probability itself as a measure on a class of sets. In Cox's theorem, probability is taken as a primitive (that is, not further analyzed) and the emphasis is on constructing a consistent assignment of probability values to propositions. In both cases, the laws of probability are the same, except for technical details.
There are other methods for quantifying uncertainty, such as the Dempster–Shafer theory or possibility theory, but those are essentially different and not compatible with the laws of probability as usually understood.
Applications.
Probability theory is applied in everyday life in risk assessment and in trade on financial markets. Governments apply probabilistic methods in environmental regulation, where it is called pathway analysis.
A good example is the effect of the perceived probability of any widespread Middle East conflict on oil prices—which have ripple effects in the economy as a whole. An assessment by a commodity trader that a war is more likely vs. less likely sends prices up or down, and signals other traders of that opinion. Accordingly, the probabilities are neither assessed independently nor necessarily very rationally. The theory of behavioral finance emerged to describe the effect of such groupthink on pricing, on policy, and on peace and conflict.
In addition to financial assessment, probability can be used to analyze trends in biology (e.g. disease spread) as well as ecology (e.g. biological Punnett squares). As with finance, risk assessment can be used as a statistical tool to calculate the likelihood of undesirable events occurring and can assist with implementing protocols to avoid encountering such circumstances.
The discovery of rigorous methods to assess and combine probability assessments has changed society. It is important for most citizens to understand how probability assessments are made, and how they contribute to decisions.
Another significant application of probability theory in everyday life is reliability. Many consumer products, such as automobiles and consumer electronics, use reliability theory in product design to reduce the probability of failure. Failure probability may influence a manufacturer's decisions on a product's warranty.
The cache language model and other statistical language models that are used in natural language processing are also examples of applications of probability theory.
Mathematical treatment.
Consider an experiment that can produce a number of results. The collection of all results is called the sample space of the experiment. The power set of the sample space is formed by considering all different collections of possible results. For example, rolling a die can produce six possible results. One collection of possible results gives an odd number on the dice. Thus, the subset {1,3,5} is an element of the power set of the sample space of dice rolls. These collections are called "events." In this case, {1,3,5} is the event that the dice falls on some odd number. If the results that actually occur fall in a given event, the event is said to have occurred.
A probability is a way of assigning every event a value between zero and one, with the requirement that the event made up of all possible results (in our example, the event {1,2,3,4,5,6}) is assigned a value of one. To qualify as a probability, the assignment of values must satisfy the requirement that if you look at a collection of mutually exclusive events (events with no common results, e.g., the events {1,6}, {3}, and {2,4} are all mutually exclusive), the probability that at least one of the events will occur is given by the sum of the probabilities of all the individual events.
The probability of an event "A" is written as formula_4, formula_5, or formula_6. This mathematical definition of probability can extend to infinite sample spaces, and even uncountable sample spaces, using the concept of a measure.
The "opposite" or "complement" of an event "A" is the event [not "A"] (that is, the event of "A" not occurring), often denoted as formula_7, or formula_8; its probability is given by . As an example, the chance of not rolling a six on a six-sided die is formula_9. See Complementary event for a more complete treatment.
If two events "A" and "B" occur on a single performance of an experiment, this is called the intersection or joint probability of "A" and "B", denoted as formula_10.
Independent events.
If two events, "A" and "B" are independent then the joint probability is
for example, if two coins are flipped the chance of both being heads is formula_12.
Mutually exclusive events.
If either event "A" or event "B" occurs on a single performance of an experiment this is called the union of the events "A" and "B" denoted as formula_13.
If two events are mutually exclusive then the probability of either occurring is
For example, the chance of rolling a 1 or 2 on a six-sided is formula_15
Not mutually exclusive events.
If the events are not mutually exclusive then
For example, when drawing a single card at random from a regular deck of cards, the chance of getting a heart or a face card (J,Q,K) (or one that is both) is formula_17, because of the 52 cards of a deck 13 are hearts, 12 are face cards, and 3 are both: here the possibilities included in the "3 that are both" are included in each of the "13 hearts" and the "12 face cards" but should only be counted once.
Conditional probability.
"Conditional probability" is the probability of some event "A", given the occurrence of some other event "B".
Conditional probability is written formula_18, and is read "the probability of "A", given "B"". It is defined by
If formula_20 then formula_18 is formally undefined by this expression. However, it is possible to define a conditional probability for some zero-probability events using a σ-algebra of such events (such as those arising from a continuous random variable).
For example, in a bag of 2 red balls and 2 blue balls (4 balls in total), the probability of taking a red ball is formula_22; however, when taking a second ball, the probability of it being either a red ball or a blue ball depends on the ball previously taken, such as, if a red ball was taken, the probability of picking a red ball again would be formula_23 since only 1 red and 2 blue balls would have been remaining.
Inverse probability.
In probability theory and applications, Bayes' rule relates the odds of event formula_24 to event formula_25, before (prior to) and after (posterior to) conditioning on another event formula_26. The odds on formula_24 to event formula_25 is simply the ratio of the probabilities of the two events. When arbitrarily many events formula_29 are of interest, not just two, the rule can be rephrased as posterior is proportional to prior times likelihood, formula_30 where the proportionality symbol means that the left hand side is proportional to (i.e., equals a constant times) the right hand side as formula_29 varies, for fixed or given formula_26 (Lee, 2012; Bertsch McGrayne, 2012). In this form it goes back to Laplace (1774) and to Cournot (1843); see Fienberg (2005). See Inverse probability and Bayes' rule.
Relation to randomness.
In a deterministic universe, based on Newtonian concepts, there would be no probability if all conditions were known (Laplace's demon), (but there are situations in which sensitivity to initial conditions exceeds our ability to measure them, i.e. know them). In the case of a roulette wheel, if the force of the hand and the period of that force are known, the number on which the ball will stop would be a certainty (though as a practical matter, this would likely be true only of a roulette wheel that had not been exactly levelled — as Thomas A. Bass' Newtonian Casino revealed). Of course, this also assumes knowledge of inertia and friction of the wheel, weight, smoothness and roundness of the ball, variations in hand speed during the turning and so forth. A probabilistic description can thus be more useful than Newtonian mechanics for analyzing the pattern of outcomes of repeated rolls of a roulette wheel. Physicists face the same situation in kinetic theory of gases, where the system, while deterministic "in principle", is so complex (with the number of molecules typically the order of magnitude of Avogadro constant 6.02·1023) that only a statistical description of its properties is feasible.
Probability theory is required to describe quantum phenomena. A revolutionary discovery of early 20th century physics was the random character of all physical processes that occur at sub-atomic scales and are governed by the laws of quantum mechanics. The objective wave function evolves deterministically but, according to the Copenhagen interpretation, it deals with probabilities of observing, the outcome being explained by a wave function collapse when an observation is made. However, the loss of determinism for the sake of instrumentalism did not meet with universal approval. Albert Einstein famously in a letter to Max Born: "I am convinced that God does not play dice". Like Einstein, Erwin Schrödinger, who discovered the wave function, believed quantum mechanics is a statistical approximation of an underlying deterministic reality. In modern interpretations, quantum decoherence accounts for subjectively probabilistic behavior.

</doc>
<doc id="429425" url="https://en.wikipedia.org/wiki?curid=429425" title="Probability amplitude">
Probability amplitude

In quantum mechanics, a probability amplitude is a complex number used in describing the behaviour of systems. The modulus squared of this quantity represents a probability or probability density.
Probability amplitudes provide a relationship between the wave function (or, more generally, of a quantum state vector) of a system and the results of observations of that system, a link first proposed by Max Born. Interpretation of values of a wave function as the probability amplitude is a pillar of the Copenhagen interpretation of quantum mechanics. In fact, the properties of the space of wave functions were being used to make physical predictions (such as emissions from atoms being at certain discrete energies) before any physical interpretation of a particular function was offered. Born was awarded half of the 1954 Nobel Prize in Physics for this understanding (see #References), and the probability thus calculated is sometimes called the "Born probability". These probabilistic concepts, namely the probability density and quantum measurements, were vigorously contested at the time by the original physicists working on the theory, such as Schrödinger and Einstein. It is the source of the mysterious consequences and philosophical difficulties in the interpretations of quantum mechanics—topics that continue to be debated even today.
Overview.
Physical.
Neglecting some technical complexities, the problem of quantum measurement is the behaviour of a quantum state, for which the value of the observable to be measured is uncertain. Such state is thought to be a coherent superposition of the observable's "eigenstates", states on which the value of the observable is uniquely defined, for different possible values of the observable.
When a measurement of is made, the system (under the Copenhagen interpretation) "jumps" to one of the eigenstates, returning the eigenvalue to which the state belongs. The superposition of states can give them unequal "weights". Intuitively it is clear that eigenstates with heavier "weights" are more "likely" to be produced. Indeed, which of the above eigenstates the system jumps to is given by a probabilistic law: the probability of the system jumping to the state is proportional to the absolute value of the corresponding numerical factor squared. These numerical factors are called probability amplitudes, and this relationship used to calculate probabilities from given pure quantum states (such as wave functions) is called the Born rule.
Different observables may define incompatible decompositions of states. Observables that do not commute define probability amplitudes on different sets.
Mathematical.
In a formal setup, any system in quantum mechanics is described by a state, which is a vector , residing in an abstract complex vector space, called a Hilbert space. It may be either infinite- or finite-dimensional. A usual presentation of that Hilbert space is a special function space, called , on certain set , that is either some configuration space or a discrete set.
For a measurable function , the condition formula_1 reads:
this integral defines the square of the norm of . If that norm is equal to , then
It actually means that any element of of the norm 1 defines a probability measure on and a non-negative real expression defines its Radon–Nikodym derivative with respect to the standard measure .
If the standard measure on is non-atomic, such as the Lebesgue measure on the real line, or on three-dimensional space, or similar measures on manifolds, then a real-valued function is called a "probability density"; see details below. If the standard measure on consists of atoms only (we shall call such sets "discrete"), and specifies the measure of any equal to , then an integral over is simply a sum and defines the value of the probability measure on the set }, in other words, the probability that the quantum system is in the state . How amplitudes and the vector are related can be understood with the standard basis of , elements of which will be denoted by or (see bra–ket notation for the angle bracket notation). In this basis
specifies the coordinate presentation of an abstract vector .
Mathematically, many presentations of the system's Hilbert space can exist. We shall consider not an arbitrary one, but a one for the observable in question. A convenient configuration space is such that each point produces some unique value of . For discrete it means that all elements of the standard basis are eigenvectors of . In other words, shall be diagonal in that basis. Then formula_5 is the "probability amplitude" for the eigenstate . If it corresponds to a non-degenerate eigenvalue of , then formula_6 gives the probability of the corresponding value of for the initial state .
For non-discrete there may not be such states as in , but the decomposition is in some sense possible; see spectral theory and Spectral theorem for accurate explanation.
Wave functions and probabilities.
If the configuration space is continuous (something like the real line or Euclidean space, see above), then there are no valid quantum states corresponding to particular , and the probability that the system is "in the state " will always be zero. An archetypical example of this is the space constructed with 1-dimensional Lebesgue measure; it is used to study a motion in one dimension. This presentation of the infinite-dimensional Hilbert space corresponds to the spectral decomposition of the coordinate operator: in this example. Although there are no such vectors as , strictly speaking, the expression can be made meaningful, for instance, with spectral theory.
Generally, it is the case when the motion of a particle is described in the position space, where the corresponding probability amplitude function is the wave function.
If the function represents the quantum state vector , then the real expression , that depends on , forms a probability density function of the given state. The difference of a "density function" from simply a numerical probability means that one should integrate this modulus-squared function over some (small) domains in to obtain probability values – as was stated above, the system can't be in some state with a positive probability. It gives to both amplitude and density function a physical dimension, unlike a dimensionless probability. For example, for a 3-dimensional wave function the amplitude has a "bizarre" dimension [L−3/2].
Note that for both continuous and infinite discrete cases not "every" measurable, or even smooth function (i.e. a possible wave function) defines an element of ; see #Normalisation below.
Discrete amplitudes.
When the set is discrete (see above), vectors represented with the Hilbert space are just column vectors composed of "amplitudes" and indexed by .
These are sometimes referred to as wave functions of a discrete variable . Discrete dynamical variables are used in such problems as a particle in an idealized reflective box and quantum harmonic oscillator. Components of the vector will be denoted by for uniformity with the previous case; there may be either finite of infinite number of components depending on the Hilbert space.
In this case, if the vector has the norm 1, then is just the probability that the quantum system resides in the state . It defines a discrete probability distribution on .
 if and only if is the same quantum state as . if and only if and are orthogonal (see inner product space). Otherwise the modulus of is between 0 and 1.
A discrete probability amplitude may be considered as a fundamental frequency in the Probability Frequency domain (spherical harmonics) for the purposes of simplifying M-theory transformation calculations.
A basic example.
Take the simplest meaningful example of the discrete case: a quantum system that can be in two possible states: for example, the polarization of a photon. When the polarization is measured, it could be the horizontal state , or the vertical state . Until its polarization is measured the photon can be in a superposition of both these states, so its state could be written as:
The probability amplitudes of for the states and are and respectively. When the photon's polarization is measured, the resulting state is either horizontal or vertical. But in a random experiment, the probability of being horizontally polarized is , and the probability of being vertically polarized is .
Therefore, a photon in a state formula_8 whose polarization was measured. It would have a probability of 1/3 to come out horizontally polarized, and a probability of 2/3 to come out vertically polarized, on measurement, when an ensemble of measurements are made. The order of such results, is, however, completely random.
Normalisation.
In the example above, the measurement must give either or , so the total probability of measuring or must be 1. This leads to a constraint that ; more generally the sum of the squared moduli of the probability amplitudes of all the possible states is equal to one. If to understand "all the possible states" as an orthonormal basis, that makes sense in the discrete case, then this condition is the same as the norm-1 condition explained above.
One can always divide any non-zero element of a Hilbert space by its norm and obtain a "normalized" state vector. Not every wave function belongs to the Hilbert space , though. Wave functions that fulfill this constraint are called normalizable.
The Schrödinger wave equation, describing states of quantum particles, has solutions that describe a system and determine precisely how the state changes with time. Suppose a wavefunction is a solution of the wave equation, giving a description of the particle (position , for time ). If the wavefunction is square integrable, "i.e."
for some , then is called the normalized wavefunction. Under the standard Copenhagen interpretation, the normalized wavefunction gives probability amplitudes for the position of the particle. Hence, at a given time , is the probability density function of the particle's position. Thus the probability that the particle is in the volume at is
Note that if any solution to the wave equation is normalisable at some time , then the defined above is always normalised, so that
is always a probability density function for all . This is key to understanding the importance of this interpretation, because for a given the particle's constant mass, initial and the potential, the Schrödinger equation fully determines subsequent wavefunction, and the above then gives probabilities of locations of the particle at all subsequent times.
The laws of calculating probabilities of events.
A. Provided a system evolves naturally (which under the Copenhagen interpretation means that the system is not subjected to measurement), the following laws apply:
Law 2 is analogous to the addition law of probability, only the probability being substituted by the probability amplitude. Similarly, Law 4 is analogous to the multiplication law of probability for independent events; note that it fails for entangled states.
B. When an experiment is performed to decide between the several alternatives, the same laws hold true for the corresponding probabilities: formula_16.
Provided one knows the probability amplitudes for events associated with an experiment, the above laws provide a complete description of quantum systems in terms of probabilities.
The above laws give way to the path integral formulation of quantum mechanics, in the formalism developed by the celebrated theoretical physicist Richard Feynman. This approach to quantum mechanics forms the stepping-stone to the path integral approach to quantum field theory.
In the context of the double-slit experiment.
Probability amplitudes have special significance because they act in quantum mechanics as the equivalent of conventional probabilities, with many analogous laws, as described above. For example, in the classic double-slit experiment, electrons are fired randomly at two slits, and the probability distribution of detecting electrons at all parts on a large screen placed behind the slits, is questioned. An intuitive answer is that , where is the probability of that event. This is obvious if one assumes that an electron passes through either slit. When nature does not have a way to distinguish which slit the electron has gone though (a much more stringent condition than simply "it is not observed"), the observed probability distribution on the screen reflects the interference pattern that is common with light waves. If one assumes the above law to be true, then this pattern cannot be explained. The particles cannot be said to go through either slit and the simple explanation does not work. The correct explanation is, however, by the association of probability amplitudes to each event. This is an example of the case A as described in the previous article. The complex amplitudes which represent the electron passing each slit ( and ) follow the law of precisely the form expected: . This is the principle of quantum superposition. The probability, which is the modulus squared of the probability amplitude, then, follows the interference pattern under the requirement that amplitudes are complex: formula_17. Here, formula_18 and formula_19 are the arguments of and respectively. A purely real formulation has too few dimensions to describe the system's state when superposition is taken into account. That is, without the arguments of the amplitudes, we cannot describe the phase-dependent interference. The crucial term formula_20 is called the "interference term", and this would be missing if we had added the probabilities.
However, one may choose to devise an experiment in which he observes which slit each electron goes through. Then case B of the above article applies, and the interference pattern is not observed on the screen.
One may go further in devising an experiment in which he gets rid of this "which-path information" by a "quantum eraser". Then, according to the Copenhagen interpretation, the case A applies again and the interference pattern is restored.
Conservation of probabilities and the Continuity equation.
Intuitively, since a normalised wave function stays normalised while evolving according to the wave equation, there will be a relationship between the change in the probability density of the particle's position and the change in the amplitude at these positions.
Define the probability current (or flux) as
measured in units of (probability)/(area × time).
Then the current satisfies the equation
The probability density is formula_23, this equation is exactly the continuity equation, appearing in many situations in physics where we need to describe the local conservation of quantities. The best example is in classical electrodynamics, where corresponds to current density corresponding to electric charge, and the density is the charge-density. The corresponding continuity equation describes the local conservation of charges.
Composite systems.
For two quantum systems with spaces and and given states and respectively, their combined state can be expressed as a function on , that gives the
product of respective probability measures. In other words, amplitudes of a non-entangled composite state are products of original amplitudes, and respective observables on the systems 1 and 2 behave on these states as independent random variables. This strengthens the probabilistic interpretation explicated above.
Amplitudes in operators.
The concept of amplitudes described above is relevant to quantum state vectors. It is also used in the context of unitary operators that are important in the scattering theory, notably in the form of S-matrices. Whereas moduli of vector components squared, for a given vector, give a fixed probability distribution, moduli of matrix elements squared are interpreted as transition probabilities just as in a random process. Like a finite-dimensional unit vector specifies a finite probability distribution, a finite-dimensional unitary matrix specifies transition probabilities between a finite number of states. Note that columns of a unitary matrix, as vectors, have the norm 1.
The "transitional" interpretation may be applied to s on non-discrete spaces as well.

</doc>
<doc id="43487" url="https://en.wikipedia.org/wiki?curid=43487" title="Probability density function">
Probability density function

In probability theory, a probability density function (PDF), or density of a continuous random variable, is a function that describes the relative likelihood for this random variable to take on a given value. The probability of the random variable falling within a particular range of values is given by the integral of this variable’s density over that range—that is, it is given by the area under the density function but above the horizontal axis and between the lowest and greatest values of the range. The probability density function is nonnegative everywhere, and its integral over the entire space is equal to one.
The terms ""probability distribution function" and "probability function"" have also sometimes been used to denote the probability density function. However, this use is not standard among probabilists and statisticians. In other sources, "probability distribution function" may be used when the probability distribution is defined as a function over general sets of values, or it may refer to the cumulative distribution function, or it may be a probability mass function rather than the density. Further confusion of terminology exists because "density function" has also been used for what is here called the "probability mass function".
Example.
Suppose a species of bacteria typically lives 4 to 6 hours. What is the probability that a bacterium lives "exactly" 5 hours? The answer is actually 0%. A lot of bacteria live for "approximately" 5 hours, but there is a negligible chance that any given bacterium dies at "exactly" 5.0000000000... hours.
Instead we might ask: What is the probability that the bacterium dies between 5 hours and 5.01 hours? Let's say the answer is 0.02 (i.e., 2%). Next: What is the probability that the bacterium dies between 5 hours and 5.001 hours? The answer is probably around 0.002, since this is 1/10th of the previous interval. The probability that the bacterium dies between 5 hours and 5.0001 hours is probably about 0.0002, and so on. 
In these three examples, the ratio (probability of dying during an interval) / (duration of the interval) is approximately constant, and equal to 2 per hour (or 2 hour−1). For example, there is 0.02 probability of dying in the 0.01-hour interval between 5 and 5.01 hours, and (0.02 probability / 0.01 hours) = 2 hour−1. This quantity 2 hour−1 is called the "probability density" for dying at around 5 hours.
Therefore, in response to the question "What is the probability that the bacterium dies at 5 hours?", a literally correct but unhelpful answer is "0", but a better answer can be written as (2 hour−1) "dt". This is the probability that the bacterium dies within a small (infinitesimal) window of time around 5 hours, where "dt" is the duration of this window. 
For example, the probability that it lives longer than 5 hours, but shorter than (5 hours + 1 nanosecond), is (2 hour−1)×(1 nanosecond) ≃ 6×10−13 (using the unit conversion 3.6×1012 nanoseconds = 1 hour).
There is a "probability density function" "f" with "f"(5 hours) = 2 hour−1. The integral of "f" over any window of time (not only infinitesimal windows but also large windows) is the probability that the bacterium dies in that window.
Absolutely continuous univariate distributions.
A probability density function is most commonly associated with absolutely continuous univariate distributions. A random variable "X" has density "fX", where "fX" is a non-negative Lebesgue-integrable function, if:
Hence, if "FX" is the cumulative distribution function of "X", then:
and (if "fX" is continuous at "x")
Intuitively, one can think of "fX"("x") d"x" as being the probability of "X" falling within the infinitesimal interval ["x", "x" + d"x"].
Formal definition.
A random variable "X" with values in a measurable space formula_4 
(usually Rn with the Borel sets as measurable subsets) has as probability distribution the measure "X"∗"P" on formula_4: the density of "X" with respect to a reference measure "μ" on formula_4 is the Radon–Nikodym derivative:
That is, "f" is any measurable function with the property that:
for any measurable set formula_9.
Discussion.
In the continuous univariate case above, the reference measure is the Lebesgue measure. The probability mass function of a discrete random variable is the density with respect to the counting measure over the sample space (usually the set of integers, or some subset thereof).
Note that it is not possible to define a density with reference to an arbitrary measure (e.g. one can't choose the counting measure as a reference for a continuous random variable). Furthermore, when it does exist, the density is almost everywhere unique.
Further details.
Unlike a probability, a probability density function can take on values greater than one; for example, the uniform distribution on the interval [0, ½] has probability density "f"("x") = 2 for 0 ≤ "x" ≤ ½ and "f"("x") = 0 elsewhere. 
The standard normal distribution has probability density
If a random variable "X" is given and its distribution admits a probability density function "f", then the expected value of "X" (if the expected value exists) can be calculated as
Not every probability distribution has a density function: the distributions of discrete random variables do not; nor does the Cantor distribution, even though it has no discrete component, i.e., does not assign positive probability to any individual point.
A distribution has a density function if and only if its cumulative distribution function "F"("x") is absolutely continuous. In this case: "F" is almost everywhere differentiable, and its derivative can be used as probability density:
If a probability distribution admits a density, then the probability of every one-point set {"a"} is zero; the same holds for finite and countable sets. 
Two probability densities "f" and "g" represent the same probability distribution precisely if they differ only on a set of Lebesgue measure zero.
In the field of statistical physics, a non-formal reformulation of the relation above between the derivative of the cumulative distribution function and the probability density function is generally used as the definition of the probability density function. This alternate definition is the following:
If "dt" is an infinitely small number, the probability that "X" is included within the interval ("t", "t" + "dt") is equal to "f"("t") "dt", or:
Link between discrete and continuous distributions.
It is possible to represent certain discrete random variables as well as random variables involving both a continuous and a discrete part with a generalized probability density function, by using the Dirac delta function. For example, let us consider a binary discrete random variable having the Rademacher distribution—that is, taking −1 or 1 for values, with probability ½ each. The density of probability associated with this variable is: 
More generally, if a discrete variable can take "n" different values among real numbers, then the associated probability density function is: 
where "x"1, …, "xn" are the discrete values accessible to the variable and "p"1, …, "pn" are the probabilities associated with these values.
This substantially unifies the treatment of discrete and continuous probability distributions. For instance, the above expression allows for determining statistical characteristics of such a discrete variable (such as its mean, its variance and its kurtosis), starting from the formulas given for a continuous distribution of the probability.
Families of densities.
It is common for probability density functions (and probability mass functions) to 
be parametrized—that is, to be characterized by unspecified parameters. For example, the normal distribution is parametrized in terms of the mean and the variance, denoted by formula_16 and formula_17 respectively, giving the family of densities
It is important to keep in mind the difference between the domain of a family of densities and the parameters of the family. Different values of the parameters describe different distributions of different random variables on the same sample space (the same set of all possible values of the variable); this sample space is the domain of the family of random variables that this family of distributions describes. A given set of parameters describes a single distribution within the family sharing the functional form of the density. From the perspective of a given distribution, the parameters are constants, and terms in a density function that contain only parameters, but not variables, are part of the normalization factor of a distribution (the multiplicative factor that ensures that the area under the density—the probability of "something" in the domain occurring— equals 1). This normalization factor is outside the kernel of the distribution. 
Since the parameters are constants, reparametrizing a density in terms of different parameters, to give a characterization of a different random variable in the family, means simply substituting the new parameter values into the formula in place of the old ones. Changing the domain of a probability density, however, is trickier and requires more work: see the section below on change of variables.
Densities associated with multiple variables.
For continuous random variables "X"1, …, "Xn", it is also possible to define a probability density function associated to the set as a whole, often called joint probability density function. This density function is defined as a function of the "n" variables, such that, for any domain "D" in the "n"-dimensional space of the values of the variables "X"1, …, "Xn", the probability that a realisation of the set variables falls inside the domain "D" is
If "F"("x"1, …, "x""n") = Pr("X"1 ≤ "x"1, …, "X""n" ≤ "x""n") is the cumulative distribution function of the vector ("X"1, …, "X""n"), then the joint probability density function can be computed as a partial derivative
Marginal densities.
For "i"=1, 2, …,"n", let "fXi"("xi") be the probability density function associated with variable "Xi" alone. This is called the “marginal” density function, and can be deduced from the probability density associated with the random variables "X"1, …, "Xn" by integrating on all values of the "n" − 1 other variables:
Independence.
Continuous random variables "X"1, …, "Xn" admitting a joint density are all independent from each other if and only if
Corollary.
If the joint probability density function of a vector of "n" random variables can be factored into a product of "n" functions of one variable
(where each "fi" is not necessarily a density) then the "n" variables in the set are all independent from each other, and the marginal probability density function of each of them is given by
Example.
This elementary example illustrates the above definition of multidimensional probability density functions in the simple case of a function of a set of two variables. Let us call formula_25 a 2-dimensional random vector of coordinates ("X", "Y"): the probability to obtain formula_25 in the quarter plane of positive "x" and "y" is
Dependent variables and change of variables.
If the probability density function of a random variable "X" is given as "fX"("x"), it is possible (but often not necessary; see below) to calculate the probability density function of some variable . This is also called a “change of variable” and is in practice used to generate a random variable of arbitrary shape using a known (for instance uniform) random number generator. 
If the function "g" is monotonic, then the resulting density function is
Here "g"−1 denotes the inverse function.
This follows from the fact that the probability contained in a differential area must be invariant under change of variables. That is,
or
For functions which are not monotonic the probability density function for "y" is
where "n"("y") is the number of solutions in "x" for the equation , and "g"−1"k"("y") are these solutions.
It is tempting to think that in order to find the expected value "E"("g"("X")) one must first find the probability density "f""g"("X") of the new random variable . However, rather than computing
one may find instead
The values of the two integrals are the same in all cases in which both "X" and "g"("X") actually have probability density functions. It is not necessary that "g" be a one-to-one function. In some cases the latter integral is computed much more easily than the former. See Law of the unconscious statistician.
Multiple variables.
The above formulas can be generalized to variables (which we will again call "y") depending on more than one other variable. "f"("x"1, …, "x""n") shall denote the probability density function of the variables that "y" depends on, and the dependence shall be . Then, the resulting density function is
where the integral is over the entire ("n"-1)-dimensional solution of the subscripted equation and the symbolic "dV" must be replaced by a parametrization of this solution for a particular calculation; the variables "x"1, …, "x""n" are then of course functions of this parametrization.
This derives from the following, perhaps more intuitive representation: Suppose x is an "n"-dimensional random variable with joint density "f". If , where "H" is a bijective, differentiable function, then y has density "g":
with the differential regarded as the Jacobian of the inverse of "H", evaluated at y.
Using the delta-function (and assuming independence) the same result is formulated as follows.
If the probability density function of independent random variables "Xi", are given as "fXi"("xi"), it is possible to calculate the probability density function of some variable . The following formula establishes a connection between the probability density function of "Y" denoted by "fY"("y") and "fXi"("xi") using the Dirac delta function:
Sums of independent random variables.
The probability density function of the sum of two independent random variables "U" and "V", each of which has a probability density function, is the convolution of their separate density functions:
It is possible to generalize the previous relation to a sum of N independent random variables, with densities "U"1, …, "UN":
This can be derived from a two-way change of variables involving "Y=U+V" and "Z=V", similarly to the example below for the quotient of independent random variables.
Products and quotients of independent random variables.
Given two independent random variables "U" and "V", each of which has a probability density function, the density of the product "Y"="UV" and quotient "Y"="U"/"V" can be computed by a change of variables.
Example: Quotient distribution.
To compute the quotient "Y"="U"/"V" of two independent random variables "U" and "V", define the following transformation:
Then, the joint density "p(Y,Z)" can be computed by a change of variables from "U,V" to "Y,Z", and "Y" can be derived by marginalizing out "Z" from the joint density.
The inverse transformation is
The Jacobian matrix formula_43 of this transformation is
Thus:
And the distribution of "Y" can be computed by marginalizing out "Z":
Note that this method crucially requires that the transformation from "U,V" to "Y,Z" be bijective. The above transformation meets this because "Z" can be mapped directly back to "V", and for a given "V" the quotient "U/V" is monotonic. This is similarly the case for the sum "U+V", difference "U-V" and product "UV".
Exactly the same method can be used to compute the distribution of other functions of multiple independent random variables.
Example: Quotient of two standard normals.
Given two standard normal variables "U" and "V", the quotient can be computed as follows. First, the variables have the following density functions:
We transform as described above:
This leads to:
This is a standard Cauchy distribution.

</doc>
<doc id="23543" url="https://en.wikipedia.org/wiki?curid=23543" title="Probability distribution">
Probability distribution

In probability and statistics, a probability distribution assigns a probability to each measurable subset of the possible outcomes of a random experiment, survey, or procedure of statistical inference. Examples are found in experiments whose sample space is non-numerical, where the distribution would be a categorical distribution; experiments whose sample space is encoded by discrete random variables, where the distribution can be specified by a probability mass function; and experiments with sample spaces encoded by continuous random variables, where the distribution can be specified by a probability density function. More complex experiments, such as those involving stochastic processes defined in continuous time, may demand the use of more general probability measures.
In applied probability, a probability distribution can be specified in a number of different ways, often chosen for mathematical convenience:
A probability distribution can either be univariate or multivariate. A univariate distribution gives the probabilities of a single random variable taking on various alternative values; a multivariate distribution (a joint probability distribution) gives the probabilities of a random vector—a set of two or more random variables—taking on various combinations of values. Important and commonly encountered univariate probability distributions include the binomial distribution, the hypergeometric distribution, and the normal distribution. The multivariate normal distribution is a commonly encountered multivariate distribution.
Introduction.
To define probability distributions for the simplest cases, one needs to distinguish between discrete and continuous random variables. In the discrete case, one can easily assign a probability to each possible value: for example, when throwing a fair , each of the six values "1" to "6" has the probability 1/6. In contrast, when a random variable takes values from a continuum then, typically, probabilities can be nonzero only if they refer to intervals: in quality control one might demand that the probability of a "500 g" package containing between 490 g and 510 g should be no less than 98%.
If the random variable is real-valued (or more generally, if a total order is defined for its possible values), the cumulative distribution function (CDF) gives the probability that the random variable is no larger than a given value; in the real-valued case, the CDF is the integral of the probability density function (pdf) provided that this function exists.
Terminology.
As probability theory is used in quite diverse applications, terminology is not uniform and sometimes confusing. The following terms are used for non-cumulative probability distribution functions:
The following terms are somewhat ambiguous as they can refer to non-cumulative or cumulative distributions, depending on authors' preferences:
Finally,
Cumulative distribution function.
Because a probability distribution Pr on the real line is determined by the probability of a scalar random variable "X" being in a half-open interval (-∞, "x"], the probability distribution is completely characterized by its cumulative distribution function:
Discrete probability distribution.
A discrete probability distribution is a "probability distribution" characterized by a probability mass function. Thus, the distribution of a random variable "X" is discrete, and "X" is called a discrete random variable, if
as "u" runs through the set of all possible values of "X". Hence, a random variable can assume only a finite or countably infinite number of values—the random variable is a discrete variable. For the number of potential values to be countably infinite, even though their probabilities sum to 1, the probabilities have to decline to zero fast enough. for example, if formula_3 for "n" = 1, 2, ..., we have the sum of probabilities 1/2 + 1/4 + 1/8 + ... = 1.
Well-known discrete probability distributions used in statistical modeling include the Poisson distribution, the Bernoulli distribution, the binomial distribution, the geometric distribution, and the negative binomial distribution. Additionally, the discrete uniform distribution is commonly used in computer programs that make equal-probability random selections between a number of choices.
Measure theoretic formulation.
A measurable function formula_4 between a probability space formula_5 and a measurable space formula_6 is
called a discrete random variable provided its image is a countable set and the pre-image of singleton sets are measurable, i.e., formula_7 for all formula_8.
The latter requirement induces a probability mass function formula_9 via formula_10. Since the pre-images of disjoint sets
are disjoint
This recovers the definition given above.
Cumulative density.
Equivalently to the above, a discrete random variable can be defined as a random variable whose cumulative distribution function (cdf) increases only by jump discontinuities—that is, its cdf increases only where it "jumps" to a higher value, and is constant between those jumps. The points where jumps occur are precisely the values which the random variable may take.
Delta-function representation.
Consequently, a discrete probability distribution is often represented as a generalized probability density function involving Dirac delta functions, which substantially unifies the treatment of continuous and discrete distributions. This is especially useful when dealing with probability distributions involving both a continuous and a discrete part.
Indicator-function representation.
For a discrete random variable "X", let "u"0, "u"1, ... be the values it can take with non-zero probability. Denote
These are disjoint sets, and by formula (1)
It follows that the probability that "X" takes any value except for "u"0, "u"1, ... is zero, and thus one can write "X" as
except on a set of probability zero, where formula_15 is the indicator function of "A". This may serve as an alternative definition of discrete random variables.
Continuous probability distribution.
A continuous probability distribution is a "probability distribution" that has a probability density function. Mathematicians also call such a distribution absolutely continuous, since its cumulative distribution function is absolutely continuous with respect to the Lebesgue measure "λ". If the distribution of "X" is continuous, then "X" is called a continuous random variable. There are many examples of continuous probability distributions: normal, uniform, chi-squared, and others.
Intuitively, a continuous random variable is the one which can take a continuous range of values—as opposed to a discrete distribution, where the set of possible values for the random variable is at most countable. While for a discrete distribution an event with probability zero is impossible (e.g., rolling on a standard die is impossible, and has probability zero), this is not so in the case of a continuous random variable. For example, if one measures the width of an oak leaf, the result of 3½ cm is possible; however, it has probability zero because uncountably many other potential values exist even between 3 cm and 4 cm. Each of these individual outcomes has probability zero, yet the probability that the outcome will fall into the interval is nonzero. This apparent paradox is resolved by the fact that the probability that "X" attains some value within an infinite set, such as an interval, cannot be found by naively adding the probabilities for individual values. Formally, each value has an infinitesimally small probability, which statistically is equivalent to zero.
Formally, if "X" is a continuous random variable, then it has a probability density function "ƒ"("x"), and therefore its probability of falling into a given interval, say is given by the integral
In particular, the probability for "X" to take any single value "a" (that is ) is zero, because an integral with coinciding upper and lower limits is always equal to zero.
The definition states that a continuous probability distribution must possess a density, or equivalently, its cumulative distribution function be absolutely continuous. This requirement is stronger than simple continuity of the cumulative distribution function, and there is a special class of distributions, singular distributions, which are neither continuous nor discrete nor a mixture of those. An example is given by the Cantor distribution. Such singular distributions however are never encountered in practice.
Note on terminology: some authors use the term "continuous distribution" to denote the distribution with continuous cumulative distribution function. Thus, their definition includes both the (absolutely) continuous and singular distributions.
By one convention, a probability distribution formula_17 is called "continuous" if its cumulative distribution function formula_18 is continuous and, therefore, the probability measure of singletons formula_19 for all formula_20.
Another convention reserves the term "continuous probability distribution" for absolutely continuous distributions. These distributions can be characterized by a probability density function: a non-negative Lebesgue integrable function formula_21 defined on the real numbers such that
Discrete distributions and some continuous distributions (like the Cantor distribution) do not admit such a density.
Kolmogorov definition.
In the measure-theoretic formalization of probability theory, a random variable is defined as a measurable function "X" from a probability space formula_23 to measurable space formula_24. A probability distribution of "X" is the pushforward measure "X"*P  of "X" , which is a probability measure on formula_24 satisfying "X"*P = P"X" −1.
Random number generation.
A frequent problem in statistical simulations (the Monte Carlo method) is the generation of pseudo-random numbers that are distributed in a given way. Most algorithms are based on a pseudorandom number generator that produces numbers "X" that are uniformly distributed in the interval [0,1). These random variates "X" are then transformed via some algorithm to create a new random variate having the required probability distribution.
Applications.
The concept of the probability distribution and the random variables which they describe underlies the mathematical discipline of probability theory, and the science of statistics. There is spread or variability in almost any value that can be measured in a population (e.g. height of people, durability of a metal, sales growth, traffic flow, etc.); almost all measurements are made with some intrinsic error; in physics many processes are described probabilistically,from the kinetic properties of gases to the quantum mechanical description of fundamental particles. For these and many other reasons, simple numbers are often inadequate for describing a quantity, while probability distributions are often more appropriate.
As a more specific example of an application, the cache language models and other statistical language models used in natural language processing to assign probabilities to the occurrence of particular words and word sequences do so by means of probability distributions.
Common probability distributions.
The following is a list of some of the most common probability distributions, grouped by the type of process that they are related to. For a more complete list, see list of probability distributions, which groups by the nature of the outcome being considered (discrete, continuous, multivariate, etc.)
Note also that all of the univariate distributions below are singly peaked; that is, it is assumed that the values cluster around a single point. In practice, actually observed quantities may cluster around multiple values. Such quantities can be modeled using a mixture distribution.

</doc>
<doc id="217133" url="https://en.wikipedia.org/wiki?curid=217133" title="Probability vector">
Probability vector

In mathematics and statistics, a probability vector or stochastic vector is a vector with non-negative entries that add up to one.
The positions (indices) of a probability vector represent the possible outcomes of a discrete random variable, and the vector gives us the probability mass function of that random variable, which is the standard way of characterizing a discrete probability distribution.
Examples.
Here are some examples of probability vectors. The vectors can be either columns or rows.
formula_1
Geometric interpretation.
Writing out the vector components of a vector formula_2 as
the vector components must sum to one:
Each individual component must have a probability between zero and one:
for all formula_6. These two requirements show that stochastic vectors have a geometric interpretation: A stochastic vector is a point on the "far face" of a standard orthogonal simplex. That is, a stochastic vector uniquely identifies a point on the face opposite of the orthogonal corner of the standard simplex.

</doc>
<doc id="348860" url="https://en.wikipedia.org/wiki?curid=348860" title="Projective representation">
Projective representation

In the field of representation theory in mathematics, a projective representation of a group "G" on a vector space "V" over a field "F" is a group homomorphism from "G" to the projective linear group
where GL("V", "F") is the general linear group of invertible linear transformations of "V" over "F" and "F"∗ is the normal subgroup consisting of multiplications of vectors in "V" by nonzero elements of "F" (that is, scalar multiples of the identity; scalar transformations).
Linear representations and projective representations.
One way in which a projective representation can arise is by taking a linear group representation of on and applying the quotient map
which is the quotient by the subgroup of scalar transformations (diagonal matrices with all diagonal entries equal). The interest for algebra is in the process in the other direction: given a "projective representation", try to 'lift' it to a conventional "linear representation".
In general, given a projective representation it cannot be lifted to a linear representation , and the obstruction to this lifting can be understood via group homology, as described below. However, one "can" lift a projective representation of to a linear representation of a different group , which will be a central extension of . To understand this, note that is a central extension of , meaning that the kernel is central (in fact, is exactly the center of ). One can pull back the projective representation along the quotient map, obtaining a "linear" representation and will be a central extension of because it is a pullback of a central extension. Thus projective representations of can be understood in terms of linear representations of (certain) central extensions of . Notably, for a perfect group there is a single universal perfect central extension of that can be used.
Group cohomology.
The analysis of the lifting question involves group cohomology. Indeed, if one fixes for each in a lifted element in lifting from back to , the lifts then satisfy
for some scalar in . It follows that the 2-cocycle or Schur multiplier satisfies the cocycle equation
for all in . This depends on the choice of the lift ; a different choice of lift will result in a different cocycle
cohomologous to . Thus defines a unique class in . This class might not be trivial. For example, in the case of the symmetric group and alternating group, Schur established that there is exactly one non-trivial class of Schur multiplier, and completely determined all the corresponding irreducible representations.
In general, a nontrivial class leads to an extension problem for . If is correctly extended we obtain a linear representation of the extended group, which induces the original projective representation when pushed back down to . The solution is always a central extension. From Schur's lemma, it follows that the irreducible representations of central extensions of , and the irreducible projective representations of , are essentially the same objects.
Projective representations of Lie groups.
Studying projective representations of Lie groups leads one to consider true representations of their central extensions (see Group extension#Lie groups). In many cases of interest it suffices to consider representations of covering groups; for a connected Lie group "G", this amounts to studying the representations of the Lie algebra of "G". Notable cases of covering groups giving interesting projective representations:

</doc>
<doc id="494418" url="https://en.wikipedia.org/wiki?curid=494418" title="Proper time">
Proper time

In relativity, proper time along a timelike (or lightlike) world line is defined as the time as measured by a clock following that line. It is thus independent of coordinates, and a Lorentz scalar. The proper time interval between two events on a world line is the change in proper time. This is the quantity of interest, since proper time itself is fixed only up to an arbitrary additive constant, namely the setting of the clock at some event along the world line. The proper time between two events depends not only on the events but also the world line connecting them, and hence on the motion of the clock between the events. It is expressed as an integral over the world line. An accelerated clock will measure a smaller elapsed time between two events than that measured by a non-accelerated (inertial) clock between the same two events. The twin paradox is an example of this effect.
In terms of four-dimensional spacetime, proper time is analogous to arc length in three-dimensional (Euclidean) space. By convention, proper time is usually represented by the Greek letter "τ" (tau) to distinguish it from coordinate time represented by "t".
By contrast, coordinate time is the time between two events as measured by an observer using that observer's own method of assigning a time to an event. In the special case of an inertial observer in special relativity, the time is measured using the observer's clock and the observer's definition of simultaneity.
The concept of proper time was introduced by Hermann Minkowski in 1908, and is a feature of Minkowski diagrams.
Mathematical formalism.
The formal definition of proper time involves describing the path through spacetime that represents a clock, observer, or test particle, and the metric structure of that spacetime. Proper time is the pseudo-Riemannian arc length of world lines in four-dimensional spacetime.
From the mathematical point of view, coordinate time is assumed to be predefined and we require an expression for proper time as a function of coordinate time. From the experimental point of view, proper time is what is measured experimentally and then coordinate time is calculated from the proper time of some inertial clocks.
In special relativity.
Let the Minkowski metric be defined by
and define
for arbitrary Lorentz frames.
Consider an infinitesimal interval
expressed in any Lorentz frame and here assumed timelike (or lightlike), separating points on a trajectory of a particle (think clock). The same interval can be expressed in coordinates such that at each moment, the particle is "at rest". Such a frame is called an instantaneous rest frame. Due to the invariance of the interval (instantaneous rest frames taken at different times are related by Lorentz transformations) one may write
since in the instantaneous rest frame, the particle is at rest. Since the interval is assumed timelike (lightlike), one may take the square root of the above expression;
or
Given this differential expression for , the proper time interval is defined as
Here is the worldline from some initial event to some final event with the ordering of the events fixed by the requirement that the final event occurs later according to the clock (for a lightlike world line, if allowed, this differs, the result is zero according to definition since in this case ) than the initial event.
Using and again the invariance of the interval, one may write
&= \int \sqrt {1 - \frac{1}{c^2} \left [ \left (\frac{dx}{dt}\right)^2 + \left (\frac{dy}{dt}\right)^2 + \left ( \frac{dz}{dt}\right)^2 \right] }dt\\
&= \int \sqrt {1 - \frac{v(t)^2}{c^2}} dt 
= \int \frac{dt}{\gamma(t)},\end{align}</math>          
where is the coordinate speed at coordinate time , and , , and are space coordinates. It should be noted that the first expression is "manifestly" Lorentz invariant. They are all Lorentz invariant, since proper time and proper time intervals are coordinate-independent by definition.
If , are parameterised by a parameter , this can be written as
If the motion is of the particle is constant, the expression simplifies to
where Δ means the change in coordinates between the initial and final events. The definition in special relativity generalizes straightforwardly to general relativity as follows below.
In general relativity.
Proper time is defined in general relativity as follows: Given a pseudo-Riemannian manifold with a local coordinates and equipped with a metric tensor , the proper time interval between two events along a timelike path is given by the line integral
This expression is, as it should be, invariant under coordinate changes. It reduces (in appropriate coordinates) to the expression of special relativity in flat spacetime.
In the same way that coordinates can be chosen such that in special relativity, this can be done in general relativity too. Then, in these coordinates,
This expression generalizes definition and can be taken as the definition. Then using invariance of the interval, equation follows from it in the same way follows from , except that here arbitrary coordinate changes are allowed.
Examples in special relativity.
Example 1: The twin "paradox".
For a twin "paradox" scenario, let there be an observer "A" who moves between the coordinates (0,0,0,0) and (10 years, 0, 0, 0) inertially. This means that "A" stays at formula_9 for 10 years of coordinate time. The proper time interval for "A" is then
So we find that being "at rest" in a special relativity coordinate system means that proper time and coordinate time are the same.
Let there now be another observer "B" who travels in the "x" direction from (0,0,0,0) for 5 years of coordinate time at 0.866"c" to (5 years, 4.33 light-years, 0, 0). Once there, "B" accelerates, and travels in the other spatial direction for 5 years to (10 years, 0, 0, 0). For each leg of the trip, the proper time interval is
So the total proper time for observer "B" to go from (0,0,0,0) to (5 years, 4.33 light-years, 0, 0) to (10 years, 0, 0, 0) is 5 years. Thus it is shown that the proper time equation incorporates the time dilation effect. In fact, for an object in a SR spacetime traveling with a velocity of v for a time formula_12, the proper time interval experienced is
which is the SR time dilation formula.
Example 2: The rotating disk.
An observer rotating around another inertial observer is in an accelerated frame of reference. For such an observer, the incremental (formula_14) form of the proper time equation is needed, along with a parameterized description of the path being taken, as shown below.
Let there be an observer "C" on a disk rotating in the "xy" plane at a coordinate angular rate of formula_15 and who is at a distance of "r" from the center of the disk with the center of the disk at "x"="y"="z"=0. The path of observer "C" is given by formula_16, where formula_17 is the current coordinate time. When "r" and formula_15 are constant, formula_19 and formula_20. The incremental proper time formula then becomes
So for an observer rotating at a constant distance of "r" from a given point in spacetime at a constant angular rate of "ω" between coordinate times formula_22 and formula_23, the proper time experienced will be
As "v"="rω" for a rotating observer, this result is as expected given the time dilation formula above, and shows the general application of the integral form of the proper time formula.
Examples in general relativity.
The difference between SR and general relativity (GR) is that in GR one can use any metric which is a solution of the Einstein field equations, not just the Minkowski metric. Because inertial motion in curved spacetimes lacks the simple expression it has in SR, the line integral form of the proper time equation must always be used.
Example 3: The rotating disk (again).
An appropriate coordinate conversion done against the Minkowski metric creates coordinates where an object on a rotating disk stays in the same spatial coordinate position. The new coordinates are
and
The "t" and "z" coordinates remain unchanged. In this new coordinate system, the incremental proper time equation is
With "r", "θ", and "z" being constant over time, this simplifies to
which is the same as in Example 2.
Now let there be an object off of the rotating disk and at inertial rest with respect to the center of the disk and at a distance of "R" from it. This object has a coordinate motion described by "dθ = -ω dt", which describes the inertially at-rest object of counter-rotating in the view of the rotating observer. Now the proper time equation becomes
So for the inertial at-rest observer, coordinate time and proper time are once again found to pass at the same rate, as expected and required for the internal self-consistency of relativity theory.
Example 4: The Schwarzschild solution — time on the Earth.
The Schwarzschild solution has an incremental proper time equation of
where
To demonstrate the use of the proper time relationship, several sub-examples involving the Earth will be used here.
For the Earth, "M" = 5.9742 × 1024 kg, meaning that "m" = 4.4354 × 10−3 m. When standing on the north pole, we can assume formula_31 (meaning that we are neither moving up or down or along the surface of the Earth). In this case, the Schwarzschild solution proper time equation becomes formula_32. Then using the polar radius of the Earth as the radial coordinate (or formula_33 meters), we find that
At the equator, the radius of the Earth is "r" = 6,378,137 meters. In addition, the rotation of the Earth needs to be taken into account. This imparts on an observer an angular velocity of formula_35 of 2"π" divided by the sidereal period of the Earth's rotation, 86162.4 seconds. So formula_36. The proper time equation then produces
This should have been the same as the previous result, but as noted above the Earth is not spherical as assumed by the Schwarzschild solution. Even so, this demonstrates how the proper time equation is used.

</doc>
<doc id="23317" url="https://en.wikipedia.org/wiki?curid=23317" title="Proton">
Proton

The proton is a subatomic particle, symbol or , with a positive electric charge of +1e elementary charge and mass slightly less than that of a neutron. Protons and neutrons, each with mass approximately one atomic mass unit, are collectively referred to as "nucleons". One or more protons are present in the nucleus of every atom. The number of protons in the nucleus is the defining property of an element, and is referred to as the atomic number. Since each element has a unique number of protons, each element has its own unique atomic number. The word "proton" is Greek for "first", and this name was given to the hydrogen nucleus by Ernest Rutherford in 1920. In previous years Rutherford had discovered that the hydrogen nucleus (known to be the lightest nucleus) could be extracted from the nuclei of nitrogen by collision. The proton was therefore a candidate to be a fundamental particle and a building block of nitrogen and all other heavier atomic nuclei.
In the modern Standard Model of particle physics, the proton is a hadron, and like the neutron, the other nucleon (particle present in atomic nuclei), is composed of three quarks. Although the proton was originally considered a fundamental or elementary particle, it is now known to be composed of three valence quarks: two up quarks and one down quark. The rest masses of the quarks contribute only about 1% of the proton's mass, however. The remainder of the proton mass is due to the kinetic energy of the quarks and to the energy of the gluon fields that bind the quarks together. Because the proton is not a fundamental particle, it possesses a physical size; the radius of the proton is about 0.84–0.87 fm.
At sufficiently low temperatures, free protons will bind to electrons. However, the character of such bound protons does not change, and they remain protons. A fast proton moving through matter will slow by interactions with electrons and nuclei, until it is captured by the electron cloud of an atom. The result is a protonated atom, which is a chemical compound of hydrogen. In vacuum, when free electrons are present, a sufficiently slow proton may pick up a single free electron, becoming a neutral hydrogen atom, which is chemically a free radical. Such "free hydrogen atoms" tend to react chemically with many other types of atoms at sufficiently low energies. When free hydrogen atoms react with each other, they form neutral hydrogen molecules (H2), which are the most common molecular component of molecular clouds in interstellar space. Such molecules of hydrogen on Earth may then serve (among many other uses) as a convenient source of protons for accelerators (as used in proton therapy) and other hadron particle physics experiments that require protons to accelerate, with the most powerful and noted example being the Large Hadron Collider.
Description.
Protons are spin-½ fermions and are composed of three valence quarks, making them baryons (a sub-type of hadrons). The two up quarks and one down quark of the proton are held together by the strong force, mediated by gluons.A modern perspective has the proton composed of the valence quarks (up, up, down), the gluons, and transitory pairs of sea quarks. The proton has an approximately exponentially decaying positive charge distribution with a mean square radius of about 0.8 fm.
Protons and neutrons are both nucleons, which may be bound together by the nuclear force to form atomic nuclei. The nucleus of the most common isotope of the hydrogen atom (with the chemical symbol "H") is a lone proton. The nuclei of the heavy hydrogen isotopes deuterium and tritium contain one proton bound to one and two neutrons, respectively. All other types of atomic nuclei are composed of two or more protons and various numbers of neutrons.
History.
The concept of a hydrogen-like particle as a constituent of other atoms was developed over a long period. As early as 1815, William Prout proposed that all atoms are composed of hydrogen atoms (which he called "protyles"), based on a simplistic interpretation of early values of atomic weights (see Prout's hypothesis), which was disproved when more accurate values were measured.
In 1886, Eugen Goldstein discovered canal rays (also known as anode rays) and showed that they were positively charged particles (ions) produced from gases. However, since particles from different gases had different values of charge-to-mass ratio (e/m), they could not be identified with a single particle, unlike the negative electrons discovered by J. J. Thomson.
Following the discovery of the atomic nucleus by Ernest Rutherford in 1911, Antonius van den Broek proposed that the place of each element in the periodic table (its atomic number) is equal to its nuclear charge. This was confirmed experimentally by Henry Moseley in 1913 using X-ray spectra.
In 1917 (in experiments reported in 1919), Rutherford proved that the hydrogen nucleus is present in other nuclei, a result usually described as the discovery of the proton. Rutherford had earlier learned to produce hydrogen nuclei as a type of radiation produced as a product of the impact of alpha particles on nitrogen gas, and recognize them by their unique penetration signature in air and their appearance in scintillation detectors. These experiments were begun when Rutherford had noticed that, when alpha particles were shot into air (mostly nitrogen), his scintillation detectors showed the signatures of typical hydrogen nuclei as a product. After experimentation Rutherford traced the reaction to the nitrogen in air, and found that when alphas were produced into pure nitrogen gas, the effect was larger. Rutherford determined that this hydrogen could have come only from the nitrogen, and therefore nitrogen must contain hydrogen nuclei. One hydrogen nucleus was being knocked off by the impact of the alpha particle, producing oxygen-17 in the process. This was the first reported nuclear reaction, 14N + α → 17O + p. (This reaction would later be observed happening directly in a cloud chamber in 1925).
Rutherford knew hydrogen to be the simplest and lightest element and was influenced by Prout's hypothesis that hydrogen was the building block of all elements. Discovery that the hydrogen nucleus is present in all other nuclei as an elementary particle, led Rutherford to give the hydrogen nucleus a special name as a particle, since he suspected that hydrogen, the lightest element, contained only one of these particles. He named this new fundamental building block of the nucleus the "proton," after the neuter singular of the Greek word for "first", πρῶτον. However, Rutherford also had in mind the word "protyle" as used by Prout. Rutherford spoke at the British Association for the Advancement of Science at its Cardiff meeting beginning 24 August 1920. Rutherford was asked by Oliver Lodge for a new name for the positive hydrogen nucleus to avoid confusion with the neutral hydrogen atom. He initially suggested both "proton" and "prouton" (after Prout). Rutherford later reported that the meeting had accepted his suggestion that the hydrogen nucleus be named the "proton", following Prout's word "protyle". The first use of the word "proton" in the scientific literature appeared in 1920.
Stability.
The free proton (a proton not bound to nucleons or electrons) is a stable particle that has not been observed to break down spontaneously to other particles. Free protons are found naturally in a number of situations in which energies or temperatures are high enough to separate them from electrons, for which they have some affinity. Free protons exist in plasmas in which temperatures are too high to allow them to combine with electrons. Free protons of high energy and velocity make up 90% of cosmic rays, which propagate in vacuum for interstellar distances. Free protons are emitted directly from atomic nuclei in some rare types of radioactive decay. Protons also result (along with electrons and antineutrinos) from the radioactive decay of free neutrons, which are unstable.
The spontaneous decay of free protons has never been observed, and the proton is therefore considered a stable particle. However, some grand unified theories of particle physics predict that proton decay should take place with lifetimes of the order of , and experimental searches have established lower bounds on the mean lifetime of the proton for various assumed decay products.
Experiments at the Super-Kamiokande detector in Japan gave lower limits for proton mean lifetime of for decay to an antimuon and a neutral pion, and for decay to a positron and a neutral pion.
Another experiment at the Sudbury Neutrino Observatory in Canada searched for gamma rays resulting from residual nuclei resulting from the decay of a proton from oxygen-16. This experiment was designed to detect decay to any product, and established a lower limit to the proton lifetime of .
However, protons are known to transform into neutrons through the process of electron capture (also called inverse beta decay). For free protons, this process does not occur spontaneously but only when energy is supplied. The equation is:
The process is reversible; neutrons can convert back to protons through beta decay, a common form of radioactive decay. In fact, a free neutron decays this way, with a mean lifetime of about 15 minutes.
Quarks and the mass of the proton.
In quantum chromodynamics, the modern theory of the nuclear force, most of the mass of the proton and the neutron is explained by special relativity. The mass of the proton is about 80–100 times greater than the sum of the rest masses of the quarks that make it up, while the gluons have zero rest mass. The extra energy of the quarks and gluons in a region within a proton, as compared to the rest energy of the quarks alone in the QCD vacuum, accounts for almost 99% of the mass. The rest mass of the proton is, thus, the invariant mass of the system of moving quarks and gluons that make up the particle, and, in such systems, even the energy of massless particles is still measured as part of the rest mass of the system.
Two terms are used in referring to the mass of the quarks that make up protons: "current quark mass" refers to the mass of a quark by itself, while "constituent quark mass" refers to the current quark mass plus the mass of the gluon particle field surrounding the quark. These masses typically have very different values. As noted, most of a proton's mass comes from the gluons that bind the current quarks together, rather than from the quarks themselves. While gluons are inherently massless, they possess energy—to be more specific, quantum chromodynamics binding energy (QCBE)—and it is this that contributes so greatly to the overall mass of the proton (see mass in special relativity). A proton has a mass of approximately 938 MeV/c2, of which the rest mass of its three valence quarks contributes only about 9.4 MeV/c2; much of the remainder can be attributed to the gluons' QCBE.
The internal dynamics of the proton are complicated, because they are determined by the quarks' exchanging gluons, and interacting with various vacuum condensates. Lattice QCD provides a way of calculating the mass of the proton directly from the theory to any accuracy, in principle. The most recent calculations claim that the mass is determined to better than 4% accuracy, even to 1% accuracy (see Figure S5 in Dürr "et al."). These claims are still controversial, because the calculations cannot yet be done with quarks as light as they are in the real world. This means that the predictions are found by a process of extrapolation, which can introduce systematic errors. It is hard to tell whether these errors are controlled properly, because the quantities that are compared to experiment are the masses of the hadrons, which are known in advance.
These recent calculations are performed by massive supercomputers, and, as noted by Boffi and Pasquini: "a detailed description of the nucleon structure is still missing because ... long-distance behavior requires a nonperturbative and/or numerical treatment..."
More conceptual approaches to the structure of the proton are: the topological soliton approach originally due to Tony Skyrme and the more accurate AdS/QCD approach that extends it to include a string theory of gluons, various QCD-inspired models like the bag model and the constituent quark model, which were popular in the 1980s, and the SVZ sum rules, which allow for rough approximate mass calculations. These methods do not have the same accuracy as the more brute-force lattice QCD methods, at least not yet.
Charge radius.
The internationally accepted value of the proton's charge radius is (see orders of magnitude for comparison to other sizes). This value is based on measurements involving a proton and an electron.
However, since 5 July 2010, an international research team has been able to make measurements involving an exotic atom made of a proton and a negatively charged muon. After a long and careful analysis of those measurements, the team concluded that the root-mean-square charge radius of a proton is ", which differs by 5.0 standard deviations from the CODATA value of ". In January 2013, an updated value for the charge radius of a proton——was published. The precision was improved by 1.7 times, but the difference with CODATA value persisted at 7σ significance.
The international research team that obtained this result at the Paul Scherrer Institut (PSI) in Villigen (Switzerland) includes scientists from the Max Planck Institute of Quantum Optics (MPQ) in Garching, the Ludwig-Maximilians-Universität (LMU) Munich and the Institut für Strahlwerkzeuge (IFWS) of the Universität Stuttgart (both from Germany), and the University of Coimbra, Portugal. They are now attempting to explain the discrepancy, and re-examining the results of both previous high-precision measurements and complicated calculations. If no errors are found in the measurements or calculations, it could be necessary to re-examine the world's most precise and best-tested fundamental theory: quantum electrodynamics.
The proton radius remains a puzzle as of early 2015.
Interaction of free protons with ordinary matter.
Although protons have affinity for oppositely charged electrons, free protons must lose sufficient velocity (and kinetic energy) in order to become closely associated and bound to electrons, since this is a relatively low-energy interaction. High energy protons, in traversing ordinary matter, lose energy by collisions with atomic nuclei, and by ionization of atoms (removing electrons) until they are slowed sufficiently to be captured by the electron cloud in a normal atom.
However, in such an association with an electron, the character of the bound proton is not changed, and it remains a proton. The attraction of low-energy free protons to any electrons present in normal matter (such as the electrons in normal atoms) causes free protons to stop and to form a new chemical bond with an atom. Such a bond happens at any sufficiently "cold" temperature (i.e., comparable to temperatures at the surface of the Sun) and with any type of atom. Thus, in interaction with any type of normal (non-plasma) matter, low-velocity free protons are attracted to electrons in any atom or molecule with which they come in contact, causing the proton and molecule to combine. Such molecules are then said to be "protonated", and chemically they often, as a result, become so-called Bronsted acids.
Proton in chemistry.
Atomic number.
In chemistry, the number of protons in the nucleus of an atom is known as the atomic number, which determines the chemical element to which the atom belongs. For example, the atomic number of chlorine is 17; this means that each chlorine atom has 17 protons and that all atoms with 17 protons are chlorine atoms. The chemical properties of each atom are determined by the number of (negatively charged) electrons, which for neutral atoms is equal to the number of (positive) protons so that the total charge is zero. For example, a neutral chlorine atom has 17 protons and 17 electrons, whereas a Cl− anion has 17 protons and 18 electrons for a total charge of −1.
All atoms of a given element are not necessarily identical, however, as the number of neutrons may vary to form different isotopes, and energy levels may differ forming different nuclear isomers. For example, there are two stable isotopes of chlorine: with 35 − 17 = 18 neutrons and with 37 − 17 = 20 neutrons.
Hydrogen ion.
In chemistry, the term proton refers to the hydrogen ion, . Since the atomic number of hydrogen is 1, a hydrogen ion has no electrons and corresponds to a bare nucleus, consisting of a proton (and 0 neutrons for the most abundant isotope "protium" ). The proton is a "bare charge" with only about 1/64,000 of the radius of a hydrogen atom, and so is extremely reactive chemically. The free proton, thus, has an extremely short lifetime in chemical systems such as liquids and it reacts immediately with the electron cloud of any available molecule. In aqueous solution, it forms the hydronium ion, H3O+, which in turn is further solvated by water molecules in clusters such as [H5O2]+ and [H9O4]+.
The transfer of in an acid–base reaction is usually referred to as "proton transfer". The acid is referred to as a proton donor and the base as a proton acceptor. Likewise, biochemical terms such as proton pump and proton channel refer to the movement of hydrated ions.
The ion produced by removing the electron from a deuterium atom is known as a deuteron, not a proton. Likewise, removing an electron from a tritium atom produces a triton.
Proton nuclear magnetic resonance (NMR).
Also in chemistry, the term "proton NMR" refers to the observation of hydrogen-1 nuclei in (mostly organic) molecules by nuclear magnetic resonance. This method uses the spin of the proton, which has the value one-half. The name refers to examination of protons as they occur in protium (hydrogen-1 atoms) in compounds, and does not imply that free protons exist in the compound being studied.
Human exposure.
The Apollo Lunar Surface Experiments Packages (ALSEP) determined that more than 95% of the particles in the solar wind are electrons and protons, in approximately equal numbers.
Protons also occur in from extrasolar origin in space, from galactic cosmic rays, where they make up about 90% of the total particle flux. These protons often have higher energy than solar wind protons, but their intensity is far more uniform and less variable than protons coming from the Sun, the production of which is heavily affected by solar proton events such as coronal mass ejections.
Research has been performed on the dose-rate effects of protons, as typically found in space travel, on human health. To be more specific, there are hopes to identify what specific chromosomes are damaged, and to define the damage, during cancer development from proton exposure. Another study looks into determining "the effects of exposure to proton irradiation on neurochemical and behavioral endpoints, including dopaminergic functioning, amphetamine-induced conditioned taste aversion learning, and spatial learning and memory as measured by the Morris water maze. Electrical charging of a spacecraft due to interplanetary proton bombardment has also been proposed for study. There are many more studies that pertain to space travel, including galactic cosmic rays and their possible health effects, and solar proton event exposure.
The American Biostack and Soviet Biorack space travel experiments have demonstrated the severity of molecular damage induced by heavy ions on micro organisms including Artemia cysts.
Antiproton.
CPT-symmetry puts strong constraints on the relative properties of particles and antiparticles and, therefore, is open to stringent tests. For example, the charges of the proton and antiproton must sum to exactly zero. This equality has been tested to one part in . The equality of their masses has also been tested to better than one part in . By holding antiprotons in a Penning trap, the equality of the charge to mass ratio of the proton and the antiproton has been tested to one part in . The magnetic moment of the antiproton has been measured with error of nuclear Bohr magnetons, and is found to be equal and opposite to that of the proton.

</doc>
<doc id="24138" url="https://en.wikipedia.org/wiki?curid=24138" title="Proton decay">
Proton decay

In particle physics, proton decay is a hypothetical form of radioactive decay in which the proton decays into lighter subatomic particles, such as a neutral pion and a positron. There is currently no experimental evidence that proton decay occurs.
In the Standard Model, protons, a type of baryon, are theoretically stable because baryon number (quark number) is conserved (under normal circumstances; however, see chiral anomaly). Therefore, protons will not decay into other particles on their own, because they are the lightest (and therefore least energetic) baryon.
Some beyond-the-Standard Model grand unified theories (GUTs) explicitly break the baryon number symmetry, allowing protons to decay via the Higgs particle, magnetic monopoles or new X bosons. Proton decay is one of the few unobserved effects of the various proposed GUTs. To date, all attempts to observe these events have failed.
Baryogenesis.
One of the outstanding problems in modern physics is the predominance of matter over antimatter in the universe. The universe, as a whole, seems to have a nonzero positive baryon number density — that is, matter exists. Since it is assumed in cosmology that the particles we see were created using the same physics we measure today, it would normally be expected that the overall baryon number should be zero, as matter and antimatter should have been created in equal amounts. This has led to a number of proposed mechanisms for symmetry breaking that favour the creation of normal matter (as opposed to antimatter) under certain conditions. This imbalance would have been exceptionally small, on the order of 1 in every (1010) particles a small fraction of a second after the Big Bang, but after most of the matter and antimatter annihilated, what was left over was all the baryonic matter in the current universe, along with a much greater number of bosons. Experiments reported in 2010 at Fermilab, however, seem to show that this imbalance is much greater than previously assumed. In an experiment involving a series of particle collisions, the amount of generated matter was approximately 1% larger than the amount of generated antimatter. The reason for this discrepancy is yet unknown.
Most grand unified theories explicitly break the baryon number symmetry, which would account for this discrepancy, typically invoking reactions mediated by very massive X bosons () or massive Higgs bosons (). The rate at which these events occur is governed largely by the mass of the intermediate or particles, so by assuming these reactions are responsible for the majority of the baryon number seen today, a maximum mass can be calculated above which the rate would be too slow to explain the presence of matter today. These estimates predict that a large volume of material will occasionally exhibit a spontaneous proton decay.
Experimental evidence.
Proton decay is one of the few unobserved effects of the various proposed GUTs, another major one being magnetic monopoles. Both became the focus of major experimental physics efforts starting in the early 1980s. Proton decay was, for a time, an extremely exciting area of experimental physics research. To date, all attempts to observe these events have failed. Recent experiments at the Super-Kamiokande water Cherenkov radiation detector in Japan gave lower limits for proton half-life, at 90% confidence level, of years via antimuon decay and years via positron decay. Newer, preliminary results estimate a half-life of no less than years via positron decay.
A 2014 result with 260kT·yr of data, searching for decay to K-mesons set a lower limit of 5.9 × 1033 yr, close to a supersymmetry (SUSY) prediction of near 1034 yr.
Theoretical motivation.
Despite the lack of observational evidence for proton decay, some grand unification theories, such as the Georgi–Glashow model, require it. According to some such theories, the proton has a half-life of about years, and decays into a positron and a neutral pion that itself immediately decays into 2 gamma ray photons:
Since a positron is an antilepton this decay preserves B-L number, which is conserved in most GUTs.
Additional decay modes are available (e.g.: → + ), both directly and when catalyzed via interaction with GUT-predicted magnetic monopoles. Though this process has not been observed experimentally, it is within the realm of experimental testability for future planned very large-scale detectors on the megaton scale. Such detectors include the Hyper-Kamiokande.
Early grand unification theories, such as the Georgi–Glashow model, which were the first consistent theories to suggest proton decay postulated that the proton's half-life would be at least 1031 years. As further experiments and calculations were performed in the 1990s, it became clear that the proton half-life could not lie below 1032 years. Many books from that period refer to this figure for the possible decay time for baryonic matter.
Although the phenomenon is referred to as "proton decay", the effect would also be seen in neutrons bound inside atomic nuclei. Free neutrons—those not inside an atomic nucleus—are already known to decay into protons (and an electron and an antineutrino) in a process called beta decay. Free neutrons have a half-life of about 10 minutes () due to the weak interaction. Neutrons bound inside a nucleus have an immensely longer half-life—apparently as great as that of the proton.
Decay operators.
Dimension-6 proton decay operators.
The dimension-6 proton decay operators are formula_1, formula_2, formula_3 and formula_4 where formula_5 is the cutoff scale for the Standard Model. All of these operators violate both baryon number ("B") and lepton number ("L") conservation but not the combination "B" − "L".
In GUT models, the exchange of an X or Y boson with the mass ΛGUT can lead to the last two operators suppressed by formula_6. The exchange of a triplet Higgs with mass "M" can lead to all of the operators suppressed by 1/"M"2. See doublet–triplet splitting problem.
Dimension-5 proton decay operators.
In supersymmetric extensions (such as the MSSM), we can also have dimension-5 operators involving two fermions and two sfermions caused by the exchange of a tripletino of mass "M". The sfermions will then exchange a gaugino or Higgsino or gravitino leaving two fermions. The overall Feynman diagram has a loop (and other complications due to strong interaction physics). This decay rate is suppressed by formula_7 where "M"SUSY is the mass scale of the superpartners.
Dimension-4 proton decay operators.
In the absence of matter parity, supersymmetric extensions of the Standard Model can give rise to the last operator suppressed by the inverse square of sdown quark mass. This is due to the dimension-4 operators
c and
ccc.
The proton decay rate is only suppressed by formula_8 which is far too fast unless the couplings are very small.

</doc>
<doc id="391278" url="https://en.wikipedia.org/wiki?curid=391278" title="Proton emission">
Proton emission

Proton emission (also known as proton radioactivity) is a type of radioactive decay in which a proton is ejected from a nucleus. Proton emission can occur from high-lying excited states in a nucleus following a beta decay, in which case the process is known as beta-delayed proton emission, or can occur from the ground state (or a low-lying isomer) of very proton-rich nuclei, in which case the process is very similar to alpha decay. 
For a proton to escape a nucleus, the proton separation energy must be negative - the proton is therefore unbound, and tunnels out of the nucleus in a finite time. Proton emission is not seen in naturally occurring isotopes; proton emitters can be produced via nuclear reactions, usually utilising some kind of particle accelerator.
Although prompt (i.e. not beta-delayed) proton emission was observed from an isomer in cobalt-53 as early as 1969, no other proton-emitting states were found until 1981, when the proton radioactive ground states of lutetium-151 and thulium-147 were observed at experiments at the GSI in West Germany. Research in the field flourished after this breakthrough, and to date more than 25 isotopes have been found to exhibit proton emission. The study of proton emission has aided the understanding of nuclear deformation, masses and structure, and it is a wonderfully pure example of quantum tunneling.
In 2002, the simultaneous emission of two protons was observed from the nucleus iron-45 in experiments at GSI and GANIL (Grand Accélérateur National d'Ions Lourds, at Caen). In 2005 it was experimentally determined (at the same facility) that zinc-54 can also undergo double proton decay.

</doc>
<doc id="10962546" url="https://en.wikipedia.org/wiki?curid=10962546" title="Pseudo-Euclidean space">
Pseudo-Euclidean space

In mathematics and theoretical physics, a pseudo-Euclidean space is a finite-dimensional real -space together with a non-degenerate indefinite quadratic form . Such a quadratic form can, given a suitable choice of basis , be applied to a vector , giving (with )
For true Euclidean spaces, , implying that the quadratic form is positive-definite rather than indefinite. Otherwise is an isotropic quadratic form. Note that if and , then , so that is a null vector. In a pseudo-Euclidean space, unlike in a Euclidean space, there exist vectors with negative magnitude.
As with the term "Euclidean space", "pseudo-Euclidean space" may refer to either an affine space or a vector space (see point–vector distinction) over real numbers.
Geometry.
The geometry of a pseudo-Euclidean space is consistent in spite of a breakdown of the some properties of Euclidean space; most notably that it is not a metric space as explained below. Though, its affine structure provides that concepts of line, plane and, generally, of an affine subspace (flat), can be used without modifications, as well as line segments.
Positive, zero, and negative magnitudes.
A null vector is a vector whose magnitude is zero. Unlike in a Euclidean space, it can be non-zero, in which case it is perpendicular to itself.
Every pseudo-Euclidean space has a linear cone of null vectors given by . When the pseudo-Euclidean space provides a model for spacetime (see below), the null cone is called the light cone of the origin.
The null cone separates two open sets of positive-magnitude and negative-magnitude vectors. If , then the set of positive-magnitude vectors is connected. If , which means the quadratic form has the only square term with positive sign, then it consists of two disjoint parts, one with and another with . Similar statements can be made for negative-magnitude vectors if is replaced with .
Distance.
The magnitude corresponds to the square of a vector (or its norm) in Euclidean case. To define the vector norm (and distance) in an invariant manner, one has to get square roots of magnitudes, which leads to possibly imaginary distances; see square root of negative numbers. But even for a triangle with positive magnitudes of all three sides (whose square roots are real and positive), the triangle inequality is not necessarily true.
That's why terms "norm" and "distance" are avoided in pseudo-Euclidean geometry, replaced with "magnitude" and "interval" respectively.
Though, for a curve whose tangent vectors all have the same sign of magnitude, the arc length is defined. It has important applications: see proper time, for example.
Rotations and spheres.
The rotations group of such space is indefinite orthogonal group , also denoted as without a reference to particular quadratic form. Such "rotations" preserve the form and, hence, the magnitude of each vector whether is it positive, zero, or negative.
Whereas Euclidean space has a unit sphere, pseudo-Euclidean space has the hypersurfaces and . Such a hypersurface called a hyperboloid or unit quasi-sphere is preserved by the appropriate indefinite orthogonal group.
Symmetric bilinear form.
The quadratic form gives rise to a symmetric bilinear form defined as follows:
The quadratic form can be expressed in terms of the bilinear form: formula_3.
When formula_4, then and are orthogonal elements of the pseudo-Euclidean space.
Some authors use the terms "inner product" or "dot product" for the bilinear form, but it does not define an inner product space and its properties do not match to dot product of Euclidean vectors, although these terms are seldom used to refer to this bilinear form.
The standard basis of the real -space is orthogonal. There are no ortho"normal" bases in a pseudo-Euclidean space because there is no vector norm.
Subspaces and orthogonality.
For a (positive-dimensional) subspace of a pseudo-Euclidean space, when the magnitude form is restricted to , following three cases are possible:
One of most jarring properties (for a Euclidean intuition) of pseudo-Euclidean vectors and flats is their orthogonality. When two non-zero Euclidean vectors are perpendicular, they are certainly not collinear. Any Euclidean linear subspace intersects with its orthogonal complement only by the {0} subspace. But the definition from the previous subsection immediately implies that any vector of zero magnitude is perpendicular to itself. Hence, for the 1-subspace generated by such non-zero vector, its orthogonal complement will be a superspace of .
The formal definition of the orthogonal complement of a vector subspace in a pseudo-Euclidean space gives a perfectly well-defined result which satisfies the equality due to the magnitude form's non-degeneracy. It is just the condition
which can be broken if the subspace contains a null direction. While subspaces form a distributive lattice, as in any vector space, they do not form a Boolean algebra with this ⊥ operation, as in inner product spaces.
For a subspace composed "entirely" of null vectors (which means that the magnitude , restricted to , equals to 0), always holds:
Such subspaces can have up to dimensions.
For a (positive) Euclidean -subspace its orthogonal complement is a -dimensional negative "Euclidean" subspace, and vice versa.
Generally, for a -dimensional subspace consisting of positive and negative dimensions (see Sylvester's law of inertia for clarification), its orthogonal "complement" has positive and negative dimensions, while the rest ones are degenerate and form the intersection.
Parallelogram law and Pythagorean theorem.
The parallelogram law takes the form
Using the square of the sum identity, for an arbitrary triangle one can express the magnitude of the third side from magnitudes of two sides and their bilinear form product:
This demonstrates that, for perpendicular vectors, a pseudo-Euclidean analog of the Pythagorean theorem holds:
Angle.
Generally, absolute value of the bilinear form on two vectors may be greater than , equal to it, or less. This causes similar problems with definition of angle (see dot product#Geometric definition) as appeared above for distances.
If (only one positive term in ), then for positive-magnitude vectors:
which permits to define hyperbolic angle, an analog of angle between these vectors through inverse hyperbolic cosine:
It corresponds to the distance on a -dimensional hyperbolic space. This is known as rapidity in the context of theory of relativity discussed below. Unlike Euclidean angle, it takes values from and equals to 0 for antiparallel vectors.
There is no reasonable definition of the angle between a null vector and another vector (either null or non-null).
Algebra and tensor calculus.
Like Euclidean spaces, a pseudo-Euclidean space possesses geometric algebra. Unlike properties above, where replacement of to changed numbers but not geometry, the sign reversal of the magnitude form actually "alters" Cℓ, so for example and are not isomorphic.
Just like over any vector space, there are pseudo-Euclidean tensors. Like with a Euclidean structure, there are raising and lowering indices operators but, unlike the case with Euclidean tensors, there is no bases where these operations do not change values of components. If there is a vector , the corresponding covariant vector is:
and with the standard-form
the first components of are numerically the same as ones of , but the rest have opposite signs.
The correspondence between contravariant and covariant tensors makes a tensor calculus on pseudo-Riemannian manifolds analogous to one on Riemannian manifolds.
Examples.
A very important pseudo-Euclidean space is Minkowski space, which is the mathematical setting in which Albert Einstein's theory of special relativity is conveniently formulated. For Minkowski space, and so that
The geometry associated with this pseudo-metric was investigated by Poincaré. Its rotation group is the Lorentz group. The Poincaré group includes also translations and plays the same role as Euclidean groups of ordinary Euclidean spaces.
Another pseudo-Euclidean space is the plane consisting of split-complex numbers, equipped with the quadratic form
This is the simplest case of a pseudo-Euclidean space (, ) and the only one where the null cone dissects the space to "four" open sets. The group consists of so named hyperbolic rotations.

</doc>
<doc id="287555" url="https://en.wikipedia.org/wiki?curid=287555" title="Pseudo-Riemannian manifold">
Pseudo-Riemannian manifold

In differential geometry, a pseudo-Riemannian manifold (also called a semi-Riemannian manifold) is a generalization of a Riemannian manifold in which the metric tensor need not be positive-definite. Instead a weaker condition of nondegeneracy is imposed on the metric tensor. 
Every tangent space of a pseudo-Riemannian manifold is a pseudo-Euclidean space described by an isotropic quadratic form.
A special case of great importance to general relativity is a Lorentzian manifold, in which one dimension has a sign opposite to that of the rest. This allows tangent vectors to be classified into timelike, null, and spacelike. Spacetime can be modeled as a 4-dimensional Lorentzian manifold.
Introduction.
Manifolds.
In differential geometry, a differentiable manifold is a space which is locally similar to a Euclidean space. In an n-dimensional Euclidean space any point can be specified by n real numbers. These are called the coordinates of the point.
An n-dimensional differentiable manifold is a generalisation of n-dimensional Euclidean space. In a manifold it may only be possible to define coordinates "locally". This is achieved by defining coordinate patches: subsets of the manifold which can be mapped into n-dimensional Euclidean space.
See Manifold, differentiable manifold, coordinate patch for more details.
Tangent spaces and metric tensors.
Associated with each point formula_1 in an formula_2-dimensional differentiable manifold formula_3 is a tangent space (denoted formula_4). This is an formula_2-dimensional vector space whose elements can be thought of as equivalence classes of curves passing through the point formula_1.
A metric tensor is a non-degenerate, smooth, symmetric, bilinear map which assigns a real number to pairs of tangent vectors at each tangent space of the manifold. Denoting the metric tensor by formula_7 we can express this as 
The map is symmetric and bilinear so if formula_9 are tangent vectors at a point formula_1 to the manifold formula_3 then we have
for any real number formula_14.
That formula_7 is non-degenerate means there are no non-zero formula_16 such that formula_17 for all formula_18.
Metric signatures.
Given a metric tensor "g" on an "n"-dimensional real manifold, the quadratic form associated with the metric tensor applied to each vector of any orthogonal basis produces "n" real values. By Sylvester's law of inertia, the number of each positive, negative and zero values produced in this manner are invariants of the metric tensor, independent of the choice of orthogonal basis. The signature of the metric tensor gives these numbers, shown in the same order. A non-degenerate metric tensor has and the signature may be denoted ("p", "q"), where .
Definition.
A pseudo-Riemannian manifold formula_19 is a differentiable manifold formula_20 equipped with a non-degenerate, smooth, symmetric metric tensor formula_21.
Such a metric is called a pseudo-Riemannian metric and its values can be positive, negative or zero.
The signature of a pseudo-Riemannian metric is , where both "p" and "q" are non-negative.
Lorentzian manifold.
A Lorentzian manifold is an important special case of a pseudo-Riemannian manifold in which the signature of the metric is (1, n−1) (or sometimes (n−1, 1), see sign convention). Such metrics are called Lorentzian metrics. They are named after the physicist Hendrik Lorentz.
Applications in physics.
After Riemannian manifolds, Lorentzian manifolds form the most important subclass of pseudo-Riemannian manifolds. They are important in applications of general relativity.
A principal basis of general relativity is that spacetime can be modeled as a 4-dimensional Lorentzian manifold of signature (3, 1) or, equivalently, (1, 3). Unlike Riemannian manifolds with positive-definite metrics, a signature of (p, 1) or (1, q) allows tangent vectors to be classified into "timelike", "null" or "spacelike" (see Causal structure).
Properties of pseudo-Riemannian manifolds.
Just as Euclidean space formula_22 can be thought of as the model Riemannian manifold, Minkowski space formula_23 with the flat Minkowski metric is the model Lorentzian manifold. Likewise, the model space for a pseudo-Riemannian manifold of signature (p, q) is formula_24 with the metric 
Some basic theorems of Riemannian geometry can be generalized to the pseudo-Riemannian case. In particular, the fundamental theorem of Riemannian geometry is true of pseudo-Riemannian manifolds as well. This allows one to speak of the Levi-Civita connection on a pseudo-Riemannian manifold along with the associated curvature tensor. On the other hand, there are many theorems in Riemannian geometry which do not hold in the generalized case. For example, it is "not" true that every smooth manifold admits a pseudo-Riemannian metric of a given signature; there are certain topological obstructions. Furthermore, a submanifold does not always inherit the structure of a pseudo-Riemannian manifold; for example, the metric tensor becomes zero on any light-like curve. The Clifton–Pohl torus provides an example of a pseudo-Riemannian manifold that is compact but not complete, a combination of properties that the Hopf–Rinow theorem disallows for Riemannian manifolds.

</doc>
<doc id="60766" url="https://en.wikipedia.org/wiki?curid=60766" title="Pseudosphere">
Pseudosphere

In geometry, the term pseudosphere is used to describe various surfaces with constant negative Gaussian curvature. Depending on context, it can refer to either a theoretical surface of constant negative curvature, to a tractricoid, or to a hyperboloid.
Theoretical pseudosphere.
In its general interpretation, a pseudosphere of radius "R" is any surface of curvature −1/"R"2, by analogy with the sphere of radius "R", which is a surface of curvature 1/"R"2. The term was introduced by Eugenio Beltrami in his 1868 paper on models of hyperbolic geometry.
Tractricoid.
The term is also used to refer to a certain surface called the tractricoid: the result of revolving a tractrix about its asymptote. As an example, the (half) pseudosphere (with radius 1) is the surface of revolution of the tractrix parametrized by
It is a singular space (the equator is a singularity), but away from the singularities, it has constant negative Gaussian curvature and therefore is locally isometric to a hyperbolic plane.
The name "pseudosphere" comes about because it is a two-dimensional surface of constant negative curvature just like a sphere with positive Gauss curvature.
Just as the sphere has at every point a positively curved geometry of a dome the whole pseudosphere has at every point the negatively curved geometry of a saddle.
As early as 1693 Christiaan Huygens found that the volume and the surface area of the pseudosphere are finite, despite the infinite extent of the shape along the axis of rotation. For a given edge radius "R", the area is 4π"R"2 just as it is for the sphere, while the volume is π"R"3 and therefore half that of a sphere of that radius.
Universal covering space.
The half pseudosphere of curvature −1 is covered by the portion of the hyperbolic upper half-plane with "y" ≥ 1. The covering map is periodic in the "x" direction of period 2π, and takes the horocycles "y" = "c" to the meridians of the pseudosphere and the vertical geodesics "x" = "c" to the tractrices that generate the pseudosphere. This mapping is a local isometry, and thus exhibits the portion "y" ≥ 1 of the upper half-plane as the universal covering space of the pseudosphere. The precise mapping is
where formula_3 is the arclength parametrization of the tractrix above.
Hyperboloid.
In some sources that use the hyperboloid model of the hyperbolic plane, the hyperboloid is referred to as a pseudosphere.
This usage of the word is because the hyperboloid can be thought of as a sphere of imaginary radius, embedded in a Minkowski space.

</doc>
<doc id="3307045" url="https://en.wikipedia.org/wiki?curid=3307045" title="Pyramid (geometry)">
Pyramid (geometry)

In geometry, a pyramid is a polyhedron formed by connecting a polygonal base and a point, called the apex. Each base edge and apex form a triangle, called a "lateral face". It is a conic solid with polygonal base. A pyramid with an "n"-sided base will have vertices, faces, and 2"n" edges. All pyramids are self-dual.
A right pyramid has its apex directly above the centroid of its base. Nonright pyramids are called oblique pyramids. A regular pyramid has a regular polygon base and is usually implied to be a "right pyramid". 
When unspecified, a pyramid is usually assumed to be a "regular" square pyramid, like the physical pyramid structures. A triangle-based is more often called a tetrahedron.
Among oblique pyramids, like acute and obtuse triangles, a pyramid can be called "acute" if its apex above the interior of the base, and "obtuse" if its apex above the exterior of the base. A right-angled pyramid has its apex above an edge or vertex of the base. In a tetrahedron these qualifiers will change based on which face is considered the base.
Pyramids are a subclass of the prismatoids. Pyramids can be doubled into bipyramid by adding a second offset point on the other side of the base plane.
Right pyramids with a regular base.
A right pyramid with a regular-based has isosceles triangle sides, with symmetry is C"n"v or [1,"n"], with order 2"n". It can be given an extended Schläfli symbol ( ) ∨ {"n"}, representing a point, ( ), joined (orthogonally offset) to a regular polygon, {n}. A join operation creates a new edge between all pairs of vertices of the two joined figures.
The trigonal or triangular pyramid with all equilateral triangles faces becomes the regular tetrahedron, one of the Platonic solids. A lower symmetry case of the triangular pyramid is C3v which has an equilateral triangle base, and 3 identical isosceles triangle sides. The square and pentagonal pyramids can also be composed of regular convex polygons, in which case they are Johnson solids.
If all edges of a square pyramid (or any convex polyhedron) are tangent to a sphere so that the average position of the tangential points are at the center of the sphere, then the pyramid is said to be canonical, and it forms half of a regular octahedron.
Right Star pyramids.
Right pyramids with regular star polygon bases are called star pyramids. For example, the pentagrammic pyramid has a pentagram base and 5 intersecting triangle sides.
Right pyramids with an irregular base.
A right pyramid can be named as ( )∨P, where ( ) is the apex point, ∨ is a join operator, and P is a base polygon. 
An isosceles triangle right tetrahedron can be written as ( )∨[( )∨{ }] as the join of a point to an isosceles triangle base, as [( )∨( )]∨{ } or { }∨{ } as the join (orthogonal offsets) of two orthogonal segments, a digonal disphenoid, containing 4 isosceles triangle faces. It has C1v symmetry from two different base-apex orientations, and C2v in its full symmetry.
A rectangular right pyramid, written as ( )∨[{ }×{ }], and a rhombic pyramid, as ( )∨[{ }+{ }], both have symmetry C2v.
Volume.
The volume of a pyramid (also any cone) is formula_1 where "b" is the area of the base and "h" the height from the base to the apex. This works for any polygon, regular or non-regular, and any location of the apex, provided that "h" is measured as the perpendicular distance from the plane which contains the base. In 499 AD Aryabhata, a mathematician-astronomer from the classical age of Indian mathematics and Indian astronomy, used this method in the "Aryabhatiya" (section 2.6).
The formula can be formally proved using calculus: By similarity, the "linear" dimensions of a cross section parallel to the base increase linearly from the apex to the base. The scaling factor (proportionality factor) is formula_2, or formula_3, where "h" is the height and "y" is the perpendicular distance from the plane of the base to the cross-section. Since the area of any cross section is proportional to the square of the shape's scaling factor, the area of a cross section at height "y" is b×formula_4, or since both b and h are constants formula_5formula_6. The volume is given by the integral
The same equation, formula_1, also holds for cones with any base. This can be proven by an argument similar to the one above; see volume of a cone.
For example, the volume of a pyramid whose base is an "n"-sided regular polygon with side length "s" and whose height is "h" is:
The formula can also be derived exactly without calculus for pyramids with rectangular bases. Consider a unit cube. Draw lines from the center of the cube to each of the 8 vertices. This partitions the cube into 6 equal square pyramids of base area 1 and height 1/2. Each pyramid clearly has volume of 1/6. From this we deduce that pyramid volume = height * base area / 3.
Next, expand the cube uniformly in three directions by unequal amounts so that the resulting rectangular solid edges are "a", "b" and "c", with solid volume "abc". Each of the 6 pyramids within are likewise expanded. And—each pyramid has the same volume "abc/6". Since pairs of pyramids have heights "a/2", "b/2" and "c/2" we see that pyramid volume = height * base area / 3 again.
Surface area.
The surface area of a pyramid is formula_10 where "B" is the base area, "P" is the base perimeter and "L" is the slant height
formula_11 where "h" is the pyramid altitude and "r" is the inradius of the base.
n-dimensional pyramids.
A 2-dimensional pyramid is a triangle, formed by a base edge connected to a noncolinear point called an apex.
A 4-dimensional pyramid is called a polyhedral pyramid, constructed by a polyhedron in a 3-space hyperplane of 4-space with another point off that hyperplane.
Higher-dimensional pyramids are constructed similarly.
The family of simplices represent pyramids in any dimension, increasing from triangle, tetrahedron, 5-cell, 5-simplex, etc. A n-dimensional simplex has the minimum "n+1" vertices, with all pairs of vertices connected by edges, all triples of vertices defining faces, all quadruples of points defining tetrahedral cells, etc.
Polyhedral pyramid.
In 4-dimensional geometry, a polyhedral pyramid is a 4-polytope constructed by a base polyhedron cell and an apex point. The lateral facets are pyramid cells, each constructed by one face of the base polyhedron and the apex. The vertices and edges of polyhedral pyramids form examples of apex graphs, graphs formed by adding one vertex (the apex) to a planar graph (the graph of the base).
The regular 5-cell (or 4-simplex) is an example of a "tetrahedral pyramid". Uniform polyhedra with circumradii less than 1 can be make polyhedral pyramids with regular tetrahedral sides. A polyhedron with "v" vertices, "e" edges, and "f" faces can be the base on a polyhedral pyramid with "v+1" vertices, "e+v" edges, "f+e" faces, and "1+f" cells.
A 4D "polyhedral pyramid" with axial symmetry can be visualized in 3D with a Schlegel diagram which is a 3D projection that places the apex at the center of the base polyhedron.
Any convex 4-polytope can be divided into polyhedral pyramids by adding an interior point and creating one pyramid from each facet to the center point. This can be useful for computing volumes.
The 4-dimensional "volume" of a polyhedral pyramid is 1/4 of the volume of the base polyhedron times its perpendicular height, compared to the area of a triangle being 1/2 the length of the base times the height and the volume of a pyramid being 1/3 the area of the base times the height.

</doc>
<doc id="26513034" url="https://en.wikipedia.org/wiki?curid=26513034" title="Pythagorean theorem">
Pythagorean theorem

In mathematics, the Pythagorean theorem, also known as Pythagoras' theorem, is a relation in Euclidean geometry among the three sides of a right triangle. It states that the square of the hypotenuse (the side opposite the right angle) is equal to the sum of the squares of the other two sides. The theorem can be written as an equation relating the lengths of the sides "a", "b" and "c", often called the "Pythagorean equation":
where "c" represents the length of the hypotenuse and "a" and "b" the lengths of the triangle's other two sides.
Although it is often argued that knowledge of the theorem predates him, the theorem is named after the ancient Greek mathematician Pythagoras ( 570 – 495 BC) as it is he who, by tradition, is credited with its first recorded proof. There is some evidence that Babylonian mathematicians understood the formula, although little of it indicates an application within a mathematical framework. Mesopotamian, Indian and Chinese mathematicians all discovered the theorem independently and, in some cases, provided proofs for special cases.
The theorem has been given numerous proofspossibly the most for any mathematical theorem. They are very diverse, including both geometric proofs and algebraic proofs, with some dating back thousands of years. The theorem can be generalized in various ways, including higher-dimensional spaces, to spaces that are not Euclidean, to objects that are not right triangles, and indeed, to objects that are not triangles at all, but "n"-dimensional solids. The Pythagorean theorem has attracted interest outside mathematics as a symbol of mathematical abstruseness, mystique, or intellectual power; popular references in literature, plays, musicals, songs, stamps and cartoons abound.
Pythagorean proof.
The Pythagorean Theorem was known long before Pythagoras, but he may well have been the first to prove it. In any event, the proof attributed to him is very simple, and is called a proof by rearrangement.
The two large squares shown in the figure each contain four identical triangles, and the only difference between the two large squares is that the triangles are arranged differently. Therefore, the white space within each of the two large squares must have equal area. Equating the area of the white space yields the Pythagorean Theorem, Q.E.D.
That Pythagoras originated this very simple proof is sometimes inferred from the writings of the later Greek philosopher and mathematician Proclus. Several other proofs of this theorem are described below, but this is known as the Pythagorean one.
Other forms of the theorem.
As pointed out in the introduction, if "c" denotes the length of the hypotenuse and "a" and "b" denote the lengths of the other two sides, the Pythagorean theorem can be expressed as the Pythagorean equation:
If the length of both "a" and "b" are known, then "c" can be calculated as
If the length of the hypotenuse "c" and of one side ("a" or "b") are known, then the length of the other side can be calculated as
or
The Pythagorean equation relates the sides of a right triangle in a simple way, so that if the lengths of any two sides are known the length of the third side can be found. Another corollary of the theorem is that in any right triangle, the hypotenuse is greater than any one of the other sides, but less than their sum.
A generalization of this theorem is the law of cosines, which allows the computation of the length of any side of any triangle, given the lengths of the other two sides and the angle between them. If the angle between the other sides is a right angle, the law of cosines reduces to the Pythagorean equation.
Other proofs of the theorem.
This theorem may have more known proofs than any other (the law of quadratic reciprocity being another contender for that distinction); the book "The Pythagorean Proposition" contains 370 proofs.
Proof using similar triangles.
This proof is based on the proportionality of the sides of two similar triangles, that is, upon the fact that the ratio of any two corresponding sides of similar triangles is the same regardless of the size of the triangles.
Let "ABC" represent a right triangle, with the right angle located at "C", as shown on the figure. Draw the altitude from point "C", and call "H" its intersection with the side "AB". Point "H" divides the length of the hypotenuse "c" into parts "d" and "e". The new triangle "ACH" is similar to triangle "ABC", because they both have a right angle (by definition of the altitude), and they share the angle at "A", meaning that the third angle will be the same in both triangles as well, marked as "θ" in the figure. By a similar reasoning, the triangle "CBH" is also similar to "ABC". The proof of similarity of the triangles requires the Triangle postulate: the sum of the angles in a triangle is two right angles, and is equivalent to the parallel postulate. Similarity of the triangles leads to the equality of ratios of corresponding sides:
The first result equates the cosines of the angles "θ", whereas the second result equates their sines.
These ratios can be written as
Summing these two equalities results in
which, after simplification, expresses the Pythagorean theorem:
The role of this proof in history is the subject of much speculation. The underlying question is why Euclid did not use this proof, but invented another. One conjecture is that the proof by similar triangles involved a theory of proportions, a topic not discussed until later in the "Elements", and that the theory of proportions needed further development at that time.
Euclid's proof.
In outline, here is how the proof in Euclid's "Elements" proceeds. The large square is divided into a left and right rectangle. A triangle is constructed that has half the area of the left rectangle. Then another triangle is constructed that has half the area of the square on the left-most side. These two triangles are shown to be congruent, proving this square has the same area as the left rectangle. This argument is followed by a similar version for the right rectangle and the remaining square. Putting the two rectangles together to reform the square on the hypotenuse, its area is the same as the sum of the area of the other two squares. The details follow.
Let "A", "B", "C" be the vertices of a right triangle, with a right angle at "A". Drop a perpendicular from "A" to the side opposite the hypotenuse in the square on the hypotenuse. That line divides the square on the hypotenuse into two rectangles, each having the same area as one of the two squares on the legs.
For the formal proof, we require four elementary lemmata:
Next, each top square is related to a triangle congruent with another triangle related in turn to one of two rectangles making up the lower square.
The proof is as follows:
This proof, which appears in Euclid's "Elements" as that of Proposition 47 in Book 1, demonstrates that the area of the square on the hypotenuse is the sum of the areas of the other two squares. This is quite distinct from the proof by similarity of triangles, which is conjectured to be the proof that Pythagoras used.
Proof by rearrangement.
We have already discussed the Pythagorean proof, which was a proof by rearrangement. The same idea is conveyed by the leftmost animation below, which consists of a large square, side , containing four identical right triangles. The triangles are shown in two arrangements, the first of which leaves two squares "a"2 and "b"2 uncovered, the second of which leaves square "c"2 uncovered. The area encompassed by the outer square never changes, and the area of the four triangles is the same at the beginning and the end, so the black square areas must be equal, therefore 
A second proof by rearrangement is given by the middle animation. A large square is formed with area "c"2, from four identical right triangles with sides "a", "b" and "c", fitted around a small central square. Then two rectangles are formed with sides "a" and "b" by moving the triangles. Combining the smaller square with these rectangles produces two squares of areas "a"2 and "b"2, which must have the same area as the initial large square.
The third, rightmost image also gives a proof. The upper two squares are divided as shown by the blue and green shading, into pieces that when rearranged can be made to fit in the lower square on the hypotenuse – or conversely the large square can be divided as shown into pieces that fill the other two. This shows the area of the large square equals that of the two smaller ones.
Algebraic proofs.
The theorem can be proved algebraically using four copies of a right triangle with sides "a", "b" and "c", arranged inside a square with side "c" as in the top half of the diagram. The triangles are similar with area formula_10, while the small square has side and area . The area of the large square is therefore
But this is a square with side "c" and area "c"2, so
A similar proof uses four copies of the same triangle arranged symmetrically around a square with side "c", as shown in the lower part of the diagram. This results in a larger square, with side and area . The four triangles and the square side "c" must have the same area as the larger square,
giving
A related proof was published by future U.S. President James A. Garfield (then a U.S. Representative). Instead of a square it uses a trapezoid, which can be constructed from the square in the second of the above proofs by bisecting along a diagonal of the inner square, to give the trapezoid as shown in the diagram. The area of the trapezoid can be calculated to be half the area of the square, that is
The inner square is similarly halved, and there are only two triangles so the proof proceeds as above except for a factor of formula_16, which is removed by multiplying by two to give the result.
Proof using differentials.
One can arrive at the Pythagorean theorem by studying how changes in a side produce a change in the hypotenuse and employing calculus.
The triangle "ABC" is a right triangle, as shown in the upper part of the diagram, with "BC" the hypotenuse. At the same time the triangle lengths are measured as shown, with the hypotenuse of length "y", the side "AC" of length "x" and the side "AB" of length "a", as seen in the lower diagram part.
If "x" is increased by a small amount "dx" by extending the side "AC" slightly to "D", then "y" also increases by "dy". These form two sides of a triangle, "CDE", which (with "E" chosen so "CE" is perpendicular to the hypotenuse) is a right triangle approximately similar to "ABC". Therefore, the ratios of their sides must be the same, that is:
This can be rewritten as follows:
This is a differential equation which is solved to give
And the constant can be deduced from "x" = 0, "y" = "a" to give the equation
This is more of an intuitive proof than a formal one: it can be made more rigorous if proper limits are used in place of "dx" and "dy".
Converse.
The converse of the theorem is also true:
For any three positive numbers "a", "b", and "c" such that , there exists a triangle with sides "a", "b" and "c", and every such triangle has a right angle between the sides of lengths "a" and "b".
An alternative statement is:
For any triangle with sides "a", "b", "c", if then the angle between "a" and "b" measures 90°.
This converse also appears in Euclid's "Elements" (Book I, Proposition 48): 
It can be proven using the law of cosines or as follows:
Let "ABC" be a triangle with side lengths "a", "b", and "c", with Construct a second triangle with sides of length "a" and "b" containing a right angle. By the Pythagorean theorem, it follows that the hypotenuse of this triangle has length "c" = , the same as the hypotenuse of the first triangle. Since both triangles' sides are the same lengths "a", "b" and "c", the triangles are congruent and must have the same angles. Therefore, the angle between the side of lengths "a" and "b" in the original triangle is a right angle.
The above proof of the converse makes use of the Pythagorean Theorem itself. The converse can also be proven without assuming the Pythagorean Theorem.
A corollary of the Pythagorean theorem's converse is a simple means of determining whether a triangle is right, obtuse, or acute, as follows. Let "c" be chosen to be the longest of the three sides and (otherwise there is no triangle according to the triangle inequality). The following statements apply:
Edsger Dijkstra has stated this proposition about acute, right, and obtuse triangles in this language:
where "α" is the angle opposite to side "a", "β" is the angle opposite to side "b", "γ" is the angle opposite to side "c", and sgn is the sign function.
Consequences and uses of the theorem.
Pythagorean triples.
A Pythagorean triple has three positive integers "a", "b", and "c", such that In other words, a Pythagorean triple represents the lengths of the sides of a right triangle where all three sides have integer lengths. Evidence from megalithic monuments in Northern Europe shows that such triples were known before the discovery of writing. Such a triple is commonly written Some well-known examples are and 
A primitive Pythagorean triple is one in which "a", "b" and "c" are coprime (the greatest common divisor of "a", "b" and "c" is 1).
The following is a list of primitive Pythagorean triples with values less than 100:
Incommensurable lengths.
One of the consequences of the Pythagorean theorem is that line segments whose lengths are incommensurable (so the ratio of which is not a rational number) can be constructed using a straightedge and compass. Pythagoras's theorem enables construction of incommensurable lengths because the hypotenuse of a triangle is related to the sides by the square root operation.
The figure on the right shows how to construct line segments whose lengths are in the ratio of the square root of any positive integer. Each triangle has a side (labeled "1") that is the chosen unit for measurement. In each right triangle, Pythagoras's theorem establishes the length of the hypotenuse in terms of this unit. If a hypotenuse is related to the unit by the square root of a positive integer that is not a perfect square, it is a realization of a length incommensurable with the unit, such as , ,  . For more detail, see Quadratic irrational.
Incommensurable lengths conflicted with the Pythagorean school's concept of numbers as only whole numbers. The Pythagorean school dealt with proportions by comparison of integer multiples of a common subunit. According to one legend, Hippasus of Metapontum ("ca." 470 B.C.) was drowned at sea for making known the existence of the irrational or incommensurable.
Complex numbers.
For any complex number
the absolute value or modulus is given by
So the three quantities, "r", "x" and "y" are related by the Pythagorean equation,
Note that "r" is defined to be a positive number or zero but "x" and "y" can be negative as well as positive. Geometrically "r" is the distance of the "z" from zero or the origin "O" in the complex plane.
This can be generalised to find the distance between two points, "z"1 and "z"2 say. The required distance is given by
so again they are related by a version of the Pythagorean equation,
Euclidean distance in various coordinate systems.
The distance formula in Cartesian coordinates is derived from the Pythagorean theorem. If and are points in the plane, then the distance between them, also called the Euclidean distance, is given by
More generally, in Euclidean "n"-space, the Euclidean distance between two points, formula_27 and formula_28, is defined, by generalization of the Pythagorean theorem, as:
If Cartesian coordinates are not used, for example, if polar coordinates are used in two dimensions or, in more general terms, if curvilinear coordinates are used, the formulas expressing the Euclidean distance are more complicated than the Pythagorean theorem, but can be derived from it. A typical example where the straight-line distance between two points is converted to curvilinear coordinates can be found in the applications of Legendre polynomials in physics. The formulas can be discovered by using Pythagoras's theorem with the equations relating the curvilinear coordinates to Cartesian coordinates. For example, the polar coordinates can be introduced as:
Then two points with locations and are separated by a distance "s":
Performing the squares and combining terms, the Pythagorean formula for distance in Cartesian coordinates produces the separation in polar coordinates as:
using the trigonometric product-to-sum formulas. This formula is the law of cosines, sometimes called the Generalized Pythagorean Theorem. From this result, for the case where the radii to the two locations are at right angles, the enclosed angle and the form corresponding to Pythagoras's theorem is regained: formula_33 The Pythagorean theorem, valid for right triangles, therefore is a special case of the more general law of cosines, valid for arbitrary triangles.
Pythagorean trigonometric identity.
In a right triangle with sides "a", "b" and hypotenuse "c", trigonometry determines the sine and cosine of the angle "θ" between side "a" and the hypotenuse as:
From that it follows:
where the last step applies Pythagoras's theorem. This relation between sine and cosine is sometimes called the fundamental Pythagorean trigonometric identity. In similar triangles, the ratios of the sides are the same regardless of the size of the triangles, and depend upon the angles. Consequently, in the figure, the triangle with hypotenuse of unit size has opposite side of size sin "θ" and adjacent side of size cos "θ" in units of the hypotenuse.
Relation to the cross product.
The Pythagorean theorem relates the cross product and dot product in a similar way:
This can be seen from the definitions of the cross product and dot product, as
with n a unit vector normal to both a and b. The relationship follows from these definitions and the Pythagorean trigonometric identity.
This can also be used to define the cross product. By rearranging the following equation is obtained
This can be considered as a condition on the cross product and so part of its definition, for example in seven dimensions.
Generalizations.
Similar figures on the three sides.
A generalization of the Pythagorean theorem extending beyond the areas of squares on the three sides to similar figures was known by Hippocrates of Chios in the 5th century BC, and was included by Euclid in his "Elements":
If one erects similar figures (see Euclidean geometry) with corresponding sides on the sides of a right triangle, then the sum of the areas of the ones on the two smaller sides equals the area of the one on the larger side.
This extension assumes that the sides of the original triangle are the corresponding sides of the three congruent figures (so the common ratios of sides between the similar figures are "a:b:c"). While Euclid's proof only applied to convex polygons, the theorem also applies to concave polygons and even to similar figures that have curved boundaries (but still with part of a figure's boundary being the side of the original triangle).
The basic idea behind this generalization is that the area of a plane figure is proportional to the square of any linear dimension, and in particular is proportional to the square of the length of any side. Thus, if similar figures with areas "A", "B" and "C" are erected on sides with corresponding lengths "a", "b" and "c" then:
But, by the Pythagorean theorem, "a"2 + "b"2 = "c"2, so "A" + "B" = "C".
Conversely, if we can prove that "A" + "B" = "C" for three similar figures without using the Pythagorean theorem, then we can work backwards to construct a proof of the theorem. For example, the starting center triangle can be replicated and used as a triangle "C" on its hypotenuse, and two similar right triangles ("A" and "B" ) constructed on the other two sides, formed by dividing the central triangle by its altitude. The sum of the areas of the two smaller triangles therefore is that of the third, thus "A" + "B" = "C" and reversing the above logic leads to the Pythagorean theorem a2 + b2 = c2.
Law of cosines.
The Pythagorean theorem is a special case of the more general theorem relating the lengths of sides in any triangle, the law of cosines:
where θ is the angle between sides "a" and "b".
When θ is 90 degrees, then cos"θ" = 0, and the formula reduces to the usual Pythagorean theorem.
Arbitrary triangle.
At any selected angle of a general triangle of sides "a, b, c", inscribe an isosceles triangle such that the equal angles at its base θ are the same as the selected angle. Suppose the selected angle θ is opposite the side labeled "c". Inscribing the isosceles triangle forms triangle "ABD" with angle θ opposite side "a" and with side "r" along "c". A second triangle is formed with angle θ opposite side "b" and a side with length "s" along "c", as shown in the figure. Tâbit ibn Qorra stated that the sides of the three triangles were related as:
As the angle θ approaches π/2, the base of the isosceles triangle narrows, and lengths "r" and "s" overlap less and less. When θ = π/2, "ADB" becomes a right triangle, "r" + "s" = "c", and the original Pythagorean theorem is regained.
One proof observes that triangle "ABC" has the same angles as triangle "ABD", but in opposite order. (The two triangles share the angle at vertex B, both contain the angle θ, and so also have the same third angle by the triangle postulate.) Consequently, "ABC" is similar to the reflection of "ABD", the triangle "DBA" in the lower panel. Taking the ratio of sides opposite and adjacent to θ,
Likewise, for the reflection of the other triangle,
Clearing fractions and adding these two relations:
the required result.
The theorem remains valid if the angle formula_46 is obtuse so the lengths "r" and "s" are non-overlapping.
General triangles using parallelograms.
Pappus' area theorem is a further generalization, that applies to triangles that are not right triangles, using parallelograms on the three sides in place of squares (squares are a special case, of course). The upper figure shows that for a scalene triangle, the area of the parallelogram on the longest side is the sum of the areas of the parallelograms on the other two sides, provided the parallelogram on the long side is constructed as indicated (the dimensions labeled with arrows are the same, and determine the sides of the bottom parallelogram). This replacement of squares with parallelograms bears a clear resemblance to the original Pythagoras's theorem, and was considered a generalization by Pappus of Alexandria in 4 A.D.
The lower figure shows the elements of the proof. Focus on the left side of the figure. The left green parallelogram has the same area as the left, blue portion of the bottom parallelogram because both have the same base "b" and height "h". However, the left green parallelogram also has the same area as the left green parallelogram of the upper figure, because they have the same base (the upper left side of the triangle) and the same height normal to that side of the triangle. Repeating the argument for the right side of the figure, the bottom parallelogram has the same area as the sum of the two green parallelograms.
Solid geometry.
In terms of solid geometry, Pythagoras's theorem can be applied to three dimensions as follows. Consider a rectangular solid as shown in the figure. The length of diagonal "BD" is found from Pythagoras's theorem as:
where these three sides form a right triangle. Using horizontal diagonal "BD" and the vertical edge "AB", the length of diagonal "AD" then is found by a second application of Pythagoras's theorem as:
or, doing it all in one step:
This result is the three-dimensional expression for the magnitude of a vector v (the diagonal AD) in terms of its orthogonal components {vk} (the three mutually perpendicular sides):
This one-step formulation may be viewed as a generalization of Pythagoras's theorem to higher dimensions. However, this result is really just the repeated application of the original Pythagoras's theorem to a succession of right triangles in a sequence of orthogonal planes.
A substantial generalization of the Pythagorean theorem to three dimensions is de Gua's theorem, named for Jean Paul de Gua de Malves: If a tetrahedron has a right angle corner (like a corner of a cube), then the square of the area of the face opposite the right angle corner is the sum of the squares of the areas of the other three faces. This result can be generalized as in the ""n"-dimensional Pythagorean theorem":
This statement is illustrated in three dimensions by the tetrahedron in the figure. The "hypotenuse" is the base of the tetrahedron at the back of the figure, and the "legs" are the three sides emanating from the vertex in the foreground. As the depth of the base from the vertex increases, the area of the "legs" increases, while that of the base is fixed. The theorem suggests that when this depth is at the value creating a right vertex, the generalization of Pythagoras's theorem applies. In a different wording:
Inner product spaces.
The Pythagorean theorem can be generalized to inner product spaces, which are generalizations of the familiar 2-dimensional and 3-dimensional Euclidean spaces. For example, a function may be considered as a vector with infinitely many components in an inner product space, as in functional analysis.
In an inner product space, the concept of perpendicularity is replaced by the concept of orthogonality: two vectors v and w are orthogonal if their inner product formula_51 is zero. The inner product is a generalization of the dot product of vectors. The dot product is called the "standard" inner product or the "Euclidean" inner product. However, other inner products are possible.
The concept of length is replaced by the concept of the norm ||v|| of a vector v, defined as:
In an inner-product space, the Pythagorean theorem states that for any two orthogonal vectors v and w we have
Here the vectors v and w are akin to the sides of a right triangle with hypotenuse given by the vector sum v + w. This form of the Pythagorean theorem is a consequence of the properties of the inner product:
where the inner products of the cross terms are zero, because of orthogonality.
A further generalization of the Pythagorean theorem in an inner product space to non-orthogonal vectors is the "parallelogram law" :
which says that twice the sum of the squares of the lengths of the sides of a parallelogram is the sum of the squares of the lengths of the diagonals. Any norm that satisfies this equality is "ipso facto" a norm corresponding to an inner product.
The Pythagorean identity can be extended to sums of more than two orthogonal vectors. If v"1", v"2", ..., v"n" are pairwise-orthogonal vectors in an inner-product space, then application of the Pythagorean theorem to successive pairs of these vectors (as described for 3-dimensions in the section on solid geometry) results in the equation
Sets of "m"-dimensional objects in "n"-dimensional space.
The broadest generalization of the Pythagorean theorem, introduced by Donald R. Conant and William A. Beyer, applies to a wide range of objects and sets of objects in any number of dimensions. Specifically, the square of the measure of an "m"-dimensional set of objects in one or more parallel "m"-dimensional flats in "n"-dimensional Euclidean space is equal to the sum of the squares of the measures of the orthogonal projections of the object(s) onto all "m"-dimensional coordinate subspaces.
In mathematical terms:
where:
Applied to Sets Containing a Single Object.
This generalized formula can be applied in the simplest case to a single one-dimensional object, a line segment, in two-dimensional space. The animation illustrates this case with a line segment shown in blue and its projections onto the "x"- and "y"- axes shown in green. The lengths of the projections squared and added together are equal to the length of the original line segment squared. This produces the familiar Pythagorean theorem formula:
where "c" is the length of the original line segment, "a" is the length of the segment projected onto the "x"-axis, and "b" is the length of the segment projected onto the "y"-axis. In the animation, "a"2 = 27, "b"2 = 9, and "c"2 = 36. Bringing the line segment together with its coordinate projections forms the traditional right triangle.
<br>
Similarly, for any two-dimensional object in three-dimensional space, the formula can be stated as:
where "D" is the area of a specified two-dimensional object, "A" is the area of the object’s projection onto the "xy"-coordinate plane, "B" is the area of the object’s projection onto the "xz"-coordinate plane, and "C" is the area of the object’s projection onto the "yz"-coordinate plane.
The animation showing a blue three-by-three square object in three dimensions of space illustrates this application of the generalization to an object of more than one dimension. As the orientation of the object changes, the proportions of the green coordinate plane projections adjust accordingly, so the squares of the areas of the projections always add up to the same value: the square of the area of the original object. In this case, the sum of the squares of the projection areas always add up to 81.
Applied to Sets Containing Multiple Objects.
The generalization applies equally to sets of multiple objects, as long as they are in the same plane or parallel planes. The measures of the objects in such a set can be added together and essentially treated as a single object. The multiple line-segment animation illustrates the generalization applied to a set of three one-dimensional objects in three dimensions of space. In this case, two sequential line segments exist in parallel to a third line segment. Because lines are one-dimensional, the coordinate subspaces onto which they are projected must also be one-dimensional. Thus, projections appear on the coordinate axes rather than on the coordinate planes. The lengths of the projected line segments on a given axis are summed, then squared, then added to the total lengths squared on the other axes. The result is the squared sum of the lengths of the original line segments. For the sake of simplicity, when projections are single points of zero length, they are not shown, since they do not affect the calculations.
The generalization applies to flat objects of any shape, regular or irregular. The multi-object animation illustrates the use of the generalization on a set of several different objects in different planes – in this case, a triangle and a circle on one plane, and a flat cat on a parallel plane (shown in blue). Projections of the set are shown in green on the coordinate plane subspaces. Objects shown initially upright in the "yz"-plane are subsequently tilted in parallel. Again, regardless of set orientation, the result remains the same. On each coordinate plane subspace, the areas of object projections are calculated individually (to avoid miscalculations due to projection overlap), then added together to produce the total projection area of the set on that plane. The projection set area is then squared for each coordinate plane. The sum of all projection set areas squared is always equal to the original set area squared.
Applied in Any Number of Dimensions.
This generalization holds regardless of the number of dimensions involved. The volume squared for a three-dimensional object or set can be calculated by summing the squares of the volumes of the associated three-dimensional projections onto three-dimensional subspaces. Any number of dimensions is valid for the set as long as one uses the same number of dimensions for the coordinate subspaces and projections.
It is the built-in symmetry of the Cartesian coordinate system where coordinates are orthogonal vectors of unit length in flat Euclidean space that allows this generalization to apply so broadly.
Non-Euclidean geometry.
The Pythagorean theorem is derived from the axioms of Euclidean geometry, and in fact, the Pythagorean theorem given above does not hold in a non-Euclidean geometry. (The Pythagorean theorem has been shown, in fact, to be equivalent to Euclid's Parallel (Fifth) Postulate.)
In other words, in non-Euclidean geometry, the relation between the sides of a triangle must necessarily take a non-Pythagorean form. For example, in spherical geometry, all three sides of the right triangle (say "a", "b", and "c") bounding an octant of the unit sphere have length equal to π/2, and all its angles are right angles, which violates the Pythagorean theorem because 
Here two cases of non-Euclidean geometry are considered—spherical geometry and hyperbolic plane geometry; in each case, as in the Euclidean case for non-right triangles, the result replacing the Pythagorean theorem follows from the appropriate law of cosines.
However, the Pythagorean theorem remains true in hyperbolic geometry and elliptic geometry if the condition that the triangle be right is replaced with the condition that two of the angles sum to the third, say "A"+"B" = "C". The sides are then related as follows: the sum of the areas of the circles with diameters "a" and "b" equals the area of the circle with diameter "c".
Spherical geometry.
For any right triangle on a sphere of radius "R" (for example, if γ in the figure is a right angle), with sides "a", "b", "c", the relation between the sides takes the form:
This equation can be derived as a special case of the spherical law of cosines that applies to all spherical triangles:
By expressing the Maclaurin series for the cosine function as an asymptotic expansion with the remainder term in big O notation,
it can be shown that as the radius "R" approaches infinity and the arguments "a/R", "b/R", and "c/R" tend to zero, the spherical relation between the sides of a right triangle approaches the Euclidean form of the Pythagorean theorem. Substituting the asymptotic expansion for each of the cosines into the spherical relation for a right triangle yields
The constants "a"4, "b"4, and "c"4 have been absorbed into the big "O" remainder terms since they are independent of the radius "R". This asymptotic relationship can be further simplified by multiplying out the bracketed quantities, cancelling the ones, multiplying through by −2, and collecting all the error terms together:
After multiplying through by "R"2, the Euclidean Pythagorean relationship "c"2 = "a"2 + "b"2 is recovered in the limit as the radius "R" approaches infinity (since the remainder term tends to zero):
Hyperbolic geometry.
For a right triangle in hyperbolic geometry with sides "a", "b", "c" and with side "c" opposite a right angle, the relation between the sides takes the form:
where cosh is the hyperbolic cosine. This formula is a special form of the hyperbolic law of cosines that applies to all hyperbolic triangles:
with γ the angle at the vertex opposite the side "c".
By using the Maclaurin series for the hyperbolic cosine, , it can be shown that as a hyperbolic triangle becomes very small (that is, as "a", "b", and "c" all approach zero), the hyperbolic relation for a right triangle approaches the form of Pythagoras's theorem.
Differential geometry.
On an infinitesimal level, in three dimensional space, Pythagoras's theorem describes the distance between two infinitesimally separated points as:
with "ds" the element of distance and ("dx", "dy", "dz") the components of the vector separating the two points. Such a space is called a Euclidean space. However, in Riemannian geometry, a generalization of this expression useful for general coordinates (not just Cartesian) and general spaces (not just Euclidean) takes the form:
which is called the metric tensor. (Sometimes, by abuse of language, the same term is applied to the set of coefficients .) It may be a function of position, and often describes curved space. A simple example is Euclidean (flat) space expressed in curvilinear coordinates. For example, in polar coordinates:
History.
There is debate whether the Pythagorean theorem was discovered once, or many times in many places, and the date of first discovery is uncertain, as is the date of the first proof. According to Joran Friberg, a historian of mathematics, evidence indicates that the Pythagorean Theorem was well-known to the mathematicians of the First Babylonian Dynasty (20th to 16th centuries BC), which would have been over a thousand years before Pythagoras was born. Other sources, such as a book by Leon Lederman and Dick Teresi, mention that Pythagoras discovered the theorem, although Teresi subsequently stated that the Babylonians developed the theorem "at least fifteen hundred years before Pythagoras was 
born." The history of the theorem can be divided into four parts: knowledge of Pythagorean triples, knowledge of the relationship among the sides of a right triangle, knowledge of the relationships among adjacent angles, and proofs of the theorem within some deductive system.
Bartel Leendert van der Waerden (1903–1996) conjectured that Pythagorean triples were discovered algebraically by the Babylonians. Written between 2000 and 1786 BC, the Middle Kingdom Egyptian "Berlin Papyrus 6619" includes a problem whose solution is the Pythagorean triple 6:8:10, but the problem does not mention a triangle. The Mesopotamian tablet "Plimpton 322", written between 1790 and 1750 BC during the reign of Hammurabi the Great, contains many entries closely related to Pythagorean triples.
In India, the "Baudhayana Sulba Sutra", the dates of which are given variously as between the 8th and 2nd century BC, contains a list of Pythagorean triples discovered algebraically, a statement of the Pythagorean theorem, and a geometrical proof of the Pythagorean theorem for an isosceles right triangle.
The "Apastamba Sulba Sutra" (c. 600 BC) contains a numerical proof of the general Pythagorean theorem, using an area computation. Van der Waerden believed that "it was certainly based on earlier traditions". Boyer (1991) thinks the elements found in the "Śulba-sũtram" may be of Mesopotamian derivation.
With contents known much earlier, but in surviving texts dating from roughly the 1st century BC, the Chinese text "Zhou Bi Suan Jing" (周髀算经), ("The Arithmetical Classic of the Gnomon and the Circular Paths of Heaven") gives a reasoning for the Pythagorean theorem for the (3, 4, 5) triangle—in China it is called the "Gougu Theorem" (勾股定理). During the Han Dynasty (202 BC to 220 AD), Pythagorean triples appear in "The Nine Chapters on the Mathematical Art", together with a mention of right triangles. Some believe the theorem arose first in China, where it is alternatively known as the "Shang Gao Theorem" (商高定理), named after the Duke of Zhou's astronomer and mathematician, whose reasoning composed most of what was in the "Zhou Bi Suan Jing".
Pythagoras, whose dates are commonly given as 569–475 BC, used algebraic methods to construct Pythagorean triples, according to Proclus's commentary on Euclid. Proclus, however, wrote between 410 and 485 AD. According to Thomas L. Heath (1861–1940), no specific attribution of the theorem to Pythagoras exists in the surviving Greek literature from the five centuries after Pythagoras lived. However, when authors such as Plutarch and Cicero attributed the theorem to Pythagoras, they did so in a way which suggests that the attribution was widely known and undoubted. "Whether this formula is rightly attributed to Pythagoras personally, [...] one can safely assume that it belongs to the very oldest period of Pythagorean mathematics."
Around 400 BC, according to Proclus, Plato gave a method for finding Pythagorean triples that combined algebra and geometry. Around 300 BC, in Euclid's "Elements", the oldest extant axiomatic proof of the theorem is presented.
In popular culture.
The Pythagorean theorem has arisen in popular culture in a variety of ways.

</doc>
<doc id="1967137" url="https://en.wikipedia.org/wiki?curid=1967137" title="QCD vacuum">
QCD vacuum

The QCD vacuum is the vacuum state of quantum chromodynamics (QCD). It is an example of a "non-perturbative" vacuum state, characterized by infinitely many non-vanishing condensates such as the gluon condensate or the quark condensate. These condensates characterize the normal phase or the confined phase of quark matter.
Another field-theoretic vacuum is the QED vacuum of quantum electrodynamics.
Symmetries and symmetry breaking.
Symmetries of the QCD Lagrangian.
Like any relativistic quantum field theory, QCD enjoys Poincaré symmetry including the discrete symmetries CPT (each of which is realized). Apart from these space-time symmetries, it also has internal symmetries. Since QCD is an SU(3) gauge theory, it has local SU(3) gauge symmetry.
Since it has many flavours of quarks, it has approximate flavour and chiral symmetry. This approximation is said to involve the chiral limit of QCD. Of these chiral symmetries, the baryon number symmetry is exact. Some of the broken symmetries include the axial U(1) symmetry of the flavour group. This is broken by the chiral anomaly. The presence of instantons implied by this anomaly also breaks CP symmetry.
In summary, the QCD Lagrangian has the following symmetries:
The following classical symmetries are broken in the QCD Lagrangian:
Spontaneous symmetry breaking.
When the Hamiltonian of a system (or the Lagrangian) has a certain symmetry, but the ground state (i.e., the vacuum) does not, then one says that spontaneous symmetry breaking (SSB) has taken place.
A familiar example of SSB is in ferromagnetic materials. Microscopically, the material consists of atoms with a non-vanishing spin, each of which acts like a tiny bar magnet, i.e., a magnetic dipole. The Hamiltonian of the material, describing the interaction of neighbouring dipoles, is invariant under rotations. At high temperature, there is no magnetization of a large sample of the material. Then one says that the symmetry of the Hamiltonian is realized by the system. However, at low temperature, there could be an overall magnetization. This magnetization has a "preferred direction", since one can tell the north magnetic pole of the sample from the south magnetic pole. In this case, there is spontaneous symmetry breaking of the rotational symmetry of the Hamiltonian.
When a continuous symmetry is spontaneously broken, massless bosons appear, corresponding to the remaining symmetry. This is called the Goldstone phenomenon and the bosons are called Goldstone bosons.
Symmetries of the QCD vacuum.
The SU(Nf) × SU(Nf) chiral flavour symmetry of the QCD Lagrangian is broken in the vacuum state of the theory. The symmetry of the vacuum state is the diagonal SU(Nf) part of the chiral group. The diagnostic for this is the formation of a non-vanishing chiral condensate formula_1, where ψi is the quark field operator, and the flavour index i is summed. The Goldstone bosons of the symmetry breaking are the pseudoscalar mesons.
When Nf=2, i.e., only the u and d quarks are treated as massless, the three pions are the Goldstone bosons. When the s quark is also treated as massless, i.e., Nf=3, all eight pseudoscalar mesons of the quark model become Goldstone bosons. The actual masses of these mesons are obtained in chiral perturbation theory through an expansion in the (small) actual masses of the quarks.
In other phases of quark matter the full chiral flavour symmetry may be recovered, or broken in completely different ways.
Evidence: experimental consequences.
The evidence for QCD condensates comes from two eras, the pre-QCD era 1950–1973 and the post-QCD era, after 1974. The pre-QCD results established that the strong interactions vacuum contains a quark chiral condensate, while the post-QCD results established that the vacuum also contains a gluon condensate.
Pre-QCD: gradient coupling.
In the 1950s, there were many attempts to produce a field theory to describe the interactions of pions and nucleons. The obvious renormalizable interaction between the two objects is the Yukawa coupling to a pseudoscalar:
And this is clearly theoretically correct, since it is leading order and it takes all the symmetries into account. But it doesn't match experiment. The interaction that does couples the nucleons to the "gradient" of the pion field.
This is the gradient-coupling model. This interaction has a very different dependence on the energy of the pion—it vanishes at zero momentum.
This type of coupling means that a coherent state of low momentum pions barely interacts at all. This is a manifestation of an approximate symmetry, a shift symmetry of the pion field. The replacement
leaves the gradient coupling alone, but not the pseudoscalar coupling.
The modern explanation for the shift symmetry was first proposed by Yoichiro Nambu. The pion field is a Goldstone boson, and the shift symmetry is the lowest order approximation to moving along the flat directions.
Pre-QCD: Goldberger–Treiman relation.
There is a mysterious relationship between the strong interaction coupling of the pions to the nucleons, the coefficient g in the gradient coupling model, and the axial vector current coefficient of the nucleon which determines the weak decay rate of the neutron. The relation is
and it is obeyed to 10% accuracy.
The constant formula_6 is the coefficient that determines the neutron decay rate. It gives the normalization of the weak interaction matrix elements for the nucleon. On the other hand, the pion-nucleon coupling is a phenomenological constant describing the scattering of bound states of quarks and gluons.
The weak interactions are current-current interactions ultimately because they come from a nonabelian gauge theory. The Goldberger Treiman relation suggests that the pions for some reason interact as if they are related to the same symmetry current.
PCAC.
The phenomenon which gives rise to the Goldberger Treiman relation was called the "Partially Conserved Axial Current" hypothesis, or PCAC. Partially conserved is an archaic term for spontaneously broken, and the axial current is now called the chiral symmetry current.
The idea is that the symmetry current which performs axial rotations on the fundamental fields does not preserve the vacuum. This means that the current J applied to the vacuum produces particles. The particles must be scalars, otherwise the vacuum wouldn't be Lorentz invariant. By index matching, the matrix element is:
where formula_8 is the momentum carried by the created pion.
Since the divergence of the axial current operator is zero, we must have
Hence the pions are massless, formula_10, in accordance with Goldstone's theorem.
Now if the scattering matrix element is considered, we have
Up to a momentum factor, which is the gradient in the coupling, it takes the same form as the axial current turning a neutron into a proton in the current-current form of the weak interaction.
Pre-QCD: soft pion emission.
Extensions of the PCAC ideas allowed Steven Weinberg to calculate the amplitudes for collisions which emit low energy pions from the amplitude for the same process with no pions. The amplitudes are those given by acting with symmetry currents on the external particles of the collision.
These successes established the basic properties of the strong interaction vacuum well before QCD.
Pseudo-Goldstone bosons.
Experimentally it is seen that the masses of the octet of pseudoscalar mesons is very much lighter than the next lightest states; i.e., the octet of vector mesons (such as the rho). The most convincing evidence for SSB of the chiral flavour symmetry of QCD is the appearance of these pseudo-Goldstone bosons. These would have been strictly massless in the chiral limit. There is convincing demonstration that the observed masses are compatible with chiral perturbation theory. The internal consistency of this argument is further checked by lattice QCD computations which allow one to vary the quark mass and check that the variation of the pseudoscalar masses with the quark mass is as required by chiral perturbation theory.
The η'.
This pattern of SSB solves one of the earlier "mysteries" of the quark model, where all the pseudoscalar mesons should have been of nearly the same mass. Since "N"f = 3, there should have been nine of these. However, one (the SU(3) singlet η') has quite a larger mass than the SU(3) octet. In the quark model, this has no natural explanation— a mystery named the η−η' mass splitting (the η is one member of the octet, which should have been degenerate in mass with the η').
In QCD, one realizes that the η' is associated with the axial U(1) which is explicitly broken through the chiral anomaly, and thus its mass is not "protected" to be small, like that of the η. The η-η' mass splitting can be explained
 through the 't Hooft instanton mechanism, whose 1/N realization is also known as Witten-Veneziano mechanism.
Current algebra and QCD sum rules.
PCAC and current algebra also provide evidence for this pattern of SSB. Direct estimates of the chiral condensate also come from such analysis.
Another method of analysis of correlation functions in QCD is through an operator product expansion (OPE). This writes the vacuum expectation value of a non-local operator as a sum over VEVs of local operators, i.e., condensates. The value of the correlation function then dictates the values of the condensates. Analysis of many separate correlation functions gives consistent results for several condensates, including the gluon condensate, the quark condensate, and many mixed and higher order condensates. In particular one obtains
Here G refers to the gluon field tensor, ψ to the quark field, and g to the QCD coupling.
These analyses are being refined further through improved sum rule estimates and direct estimates in lattice QCD. They provide the "raw data" which must be explained by models of the QCD vacuum.
Models of the QCD vacuum.
A full solution of QCD would automatically give a full description of the vacuum, confinement and the hadron spectrum. Lattice QCD is making rapid progress towards providing the solution as a systematically improvable numerical computation. However, approximate models of the QCD vacuum remain useful in more restricted domains. The purpose of these models is to make quantitative sense of some set of condensates and hadron properties such as masses and form factors.
This section is devoted to models. Opposed to these are systematically improvable computational procedures such as large N QCD and lattice QCD, which are described in their own articles.
The Savvidy vacuum, instabilities and structure.
The Savvidy vacuum is a model of the QCD vacuum which at a basic level is a statement that it cannot be the conventional Fock vacuum empty of particles and fields. In 1977, George Savvidy showed that the QCD vacuum with zero field strength is unstable, and decays into a state with a calculable non vanishing value of the field. Since condensates are scalar, it seems like a good first approximation that the vacuum contains some non-zero but homogeneous field which gives rise to these condensates. This would then be a more complicated version of the Higgs mechanism. However, Stanley Mandelstam showed that a homogeneous vacuum field is also unstable. The instability of a homogeneous gluon field was argued by Niels Kjær Nielsen and Poul Olesen in their 1978 paper. These arguments suggest that the scalar condensates are an effective long-distance description of the vacuum, and at short distances, below the QCD scale, the vacuum may have structure.
The dual superconducting model.
In a type II superconductor, electric charges condense into Cooper pairs. As a result, magnetic flux is squeezed into tubes. In the dual superconductor picture of the QCD vacuum, chromomagnetic monopoles condense into dual Cooper pairs, causing chromoelectric flux to be squeezed into tubes. As a result, confinement and the "string picture" of hadrons follows. This dual superconductor picture is due to Gerard 't Hooft and Stanley Mandelstam. 't Hooft showed further that an Abelian projection of a non-Abelian gauge theory contains magnetic monopoles.
While the vortices in a type II superconductor are neatly arranged into a hexagonal or occasionally square lattice, as is reviewed in Olesen's 1980 seminar one may expect a much more complicated and possibly dynamical structure in QCD. For example, nonabelian Abrikosov-Nielsen-Olesen vortices may vibrate wildly or be knotted.
String models.
String models of confinement and hadrons have a long history. They were first invented to explain certain aspects of crossing symmetry in the scattering of two mesons. They were also found to be useful in the description of certain properties of the Regge trajectory of the hadrons. These early developments took on a life of their own called the dual resonance model (later renamed string theory). However, even after the development of QCD string models continued to play a role in the physics of strong interactions. These models are called "non-fundamental strings" or QCD strings, since they should be derived from QCD, as they are, in certain approximations such as the strong coupling limit of lattice QCD.
The model states that the colour electric flux between a quark and an antiquark collapses into a string, rather than spreading out into a Coulomb field as the normal electric flux does. This string also obeys a different force law. It behaves as if the string had constant tension, so that separating out the ends (quarks) would give a potential energy increasing linearly with the separation. When the energy is higher than that of a meson, the string breaks and the two new ends become a quark-antiquark pair, thus describing the creation of a meson. Thus confinement is incorporated naturally into the model.
In the form of the Lund model Monte Carlo program, this picture has had remarkable success in explaining experimental data collected in electron-electron and hadron-hadron collisions.
Bag models.
Strictly, these models are not models of the QCD vacuum, but of physical single particle quantum states — the hadrons. The model proposed originally in 1974 by A. Chodos "et al." 
consists of inserting a quark model in a "perturbative vacuum" inside a volume of space called a bag. Outside this bag is the real QCD vacuum, whose effect is taken into account through the difference between energy density of the true QCD vacuum and the perturbative vacuum (bag constant "B") and boundary conditions imposed on the quark wave functions and the gluon field. The hadron spectrum is obtained by solving the Dirac equation for quarks and the Yang–Mills equations for gluons. 
The wave functions of the quarks satisfy the boundary conditions of a fermion in an infinitely deep potential well of scalar type with respect to the Lorentz group. 
The boundary conditions for the gluon field are those of the dual color superconductor. The role of such a superconductor is attributed to the physical vacuum of QCD. Bag models strictly prohibit the existence of open color (free quarks, free gluons, etc.) and lead in particular to string models of hadrons.
The chiral bag model couples the axial vector current formula_16 of the quarks at the bag boundary to a pionic field outside of the bag. In the most common formulation, the chiral bag model basically replaces the interior of the skyrmion with the bag of quarks. Very curiously, most physical properties of the nucleon become mostly insensitive to the bag radius. Prototypically, the baryon number of the chiral bag remains an integer, independent of bag radius: the exterior baryon number is identified with the topological winding number density of the Skyrme soliton, while the interior baryon number consists of the valence quarks (totaling to one) plus the spectral asymmetry of the quark eigenstates in the bag. The spectral asymmetry is just the vacuum expectation value formula_17 summed over all of the quark eigenstates in the bag. Other values, such as the total mass and the axial coupling constant formula_18, are not precisely invariant like the baryon number, but are mostly insensitive to the bag radius, as long as the bag radius is kept below the nucleon diameter. Because the quarks are treated as free quarks inside the bag, the radius-independence in a sense validates the idea of asymptotic freedom.
Instanton ensemble.
Another view states that BPST-like instantons play an important role in the vacuum structure of QCD. These instantons were discovered in 1975 by Belavin, Polyakov, Schwartz and Tyupkin as topologically stable solutions to the Yang-Mills field equations. They represent tunneling transitions from one vacuum state to another. These instantons are indeed found in lattice calculations. The first computations performed with instantons used the dilute gas approximation. The results obtained did not solve the infrared problem of QCD, making many physicists turn away from instanton physics. Later, though, an instanton liquid model was proposed, turning out to be more promising an approach.
The dilute instanton gas model departs from the supposition that the QCD vacuum consists of a gas of BPST-like instantons. Although only the solutions with one or few instantons (or anti-instantons) are known exactly, a dilute gas of instantons and anti-instantons can be approximated by considering a superposition of one-instanton solutions at great distances from one another. 't Hooft calculated the effective action for such an ensemble, and he found an infrared divergence for big instantons, meaning that an infinite amount of infinitely big instantons would populate the vacuum.
Later, an instanton liquid model was studied. This model starts from the assumption that an ensemble of instantons cannot be described by a mere sum of separate instantons. Various models have been proposed, introducing interactions between instantons or using variational methods (like the "valley approximation") endeavoring to approximate the exact multi-instanton solution as closely as possible. Many phenomenological successes have been reached. Whether an instanton liquid can explain confinement in 3+1 dimensional QCD is not known, but many physicists think that it is unlikely.
Center vortex picture.
A more recent picture of the QCD vacuum is one in which center vortices play an important role. These vortices are topological defects carrying a center element as charge. These vortices are usually studied using lattice simulations, and it has been found that the behavior of the vortices is closely linked with the confinement-deconfinement phase transition: in the confining phase vortices percolate and fill the space-time volume, in the deconfining phase they are much suppressed. Also it has been shown that the string tension vanished upon removal of center vortices from the simulations, hinting at an important role for center vortices.

</doc>
<doc id="25175" url="https://en.wikipedia.org/wiki?curid=25175" title="Quadratic equation">
Quadratic equation

 
In elementary algebra, a quadratic equation (from the Latin "quadratus" for "square") is any equation having the form 
where represents an unknown, and , , and represent known numbers such that is not equal to . If , then the equation is linear, not quadratic. The numbers , , and are the "coefficients" of the equation, and may be distinguished by calling them, respectively, the "quadratic coefficient", the "linear coefficient" and the "constant" or "free term". 
Because the quadratic equation involves only one unknown, it is called "univariate". The quadratic equation only contains powers of that are non-negative integers, and therefore it is a polynomial equation, and in particular it is a second degree polynomial equation since the greatest power is two.
Quadratic equations can be solved by a process known in American English as factoring and in other varieties of English as "factorising", by completing the square, by using the quadratic formula, or by graphing. Solutions to problems equivalent to the quadratic equation were known as early as 2000 BC.
Solving the quadratic equation.
A quadratic equation with real or complex coefficients has two solutions, called "roots". These two solutions may or may not be distinct, and they may or may not be real.
Factoring by inspection.
It may be possible to express a quadratic equation as a product . In some cases, it is possible, by simple inspection, to determine values of "p", "q", "r," and "s" that make the two forms equivalent to one another. If the quadratic equation is written in the second form, then the "Zero Factor Property" states that the quadratic equation is satisfied if or . Solving these two linear equations provides the roots of the quadratic.
For most students, factoring by inspection is the first method of solving quadratic equations to which they are exposed. If one is given a quadratic equation in the form , the sought factorization has the form , and one has to find two numbers and that add up to and whose product is (this is sometimes called "Vieta's rule" and is related to Vieta's formulas). As an example, factors as .The more general case where does not equal can require a considerable effort in trial and error guess-and-check, assuming that it can be factored at all by inspection. 
Except for special cases such as where or , factoring by inspection only works for quadratic equations that have rational roots. This means that the great majority of quadratic equations that arise in practical applications cannot be solved by factoring by inspection.
Completing the square.
The process of completing the square makes use of the algebraic identity
which represents a well-defined algorithm that can be used to solve any quadratic equation. Starting with a quadratic equation in standard form, 
We illustrate use of this algorithm by solving 
The plus-minus symbol "±" indicates that both and are solutions of the quadratic equation.
Quadratic formula and its derivation.
Completing the square can be used to derive a general formula for solving quadratic equations, called the quadratic formula. The mathematical proof will now be briefly summarized. It can easily be seen, by polynomial expansion, that the following equation is equivalent to the quadratic equation:
Taking the square root of both sides, and isolating , gives:
Some sources, particularly older ones, use alternative parameterizations of the quadratic equation such as or  , where has a magnitude one half of the more common one, possibly with opposite sign. These result in slightly different forms for the solution, but are otherwise equivalent.
A number of alternative derivations can be found in the literature. These proofs are simpler than the standard completing the square method, represent interesting applications of other frequently used techniques in algebra, or offer insight into other areas of mathematics.
A lesser known quadratic formula, as used in Muller's method, and which can be found from Vieta's formulas, provides the same roots via the equation:
One property of this form is that it yields one valid root when , while the other root contains division by zero, because when , the quadratic equation becomes a linear equation, which has one root. By contrast, in this case the more common formula has division by zero in both cases.
Reduced quadratic equation.
It is sometimes convenient to reduce a quadratic equation so that its leading coefficient is one. This is done by dividing both sides by "a", which is always possible since "a" is non-zero. This produces the "reduced quadratic equation":
where "p" = "b"/"a" and "q" = "c"/"a". This monic equation has the same solutions as the original.
The quadratic formula for the solutions of the reduced quadratic equation, written in terms of its coefficients, is:
Discriminant.
In the quadratic formula, the expression underneath the square root sign is called the "discriminant" of the quadratic equation, and is often represented using an upper case or an upper case Greek delta:
A quadratic equation with "real" coefficients can have either one or two distinct real roots, or two distinct complex roots. In this case the discriminant determines the number and nature of the roots. There are three cases:
Thus the roots are distinct if and only if the discriminant is non-zero, and the roots are real if and only if the discriminant is non-negative.
Geometric interpretation.
The function is the quadratic function. The graph of any quadratic function has the same general shape, which is called a parabola. The location and size of the parabola, and how it opens, depend on the values of , , and . As shown in Figure 1, if , the parabola has a minimum point and opens upward. If , the parabola has a maximum point and opens downward. The extreme point of the parabola, whether minimum or maximum, corresponds to its vertex. The "-coordinate" of the vertex will be located at formula_18, and the "-coordinate" of the vertex may be found by substituting this "-value" into the function. The "-intercept" is located at the point .
The solutions of the quadratic equation correspond to the roots of the function , since they are the values of for which . As shown in Figure 2, if , , and are real numbers and the domain of is the set of real numbers, then the roots of are exactly the -coordinates of the points where the graph touches the -axis. As shown in Figure 3, if the discriminant is positive, the graph touches the -axis at two points; if zero, the graph touches at one point; and if negative, the graph does not touch the -axis.
Quadratic factorization.
The term
is a factor of the polynomial
if and only if is a root of the quadratic equation
It follows from the quadratic formula that
In the special case where the quadratic has only one distinct root ("i.e." the discriminant is zero), the quadratic polynomial can be factored as
Graphing for real roots.
For most of the 20th century, graphing was rarely mentioned as a method for solving quadratic equations in high school or college algebra texts. Students learned to solve quadratic equations by factoring, completing the square, and applying the quadratic formula. Recently, graphing calculators have become common in schools and graphical methods have started to appear in textbooks, but they are generally not highly emphasized.
Being able to use a graphing calculator to solve a quadratic equation requires the ability to produce a graph of , the ability to scale the graph appropriately to the dimensions of the graphing surface, and the recognition that when , is a solution to the equation. The skills required to solve a quadratic equation on a calculator are in fact applicable to finding the real roots of any arbitrary function.
Since an arbitrary function may cross the -axis at multiple points, graphing calculators generally require one to identify the desired root by positioning a cursor at a "guessed" value for the root. (Some graphing calculators require bracketing the root on both sides of the zero.) The calculator then proceeds, by an iterative algorithm, to refine the estimated position of the root to the limit of calculator accuracy.
Avoiding loss of significance.
Although the quadratic formula provides what in principle should be an exact solution, it does not, from a numerical analysis standpoint, provide a completely stable method for evaluating the roots of a quadratic equation. If the two roots of the quadratic equation vary greatly in absolute magnitude, will be very close in magnitude to formula_24, and the subtraction of two nearly equal numbers will cause loss of significance or catastrophic cancellation. A second form of cancellation can occur between the terms and of the discriminant, which can lead to loss of up to half of correct significant figures.
Examples and applications.
The golden ratio is found as the solution of the quadratic equation formula_25
The equations of the circle and the other conic sections—ellipses, parabolas, and hyperbolas—are quadratic equations in two variables.
Given the cosine or sine of an angle, finding the cosine or sine of the angle that is half as large involves solving a quadratic equation.
The process of simplifying expressions involving the square root of an expression involving the square root of another expression involves finding the two solutions of a quadratic equation.
Descartes' theorem states that for every four kissing (mutually tangent) circles, their radii satisfy a particular quadratic equation.
The equation given by Fuss' theorem, giving the relation among the radius of a bicentric quadrilateral's inscribed circle, the radius of its circumscribed circle, and the distance between the centers of those circles, can be expressed as a quadratic equation for which the distance between the two circles' centers in terms of their radii is one of the solutions. The other solution of the same equation in terms of the relevant radii gives the distance between the circumscribed circle's center and the center of the excircle of an ex-tangential quadrilateral.
History.
Babylonian mathematicians, as early as 2000 BC (displayed on Old Babylonian clay tablets) could solve problems relating the areas and sides of rectangles. There is evidence dating this algorithm as far back as the Third Dynasty of Ur. In modern notation, the problems typically involved solving a pair of simultaneous equations of the form:
which are equivalent to the equation:
The steps given by Babylonian scribes for solving the above rectangle problem were as follows:
Geometric methods were used to solve quadratic equations in Babylonia, Egypt, Greece, China, and India. The Egyptian Berlin Papyrus, dating back to the Middle Kingdom (2050 BC to 1650 BC), contains the solution to a two-term quadratic equation. In the Indian Sulba Sutras, circa 8th century BC, quadratic equations of the form and were explored using geometric methods. Babylonian mathematicians from circa 400 BC and Chinese mathematicians from circa 200 BC used geometric methods of dissection to solve quadratic equations with positive roots. Rules for quadratic equations were given in the "The Nine Chapters on the Mathematical Art", a Chinese treatise on mathematics. These early geometric methods do not appear to have had a general formula. Euclid, the Greek mathematician, produced a more abstract geometrical method around 300 BC. With a purely geometric approach Pythagoras and Euclid created a general procedure to find solutions of the quadratic equation. In his work "Arithmetica", the Greek mathematician Diophantus solved the quadratic equation, but giving only one root, even when both roots were positive.
In 628 AD, Brahmagupta, an Indian mathematician, gave the first explicit (although still not completely general) solution of the quadratic equation as follows: "To the absolute number multiplied by four times the [coefficient of the] square, add the square of the [coefficient of the] middle term; the square root of the same, less the [coefficient of the] middle term, being divided by twice the [coefficient of the] square is the value." ("Brahmasphutasiddhanta", Colebrook translation, 1817, page 346) This is equivalent to:
The "Bakhshali Manuscript" written in India in the 7th century AD contained an algebraic formula for solving quadratic equations, as well as quadratic indeterminate equations (originally of type Muhammad ibn Musa al-Khwarizmi (Persia, 9th century), inspired by Brahmagupta, developed a set of formulas that worked for positive solutions. Al-Khwarizmi goes further in providing a full solution to the general quadratic equation, accepting one or two numerical answers for every quadratic equation, while providing geometric proofs in the process. He also described the method of completing the square and recognized that the discriminant must be positive, which was proven by his contemporary 'Abd al-Hamīd ibn Turk (Central Asia, 9th century) who gave geometric figures to prove that if the discriminant is negative, a quadratic equation has no solution. While al-Khwarizmi himself did not accept negative solutions, later Islamic mathematicians that succeeded him accepted negative solutions, as well as irrational numbers as solutions. Abū Kāmil Shujā ibn Aslam (Egypt, 10th century) in particular was the first to accept irrational numbers (often in the form of a square root, cube root or fourth root) as solutions to quadratic equations or as coefficients in an equation. The 9th century Indian mathematician Sridhara wrote down rules for solving quadratic equations.
The Jewish mathematician Abraham bar Hiyya Ha-Nasi (12th century, Spain) authored the first European book to include the full solution to the general quadratic equation. His solution was largely based on Al-Khwarizmi's work. The writing of the Chinese mathematician Yang Hui (1238–1298 AD) is the first known one in which quadratic equations with negative coefficients of 'x' appear, although he attributes this to the earlier Liu Yi. By 1545 Gerolamo Cardano compiled the works related to the quadratic equations. The quadratic formula covering all cases was first obtained by Simon Stevin in 1594. In 1637 René Descartes published "La Géométrie" containing the quadratic formula in the form we know today. The first appearance of the general solution in the modern mathematical literature appeared in an 1896 paper by Henry Heaton.
Advanced topics.
Alternative methods of root calculation.
Vieta's formulas.
Vieta's formulas give a simple relation between the roots of a polynomial and its coefficients. In the case of the quadratic polynomial, they take the following form:
and
These results follow immediately from the relation:
which can be compared term by term with
The first formula above yields a convenient expression when graphing a quadratic function. Since the graph is symmetric with respect to a vertical line through the vertex, when there are two real roots the vertex's -coordinate is located at the average of the roots (or intercepts). Thus the -coordinate of the vertex is given by the expression
The -coordinate can be obtained by substituting the above result into the given quadratic equation, giving
As a practical matter, Vieta's formulas provide a useful method for finding the roots of a quadratic in the case where one root is much smaller than the other. If , then , and we have the estimate:
The second Vieta's formula then provides:
These formulas are much easier to evaluate than the quadratic formula under the condition of one large and one small root, because the quadratic formula evaluates the small root as the difference of two very nearly equal numbers (the case of large ), which causes round-off error in a numerical evaluation. Figure 5 shows the difference between (i) a direct evaluation using the quadratic formula (accurate when the roots are near each other in value) and (ii) an evaluation based upon the above approximation of Vieta's formulas (accurate when the roots are widely spaced). As the linear coefficient increases, initially the quadratic formula is accurate, and the approximate formula improves in accuracy, leading to a smaller difference between the methods as increases. However, at some point the quadratic formula begins to lose accuracy because of round off error, while the approximate method continues to improve. Consequently the difference between the methods begins to increase as the quadratic formula becomes worse and worse.
This situation arises commonly in amplifier design, where widely separated roots are desired to ensure a stable operation (see step response).
Trigonometric solution.
In the days before calculators, people would use mathematical tables—lists of numbers showing the results of calculation with varying arguments—to simplify and speed up computation. Tables of logarithms and trigonometric functions were common in math and science textbooks. Specialized tables were published for applications such as astronomy, celestial navigation and statistics. Methods of numerical approximation existed, called prosthaphaeresis, that offered shortcuts around time-consuming operations such as multiplication and taking powers and roots. Astronomers, especially, were concerned with methods that could speed up the long series of computations involved in celestial mechanics calculations.
It is within this context that we may understand the development of means of solving quadratic equations by the aid of trigonometric substitution. Consider the following alternate form of the quadratic equation,
[1]   formula_38
where the sign of the ± symbol is chosen so that and may both be positive. By substituting
[2]   formula_39
and then multiplying through by , we obtain
[3]   formula_40
Introducing functions of and rearranging, we obtain
[4]   formula_41
[5]   formula_42
where the subscripts and correspond, respectively, to the use of a negative or positive sign in equation [1]. Substituting the two values of or found from equations [4] or [5] into [2] gives the required roots of [1]. Complex roots occur in the solution based on equation [5] if the absolute value of exceeds unity. The amount of effort involved in solving quadratic equations using this mixed trigonometric and logarithmic table look-up strategy was two-thirds the effort using logarithmic tables alone. Calculating complex roots would require using a different trigonometric form.
Solution for complex roots in polar coordinates.
If the quadratic equation formula_1 with real coefficients has two complex roots—the case where formula_52 requiring "a" and "c" to have the same sign as each other—then the solutions for the roots can be expressed in polar form as
where formula_54 and formula_55
Geometric solution.
The quadratic equation may be solved geometrically in a number of ways. One way is via Lill's method. The three coefficients , , are drawn with right angles between them as in SA, AB, and BC in Figure 6. A circle is drawn with the start and end point SC as a diameter. If this cuts the middle line AB of the three then the equation has a solution, and the solutions are given by negative of the distance along this line from A divided by the first coefficient or SA. If is the coefficients may be read off directly. Thus the solutions in the diagram are −AX1/SA and −AX2/SA.
The Carlyle circle, named after Thomas Carlyle, has the property that the solutions of the quadratic equation are the horizontal coordinates of the intersections of the circle with the horizontal axis. Carlyle circles have been used to develop ruler-and-compass constructions of regular polygons.
Generalization of quadratic equation.
The formula and its derivation remain correct if the coefficients , and are complex numbers, or more generally members of any field whose characteristic is not . (In a field of characteristic 2, the element is zero and it is impossible to divide by it.)
The symbol
in the formula should be understood as "either of the two elements whose square is , if such elements exist". In some fields, some elements have no square roots and some have two; only zero has just one square root, except in fields of characteristic . Even if a field does not contain a square root of some number, there is always a quadratic extension field which does, so the quadratic formula will always make sense as a formula in that extension field.
Characteristic 2.
In a field of characteristic , the quadratic formula, which relies on being a unit, does not hold. Consider the monic quadratic polynomial
over a field of characteristic . If , then the solution reduces to extracting a square root, so the solution is
and there is only one root since
In summary,
See quadratic residue for more information about extracting square roots in finite fields.
In the case that , there are two distinct roots, but if the polynomial is irreducible, they cannot be expressed in terms of square roots of numbers in the coefficient field. Instead, define the 2-root of to be a root of the polynomial , an element of the splitting field of that polynomial. One verifies that is also a root. In terms of the 2-root operation, the two roots of the (non-monic) quadratic are
and
For example, let denote a multiplicative generator of the group of units of , the Galois field of order four (thus and are roots of over . Because , is the unique solution of the quadratic equation . On the other hand, the polynomial is irreducible over , but it splits over , where it has the two roots and , where is a root of in .
This is a special case of Artin–Schreier theory.

</doc>
<doc id="251478" url="https://en.wikipedia.org/wiki?curid=251478" title="Quadratic form">
Quadratic form

In mathematics, a quadratic form is a homogeneous polynomial of degree two in a number of variables. For example,
is a quadratic form in the variables "x" and "y".
Quadratic forms occupy a central place in various branches of mathematics, including number theory, linear algebra, group theory (orthogonal group), differential geometry (Riemannian metric), differential topology (intersection forms of four-manifolds), and Lie theory (the Killing form).
Introduction.
Quadratic forms are homogeneous quadratic polynomials in "n" variables. In the cases of one, two, and three variables they are called unary, binary, and ternary and have the following explicit form:
where "a", ..., "f" are the coefficients. Note that quadratic functions, such as in the one variable case, are not quadratic forms, as they are typically not homogeneous (unless "b" and "c" are both 0).
The theory of quadratic forms and methods used in their study depend in a large measure on the nature of the coefficients, which may be real or complex numbers, rational numbers, or integers. In linear algebra, analytic geometry, and in the majority of applications of quadratic forms, the coefficients are real or complex numbers. In the algebraic theory of quadratic forms, the coefficients are elements of a certain field. In the arithmetic theory of quadratic forms, the coefficients belong to a fixed commutative ring, frequently the integers Z or the "p"-adic integers Z"p". Binary quadratic forms have been extensively studied in number theory, in particular, in the theory of quadratic fields, continued fractions, and modular forms. The theory of integral quadratic forms in "n" variables has important applications to algebraic topology.
Using homogeneous coordinates, a non-zero quadratic form in "n" variables defines an ("n"−2)-dimensional quadric in the ("n"−1)-dimensional projective space. This is a basic construction in projective geometry. In this way one may visualize 3-dimensional real quadratic forms as conic sections.
A closely related notion with geometric overtones is a quadratic space, which is a pair ("V","q"), with "V" a vector space over a field "K", and a quadratic form on "V". An example is given by the three-dimensional Euclidean space and the square of the Euclidean norm expressing the distance between a point with coordinates ("x","y","z") and the origin:
History.
The study of particular quadratic forms, in particular the question of whether a given integer can be the value of a quadratic form over the integers, dates back many centuries. One such case is Fermat's theorem on sums of two squares, which determines when an integer may be expressed in the form , where "x", "y" are integers. This problem is related to the problem of finding Pythagorean triples, which appeared in the second millennium B.C.
In 628, the Indian mathematician Brahmagupta wrote "Brāhmasphuṭasiddhānta" which includes, among many other things, a study of equations of the form . In particular he considered what is now called Pell's equation, , and found a method for its solution. In Europe this problem was studied by Brouncker, Euler and Lagrange.
In 1801 Gauss published "Disquisitiones Arithmeticae," a major portion of which was devoted to a complete theory of binary quadratic forms over the integers. Since then, the concept has been generalized, and the connections with quadratic number fields, the modular group, and other areas of mathematics have been further elucidated.
Real quadratic forms.
Any "n"×"n" real symmetric matrix "A" determines a quadratic form "q""A" in "n" variables by the formula
Conversely, given a quadratic form in "n" variables, its coefficients can be arranged into an "n"×"n" symmetric matrix. One of the most important questions in the theory of quadratic forms is how much can one simplify a quadratic form "q" by a homogeneous linear change of variables. A fundamental theorem due to Jacobi asserts that "q" can be brought to a diagonal form
so that the corresponding symmetric matrix is diagonal, and this is even possible to accomplish with a change of variables given by an orthogonal matrix – in this case the coefficients λ1, λ2, ..., λ"n" are in fact determined uniquely up to a permutation. If the change of variables is given by an invertible matrix, not necessarily orthogonal, then the coefficients λ"i" can be made to be 0,1, and −1. Sylvester's law of inertia states that the numbers of 1 and −1 are invariants of the quadratic form, in the sense that any other diagonalization will contain the same number of each. The signature of the quadratic form is the triple ("n"0, "n"+, "n"−) where "n"0 is the number 0s and "n"± is the number of ±1s. Sylvester's law of inertia shows that this is a well-defined quantity attached to the quadratic form. The case when all λ"i" have the same sign is especially important: in this case the quadratic form is called positive definite (all 1) or negative definite (all −1); if none of the terms are 0 then the form is called ; this includes positive definite, negative definite, and indefinite (a mix of 1 and −1); equivalently, a nondegenerate quadratic form is one whose associated symmetric form is a nondegenerate "bilinear" form. A real vector space with an indefinite nondegenerate quadratic form of index ("p", "q") ("p" 1s, "q" −1s) is often denoted as R"p","q" particularly in the physical theory of space-time.
The discriminant of a quadratic form, concretely the class of the determinant of a representing matrix in "K"/("K*")2 (up to non-zero squares) can also be defined, and for a real quadratic form is a cruder invariant than signature, taking values of only “positive, zero, or negative”. Zero corresponds to degenerate, while for a non-degenerate form it is the parity of the number of negative coefficients, formula_8
These results are reformulated in a different way below.
Let "q" be a quadratic form defined on an "n"-dimensional real vector space. Let "A" be the matrix of the quadratic form "q" in a given basis. This means that "A" is a symmetric "n"×"n" matrix such that
where "x" is the column vector of coordinates of "v" in the chosen basis. Under a change of basis, the column "x" is multiplied on the left by an "n"×"n" invertible matrix "S", and the symmetric square matrix "A" is transformed into another symmetric square matrix "B" of the same size according to the formula
Any symmetric matrix "A" can be transformed into a diagonal matrix
by a suitable choice of an orthogonal matrix "S", and the diagonal entries of "B" are uniquely determined — this is Jacobi's theorem. If "S" is allowed to be any invertible matrix then "B" can be made to have only 0,1, and −1 on the diagonal, and the number of the entries of each type ("n"0 for 0, "n"+ for 1, and "n"− for −1) depends only on "A". This is one of the formulations of Sylvester's law of inertia and the numbers "n"+ and "n"− are called the positive and negative indices of inertia. Although their definition involved a choice of basis and consideration of the corresponding real symmetric matrix "A", Sylvester's law of inertia means that they are invariants of the quadratic form "q".
The quadratic form "q" is positive definite (resp., negative definite) if "q"("v") > 0 (resp., "q"("v") < 0) for every nonzero vector "v". When "q"("v") assumes both positive and negative values, "q" is an indefinite quadratic form. The theorems of Jacobi and Sylvester show that any positive definite quadratic form in "n" variables can be brought to the sum of "n" squares by a suitable invertible linear transformation: geometrically, there is only "one" positive definite real quadratic form of every dimension. Its isometry group is a "compact" orthogonal group O("n"). This stands in contrast with the case of indefinite forms, when the corresponding group, the indefinite orthogonal group O("p", "q"), is non-compact. Further, the isometry groups of "Q" and −"Q" are the same (O("p", "q") ≈ O("q", "p")), but the associated Clifford algebras (and hence Pin groups) are different.
Definitions.
An "n"-ary quadratic form over a field "K" is a homogeneous polynomial of degree 2 in "n" variables with coefficients in "K":
This formula may be rewritten using matrices: let "x" be the column vector with components "x"1, ..., "x""n" and be the "n"×"n" matrix over "K" whose entries are the coefficients of "q". Then
Two "n"-ary quadratic forms φ and ψ over "K" are equivalent if there exists a nonsingular linear transformation such that
"Let us assume that the characteristic of "K" is different from 2."
(The theory of quadratic forms over a field of characteristic 2 has important differences and many definitions and theorems have to be modified.) The coefficient matrix "A" of "q" may be replaced by the symmetric matrix with the same quadratic form, so it may be assumed from the outset that "A" is symmetric. Moreover, a symmetric matrix "A" is uniquely determined by the corresponding quadratic form. Under an equivalence "C", the symmetric matrix "A" of φ and the symmetric matrix "B" of ψ are related as follows:
The associated bilinear form of a quadratic form "q" is defined by
Thus, "b""q" is a symmetric bilinear form over "K" with matrix "A". Conversely, any symmetric bilinear form "b" defines a quadratic form
and these two processes are the inverses of one another. As a consequence, over a field of characteristic not equal to 2, the theories of symmetric bilinear forms and of quadratic forms in "n" variables are essentially the same.
Quadratic spaces.
A quadratic form "q" in "n" variables over "K" induces a map from the "n"-dimensional coordinate space "K""n" into "K":
The map "Q" is a homogeneous function of degree 2, which means that it has the property that, for all "a" in "K" and "v" in "V":
When the characteristic of "K" is not 2, the map defined below is bilinear over "K":
This bilinear form "B" is symmetric, i.e. for all "x", "y" in "V", and it determines to "Q": for all "x" in "V".
When the characteristic of "K" is 2, so that 2 is not a unit, it is still possible to use a quadratic form to define a symmetric bilinear form . However, "Q"("x") can no longer be recovered from this "B"′ in the same way, since for all "x" (and is thus alternating). Alternately, there always exists a bilinear form "B"″ (not in general either unique or symmetric) such that .
The pair consisting of a finite-dimensional vector space "V" over "K" and a quadratic map from "V" to "K" is called a quadratic space, and "B" as defined here is the associated symmetric bilinear form of "Q". The notion of a quadratic space is a coordinate-free version of the notion of quadratic form. Sometimes, "Q" is also called a quadratic form.
Two "n"-dimensional quadratic spaces and are isometric if there exists an invertible linear transformation (isometry) such that
The isometry classes of "n"-dimensional quadratic spaces over "K" correspond to the equivalence classes of "n"-ary quadratic forms over "K".
Further definitions.
Two elements "v" and "w" of "V" are called orthogonal if . The kernel of a bilinear form "B" consists of the elements that are orthogonal to every element of "V". "Q" is non-singular if the kernel of its associated bilinear form is 0. If there exists a non-zero "v" in "V" such that , the quadratic form "Q" is isotropic, otherwise it is anisotropic. This terminology also applies to vectors and subspaces of a quadratic space. If the restriction of "Q" to a subspace "U" of "V" is identically zero, "U" is totally singular.
The orthogonal group of a non-singular quadratic form "Q" is the group of the linear automorphisms of "V" that preserve "Q", i.e. the group of isometries of ("V", "Q") into itself.
Equivalence of forms.
Every quadratic form "q" in "n" variables over a field of characteristic not equal to 2 is equivalent to a diagonal form
Such a diagonal form is often denoted by
Classification of all quadratic forms up to equivalence can thus be reduced to the case of diagonal forms.
Geometric meaning.
Using Cartesian coordinates in three dimensions, let formula_24, and let formula_25 be a symmetric 3-by-3 matrix. Then the geometric nature of the solution set of the equation formula_26 depends on the eigenvalues of the matrix formula_25.
If all eigenvalues of formula_25 are non-zero, then the solution set is an ellipsoid or a hyperboloid. If all the eigenvalues are positive, then it is an ellipsoid; if all the eigenvalues are negative, then it is an "imaginary ellipsoid" (we get the equation of an ellipsoid but with imaginary radii); if some eigenvalues are positive and some are negative, then it is a hyperboloid.
If there exist one or more eigenvalues formula_29, then the shape depends on the corresponding formula_30. If the corresponding formula_31, then the solution set is a paraboloid (either elliptic or hyperbolic); if the corresponding formula_32, then the dimension formula_33 degenerates and does not get into play, and the geometric meaning will be determined by other eigenvalues and other components of formula_34. When the solution set is a paraboloid, whether it is elliptic or hyperbolic is determined by whether all other non-zero eigenvalues are of the same sign: if they are, then it is elliptic; otherwise, it is hyperbolic.
Integral quadratic forms.
Quadratic forms over the ring of integers are called integral quadratic forms, whereas the corresponding modules are quadratic lattices (sometimes, simply lattices). They play an important role in number theory and topology.
An integral quadratic form has integer coefficients, such as "x"2 + "xy" + "y"2; equivalently, given a lattice Λ in a vector space "V" (over a field with characteristic 0, such as Q or R), a quadratic form "Q" is integral "with respect to" Λ if and only if it is integer-valued on Λ, meaning if .
This is the current use of the term; in the past it was sometimes used differently, as detailed below.
Historical use.
Historically there was some confusion and controversy over whether the notion of integral quadratic form should mean:
This debate was due to the confusion of quadratic forms (represented by polynomials) and symmetric bilinear forms (represented by matrices), and "twos out" is now the accepted convention; "twos in" is instead the theory of integral symmetric bilinear forms (integral symmetric matrices).
In "twos in", binary quadratic forms are of the form formula_35, represented by the symmetric matrix
this is the convention Gauss uses in Disquisitiones Arithmeticae.
In "twos out", binary quadratic forms are of the form formula_37, represented by the symmetric matrix
Several points of view mean that "twos out" has been adopted as the standard convention. Those include:
Universal quadratic forms.
An integral quadratic form whose image consists of all the positive integers is sometimes called "universal". Lagrange's four-square theorem shows that formula_39 is universal. Ramanujan generalized this to formula_40 and found 54 multisets {"a","b","c","d"} that can each generate all positive integers, namely,
There are also forms whose image consists of all but one of the positive integers. For example, {1,2,5,5} has 15 as the exception. Recently, the 15 and 290 theorems have completely characterized universal integral quadratic forms: if all coefficients are integers, then it represents all positive integers if and only if it represents all integers up through 290; if it has an integral matrix, it represents all positive integers if and only if it represents all integers up through 15.

</doc>
<doc id="187240" url="https://en.wikipedia.org/wiki?curid=187240" title="Quadratic function">
Quadratic function

 
In algebra, a quadratic function, a quadratic polynomial, a polynomial of degree 2, or simply a quadratic, is a polynomial function in one or more variables in which the highest-degree term is of the second degree. For example, a quadratic function in three variables "x", "y," and "z" contains exclusively terms "x"2, "y"2, "z"2, "xy", "xz", "yz", "x", "y", "z", and a constant:
with at least one of the coefficients "a, b, c, d, e," or "f" of the second-degree terms being non-zero.
A "univariate" (single-variable) quadratic function has the form
in the single variable "x". The graph of a univariate quadratic function is a parabola whose axis of symmetry is parallel to the -axis, as shown at right.
If the quadratic function is set equal to zero, then the result is a quadratic equation. The solutions to the univariate equation are called the roots of the univariate function.
The bivariate case in terms of variables "x" and "y" has the form
with at least one of "a, b, c" not equal to zero, and an equation setting this function equal to zero gives rise to a conic section (a circle or other ellipse, a parabola, or a hyperbola).
In general there can be an arbitrarily large number of variables, in which case the resulting surface is called a quadric, but the highest degree term must be of degree 2, such as "x"2, "xy", "yz", etc.
Etymology.
The adjective "quadratic" comes from the Latin word "quadrātum" ("square"). A term like is called a square in algebra because it is the area of a "square" with side .
In general, a prefix quadr(i)- indicates the number . Examples are quadrilateral and quadrant. "Quadratum" is the Latin word for square because a square has four sides.
Terminology.
Coefficients.
The coefficients of a polynomial are often taken to be real or complex numbers, but in fact, a polynomial may be defined over any ring.
Degree.
When using the term "quadratic polynomial", authors sometimes mean "having degree exactly 2", and sometimes "having degree at most 2". If the degree is less than 2, this may be called a "degenerate case". Usually the context will establish which of the two is meant.
Sometimes the word "order" is used with the meaning of "degree", e.g. a second-order polynomial.
Variables.
A quadratic polynomial may involve a single variable "x" (the univariate case), or multiple variables such as "x", "y", and "z" (the multivariate case).
The one-variable case.
Any single-variable quadratic polynomial may be written as 
where "x" is the variable, and "a", "b", and "c" represent the coefficients. In elementary algebra, such polynomials often arise in the form of a quadratic equation formula_5. The solutions to this equation are called the roots of the quadratic polynomial, and may be found through factorization, completing the square, graphing, Newton's method, or through the use of the quadratic formula. Each quadratic polynomial has an associated quadratic function, whose graph is a parabola.
Bivariate case.
Any quadratic polynomial with two variables may be written as 
where "x" and "y" are the variables and "a", "b", "c", "d", "e", and "f" are the coefficients. Such polynomials are fundamental to the study of conic sections, which are characterized by equating the expression for "f" ("x", "y") to zero. 
Similarly, quadratic polynomials with three or more variables correspond to quadric surfaces and hypersurfaces. In linear algebra, quadratic polynomials can be generalized to the notion of a quadratic form on a vector space.
Forms of a univariate quadratic function.
A univariate quadratic function can be expressed in three formats:
To convert the standard form to factored form, one needs only the quadratic formula to determine the two roots and . To convert the standard form to vertex form, one needs a process called completing the square. To convert the factored form (or vertex form) to standard form, one needs to multiply, expand and/or distribute the factors.
Graph of the univariate function.
Regardless of the format, the graph of a univariate quadratic function "f"("x")="ax"2+"bx+c" is a parabola (as shown at the right). Equivalently, this is the graph of the bivariate quadratic equation "y" = "ax"2+"bx+c".
The coefficient controls the degree of curvature of the graph; a larger magnitude of gives the graph a more closed (sharply curved) appearance.
The coefficients and together control the location of the axis of symmetry of the parabola (also the -coordinate of the vertex) which is at 
The coefficient controls the height of the parabola; more specifically, it is the height of the parabola where it intercepts the -axis.
Vertex.
The vertex of a parabola is the place where it turns; hence, it is also called the turning point. If the quadratic function is in vertex form, the vertex is . By the method of completing the square, one can turn the standard form
into
so the vertex of the parabola in standard form is
If the quadratic function is in factored form
the average of the two roots, i.e.,
is the -coordinate of the vertex, and hence the vertex is
The vertex is also the maximum point if , or the minimum point if .
The vertical line
that passes through the vertex is also the axis of symmetry of the parabola.
Maximum and minimum points.
Using calculus, the vertex point, being a maximum or minimum of the function, can be obtained by finding the roots of the derivative:
giving
with the corresponding function value
so again the vertex point coordinates can be expressed as
Roots of the univariate function.
Exact roots.
The roots (zeros) of the univariate quadratic function
are the values of for which .
When the coefficients , , and , are real or complex, the roots are
where the discriminant is defined as
Upper bound on the magnitude of the roots.
The modulus of the roots of a quadratic formula_25 can be no greater than formula_26 where formula_27 is the golden ratio formula_28
The square root of a univariate quadratic function.
The square root of a univariate quadratic function gives rise to one of the four conic sections, almost always either to an ellipse or to a hyperbola. 
If formula_29 then the equation formula_30 describes a hyperbola, as can be seen by squaring both sides. The directions of the axes of the hyperbola are determined by the ordinate of the minimum point of the corresponding parabola formula_31. If the ordinate is negative, then the hyperbola's major axis (through its vertices) is horizontal, while if the ordinate is positive then the hyperbola's major axis is vertical. 
If formula_32 then the equation formula_30 describes either a circle or other ellipse or nothing at all. If the ordinate of the maximum point of the corresponding parabola
formula_31 is positive, then its square root describes an ellipse, but if the ordinate is negative then it describes an empty locus of points.
Iteration.
To iterate a function formula_35, one applies the function repeatedly, using the output from one iteration as the input to the next.
One cannot always deduce the analytic form of formula_36, which means the "n"th iteration of formula_37. (The superscript can be extended to negative numbers, referring to the iteration of the inverse of formula_37 if the inverse exists.) But there are some analytically tractable cases.
For example, for the iterative equation
one has
where
So by induction,
can be obtained, where formula_44 can be easily computed as
Finally, we have
as the solution.
See Topological conjugacy for more detail about the relationship between "f" and "g". And see Complex quadratic polynomial for the chaotic behavior in the general iteration.
The logistic map 
with parameter 2<"r"<4 can be solved in certain cases, one of which is chaotic and one of which is not. In the chaotic case "r"=4 the solution is
where the initial condition parameter formula_49 is given by formula_50. For rational formula_49, after a finite number of iterations formula_52 maps into a periodic sequence. But almost all formula_49 are irrational, and, for irrational formula_49, formula_52 never repeats itself – it is non-periodic and exhibits sensitive dependence on initial conditions, so it is said to be chaotic.
The solution of the logistic map when "r"=2 is 
formula_56
for formula_57. Since formula_58 for any value of formula_59 other than the unstable fixed point 0, the term formula_60 goes to 0 as "n" goes to infinity, so formula_52 goes to the stable fixed point formula_62
Bivariate (two variable) quadratic function.
A bivariate quadratic function is a second-degree polynomial of the form
where "A, B, C, D", and "E" are fixed coefficients and "F" is the constant term.
Such a function describes a quadratic surface. Setting formula_64 equal to zero describes the intersection of the surface with the plane formula_65, which is a locus of points equivalent to a conic section.
Minimum/maximum.
If formula_66 the function has no maximum or minimum; its graph forms an hyperbolic paraboloid.
If formula_67 the function has a minimum if "A">0, and a maximum if "A"<0; its graph forms an elliptic paraboloid. In this case the minimum or maximum occurs at formula_68 where:
If formula_71 and formula_72 the function has no maximum or minimum; its graph forms a parabolic cylinder.
If formula_71 and formula_74 the function achieves the maximum/minimum at a line—a minimum if "A">0 and a maximum if "A"<0; its graph forms a parabolic cylinder.

</doc>
<doc id="145570" url="https://en.wikipedia.org/wiki?curid=145570" title="Quadric">
Quadric

In mathematics, a quadric, or quadric surface, is any "D"-dimensional hypersurface in -dimensional space defined as the locus of zeros of a quadratic polynomial. In coordinates , the general quadric is defined by the algebraic equation
which may be compactly written in vector and matrix notation as:
where is a row vector, "x"T is the transpose of "x" (a column vector), "Q" is a matrix and "P" is a -dimensional row vector and "R" a scalar constant. The values "Q", "P" and "R" are often taken to be over real numbers or complex numbers, but a quadric may be defined over any ring. In general, the locus of zeros of a set of polynomials is known as an algebraic set, and is studied in the branch of algebraic geometry.
A quadric is thus an example of an algebraic set. For the projective theory see Quadric (projective geometry).
Euclidean plane and space.
Quadrics in the Euclidean plane are those of dimension "D" = 1, which is to say that they are curves. Such quadrics are the same as conic sections, and are typically known as conics rather than quadrics.
In Euclidean space, quadrics have dimension "D" = 2, and are known as quadric surfaces. By making a suitable Euclidean change of variables, any quadric in Euclidean space can be put into a certain normal form by choosing as the coordinate directions the principal axes of the quadric. In three-dimensional Euclidean space there are 16 such normal forms.
Of these 16 forms, five are nondegenerate, and the remaining are degenerate forms. Degenerate forms include planes, lines, points or even no points at all. 
Projective geometry.
The quadrics can be treated in a uniform manner by introducing homogeneous coordinates on a Euclidean space, thus effectively regarding it as a projective space. Thus if the original (affine) coordinates on R"D"+1 are 
one introduces new coordinates on R"D"+2
related to the original coordinates by formula_5. In the new variables, every quadric is defined by an equation of the form
where the coefficients "a""ij" are symmetric in "i" and "j". Regarding "Q"("X") = 0 as an equation in projective space exhibits the quadric as a projective algebraic variety. The quadric is said to be non-degenerate if the quadratic form is non-singular; equivalently, if the matrix ("a""ij") is invertible.
In real projective space, by Sylvester's law of inertia, a non-singular quadratic form "Q"("X") may be put into the normal form
by means of a suitable projective transformation (normal forms for singular quadrics can have zeros as well as ±1 as coefficients). For surfaces in space (dimension "D" = 2) there are exactly three nondegenerate cases:
The first case is the empty set.
The second case generates the ellipsoid, the elliptic paraboloid or the hyperboloid of two sheets, depending on whether the chosen plane at infinity cuts the quadric in the empty set, in a point, or in a nondegenerate conic respectively. These all have positive Gaussian curvature.
The third case generates the hyperbolic paraboloid or the hyperboloid of one sheet, depending on whether the plane at infinity cuts it in two lines, or in a nondegenerate conic respectively. These are doubly ruled surfaces of negative Gaussian curvature.
The degenerate form
generates the elliptic cylinder, the parabolic cylinder, the hyperbolic cylinder, or the cone, depending on whether the plane at infinity cuts it in a point, a line, two lines, or a nondegenerate conic respectively. These are singly ruled surfaces of zero Gaussian curvature.
We see that projective transformations don't mix Gaussian curvatures of different sign. This is true for general surfaces. 
In complex projective space all of the nondegenerate quadrics become indistinguishable from each other.
Probability and statistics.
The elliptical distributions, which generalize the multivariate normal distribution and are used in finance, can be defined in terms of their density functions. When they exist, the density functions "f" have the structure:
where formula_11 is a scale factor, formula_12 is an formula_13-dimensional random row vector with median vector formula_14 (which is also the mean vector if the latter exists), formula_15 is a positive definite matrix which is proportional to the covariance matrix if the latter exists, and formula_16 is a function mapping from the non-negative reals to the non-negative reals giving a finite area under the curve. The multivariate normal distribution is the special case in which formula_17 for quadratic form formula_18.
Thus the density function is a scalar-to-scalar transformation of a quadric expression. Moreover, the equation for any iso-density surface states that the quadric expression equals some constant specific to that value of the density.

</doc>
<doc id="25182" url="https://en.wikipedia.org/wiki?curid=25182" title="Quantization (physics)">
Quantization (physics)

In physics, quantization is the process of transition from a classical understanding of physical phenomena to a newer understanding known as "quantum mechanics". It is a procedure for constructing a quantum field theory starting from a classical field theory. This is a generalization of the procedure for building quantum mechanics from classical mechanics. One also speaks of field quantization, as in the "quantization of the electromagnetic field", where one refers to photons as field "quanta" (for instance as light quanta). This procedure is basic to theories of particle physics, nuclear physics, condensed matter physics, and quantum optics.
Quantization methods.
Quantization converts classical fields into operators acting on quantum states of the field theory. The lowest energy state is called the vacuum state. The reason for quantizing a theory is to deduce properties of materials, objects or particles through the computation of quantum amplitudes, which may be very complicated. Such computations have to deal with certain subtleties called renormalization, which, if neglected, can often lead to nonsense results, such as the appearance of infinities in various amplitudes. The full specification of a quantization procedure requires methods of performing renormalization.
The first method to be developed for quantization of field theories was canonical quantization. While this is extremely easy to implement on sufficiently simple theories, there are many situations where other methods of quantization yield more efficient procedures for computing quantum amplitudes. However, the use of canonical quantization has left its mark on the language and interpretation of quantum field theory.
Canonical quantization.
Canonical quantization of a field theory is analogous to the construction of quantum mechanics from classical mechanics. The classical field is treated as a dynamical variable called the canonical coordinate, and its time-derivative is the canonical momentum. One introduces a commutation relation between these which is exactly the same as the commutation relation between a particle's position and momentum in quantum mechanics. Technically, one converts the field to an operator, through combinations of creation and annihilation operators. The field operator acts on quantum states of the theory. The lowest energy state is called the vacuum state. The procedure is also called second quantization.
This procedure can be applied to the quantization of any field theory: whether of fermions or bosons, and with any internal symmetry. However, it leads to a fairly simple picture of the vacuum state and is not easily amenable to use in some quantum field theories, such as quantum chromodynamics which is known to have a complicated vacuum characterized by many different condensates.
Covariant canonical quantization.
There is a way to perform a canonical quantization without having to resort to the non covariant approach of foliating spacetime and choosing a Hamiltonian. This method is based upon a classical action, but is different from the functional integral approach.
The method does not apply to all possible actions (for instance, actions with a noncausal structure or actions with gauge "flows"). It starts with the classical algebra of all (smooth) functionals over the configuration space. This algebra is quotiented over by the ideal generated by the Euler–Lagrange equations. Then, this quotient algebra is converted into a Poisson algebra by introducing a Poisson bracket derivable from the action, called the Peierls bracket. This Poisson algebra is then formula_1-deformed in the same way as in canonical quantization.
There is also a way to quantize actions with gauge "flows". It involves the Batalin–Vilkovisky formalism, an extension of the BRST formalism.
Geometric quantization.
In mathematical physics, geometric quantization is a mathematical approach to defining a quantum theory corresponding to a given classical theory. It attempts to carry out quantization, for which there is in general no exact recipe, in such a way that certain analogies between the classical theory and the quantum theory remain manifest. For example, the similarity between the Heisenberg equation in the Heisenberg picture of quantum mechanics and the Hamilton equation in classical physics should be built in.<br>
One of the earliest attempts at a natural quantization was Weyl quantization, proposed by Hermann Weyl in 1927. Here, an attempt is made to associate a quantum-mechanical observable (a self-adjoint operator on a Hilbert space) with a real-valued function on classical phase space. The position and momentum in this phase space are mapped to the generators of the Heisenberg group, and the Hilbert space appears as a group representation of the Heisenberg group. In 1946, H. J. Groenewold considered the product of a pair of such observables and asked what the corresponding function would be on the classical phase space. This led him to discover the phase-space star-product of a pair of functions.
More generally, this technique leads to deformation quantization, where the ★-product is taken to be a deformation of the algebra of functions on a symplectic manifold or Poisson manifold. However, as a natural quantization scheme (a functor), Weyl's map is not satisfactory. For example, the Weyl map of the classical angular-momentum-squared is not just the quantum angular momentum squared operator, but it further contains a constant term 3ħ2/2. (This extra term is actually physically significant, since it accounts for the nonvanishing angular momentum of the ground-state Bohr orbit in the hydrogen atom. As a mere representation change, however, Weyl's map underlies the alternate Phase space formulation of conventional quantum mechanics.
The geometric quantization procedure falls into the following three steps: prequantization, polarization, and metaplectic correction. Prequantization of a symplectic manifold provides a representation of elements of the Poisson algebra of smooth real functions on by first order differential operators on sections of a complex line bundle . In accordance with the Kostant – Souriau prequantization formula, these operators are expressed via a connection on whose curvature form obeys the prequantization condition. By polarization is meant an integrable maximal distribution on such that for all . Integrable means for (sections of T). The quantum algebra of a symplectic manifold consists of the operators of functions whose Hamiltonian vector fields satisfy the condition. In accordance with the metaplectic correction, elements of the quantum algebra act in the pre-Hilbert space of half-forms with values in the prequantization Line bundle on a symplectic manifold . The quantization is simply where is the Lie derivative of a half-form with respect to a vector field X. Geometric quantization of Poisson manifolds and symplectic foliations also is developed. For instance, this is the case of partially integrable and superintegrable Hamiltonian systems and non-autonomous mechanics.
Loop quantization.
See Loop quantum gravity.
Path integral quantization.
A classical mechanical theory is given by an action with the permissible configurations being the ones which are extremal with respect to functional variations of the action. A quantum-mechanical description of the classical system can also be constructed from the action of the system by means of the path integral formulation.
Quantum statistical mechanics approach.
See Uncertainty principle
Schwinger's variational approach.
See Schwinger's quantum action principle

</doc>
<doc id="20646064" url="https://en.wikipedia.org/wiki?curid=20646064" title="Quantum">
Quantum

In physics, a quantum (plural: quanta) is the minimum amount of any physical entity involved in an interaction. Behind this, one finds the fundamental notion that a physical property may be "quantized," referred to as "the hypothesis of quantization". This means that the magnitude can take on only certain discrete values. 
A photon is a single quantum of (visible) light as well as all other forms of electromagnetic radiation and can be referred to as a "light quantum". The energy of an electron bound to an atom is quantized, which results in the stability of atoms, and hence of matter in general.
As incorporated into the theory of quantum mechanics, this is regarded by physicists as part of the fundamental framework for understanding and describing nature.
Etymology and discovery.
The word "quantum" comes from the Latin "quantus", meaning "how much". "Quanta", short for "quanta of electricity" (electrons) was used in a 1902 article on the photoelectric effect by Philipp Lenard, who credited Hermann von Helmholtz for using the word in the area of electricity. However, the word quantum in general was well known before 1900. It was often used by physicians, such as in the term quantum satis. Both Helmholtz and Julius von Mayer were physicians as well as physicists. Helmholtz used "quantum" with reference to heat in his article on Mayer's work, and indeed, the word "quantum" can be found in the formulation of the first law of thermodynamics by Mayer in his letter dated July 24, 1841. Max Planck used "quanta" to mean "quanta of matter and electricity", gas, and heat. In 1905, in response to Planck's work and the experimental work of Lenard (who explained his results by using the term "quanta of electricity"), Albert Einstein suggested that radiation existed in spatially localized packets which he called "quanta of light" ("Lichtquanta").
The concept of quantization of radiation was discovered in 1900 by Max Planck, who had been trying to understand the emission of radiation from heated objects, known as black-body radiation. By assuming that energy can only be absorbed or released in tiny, differential, discrete packets he called "bundles" or "energy elements", Planck accounted for the fact that certain objects change colour when heated. On December 14, 1900, Planck reported his revolutionary findings to the German Physical Society, and introduced the idea of quantization for the first time as a part of his research on black-body radiation. As a result of his experiments, Planck deduced the numerical value of "h", known as the Planck constant, and could also report a more precise value for the Avogadro–Loschmidt number, the number of real molecules in a mole and the unit of electrical charge, to the German Physical Society. After his theory was validated, Planck was awarded the Nobel Prize in Physics in 1918 for his discovery.
Beyond electromagnetic radiation.
While quantization was first discovered in electromagnetic radiation, it describes a fundamental aspect of energy not just restricted to photons.
In the attempt to bring experiment into agreement with theory, Max Planck postulated that electromagnetic energy is absorbed or emitted in discrete packets, or quanta.

</doc>
