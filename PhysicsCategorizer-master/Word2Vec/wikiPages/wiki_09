<doc id="27859" url="https://en.wikipedia.org/wiki?curid=27859" title="Sphere">
Sphere

A sphere (from Greek σφαῖρα — "sphaira", "globe, ball") is a perfectly round geometrical object in three-dimensional space that is the surface of a completely round ball, (viz., analogous to a circular object in two dimensions). Like a circle, which geometrically is a two-dimensional object, a sphere is defined mathematically as the set of points that are all at the same distance from a given point, but in three-dimensional space. This distance is the radius of the ball, and the given point is the center of the mathematical ball. The longest straight line through the ball, connecting two points of the sphere, passes through the center and its length is thus twice the radius; it is a diameter of the ball.
While outside mathematics the terms "sphere" and "ball" are sometimes used interchangeably, in mathematics a distinction is made between the sphere (a two-dimensional closed surface embedded in three-dimensional Euclidean space) and the ball (a three-dimensional shape that includes the sphere as well as everything inside the sphere). The ball and the sphere share the same radius, diameter, and center. "
Surface area.
The surface area of a sphere is:
Archimedes first derived this formula from the fact that the projection to the lateral surface of a circumscribed cylinder (for example, the Lambert cylindrical equal-area projection) is area-preserving; it equals the derivative of the formula for the volume with respect to because the total volume inside a sphere of radius can be thought of as the summation of the surface area of an infinite number of spherical shells of infinitesimal thickness concentrically stacked inside one another from radius 0 to radius . At infinitesimal thickness the discrepancy between the inner and outer surface area of any given shell is infinitesimal, and the elemental volume at radius is simply the product of the surface area at radius and the infinitesimal thickness.
At any given radius , the incremental volume () equals the product of the surface area at radius and the thickness of a shell ():
The total volume is the summation of all shell volumes:
In the limit as approaches zero this equation becomes:
Substitute :
Differentiating both sides of this equation with respect to yields as a function of :
Which is generally abbreviated as:
Alternatively, the area element on the sphere is given in spherical coordinates by . In cartesian coordinates, the area element is
For more generality, see area element.
The total area can thus be obtained by integration:
Enclosed volume.
In 3 dimensions, the volume inside a sphere (that is the volume of a ball) is derived to be
where "r" is the radius of the sphere and π is the constant pi. Archimedes first derived this formula, which shows that the volume inside a sphere is 2/3 that of a circumscribed cylinder. (This assertion follows from Cavalieri's principle.) In modern mathematics, this formula can be derived using integral calculus, i.e. disk integration to sum the volumes of an infinite number of circular disks of infinitesimally small thickness stacked centered side by side along the "x" axis from where the disk has radius "r" (i.e. ) to where the disk has radius 0 (i.e. ).
At any given "x", the incremental volume ("δV") equals the product of the cross-sectional area of the disk at "x" and its thickness ("δx"):
The total volume is the summation of all incremental volumes:
In the limit as δx approaches zero this equation becomes:
At any given "x", a right-angled triangle connects "x", "y" and "r" to the origin; hence, applying the Pythagorean theorem yields:
Thus, substituting "y" with a function of "x" gives:
Which can now be evaluated as follows:
Therefore the volume of a sphere is:
Alternatively this formula is found using spherical coordinates, with volume element
so
For most practical purposes, the volume inside a sphere inscribed in a cube can be approximated as 52.4% of the volume of the cube, since formula_20. For example, a sphere with diameter 1m has 52.4% the volume of a cube with edge length 1m, or about 0.524m3.
In higher dimensions, the analog of a sphere is called a hypersphere, which encloses an "n"-ball. General recursive and non-recursive formulas exist for the volume of an "n"-ball.
Equations in three-dimensional space.
In analytic geometry, a sphere with center ("x"0, "y"0, "z"0) and radius "r" is the locus of all points ("x", "y", "z") such that
The points on the sphere with radius "r" can be parameterized via
A sphere of any radius centered at zero is an integral surface of the following differential form:
This equation reflects that position and velocity vectors of a point traveling on the sphere are always orthogonal to each other.
The sphere has the smallest surface area of all surfaces that enclose a given volume, and it encloses the largest volume among all closed surfaces with a given surface area. The sphere therefore appears in nature: for example, bubbles and small water drops are roughly spherical because the surface tension locally minimizes surface area.
The surface area relative to the mass of a sphere is called the specific surface area and can be expressed from the above stated equations as
where formula_27 is the ratio of mass to volume.
A sphere can also be defined as the surface formed by rotating a circle about any diameter. Replacing the circle with an ellipse rotated about its major axis, the shape becomes a prolate spheroid; rotated about the minor axis, an oblate spheroid.
Terminology.
Pairs of points on a sphere that lie on a straight line through the sphere's center are called antipodal points. A great circle is a circle on the sphere that has the same center and radius as the sphere and consequently divides it into two equal parts. The shortest distance along the surface between two distinct non-antipodal points on the surface is on the unique great circle that includes the two points. Equipped with the great-circle distance, a great circle becomes the Riemannian circle.
If a particular point on a sphere is (arbitrarily) designated as its "north pole", then the corresponding antipodal point is called the "south pole," and the equator is the great circle that is equidistant to them. Great circles through the two poles are called lines (or meridians) of longitude, and the line connecting the two poles is called the axis of rotation. Circles on the sphere that are parallel to the equator are lines of latitude. This terminology is also used for such approximately spheroidal astronomical bodies as the planet Earth (see geoid).
Hemisphere.
Any plane that includes the center of a sphere divides it into two equal hemispheres. Any two intersecting planes that include the center of a sphere subdivide the sphere into four lunes or biangles, the vertices of which all coincide with the antipodal points lying on the line of intersection of the planes.
The antipodal quotient of the sphere is the surface called the real projective plane, which can also be thought of as the northern hemisphere with antipodal points of the equator identified.
The round hemisphere is conjectured to be the optimal (least area) filling of the Riemannian circle.
The circles of intersection of any plane not intersecting the sphere's center and the sphere's surface are called "spheric sections".
Generalization to other dimensions.
Spheres can be generalized to spaces of any dimension. For any natural number "n", an ""n"-sphere," often written as "formula_28", is the set of points in ()-dimensional Euclidean space that are at a fixed distance "r" from a central point of that space, where "r" is, as before, a positive real number. In particular:
Spheres for "n" > 2 are sometimes called hyperspheres.
The "n"-sphere of unit radius centered at the origin is denoted "S""n" and is often referred to as "the" "n"-sphere. Note that the ordinary sphere is a 2-sphere, because it is a 2-dimensional surface (which is embedded in 3-dimensional space).
The surface area of the ()-sphere of radius 1 is
where Γ("z") is Euler's Gamma function.
Another expression for the surface area is
and the volume is the surface area times formula_35 or
Generalization to metric spaces.
More generally, in a metric space ("E","d"), the sphere of center "x" and radius is the set of points "y" such that .
If the center is a distinguished point that is considered to be the origin of "E", as in a normed space, it is not mentioned in the definition and notation. The same applies for the radius if it is taken to equal one, as in the case of a unit sphere.
Unlike a ball, even a large sphere may be an empty set. For example, in Z"n" with Euclidean metric, a sphere of radius "r" is nonempty only if "r"2 can be written as sum of "n" squares of integers.
Topology.
In topology, an "n"-sphere is defined as a space homeomorphic to the boundary of an ("n" + 1)-ball; thus, it is homeomorphic to the Euclidean "n"-sphere, but perhaps lacking its metric.
The "n"-sphere is denoted "Sn". It is an example of a compact topological manifold without boundary. A sphere need not be smooth; if it is smooth, it need not be diffeomorphic to the Euclidean sphere.
The Heine–Borel theorem implies that a Euclidean "n"-sphere is compact. The sphere is the inverse image of a one-point set under the continuous function ||"x"||. Therefore, the sphere is closed. "Sn" is also bounded; therefore it is compact.
Remarkably, it is possible to turn an ordinary sphere inside out in a three-dimensional space with possible self-intersections but without creating any crease, in a process called sphere eversion.
Spherical geometry.
The basic elements of Euclidean plane geometry are points and lines. On the sphere, points are defined in the usual sense. The analogue of the "line" is the geodesic, which is a great circle; the defining characteristic of the latter is that the plane containing all its points also passes through the center of the sphere. Measuring by arc length shows that the shortest path between two points lying entirely on the sphere is a segment of the great circle that includes the points.
Many theorems from classical geometry hold true for spherical geometry as well, but not all do because the sphere fails to satisfy some of classical geometry's postulates, including the parallel postulate. In spherical trigonometry, angles are defined between great circles. Thus spherical trigonometry differs from ordinary trigonometry in many respects. For example, the sum of the interior angles of a spherical triangle exceeds 180 degrees. Also, any two similar spherical triangles are congruent.
Eleven properties of the sphere.
In their book "Geometry and the imagination" David Hilbert and Stephan Cohn-Vossen describe eleven properties of the sphere and discuss whether these properties uniquely determine the sphere. Several properties hold for the plane, which can be thought of as a sphere with infinite radius. These properties are:
Cubes in relation to spheres.
For every sphere there are multiple cuboids that may be inscribed within the sphere. The largest cuboid which can be inscribed within a sphere is a cube.

</doc>
<doc id="368621" url="https://en.wikipedia.org/wiki?curid=368621" title="Sphere packing">
Sphere packing

In geometry, a sphere packing is an arrangement of non-overlapping spheres within a containing space. The spheres considered are usually all of identical size, and the space is usually three-dimensional Euclidean space. However, sphere packing problems can be generalised to consider unequal spheres, "n"-dimensional Euclidean space (where the problem becomes circle packing in two dimensions, or hypersphere packing in higher dimensions) or to non-Euclidean spaces such as hyperbolic space.
A typical sphere packing problem is to find an arrangement in which the spheres fill as large a proportion of the space as possible. The proportion of space filled by the spheres is called the density of the arrangement. As the local density of a packing in an infinite space can vary depending on the volume over which it is measured, the problem is usually to maximise the average or asymptotic density, measured over a large enough volume.
For equal spheres the densest packing uses approximately 74% of the volume. Random packing of equal spheres generally have a density around 64%.
Classification and terminology.
A lattice arrangement (commonly called a regular arrangement) is one in which the centers of the spheres form a very symmetric pattern which only needs n vectors to be uniquely defined (in n-dimensional Euclidean space). Lattice arrangements are periodic. Arrangements in which the spheres do not form a lattice (often referred to as irregular) can still be periodic, but also aperiodic (properly speaking non-periodic) or random. Lattice arrangements are easier to handle than irregular ones—their high degree of symmetry makes it easier to classify them and to measure their densities.
Regular packing.
Dense packing.
In three-dimensional Euclidean space, the densest packing of equal spheres is achieved by a family of structures called close-packed structures. One method for generating such a structure is as follows. Consider a plane with a compact arrangement of spheres on it. For any three neighbouring spheres, a fourth sphere can be placed on top in the hollow between the three bottom spheres. If we do this "everywhere" in a second plane above the first, we create a new compact layer. A third layer can be placed directly above the first one, or the spheres can be offset, vertically above another set of hollows of the first layer. There are thus three types of planes, called A, B and C.
Two simple arrangements within the close-packed family correspond to regular lattices. One is called cubic close packing (or face centred cubic, "FCC") — where the layers are alternated in the ABCABC… sequence. The other is called hexagonal close packing ("HCP") — where the layers are alternated in the ABAB… sequence. But many layer stacking sequences are possible (ABAC, ABCBA, ABCBAC, etc.), and still generate a close-packed structure. In all of these arrangements each sphere is surrounded by 12 other spheres, and the average density is 
Gauss proved in 1831 that these packings have the highest density amongst all possible lattice packings.
In 1611 Johannes Kepler had conjectured that this is the maximum possible density amongst both regular and irregular arrangements — this became known as the Kepler conjecture. In 1998, Thomas Callister Hales, following the approach suggested by László Fejes Tóth in 1953, announced a proof of the Kepler conjecture. Hales' proof is a proof by exhaustion involving checking of many individual cases using complex computer calculations. Referees said that they were "99% certain" of the correctness of Hales' proof. On 10 August 2014 Hales announced the completion of a formal proof using automated proof checking, removing any doubt.
Other common lattice packings.
Some other lattice packings are often found in physical systems. These include the cubic lattice with a density of formula_2, the hexagonal lattice with a density of formula_3 , and the tetrahedral lattice with a density of formula_4 and loosest possible at a density of 0.0555.
Jammed packings with a low density.
Packings where all spheres are constrained by their neighbours to stay in one location are called rigid or jammed. The strictly jammed sphere packing with the lowest density is a diluted ("tunneled") fcc crystal with a density of only 0.49365.
Irregular packing.
If we attempt to build a densely packed collection of spheres we will be tempted to always place the next sphere in a hollow between three packed spheres. If five spheres are assembled in this way, they will be consistent with one of the regularly packed arrangements described above. However, the sixth sphere placed in this way will render the structure inconsistent with any regular arrangement. This results in the possibility of a "random close packing" of spheres which is stable against compression.
When spheres are randomly added to a container and then compressed, they will generally form what is known as an "irregular" or "jammed" packing configuration when they can be compressed no more. This irregular packing will generally have a density of about 64%. Recent research predicts analytically that it cannot exceed a density limit of 63.4% This situation is unlike the case of one or two dimensions, where compressing a collection of 1-dimensional or 2-dimensional spheres (i.e. line segments or disks) will yield a regular packing.
Hypersphere packing.
The sphere packing problem is the three-dimensional version of a class of ball-packing problems in arbitrary dimensions. In two dimensions, the equivalent problem is packing circles on a plane.
In dimensions higher than three, the densest regular packings of hyperspheres are known up to 8 dimensions. Very little is known about irregular hypersphere packings; it is possible that in some dimensions the densest packing may be irregular. Some support for this conjecture comes from the fact that in certain dimensions (e.g. 10) the densest known irregular packing is denser than the densest known regular packing.
Dimension 24 is special due to the existence of the Leech lattice, which has the best kissing number and is the densest lattice packing. No better irregular packing is known, and an irregular packing could, at best, improve over the Leech lattice packing by a factor of less than 1+2.
Another line of research in high dimensions is trying to find asymptotic bounds for the density of the densest packings. Currently the best known result is that there exists a lattice in dimension "n" with density bigger or equal to formula_5 for some number "c".
Unequal sphere packing.
Many problems in the chemical and physical sciences can be related to packing problems where more than one size of sphere is available. Here there is a choice between separating the spheres into regions of close-packed equal spheres, or combining the multiple sizes of spheres into a compound or interstitial packing. When many sizes of spheres (or a distribution) are available, the problem quickly becomes intractable, but some studies of binary hard spheres (two sizes) are available.
When the second sphere is much smaller than the first, it is possible to arrange the large spheres in a close-packed arrangement, and then arrange the small spheres within the octahedral and tetrahedral gaps. The density of this interstitial packing depends sensitively on the radius ratio, but in the limit of extreme size ratios, the smaller spheres can fill the gaps with the same density as the larger spheres filled space. Even if the large spheres are not in a close-packed arrangement, it is always possible to insert some smaller spheres of up to 0.29099 of the radius of the larger sphere.
When the smaller sphere has a radius greater than 0.41421 of the radius of the larger sphere, it is no longer possible to fit into even the octahedral holes of the close-packed structure. Thus, beyond this point, either the host structure must expand to accommodate the interstitials (which compromises the overall density), or rearrange into a more complex crystalline compound structure. Structures are known which exceed the close packing density for radius ratios up to 0.659786.
Upper bounds for the density that can be obtained in such binary packings have also been obtained.
In many chemical situations such as ionic crystals, the stoichiometry is constrained by the charges of the constituent ions. This additional constraint on the packing, together with the need to minimize the Coulomb energy of interacting charges leads to a diversity of optimal packing arrangements.
Hyperbolic space.
Although the concept of circles and spheres can be extended to hyperbolic space, finding the densest packing becomes much more difficult. In a hyperbolic space there is no limit to the number of spheres that can surround another sphere (for example, Ford circles can be thought of as an arrangement of identical hyperbolic circles in which each circle is surrounded by an infinite number of other circles). The concept of average density also becomes much more difficult to define accurately. The densest packings in any hyperbolic space are almost always irregular.
Despite this difficulty, K. Böröczky gives a universal upper bound for the density of sphere packings of hyperbolic n-space where formula_6. In three dimensions the Böröczky bound is approximately 85.327613%, and is realized by the horosphere packing of the order-6 tetrahedral honeycomb with Schläfli symbol {3,3,6}. In addition to this configuration at least three other horosphere packings are known to exist in hyperbolic 3-space that realize the density upper bound.
Touching pairs, triplets, and quadruples.
The contact graph of an arbitrary finite packing of unit balls is the graph whose vertices correspond to the packing elements and whose two vertices are connected by an edge if the corresponding two packing elements touch each other. The cardinality of the edge set of the contact graph gives the number of touching pairs, the number of 3-cycles in the contact graph gives the number of touching triplets, and the number of tetrahedrons in the contact graph gives the number of touching quadruples (in general for a contact graph associated with a sphere packing in n-dimensions that the cardinality of the set of n-simplices in the contact graph gives the number of touching (n+1)-tuples in the sphere packing). In the case of 3-dimensional Euclidean space, non-trivial upper bounds on the number of touching pairs, triplets, and quadruples were proved by Karoly Bezdek and Samuel Reid at the University of Calgary.
Other spaces.
Sphere packing on the corners of a hypercube (with the spheres defined by Hamming distance) corresponds to designing error-correcting codes: if the spheres have radius "t", then their centers are codewords of a "2t+1"-error-correcting code. Lattice packings correspond to linear codes. There are other, subtler relationships between Euclidean sphere packing and error-correcting codes. For example, the binary Golay code is closely related to the 24-dimensional Leech lattice.
For further details on these connections, see the book of Conway and Sloane.

</doc>
<doc id="29181" url="https://en.wikipedia.org/wiki?curid=29181" title="Spherical coordinate system">
Spherical coordinate system

In mathematics, a spherical coordinate system is a coordinate system for three-dimensional space where the position of a point is specified by three numbers: the radial distance of that point from a fixed origin, its polar angle measured from a fixed zenith direction, and the azimuth angle of its orthogonal projection on a reference plane that passes through the origin and is orthogonal to the zenith, measured from a fixed reference direction on that plane.
The radial distance is also called the radius or radial coordinate. The polar angle may be called co-latitude, zenith angle, normal angle, or inclination angle.
The use of symbols and the order of the coordinates differs between sources. In one system frequently encountered in physics ("r", "θ", "φ") gives the radial distance, polar angle, and azimuthal angle, whereas in another system used in many mathematics books ("r", "θ", "φ") gives the radial distance, azimuthal angle, and polar angle. In both systems "ρ" is often used instead of "r". Other conventions are also used, so great care needs to be taken to check which one is being used. 
A number of different spherical coordinate systems following other conventions are used outside mathematics. In a geographical coordinate system positions are measured in latitude, longitude and height or altitude. There are a number of different celestial coordinate systems based on different fundamental planes and with different terms for the various coordinates. The spherical coordinate systems used in mathematics normally use radians rather than degrees and measure the azimuthal (when reference is north and the angle is increasing positively) angle counter-clockwise rather than clockwise. The inclination angle is often replaced by the elevation angle measured from the reference plane. Elevation angle of zero is at the horizon.
The spherical coordinate system generalises the two-dimensional polar coordinate system. It can also be extended to higher-dimensional spaces and is then referred to as a hyperspherical coordinate system.
Definition.
To define a spherical coordinate system, one must choose two orthogonal directions, the "zenith" and the "azimuth reference", and an "origin" point in space. These choices determine a reference plane that contains the origin and is perpendicular to the zenith. The spherical coordinates of a point "P" are then defined as follows:
The sign of the azimuth is determined by choosing what is a "positive" sense of turning about the zenith. This choice is arbitrary, and is part of the coordinate system's definition.
The "elevation" angle is 90 degrees (π/2 radians) minus the inclination angle.
If the inclination is zero or 180 degrees (π radians), the azimuth is arbitrary. If the radius is zero, both azimuth and inclination are arbitrary.
In linear algebra, the vector from the origin "O" to the point "P" is often called the "position vector" of "P".
Conventions.
Several different conventions exist for representing the three coordinates, and for the order in which they should be written. The use of ("r", "θ", "φ") to denote radial distance, inclination (or elevation), and azimuth, respectively, is common practice in physics, and is specified by ISO standard 80000-2 :2009, and earlier in ISO 31-11 (1992).
However, some authors (including mathematicians) use "φ" for inclination (or elevation) and "θ" for azimuth, which "provides a logical extension of the usual polar coordinates notation".<ref name="http://mathworld.wolfram.com/SphericalCoordinates.html"></ref> Some authors may also list the azimuth before the inclination (or elevation), and/or use "ρ" (rho) instead of "r" for radial distance. Some combinations of these choices result in a left-handed coordinate system. The standard convention ("r", "θ", "φ") conflicts with the usual notation for the two-dimensional polar coordinates, where "θ" is often used for the azimuth. It may also conflict with the notation used for three-dimensional cylindrical coordinates.
The angles are typically measured in degrees (°) or radians (rad), where 360° = 2π rad. Degrees are most common in geography, astronomy, and engineering, whereas radians are commonly used in mathematics and theoretical physics. The unit for radial distance is usually determined by the context.
When the system is used for physical three-space, it is customary to use positive sign for azimuth angles that are measured in the counter-clockwise sense from the reference direction on the reference plane, as seen from the zenith side of the plane. This convention is used, in particular, for geographical coordinates, where the "zenith" direction is north and positive azimuth (longitude) angles are measured eastwards from some prime meridian.
Unique coordinates.
Any spherical coordinate triplet ("r", "θ", "φ") specifies a single point of three-dimensional space. On the other hand, every point has infinitely many equivalent spherical coordinates. One can add or subtract any number of full turns to either angular measure without changing the angles themselves, and therefore without changing the point. It is also convenient, in many contexts, to allow negative radial distances, with the convention that (−"r", "θ", "φ") is equivalent to ("r", "θ" + 180°, "φ") for any "r", "θ", and "φ". Moreover, ("r", −"θ", "φ") is equivalent to ("r", "θ", "φ" + 180°).
If it is necessary to define a unique set of spherical coordinates for each point, one may restrict their ranges. A common choice is:
However, the azimuth "φ" is often restricted to the interval , or in radians, instead of . This is the standard convention for geographic longitude.
The range [0°, 180°] for inclination is equivalent to [−90°, +90°] for elevation (latitude).
Even with these restrictions, if "θ" is zero or 180° (elevation is 90° or −90°) then the azimuth angle is arbitrary; and if "r" is zero, both azimuth and inclination/elevation are arbitrary. To make the coordinates unique, one can use the convention that in these cases the arbitrary coordinates are zero.
Plotting.
To plot a dot from its spherical coordinates ("r", "θ", "φ"), where "θ" is inclination, move "r" units from the origin in the zenith direction, rotate by "θ" about the origin towards the azimuth reference direction, and rotate by "φ" about the zenith in the proper direction.
Applications.
The geographic coordinate system uses the azimuth and elevation of the spherical coordinate system to express locations on Earth, calling them respectively longitude and latitude. Just as the two-dimensional Cartesian coordinate system is useful on the plane, a two-dimensional spherical coordinate system is useful on the surface of a sphere. In this system, the sphere is taken as a unit sphere, so the radius is unity and can generally be ignored. This simplification can also be very useful when dealing with objects such as rotational matrices.
Spherical coordinates are useful in analyzing systems that have some degree of symmetry about a point, such as volume integrals inside a sphere, the potential energy field surrounding a concentrated mass or charge, or global weather simulation in a planet's atmosphere. A sphere that has the Cartesian equation "x"2 + "y"2 + "z"2 = "c"2 has the simple equation "r" = "c" in spherical coordinates.
Two important partial differential equations that arise in many physical problems, Laplace's equation and the Helmholtz equation, allow a separation of variables in spherical coordinates. The angular portions of the solutions to such equations take the form of spherical harmonics.
Another application is ergonomic design, where "r" is the arm length of a stationary person and the angles describe the direction of the arm as it reaches out.
Three dimensional modeling of loudspeaker output patterns can be used to predict their performance. A number of polar plots are required, taken at a wide selection of frequencies, as the pattern changes greatly with frequency. Polar plots help to show that many loudspeakers tend toward omnidirectionality at lower frequencies.
The spherical coordinate system is also commonly used in 3D game development to rotate the camera around the player's position.
In geography.
To a first approximation, the geographic coordinate system uses elevation angle (latitude) in degrees north of the equator plane, in the range −90° ≤ "φ" ≤ 90°, instead of inclination. Latitude is either geocentric latitude, measured at the Earth's center and designated variously by "ψ", "q", "φ"′, "φ"c, "φ"g or geodetic latitude, measured by the observer's local vertical, and commonly designated "φ". The azimuth angle (longitude), commonly denoted by "λ", is measured in degrees east or west from some conventional reference meridian (most commonly the IERS Reference Meridian), so its domain is −180° ≤ "λ" ≤ 180°. For positions on the Earth or other solid celestial body, the reference plane is usually taken to be the plane perpendicular to the axis of rotation. 
The polar angle, which is 90° minus the latitude and ranges from 0 to 180°, is called colatitude in geography.
Instead of the radial distance, geographers commonly use altitude above some reference surface, which may be the sea level or "mean" surface level for planets without liquid oceans. The radial distance "r" can be computed from the altitude by adding the mean radius of the planet's reference surface, which is approximately 6,360 ± 11 km for Earth.
However, modern geographical coordinate systems are quite complex, and the positions implied by these simple formulae may be wrong by several kilometers. The precise standard meanings of latitude, longitude and altitude are currently defined by the World Geodetic System (WGS), and take into account the flattening of the Earth at the poles (about 21 km) and many other details.
In astronomy.
In astronomy there are a series of spherical coordinate systems that measure the elevation angle from different fundamental planes. These reference planes are the observer's horizon, the celestial equator (defined by Earth's rotation), the plane of the ecliptic (defined by Earth's orbit around the Sun), and the galactic equator (defined by the rotation of the Milky Way).
Coordinate system conversions.
As the spherical coordinate system is only one of many three-dimensional coordinate systems, there exist equations for converting coordinates between the spherical coordinate system and others.
Cartesian coordinates.
The spherical coordinates of a point in the ISO convention ("radius r", "inclination θ", "azimuth φ") can be obtained from its Cartesian coordinates ("x", "y", "z") by the formulae
The inverse tangent denoted in must be suitably defined, taking into account the correct quadrant of ("x","y"). See the article on atan2.
Alternatively, the conversion can be considered as two sequential rectangular to polar conversions: the first in the Cartesian "x"–"y" plane from ("x","y") to ("R","φ"), where "R" is the projection of "r" onto the "x"–"y" plane, and the second in the Cartesian "z"–"R" plane from ("z","R") to ("r","θ"). The correct quadrants for "φ" and "θ" are implied by the correctness of the planar rectangular to polar conversions.
These formulae assume that the two systems have the same origin, that the spherical reference plane is the Cartesian "x"–"y" plane, that "θ" is inclination from the "z" direction, and that the azimuth angles are measured from the Cartesian "x" axis (so that the "y" axis has "φ" = +90°). If "θ" measures elevation from the reference plane instead of inclination from the zenith the arccos above becomes an arcsin, and the cos "θ" and sin "θ" below become switched.
Conversely, the Cartesian coordinates may be retrieved from the spherical coordinates ("radius r", "inclination θ", "azimuth φ"), where , , , by:
Cylindrical coordinates.
Cylindrical coordinates ("radius ρ", "azimuth φ", "elevation z") may be converted into spherical coordinates ("radius r", "inclination θ", "azimuth φ"), by the formulas
Conversely, the spherical coordinates may be converted into cylindrical coordinates by the formulae
These formulae assume that the two systems have the same origin and same reference plane, measure the azimuth angle "φ" in the same sense from the same axis, and that the spherical angle "θ" is inclination from the cylindrical "z" axis.
Integration and differentiation in spherical coordinates.
The following equations assume that "θ" is inclination from the z (polar) axis (ambiguous since "x", "y", and "z" are mutually normal):
The line element for an infinitesimal displacement from formula_13 to formula_14 is
where
are the local orthogonal unit vectors in the directions of increasing , , and , respectively,
and formula_19, formula_20, and formula_21 are the unit vectors in Cartesian coordinates.
The surface element spanning from formula_22 to formula_23 and formula_24 to formula_25 on a spherical surface at (constant) radius formula_26 is
Thus the differential solid angle is
The surface element in a surface of polar angle formula_22 constant (a cone with vertex the origin) is
The surface element in a surface of azimuth formula_24 constant (a vertical half-plane) is
The volume element spanning from formula_26 to formula_34, formula_22 to formula_23, and formula_24 to formula_25 is
Thus, for example, a function formula_40 can be integrated over every point in R3 by the triple integral
The del operator in this system leads to the following expressions for gradient, divergence, curl and Laplacian:
Kinematics.
In spherical coordinates the position of a point is written
Its velocity is then
and its acceleration is
In the case of a constant "φ" or formula_46, this reduces to vector calculus in polar coordinates.

</doc>
<doc id="203056" url="https://en.wikipedia.org/wiki?curid=203056" title="Spherical harmonics">
Spherical harmonics

In mathematics, spherical harmonics are a series of special functions defined on the surface of a sphere used to solve some kinds of differential equations. As Fourier series are a series of functions used to represent functions on a circle, spherical harmonics are a series of functions that are used to represent functions defined on the surface of a sphere. Spherical harmonics are functions defined in terms of spherical coordinates and are organized by angular frequency, as seen in the rows of functions in the illustration on the right.
Spherical harmonics are defined as the angular portion of a set of solutions to Laplace's equation in three dimensions. Represented in a system of spherical coordinates, Laplace's spherical harmonics formula_1 are a specific set of spherical harmonics that forms an orthogonal system, first introduced by Pierre Simon de Laplace in 1782. 
Spherical harmonics are important in many theoretical and practical applications, particularly in the computation of atomic orbital electron configurations, representation of gravitational fields, geoids, and the magnetic fields of planetary bodies and stars, and characterization of the cosmic microwave background radiation. In 3D computer graphics, spherical harmonics play a role in a wide variety of topics including indirect lighting (ambient occlusion, global illumination, precomputed radiance transfer, etc.) and modelling of 3D shapes.
History.
Spherical harmonics were first investigated in connection with the Newtonian potential of Newton's law of universal gravitation in three dimensions. In 1782, Pierre-Simon de Laplace had, in his "Mécanique Céleste", determined that the gravitational potential at a point x associated to a set of point masses "m""i" located at points x"i" was given by
Each term in the above summation is an individual Newtonian potential for a point mass. Just prior to that time, Adrien-Marie Legendre had investigated the expansion of the Newtonian potential in powers of "r" = |x| and "r"1 = |x1|. He discovered that if "r" ≤ "r"1 then
where γ is the angle between the vectors x and x1. The functions "P""i" are the Legendre polynomials, and they are a special case of spherical harmonics. Subsequently, in his 1782 memoire, Laplace investigated these coefficients using spherical coordinates to represent the angle γ between x1 and x. (See Applications of Legendre polynomials in physics for a more detailed analysis.)
In 1867, William Thomson (Lord Kelvin) and Peter Guthrie Tait introduced the solid spherical harmonics in their "Treatise on Natural Philosophy", and also first introduced the name of "spherical harmonics" for these functions. The solid harmonics were homogeneous solutions of Laplace's equation
By examining Laplace's equation in spherical coordinates, Thomson and Tait recovered Laplace's spherical harmonics. The term "Laplace's coefficients" was employed by William Whewell to describe the particular system of solutions introduced along these lines, whereas others reserved this designation for the zonal spherical harmonics that had properly been introduced by Laplace and Legendre.
The 19th century development of Fourier series made possible the solution of a wide variety of physical problems in rectangular domains, such as the solution of the heat equation and wave equation. This could be achieved by expansion of functions in series of trigonometric functions. Whereas the trigonometric functions in a Fourier series represent the fundamental modes of vibration in a string, the spherical harmonics represent the fundamental modes of vibration of a sphere in much the same way. Many aspects of the theory of Fourier series could be generalized by taking expansions in spherical harmonics rather than trigonometric functions. This was a boon for problems possessing spherical symmetry, such as those of celestial mechanics originally studied by Laplace and Legendre.
The prevalence of spherical harmonics already in physics set the stage for their later importance in the 20th century birth of quantum mechanics. The spherical harmonics are eigenfunctions of the square of the orbital angular momentum operator
and therefore they represent the different quantized configurations of atomic orbitals.
Laplace's spherical harmonics.
Laplace's equation imposes that the divergence of the gradient of a scalar field is zero. In spherical coordinates this is:
Consider the problem of finding solutions of the form . By separation of variables, two differential equations result by imposing Laplace's equation:
The second equation can be simplified under the assumption that has the form . Applying separation of variables again to the second equation gives way to the pair of differential equations
for some number . A priori, is a complex constant, but because must be a periodic function whose period evenly divides , is necessarily an integer and Φ is a linear combination of the complex exponentials . The solution function is regular at the poles of the sphere, where . Imposing this regularity in the solution of the second equation at the boundary points of the domain is a Sturm–Liouville problem that forces the parameter to be of the form for some non-negative integer with ; this is also explained below in terms of the orbital angular momentum. Furthermore, a change of variables transforms this equation into the Legendre equation, whose solution is a multiple of the associated Legendre polynomial . Finally, the equation for has solutions of the form ; requiring the solution to be regular throughout forces .
Here the solution was assumed to have the special form . For a given value of , there are independent solutions of this form, one for each integer with . These angular solutions are a product of trigonometric functions, here represented as a complex exponential, and associated Legendre polynomials:
which fulfill
Here is called a spherical harmonic function of degree and order , is an associated Legendre polynomial, is a normalization constant, and and represent colatitude and longitude, respectively. In particular, the colatitude , or polar angle, ranges from at the North Pole, to at the Equator, to at the South Pole, and the longitude , or azimuth, may assume all values with . For a fixed integer , every solution of the eigenvalue problem
is a linear combination of . In fact, for any such solution, is the expression in spherical coordinates of a homogeneous polynomial that is harmonic (see below), and so counting dimensions shows that there are linearly independent such polynomials.
The general solution to Laplace's equation in a ball centered at the origin is a linear combination of the spherical harmonic functions multiplied by the appropriate scale factor ,
where the are constants and the factors are known as solid harmonics. Such an expansion is valid in the ball
Orbital angular momentum.
In quantum mechanics, Laplace's spherical harmonics are understood in terms of the orbital angular momentum
The is conventional in quantum mechanics; it is convenient to work in units in which . The spherical harmonics are eigenfunctions of the square of the orbital angular momentum
Laplace's spherical harmonics are the joint eigenfunctions of the square of the orbital angular momentum and the generator of rotations about the azimuthal axis:
These operators commute, and are densely defined self-adjoint operators on the Hilbert space of functions "f" square-integrable with respect to the normal distribution on R3:
Furthermore, L2 is a positive operator.
If "Y" is a joint eigenfunction of L2 and "L""z", then by definition
for some real numbers "m" and λ. Here "m" must in fact be an integer, for "Y" must be periodic in the coordinate φ with period a number that evenly divides 2π. Furthermore, since
and each of "L""x", "L""y", "L""z" are self-adjoint, it follows that λ ≥ "m"2.
Denote this joint eigenspace by "E"λ,"m", and define the raising and lowering operators by
Then "L"+ and "L"− commute with L2, and the Lie algebra generated by "L"+, "L"−, "L""z" is the special linear Lie algebra, with commutation relations
Thus (it is a "raising operator") and (it is a "lowering operator"). In particular, must be zero for "k" sufficiently large, because the inequality λ ≥ "m"2 must hold in each of the nontrivial joint eigenspaces. Let "Y" ∈ "E"λ,"m" be a nonzero joint eigenfunction, and let "k" be the least integer such that
Then, since
it follows that
Thus λ = ℓ(ℓ+1) for the positive integer .
Conventions.
Orthogonality and normalization.
Several different normalizations are in common use for the Laplace spherical harmonic functions. Throughout the section, we use the standard convention that (see associated Legendre polynomials)
which is the natural normalization given by Rodrigues' formula.
In seismology, the Laplace spherical harmonics are generally defined as (this is the convention used in this article)
while in quantum mechanics: 
which are orthonormal
where δ"ij" is the Kronecker delta and "d"Ω = sinθ "d"φ "d"θ. This normalization is used in quantum mechanics because it ensures that probability is normalized, i.e.
The Condon-Shortley phase convention is used here for consistency. The corresponding inverse equations are
The real spherical harmonics are sometimes known as "tesseral spherical harmonics". These functions have the same orthonormality properties as the complex ones above.
The harmonics with "m" > 0 are said to be of cosine type, and those with "m" < 0 of sine type. The reason for this can be seen by writing the functions in terms of the Legendre polynomials as
formula_32
The same sine and cosine factors can be also seen in the following subsection that deals with the cartesian representation.
See here for a list of real spherical harmonics up to and including formula_33, which can be seen to be consistent with the output of the equations above.
Use in quantum chemistry.
As is known from the analytic solutions for the hydrogen atom, the eigenfunctions of the angular part of the wave function are spherical harmonics.
However, the solutions of the non-relativistic Schrödinger equation without magnetic terms can be made real.
This is why the real forms are extensively used in basis functions for quantum chemistry, as the programs don't then need to use complex algebra. Here, it is important to note that the real functions span the same space as the complex ones would.
For example, as can be seen from the table of spherical harmonics, the usual "p" functions (formula_34) are complex and mix axis directions, but the real versions are essentially just "x", "y" and "z".
Spherical harmonics in Cartesian form.
The following expresses normalized spherical harmonics in Cartesian coordinates (Condon-Shortley phase):
and for "m" = 0:
Here
and
For formula_40 this reduces to
Examples.
Using the expressions for formula_42, formula_43, and formula_44 listed explicitly above we obtain: 
It may be verified that this agrees with the function listed here and here.
Real form.
Using the equations above to form the real spherical harmonics, it is seen that for formula_47 only the formula_48 terms (cosines) are included, and for formula_49 only the formula_50 terms (sines) are included:
and for "m" = 0:
Spherical harmonics expansion.
The Laplace spherical harmonics form a complete set of orthonormal functions and thus form an orthonormal basis of the Hilbert space of square-integrable functions. On the unit sphere, any square-integrable function can thus be expanded as a linear combination of these:
This expansion holds in the sense of mean-square convergence — convergence in L2 of the sphere — which is to say that
The expansion coefficients are the analogs of Fourier coefficients, and can be obtained by multiplying the above equation by the complex conjugate of a spherical harmonic, integrating over the solid angle Ω, and utilizing the above orthogonality relationships. This is justified rigorously by basic Hilbert space theory. For the case of orthonormalized harmonics, this gives:
If the coefficients decay in ℓ sufficiently rapidly — for instance, exponentially — then the series also converges uniformly to "f".
A square-integrable function "f" can also be expanded in terms of the real harmonics "Y"ℓ"m" above as a sum
The convergence of the series holds again in the same sense, but the benefit of the real expansion is that for real functions "f" the expansion coefficients become real.
Spectrum analysis.
Power spectrum in signal processing.
The total power of a function "f" is defined in the signal processing literature as the integral of the function squared, divided by the area of its domain. Using the orthonormality properties of the real unit-power spherical harmonic functions, it is straightforward to verify that the total power of a function defined on the unit sphere is related to its spectral coefficients by a generalization of Parseval's theorem (here, the theorem is stated for Schmidt semi-normalized harmonics, the relationship is slightly different for orthonormal harmonics):
where
is defined as the angular power spectrum (for Schmidt semi-normalized harmonics). In a similar manner, one can define the cross-power of two functions as
where
is defined as the cross-power spectrum. If the functions "f" and "g" have a zero mean (i.e., the spectral coefficients "f"00 and "g"00 are zero), then "S""ff"(ℓ) and "S""fg"(ℓ) represent the contributions to the function's variance and covariance for degree ℓ, respectively. It is common that the (cross-)power spectrum is well approximated by a power law of the form
When β = 0, the spectrum is "white" as each degree possesses equal power. When β < 0, the spectrum is termed "red" as there is more power at the low degrees with long wavelengths than higher degrees. Finally, when β > 0, the spectrum is termed "blue". The condition on the order of growth of "S""ff"(ℓ) is related to the order of differentiability of "f" in the next section.
Differentiability properties.
One can also understand the differentiability properties of the original function "f" in terms of the asymptotics of "S""ff"(ℓ). In particular, if "S""ff"(ℓ) decays faster than any rational function of ℓ as ℓ → ∞, then "f" is infinitely differentiable. If, furthermore, "S""ff"(ℓ) decays exponentially, then "f" is actually real analytic on the sphere.
The general technique is to use the theory of Sobolev spaces. Statements relating the growth of the "S""ff"(ℓ) to differentiability are then similar to analogous results on the growth of the coefficients of Fourier series. Specifically, if
then "f" is in the Sobolev space "H""s"("S"2). In particular, the Sobolev embedding theorem implies that "f" is infinitely differentiable provided that
for all "s".
Algebraic properties.
Addition theorem.
A mathematical result of considerable interest and use is called the "addition theorem" for spherical harmonics. This is a generalization of the trigonometric identity
in which the role of the trigonometric functions appearing on the right-hand side is played by the spherical harmonics and that of the left-hand side is played by the Legendre polynomials.
Consider two unit vectors x and y, having spherical coordinates (θ,φ) and (θ′,φ′), respectively. The addition theorem states
where "P"ℓ is the Legendre polynomial of degree ℓ. This expression is valid for both real and complex harmonics. The result can be proven analytically, using the properties of the Poisson kernel in the unit ball, or geometrically by applying a rotation to the vector y so that it points along the "z"-axis, and then directly calculating the right-hand side.
In particular, when x = y, this gives Unsöld's theorem
which generalizes the identity cos2θ + sin2θ = 1 to two dimensions.
In the expansion (), the left-hand side "P"ℓ(x·y) is a constant multiple of the degree ℓ zonal spherical harmonic. From this perspective, one has the following generalization to higher dimensions. Let "Y""j" be an arbitrary orthonormal basis of the space Hℓ of degree ℓ spherical harmonics on the "n"-sphere. Then formula_66, the degree ℓ zonal harmonic corresponding to the unit vector "x", decomposes as
Furthermore, the zonal harmonic formula_67 is given as a constant multiple of the appropriate Gegenbauer polynomial:
Combining () and () gives () in dimension "n" = 2 when x and y are represented in spherical coordinates. Finally, evaluating at x = y gives the functional identity
where ω"n"−1 is the volume of the ("n"−1)-sphere.
Clebsch–Gordan coefficients.
The Clebsch–Gordan coefficients are the coefficients appearing in the expansion of the product of two spherical harmonics in terms of spherical harmonics itself. A variety of techniques are available for doing essentially the same calculation, including the Wigner 3-jm symbol, the Racah coefficients, and the Slater integrals. Abstractly, the Clebsch–Gordan coefficients express the tensor product of two irreducible representations of the rotation group as a sum of irreducible representations: suitably normalized, the coefficients are then the multiplicities.
Parity.
The spherical harmonics have well defined parity in the sense that they are either even or odd with respect to reflection about the origin. Reflection about the origin is represented by the operator formula_69. For the spherical angles, formula_70 this corresponds to the replacement formula_71. The associated Legendre polynomials gives (−1)ℓ+"m" and from the exponential function we have (−1)"m", giving together for the spherical harmonics a parity of (−1)ℓ:
This remains true for spherical harmonics in higher dimensions: applying a point reflection to a spherical harmonic of degree ℓ changes the sign by a factor of (−1)ℓ.
Visualization of the spherical harmonics.
The Laplace spherical harmonics formula_1 can be visualized by considering their "nodal lines", that is, the set of points on the sphere where formula_74, or alternatively where formula_75. Nodal lines of formula_1 are composed of circles: some are latitudes and others are longitudes. One can determine the number of nodal lines of each type by counting the number of zeros of formula_1 in the latitudinal and longitudinal directions independently. For the latitudinal direction, the real and imaginary components of the associated Legendre polynomials each possess ℓ−|"m"| zeros, whereas for the longitudinal direction, the trigonometric sin and cos functions possess 2|"m"| zeros.
When the spherical harmonic order "m" is zero (upper-left in the figure), the spherical harmonic functions do not depend upon longitude, and are referred to as zonal. Such spherical harmonics are a special case of zonal spherical functions. When ℓ = |"m"| (bottom-right in the figure), there are no zero crossings in latitude, and the functions are referred to as sectoral. For the other cases, the functions checker the sphere, and they are referred to as tesseral.
More general spherical harmonics of degree ℓ are not necessarily those of the Laplace basis formula_1, and their nodal sets can be of a fairly general kind.
List of spherical harmonics.
Analytic expressions for the first few orthonormalized Laplace spherical harmonics that use the Condon-Shortley phase convention:
Higher dimensions.
The classical spherical harmonics are defined as functions on the unit sphere "S"2 inside three-dimensional Euclidean space. Spherical harmonics can be generalized to higher-dimensional Euclidean space R"n" as follows. Let Pℓ denote the space of homogeneous polynomials of degree ℓ in "n" variables. That is, a polynomial "P" is in Pℓ provided that
Let Aℓ denote the subspace of Pℓ consisting of all harmonic polynomials; these are the solid spherical harmonics. Let Hℓ denote the space of functions on the unit sphere
obtained by restriction from Aℓ.
The following properties hold:
An orthogonal basis of spherical harmonics in higher dimensions can be constructed inductively by the method of separation of variables, by solving the Sturm-Liouville problem for the spherical Laplacian
where φ is the axial coordinate in a spherical coordinate system on "S""n"−1. The end result of such a procedure is
where the indices satisfy |ℓ1| ≤ ℓ2 ≤ ... ≤ ℓ"n"−1 and the eigenvalue is −ℓ"n"−1(ℓ"n"−1 + "n"−2). The functions in the product are defined in terms of the Legendre function
Connection with representation theory.
The space Hℓ of spherical harmonics of degree ℓ is a representation of the symmetry group of rotations around a point (SO(3)) and its double-cover SU(2). Indeed, rotations act on the two-dimensional sphere, and thus also on Hℓ by function composition
for ψ a spherical harmonic and ρ a rotation. The representation Hℓ is an irreducible representation of SO(3).
The elements of Hℓ arise as the restrictions to the sphere of elements of Aℓ: harmonic polynomials homogeneous of degree ℓ on three-dimensional Euclidean space R3. By polarization of ψ ∈ Aℓ, there are coefficients formula_100 symmetric on the indices, uniquely determined by the requirement
The condition that ψ be harmonic is equivalent to the assertion that the tensor formula_100 must be trace free on every pair of indices. Thus as an irreducible representation of SO(3), Hℓ is isomorphic to the space of traceless symmetric tensors of degree ℓ.
More generally, the analogous statements hold in higher dimensions: the space Hℓ of spherical harmonics on the "n"-sphere is the irreducible representation of SO("n"+1) corresponding to the traceless symmetric ℓ-tensors. However, whereas every irreducible tensor representation of SO(2) and SO(3) is of this kind, the special orthogonal groups in higher dimensions have additional irreducible representations that do not arise in this manner.
The special orthogonal groups have additional spin representations that are not tensor representations, and are "typically" not spherical harmonics. An exception are the spin representation of SO(3): strictly speaking these are representations of the double cover SU(2) of SO(3). In turn, SU(2) is identified with the group of unit quaternions, and so coincides with the 3-sphere. The spaces of spherical harmonics on the 3-sphere are certain spin representations of SO(3), with respect to the action by quaternionic multiplication.
Generalizations.
The angle-preserving symmetries of the two-sphere are described by the group of Möbius transformations PSL(2,C). With respect to this group, the sphere is equivalent to the usual Riemann sphere. The group PSL(2,C) is isomorphic to the (proper) Lorentz group, and its action on the two-sphere agrees with the action of the Lorentz group on the celestial sphere in Minkowski space. The analog of the spherical harmonics for the Lorentz group is given by the hypergeometric series; furthermore, the spherical harmonics can be re-expressed in terms of the hypergeometric series, as SO(3) = PSU(2) is a subgroup of PSL(2,C).
More generally, hypergeometric series can be generalized to describe the symmetries of any symmetric space; in particular, hypergeometric series can be developed for any Lie group.

</doc>
<doc id="19593829" url="https://en.wikipedia.org/wiki?curid=19593829" title="Spin (physics)">
Spin (physics)

In quantum mechanics and particle physics, spin is an intrinsic form of angular momentum carried by elementary particles, composite particles (hadrons), and atomic nuclei.
Spin is one of two types of angular momentum in quantum mechanics, the other being "orbital angular momentum". The orbital angular momentum operator is the quantum-mechanical counterpart to the classical notion of angular momentum: it arises when a particle executes a rotating or twisting trajectory (such as when an electron orbits a nucleus). The existence of spin angular momentum is inferred from experiments, such as the Stern–Gerlach experiment, in which particles are observed to possess angular momentum that cannot be accounted for by orbital angular momentum alone.
In some ways, spin is like a vector quantity; it has a definite magnitude, and it has a "direction" (but quantization makes this "direction" different from the direction of an ordinary vector). All elementary particles of a given kind have the same magnitude of spin angular momentum, which is indicated by assigning the particle a "spin quantum number".
The SI unit of spin is the joule-second, just as with classical angular momentum. In practice, however, it is written as a multiple of the reduced Planck constant "ħ", usually in natural units, where the "ħ" is omitted, resulting in a unitless number. Spin quantum numbers are unitless numbers by definition.
When combined with the spin-statistics theorem, the spin of electrons results in the Pauli exclusion principle, which in turn underlies the periodic table of chemical elements.
Wolfgang Pauli was the first to propose the concept of spin, but he did not name it. In 1925, Ralph Kronig, George Uhlenbeck and Samuel Goudsmit at Leiden University suggested a physical interpretation of particles spinning around their own axis. The mathematical theory was worked out in depth by Pauli in 1927. When Paul Dirac derived his relativistic quantum mechanics in 1928, electron spin was an essential part of it.
Quantum number.
As the name suggests, spin was originally conceived as the rotation of a particle around some axis. This picture is correct so far as spin obeys the same mathematical laws as quantized angular momenta do. On the other hand, spin has some peculiar properties that distinguish it from orbital angular momenta:
The conventional definition of the spin quantum number, "s", is "s" = "n"/2, where "n" can be any non-negative integer. Hence the allowed values of "s" are 0, 1/2, 1, 3/2, 2, etc. The value of "s" for an elementary particle depends only on the type of particle, and cannot be altered in any known way (in contrast to the "spin direction" described below). The spin angular momentum, "S", of any physical system is quantized. The allowed values of "S" are:
where "h" is the Planck constant. In contrast, orbital angular momentum can only take on integer values of "s"; i.e., even-numbered values of "n".
Fermions and bosons.
Those particles with half-integer spins, such as 1/2, 3/2, 5/2, are known as fermions, while those particles with integer spins, such as 0, 1, 2, are known as bosons. The two families of particles obey different rules and "broadly" have different roles in the world around us. A key distinction between the two families is that fermions obey the Pauli exclusion principle; that is, there cannot be two identical fermions simultaneously having the same quantum numbers (meaning, roughly, having the same position, velocity and spin direction). In contrast, bosons obey the rules of Bose–Einstein statistics and have no such restriction, so they may "bunch together" even if in identical states. Also, composite particles can have spins different from the particles which comprise them. For example, a helium atom can have spin 0 and therefore can behave like a boson even though the quarks and electrons which make it up are all fermions.
This has profound practical applications:
Theoretical and experimental studies have shown that the spin possessed by elementary particles cannot be explained by postulating that they are made up of even smaller particles rotating about a common center of mass analogous to a classical electron radius; as far as can be presently determined, these elementary particles have no inner structure. The spin of an elementary particle is therefore seen as a truly intrinsic physical property, akin to the particle's electric charge and rest mass.
Spin-statistics theorem.
The proof that particles with half-integer spin (fermions) obey Fermi–Dirac statistics and the Pauli Exclusion Principle, and particles with integer spin (bosons) obey Bose–Einstein statistics, occupy "symmetric states", and thus can share quantum states, is known as the spin-statistics theorem. The theorem relies on both quantum mechanics and the theory of special relativity, and this connection between spin and statistics has been called "one of the most important applications of the special relativity theory".
Magnetic moments.
Particles with spin can possess a magnetic dipole moment, just like a rotating electrically charged body in classical electrodynamics. These magnetic moments can be experimentally observed in several ways, e.g. by the deflection of particles by inhomogeneous magnetic fields in a Stern–Gerlach experiment, or by measuring the magnetic fields generated by the particles themselves.
The intrinsic magnetic moment μ of a spin-1/2 particle with charge "q", mass "m", and spin angular momentum S, is
where the dimensionless quantity "gs" is called the spin g-factor. For exclusively orbital rotations it would be 1 (assuming that the mass and the charge occupy spheres of equal radius).
The electron, being a charged elementary particle, possesses a nonzero magnetic moment. One of the triumphs of the theory of quantum electrodynamics is its accurate prediction of the electron "g"-factor, which has been experimentally determined to have the value , with the digits in parentheses denoting measurement uncertainty in the last two digits at one standard deviation. The value of 2 arises from the Dirac equation, a fundamental equation connecting the electron's spin with its electromagnetic properties, and the correction of ... arises from the electron's interaction with the surrounding electromagnetic field, including its own field. Composite particles also possess magnetic moments associated with their spin. In particular, the neutron possesses a non-zero magnetic moment despite being electrically neutral. This fact was an early indication that the neutron is not an elementary particle. In fact, it is made up of quarks, which are electrically charged particles. The magnetic moment of the neutron comes from the spins of the individual quarks and their orbital motions.
Neutrinos are both elementary and electrically neutral. The minimally extended Standard Model that takes into account non-zero neutrino masses predicts neutrino magnetic moments of:
where the "μ"ν are the neutrino magnetic moments, "m"ν are the neutrino masses, and "μ"B is the Bohr magneton. New physics above the electroweak scale could, however, lead to significantly higher neutrino magnetic moments. It can be shown in a model independent way that neutrino magnetic moments larger than about 10−14 μB are unnatural, because they would also lead to large radiative contributions to the neutrino mass. Since the neutrino masses cannot exceed about 1 eV, these radiative corrections must then be assumed to be fine tuned to cancel out to a large degree.
The measurement of neutrino magnetic moments is an active area of research. , the latest experimental results have put the neutrino magnetic moment at less than times the electron's magnetic moment.
In ordinary materials, the magnetic dipole moments of individual atoms produce magnetic fields that cancel one another, because each dipole points in a random direction. Ferromagnetic materials below their Curie temperature, however, exhibit magnetic domains in which the atomic dipole moments are locally aligned, producing a macroscopic, non-zero magnetic field from the domain. These are the ordinary "magnets" with which we are all familiar.
In paramagnetic materials, the magnetic dipole moments of individual atoms spontaneously align with an externally applied magnetic field. In diamagnetic materials, on the other hand, the magnetic dipole moments of individual atoms spontaneously align oppositely to any externally applied magnetic field, even if it requires energy to do so.
The study of the behavior of such "spin models" is a thriving area of research in condensed matter physics. For instance, the Ising model describes spins (dipoles) that have only two possible states, up and down, whereas in the Heisenberg model the spin vector is allowed to point in any direction. These models have many interesting properties, which have led to interesting results in the theory of phase transitions.
Direction.
Spin projection quantum number and multiplicity.
In classical mechanics, the angular momentum of a particle possesses not only a magnitude (how fast the body is rotating), but also a direction (either up or down on the axis of rotation of the particle). Quantum mechanical spin also contains information about direction, but in a more subtle form. Quantum mechanics states that the component of angular momentum measured along any direction can only take on the values 
where "Si" is the spin component along the "i"-axis (either "x", "y", or "z"), "si" is the spin projection quantum number along the "i"-axis, and "s" is the principal spin quantum number (discussed in the previous section). Conventionally the direction chosen is the "z"-axis:
where "Sz" is the spin component along the "z"-axis, "sz" is the spin projection quantum number along the "z"-axis.
One can see that there are 2"s"+1 possible values of "s"z. The number "2"s" + 1" is the multiplicity of the spin system. For example, there are only two possible values for a spin-1/2 particle: "s"z = +1/2 and "s"z = −1/2. These correspond to quantum states in which the spin is pointing in the +z or −z directions respectively, and are often referred to as "spin up" and "spin down". For a spin-3/2 particle, like a delta baryon, the possible values are +3/2, +1/2, −1/2, −3/2.
Vector.
For a given quantum state, one could think of a spin vector formula_6 whose components are the expectation values of the spin components along each axis, i.e., formula_7. This vector then would describe the "direction" in which the spin is pointing, corresponding to the classical concept of the axis of rotation. It turns out that the spin vector is not very useful in actual quantum mechanical calculations, because it cannot be measured directly: "sx", "sy" and "sz" cannot possess simultaneous definite values, because of a quantum uncertainty relation between them. However, for statistically large collections of particles that have been placed in the same pure quantum state, such as through the use of a Stern–Gerlach apparatus, the spin vector does have a well-defined experimental meaning: It specifies the direction in ordinary space in which a subsequent detector must be oriented in order to achieve the maximum possible probability (100%) of detecting every particle in the collection. For spin-1/2 particles, this maximum probability drops off smoothly as the angle between the spin vector and the detector increases, until at an angle of 180 degrees—that is, for detectors oriented in the opposite direction to the spin vector—the expectation of detecting particles from the collection reaches a minimum of 0%.
As a qualitative concept, the spin vector is often handy because it is easy to picture classically. For instance, quantum mechanical spin can exhibit phenomena analogous to classical gyroscopic effects. For example, one can exert a kind of "torque" on an electron by putting it in a magnetic field (the field acts upon the electron's intrinsic magnetic dipole moment—see the following section). The result is that the spin vector undergoes precession, just like a classical gyroscope. This phenomenon is known as electron spin resonance (ESR). The equivalent behaviour of protons in atomic nuclei is used in nuclear magnetic resonance (NMR) spectroscopy and imaging.
Mathematically, quantum mechanical spin states are described by vector-like objects known as spinors. There are subtle differences between the behavior of spinors and vectors under coordinate rotations. For example, rotating a spin-1/2 particle by 360 degrees does not bring it back to the same quantum state, but to the state with the opposite quantum phase; this is detectable, in principle, with interference experiments. To return the particle to its exact original state, one needs a 720 degree rotation. A spin-zero particle can only have a single quantum state, even after torque is applied. Rotating a spin-2 particle 180 degrees can bring it back to the same quantum state and a spin-4 particle should be rotated 90 degrees to bring it back to the same quantum state. The spin 2 particle can be analogous to a straight stick that looks the same even after it is rotated 180 degrees and a spin 0 particle can be imagined as sphere which looks the same after whatever angle it is turned through.
Mathematical formulation.
Operator.
Spin obeys commutation relations analogous to those of the orbital angular momentum:
where formula_9 is the Levi-Civita symbol. It follows (as with angular momentum) that the eigenvectors of "S"2 and "S"z (expressed as kets in the total "S" basis) are:
The spin raising and lowering operators acting on these eigenvectors give:
But unlike orbital angular momentum the eigenvectors are not spherical harmonics. They are not functions of "θ" and "φ". There is also no reason to exclude half-integer values of "s" and "m".
In addition to their other properties, all quantum mechanical particles possess an intrinsic spin (though it may have the intrinsic spin 0, too). The spin is quantized in units of the reduced Planck constant, such that the state function of the particle is, say, not formula_13, but formula_14 where formula_15 is out of the following discrete set of values:
One distinguishes bosons (integer spin) and fermions (half-integer spin). The total angular momentum conserved in interaction processes is then the "sum" of the orbital angular momentum and the spin.
Pauli matrices.
The quantum mechanical operators associated with spin-formula_17 observables are:
where in Cartesian components:
For the special case of spin-1/2 particles, "σx", "σy" and "σz" are the three Pauli matrices, given by:
Pauli exclusion principle.
For systems of "N" identical particles this is related to the Pauli exclusion principle, which states that by interchanges of any two of the "N" particles one must have
Thus, for bosons the prefactor (−1)2"s" will reduce to +1, for fermions to −1. In quantum mechanics all particles are either bosons or fermions. In some speculative relativistic quantum field theories "supersymmetric" particles also exist, where linear combinations of bosonic and fermionic components appear. In two dimensions, the prefactor (−1)2"s" can be replaced by any complex number of magnitude 1 such as in the Anyon.
The above permutation postulate for "N"-particle state functions has most-important consequences in daily life, e.g. the periodic table of the chemists or biologists.
Rotations.
As described above, quantum mechanics states that components of angular momentum measured along any direction can only take a number of discrete values. The most convenient quantum mechanical description of particle's spin is therefore with a set of complex numbers corresponding to amplitudes of finding a given value of projection of its intrinsic angular momentum on a given axis. For instance, for a spin 1/2 particle, we would need two numbers "a"±1/2, giving amplitudes of finding it with projection of angular momentum equal to "ħ"/2 and −"ħ"/2, satisfying the requirement
For a generic particle with spin "s", we would need 2"s" + 1 such parameters. Since these numbers depend on the choice of the axis, they transform into each other non-trivially when this axis is rotated. It's clear that the transformation law must be linear, so we can represent it by associating a matrix with each rotation, and the product of two transformation matrices corresponding to rotations A and B must be equal (up to phase) to the matrix representing rotation AB. Further, rotations preserve the quantum mechanical inner product, and so should our transformation matrices:
Mathematically speaking, these matrices furnish a unitary projective representation of the rotation group SO(3). Each such representation corresponds to a representation of the covering group of SO(3), which is SU(2). There is one "n"-dimensional irreducible representation of SU(2) for each dimension, though this representation is "n"-dimensional real for odd "n" and "n"-dimensional complex for even "n" (hence of real dimension 2"n"). For a rotation by angle "θ" in the plane with normal vector formula_25, "U" can be written
where formula_27 is a and S is the vector of spin operators.
A generic rotation in 3-dimensional space can be built by compounding operators of this type using Euler angles:
An irreducible representation of this group of operators is furnished by the Wigner D-matrix:
where 
is Wigner's small d-matrix. Note that for γ = 2π and α = β = 0; i.e., a full rotation about the z-axis, the Wigner D-matrix elements become
Recalling that a generic spin state can be written as a superposition of states with definite "m", we see that if "s" is an integer, the values of "m" are all integers, and this matrix corresponds to the identity operator. However, if "s" is a half-integer, the values of "m" are also all half-integers, giving (−1)2"m" = −1 for all "m", and hence upon rotation by 2π the state picks up a minus sign. This fact is a crucial element of the proof of the spin-statistics theorem.
Lorentz transformations.
We could try the same approach to determine the behavior of spin under general Lorentz transformations, but we would immediately discover a major obstacle. Unlike SO(3), the group of Lorentz transformations SO(3,1) is non-compact and therefore does not have any faithful, unitary, finite-dimensional representations.
In case of spin 1/2 particles, it is possible to find a construction that includes both a finite-dimensional representation and a scalar product that is preserved by this representation. We associate a 4-component Dirac spinor formula_32 with each particle. These spinors transform under Lorentz transformations according to the law
where formula_34 are gamma matrices and formula_35 is an antisymmetric 4×4 matrix parametrizing the transformation. It can be shown that the scalar product
is preserved. It is not, however, positive definite, so the representation is not unitary.
Metrology along the "x", "y", and "z" axes.
Each of the (Hermitian) Pauli matrices has two eigenvalues, +1 and −1. The corresponding normalized eigenvectors are:
By the postulates of quantum mechanics, an experiment designed to measure the electron spin on the "x", "y" or "z" axis can only yield an eigenvalue of the corresponding spin operator ("Sx", "Sy" or "Sz") on that axis, i.e. "ħ"/2 or –"ħ"/2. The quantum state of a particle (with respect to spin), can be represented by a two component spinor:
When the spin of this particle is measured with respect to a given axis (in this example, the "x"-axis), the probability that its spin will be measured as "ħ"/2 is just formula_39. Correspondingly, the probability that its spin will be measured as –"ħ"/2 is just formula_40. Following the measurement, the spin state of the particle will collapse into the corresponding eigenstate. As a result, if the particle's spin along a given axis has been measured to have a given eigenvalue, all measurements will yield the same eigenvalue (since formula_41, etc), provided that no measurements of the spin are made along other axes.
Metrology along an arbitrary axis.
The operator to measure spin along an arbitrary axis direction is easily obtained from the Pauli spin matrices. Let "u" = ("ux", "uy", "uz") be an arbitrary unit vector. Then the operator for spin in this direction is simply 
The operator "Su" has eigenvalues of ±"ħ"/2, just like the usual spin matrices. This method of finding the operator for spin in an arbitrary direction generalizes to higher spin states, one takes the dot product of the direction with a vector of the three operators for the three "x", "y", "z" axis directions.
A normalized spinor for spin-1/2 in the ("ux", "uy", "uz") direction (which works for all spin states except spin down where it will give 0/0), is:
The above spinor is obtained in the usual way by diagonalizing the formula_44 matrix and finding the eigenstates corresponding to the eigenvalues. In quantum mechanics, vectors are termed "normalized" when multiplied by a normalizing factor, which results in the vector having a length of unity.
Compatibility of metrology.
Since the Pauli matrices do not commute, measurements of spin along the different axes are incompatible. This means that if, for example, we know the spin along the "x"-axis, and we then measure the spin along the "y"-axis, we have invalidated our previous knowledge of the "x"-axis spin. This can be seen from the property of the eigenvectors (i.e. eigenstates) of the Pauli matrices that:
So when physicists measure the spin of a particle along the "x"-axis as, for example, "ħ"/2, the particle's spin state collapses into the eigenstate formula_46. When we then subsequently measure the particle's spin along the y-axis, the spin state will now collapse into either formula_47 or formula_48, each with probability 1/2. Let us say, in our example, that we measure –"ħ"/2. When we now return to measure the particle's spin along the x-axis again, the probabilities that we will measure "ħ"/2 or –"ħ"/2 are each 1/2 (i.e. they are formula_49 and formula_50 respectively). This implies that the original measurement of the spin along the x-axis is no longer valid, since the spin along the "x"-axis will now be measured to have either eigenvalue with equal probability.
Higher spins.
The spin-1/2 operator form the fundamental representation of SU(2). By taking Kronecker products of this representation with itself repeatedly, one may construct all higher irreducible representations. That is, the resulting spin operators for higher spin systems in three spatial dimensions, for arbitrarily large "s", can be calculated using this spin operator and ladder operators.
The resulting spin matrices for spin 1 are:
for spin they are
and for spin they are
The generalization of these matrices for arbitrary "s" is
Also useful in the quantum mechanics of multiparticle systems, the general Pauli group "Gn" is defined to consist of all "n"-fold tensor products of Pauli matrices.
The analog formula of Euler's formula in terms of the Pauli matrices:
for higher spins is tractable, but less simple.
Parity.
In tables of the spin quantum number "s" for nuclei or particles, the spin is often followed by a "+" or "−". This refers to the parity with "+" for even parity (wave function unchanged by spatial inversion) and "−" for odd parity (wave function negated by spatial inversion). For example, see the isotopes of bismuth.
Applications.
Spin has important theoretical implications and practical applications. Well-established "direct" applications of spin include:
Electron spin plays an important role in magnetism, with applications for instance in computer memories. The manipulation of "nuclear spin" by radiofrequency waves (nuclear magnetic resonance) is important in chemical spectroscopy and medical imaging.
Spin-orbit coupling leads to the fine structure of atomic spectra, which is used in atomic clocks and in the modern definition of the second. Precise measurements of the g-factor of the electron have played an important role in the development and verification of quantum electrodynamics. "Photon spin" is associated with the polarization of light.
An emerging application of spin is as a binary information carrier in spin transistors. The original concept, proposed in 1990, is known as Datta-Das spin transistor. Electronics based on spin transistors are referred to as spintronics. The manipulation of spin in dilute magnetic semiconductor materials, such as metal-doped ZnO or TiO2 imparts a further degree of freedom and has the potential to facilitate the fabrication of more efficient electronics.
There are many "indirect" applications and manifestations of spin and the associated Pauli exclusion principle, starting with the periodic table of chemistry.
History.
Spin was first discovered in the context of the emission spectrum of alkali metals. In 1924 Wolfgang Pauli introduced what he called a "two-valued quantum degree of freedom" associated with the electron in the outermost shell. This allowed him to formulate the Pauli exclusion principle, stating that no two electrons can share the same quantum state at the same time.
The physical interpretation of Pauli's "degree of freedom" was initially unknown. Ralph Kronig, one of Landé's assistants, suggested in early 1925 that it was produced by the self-rotation of the electron. When Pauli heard about the idea, he criticized it severely, noting that the electron's hypothetical surface would have to be moving faster than the speed of light in order for it to rotate quickly enough to produce the necessary angular momentum. This would violate the theory of relativity. Largely due to Pauli's criticism, Kronig decided not to publish his idea.
In the autumn of 1925, the same thought came to two Dutch physicists, George Uhlenbeck and Samuel Goudsmit at Leiden University. Under the advice of Paul Ehrenfest, they published their results. It met a favorable response, especially after Llewellyn Thomas managed to resolve a factor-of-two discrepancy between experimental results and Uhlenbeck and Goudsmit's calculations (and Kronig's unpublished results). This discrepancy was due to the orientation of the electron's tangent frame, in addition to its position.
Mathematically speaking, a fiber bundle description is needed. The tangent bundle effect is additive and relativistic; that is, it vanishes if "c" goes to infinity. It is one half of the value obtained without regard for the tangent space orientation, but with opposite sign. Thus the combined effect differs from the latter by a factor two (Thomas precession).
Despite his initial objections, Pauli formalized the theory of spin in 1927, using the modern theory of quantum mechanics invented by Schrödinger and Heisenberg. He pioneered the use of Pauli matrices as a representation of the spin operators, and introduced a two-component spinor wave-function.
Pauli's theory of spin was non-relativistic. However, in 1928, Paul Dirac published the Dirac equation, which described the relativistic electron. In the Dirac equation, a four-component spinor (known as a "Dirac spinor") was used for the electron wave-function. In 1940, Pauli proved the "spin-statistics theorem", which states that fermions have half-integer spin and bosons integer spin.
In retrospect, the first direct experimental evidence of the electron spin was the Stern–Gerlach experiment of 1922. However, the correct explanation of this experiment was only given in 1927.

</doc>
<doc id="411231" url="https://en.wikipedia.org/wiki?curid=411231" title="Spin group">
Spin group

In mathematics the spin group Spin("n") is the double cover of the special orthogonal group , such that there exists a short exact sequence of Lie groups
As a Lie group, Spin("n") therefore shares its dimension, , and its Lie algebra with the special orthogonal group. 
For "n" > 2, Spin("n") is simply connected and so coincides with the universal cover of SO("n").
The non-trivial element of the kernel is denoted −1 , which should not be confused with the orthogonal transform of reflection through the origin, generally denoted −"I" .
Spin("n") can be constructed as a subgroup of the invertible elements in the Clifford algebra "C"ℓ("n").
Accidental isomorphisms.
In low dimensions, there are isomorphisms among the classical Lie groups called "accidental isomorphisms". For instance, there are isomorphisms between low-dimensional spin groups and certain classical Lie groups, owing to low-dimensional isomorphisms between the root systems (and corresponding isomorphisms of Dynkin diagrams) of the different families of simple Lie algebras. Specifically, we have
There are certain vestiges of these isomorphisms left over for  = 7, 8 (see Spin(8) for more details). For higher , these isomorphisms disappear entirely.
Indefinite signature.
In indefinite signature, the spin group Spin("p", "q") is constructed through Clifford algebras in a similar way to standard spin groups. It is a connected double cover of SO0("p", "q"), the connected component of the identity of the indefinite orthogonal group SO("p", "q") (there are a variety of conventions on the connectedness of Spin("p", "q"); in this article, it is taken to be connected for  ). As in definite signature, there are some accidental isomorphisms in low dimensions:
Note that Spin("p", "q") = Spin("q", "p").
Topological considerations.
Connected and simply connected Lie groups are classified by their Lie algebra. So if "G" is a connected Lie group with a simple Lie algebra, with "G"′ the universal cover of "G", there is an inclusion
with Z("G"′) the center of "G"′. This inclusion and the Lie algebra formula_7 of "G" determine "G" entirely (note that it is not the fact that formula_7 and π1("G") determine "G" entirely; for instance SL(2, R) and PSL(2, R) have the same Lie algebra and same fundamental group Z, but are not isomorphic).
The definite signature Spin("n") are all simply connected for "n" > 2 , so they are the universal coverings for SO("n").
In indefinite signature, Spin("p", "q") is not connected, and in general the identity component, Spin0("p",  "q"), is not simply connected, thus it is not a universal cover. The fundamental group is most easily understood by considering the maximal compact subgroup of SO("p",  "q") , which is SO("p") × SO("q"), and noting that rather than being the product of the 2-fold covers (hence a 4-fold cover), Spin("p",  "q") is the "diagonal" 2-fold cover – it is a 2-fold quotient of the 4-fold cover. Explicitly, the maximal compact connected subgroup of Spin("p",  "q") is
This allows us to calculate the fundamental groups of Spin("p", "q"), taking "p" ≥ "q":
Thus once "p", "q" > 2 the fundamental group is Z2, as it is a 2-fold quotient of a product of two universal covers.
The maps on fundamental groups are given as follows. For "p", "q" > 2, this implies that the map π1(Spin("p", "q")) → π1(SO("p", "q")) is given by 1 ∈ Z2 going to (1,1) ∈ Z2 × Z2. For "p" = 2, "q" > 2 , this map is given by 1 ∈ Z → (1,1) ∈ Z × Z2. And finally, for "p" = "q" = 2 , (1,0) ∈ Z × Z is sent to (1,1) ∈ Z × Z and (0, 1) is sent to (1, −1).
Center.
The center of the spin groups, for n≥3, (complex and real) are given as follows:
Quotient groups.
Quotient groups can be obtained from a spin group by quotienting out by a subgroup of the center, with the spin group then being a covering group of the resulting quotient, and both groups having the same Lie algebra.
Quotienting out by the entire center yields the minimal such group, the projective special orthogonal group, which is centerless, while quotienting out by {±1} yields the special orthogonal group – if the center equals {±1} (namely in odd dimension), these two quotient groups agree. If the spin group is simply connected (as Spin("n") is for "n" > 2), then Spin is the "maximal" group in the sequence, and one has a sequence of three groups,
splitting by parity yields:
which are the three compact real forms (or two, if SO = PSO ) of the compact Lie algebra formula_11
The homotopy groups of the cover and the quotient are related by the long exact sequence of a fibration, with discrete fiber (the fiber being the kernel) – thus all homotopy groups for "k" > 1 are equal, but π0 and π1 may differ.
For "n" > 2, Spin("n") is simply connected (π0 = π1 = {1} is trivial), so SO("n") is connected and has fundamental group Z2 while PSO("n") is connected and has fundamental group equal to the center of Spin("n").
In indefinite signature the covers and homotopy groups are more complicated – Spin("p", "q") is not simply connected, and quotienting also affects connected components. The analysis is simpler if one considers the maximal (connected) compact SO("p") × SO("q") ⊂ SO("p", "q") and the component group of Spin("p", "q").
Discrete subgroups.
Discrete subgroups of the spin group can be understood by relating them to discrete subgroups of the special orthogonal group (rotational point groups).
Given the double cover Spin("n") → SO("n"), by the lattice theorem, there is a Galois connection between subgroups of Spin("n") and subgroups of SO("n") (rotational point groups): the image of a subgroup of Spin("n") is a rotational point group, and the preimage of a point group is a subgroup of Spin("n"), and the closure operator on subgroups of Spin("n") is multiplication by {±1}. These may be called "binary point groups"; most familiar is the 3-dimensional case, known as binary polyhedral groups.
Concretely, every binary point group is either the preimage of a point group (hence denoted 2"G", for the point group "G"), or is an index 2 subgroup of the preimage of a point group which maps (isomorphically) onto the point group; in the latter case the full binary group is abstractly formula_12 (since {±1} is central). As an example of these latter, given a cyclic group of odd order formula_13 in SO("n"), its preimage is a cyclic group of twice the order, formula_14 and the subgroup "C"2"k"+1 < Spin("n") maps isomorphically to "C"2"k"+1 < SO("n").
Of particular note are two series:
For point groups that reverse orientation, the situation is more complicated, as there are two pin groups, so there are two possible binary groups corresponding to a given point group.
Complex case.
The spin"c" group is defined by the exact sequence 
This has important applications in 4-manifold theory and Seiberg–Witten theory.

</doc>
<doc id="4644081" url="https://en.wikipedia.org/wiki?curid=4644081" title="Spin magnetic moment">
Spin magnetic moment

In physics, mainly quantum mechanics and particle physics, a spin magnetic moment is the magnetic moment induced by the spin of elementary particles. For example the electron is an elementary spin-1/2 fermion. Quantum electrodynamics gives the most accurate prediction of the anomalous magnetic moment of the electron.
"Spin" is a non–classical property of elementary particles, since classically the "spin angular momentum" of a material object is really just the total "orbital" angular momenta of the object's constituents about the rotation axis. Elementary particles are conceived as concepts which have no axis to "spin" around (see wave-particle duality).
In general, a magnetic moment can be defined in terms of an electric current and the area enclosed by the current loop. Since angular momentum corresponds to rotational motion, the magnetic moment can be related to the orbital angular momentum of the charge carriers in the constituting the current. However, in magnetic materials, the atomic and molecular dipoles have magnetic moments not just because of their quantized orbital angular momentum, but the spin of elementary particles constituting them (electrons, and the quarks in the protons and neutrons of the atomic nuclei). A particle may have a spin magnetic moment without having an electric charge; the neutron is electrically neutral but has a non–zero magnetic moment, because of its internal quark structure.
Calculation.
We can calculate the observable spin magnetic moment, a vector, ""S, for a sub-atomic particle with charge "q", mass "m", and spin angular momentum (also a vector), , via:
where formula_1 is the gyromagnetic ratio, "g" is a dimensionless number, called the g-factor, "q" is the charge, and "m" is the mass. The "g"-factor depends on the particle: it is "g" = −2.0023 for the electron, "g" = 5.586 for the proton, and for the neutron. The proton and neutron are composed of quarks, which have a non-zero charge and a spin of "ħ"/2, and this must be taken into account when calculating their g-factors. Even though the neutron has a charge , its quarks give it a magnetic moment. The proton and electron's spin magnetic moments can be calculated by setting and , respectively, where "e" is the elementary charge.
The intrinsic electron magnetic dipole moment is approximately equal to the Bohr magneton "μ"B because and the electron's spin is also "ħ"/2:
Equation () is therefore normally written as
Just like the "total spin angular momentum" cannot be measured, neither can the "total spin magnetic moment" be measured. Equations (), (), () give the physical observable, that component of the magnetic moment measured along an axis, relative to or along the applied field direction. Assuming a Cartesian coordinate system, conventionally, the "z"-axis is chosen but the observable values of the component of spin angular momentum along all three axes are each ±"ħ"/2. However, in order to obtain the magnitude of the total spin angular momentum, be replaced by its eigenvalue, , where "s" is the spin quantum number. In turn, calculation of the magnitude of the total spin magnetic moment requires that () be replaced by:
Thus, for a single electron, with spin quantum number , the component of the magnetic moment along the field direction is, from (), , while the (magnitude of the) total spin magnetic moment is, from (), , or approximately 1.73 Bohr magnetons.
The analysis is readily extended to the spin-only magnetic moment of an atom. For example, the total spin magnetic moment (sometimes referred to as the "effective magnetic moment" when the orbital moment contribution to the total magnetic moment is neglected) of a transition metal ion with a single d shell electron outside of closed shells (e.g. Titanium Ti3+) is 1.73 "μ"B since , while an atom with two unpaired electrons (e.g. Vanadium V3+) with would have an effective magnetic moment of .
Spin in chemistry.
Spin magnetic moments create a basis for one of the most important principles in chemistry, the Pauli exclusion principle. This principle, first suggested by Wolfgang Pauli, governs most of modern-day chemistry. The theory plays further roles than just the explanations of doublets within electromagnetic spectrum. This additional quantum number, spin, became the basis for the modern standard model used today, which includes the use of Hund's rules, and an explanation of beta decay.
History.
The idea of a spin angular momentum was first proposed in a 1925 publication by George Uhlenbeck and Samuel Goudsmit to explain hyperfine splitting in atomic spectra. In 1928, Paul Dirac provided a rigorous theoretical foundation for the concept in the Dirac equation for the wavefunction of the electron. 

</doc>
<doc id="1077261" url="https://en.wikipedia.org/wiki?curid=1077261" title="Spin quantum number">
Spin quantum number

In atomic physics, the spin quantum number is a quantum number that parameterizes the intrinsic angular momentum (or spin angular momentum, or simply spin) of a given particle. The spin quantum number is the fourth of a set of quantum numbers (the principal quantum number, the azimuthal quantum number, the magnetic quantum number, and the spin quantum number), which describe the unique quantum state of an electron and is designated by the letter . It describes the energy, shape and orientation of orbitals.
Derivation.
As a solution for a certain partial differential equation, the quantized angular momentum (see angular momentum quantum number) can be written as:
where
Given an arbitrary direction "z" (usually determined by an external magnetic field) the spin "z"-projection is given by
where is the secondary spin quantum number, ranging from − to + in steps of one. This generates different values of .
The allowed values for "s" are non-negative integers or half-integers. Fermions (such as the electron, proton or neutron) have half-integer values, whereas bosons (e.g., photon, mesons) have integer spin values.
Algebra.
The algebraic theory of spin is a carbon copy of the Angular momentum in quantum mechanics theory.
First of all, spin satisfies the fundamental commutation relation:
where εlmn is the (antisymmetric) Levi-Civita symbol. This means that it is impossible to know two coordinates of the spin at the same time because of the restriction of the uncertainty principle.
Next, the eigenvectors of formula_9 and formula_10 satisfy:
where formula_14 are the creation and annihilation (or "raising" and "lowering" or "up" and "down") operators.
Electron spin.
Early attempts to explain the behavior of electrons in atoms focused on solving the Schrödinger wave equation for the hydrogen atom, the simplest possible case, with a single electron bound to the atomic nucleus. This was successful in explaining many features of atomic spectra.
The solutions required each possible state of the electron to be described by three "quantum numbers". These were identified as, respectively, the electron "shell" number , the "orbital" number , and the "orbital angular momentum" number . Angular momentum is a so-called "classical" concept measuring the momentum of a mass in circular motion about a point. The shell numbers start at 1 and increase indefinitely. Each shell of number contains ² orbitals. Each orbital is characterized by its number , where takes integer values from "0" to −1, and its angular momentum number , where takes integer values from + to −. By means of a variety of approximations and extensions, physicists were able to extend their work on hydrogen to more complex atoms containing many electrons.
Atomic spectra measure radiation absorbed or emitted by electrons "jumping" from one "state" to another, where a state is represented by values of , , and . The so-called "Transition rule" limits what "jumps" are possible. In general, a jump or "transition" is allowed only if all three numbers change in the process. This is because a transition will be able to cause the emission or absorption of electromagnetic radiation only if it involves a change in the electromagnetic dipole of the atom.
However, it was recognized in the early years of quantum mechanics that atomic spectra measured in an external magnetic field (see Zeeman effect) cannot be predicted with just , , and . A solution to this problem was suggested in early 1925 by George Uhlenbeck and Samuel Goudsmit, students of Paul Ehrenfest (who rejected the idea), and independently by Ralph Kronig, one of Landé's assistants. Uhlenbeck, Goudsmit, and Kronig introduced the idea of the self-rotation of the electron, which would naturally give rise to an angular momentum vector in addition to the one associated with orbital motion (quantum numbers and ).
The spin angular momentum is characterized by a quantum number; s = 1/2 specifically for electrons. In a way analogous to other quantized angular momenta, L, it is possible to obtain an expression for the total spin angular momentum:
where
The hydrogen spectra fine structure is observed as a doublet corresponding to two possibilities for the "z"-component of the angular momentum, where for any given direction "z":
whose solution has only two possible "z"-components for the electron. In the electron, the two different spin orientations are sometimes called "spin-up" or "spin-down".
The spin property of an electron would give rise to magnetic moment, which was a requisite for the fourth quantum number. The electron spin magnetic moment is given by the formula:
where
and by the equation:
where formula_20 is the Bohr magneton.
When atoms have even numbers of electrons the spin of each electron in each orbital has opposing orientation to that of its immediate neighbor(s). However, many atoms have an odd number of electrons or an arrangement of electrons in which there is an unequal number of "spin-up" and "spin-down" orientations. These atoms or electrons are said to have unpaired spins that are detected in electron spin resonance.
Detection of spin.
When lines of the hydrogen spectrum are examined at very high resolution, they are found to be closely spaced doublets. This splitting is called fine structure, and was one of the first experimental evidences for electron spin. The direct observation of the electron's intrinsic angular momentum was achieved in the Stern–Gerlach experiment.
The Stern–Gerlach experiment.
The theory of spatial quantization of the spin moment of the momentum of electrons of atoms situated in the magnetic field needed to be proved experimentally. In 1920 (two years before the theoretical description of the spin was created) Otto Stern and Walter Gerlach observed it in the experiment they conducted.
Silver atoms were evaporated using an electric furnace in a vacuum. Using thin slits, the atoms were guided into a flat beam and the beam sent through an in-homogeneous magnetic field before colliding with a metallic plate. The laws of classical physics predict that the collection of condensed silver atoms on the plate should form a thin solid line in the same shape as the original beam. However, the in-homogeneous magnetic field caused the beam to split in two separate directions, creating two lines on the metallic plate.
The phenomenon can be explained with the spatial quantization of the spin moment of momentum. In atoms the electrons are paired such that one spins upward and one downward, neutralizing the effect of their spin on the action of the atom as a whole. But in the valence shell of silver atoms, there is a single electron whose spin remains unbalanced.
The unbalanced spin creates spin magnetic moment, making the electron act like a very small magnet. As the atoms pass through the in-homogeneous magnetic field, the force moment in the magnetic field influences the electron's dipole until its position matches the direction of the stronger field. The atom would then be pulled toward or away from the stronger magnetic field a specific amount, depending on the value of the valence electron's spin. When the spin of the electron is +1/2 the atom moves away from the stronger field, and when the spin is −1/2 the atom moves toward it. Thus the beam of silver atoms is split while traveling through the in-homogeneous magnetic field, according to the spin of each atom's valence electron.
In 1927 Phipps and Taylor conducted a similar experiment, using atoms of hydrogen with similar results. Later scientists conducted experiments using other atoms that have only one electron in their valence shell: (copper, gold, sodium, potassium). Every time there two lines formed on the metallic plate.
The atomic nucleus also may have spin, but protons and neutrons are much heavier than electrons (about 1836 times), and the magnetic dipole moment is inversely proportional to the mass. So the nuclear magnetic dipole momentum is much smaller than that of the whole atom. This small magnetic dipole was later measured by Stern, Frisch and Easterman.
Dirac equation solves spin.
When the idea of electron spin was first introduced in 1925, even Wolfgang Pauli had trouble accepting Ralph Kronig's model. The problem was not that a rotating charged particle would have given rise to a magnetic field but that the electron was so small that the equatorial speed of the electron would have to be greater than the speed of light for the magnetic moment to be of the observed strength.
In 1930, Paul Dirac developed a new version of the Wave Equation which was relativistically invariant (unlike Schrödinger's one), and predicted the magnetic moment correctly, and at the same time treated the electron as a point particle. In the Dirac equation all four quantum numbers including the additional quantum number, , arose naturally during its solution.

</doc>
<doc id="2778009" url="https://en.wikipedia.org/wiki?curid=2778009" title="Spin tensor">
Spin tensor

In mathematics, mathematical physics, and theoretical physics, the spin tensor is a quantity used to describe the rotational motion of particles in spacetime. The tensor has application in 
general relativity and special relativity, as well as quantum mechanics, relativistic quantum mechanics, and quantum field theory.
The Euclidean group SE(d) of direct isometries is generated by translations and rotations. Its Lie algebra is written formula_1.
This article uses Cartesian coordinates and tensor index notation.
Background on Noether currents.
The Noether current for translations in space is momentum, while the current for increments in time is energy. These two statements combine into one in spacetime: translations in spacetime, i.e. a displacement between two events, is generated by the four-momentum "P". Conservation of four-momentum is given by the continuity equation:
where formula_3 is the stress–energy tensor, and ∂ are partial derivatives that make up the four gradient (in non-Cartesian coordinates this must be replaced by the covariant derivative). Integrating over spacetime:
gives the four-momentum vector at time "t".
The Noether current for a rotation about the point "y" is given by a tensor of 3rd order, denoted formula_5. Because of the Lie algebra relations
where the 0 subscript indicates the origin (unlike momentum, angular momentum depends on the origin), the integral:
gives the angular momentum tensor formula_8 at time "t".
Definition.
The spin tensor is defined at a point x to be the value of the Noether current at x of a rotation about "x",
The continuity equation
implies:
and therefore, the stress–energy tensor is not a symmetric tensor.
The quantity "S" is the density of spin angular momentum (spin in this case is not only for a point-like particle, but also for an extended bodies), and "M" is the density of orbital angular momentum. The total angular momentum is always the sum of spin and orbital contributions.
The relation:
gives the torque density showing the rate of conversion between the orbital angular momentum and spin.
Examples.
Examples of materials with a nonzero spin density are molecular fluids, the electromagnetic field and turbulent fluids. For molecular fluids, the individual molecules may be spinning. The electromagnetic field can have circularly polarized light. For turbulent fluids, we may arbitrarily make a distinction between long wavelength phenomena and short wavelength phenomena. A long wavelength vorticity may be converted via turbulence into tinier and tinier vortices transporting the angular momentum into smaller and smaller wavelengths while simultaneously reducing the vorticity. This can be approximated by the eddy viscosity.

</doc>
<doc id="19621522" url="https://en.wikipedia.org/wiki?curid=19621522" title="Spin-flip">
Spin-flip

A black hole spin-flip occurs when the spin axis of a rotating black hole undergoes a sudden change in orientation due to absorption of a second (smaller) black hole.
Spin-flips are believed to be a consequence of galaxy mergers, when two supermassive black holes form a bound pair at the center of the merged galaxy and coalesce after emitting gravitational waves.
Spin-flips are significant astrophysically since a number of physical processes are associated with black hole spins; for instance, jets in active galaxies are believed to be launched parallel to the spin axes of supermassive black holes.
A change in the rotation axis of a black hole due to a spin-flip would therefore result in a change in the direction of the jet.
Physics of Spin-Flips.
A spin-flip is a late stage in the evolution of a binary black hole. The binary consists of two black holes, with masses formula_1 and formula_2, that revolve around their common center of mass. The total angular momentum formula_3 of the binary system is the sum of the angular momentum of the orbit, formula_4, plus the spin angular momenta formula_5 of the two holes. If we write formula_6 as the masses of each hole and formula_7 as their Kerr parameters, then use the angle from north of their spin axes as given by formula_8, we can write,
formula_9
formula_10
formula_11
If the orbital separation is sufficiently small, emission of energy and angular momentum in the form of gravitational radiation will cause the orbital separation to drop. Eventually, the smaller hole formula_2 reaches the innermost stable circular orbit, or ISCO, around the larger hole. Once the ISCO is reached, there no longer exists a stable orbit, and the smaller hole plunges into the larger hole, coalescing with it. The final angular momentum after coalescence is just
formula_13
the spin angular momentum of the single, coalesced hole. Neglecting the angular momentum that is carried away by gravitational waves during the final plunge—which is small—conservation of angular momentum implies
formula_14
formula_15 is of order formula_16 times formula_17 and can be ignored if formula_2 is much smaller than formula_19. Making this approximation,
formula_20
This equation states that the final spin of the hole is the sum of the larger hole's initial spin plus the orbital angular momentum of the smaller hole at the last stable orbit. Since the vectors formula_17 and formula_22 are generically oriented in different directions, formula_23 will point in a different direction than formula_24—a spin-flip.
The angle by which the black hole's spin re-orients itself depends on the relative size of formula_25 and formula_26, and on the angle between them. At one extreme, if formula_24 is very small, the final spin will be dominated by formula_25 and the flip angle can be large. At the other extreme, the larger black hole might be a maximally-rotating Kerr black hole initially. Its spin angular momentum is then of order
formula_29
The orbital angular momentum of the smaller hole at the ISCO depends on the direction of its orbit, but is of order
formula_30
Comparing these two expressions, it follows that even a fairly small hole, with mass about one-fifth that of the larger hole, can reorient the larger hole by 90 degrees or more.
Connection with radio galaxies.
Black hole spin-flips were first discussed in the context of a particular class of radio galaxy, the X-shaped radio sources. The X-shaped galaxies exhibit two, misaligned pairs of radio lobes: the "active" lobes and the "wings". It is believed that the wings are oriented in the direction of the jet prior to the spin-flip, and that the active lobes point in the current jet direction. The spin-flip could have been caused by absorption of a second black hole during a galaxy merger.

</doc>
<doc id="3017092" url="https://en.wikipedia.org/wiki?curid=3017092" title="Spin-½">
Spin-½

In quantum mechanics, spin is an intrinsic property of all elementary particles. Fermions, the particles that constitute ordinary matter, have half-integer spin. All known elementary fermions have a spin of ½.
Overview.
Particles having net spin ½ include the proton, neutron, electron, neutrino, and quarks. The dynamics of spin-½ objects cannot be accurately described using classical physics; they are among the simplest systems which require quantum mechanics to describe them. As such, the study of the behavior of spin-½ systems forms a central part of quantum mechanics.
A spin-½ particle is characterized by an angular momentum quantum number for spin s of 1/2. In solutions of the Schrödinger equation, angular momentum is quantized according to this number, so that total spin angular momentum
However, the observed fine structure when the electron is observed along one axis, such as the Z-axis, is quantized in terms of a magnetic quantum number, which can be viewed as a quantization of a vector component of this total angular momentum, which can have only the values of ±½ħ.
Note that these values for angular momentum are functions only of the reduced Planck constant (the angular momentum of any photon), with no dependence on mass or charge.
Stern–Gerlach experiment.
The necessity of introducing half-integral spin goes back experimentally to the results of the Stern–Gerlach experiment. A beam of atoms is run through a strong heterogeneous magnetic field, which then splits into N parts depending on the intrinsic angular momentum of the atoms. It was found that for silver atoms, the beam was split in two—the ground state therefore could not be integral, because even if the intrinsic angular momentum of the atoms were as small as possible, 1, the beam would be split into 3 parts, corresponding to atoms with Lz = −1, 0, and +1. The conclusion was that silver atoms had net intrinsic angular momentum of .
General properties.
Spin-½ objects are all fermions (a fact explained by the spin statistics theorem) and satisfy the Pauli exclusion principle. Spin-½ particles can have a permanent magnetic moment along the direction of their spin, and this magnetic moment gives rise to electromagnetic interactions that depend on the spin. One such effect that was important in the discovery of spin is the Zeeman effect, the splitting of a spectral line into several components in the presence of a static magnetic field.
Unlike in more complicated quantum mechanical systems, the spin of a spin-½ particle can be expressed as a linear combination of just two eigenstates, or eigenspinors. These are traditionally labeled spin up and spin down. Because of this, the quantum-mechanical spin operators can be represented as simple 2 × 2 matrices. These matrices are called the Pauli matrices.
Creation and annihilation operators can be constructed for spin-½ objects; these obey the same commutation relations as other angular momentum operators.
Connection to the uncertainty principle.
One consequence of the generalized uncertainty principle is that the spin projection operators (which measure the spin along a given direction like "x", "y", or "z") cannot be measured simultaneously. Physically, this means that it is ill-defined what axis a particle is spinning about. A measurement of the "z" component of spin destroys any information about the "x" and "y" components that might previously have been obtained.
Complex phase.
Mathematically, quantum mechanical spin is not described by a vector as in classical angular momentum. It is described by a complex-valued vector with two components called a spinor. There are subtle differences between the behavior of spinors and vectors under coordinate rotations, stemming from the behavior of a vector space over a complex field.
When a spinor is rotated by 360 degrees (one full turn), it transforms to its negative, and then after a further rotation of 360 degrees it transforms back to its initial value again. This is because in quantum theory the state of a particle or system is represented by a complex probability amplitude (wavefunction) Ψ, and when the system is measured, the probability of finding the system in the state Ψ equals |Ψ|2 = Ψ*Ψ, the square of the absolute value of the amplitude.
Suppose a detector that can be rotated measures a particle in which the probabilities of detecting some state are affected by the rotation of the detector. When the system is rotated through 360 degrees, the observed output and physics are the same as initially but the amplitudes are changed for a spin-½ particle by a factor of −1 or a phase shift of half of 360 degrees. When the probabilities are calculated, the −1 is squared, (−1)2 = 1, so the predicted physics is the same as in the starting position. Also, in a spin-½ particle there are only two spin states and the amplitudes for both change by the same −1 factor, so the interference effects are identical, unlike the case for higher spins. The complex probability amplitudes are something of a theoretical construct which cannot be directly observed.
If the probability amplitudes rotated by the same amount as the detector, then they would have changed by a factor of −1 when the equipment was rotated by 180 degrees, which when squared would predict the same output as at the start, but experiments show this to be wrong. If the detector is rotated by 180 degrees, the result with spin-½ particles can be different to what it would be if not rotated, hence the factor of a half is necessary to make the predictions of the theory match the experiments.
Mathematical description.
NRQM (Non-relativistic quantum mechanics).
The quantum state of a spin-½ particle can be described by a two-component complex-valued vector called a spinor. Observable states of the particle are then found by the spin operators "Sx", "Sy", and "Sz," and the total spin operator S.
Observables.
When spinors are used to describe the quantum states, the three spin operators ("Sx", "Sy", "Sz,") can be described by 2×2 matrices called the Pauli matrices whose eigenvalues are ±.
For example, the spin projection operator "Sz" affects a measurement of the spin in the "z" direction.
The two eigenvalues of "Sz", ±, then correspond to the following eigenspinors:
These vectors form a complete basis for the Hilbert space describing the spin-½ particle. Thus, linear combinations of these two states can represent all possible states of the spin, including in the x and y directions.
The ladder operators are:
Since "S±"="Sx"±"iSy", "Sx"=("S+"+"S-") and "Sy"=("S+"-"S-"). Thus:
Their normalized eigenspinors can be found in the usual way. For "Sx", they are:
For "Sy", they are:
RQM (relativistic quantum mechanics).
While NRQM defines spin-½ with 2 dimensions in Hilbert space with dynamics that are described in 3-dimensional space and time, RQM define the spin with 4 dimensions in Hilbert space and dynamics described by 4-dimensional space-time.
Observables.
As a consequence of the four-dimensional nature of space-time in relativity, relativistic quantum mechanics uses 4×4 matrices to describe spin operators and observables.
Spin as a consequence of combining quantum theory and special relativity.
When physicist Paul Dirac tried to modify the Schrödinger equation so that it was consistent with Einstein's theory of relativity, he found it was only possible by including matrices in the resulting Dirac Equation, implying the wave must have multiple components leading to spin.

</doc>
<doc id="29276" url="https://en.wikipedia.org/wiki?curid=29276" title="Spinor">
Spinor

In geometry and physics, spinors are elements of a (complex) vector space that can be associated with Euclidean space. Like geometric vectors and more general tensors, spinors transform linearly when the Euclidean space is subjected to a slight (infinitesimal) rotation. When a sequence of such small rotations is composed (integrated) to form an overall final rotation, however, the resulting spinor transformation depends on which sequence of small rotations was used, "unlike" for vectors and tensors. A spinor transforms to its negative when the space is rotated through a complete turn from 0° to 360° (see picture), and it is this property that characterizes spinors. It is also possible to associate a substantially similar notion of spinor to Minkowski space in which case the Lorentz transformations of special relativity play the role of rotations. Spinors were introduced in geometry by Élie Cartan in 1913. In the 1920s physicists discovered that spinors are essential to describe the intrinsic angular momentum, or "spin", of the electron and other subatomic particles.
Spinors are characterized by the specific way in which they behave under rotations. They change in different ways depending not just on the overall final rotation, but the details of how that rotation was achieved (by a continuous path in the rotation group). There are two topologically distinguishable classes (homotopy classes) of paths through rotations that result in the same overall rotation, as famously illustrated by the belt trick puzzle (below). These two inequivalent classes yield spinor transformations of opposite sign. The spin group is the group of all rotations keeping track of the class. It doubly covers the rotation group, since each rotation can be obtained in two inequivalent ways as the endpoint of a path. The space of spinors by definition is equipped with a (complex) linear representation of the spin group, meaning that elements of the spin group act as linear transformations on the space of spinors, in a way that genuinely depends on the homotopy class.
Although spinors can be defined purely as elements of a representation space of the spin group (or its Lie algebra of infinitesimal rotations), they are typically defined as elements of a vector space that carries a linear representation of the Clifford algebra. The Clifford algebra is an associative algebra that can be constructed from Euclidean space and its inner product in a basis independent way. Both the spin group and its Lie algebra are embedded inside the Clifford algebra in a natural way, and in applications the Clifford algebra is often the easiest to work with. After choosing an orthonormal basis of Euclidean space, a representation of the Clifford algebra is generated by gamma matrices, matrices that satisfy a set of canonical anti-commutation relations. The spinors are the column vectors on which these matrices act. In three Euclidean dimensions, for instance, the Pauli spin matrices are a set of gamma matrices, and the two-component complex column vectors on which these matrices act are spinors. However, the particular matrix representation of the Clifford algebra, and hence what precisely constitutes a "column vector" (or spinor), involves the choice of basis and gamma matrices in an essential way. As a representation of the spin group, this realization of spinors as (complex) column vectors will either be irreducible if the dimension is odd, or it will decompose into a pair of so-called "half-spin" or Weyl representations if the dimension is even.
Introduction.
What characterizes spinors and distinguishes them from geometric vectors and other tensors is subtle. Consider applying a rotation to the coordinates of a system. No object in the system itself has moved, only the coordinates have, so there will always be a compensating change in those coordinate values when applied to any object of the system. Geometrical vectors, for example, have components that will undergo "the same" rotation as the coordinates. More broadly, any tensor associated with the system (for instance, the stress of some medium) also has coordinate descriptions that adjust to compensate for changes to the coordinate system itself. Spinors do not appear at this level of the description of a physical system, when one is concerned only with the properties of a single isolated rotation of the coordinates. Rather, spinors appear when we imagine that instead of a single rotation, the coordinate system is gradually (continuously) rotated between some initial and final configuration. For any of the familiar and intuitive ("tensorial") quantities associated with the system, the transformation law does not depend on the precise details of how the coordinates arrived at their final configuration. Spinors, on the other hand, are constructed in such a way that makes them "sensitive" to how the gradual rotation of the coordinates arrived there: they exhibit path-dependence. It turns out that, for any final configuration of the coordinates, there are actually two ("topologically") inequivalent "gradual" (continuous) rotations of the coordinate system that result in this same configuration. This ambiguity is called the homotopy class of the gradual rotation. The belt trick puzzle (shown) famously demonstrates two different rotations, one through an angle of 2π and the other through an angle of 4π, having the same final configurations but different classes. Spinors actually exhibit a sign-reversal that genuinely depends on this homotopy class. This distinguishes them from vectors and other tensors, none of which can feel the class.
Spinors can be exhibited as concrete objects using a choice of Cartesian coordinates. In three Euclidean dimensions, for instance, spinors can be constructed by making a choice of Pauli spin matrices corresponding to (angular momenta about) the three coordinate axes. These are 2×2 matrices with complex entries, and the two-component complex column vectors on which these matrices act by matrix multiplication are the spinors. In this case, the spin group is isomorphic to the group of 2×2 unitary matrices with determinant one, which naturally sits inside the matrix algebra. This group acts by conjugation on the real vector space spanned by the Pauli matrices themselves, realizing it as a group of rotations among them, but it also acts on the column vectors (that is, the spinors).
More generally, a Clifford algebra can be constructed from any vector space equipped with a (nondegenerate) quadratic form, such as Euclidean space with its standard dot product or Minkowski space with its standard Lorentz metric. Given a suitably normalized basis of , the Clifford algebra is generated by gamma matrices, matrices that satisfy a set of canonical anti-commutation relations, and the space of spinors is the space of column vectors with formula_1 components on which those matrices act. Although the Clifford algebra can be defined abstractly in a coordinate-independent way, its particular realization as a specific algebra of matrices depends on which orthogonal axes the gamma matrices represent. So what precisely constitutes a "column vector" (or spinor) also depends on such arbitrary choices. The orthogonal Lie algebra (i.e., the infinitesimal "rotations") and the spin group associated to the quadratic form are both (canonically) contained in the Clifford algebra, so every Clifford algebra representation also defines a representation of the Lie algebra and the spin group. Depending on the dimension and metric signature, this realization of spinors as column vectors may be irreducible or it may decompose into a pair of so-called "half-spin" or Weyl representations.
Overview.
There are essentially two frameworks for viewing the notion of a spinor.
One is representation theoretic. In this point of view, one knows beforehand that there are some representations of the Lie algebra of the orthogonal group that cannot be formed by the usual tensor constructions. These missing representations are then labeled the spin representations, and their constituents "spinors". In this view, a spinor must belong to a representation of the double cover of the rotation group , or more generally of double cover of the generalized special orthogonal group on spaces with metric signature . These double covers are Lie groups, called the spin groups or . All the properties of spinors, and their applications and derived objects, are manifested first in the spin group. Representations of the double covers of these groups yield projective representations of the groups themselves, which do not meet the full definition of a representation.
The other point of view is geometrical. One can explicitly construct the spinors, and then examine how they behave under the action of the relevant Lie groups. This latter approach has the advantage of providing a concrete and elementary description of what a spinor is. However, such a description becomes unwieldy when complicated properties of spinors, such as Fierz identities, are needed.
Clifford algebras.
The language of Clifford algebras (sometimes called geometric algebras) provides a complete picture of the spin representations of all the spin groups, and the various relationships between those representations, via the classification of Clifford algebras. It largely removes the need for "ad hoc" constructions.
In detail, let "V" be a finite-dimensional complex vector space with nondegenerate bilinear form "g". The Clifford algebra is the algebra generated by "V" along with the anticommutation relation . It is an abstract version of the algebra generated by the gamma or Pauli matrices. If "V" = C"n", with the standard form we denote the Clifford algebra by Cℓ"n"(C). Since by the choice of an orthonormal basis every complex vectorspace with non-degenerate form is isomorphic to this standard example, this notation is abused more generally if . If is even, Cℓ"n"(C) is isomorphic as an algebra (in a non-unique way) to the algebra of complex matrices (by the Artin-Wedderburn theorem and the easy to prove fact that the Clifford algebra is central simple). If is odd, Cℓ2"k"+1(C) is isomorphic to the algebra of two copies of the complex matrices. Therefore, in either case has a unique (up to isomorphism) irreducible representation (also called simple Clifford module), commonly denoted by Δ, of dimension 2["n"/2]. Since the Lie algebra is embedded as a Lie subalgebra in equipped with the Clifford algebra commutator as Lie bracket, the space Δ is also a Lie algebra representation of called a spin representation. If "n" is odd, this Lie algebra representation is irreducible. If "n" is even, it splits further into two irreducible representations called the Weyl or "half-spin representations".
Irreducible representations over the reals in the case when "V" is a real vector space are much more intricate, and the reader is referred to the Clifford algebra article for more details.
Spin groups.
Spinors form a vector space, usually over the complex numbers, equipped with a linear group representation of the spin group that does not factor through a representation of the group of rotations (see diagram). The spin group is the group of rotations keeping track of the homotopy class. Spinors are needed to encode basic information about the topology of the group of rotations because that group is not simply connected, but the simply connected spin group is its double cover. So for every rotation there are two elements of the spin group that represent it. Geometric vectors and other tensors cannot feel the difference between these two elements, but they produce "opposite" signs when they affect any spinor under the representation. Thinking of the elements of the spin group as homotopy classes of one-parameter families of rotations, each rotation is represented by two distinct homotopy classes of paths to the identity. If a one-parameter family of rotations is visualized as a ribbon in space, with the arc length parameter of that ribbon being the parameter (its tangent, normal, binormal frame actually gives the rotation), then these two distinct homotopy classes are visualized in the two states of the belt trick puzzle (above). The space of spinors is an auxiliary vector space that can be constructed explicitly in coordinates, but ultimately only exists up to isomorphism in that there is no "natural" construction of them that does not rely on arbitrary choices such as coordinate systems. A notion of spinors can be associated, as such an auxiliary mathematical object, with any vector space equipped with a quadratic form such as Euclidean space with its standard dot product, or Minkowski space with its Lorentz metric. In the latter case, the "rotations" include the Lorentz boosts, but otherwise the theory is substantially similar.
Terminology in physics.
The most typical type of spinor, the Dirac spinor, is an element of the fundamental representation of , the complexification of the Clifford algebra , into which the spin group may be embedded. On a 2"k"- or 2"k"+1-dimensional space a Dirac spinor may be represented as a vector of 2"k" complex numbers. (See Special unitary group.) In even dimensions, this representation is reducible when taken as a representation of and may be decomposed into two: the left-handed and right-handed Weyl spinor representations. In addition, sometimes the non-complexified version of has a smaller real representation, the Majorana spinor representation. If this happens in an even dimension, the Majorana spinor representation will sometimes decompose into two Majorana–Weyl spinor representations. Dirac and Weyl spinors are complex representations while Majorana spinors are real representations.
The Dirac, Lorentz, Weyl, and Majorana spinors are interrelated, and their relation can be elucidated on the basis of real geometric algebra. 
Massive particles, such as electrons, are described as Dirac spinors. The classical neutrino of the standard model of particle physics is an example of a Weyl spinor. However, because of observed neutrino oscillation, it is now believed that they are not Weyl spinors, but perhaps instead Majorana spinors. It is not known whether (spin-1/2) Weyl spinors exist in nature. In 2015, an international team led by Princeton University scientists announced that they had found a quasiparticle that behaves as a Weyl fermion.
Spinors in representation theory.
One major mathematical application of the construction of spinors is to make possible the explicit construction of linear representations of the Lie algebras of the special orthogonal groups, and consequently spinor representations of the groups themselves. At a more profound level, spinors have been found to be at the heart of approaches to the Atiyah–Singer index theorem, and to provide constructions in particular for discrete series representations of semisimple groups.
The spin representations of the special orthogonal Lie algebras are distinguished from the tensor representations given by Weyl's construction by the weights. Whereas the weights of the tensor representations are integer linear combinations of the roots of the Lie algebra, those of the spin representations are half-integer linear combinations thereof. Explicit details can be found in the spin representation article.
Attempts at intuitive understanding.
The spinor can be described, in simple terms, as “vectors of a space the transformations of which are related in a particular way to rotations in physical space”. Stated differently:
Several ways of illustrating everyday analogies have been formulated in terms of the plate trick, tangloids and other examples of orientation entanglement.
Nonetheless, the concept is generally considered notoriously difficult to understand, as illustrated by Michael Atiyah's statement that is recounted by Dirac's biographer Graham Farmelo:
History.
The most general mathematical form of spinors was discovered by Élie Cartan in 1913. The word "spinor" was coined by Paul Ehrenfest in his work on quantum physics.
Spinors were first applied to mathematical physics by Wolfgang Pauli in 1927, when he introduced his spin matrices. The following year, Paul Dirac discovered the fully relativistic theory of electron spin by showing the connection between spinors and the Lorentz group. By the 1930s, Dirac, Piet Hein and others at the Niels Bohr Institute (then known as the Institute for Theoretical Physics of the University of Copenhagen) created toys such as Tangloids to teach and model the calculus of spinors.
Spinor spaces were represented as left ideals of a matrix algebra in 1930, by G. Juvet and by Fritz Sauter. More specifically, instead of representing spinors as complex-valued 2D column vectors as Pauli had done, they represented them as complex-valued 2 × 2 matrices in which only the elements of the left column are non-zero. In this manner the spinor space became a minimal left ideal in .
In 1947 Marcel Riesz constructed spinor spaces as elements of a minimal left ideal of Clifford algebras. In 1966/1967, David Hestenes replaced spinor spaces by the even subalgebra Cℓ01,3(R) of the spacetime algebra Cℓ1,3(R). As of the 1980s, the theoretical physics group at Birkbeck College around David Bohm and Basil Hiley has been developing algebraic approaches to quantum theory that build on Sauter and Riesz' identification of spinors with minimal left ideals.
Examples.
Some simple examples of spinors in low dimensions arise from considering the even-graded subalgebras of the Clifford algebra . This is an algebra built up from an orthonormal basis of mutually orthogonal vectors under addition and multiplication, "p" of which have norm +1 and "q" of which have norm −1, with the product rule for the basis vectors
Two dimensions.
The Clifford algebra Cℓ2,0(R) is built up from a basis of one unit scalar, 1, two orthogonal unit vectors, "σ"1 and "σ"2, and one unit pseudoscalar . From the definitions above, it is evident that , and .
The even subalgebra Cℓ02,0(R), spanned by "even-graded" basis elements of Cℓ2,0(R), determines the space of spinors via its representations. It is made up of real linear combinations of 1 and "σ"1"σ"2. As a real algebra, Cℓ02,0(R) is isomorphic to field of complex numbers C. As a result, it admits a conjugation operation (analogous to complex conjugation), sometimes called the "reverse" of a Clifford element, defined by
which, by the Clifford relations, can be written
The action of an even Clifford element on vectors, regarded as 1-graded elements of Cℓ2,0(R), is determined by mapping a general vector to the vector
where "γ"∗ is the conjugate of "γ", and the product is Clifford multiplication. In this situation, a spinor is an ordinary complex number. The action of "γ" on a spinor "φ" is given by ordinary complex multiplication:
An important feature of this definition is the distinction between ordinary vectors and spinors, manifested in how the even-graded elements act on each of them in different ways. In general, a quick check of the Clifford relations reveals that even-graded elements conjugate-commute with ordinary vectors:
On the other hand, comparing with the action on spinors , "γ" on ordinary vectors acts as the "square" of its action on spinors.
Consider, for example, the implication this has for plane rotations. Rotating a vector through an angle of "θ" corresponds to , so that the corresponding action on spinors is via . In general, because of logarithmic branching, it is impossible to choose a sign in a consistent way. Thus the representation of plane rotations on spinors is two-valued.
In applications of spinors in two dimensions, it is common to exploit the fact that the algebra of even-graded elements (that is just the ring of complex numbers) is identical to the space of spinors. So, by abuse of language, the two are often conflated. One may then talk about "the action of a spinor on a vector." In a general setting, such statements are meaningless. But in dimensions 2 and 3 (as applied, for example, to computer graphics) they make sense.
Three dimensions.
The Clifford algebra Cℓ3,0(R) is built up from a basis of one unit scalar, 1, three orthogonal unit vectors, "σ"1, "σ"2 and "σ"3, the three unit bivectors "σ"1"σ"2, "σ"2"σ"3, "σ"3"σ"1 and the pseudoscalar . It is straightforward to show that , and .
The sub-algebra of even-graded elements is made up of scalar dilations,
and vector rotations
where
corresponds to a vector rotation through an angle "θ" about an axis defined by a unit vector .
As a special case, it is easy to see that, if , this reproduces the "σ"1"σ"2 rotation considered in the previous section; and that such rotation leaves the coefficients of vectors in the "σ"3 direction invariant, since
The bivectors "σ"2"σ"3, "σ"3"σ"1 and "σ"1"σ"2 are in fact Hamilton's quaternions i, j and k, discovered in 1843:
With the identification of the even-graded elements with the algebra H of quaternions, as in the case of two dimensions the only representation of the algebra of even-graded elements is on itself. Thus the (real) spinors in three-dimensions are quaternions, and the action of an even-graded element on a spinor is given by ordinary quaternionic multiplication.
Note that the expression (1) for a vector rotation through an angle "θ", the angle appearing in "γ" was halved. Thus the spinor rotation (ordinary quaternionic multiplication) will rotate the spinor "ψ" through an angle one-half the measure of the angle of the corresponding vector rotation. Once again, the problem of lifting a vector rotation to a spinor rotation is two-valued: the expression (1) with in place of "θ"/2 will produce the same vector rotation, but the negative of the spinor rotation.
The spinor/quaternion representation of rotations in 3D is becoming increasingly prevalent in computer geometry and other applications, because of the notable brevity of the corresponding spin matrix, and the simplicity with which they can be multiplied together to calculate the combined effect of successive rotations about different axes.
Explicit constructions.
A space of spinors can be constructed explicitly with concrete and abstract constructions. The
equivalence of these constructions are a consequence of the uniqueness of the spinor representation of the complex Clifford algebra. For a complete example in dimension 3, see spinors in three dimensions.
Component spinors.
Given a vector space "V" and a quadratic form "g" an explicit matrix representation of the Clifford algebra can be defined as follows. Choose an orthonormal basis for "V" i.e. where and for . Let . Fix a set of matrices such that (i.e. fix a convention for the gamma matrices). Then the assignment extends uniquely to an algebra homomorphism by sending the monomial in the Clifford algebra to the product of matrices and extending linearly. The space on which the gamma matrices act is a now a space of spinors. One needs to construct such matrices explicitly, however. In dimension 3, defining the gamma matrices to be the Pauli sigma matrices gives rise to the familiar two component spinors used in non relativistic quantum mechanics. Likewise using the Dirac gamma matrices gives rise to the 4 component Dirac spinors used in 3+1 dimensional relativistic quantum field theory. In general, in order to define gamma matrices of the required kind, one can use the Weyl–Brauer matrices.
In this construction the representation of the Clifford algebra , the Lie algebra , and the Spin group , all depend on the choice of the orthonormal basis and the choice of the gamma matrices. This can cause confusion over conventions, but invariants like traces are independent of choices. In particular, all physically observable quantities must be independent of such choices. In this construction a spinor can be represented as a vector of 2"k" complex numbers and is denoted with spinor indices (usually "α", "β", "γ"). In the physics literature, abstract spinor indices are often used to denote spinors even when an abstract spinor construction is used.
Abstract spinors.
There are at least two different, but essentially equivalent, ways to define spinors abstractly. One approach seeks to identify the minimal ideals for the left action of on itself. These are subspaces of the Clifford algebra of the form , admitting the evident action of by left-multiplication: . There are two variations on this theme: one can either find a primitive element that is a nilpotent element of the Clifford algebra, or one that is an idempotent. The construction via nilpotent elements is more fundamental in the sense that an idempotent may then be produced from it. In this way, the spinor representations are identified with certain subspaces of the Clifford algebra itself. The second approach is to construct a vector space using a distinguished subspace of , and then specify the action of the Clifford algebra "externally" to that vector space.
In either approach, the fundamental notion is that of an isotropic subspace . Each construction depends on an initial freedom in choosing this subspace. In physical terms, this corresponds to the fact that there is no measurement protocol that can specify a basis of the spin space, even if a preferred basis of is given.
As above, we let be an -dimensional complex vector space equipped with a nondegenerate bilinear form. If is a real vector space, then we replace by its complexification and let denote the induced bilinear form on . Let be a maximal isotropic subspace, i.e. a maximal subspace of such that . If is even, then let be an isotropic subspace complementary to . If is odd, let be a maximal isotropic subspace with , and let be the orthogonal complement of . In both the even- and odd-dimensional cases and have dimension . In the odd-dimensional case, is one-dimensional, spanned by a unit vector .
Minimal ideals.
Since "W"′ is isotropic, multiplication of elements of "W"′ inside is skew. Hence vectors in "W"′ anti-commute, and is just the exterior algebra Λ∗"W"′. Consequently, the "k"-fold product of "W"′ with itself, "W"′"k", is one-dimensional. Let "ω" be a generator of "W"′"k". In terms of a basis of in "W"′, one possibility is to set
Note that (i.e., "ω" is nilpotent of order 2), and moreover, for all . The following facts can be proven easily:
In detail, suppose for instance that "n" is even. Suppose that "I" is a non-zero left ideal contained in . We shall show that "I" must be equal to by proving that it contains a nonzero scalar multiple of "ω".
Fix a basis "w"i of "W" and a complementary basis "w"i′ of "W"′ so that
Note that any element of "I" must have the form "αω", by virtue of our assumption that . Let be any such element. Using the chosen basis, we may write
where the "a"i1…ip are scalars, and the "B"j are auxiliary elements of the Clifford algebra. Observe now that the product
Pick any nonzero monomial "a" in the expansion of "α" with maximal homogeneous degree in the elements "w"i:
then
is a nonzero scalar multiple of "ω", as required.
Note that for "n" even, this computation also shows that
as a vector space. In the last equality we again used that "W" is isotropic. In physics terms, this shows that Δ is built up like a Fock space by creating spinors using anti-commuting creation operators in "W" acting on a vacuum "ω".
Exterior algebra construction.
The computations with the minimal ideal construction suggest that a spinor representation can
also be defined directly using the exterior algebra of the isotropic subspace "W".
Let denote the exterior algebra of "W" considered as vector space only. This will be the spin representation, and its elements will be referred to as spinors.
The action of the Clifford algebra on Δ is defined first by giving the action of an element of "V" on Δ, and then showing that this action respects the Clifford relation and so extends to a homomorphism of the full Clifford algebra into the endomorphism ring End(Δ) by the universal property of Clifford algebras. The details differ slightly according to whether the dimension of "V" is even or odd.
When dim("V") is even, where "W"′ is the chosen isotropic complement. Hence any decomposes uniquely as with and . The action of "v" on a spinor is given by
where "i"("w"′) is interior product with "w"′ using the non degenerate quadratic form to identify "V" with "V"∗, and ε(w) denotes the exterior product. It may be verified that
and so "c" respects the Clifford relations and extends to a homomorphism from the Clifford algebra to End(Δ).
The spin representation Δ further decomposes into a pair of irreducible complex representations of the Spin group (the half-spin representations, or Weyl spinors) via
When dim("V") is odd, , where "U" is spanned by a unit vector "u" orthogonal to "W". The Clifford action "c" is defined as before on , while the Clifford action of (multiples of) "u" is defined by
As before, one verifies that "c" respects the Clifford relations, and so induces a homomorphism.
Hermitian vector spaces and spinors.
If the vector space "V" has extra structure that provides a decomposition of its complexification into two maximal isotropic subspaces, then the definition of spinors (by either method) becomes natural.
The main example is the case that the real vector space "V" is a hermitian vector space , i.e., "V" is equipped with a complex structure "J" that is an orthogonal transformation with respect to the inner product "g" on "V". Then splits in the ±"i" eigenspaces of "J". These eigenspaces are isotropic for the complexification of "g" and can be identified with the complex vector space and its complex conjugate . Therefore, for a hermitian vector space the vector space Λ (as well as its complex conjugate Λ"V") is a spinor space for the underlying real euclidean vector space.
With the Clifford action as above but with contraction using the hermitian form, this construction gives a spinor space at every point of an almost Hermitian manifold and is the reason why every almost complex manifold (in particular every symplectic manifold) has a Spinc structure. Likewise, every complex vector bundle on a manifold carries a Spinc structure.
Clebsch–Gordan decomposition.
A number of Clebsch–Gordan decompositions are possible on the tensor product of one spin representation with another. These decompositions express the tensor product in terms of the alternating representations of the orthogonal group.
For the real or complex case, the alternating representations are
In addition, for the real orthogonal groups, there are three characters (one-dimensional representations)
The Clebsch–Gordan decomposition allows one to define, among other things:
Even dimensions.
If is even, then the tensor product of Δ with the contragredient representation decomposes as
which can be seen explicitly by considering (in the Explicit construction) the action of the Clifford algebra on decomposable elements . The rightmost formulation follows from the transformation properties of the Hodge star operator. Note that on restriction to the even Clifford algebra, the paired summands are isomorphic, but under the full Clifford algebra they are not.
There is a natural identification of Δ with its contragredient representation via the conjugation in the Clifford algebra:
So also decomposes in the above manner. Furthermore, under the even Clifford algebra, the half-spin representations decompose
For the complex representations of the real Clifford algebras, the associated reality structure on the complex Clifford algebra descends to the space of spinors (via the explicit construction in terms of minimal ideals, for instance). In this way, we obtain the complex conjugate of the representation Δ, and the following isomorphism is seen to hold:
In particular, note that the representation Δ of the orthochronous spin group is a unitary representation. In general, there are Clebsch–Gordan decompositions
In metric signature , the following isomorphisms hold for the conjugate half-spin representations
Using these isomorphisms, one can deduce analogous decompositions for the tensor products of the half-spin representations .
Odd dimensions.
If is odd, then
In the real case, once again the isomorphism holds
Hence there is a Clebsch–Gordan decomposition (again using the Hodge star to dualize) given by
Consequences.
There are many far-reaching consequences of the Clebsch–Gordan decompositions of the spinor spaces. The most fundamental of these pertain to Dirac's theory of the electron, among whose basic requirements are

</doc>
<doc id="1871162" url="https://en.wikipedia.org/wiki?curid=1871162" title="Spin–orbit interaction">
Spin–orbit interaction

In quantum physics, the spin–orbit interaction (also called spin–orbit effect or spin–orbit coupling) is an interaction of a particle's spin with its motion. The first and best known example of this is that spin–orbit interaction causes shifts in an electron's atomic energy levels due to electromagnetic interaction between the electron's spin and the magnetic field generated by the electron's orbit around the nucleus. This is detectable as a splitting of spectral lines. A similar effect, due to the relationship between angular momentum and the strong nuclear force, occurs for protons and neutrons moving inside the nucleus, leading to a shift in their energy levels in the nucleus shell model. In the field of spintronics, spin–orbit effects for electrons in semiconductors and other materials are explored for technological applications. The spin–orbit interaction is one cause of magnetocrystalline anisotropy.
Spin–orbit interaction in atomic energy levels.
This section presents a relatively simple and quantitative description of the spin–orbit interaction for an electron bound to an atom, up to first order in perturbation theory, using some semiclassical electrodynamics and non-relativistic quantum mechanics. This gives results that agree reasonably well with observations. A more rigorous derivation of the same result would start with the Dirac equation, and achieving a more precise result would involve calculating small corrections from quantum electrodynamics.
Energy of a magnetic moment.
The energy of a magnetic moment in a magnetic field is given by:
where μ is the magnetic moment of the particle and B is the magnetic field it experiences.
Magnetic field.
We shall deal with the magnetic field first. Although in the rest frame of the nucleus, there is no magnetic field acting on the electron, there "is" one in the rest frame of the electron. Ignoring for now that this frame is not inertial, in SI units we end up with the equation
where v is the velocity of the electron and E the electric field it travels through. Now we know that E is radial so we can rewrite formula_3.
Also we know that the momentum of the electron formula_4. Substituting this in and changing the order of the cross product gives:
Next, we express the electric field as the gradient of the electric potential formula_6. Here we make the central field approximation, that is, that the electrostatic potential is spherically symmetric, so is only a function of radius. This approximation is exact for hydrogen and hydrogen-like systems. Now we can say
where formula_8 is the potential energy of the electron in the central field, and "e" is the elementary charge. Now we remember from classical mechanics that the angular momentum of a particle formula_9. Putting it all together we get
It is important to note at this point that "B" is a positive number multiplied by "L", meaning that the magnetic field is parallel to the orbital angular momentum of the particle, which is itself perpendicular to the particle's velocity.
Magnetic moment of the electron.
The magnetic moment of the electron is
where formula_12 is the spin angular momentum vector, formula_13 is the Bohr magneton and formula_14 is the electron spin g-factor. Here, formula_15 is a negative constant multiplied by the spin, so the magnetic moment is antiparallel to the spin angular momentum.
The spin–orbit potential consists of two parts. The Larmor part is connected to the interaction of 
the magnetic moment of the electron with the magnetic field of the nucleus in the co-moving frame of the electron. The second contribution is related to Thomas precession.
Larmor interaction energy.
The Larmor interaction energy is
Substituting in this equation expressions for the magnetic moment and the magnetic field, one gets
Now, we have to take into account Thomas precession correction for the electron's curved trajectory.
Thomas interaction energy.
In 1926 Llewellyn Thomas relativistically recomputed the doublet separation in the fine structure of the atom. Thomas precession rate, formula_18, is related to the angular frequency of the orbital motion, formula_19, of a spinning particle as follows 
where formula_21 is the Lorentz factor of the moving particle. The Hamiltonian producing the spin 
precession formula_18 is given by
To the first order in formula_24, we obtain
Total interaction energy.
The total spin–orbit potential in an external electrostatic potential takes the form 
The net effect of Thomas precession is the reduction of the Larmor interaction energy by factor 1/2 which came to be known as the "Thomas half".
Evaluating the energy shift.
Thanks to all the above approximations, we can now evaluate the detailed energy shift in this model. In particular, we wish to find a basis that diagonalizes both "H0" (the non-perturbed Hamiltonian) and "ΔH". To find out what basis this is, we first define the total angular momentum operator
Taking the dot product of this with itself, we get
(since L and S commute), and therefore
It can be shown that the five operators "H"0, J2, L2, S2, and "J"z all commute with each other and with Δ"H". Therefore, the basis we were looking for is the simultaneous eigenbasis of these five operators (i.e., the basis where all five are diagonal). Elements of this basis have the five quantum numbers: "n" (the "principal quantum number") "j" (the "total angular momentum quantum number"), "l" (the "orbital angular momentum quantum number"), "s" (the "spin quantum number"), and "j"z (the ""z"-component of total angular momentum").
To evaluate the energies, we note that
for hydrogenic wavefunctions (here formula_31 is the Bohr radius divided by the nuclear charge "Z"); and
Final energy shift.
We can now say
where
Spin–orbit interaction in solids.
A crystalline solid (semiconductor, metal etc.) is characterized by its band structure. While on the overall scale (including the core levels) the spin–orbit interaction is still a small perturbation, it may play a relatively more important role if we zoom in to bands close to the Fermi level (formula_36). The atomic formula_37 interaction for example splits bands which would be otherwise degenerate and the particular form of this spin–orbit splitting (typically of the order of few to few hundred millielectronvolts) depends on the particular system. The bands of interest can be then described by various effective models, usually based on some perturbative approach. An example of how the atomic spin–orbit interaction influences the band structure of a crystal is explained in the article about Rashba interaction.
Examples of effective Hamiltonians.
Hole bands of a bulk (3D) zinc-blende semiconductor will be split by formula_38 into heavy and light holes (which form a formula_39 quadruplet in the formula_40-point of the Brillouin zone) and a split-off band (formula_41 doublet). Including two conduction bands (formula_42 doublet in the formula_40-point), the system is described by the effective eight-band model of Kohn and Luttinger. If only top of the valence band is of interest (for example when formula_44, Fermi level measured from the top of the valence band), the proper four-band effective model is
where formula_46 are the Luttinger parameters (analogous to the single effective mass of a one-band model of electrons) and formula_47 are angular momentum 3/2 matrices (formula_48 is the free electron mass). In combination with magnetization, this type of spin–orbit interaction will distort the electronic bands depending on the magnetization direction, thereby causing Magnetocrystalline anisotropy (a special type of Magnetic anisotropy).
If the semiconductor moreover lacks the inversion symmetry, the hole bands will exhibit cubic Dresselhaus splitting. Within the four bands (light and heavy holes), the dominant term is
where the material parameter formula_50 for GaAs (see pp. 72 in Winkler's book, according to more recent data the Dresselhaus constant in GaAs is 9 eVÅ3; the total Hamiltonian will be formula_51). Two-dimensional electron gas in an asymmetric quantum well (or heterostructure) will feel the Rashba interaction. The appropriate two-band effective Hamiltonian is
where formula_53 is the 2 × 2 identity matrix, formula_54 the Pauli matrices and formula_55 the electron effective mass. The spin–orbit part of the Hamiltonian, formula_56 is parametrized by formula_57, sometimes called the Rashba parameter (its definition somewhat varies), which is related to the structure asymmetry.
Above expressions for spin–orbit interaction couple spin matrices formula_58 and formula_59 to the quasi-momentum formula_60, and to the vector potential formula_61 of an "AC" electric field through the Peierls substitution formula_62. They are lower order terms of the Luttinger–Kohn formula_63 expansion in powers of formula_64. Next terms of this expansion also produce terms that couple spin operators of the electron coordinate formula_65. Indeed, a cross product formula_66 is invariant with respect to time inversion. In cubic crystals, it has a symmetry of a vector and acquires a meaning of a spin–orbit contribution formula_67 to the operator of coordinate. For electrons in semiconductors with a narrow gap formula_68 between the conduction and heavy hole bands, Yafet derived the equation
where formula_70 is a free electron mass, and formula_71 is a formula_71-factor properly renormalized for spin–orbit interaction. This operator couples electron spin formula_73 directly to the electric field formula_74 through the interaction energy formula_75.
Electron spin in inhomogeneous magnetic field.
Distinctive feature of spin-orbit interaction is presence in the Hamiltonian of a term that includes a product of orbital and spin operators. In atomic systems these are orbital and spin angular momenta formula_76 and formula_77, respectively, and in solids the quasimomentum formula_60 and Pauli matrices formula_59. This term couples orbital and spin dynamics. In particular, it allows manipulating electron spin by "ac" electric field through Electric Dipole Spin Resonance (EDSR).
A similar effect can be achieved through the Larmor energy formula_80 if the magnetic field formula_81 is inhomogeneous. Then the derivatives such as formula_82 play a role similar to spin-orbit coupling and allow electrical manipulation of electron spin. In solids, formula_80 should be changed to formula_84, where formula_85 is the Bohr magneton and formula_86 is a formula_71-factor tensor; formula_86 can also be formula_65-dependent. In particular, this mechanism is currently used for EDSR in nanostructures.
Further reading.
A. Manchon, H. C. Koo, J. Nitta, S. M. Frolov, and R. A. Duine, New perspectives for Rashba spin–orbit coupling, Nature Materials 14, 871-882 (2015), http://www.nature.com.ezp-prod1.hul.harvard.edu/nmat/journal/v14/n9/pdf/nmat4360.pdf

</doc>
<doc id="409529" url="https://en.wikipedia.org/wiki?curid=409529" title="Spontaneous fission">
Spontaneous fission

Spontaneous fission (SF) is a form of radioactive decay that is found only in very heavy chemical elements. The nuclear binding energy of the elements reaches its maximum at an atomic mass number of about 58; spontaneous breakdown into smaller nuclei and a few isolated nuclear particles becomes possible at greater atomic mass numbers.
Because of constraints in forming the daughter fission-product nuclei, spontaneous fission into known nuclides becomes "theoretically" possible (that is, energetically possible) for some atomic nuclei with atomic masses greater than 92 atomic mass units (amu), with the probability of spontaneous fission increasing as the atomic mass increases above this value.
History.
The first nuclear fission process discovered was the fission induced by neutrons. Because cosmic rays produce some neutrons, it was difficult to distinguish between induced and spontaneous events. Cosmic rays can be reliably shielded by a thick layer of rock or water. The spontaneous fission was identified in 1940 by Soviet physicists Georgy Flyorov and Konstantin Petrzhak by their observations of uranium in the Moscow Metro Dinamo station, underground.
Cluster decay was shown to be a superasymmetric spontaneous fission process.
Feasibility.
Elemental.
The lightest natural nuclides that are hypothetically subject to spontaneous fission are niobium-93 and molybdenum-94 (elements 41 and 42, respectively). Spontaneous fission has never been observed in the naturally occurring isotopes of these elements, however. In practice, these are stable isotopes.
Spontaneous fission is feasible over practical observation times only for atomic masses of 232 amu or more. These are elements at least as heavy as thorium-232 – which has a half-life somewhat longer than the age of the universe. Thorium-232 is the lightest primordial nuclide that has left evidence of undergoing spontaneous fission in its minerals.
The known elements most susceptible to spontaneous fission are the synthetic high-atomic-number actinide elements with odd atomic numbers: mendelevium and lawrencium, and also some of the transactinide superheavy elements, such as rutherfordium.
For naturally occurring thorium, uranium-235, and uranium-238, spontaneous fission does occur rarely, but in the vast majority of the radioactive decay of these atoms, alpha decay or beta decay occurs instead. Hence, the spontaneous fission of these isotopes is usually negligible, except in using the exact branching ratios when finding the radioactivity of a sample of these elements.
Mathematical.
Mathematically, the criterion for whether spontaneous fission can occur in a time short enough to be observed by present methods, is approximately:
where Z is the atomic number and A is the mass number (e.g. formula_2 = 36 for uranium-235).
Spontaneous fission rates.
Spontaneous fission rates:
In practice will invariably contain a certain amount of due to the tendency of to absorb an additional neutron during production. 's high rate of spontaneous fission events makes it an undesirable contaminant. Weapons-grade plutonium contains no more than 7.0% .
The rarely used gun-type atomic bomb has a critical insertion time of about one millisecond, and the probability of a fission during this time interval should be small. 
Therefore only is suitable. Almost all nuclear bombs use some kind of implosion method.
Spontaneous fission can occur much more rapidly when the nucleus of an atom undergoes superdeformation.
Poisson process.
Spontaneous fission gives much the same result as induced nuclear fission. However, like other forms of radioactive decay, it occurs due to quantum tunneling, without the atom having been struck by a neutron or other particle as in induced nuclear fission. Spontaneous fissions release neutrons as all fissions do, so if a critical mass is present, a spontaneous fission can initiate a self-sustaining chain reaction. Radioisotopes for which spontaneous fission is not negligible can be used as neutron sources. For example, californium-252 (half-life 2.645 years, SF branch ratio about 3.1 percent) can be used for this purpose. The neutrons released can be used to inspect airline luggage for hidden explosives, to gauge the moisture content of soil in highway and building construction, or to measure the moisture of materials stored in silos, for example.
As long as the spontaneous fission gives a negligible reduction of the number of nuclei that can undergo such fission, this process can be approximated closely as a Poisson process. In this situation, for short time intervals the probability of a spontaneous fission is directly proportional to the length of time.
The spontaneous fission of uranium-238 and uranium-235 does leave trails of damage in the crystal structure of uranium-containing minerals when the fission fragments recoil through them. These trails, or "fission tracks", are the foundation of the radiometric dating method called fission track dating.

</doc>
<doc id="297466" url="https://en.wikipedia.org/wiki?curid=297466" title="Spontaneous symmetry breaking">
Spontaneous symmetry breaking

Spontaneous symmetry breaking is a mode of realization of symmetry breaking in a physical system, where the underlying laws are invariant under a symmetry transformation, but the system as a whole changes under such transformations, in contrast to explicit symmetry breaking. It is a spontaneous process by which a system in a symmetrical state ends up in an asymmetrical state. It thus describes systems where the equations of motion or the Lagrangian obey certain symmetries, but the lowest-energy solutions do not exhibit that symmetry.
Consider a symmetrical upward dome with a trough circling the bottom. If a ball is put at the very peak of the dome, the system is symmetrical with respect to a rotation around the center axis. But the ball may "spontaneously break" this symmetry by rolling down the dome into the trough, a point of lowest energy. Afterward, the ball has come to a rest at some fixed point on the perimeter. The dome and the ball retain their individual symmetry, but the system does not.
Most simple phases of matter and phase transitions, like crystals, magnets, and conventional superconductors can be simply understood from the viewpoint of spontaneous symmetry breaking. Notable exceptions include topological phases of matter like the fractional quantum Hall effect.
Spontaneous symmetry breaking in physics.
Particle physics.
In particle physics the force carrier particles are normally specified by field equations with gauge symmetry; their equations predict that certain measurements will be the same at any point in the field. For instance, field equations might predict that the mass of two quarks is constant. Solving the equations to find the mass of each quark might give two solutions. In one solution, quark A is heavier than quark B. In the second solution, quark B is heavier than quark A "by the same amount". The symmetry of the equations is not reflected by the individual solutions, but it is reflected by the range of solutions. An actual measurement reflects only one solution, representing a breakdown in the symmetry of the underlying theory. "Hidden" is perhaps a better term than "broken" because the symmetry is always there in these equations. This phenomenon is called "spontaneous" symmetry breaking because "nothing" (that we know) breaks the symmetry in the equations.
Chiral symmetry.
Chiral symmetry breaking is an example of spontaneous symmetry breaking affecting the chiral symmetry of the strong interactions in particle physics. It is a property of quantum chromodynamics, the quantum field theory describing these interactions, and is responsible for the bulk of the mass (over 99%) of the nucleons, and thus of all common matter, as it converts very light bound quarks into 100 times heavier constituents of baryons. The approximate Nambu–Goldstone bosons in this spontaneous symmetry breaking process are the pions, whose mass is an order of magnitude lighter than the mass of the nucleons. It served as the prototype and significant ingredient of the Higgs mechanism underlying the electroweak symmetry breaking.
Higgs mechanism.
The strong, weak, and electromagnetic forces can all be understood as arising from gauge symmetries. The Higgs mechanism, the spontaneous symmetry breaking of gauge symmetries, is an important component in understanding the superconductivity of metals and the origin of particle masses in the standard model of particle physics. One important consequence of the distinction between true symmetries and "gauge symmetries", is that the spontaneous breaking of a gauge symmetry does not give rise to characteristic massless Nambu–Goldstone modes, but only massive modes, like the plasma mode in a superconductor, or the Higgs mode observed in particle physics.
In the standard model of particle physics, spontaneous symmetry breaking of the gauge symmetry associated with the electro-weak force generates masses for several particles, and separates the electromagnetic and weak forces. The W and Z bosons are the elementary particles that mediate the weak interaction, while the photon mediates the electromagnetic interaction. At energies much greater than 100 GeV all these particles behave in a similar manner. The Weinberg–Salam theory predicts that, at lower energies, this symmetry is broken so that the photon and the massive W and Z bosons emerge. In addition, fermions develop mass consistently.
Without spontaneous symmetry breaking, the Standard Model of elementary particle interactions requires the existence of a number of particles. However, some particles (the W and Z bosons) would then be predicted to be massless, when, in reality, they are observed to have mass. To overcome this, spontaneous symmetry breaking is augmented by the Higgs mechanism to give these particles mass. It also suggests the presence of a new particle, the Higgs boson, reported as possibly identifiable with a boson detected in 2012. (If the Higgs boson were not confirmed to have been found, it would mean that the simplest implementation of the Higgs mechanism and spontaneous symmetry breaking "as they are currently formulated" require modification.)
Superconductivity of metals is a condensed-matter analog of the Higgs phenomena, in which a condensate of Cooper pairs of electrons spontaneously breaks the U(1) gauge "symmetry" associated with light and electromagnetism.
Condensed matter physics.
Most phases of matter can be understood through the lens of spontaneous symmetry breaking. For example, crystals are periodic arrays of atoms that are not invariant under all translations (only under a small subset of translations by a lattice vector). Magnets have north and south poles that are oriented in a specific direction, breaking rotational symmetry. In addition to these examples, there are a whole host of other symmetry-breaking phases of matter including nematic phases of liquid crystals, charge- and spin-density waves, superfluids and many others.
There are several known examples of matter that cannot be described by spontaneous symmetry breaking, including: topologically ordered phases of matter like fractional quantum Hall liquids, and spin-liquids. These states do not break any symmetry, but are distinct phases of matter. Unlike the case of spontaneous symmetry breaking, there is not a general framework for describing such states.
Continuous symmetry.
The ferromagnet is the canonical system which spontaneously breaks the continuous symmetry of the spins below the Curie temperature and at , where "h" is the external magnetic field. Below the Curie temperature the energy of the system is invariant under inversion of the magnetization "m"(x) such that . The symmetry is spontaneously broken as when the Hamiltonian becomes invariant under the inversion transformation, but the expectation value is not invariant.
Spontaneously, symmetry broken phases of matter are characterized by an order parameter that describes the quantity which breaks the symmetry under consideration. For example, in a magnet, the order parameter is the local magnetization.
Spontaneously breaking of a continuous symmetry is inevitably accompanied by gapless (meaning that these modes do not cost any energy to excite) Nambu–Goldstone modes associated with slow long-wavelength fluctuations of the order parameter. For example, vibrational modes in a crystal, known as phonons, are associated with slow density fluctuations of the crystal's atoms. The associated Goldstone mode for magnets are oscillating waves of spin known as spin-waves. For symmetry-breaking states, whose order parameter is not a conserved quantity, Nambu–Goldstone modes are typically massless and propagate at a constant velocity.
An important theorem, due to Mermin and Wagner, states that, at finite temperature, thermally activated fluctuations of Nambu–Goldstone modes destroy the long-range order, and prevent spontaneous symmetry breaking in one- and two-dimensional systems. Similarly, quantum fluctuations of the order parameter prevent most types of continuous symmetry breaking in one-dimensional systems even at zero temperature (an important exception is ferromagnets, whose order parameter, magnetization, is an exactly conserved quantity and does not have any quantum fluctuations).
Other long-range interacting systems such as cylindrical curved surfaces interacting via the Coulomb potential or Yukawa potential has been shown to break translational and rotational symmetries. It was shown, in the presence of a symmetric Hamiltonian, and in the limit of infinite volume, the system spontaneously adopts a chiral configuration, i.e. breaks mirror plane symmetry.
Dynamical symmetry breaking.
Dynamical symmetry breaking (DSB) is a special form of spontaneous symmetry breaking where the ground state of the system has reduced symmetry properties compared to its theoretical description (Lagrangian).
Dynamical breaking of a global symmetry is a spontaneous symmetry breaking, that happens not at the (classical) tree level (i.e. at the level of the bare action), but due to quantum corrections (i.e. at the level of the effective action).
Dynamical breaking of a gauge symmetry is subtler. In the conventional spontaneous gauge symmetry breaking, there exists an unstable Higgs particle in the theory, which drives the vacuum to a symmetry-broken phase (see e.g. Electroweak interaction). In dynamical gauge symmetry breaking, however, no unstable Higgs particle operates in the theory, but the bound states of the system itself provide the unstable fields that render the phase transition. For example, Bardeen, Hill, and Lindner published a paper which attempts to replace the conventional Higgs mechanism in the standard model, by a DSB that is driven by a bound state of top-antitop quarks (such models, where a composite particle plays the role of the Higgs boson, are often referred to as "Composite Higgs models"). Dynamical breaking of gauge symmetries is often due to creation of a fermionic condensate; for example the quark condensate, which is connected to the dynamical breaking of chiral symmetry in quantum chromodynamics. Conventional superconductivity is the paradigmatic example from the condensed matter side, where phonon-mediated attractions lead electrons to become bound in pairs and then condense, thereby breaking the electromagnetic gauge symmetry.
Generalisation and technical usage.
For spontaneous symmetry breaking to occur, there must be a system in which there are several equally likely outcomes. The system as a whole is therefore symmetric with respect to these outcomes. (If we consider any two outcomes, the probability is the same. This contrasts sharply to explicit symmetry breaking.) However, if the system is sampled (i.e. if the system is actually used or interacted with in any way), a specific outcome must occur. Though the system as a whole is symmetric, it is never encountered with this symmetry, but only in one specific asymmetric state. Hence, the symmetry is said to be spontaneously broken in that theory. Nevertheless, the fact that each outcome is equally likely is a reflection of the underlying symmetry, which is thus often dubbed "hidden symmetry", and has crucial formal consequences. (See the article on the Goldstone boson).
When a theory is symmetric with respect to a symmetry group, but requires that one element of the group be distinct, then spontaneous symmetry breaking has occurred. The theory must not dictate "which" member is distinct, only that "one is". From this point on, the theory can be treated as if this element actually is distinct, with the proviso that any results found in this way must be resymmetrized, by taking the average of each of the elements of the group being the distinct one.
The crucial concept in physics theories is the order parameter. If there is a field (often a background field) which acquires an expectation value (not necessarily a "vacuum" expectation value) which is not invariant under the symmetry in question, we say that the system is in the ordered phase, and the symmetry is spontaneously broken. This is because other subsystems interact with the order parameter, which specifies a "frame of reference" to be measured against. In that case, the vacuum state does not obey the initial symmetry (which would keep it invariant, in the linearly realized Wigner mode in which it would be a singlet), and, instead changes under the (hidden) symmetry, now implemented in the (nonlinear) Nambu–Goldstone mode. Normally, in the absence of the Higgs mechanism, massless Goldstone bosons arise.
The symmetry group can be discrete, such as the space group of a crystal, or continuous (e.g., a Lie group), such as the rotational symmetry of space. However, if the system contains only a single spatial dimension, then only discrete symmetries may be broken in a vacuum state of the full quantum theory, although a classical solution may break a continuous symmetry.
A pedagogical example: the Mexican hat potential.
In the simplest idealized relativistic model, the spontaneously broken symmetry is summarized through an illustrative scalar field theory. The relevant Lagrangian, which essentially dictates how a system behaves, can be split up into kinetic and potential terms,
It is in this potential term "V"("Φ") that the symmetry breaking is triggered. An example of a potential, due to Jeffrey Goldstone is illustrated in the graph at the right.
This potential has an infinite number of possible minima (vacuum states) given by
for any real "θ" between 0 and 2"π". The system also has an unstable vacuum state corresponding to . This state has a U(1) symmetry. However, once the system falls into a specific stable vacuum state (amounting to a choice of "θ"), this symmetry will appear to be lost, or "spontaneously broken".
In fact, any other choice of "θ" would have exactly the same energy, implying the existence of a massless Nambu–Goldstone boson, the mode running around the circle at the minimum of this potential, and indicating there is some memory of the original symmetry in the Lagrangian.
Nobel Prize.
On October 7, 2008, the Royal Swedish Academy of Sciences awarded the 2008 Nobel Prize in Physics to three scientists for their work in subatomic physics symmetry breaking. Yoichiro Nambu, of the University of Chicago, won half of the prize for the discovery of the mechanism of spontaneous broken symmetry in the context of the strong interactions, specifically chiral symmetry breaking. Physicists Makoto Kobayashi and Toshihide Maskawa shared the other half of the prize for discovering the origin of the explicit breaking of CP symmetry in the weak interactions. This origin is ultimately reliant on the Higgs mechanism, but, so far understood as a "just so" feature of Higgs couplings, not a spontaneously broken symmetry phenomenon.

</doc>
<doc id="27590" url="https://en.wikipedia.org/wiki?curid=27590" title="Standard deviation">
Standard deviation

In statistics, the standard deviation (SD, also represented by the Greek letter sigma σ or s) is a measure that is used to quantify the amount of variation or dispersion of a set of data values. A standard deviation close to 0 indicates that the data points tend to be very close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the data points are spread out over a wider range of values.
The standard deviation of a random variable, statistical population, data set, or probability distribution is the square root of its variance. It is algebraically simpler, though in practice less robust, than the average absolute deviation.
A useful property of the standard deviation is that, unlike the variance, it is expressed in the same units as the data. There are also other measures of deviation from the norm, including mean absolute deviation, which provide different mathematical properties from standard deviation.
In addition to expressing the variability of a population, the standard deviation is commonly used to measure confidence in statistical conclusions. For example, the margin of error in polling data is determined by calculating the expected standard deviation in the results if the same poll were to be conducted multiple times. The reported margin of error is typically about twice the standard deviation—the half-width of a 95 percent confidence interval. In science, researchers commonly report the standard deviation of experimental data, and only effects that fall much farther than two standard deviations away from what would have been expected are considered statistically significant—normal random error or variation in the measurements is in this way distinguished from causal variation. The standard deviation is also important in finance, where the standard deviation on the rate of return on an investment is a measure of the volatility of the investment.
When only a sample of data from a population is available, the term standard deviation of the sample or sample standard deviation can refer to either the above-mentioned quantity as applied to those data or to a modified quantity that is a better estimate of the population standard deviation (the standard deviation of the entire population).
Basic examples.
For a finite set of numbers, the standard deviation is found by taking the square root of the average of the squared deviations of the values from their average value. For example, the marks of a class of eight students (that is, a population) are the following eight values:
These eight data points have the mean (average) of 5:
First, calculate the deviations of each data point from the mean, and square the result of each:
The variance is the mean of these values:
and the "population" standard deviation is equal to the square root of the variance:
This formula is valid only if the eight values with which we began form the complete population. If the values instead were a random sample drawn from some larger parent population (for example, they were 8 marks randomly chosen from a class of 20), then we would have divided by instead of in the denominator of the last formula, and then the quantity thus obtained would be called the "sample" standard deviation. Dividing by "n"−1 gives a better estimate of the population standard deviation for the larger parent population than dividing by "n", which gives a result which is correct for the sample only. This is known as "Bessel's correction".
As a slightly more complicated real-life example, the average height for adult men in the United States is about 70 inches, with a standard deviation of around 3 inches. This means that most men (about 68%, assuming a normal distribution) have a height within 3 inches of the mean (67–73 inches)  – one standard deviation – and almost all men (about 95%) have a height within 6 inches of the mean (64–76 inches) – two standard deviations. If the standard deviation were zero, then all men would be exactly 70 inches tall. If the standard deviation were 20 inches, then men would have much more variable heights, with a typical range of about 50–90 inches. Three standard deviations account for 99.7% of the sample population being studied, assuming the distribution is normal (bell-shaped).
Definition of population values.
Let "X" be a random variable with mean value "μ":
Here the operator "E" denotes the average or expected value of "X". Then the standard deviation of "X" is the quantity
(derived using the properties of expected value).
In other words, the standard deviation "σ" (sigma) is the square root of the variance of "X"; i.e., it is the square root of the average value of ("X" − "μ")2.
The standard deviation of a (univariate) probability distribution is the same as that of a random variable having that distribution. Not all random variables have a standard deviation, since these expected values need not exist. For example, the standard deviation of a random variable that follows a Cauchy distribution is undefined because its expected value "μ" is undefined.
Discrete random variable.
In the case where "X" takes random values from a finite data set "x"1, "x"2, ..., "xN", with each value having the same probability, the standard deviation is
or, using summation notation,
If, instead of having equal probabilities, the values have different probabilities, let "x"1 have probability "p"1, "x"2 have probability "p"2, ..., "x""N" have probability "p"N. In this case, the standard deviation will be
Continuous random variable.
The standard deviation of a continuous real-valued random variable "X" with probability density function "p"("x") is
and where the integrals are definite integrals taken for "x" ranging over the set of possible values of the random variable "X".
In the case of a parametric family of distributions, the standard deviation can be expressed in terms of the parameters. For example, in the case of the log-normal distribution with parameters "μ" and "σ"2, the standard deviation is [(exp("σ"2) − 1)exp(2"μ" + "σ"2)]1/2.
Estimation.
One can find the standard deviation of an entire population in cases (such as standardized testing) where every member of a population is sampled. In cases where that cannot be done, the standard deviation "σ" is estimated by examining a random sample taken from the population and computing a statistic of the sample, which is used as an estimate of the population standard deviation. Such a statistic is called an estimator, and the estimator (or the value of the estimator, namely the estimate) is called a sample standard deviation, and is denoted by "s" (possibly with modifiers). However, unlike in the case of estimating the population mean, for which the sample mean is a simple estimator with many desirable properties (unbiased, efficient, maximum likelihood), there is no single estimator for the standard deviation with all these properties, and unbiased estimation of standard deviation is a very technically involved problem. Most often, the standard deviation is estimated using the "corrected sample standard deviation" (using "N" − 1), defined below, and this is often referred to as the "sample standard deviation", without qualifiers. However, other estimators are better in other respects: the uncorrected estimator (using "N") yields lower mean squared error, while using "N" − 1.5 (for the normal distribution) almost completely eliminates bias.
Uncorrected sample standard deviation.
Firstly, the formula for the "population" standard deviation (of a finite population) can be applied to the sample, using the size of the sample as the size of the population (though the actual population size from which the sample is drawn may be much larger). This estimator, denoted by "s""N", is known as the uncorrected sample standard deviation, or sometimes the standard deviation of the sample (considered as the entire population), and is defined as follows:
where formula_13 are the observed values of the sample items and formula_14 is the mean value of these observations, while the denominator "N" stands for the size of the sample: this is the square root of the sample variance, which is the average of the squared deviations about the sample mean.
This is a consistent estimator (it converges in probability to the population value as the number of samples goes to infinity), and is the maximum-likelihood estimate when the population is normally distributed. However, this is a biased estimator, as the estimates are generally too low. The bias decreases as sample size grows, dropping off as 1/"n", and thus is most significant for small or moderate sample sizes; for formula_15 the bias is below 1%. Thus for very large sample sizes, the uncorrected sample standard deviation is generally acceptable. This estimator also has a uniformly smaller mean squared error than the corrected sample standard deviation.
Corrected sample standard deviation.
If the "biased sample variance" (the second central moment of the sample, which is a downward-biased estimate of the population variance) is used to compute an estimate of the population's standard deviation, the result is
Here taking the square root introduces further downward bias, by Jensen's inequality, due to the square root being a concave function. The bias in the variance is easily corrected, but the bias from the square root is more difficult to correct, and depends on the distribution in question.
An unbiased estimator for the "variance" is given by applying Bessel's correction, using "N" − 1 instead of "N" to yield the "unbiased sample variance," denoted "s"2:
This estimator is unbiased if the variance exists and the sample values are drawn independently with replacement. "N" − 1 corresponds to the number of degrees of freedom in the vector of deviations from the mean, formula_18
Taking square roots reintroduces bias (because the square root is a nonlinear function, which does not commute with the expectation), yielding the corrected sample standard deviation, denoted by "s:"
As explained above, while "s"2 is an unbiased estimator for the population variance, "s" is still a biased estimator for the population standard deviation, though markedly less biased than the uncorrected sample standard deviation. The bias is still significant for small samples ("N" less than 10), and also drops off as 1/"N" as sample size increases. This estimator is commonly used and generally known simply as the "sample standard deviation".
Unbiased sample standard deviation.
For unbiased estimation of standard deviation, there is no formula that works across all distributions, unlike for mean and variance. Instead, "s" is used as a basis, and is scaled by a correction factor to produce an unbiased estimate. For the normal distribution, an unbiased estimator is given by "s"/"c"4, where the correction factor (which depends on "N") is given in terms of the Gamma function, and equals:
This arises because the sampling distribution of the sample standard deviation follows a (scaled) chi distribution, and the correction factor is the mean of the chi distribution.
An approximation can be given by replacing "N" − 1 with "N" − 1.5, yielding:
The error in this approximation decays quadratically (as 1/"N"2), and it is suited for all but the smallest samples or highest precision: for n = 3 the bias is equal to 1.3%, and for n = 9 the bias is already less than 0.1%.
For other distributions, the correct formula depends on the distribution, but a rule of thumb is to use the further refinement of the approximation:
where "γ"2 denotes the population excess kurtosis. The excess kurtosis may be either known beforehand for certain distributions, or estimated from the data.
Confidence interval of a sampled standard deviation.
The standard deviation we obtain by sampling a distribution is itself not absolutely accurate, both for mathematical reasons (explained here by the confidence interval) and for practical reasons of measurement (measurement error). The mathematical effect can be described by the confidence interval or CI. 
To show how a larger sample will increase the confidence interval, consider the following examples: 
For a small population of N=2, the 95% CI of the SD is from 0.45*SD to 31.9*SD. In other words, the standard deviation of the distribution in 95% of the cases can be larger by a factor of 31 or smaller by a factor of 2. For a larger population of N=10, the CI is 0.69*SD to 1.83*SD. So even with a sample population of 10, the actual SD can still be almost a factor 2 higher than the sampled SD. For a sample population N=100, this is down to 0.88*SD to 1.16*SD. To be more certain that the sampled SD is close to the actual SD we need to sample a large number of points.
Identities and mathematical properties.
The standard deviation is invariant under changes in location, and scales directly with the scale of the random variable. Thus, for a constant "c" and random variables "X" and "Y":
The standard deviation of the sum of two random variables can be related to their individual standard deviations and the covariance between them:
where formula_27 and formula_28 stand for variance and covariance, respectively.
The calculation of the sum of squared deviations can be related to moments calculated directly from the data. In the following formula, the letter E is interpreted to mean expected value, i.e., mean.
The sample standard deviation can be computed as:
For a finite population with equal probabilities at all points, we have
This means that the standard deviation is equal to the square root of the difference between the average of the squares of the values and the square of the average value.
See computational formula for the variance for proof, and for an analogous result for the sample standard deviation.
Interpretation and application.
A large standard deviation indicates that the data points can spread far from the mean and a small standard deviation indicates that they are clustered closely around the mean.
For example, each of the three populations {0, 0, 14, 14}, {0, 6, 8, 14} and {6, 6, 8, 8} has a mean of 7. Their standard deviations are 7, 5, and 1, respectively. The third population has a much smaller standard deviation than the other two because its values are all close to 7. It will have the same units as the data points themselves. If, for instance, the data set {0, 6, 8, 14} represents the ages of a population of four siblings in years, the standard deviation is 5 years. As another example, the population {1000, 1006, 1008, 1014} may represent the distances traveled by four athletes, measured in meters. It has a mean of 1007 meters, and a standard deviation of 5 meters.
Standard deviation may serve as a measure of uncertainty. In physical science, for example, the reported standard deviation of a group of repeated measurements gives the precision of those measurements. When deciding whether measurements agree with a theoretical prediction, the standard deviation of those measurements is of crucial importance: if the mean of the measurements is too far away from the prediction (with the distance measured in standard deviations), then the theory being tested probably needs to be revised. This makes sense since they fall outside the range of values that could reasonably be expected to occur, if the prediction were correct and the standard deviation appropriately quantified. See prediction interval.
While the standard deviation does measure how far typical values tend to be from the mean, other measures are available. An example is the mean absolute deviation, which might be considered a more direct measure of average distance, compared to the root mean square distance inherent in the standard deviation.
Application examples.
The practical value of understanding the standard deviation of a set of values is in appreciating how much variation there is from the average (mean).
Experiment, industrial and hypothesis testing.
Standard deviation is often used to compare real-world data against a model to test the model.
For example, in industrial applications the weight of products coming off a production line may need to legally be some value. By weighing some fraction of the products an average weight can be found, which will always be slightly different to the long term average. By using standard deviations a minimum and maximum value can be calculated that the averaged weight will be within some very high percentage of the time (99.9% or more). If it falls outside the range then the production process may need to be corrected. Statistical tests such as these are particularly important when the testing is relatively expensive. For example, if the product needs to be opened and drained and weighed, or if the product was otherwise used up by the test.
In experimental science a theoretical model of reality is used. Particle physics uses a standard of "5 sigma" for the declaration of a discovery. At five-sigma there is only one chance in nearly two million that a random fluctuation would yield the result. This level of certainty was required in order to assert that a particle consistent with the Higgs boson had been discovered in two independent experiments at CERN.
Weather.
As a simple example, consider the average daily maximum temperatures for two cities, one inland and one on the coast. It is helpful to understand that the range of daily maximum temperatures for cities near the coast is smaller than for cities inland. Thus, while these two cities may each have the same average maximum temperature, the standard deviation of the daily maximum temperature for the coastal city will be less than that of the inland city as, on any particular day, the actual maximum temperature is more likely to be farther from the average maximum temperature for the inland city than for the coastal one.
Finance.
In finance, standard deviation is often used as a measure of the risk associated with price-fluctuations of a given asset (stocks, bonds, property, etc.), or the risk of a portfolio of assets (actively managed mutual funds, index mutual funds, or ETFs). Risk is an important factor in determining how to efficiently manage a portfolio of investments because it determines the variation in returns on the asset and/or portfolio and gives investors a mathematical basis for investment decisions (known as mean-variance optimization). The fundamental concept of risk is that as it increases, the expected return on an investment should increase as well, an increase known as the risk premium. In other words, investors should expect a higher return on an investment when that investment carries a higher level of risk or uncertainty. When evaluating investments, investors should estimate both the expected return and the uncertainty of future returns. Standard deviation provides a quantified estimate of the uncertainty of future returns.
For example, let's assume an investor had to choose between two stocks. Stock A over the past 20 years had an average return of 10 percent, with a standard deviation of 20 percentage points (pp) and Stock B, over the same period, had average returns of 12 percent but a higher standard deviation of 30 pp. On the basis of risk and return, an investor may decide that Stock A is the safer choice, because Stock B's additional two percentage points of return is not worth the additional 10 pp standard deviation (greater risk or uncertainty of the expected return). Stock B is likely to fall short of the initial investment (but also to exceed the initial investment) more often than Stock A under the same circumstances, and is estimated to return only two percent more on average. In this example, Stock A is expected to earn about 10 percent, plus or minus 20 pp (a range of 30 percent to −10 percent), about two-thirds of the future year returns. When considering more extreme possible returns or outcomes in future, an investor should expect results of as much as 10 percent plus or minus 60 pp, or a range from 70 percent to −50 percent, which includes outcomes for three standard deviations from the average return (about 99.7 percent of probable returns).
Calculating the average (or arithmetic mean) of the return of a security over a given period will generate the expected return of the asset. For each period, subtracting the expected return from the actual return results in the difference from the mean. Squaring the difference in each period and taking the average gives the overall variance of the return of the asset. The larger the variance, the greater risk the security carries. Finding the square root of this variance will give the standard deviation of the investment tool in question.
Population standard deviation is used to set the width of Bollinger Bands, a widely adopted technical analysis tool. For example, the upper Bollinger Band is given as The most commonly used value for "n" is 2; there is about a five percent chance of going outside, assuming a normal distribution of returns.
Financial time series are known to be non-stationary series, whereas the statistical calculations above, such as standard deviation, apply only to stationary series. To apply the above statistical tools to non-stationary series, the series first must be transformed to a stationary series, enabling use of statistical tools that now have a valid basis from which to work.
Geometric interpretation.
To gain some geometric insights and clarification, we will start with a population of three values, "x"1, "x"2, "x"3. This defines a point "P" = ("x"1, "x"2, "x"3) in R3. Consider the line "L" = {("r", "r", "r") : "r" ∈ R}. This is the "main diagonal" going through the origin. If our three given values were all equal, then the standard deviation would be zero and "P" would lie on "L". So it is not unreasonable to assume that the standard deviation is related to the "distance" of "P" to "L". And that is indeed the case. To move orthogonally from "L" to the point "P", one begins at the point:
whose coordinates are the mean of the values we started out with. 
formula_33 is on formula_34 therefore formula_35 with formula_36 
The line formula_34 is to be orthogonal to the vector from formula_33 to formula_39. Therefore: 
formula_40
A little algebra shows that the distance between "P" and "M" (which is the same as the orthogonal distance between "P" and the line "L") formula_41 is equal to the standard deviation of the vector "x"1, "x"2, "x"3, multiplied by the square root of the number of dimensions of the vector (3 in this case.)
Chebyshev's inequality.
An observation is rarely more than a few standard deviations away from the mean. Chebyshev's inequality ensures that, for all distributions for which the standard deviation is defined, the amount of data within a number of standard deviations of the mean is at least as much as given in the following table.
Rules for normally distributed data.
The central limit theorem says that the distribution of an average of many independent, identically distributed random variables tends toward the famous bell-shaped normal distribution with a probability density function of:
where "μ" is the expected value of the random variables, "σ" equals their distribution's standard deviation divided by "n"1/2, and "n" is the number of random variables. The standard deviation therefore is simply a scaling variable that adjusts how broad the curve will be, though it also appears in the normalizing constant.
If a data distribution is approximately normal, then the proportion of data values within "z" standard deviations of the mean is defined by:
where formula_44 is the error function. The proportion that is less than or equal to a number, x, is given by the cumulative distribution function:
If a data distribution is approximately normal then about 68 percent of the data values are within one standard deviation of the mean (mathematically, μ ± σ, where μ is the arithmetic mean), about 95 percent are within two standard deviations (μ ± 2σ), and about 99.7 percent lie within three standard deviations (μ ± 3σ). This is known as the "68-95-99.7 rule", or "the empirical rule".
For various values of "z", the percentage of values expected to lie in and outside the symmetric interval, CI = (−"zσ", "zσ"), are as follows:
Relationship between standard deviation and mean.
The mean and the standard deviation of a set of data are descriptive statistics usually reported together. In a certain sense, the standard deviation is a "natural" measure of statistical dispersion if the center of the data is measured about the mean. This is because the standard deviation from the mean is smaller than from any other point. The precise statement is the following: suppose "x"1, ..., "x""n" are real numbers and define the function:
Using calculus or by completing the square, it is possible to show that σ("r") has a unique minimum at the mean:
Variability can also be measured by the coefficient of variation, which is the ratio of the standard deviation to the mean. It is a dimensionless number.
Standard deviation of the mean.
Often, we want some information about the precision of the mean we obtained. We can obtain this by determining the standard deviation of the sampled mean.
Assuming statistical independence of the values in the sample, the standard deviation of the mean is related to the standard deviation of the distribution by:
where "N" is the number of observations in the sample used to estimate the mean. This can easily be proven with (see basic properties of the variance):
hence
Resulting in:
It should be emphasized that in order to estimate standard deviation of the mean formula_52 it is necessary to know standard deviation of the entire population formula_53 beforehand. However, in most applications this parameter is unknown. For example, if series of 10 measurements of previously unknown quantity is performed in laboratory, it is possible to calculate resulting sample mean and sample standard deviation, but it is impossible to calculate standard deviation of the mean.
Rapid calculation methods.
The following two formulas can represent a running (repeatedly updated) standard deviation. A set of two power sums "s"1 and "s"2 are computed over a set of "N" values of "x", denoted as "x"1, ..., "x""N":
Given the results of these running summations, the values "N", "s"1, "s"2 can be used at any time to compute the "current" value of the running standard deviation:
Where N, as mentioned above, is the size of the set of values.
Similarly for sample standard deviation,
In a computer implementation, as the three "s""j" sums become large, we need to consider round-off error, arithmetic overflow, and arithmetic underflow. The method below calculates the running sums method with reduced rounding errors. This is a "one pass" algorithm for calculating variance of "n" samples without the need to store prior data during the calculation. Applying this method to a time series will result in successive values of standard deviation corresponding to "n" data points as "n" grows larger with each new sample, rather than a constant-width sliding window calculation.
For "k" = 1, ..., "n":
where A is the mean value.
Note: formula_59 since formula_60 or formula_61
Sample variance:
Population variance:
Weighted calculation.
When the values "xi" are weighted with unequal weights "wi", the power sums "s"0, "s"1, "s"2 are each computed as:
And the standard deviation equations remain unchanged. Note that "s"0 is now the sum of the weights and not the number of samples "N".
The incremental method with reduced rounding errors can also be applied, with some additional complexity.
A running sum of weights must be computed for each "k" from 1 to "n":
and places where 1/"n" is used above must be replaced by "wi"/"Wn":
In the final division,
and
where n is the total number of elements, and n' is the number of elements with non-zero weights.
The above formulas become equal to the simpler formulas given above if weights are taken as equal to one.
Combining standard deviations.
Population-based statistics.
The populations of sets, which may overlap, can be calculated simply as follows:
Standard deviations of non-overlapping () sub-populations can be aggregated as follows if the size (actual or relative to one another) and means of each are known:
For example, suppose it is known that the average American man has a mean height of 70 inches with a standard deviation of three inches and that the average American woman has a mean height of 65 inches with a standard deviation of two inches. Also assume that the number of men, "N", is equal to the number of women. Then the mean and standard deviation of heights of American adults could be calculated as:
For the more general case of "M" non-overlapping populations, "X"1 through "X""M", and the aggregate population formula_72:
where
If the size (actual or relative to one another), mean, and standard deviation of two overlapping populations are known for the populations as well as their intersection, then the standard deviation of the overall population can still be calculated as follows:
If two or more sets of data are being added together datapoint by datapoint, the standard deviation of the result can be calculated if the standard deviation of each data set and the covariance between each pair of data sets is known:
For the special case where no correlation exists between any pair of data sets, then the relation reduces to the root-mean-square:
Sample-based statistics.
Standard deviations of non-overlapping () sub-samples can be aggregated as follows if the actual size and means of each are known:
For the more general case of "M" non-overlapping data sets, "X"1 through "X""M", and the aggregate data set formula_72:
where:
If the size, mean, and standard deviation of two overlapping samples are known for the samples as well as their intersection, then the standard deviation of the aggregated sample can still be calculated. In general:
History.
The term "standard deviation" was first used in writing by Karl Pearson in 1894, following his use of it in lectures. This was as a replacement for earlier alternative names for the same idea: for example, Gauss used "mean error". It may be worth noting in passing that the mean error is mathematically distinct from the standard deviation.

</doc>
<doc id="552520" url="https://en.wikipedia.org/wiki?curid=552520" title="Standard error">
Standard error

The standard error (SE) is the standard deviation of the sampling distribution of a statistic, most commonly of the mean. The term may also be used to refer to an estimate of that standard deviation, derived from a particular sample used to compute the estimate.
For example, the sample mean is the usual estimator of a population mean. However, different samples drawn from that same population would in general have different values of the sample mean, so there is a distribution of sampled means (with its own mean and variance). The standard error of the mean (SEM) (i.e., of using the sample mean as a method of estimating the population mean) is the standard deviation of those sample means over all possible samples (of a given size) drawn from the population. Secondly, the standard error of the mean can refer to an estimate of that standard deviation, computed from the sample of data being analyzed at the time.
In regression analysis, the term "standard error" is also used in the phrase standard error of the regression to mean the ordinary least squares estimate of the standard deviation of the underlying errors.
Standard error of the mean.
The standard error of the mean (SEM) is the standard deviation of the sample-mean's estimate of a population mean. (It can also be viewed as the standard deviation of the error in the sample mean with respect to the true mean, since the sample mean is an unbiased estimator.) SEM is usually estimated by the sample estimate of the population standard deviation (sample standard deviation) divided by the square root of the sample size (assuming statistical independence of the values in the sample):
where
This estimate may be compared with the formula for the true standard deviation of the sample mean:
where
This formula may be derived from what we know about the variance of a sum of independent random variables.
"Note:" the standard error and the standard deviation of small samples tend to systematically underestimate the population standard error and deviations: the standard error of the mean is a biased estimator of the population standard error. With n = 2 the underestimate is about 25%, but for n = 6 the underestimate is only 5%. Gurland and Tripathi (1971) provide a correction and equation for this effect. Sokal and Rohlf (1981) give an equation of the correction factor for small samples of "n" < 20. See unbiased estimation of standard deviation for further discussion.
"A practical result:" Decreasing the uncertainty in a mean value estimate by a factor of two requires acquiring four times as many observations in the sample. Or decreasing standard error by a factor of ten requires a hundred times as many observations.
Student approximation when "σ" value is unknown.
In many practical applications, the true value of "σ" is unknown. As a result, we need to use a distribution that takes into account that spread of possible "σ"'s.
When the true underlying distribution is known to be Gaussian, although with unknown σ then the resulting estimated distribution follows the Student t-distribution. The standard error is the standard deviation of the student t-distribution. t-distributions are slightly different from the gaussian, and vary depending on the size of the sample. To estimate the Standard error of a student t-distribution it is sufficient to use the sample standard deviation "s" instead of "σ", and we could use this value to calculate confidence intervals.
"Note:" The Student's probability distribution is a good approximation of the Gaussian when the sample size is over 100.
Assumptions and usage.
If the data are assumed to be normally distributed, quantiles of the normal distribution and the sample mean and standard error can be used to calculate approximate confidence intervals for the mean. The following expressions can be used to calculate the upper and lower 95% confidence limits, where formula_15 is equal to the sample mean, formula_16 is equal to the standard error for the sample mean, and 1.96 is the 0.975 quantile of the normal distribution:
In particular, the standard error of a sample statistic (such as sample mean) is the estimated standard deviation of the error in the process by which it was generated. In other words, it is the standard deviation of the sampling distribution of the sample statistic. The notation for standard error can be any one of SE, SEM (for standard error of "measurement" or "mean"), or SE.
Standard errors provide simple measures of uncertainty in a value and are often used because:
Standard error of mean versus standard deviation.
In scientific and technical literature, experimental data is often summarized either using the mean and standard deviation or the mean with the standard error. This often leads to confusion about their interchangeability. However, the mean and standard deviation are descriptive statistics, whereas the standard error of the mean describes bounds on a random sampling process. Despite the small difference in equations for the standard deviation and the standard error, this small difference changes the meaning of what is being reported from a description of the variation in measurements to a probabilistic statement about how the number of samples will provide a better bound on estimates of the population mean, in light of the central limit theorem.
Put simply, the standard error of the sample is an estimate of how far the sample mean is likely to be from the population mean, whereas the standard deviation of the sample is the degree to which individuals within the sample differ from the sample mean. If the population standard deviation is finite, the standard error of the sample will tend to zero with increasing sample size, because the estimate of the population mean will improve, while the standard deviation of the sample will tend to the population standard deviation as the sample size increases.
Correction for finite population.
The formula given above for the standard error assumes that the sample size is much smaller than the population size, so that the population can be considered to be effectively infinite in size. This is usually the case even with finite populations, because most of the time, people are primarily interested in managing the processes that created the existing finite population; this is called an analytic study, following W. Edwards Deming. If people are interested in managing an existing finite population that will not change over time, then it is necessary to adjust for the population size; this is called an enumerative study.
When the sampling fraction is large (approximately at 5% or more) in an enumerative study, the estimate of the error must be corrected by multiplying by a "finite population correction"
to account for the added precision gained by sampling close to a larger percentage of the population. The effect of the FPC is that the error becomes zero when the sample size "n" is equal to the population size "N".
Correction for correlation in the sample.
If values of the measured quantity A are not statistically independent but have been obtained from known locations in parameter space x, an unbiased estimate of the true standard error of the mean (actually a correction on the standard deviation part) may be obtained by multiplying the calculated standard error of the sample by the factor "f":
where the sample bias coefficient ρ is the widely used Prais-Winsten estimate of the autocorrelation-coefficient (a quantity between −1 and +1) for all sample point pairs. This approximate formula is for moderate to large sample sizes; the reference gives the exact formulas for any sample size, and can be applied to heavily autocorrelated time series like Wall Street stock quotes. Moreover this formula works for positive and negative ρ alike. See also unbiased estimation of standard deviation for more discussion.
Relative standard error.
The relative standard error of a sample mean is the standard error divided by the mean and expressed as a percentage. The relative standard error only makes sense if the variable for which it is calculated cannot have a mean of zero.
As an example of the use of the relative standard error, consider two surveys of household income that both result in a sample mean of $50,000. If one survey has a standard error of $10,000 and the other has a standard error of $5,000, then the relative standard errors are 20% and 10% respectively. The survey with the lower relative standard error can be said to have a more precise measurement, since it has proportionately less sampling variation around the mean. In fact, data organizations often set reliability standards that their data must reach before publication. For example, the U.S. National Center for Health Statistics typically does not report an estimated mean if its relative standard error exceeds 30%. (NCHS also typically requires at least 30 observations – if not more – for an estimate to be reported.)

</doc>
<doc id="47641" url="https://en.wikipedia.org/wiki?curid=47641" title="Standard Model">
Standard Model

The Standard Model of particle physics is a theory concerning the electromagnetic, weak, and strong nuclear interactions, as well as classifying all the subatomic particles known. It was developed throughout the latter half of the 20th century, as a collaborative effort of scientists around the world. The current formulation was finalized in the mid-1970s upon experimental confirmation of the existence of quarks. Since then, discoveries of the top quark (1995), the tau neutrino (2000), and more recently the Higgs boson (2012), have given further credence to the Standard Model. Because of its success in explaining a wide variety of experimental results, the Standard Model is sometimes regarded as a "theory of almost everything".
Although the Standard Model is believed to be theoretically self-consistent and has demonstrated huge and continued successes in providing experimental predictions, it does leave some phenomena unexplained and it falls short of being a complete theory of fundamental interactions. It does not incorporate the full theory of gravitation as described by general relativity, or account for the accelerating expansion of the universe (as possibly described by dark energy). The model does not contain any viable dark matter particle that possesses all of the required properties deduced from observational cosmology. It also does not incorporate neutrino oscillations (and their non-zero masses).
The development of the Standard Model was driven by theoretical and experimental particle physicists alike. For theorists, the Standard Model is a paradigm of a quantum field theory, which exhibits a wide range of physics including spontaneous symmetry breaking, anomalies, non-perturbative behavior, etc. It is used as a basis for building more exotic models that incorporate hypothetical particles, extra dimensions, and elaborate symmetries (such as supersymmetry) in an attempt to explain experimental results at variance with the Standard Model, such as the existence of dark matter and neutrino oscillations.
Historical background.
The first step towards the Standard Model was Sheldon Glashow's discovery in 1961 of a way to combine the electromagnetic and weak interactions. In 1967 Steven Weinberg and Abdus Salam incorporated the Higgs mechanism into Glashow's electroweak theory, giving it its modern form.
The Higgs mechanism is believed to give rise to the masses of all the elementary particles in the Standard Model. This includes the masses of the W and Z bosons, and the masses of the fermions, i.e. the quarks and leptons.
After the neutral weak currents caused by Z boson exchange were discovered at CERN in 1973, the electroweak theory became widely accepted and Glashow, Salam, and Weinberg shared the 1979 Nobel Prize in Physics for discovering it. The W and Z bosons were discovered experimentally in 1981, and their masses were found to be as the Standard Model predicted.
The theory of the strong interaction, to which many contributed, acquired its modern form around 1973–74, when experiments confirmed that the hadrons were composed of fractionally charged quarks.
Overview.
At present, matter and energy are best understood in terms of the kinematics and interactions of elementary particles. To date, physics has reduced the laws governing the behavior and interaction of all known forms of matter and energy to a small set of fundamental laws and theories. A major goal of physics is to find the "common ground" that would unite all of these theories into one integrated theory of everything, of which all the other known laws would be special cases, and from which the behavior of all matter and energy could be derived (at least in principle).
Particle content.
The Standard Model includes members of several classes of elementary particles (fermions, gauge bosons, and the Higgs boson), which in turn can be distinguished by other characteristics, such as color charge.
Fermions.
The Standard Model includes 12 elementary particles of spin-½ known as fermions. According to the spin-statistics theorem, fermions respect the Pauli exclusion principle. Each fermion has a corresponding antiparticle.
The fermions of the Standard Model are classified according to how they interact (or equivalently, by what charges they carry). There are six quarks (up, down, charm, strange, top, bottom), and six leptons (electron, electron neutrino, muon, muon neutrino, tau, tau neutrino). Pairs from each classification are grouped together to form a generation, with corresponding particles exhibiting similar physical behavior (see table).
The defining property of the quarks is that they carry color charge, and hence, interact via the strong interaction. A phenomenon called color confinement results in quarks being very strongly bound to one another, forming color-neutral composite particles (hadrons) containing either a quark and an antiquark (mesons) or three quarks (baryons). The familiar proton and the neutron are the two baryons having the smallest mass. Quarks also carry electric charge and weak isospin. Hence they interact with other fermions both electromagnetically and via the weak interaction.
The remaining six fermions do not carry colour charge and are called leptons. The three neutrinos do not carry electric charge either, so their motion is directly influenced only by the weak nuclear force, which makes them notoriously difficult to detect. However, by virtue of carrying an electric charge, the electron, muon, and tau all interact electromagnetically.
Each member of a generation has greater mass than the corresponding particles of lower generations. The first generation charged particles do not decay; hence all ordinary (baryonic) matter is made of such particles. Specifically, all atoms consist of electrons orbiting around atomic nuclei, ultimately constituted of up and down quarks. Second and third generation charged particles, on the other hand, decay with very short half lives, and are observed only in very high-energy environments. Neutrinos of all generations also do not decay, and pervade the universe, but rarely interact with baryonic matter.
Gauge bosons.
In the Standard Model, gauge bosons are defined as force carriers that mediate the strong, weak, and electromagnetic fundamental interactions.
Interactions in physics are the ways that particles influence other particles. At a macroscopic level, electromagnetism allows particles to interact with one another via electric and magnetic fields, and gravitation allows particles with mass to attract one another in accordance with Einstein's theory of general relativity. The Standard Model explains such forces as resulting from matter particles exchanging other particles, generally referred to as "force mediating particles". When a force-mediating particle is exchanged, at a macroscopic level the effect is equivalent to a force influencing both of them, and the particle is therefore said to have "mediated" (i.e., been the agent of) that force. The Feynman diagram calculations, which are a graphical representation of the perturbation theory approximation, invoke "force mediating particles", and when applied to analyze high-energy scattering experiments are in reasonable agreement with the data. However, perturbation theory (and with it the concept of a "force-mediating particle") fails in other situations. These include low-energy quantum chromodynamics, bound states, and solitons.
The gauge bosons of the Standard Model all have spin (as do matter particles). The value of the spin is 1, making them bosons. As a result, they do not follow the Pauli exclusion principle that constrains fermions: thus bosons (e.g. photons) do not have a theoretical limit on their spatial density (number per volume). The different types of gauge bosons are described below.
The interactions between all the particles described by the Standard Model are summarized by the diagrams on the right of this section.
Higgs boson.
The Higgs particle is a massive scalar elementary particle theorized by Robert Brout, François Englert, Peter Higgs, Gerald Guralnik, C. R. Hagen, and Tom Kibble in 1964 (see 1964 PRL symmetry breaking papers) and is a key building block in the Standard Model. It has no intrinsic spin, and for that reason is classified as a boson (like the gauge bosons, which have integer spin).
The Higgs boson plays a unique role in the Standard Model, by explaining why the other elementary particles, except the photon and gluon, are massive. In particular, the Higgs boson explains why the photon has no mass, while the W and Z bosons are very heavy. Elementary particle masses, and the differences between electromagnetism (mediated by the photon) and the weak force (mediated by the W and Z bosons), are critical to many aspects of the structure of microscopic (and hence macroscopic) matter. In electroweak theory, the Higgs boson generates the masses of the leptons (electron, muon, and tau) and quarks. As the Higgs boson is massive, it must interact with itself.
Because the Higgs boson is a very massive particle and also decays almost immediately when created, only a very high-energy particle accelerator can observe and record it. Experiments to confirm and determine the nature of the Higgs boson using the Large Hadron Collider (LHC) at CERN began in early 2010, and were performed at Fermilab's Tevatron until its closure in late 2011. Mathematical consistency of the Standard Model requires that any mechanism capable of generating the masses of elementary particles become visible at energies above ; therefore, the LHC (designed to collide two 7 to 8 TeV proton beams) was built to answer the question of whether the Higgs boson actually exists.
On 4 July 2012, the two main experiments at the LHC (ATLAS and CMS) both reported independently that they found a new particle with a mass of about (about 133 proton masses, on the order of 10−25 kg), which is "consistent with the Higgs boson." Although it has several properties similar to the predicted "simplest" Higgs, they acknowledged that further work would be needed to conclude that it is indeed the Higgs boson, and exactly which version of the Standard Model Higgs is best supported if confirmed.
On 14 March 2013 the Higgs Boson was tentatively confirmed to exist.
Total particle count.
Counting particles by a rule that distinguishes between particles and their corresponding antiparticles, and among the many color states of quarks and gluons, gives a total of 61 elementary particles.
Theoretical aspects.
Construction of the Standard Model Lagrangian.
Technically, quantum field theory provides the mathematical framework for the Standard Model, in which a Lagrangian controls the dynamics and kinematics of the theory. Each kind of particle is described in terms of a dynamical field that pervades space-time. The construction of the Standard Model proceeds following the modern method of constructing most field theories: by first postulating a set of symmetries of the system, and then by writing down the most general renormalizable Lagrangian from its particle (field) content that observes these symmetries.
The global Poincaré symmetry is postulated for all relativistic quantum field theories. It consists of the familiar translational symmetry, rotational symmetry and the inertial reference frame invariance central to the theory of special relativity. The local SU(3)×SU(2)×U(1) gauge symmetry is an internal symmetry that essentially defines the Standard Model. Roughly, the three factors of the gauge symmetry give rise to the three fundamental interactions. The fields fall into different representations of the various symmetry groups of the Standard Model (see table). Upon writing the most general Lagrangian, one finds that the dynamics depend on 19 parameters, whose numerical values are established by experiment. The parameters are summarized in the table above (note: with the Higgs mass is at 125 GeV, the Higgs self-coupling strength "λ" ~ 1/8).
Quantum chromodynamics sector.
The quantum chromodynamics (QCD) sector defines the interactions between quarks and gluons, with SU(3) symmetry, generated by Ta. Since leptons do not interact with gluons, they are not affected by this sector. The Dirac Lagrangian of the quarks coupled to the gluon fields is given by
formula_2 is the SU(3) gauge field containing the gluons, formula_3 are the Dirac matrices, D and U are the Dirac spinors associated with up- and down-type quarks, and gs is the strong coupling constant.
Electroweak sector.
The electroweak sector is a Yang–Mills gauge theory with the simple symmetry group U(1)×SU(2)L,
where "B""μ" is the U(1) gauge field; "Y"W is the weak hypercharge—the generator of the U(1) group; formula_5 is the
three-component SU(2) gauge field; formula_6 are the Pauli matrices—infinitesimal generators of the SU(2) group. The subscript L indicates that they only act on left fermions; "g"′ and "g" are coupling constants.
Higgs sector.
In the Standard Model, the Higgs field is a complex scalar of the group SU(2)L:
where the indices + and 0 indicate the electric charge ("Q") of the components. The weak isospin ("Y"W) of both components is 1.
Before symmetry breaking, the Higgs Lagrangian is:
which can also be written as:
Fundamental forces.
The Standard Model classified all four fundamental forces in nature. In the Standard Model, a force is described as an exchange of bosons between the objects affected, such as a photon for the electromagnetic force and a gluon for the strong interaction. Those particles are called force carriers.
Tests and predictions.
The Standard Model (SM) predicted the existence of the W and Z bosons, gluon, and the top and charm quarks before these particles were observed. Their predicted properties were experimentally confirmed with good precision. To give an idea of the success of the SM, the following table compares the measured masses of the W and Z bosons with the masses predicted by the SM:
The SM also makes several predictions about the decay of Z bosons, which have been experimentally confirmed by the Large Electron-Positron Collider at CERN.
In May 2012 BaBar Collaboration reported that their recently analyzed data may suggest possible flaws in the Standard Model of particle physics. These data show that a particular type of particle decay called "B to D-star-tau-nu" happens more often than the Standard Model says it should. In this type of decay, a particle called the B-bar meson decays into a D meson, an antineutrino and a tau-lepton.
While the level of certainty of the excess (3.4 sigma) is not enough to claim a break from the Standard Model, the results are a potential sign of something amiss and are likely to impact existing theories, including those attempting to deduce the properties of Higgs bosons.
On December 13, 2012, physicists reported the constancy, over space and time, of a basic physical constant of nature that supports the "standard model of physics". The scientists, studying methanol molecules in a distant galaxy, found the change (∆μ/μ) in the proton-to-electron mass ratio μ to be equal to "(0.0 ± 1.0) × 10−7 at redshift z = 0.89" and consistent with "a null result".
Challenges.
Self-consistency of the Standard Model (currently formulated as a non-abelian gauge theory quantized through path-integrals) has not been mathematically proven. While regularized versions useful for approximate computations (for example lattice gauge theory) exist, it is not known whether they converge (in the sense of S-matrix elements) in the limit that the regulator is removed. A key question related to the consistency is the Yang–Mills existence and mass gap problem.
Experiments indicate that neutrinos have mass, which the classic Standard Model did not allow. To accommodate this finding, the classic Standard Model can be modified to include neutrino mass.
If one insists on using only Standard Model particles, this can be achieved by adding a non-renormalizable interaction of leptons with the Higgs boson. On a fundamental level, such an interaction emerges in the seesaw mechanism where heavy right-handed neutrinos are added to the theory.
This is natural in the left-right symmetric extension of the Standard Model and in certain grand unified theories. As long as new physics appears below or around 1014 GeV, the neutrino masses can be of the right order of magnitude.
Theoretical and experimental research has attempted to extend the Standard Model into a Unified field theory or a Theory of everything, a complete theory explaining all physical phenomena including constants. Inadequacies of the Standard Model that motivate such research include:
Currently, no proposed Theory of Everything has been widely accepted or verified.

</doc>
<doc id="2024795" url="https://en.wikipedia.org/wiki?curid=2024795" title="Standard Model (mathematical formulation)">
Standard Model (mathematical formulation)

This article describes the mathematics of the Standard Model of particle physics, a gauge quantum field theory containing the internal symmetries of the unitary product group . The theory is commonly viewed as containing the fundamental set of particles – the leptons, quarks, gauge bosons and the Higgs particle.
The Standard Model is renormalizable and mathematically self-consistent, however despite having huge and continued successes in providing experimental predictions it does leave some unexplained phenomena. In particular, although the physics of special relativity is incorporated, general relativity is not, and the Standard Model will fail at energies or distances where the graviton is expected to emerge. Therefore in a modern field theory context, it is seen as an effective field theory.
This article requires some background in physics and mathematics, but is designed as both an introduction and a reference.
Quantum field theory.
The standard model is a quantum field theory, meaning its fundamental objects are "quantum fields" which are defined at all points in spacetime. These fields are
That these are "quantum" rather than "classical" fields has the mathematical consequence that they are operator-valued. In particular, values of the fields generally do not commute. As operators, they act upon the quantum state (ket vector).
The dynamics of the quantum state and the fundamental fields are determined by the Lagrangian density formula_2 (usually for short just called the Lagrangian). This plays a role similar to that of the Schrödinger equation in non-relativistic quantum mechanics, but a Lagrangian is not an equation – rather, it is a polynomial function of the fields and their derivatives, and used with the principle of least action. While it would be possible to derive a system of differential equations governing the fields from the Langrangian, it is more common to use other techniques to compute with quantum field theories.
The standard model is furthermore a gauge theory, which means there are degrees of freedom in the mathematical formalism which do not correspond to changes in the physical state. The gauge group of the standard model is , where U(1) acts on and , acts on and , and SU(3) acts on . The fermion field also transforms under these symmetries, although all of them leave some parts of it unchanged.
The role of the quantum fields.
In classical mechanics, the state of a system can usually be captured by a small set of variables, and the dynamics of the system is thus determined by the time evolution of these variables. In classical field theory, the "field" is part of the state of the system, so in order to describe it completely one effectively introduces separate variables for every point in spacetime (even though there are many restrictions on how the values of the field "variables" may vary from point to point, for example in the form of field equations involving partial derivatives of the fields).
In quantum mechanics, the classical variables are turned into operators, but these do not capture the state of the system, which is instead encoded into a wavefunction or more abstract ket vector. If is an eigenstate with respect to an operator , then for the corresponding eigenvalue , and hence letting an operator act on is analogous to multiplying by the value of the classical variable to which corresponds. By extension, a classical formula where all variables have been replaced by the corresponding operators will behave like an operator which, when it acts upon the state of the system, multiplies it by the analogue of the quantity that the classical formula would compute. The formula as such does however not contain any information about the state of the system; it would evaluate to the same operator regardless of what state the system is in.
Quantum fields relate to quantum mechanics as classical fields do to classical mechanics, i.e., there is a separate operator for every point in spacetime, and these operators do not carry any information about the state of the system; they are merely used to exhibit some aspect of the state, at the point to which they belong. In particular, the quantum fields are "not" wavefunctions, even though the equations which govern their time evolution may be deceptively similar to those of the corresponding wavefunction in a semiclassical formulation. There is no variation in strength of the fields between different points in spacetime; the variation that happens is rather one of phase factors.
Vectors, scalars, and spinors.
Mathematically it may look as though all of the fields are vector-valued (in addition to being operator-valued), since they all have several components, can be multiplied by matrices, etc., but physicists assign a more specific physical meaning to the word: a vector is something which transforms like a four-vector under Lorentz transformations, and a scalar is something which is invariant under Lorentz transformations. The , and fields are all vectors in this sense, so the corresponding particles are said to be vector bosons. The Higgs field is a scalar.
The fermion field does transform under Lorentz transformations, but not like a vector should; rotations will only turn it by half the angle a proper vector should. Therefore these constitute a third kind of quantity, which is known as a spinor.
It is common to make use of abstract index notation for the vector fields, in which case the vector fields all come with a Lorentzian index , like so: formula_3, and formula_4. If abstract index notation is used also for spinors then these will carry a spinorial index and the Dirac gamma will carry one Lorentzian and two spinorian indices, but it is more common to regard spinors as column matrices and the Dirac gamma as a matrix which additionally carries a Lorentzian index. The Feynman slash notation can be used to turn a vector field into a linear operator on spinors, like so: formula_5; this may involve raising and lowering indices.
Alternative presentations of the fields.
As is common in quantum theory, there is more than one way to look at things. At first the basic fields given above may not seem to correspond well with the "fundamental particles" in the chart above, but there are several alternative presentations which, in particular contexts, may be more appropriate than those that are given above.
Fermions.
Rather than having one fermion field , it can be split up into separate components for each type of particle. This mirrors the historical evolution of quantum field theory, since the electron component (describing the electron and its antiparticle the positron) is then the original field of quantum electrodynamics, which was later accompanied by and fields for the muon and tauon respectively (and their antiparticles). Electroweak theory added formula_6, and formula_7 for the corresponding neutrinos, and the quarks add still further components. In order to be four-spinors like the electron and other lepton components, there must be one quark component for every combination of flavour and colour, bringing the total to 24 (3 for charged leptons, 3 for neutrinos, and 2·3·3 = 18 for quarks).
An important definition is the barred fermion field formula_8 is defined to be formula_9, where formula_10 denotes the Hermitian adjoint and is the zeroth gamma matrix. If is thought of as an matrix then formula_8 should be thought of as a matrix.
A chiral theory.
An independent decomposition of is that into chirality components:
where formula_14 is the fifth gamma matrix. This is very important in the Standard Model because "left and right chirality components are treated differently by the gauge interactions".
In particular, under weak isospin SU(2) transformations the left-handed particles are weak-isospin doublets, whereas the right-handed are singlets – i.e. the weak isospin of is zero. Put more simply, the weak interaction could rotate e.g. a left-handed electron into a left-handed neutrino (with emission of a ), but could not do so with the same right-handed particles. As an aside, the right-handed neutrino originally did not exist in the standard model – but the discovery of neutrino oscillation implies that neutrinos must have mass, and since chirality can change during the propagation of a massive particle, right-handed neutrinos must exist in reality. This does not however change the (experimentally-proven) chiral nature of the weak interaction.
Furthermore, acts differently on formula_15 than on formula_16 (because they have different weak hypercharges).
Mass and interaction eigenstates.
A distinction can thus be made between, for example, the mass and interaction eigenstates of the neutrino. The former is the state which propagates in free space, whereas the latter is the "different" state that participates in interactions. Which is the "fundamental" particle? For the neutrino, it is conventional to define the "flavour" (, , or ) by the interaction eigenstate, whereas for the quarks we define the flavour (up, down, etc.) by the mass state. We can switch between these states using the CKM matrix for the quarks, or the PMNS matrix for the neutrinos (the charged leptons on the other hand are eigenstates of both mass and flavour).
As an aside, if a complex phase term exists within either of these matrices, it will give rise to direct CP violation, which could explain the dominance of matter over antimatter in our current universe. This has been proven for the CKM matrix, and is expected for the PMNS matrix.
Positive and negative energies.
Finally, the quantum fields are sometimes decomposed into "positive" and "negative" energy parts: . This is not so common when a quantum field theory has been set up, but often features prominently in the process of quantizing a field theory.
Bosons.
Due to the Higgs mechanism, the electroweak boson fields formula_1, and "mix" to create the states which are physically observable. To retain gauge invariance, the underlying fields must be massless, but the observable states can "gain masses" in the process. These states are:
The massive neutral boson:
The massless neutral boson:
The massive charged W bosons:
where is the Weinberg angle.
The field is the photon, which corresponds classically to the well-known electromagnetic four-potential – i.e. the electric and magnetic fields. The field actually contributes in every process the photon does, but due to its large mass, the contribution is usually negligible.
Perturbative QFT and the interaction picture.
Much of the qualitative descriptions of the standard model in terms of "particles" and "forces" comes from the perturbative quantum field theory view of the model. In this, the Langrangian is decomposed as formula_21 into separate "free field" and "interaction" Langrangians. The free fields care for particles in isolation, whereas processes involving several particles arise through interactions. The idea is that the state vector should only change when particles interact, meaning a free particle is one whose quantum state is constant. This corresponds to the interaction picture in quantum mechanics.
In the more common Schrödinger picture, even the states of free particles change over time: typically the phase changes at a rate which depends on their energy. In the alternative Heisenberg picture, state vectors are kept constant, at the price of having the operators (in particular the observables) be time-dependent. The interaction picture constitutes an intermediate between the two, where some time dependence is placed in the operators (the quantum fields) and some in the state vector. In QFT, the former is called the free field part of the model, and the latter is called the interaction part. The free field model can be solved exactly, and then the solutions to the full model can be expressed as perturbations of the free field solutions, for example using the Dyson series.
It should be observed that the decomposition into free fields and interactions is in principle arbitrary. For example renormalization in QED modifies the mass of the free field electron to match that of a physical electron (with an electromagnetic field), and will in doing so add a term to the free field Lagrangian which must be cancelled by a counterterm in the interaction Lagrangian, that then shows up as a two-line vertex in the Feynman diagrams. This is also how the Higgs field is thought to give particles mass: the part of the interaction term which corresponds to the (nonzero) vacuum expectation value of the Higgs field is moved from the interaction to the free field Lagrangian, where it looks just like a mass term having nothing to do with Higgs.
Free fields.
Under the usual free/interaction decomposition, which is suitable for low energies, the free fields obey the following equations:
These equations can be solved exactly. One usually does so by considering first solutions that are periodic with some period along each spatial axis; later taking the limit: will lift this periodicity restriction.
In the periodic case, the solution for a field (any of the above) can be expressed as a Fourier series of the form
where:
In the limit , the sum would turn into an integral with help from the hidden inside . The numeric value of also depends on the normalization chosen for formula_36 and formula_37.
Technically, formula_38 is the Hermitian adjoint of the operator in the inner product space of ket vectors. The identification of formula_38 and as creation and annihilation operators comes from comparing conserved quantities for a state before and after one of these have acted upon it. formula_38 can for example be seen to add one particle, because it will add to the eigenvalue of the a-particle number operator, and the momentum of that particle ought to be since the eigenvalue of the vector-valued momentum operator increases by that much. For these derivations, one starts out with expressions for the operators in terms of the quantum fields. That the operators with formula_10 are creation operators and the one without annihilation operators is a convention, imposed by the sign of the commutation relations postulated for them.
An important step in preparation for calculating in perturbative quantum field theory is to separate the "operator" factors and above from their corresponding vector or spinor factors and . The vertices of Feynman graphs come from the way that and from different factors in the interaction Lagrangian fit together, whereas the edges come from the way that the s and s must be moved around in order to put terms in the Dyson series on normal form.
Interaction terms and the path integral approach.
The Lagrangian can also be derived without using creation and annihilation operators (the "canonical" formalism), by using a "path integral" approach, pioneered by Feynman building on the earlier work of Dirac. See e.g. Path integral formulation on Wikipedia or A. Zee's QFT in a nutshell. This is one possible way that the Feynman diagrams, which are pictorial representations of interaction terms, can be derived relatively easily. A quick derivation is indeed presented at the article on Feynman diagrams.
Lagrangian formalism.
We can now give some more detail about the aforementioned free and interaction terms appearing in the Standard Model Lagrangian density. Any such term must be both gauge and reference-frame invariant, otherwise the laws of physics would depend on an arbitrary choice or the frame of an observer. Therefore the global Poincaré symmetry, consisting of translational symmetry, rotational symmetry and the inertial reference frame invariance central to the theory of special relativity must apply. The local gauge symmetry is the internal symmetry. The three factors of the gauge symmetry together give rise to the three fundamental interactions, after some appropriate relations have been defined, as we shall see.
A complete formulation of the Standard Model Lagrangian with all the terms written together can be found e.g. here.
Kinetic terms.
A free particle can be represented by a mass term, and a "kinetic" term which relates to the "motion" of the fields.
Fermion fields.
The kinetic term for a Dirac fermion is
where the notations are carried from earlier in the article. can represent any, or all, Dirac fermions in the standard model. Generally, as below, this term is included within the couplings (creating an overall "dynamical" term).
Gauge fields.
For the spin-1 fields, first define the field strength tensor
for a given gauge field (here we use ), with gauge coupling constant . The quantity is the structure constant of the particular gauge group, defined by the commutator
where are the generators of the group. In an Abelian (commutative) group (such as the we use here), since the generators all commute with each other, the structure constants vanish. Of course, this is not the case in general – the standard model includes the non-Abelian and groups (such groups lead to what is called a Yang–Mills gauge theory).
We need to introduce three gauge fields corresponding to each of the subgroups .
The kinetic term can now be written simply as
where the traces are over the and indices hidden in and respectively. The two-index objects are the field strengths derived from and the vector fields. There are also two extra hidden parameters: the theta angles for and .
Coupling terms.
The next step is to "couple" the gauge fields to the fermions, allowing for interactions.
Electroweak sector.
The electroweak sector interacts with the symmetry group , where the subscript L indicates coupling only to left-handed fermions.
Where is the gauge field; is the weak hypercharge (the generator of the group); is the three-component gauge field; and the components of are the Pauli matrices (infinitesimal generators of the group) whose eigenvalues give the weak isospin. Note that we have to redefine a new symmetry of "weak hypercharge", different from QED, in order to achieve the unification with the weak force. The electric charge , third component of weak isospin (also called or ) and weak hypercharge are related by
or by the alternate convention . The first convention (used in this article) is equivalent to the earlier Gell-Mann–Nishijima formula. We can then define the conserved current for weak isospin as
and for weak hypercharge as
where formula_53 is the electric current and formula_54 the third weak isospin current. As explained above, "these currents mix" to create the physically observed bosons, which also leads to testable relations between the coupling constants.
To explain in a simpler way, we can see the effect of the electroweak interaction by picking out terms from the Lagrangian. We see that the SU(2) symmetry acts on each (left-handed) fermion doublet contained in , for example
where the particles are understood to be left-handed, and where
This is an interaction corresponding to a "rotation in weak isospin space" or in other words, a "transformation between and via emission of a boson". The symmetry, on the other hand, is similar to electromagnetism, but acts on all "weak hypercharged" fermions (both left and right handed) via the neutral , as well as the "charged" fermions via the photon.
Quantum chromodynamics sector.
The quantum chromodynamics (QCD) sector defines the interactions between quarks and gluons, with symmetry, generated by . Since leptons do not interact with gluons, they are not affected by this sector. The Dirac Lagrangian of the quarks coupled to the gluon fields is given by
where and are the Dirac spinors associated with up- and down-type quarks, and other notations are continued from the previous section.
Mass terms and the Higgs mechanism.
Mass terms.
The mass term arising from the Dirac Lagrangian (for any fermion ) is formula_58 which is "not" invariant under the electroweak symmetry. This can be seen by writing in terms of left and right handed components (skipping the actual calculation):
i.e. contribution from formula_60 and formula_61 terms do not appear. We see that the mass-generating interaction is achieved by constant flipping of particle chirality. The spin-half particles have no right/left chirality pair with the same representations and equal and opposite weak hypercharges, so assuming these gauge charges are conserved in the vacuum, none of the spin-half particles could ever swap chirality, and must remain massless. Additionally, we know experimentally that the W and Z bosons are massive, but a boson mass term contains the combination e.g. , which clearly depends on the choice of gauge. Therefore, none of the standard model fermions "or" bosons can "begin" with mass, but must acquire it by some other mechanism.
The Higgs mechanism.
The solution to both these problems comes from the Higgs mechanism, which involves scalar fields (the number of which depend on the exact form of Higgs mechanism) which (to give the briefest possible description) are "absorbed" by the massive bosons as degrees of freedom, and which couple to the fermions via Yukawa coupling to create what looks like mass terms.
In the Standard Model, the Higgs field is a complex scalar of the group :
where the superscripts and indicate the electric charge () of the components. The weak hypercharge () of both components is .
The Higgs part of the Lagrangian is
where and , so that the mechanism of spontaneous symmetry breaking can be used. There is a parameter here, at first hidden within the shape of the potential, that is very important. In a unitarity gauge one can set and make real. Then formula_64 is the non-vanishing vacuum expectation value of the Higgs field. has units of mass, and it is the only parameter in the Standard Model which is not dimensionless. It is also much smaller than the Planck scale; it is approximately equal to the Higgs mass, and sets the scale for the mass of everything else. This is the only real fine-tuning to a small nonzero value in the Standard Model, and it is called the Hierarchy problem. Quadratic terms in and arise, which give masses to the W and Z bosons:
The Yukawa interaction terms are
where are matrices of Yukawa couplings, with the term giving the coupling of the generations and .
Neutrino masses.
As previously mentioned, evidence shows neutrinos must have mass. But within the standard model, the right-handed neutrino does not exist, so even with a Yukawa coupling neutrinos remain massless. An obvious solution is to simply "add a right-handed neutrino" resulting in a Dirac mass term as usual. This field however must be a sterile neutrino, since being right-handed it experimentally belongs to an isospin singlet () and also has charge , implying (see above) i.e. it does not even participate in the weak interaction. Current experimental status is that evidence for observation of sterile neutrinos is not convincing.
Another possibility to consider is that the neutrino satisfies the Majorana equation, which at first seems possible due to its zero electric charge. In this case the mass term is
where denotes a charge conjugated (i.e. anti-) particle, and the terms are consistently all left (or all right) chirality (note that a left-chirality projection of an antiparticle is a right-handed field; care must be taken here due to different notations sometimes used). Here we are essentially flipping between LH neutrinos and RH anti-neutrinos (it is furthermore possible but "not" necessary that neutrinos are their own antiparticle, so these particles are the same). However for the left-chirality neutrinos, this term changes weak hypercharge by 2 units - not possible with the standard Higgs interation, requiring the Higgs field to be extended to include an extra triplet with weak hypercharge 2 - whereas for right-chirality neutrinos, no Higgs extensions are necessary. For both left and right chirality cases, Majorana terms violate lepton number, but possibly at a level beyond the current sensitivity of experiments to detect such violations.
It is possible to include both Dirac and Majorana mass terms in the same theory, which (in contrast to the Dirac-mass-only approach) can provide a "natural" explanation for the smallness of the observed neutrino masses, by linking the RH neutrinos to yet-unknown physics around the GUT scale (see seesaw mechanism).
Since in any case new fields must be postulated to explain the experimental results, neutrinos are an obvious gateway to searching physics beyond the Standard Model.
Detailed Information.
This section provides more detail on some aspects, and some reference material.
Field content in detail.
The Standard Model has the following fields. These describe one "generation" of leptons and quarks, and there are three generations, so there are three copies of each field. By CPT symmetry, there is a set of right-handed fermions with the opposite quantum numbers. The column "representation" indicates under which representations of the gauge groups that each field transforms, in the order (SU(3), SU(2), U(1)). Symbols used are common but not universal; superscript C denotes an antiparticle; and for the U(1) group, the value of the weak hypercharge is listed. Note that there are twice as many left-handed lepton field components as left-handed antilepton field components in each generation, but an equal number of left-handed quark and antiquark fields.
Fermion content.
This table is based in part on data gathered by the Particle Data Group.
Free parameters.
Upon writing the most general Lagrangian without neutrinos, one finds that the dynamics depend on 19 parameters, whose numerical values are established by experiment. With neutrinos 7 more parameters are needed, 3 masses and 4 PMNS matrix parameters, for a total of 26 parameters. The neutrino parameter values are still uncertain. The 19 certain parameters are summarized here (note: with the Higgs mass is at 125 GeV, the Higgs self-coupling strength "λ" ~ 1/8).
Additional symmetries of the Standard Model.
From the theoretical point of view, the Standard Model exhibits four additional global symmetries, not postulated at the outset of its construction, collectively denoted accidental symmetries, which are continuous U(1) global symmetries. The transformations leaving the Lagrangian invariant are:
The first transformation rule is shorthand meaning that all quark fields for all generations must be rotated by an identical phase simultaneously. The fields and formula_72 are the 2nd (muon) and 3rd (tau) generation analogs of and formula_73 fields.
By Noether's theorem, each symmetry above has an associated conservation law: the conservation of baryon number, electron number, muon number, and tau number. Each quark is assigned a baryon number of formula_74, while each antiquark is assigned a baryon number of formula_75. Conservation of baryon number implies that the number of quarks minus the number of antiquarks is a constant. Within experimental limits, no violation of this conservation law has been found.
Similarly, each electron and its associated neutrino is assigned an electron number of +1, while the anti-electron and the associated anti-neutrino carry a −1 electron number. Similarly, the muons and their neutrinos are assigned a muon number of +1 and the tau leptons are assigned a tau lepton number of +1. The Standard Model predicts that each of these three numbers should be conserved separately in a manner similar to the way baryon number is conserved. These numbers are collectively known as lepton family numbers (LF).
In addition to the accidental (but exact) symmetries described above, the Standard Model exhibits several approximate symmetries. These are the "SU(2) custodial symmetry" and the "SU(2) or SU(3) quark flavor symmetry."
The U(1) symmetry.
For the leptons, the gauge group can be written . The two U(1) factors can be combined into where l is the lepton number. Gauging of the lepton number is ruled out by experiment, leaving only the possible gauge group . A similar argument in the quark sector also gives the same result for the electroweak theory.
The charged and neutral current couplings and Fermi theory.
The charged currents formula_76 are
These charged currents are precisely those that entered the Fermi theory of beta decay. The action contains the charge current piece
For energy much less than the mass of the W-boson, the effective theory becomes the current–current interaction of the Fermi theory.
However, gauge invariance now requires that the component formula_79 of the gauge field also be coupled to a current that lies in the triplet of SU(2). However, this mixes with the U(1), and another current in that sector is needed. These currents must be uncharged in order to conserve charge. So we require the neutral currents
The neutral current piece in the Lagrangian is then

</doc>
<doc id="26860351" url="https://en.wikipedia.org/wiki?curid=26860351" title="Standard-Model Extension">
Standard-Model Extension

Standard-Model Extension (SME) is an effective field theory that contains the Standard Model, general relativity, and all possible operators that break Lorentz symmetry.
Violations of this fundamental symmetry can be studied within this general framework. CPT violation implies the breaking of Lorentz symmetry,
and the SME includes operators that both break and preserve CPT symmetry.
Development.
In 1989, Alan Kostelecký and Stuart Samuel proved that interactions in string theories could lead to the spontaneous breaking of Lorentz symmetry.
Later studies have indicated that loop-quantum gravity, non-commutative field theories, brane-world scenarios, and random dynamics models also involve the breakdown of Lorentz invariance.
Interest in Lorentz violation has grown rapidly in the last decades because it can arise in these and other candidate theories for quantum gravity. In the early 1990s, it was shown in the context of bosonic superstrings that string interactions can also spontaneously break CPT symmetry. This work
suggested that experiments with kaon interferometry would be promising for seeking possible signals of CPT violation due to their high sensitivity.
The SME was conceived to facilitate experimental investigations of Lorentz and CPT symmetry, given the theoretical motivation for violation of these symmetries. An initial step, in 1995, was the introduction of effective interactions.
Although Lorentz-breaking interactions are motivated by constructs such as string theory, the low-energy effective action appearing in the SME is independent of the underlying theory. Each term in the effective theory involves the expectation of a tensor field in the underlying theory. These coefficients are small due to Planck-scale suppression, and in principle are measurable in experiments. The first case considered the mixing of neutral mesons, because their interferometric nature makes them highly sensitive to suppressed effects.
In 1997 and 1998, two papers by Don Colladay and Alan Kostelecký gave birth to the minimal SME in flat spacetime. This provided a framework for Lorentz violation across the spectrum of standard-model particles, and provided information about types of signals for potential new experimental searches.
In 2004, the leading Lorentz-breaking terms in curved spacetimes were published,
thereby completing the picture for the minimal SME. In 1999, Sidney Coleman and Sheldon Glashow presented a special
isotropic limit of the SME.
Higher-order Lorentz violating terms have been studied in various contexts, including electrodynamics.
Lorentz transformations: observer vs. particle.
Lorentz violation implies a measurable difference between two systems differing only by a particle Lorentz transformation. The distinction between particle and observer transformations is essential to understanding Lorentz violation in physics.
In special relativity, observer Lorentz transformations relate measurements made in reference frames with differing velocities and orientations. The coordinates in the one system are related to those in the other by an observer Lorentz transformation—a rotation, a boost, or a combination of both. Both observers will agree on the laws of physics, since this transformation is simply a change of coordinates. On the other hand, identical experiments can be rotated or boosted relative to each other, while being studied by the same inertial observer. These transformations are called particle transformations, because the matter and fields of the experiment are physically transformed into the new configuration.
In a conventional vacuum, observer and particle transformations can be related to each other in a simple way—basically one is the inverse of the other. This apparent equivalence is often expressed using the terminology of active and passive transformations. The equivalence fails in Lorentz-violating theories, however, because fixed background fields are the source of the symmetry breaking. These background fields are tensor-like quantities, creating preferred directions and boost-dependent effects. The fields extend over all space and time, and are essentially frozen. When an experiment sensitive to one of the background fields is rotated or boosted, i.e. particle transformed, the background fields remain unchanged, and measurable effects are possible. Observer Lorentz symmetry is expected for all theories, including Lorentz violating ones, since a change in the coordinates cannot affect the physics. This invariance is implemented in field theories by writing a scalar lagrangian, with properly contracted spacetime indices. Particle Lorentz breaking enters if the theory includes fixed SME background fields filling the universe.
Building the SME.
The SME can be expressed as a Lagrangian with various terms. Each Lorentz-violating term is an observer scalar constructed by contracting standard field operators with controlling coefficients called coefficients for Lorentz violation. Notice that these are not parameters of the theory, since they can in principle be measured by appropriate experiments. The coefficients are expected to be small because of the Planck-scale suppression, so perturbative methods are appropriate. In some cases, other suppression mechanisms could mask large Lorentz violations. For instance, large violations that may exist in gravity could have gone undetected so far because of couplings with weak gravitational fields.
Stability and causality of the theory have been studied in detail.
Spontaneous Lorentz symmetry breaking.
In field theory, there are two possible ways to implement the breaking of a symmetry: explicit and spontaneous. A key result in the formal theory of Lorentz violation, published by Kostelecký in 2004, is that explicit Lorentz violation leads to incompatibility of the Bianchi identities with the covariant conservation laws for the energy-momentum and spin-density tensors, whereas spontaneous Lorentz breaking evades this difficulty. This theorem requires that any breaking of Lorentz symmetry must be dynamical. Formal studies of the possible causes of the breakdown of Lorentz symmetry include investigations of the fate of the expected Nambu-Goldstone modes. Goldstone's theorem implies that the spontaneous breaking must be accompanied by massless bosons. These modes might be identified with the photon,
the graviton,
spin-dependent interactions,
and spin-independent interactions.
Experimental searches.
The possible signals of Lorentz violation in any experiment can be calculated from the SME. 
It has therefore proven to be a remarkable tool in the search for Lorentz violation across the landscape of experimental physics. Up until the present, experimental results have taken the form of upper bounds on the SME coefficients. Since the results will be numerically different for different inertial reference frames, the standard frame adopted for reporting results is the Sun-centered frame. This frame is a practical and appropriate choice, since it is accessible and inertial on the time scale of hundreds of years.
Typical experiments seek couplings between the background fields and various particle properties such as spin, or propagation direction. One of the key signals of Lorentz violation arises because experiments on Earth are unavoidably rotating and revolving relative to the Sun-centered frame. These motions lead to both annual and sidereal variations of the measured coefficients for Lorentz violation. Since the translational motion of the Earth around the Sun is nonrelativistic, annual variations are typically suppressed by a factor 10−4. This makes sidereal variations the leading time-dependent effect to look for in experimental data.
Measurements of SME coefficients have been done with experiments involving:
All experimental results for SME coefficients are tabulated in the Data Tables for Lorentz and CPT Violation.

</doc>
<doc id="41741" url="https://en.wikipedia.org/wiki?curid=41741" title="Standing wave">
Standing wave

In physics, a standing wave – also known as a stationary wave – is a wave in a medium in which each point on the axis of the wave has an associated constant amplitude. The locations at which the amplitude is minimum are called nodes, and the locations where the amplitude is maximum are called antinodes. 
This phenomenon can occur because the medium is moving in the opposite direction to the wave, or it can arise in a stationary medium as a result of interference between two waves traveling in opposite directions. The most common cause of standing waves is the phenomenon of resonance, in which standing waves occur inside a resonator due to interference between waves reflected back and forth at the resonator's resonant frequency. 
For waves of equal amplitude traveling in opposing directions, there is on average no net propagation of energy.
Moving medium.
As an example of the first type, under certain meteorological conditions standing waves form in the atmosphere in the lee of mountain ranges. Such waves are often exploited by glider pilots.
Standing waves and hydraulic jumps also form on fast flowing river rapids and tidal currents such as the Saltstraumen maelstrom. Many standing river waves are popular river surfing breaks.
Opposing waves.
As an example of the second type, a "standing wave" in a transmission line is a wave in which the distribution of current, voltage, or field strength is formed by the superposition of two waves of the same frequency propagating in opposite directions. The effect is a series of nodes (zero displacement) and anti-nodes (maximum displacement) at fixed points along the transmission line. Such a standing wave may be formed when a wave is transmitted into one end of a transmission line and is reflected from the other end by an impedance mismatch, "i.e.", discontinuity, such as an open circuit or a short. The failure of the line to transfer power at the standing wave frequency will usually result in attenuation distortion.
In practice, losses in the transmission line and other components mean that a perfect reflection and a pure standing wave are never achieved. The result is a "partial standing wave", which is a superposition of a standing wave and a traveling wave. The degree to which the wave resembles either a pure standing wave or a pure traveling wave is measured by the standing wave ratio (SWR).
Another example is standing waves in the open ocean formed by waves with the same wave period moving in opposite directions. These may form near storm centres, or from reflection of a swell at the shore, and are the source of microbaroms and microseisms.
Mathematical description.
In one dimension, two waves with the same frequency, wavelength and amplitude traveling in opposite directions will interfere and produce a standing wave or stationary wave. For example: a wave traveling to the right along a taut string and hitting the end will reflect back in the other direction along the string, and the two waves will superpose to produce a standing wave. The reflective wave has to have the same amplitude and frequency as the incoming wave.
If the string is held at both ends, forcing zero movement at the ends, the ends become zeroes or "nodes" of the wave. The length of the string then becomes a measure of which waves the string will entertain: the longest wavelength is called the "fundamental". Half a wavelength of the fundamental fits on the string. Shorter wavelengths also can be supported as long as multiples of half a wavelength fit on the string. The frequencies of these waves all are multiples of the fundamental, and are called "harmonics" or "overtones". For example, a guitar player can select an overtone by putting a finger on a string to force a node at the proper position between the ends of the string, suppressing all harmonics that do not share this node.
Harmonic waves travelling in opposite directions can be represented by the equations below:
and
where:
So the resultant wave "y" equation will be the sum of "y1" and "y2":
Using the trigonometric sum-to-product identity for 'sin("u") + sin("v")' to simplify:
This describes a wave that oscillates in time, but has a spatial dependence that is stationary: sin("kx"). At locations "x" = 0, "λ"/2, "λ", 3"λ"/2, ... called the nodes the amplitude is always zero, whereas at locations "x" = "λ"/4, 3"λ"/4, 5"λ"/4, ... called the anti-nodes, the amplitude is maximum. The distance between two conjugative nodes or anti-nodes is "λ"/2.
Standing waves can also occur in two- or three-dimensional resonators. With standing waves on two-dimensional membranes such as drumheads, illustrated in the animations above, the nodes become nodal lines, lines on the surface at which there is no movement, that separate regions vibrating with opposite phase. These nodal line patterns are called Chladni figures. In three-dimensional resonators, such as musical instrument sound boxes and microwave cavity resonators, there are nodal surfaces.
Examples.
One easy example to understand standing waves is two people shaking either end of a jump rope. If they shake in sync the rope can form a regular pattern of waves oscillating up and down, with stationary points along the rope where the rope is almost still (nodes) and points where the arc of the rope is maximum (antinodes)
Sound waves.
Standing waves are also observed in physical media such as strings and columns of air. Any waves traveling along the medium will reflect back when they reach the end. This effect is most noticeable in musical instruments where, at various multiples of a vibrating string or air column's natural frequency, a standing wave is created, allowing harmonics to be identified. Nodes occur at fixed ends and anti-nodes at open ends. If fixed at only one end, only odd-numbered harmonics are available. At the open end of a pipe the anti-node will not be exactly at the end as it is altered by its contact with the air and so end correction is used to place it exactly. The density of a string will affect the frequency at which harmonics will be produced; the greater the density the lower the frequency needs to be to produce a standing wave of the same harmonic.
Light.
Standing waves are also observed in optical media such as optical wave guides, optical cavities, etc. Lasers use optical cavities in the form of a pair of facing mirrors. The gain medium in the cavity (such as a crystal) emits light coherently, exciting standing waves of light in the cavity. The wavelength of light is very short (in the range of nanometers, 10−9 m) so the standing waves are microscopic in size. One use for standing light waves is to measure small distances, using optical flats.
Mechanical waves.
Standing waves can be mechanically induced into solid medium using resonance. One easy to understand example is two people shaking either end of a jump rope. If they shake in sync, the rope will form a regular pattern with nodes and antinodes and appear to be stationary, hence the name standing wave. Similarly a cantilever beam can have a standing wave imposed on it by applying a base excitation. In this case the free end moves the greatest distance laterally compared to any location along the beam. Such a device can be used as a sensor to track changes in frequency or phase of the resonance of the fiber. One application is as a measurement device for dimensional metrology.
Seismic waves.
Standing surface waves on the Earth are observed as free oscillations of the Earth.
Faraday waves.
The Faraday wave is a non-linear standing wave at the air-liquid interface induced by hydrodynamic instability. It can be used as a liquid-based template to assemble microscale materials.

</doc>
<doc id="28481" url="https://en.wikipedia.org/wiki?curid=28481" title="Statistical mechanics">
Statistical mechanics

Statistical mechanics is a branch of physics that studies the average behaviour of a mechanical system composed of identical or equivalent subunits.
The classical view of the universe was that its fundamental laws are mechanical in nature, and that all physical systems are therefore governed by mechanical laws at a microscopic level. These laws are precise equations of motion that map any given initial state to a corresponding future state at a later time. There is however a disconnection between these laws and everyday life experiences, as we do not find it necessary (nor even theoretically possible) to know exactly at a microscopic level the simultaneous positions and velocities of each molecule while carrying out processes at the human scale (for example, when performing a chemical reaction). Statistical mechanics is a collection of mathematical tools that are used to fill this disconnection between the laws of mechanics and the practical experience of incomplete knowledge.
A common use of statistical mechanics is in explaining the thermodynamic behaviour of large systems. Microscopic mechanical laws do not contain concepts such as temperature, heat, or entropy, however, statistical mechanics shows how these concepts arise from the natural uncertainty that arises about the state of a system when that system is prepared in practice. The benefit of using statistical mechanics is that it provides exact methods to connect thermodynamic quantities (such as heat capacity) to microscopic behaviour, whereas in classical thermodynamics the only available option would be to just measure and tabulate such quantities for various materials. Statistical mechanics also makes it possible to "extend" the laws of thermodynamics to cases which are not considered in classical thermodynamics, for example microscopic systems and other mechanical systems with few degrees of freedom. This branch of statistical mechanics which treats and extends classical thermodynamics is known as statistical thermodynamics or equilibrium statistical mechanics.
Statistical mechanics also finds use outside equilibrium. An important subbranch known as non-equilibrium statistical mechanics deals with the issue of microscopically modelling the speed of irreversible processes that are driven by imbalances. Examples of such processes include chemical reactions, or flows of particles and heat. Unlike with equilibrium, there is no exact formalism that applies to non-equilibrium statistical mechanics in general and so this branch of statistical mechanics remains an active area of theoretical research.
Principles: mechanics and ensembles.
In physics there are two types of mechanics usually examined: classical mechanics and quantum mechanics. For both types of mechanics, the standard mathematical approach is to consider two ingredients:
Using these two ingredients, the state at any other time, past or future, can in principle be calculated.
Whereas ordinary mechanics only considers the behaviour of a single state, statistical mechanics introduces the statistical ensemble, which is a large collection of virtual, independent copies of the system in various states. The statistical ensemble is a probability distribution over all possible states of the system. In classical statistical mechanics, the ensemble is a probability distribution over phase points (as opposed to a single phase point in ordinary mechanics), usually represented as a distribution in a phase space with canonical coordinates. In quantum statistical mechanics, the ensemble is a probability distribution over pure states, and can be compactly summarized as a density matrix.
As is usual for probabilities, the ensemble can be interpreted in different ways:
These two meanings are equivalent for many purposes, and will be used interchangeably in this article.
However the probability is interpreted, each state in the ensemble evolves over time according to the equation of motion. Thus, the ensemble itself (the probability distribution over states) also evolves, as the virtual systems in the ensemble continually leave one state and enter another. The ensemble evolution is given by the Liouville equation (classical mechanics) or the von Neumann equation (quantum mechanics). These equations are simply derived by the application of the mechanical equation of motion separately to each virtual system contained in the ensemble, with the probability of the virtual system being conserved over time as it evolves from state to state.
One special class of ensemble is those ensembles that do not evolve over time. These ensembles are known as "equilibrium ensembles" and their condition is known as "statistical equilibrium". Statistical equilibrium occurs if, for each state in the ensemble, the ensemble also contains all of its future and past states with probabilities equal to the probability of being in that state. The study of equilibrium ensembles of isolated systems is the focus of statistical thermodynamics. Non-equilibrium statistical mechanics addresses the more general case of ensembles that change over time, and/or ensembles of non-isolated systems.
Statistical thermodynamics.
The primary goal of statistical thermodynamics (also known as equilibrium statistical mechanics) is to explain the classical thermodynamics of materials in terms of the properties of their constituent particles and the interactions between them. In other words, statistical thermodynamics provides a connection between the macroscopic properties of materials in thermodynamic equilibrium, and the microscopic behaviours and motions occurring inside the material.
As an example, one might ask what is it about a thermodynamic system of NH3 molecules that determines the free energy characteristic of that compound? Classical thermodynamics does not provide the answer. If, for example, we were given spectroscopic data, of this body of gas molecules, such as bond length, bond angle, bond rotation, and flexibility of the bonds in NH3 we should see that the free energy could not be other than it is. To prove this true, we need to bridge the gap between the microscopic realm of atoms and molecules and the macroscopic realm of classical thermodynamics. Statistical mechanics demonstrates how the thermodynamic parameters of a system, such as temperature and pressure, are related to microscopic behaviours of such constituent atoms and molecules.
Although we may understand a system generically, in general we lack information about the state of a specific instance of that system. For this reason the notion of statistical ensemble (a probability distribution over possible states) is necessary. Furthermore, in order to reflect that the material is in a thermodynamic equilibrium, it is necessary to introduce a corresponding statistical mechanical definition of equilibrium. The analogue of thermodynamic equilibrium in statistical thermodynamics is the ensemble property of statistical equilibrium, described in the previous section. An additional assumption in statistical thermodynamics is that the system is isolated (no varying external forces are acting on the system), so that its total energy does not vary over time. A sufficient (but not necessary) condition for statistical equilibrium with an isolated system is that the probability distribution is a function only of conserved properties (total energy, total particle numbers, etc.).
Fundamental postulate.
There are many different equilibrium ensembles that can be considered, and only some of them correspond to thermodynamics. An additional postulate is necessary to motivate why the ensemble for a given system should have one form or another.
A common approach found in many textbooks is to take the "equal a priori probability postulate". This postulate states that
The equal a priori probability postulate therefore provides a motivation for the microcanonical ensemble described below. There are various arguments in favour of the equal a priori probability postulate:
Other fundamental postulates for statistical mechanics have also been proposed.
In any case, the reason for establishing the microcanonical ensemble is mainly axiomatic. The microcanonical ensemble itself is mathematically awkward to use for real calculations, and even very simple finite systems can only be solved approximately. However, it is possible to use the microcanonical ensemble to construct a hypothetical infinite thermodynamic reservoir that has an exactly defined notion of temperature and chemical potential. Once this reservoir has been established, it can be used to justify exactly the canonical ensemble or grand canonical ensemble (see below) for any other system by considering the contact of this system with the reservoir. These other ensembles are those actually used in practical statistical mechanics calculations as they are mathematically simpler and also correspond to a much more realistic situation (energy not known exactly).
Three thermodynamic ensembles.
There are three equilibrium ensembles with a simple form that can be defined for any isolated system bounded inside a finite volume. These are the most often discussed ensembles in statistical thermodynamics. In the macroscopic limit (defined below) they all correspond to classical thermodynamics.
Statistical fluctuations and the macroscopic limit.
The thermodynamic ensembles' most significant difference is that they either admit uncertainty in the variables of energy or particle number, or that those variables are fixed to particular values. While this difference can be observed in some cases, for macroscopic systems the thermodynamic ensembles are usually observationally equivalent.
The limit of large systems in statistical mechanics is known as the thermodynamic limit. In the thermodynamic limit the microcanonical, canonical, and grand canonical ensembles tend to give identical predictions about thermodynamic characteristics. This means that one can specify either total energy or temperature and arrive at the same result; likewise one can specify either total particle number or chemical potential. Given these considerations, the best ensemble to choose for the calculation of the properties of a macroscopic system is usually just the ensemble which allows the result to be derived most easily.
Important cases where the thermodynamic ensembles "do not" give identical results include:
In these cases the correct thermodynamic ensemble must be chosen as there are observable differences between these ensembles not just in the size of fluctuations, but also in average quantities such as the distribution of particles. The correct ensemble is that which corresponds to the way the system has been prepared and characterized—in other words, the ensemble that reflects the knowledge about that system.
Illustrative example (a gas).
The above concepts can be illustrated for the specific case of one liter of ammonia gas at standard conditions. (Note that statistical thermodynamics is not restricted to the study of macroscopic gases, and the example of a gas is given here to illustrate concepts. Statistical mechanics and statistical thermodynamics apply to all mechanical systems (including microscopic systems) and to all phases of matter: liquids, solids, plasmas, gases, nuclear matter, quark matter.)
A simple way to prepare one litre sample of ammonia in a standard condition is to take a very large reservoir of ammonia at those standard conditions, and connect it to a previously evacuated one-litre container. After ammonia gas has entered the container and the container has been given time to reach thermodynamic equilibrium with the reservoir, the container is then sealed and isolated. In thermodynamics, this is a repeatable process resulting in a very well defined sample of gas with a precise description. We now consider the corresponding precise description in statistical thermodynamics.
Although this process is well defined and repeatable in a macroscopic sense, we have no information about the exact locations and velocities of each and every molecule in the container of gas. Moreover, we do not even know exactly how many molecules are in the container; even supposing we knew exactly the average density of the ammonia gas in general, we do not know how many molecules of the gas happened to be inside our container at the moment when we sealed it. The sample is in equilibrium and is in equilibrium with the reservoir: we could reconnect it to the reservoir for some time, and then re-seal it, and our knowledge about the state of the gas would not change. In this case, our knowledge about the state of the gas is precisely described by the grand canonical ensemble. Provided we have an accurate microscopic model of the ammonia gas, we could in principle compute all thermodynamic properties of this sample of gas by using the distribution provided by the grand canonical ensemble.
Hypothetically, we could use an extremely sensitive weight scale to measure exactly the mass of the container before and after introducing the ammonia gas, so that we can exactly know the number of ammonia molecules. After we make this measurement, then our knowledge about the gas would correspond to the canonical ensemble. Finally, suppose by some hypothetical apparatus we can measure exactly the number of molecules and also measure exactly the total energy of the system. Supposing furthermore that this apparatus gives us no further information about the molecules' positions and velocities, our knowledge about the system would correspond to the microcanonical ensemble.
Even after making such measurements, however, our expectations about the behaviour of the gas do not change appreciably. This is because the gas sample is macroscopic and approximates very well the thermodynamic limit, so the different ensembles behave similarly. This can be demonstrated by considering how small the actual fluctuations would be.
Suppose that we knew the number density of ammonia gas was exactly molecules per liter inside the reservoir of ammonia gas used to fill the one-litre container. In describing the container with the grand canonical ensemble, then, the average number of molecules would be formula_1 and the uncertainty (standard deviation) in the number of molecules would be formula_2 (assuming Poisson distribution), which is relatively very small compared to the total number of molecules. Upon measuring the particle number (thus arriving at a canonical ensemble) we should find very nearly molecules. For example, the probability of finding more than or less than molecules would be about 1 in 103000000000.
Calculation methods.
Once the characteristic state function for an ensemble has been calculated for a given system, that system is 'solved' (macroscopic observables can be extracted from the characteristic state function). Calculating the characteristic state function of a thermodynamic ensemble is not necessarily a simple task, however, since it involves considering every possible state of the system. While some hypothetical systems have been exactly solved, the most general (and realistic) case is too complex for exact solution. Various approaches exist to approximate the true ensemble and allow calculation of average quantities.
Exact.
There are some cases which allow exact solutions.
Monte Carlo.
One approximate approach that is particularly well suited to computers is the Monte Carlo method, which examines just a few of the possible states of the system, with the states chosen randomly (with a fair weight). As long as these states form a representative sample of the whole set of states of the system, the approximate characteristic function is obtained. As more and more random samples are included, the errors are reduced to an arbitrarily low level.
Non-equilibrium statistical mechanics.
There are many physical phenomena of interest that involve quasi-thermodynamic processes out of equilibrium, for example:
All of these processes occur over time with characteristic rates, and these rates are of importance for engineering. The field of non-equilibrium statistical mechanics is concerned with understanding these non-equilibrium processes at the microscopic level. (Statistical thermodynamics can only be used to calculate the final result, after the external imbalances have been removed and the ensemble has settled back down to equilibrium.)
In principle, non-equilibrium statistical mechanics could be mathematically exact: ensembles for an isolated system evolve over time according to deterministic equations such as Liouville's equation or its quantum equivalent, the von Neumann equation. These equations are the result of applying the mechanical equations of motion independently to each state in the ensemble. Unfortunately, these ensemble evolution equations inherit much of the complexity of the underlying mechanical motion, and so exact solutions are very difficult to obtain. Moreover, the ensemble evolution equations are fully reversible and do not destroy information (the ensemble's Gibbs entropy is preserved). In order to make headway in modelling irreversible processes, it is necessary to add additional ingredients besides probability and reversible mechanics.
Non-equilibrium mechanics is therefore an active area of theoretical research as the range of validity of these additional assumptions continues to be explored. A few approaches are described in the following subsections.
Stochastic methods.
One approach to non-equilibrium statistical mechanics is to incorporate stochastic (random) behaviour into the system. Stochastic behaviour destroys information contained in the ensemble. While this is technically inaccurate (aside from hypothetical situations involving black holes, a system cannot in itself cause loss of information), the randomness is added to reflect that information of interest becomes converted over time into subtle correlations within the system, or to correlations between the system and environment. These correlations appear as chaotic or pseudorandom influences on the variables of interest. By replacing these correlations with randomness proper, the calculations can be made much easier.
Near-equilibrium methods.
Another important class of non-equilibrium statistical mechanical models deals with systems that are only very slightly perturbed from equilibrium. With very small perturbations, the response can be analysed in linear response theory. A remarkable result, as formalized by the fluctuation-dissipation theorem, is that the response of a system when near equilibrium is precisely related to the fluctuations that occur when the system is in total equilibrium. Essentially, a system that is slightly away from equilibrium—whether put there by external forces or by fluctuations—relaxes towards equilibrium in the same way, since the system cannot tell the difference or "know" how it came to be away from equilibrium.
This provides an indirect avenue for obtaining numbers such as ohmic conductivity and thermal conductivity by extracting results from equilibrium statistical mechanics. Since equilibrium statistical mechanics is mathematically well defined and (in some cases) more amenable for calculations, the fluctuation-dissipation connection can be a convenient shortcut for calculations in near-equilibrium statistical mechanics.
A few of the theoretical tools used to make this connection include:
Hybrid methods.
An advanced approach uses a combination of stochastic methods and linear response theory. As an example, one approach to compute quantum coherence effects (weak localization, conductance fluctuations) in the conductance of an electronic system is the use of the Green-Kubo relations, with the inclusion of stochastic dephasing by interactions between various electrons by use of the Keldysh method.
Applications outside thermodynamics.
The ensemble formalism also can be used to analyze general mechanical systems with uncertainty in knowledge about the state of a system. Ensembles are also used in:
History.
In 1738, Swiss physicist and mathematician Daniel Bernoulli published "Hydrodynamica" which laid the basis for the kinetic theory of gases. In this work, Bernoulli posited the argument, still used to this day, that gases consist of great numbers of molecules moving in all directions, that their impact on a surface causes the gas pressure that we feel, and that what we experience as heat is simply the kinetic energy of their motion.
In 1859, after reading a paper on the diffusion of molecules by Rudolf Clausius, Scottish physicist James Clerk Maxwell formulated the Maxwell distribution of molecular velocities, which gave the proportion of molecules having a certain velocity in a specific range. This was the first-ever statistical law in physics. Five years later, in 1864, Ludwig Boltzmann, a young student in Vienna, came across Maxwell’s paper and was so inspired by it that he spent much of his life developing the subject further.
Statistical mechanics proper was initiated in the 1870s with the work of Boltzmann, much of which was collectively published in his 1896 "Lectures on Gas Theory". Boltzmann's original papers on the statistical interpretation of thermodynamics, the H-theorem, transport theory, thermal equilibrium, the equation of state of gases, and similar subjects, occupy about 2,000 pages in the proceedings of the Vienna Academy and other societies. Boltzmann introduced the concept of an equilibrium statistical ensemble and also investigated for the first time non-equilibrium statistical mechanics, with his "H"-theorem.
The term "statistical mechanics" was coined by the American mathematical physicist J. Willard Gibbs in 1884. "Probabilistic mechanics" might today seem a more appropriate term, but "statistical mechanics" is firmly entrenched. Shortly before his death, Gibbs published in 1902 "Elementary Principles in Statistical Mechanics", a book which formalized statistical mechanics as a fully general approach to address all mechanical systems—macroscopic or microscopic, gaseous or non-gaseous. Gibbs' methods were initially derived in the framework classical mechanics, however they were of such generality that they were found to adapt easily to the later quantum mechanics, and still form the foundation of statistical mechanics to this day.

</doc>
<doc id="160995" url="https://en.wikipedia.org/wiki?curid=160995" title="Statistical significance">
Statistical significance

In statistical hypothesis testing, statistical significance (or a statistically significant result) is attained when a "p"-value is less than the significance level (denoted α, alpha). 
The "p"-value is the probability of obtaining at least as extreme results given that the null hypothesis is true whereas the significance level α is the probability of rejecting the null hypothesis given that it is true. 
Equivalently, when the null hypothesis specifies the value of a parameter, the data are said to be statistically significant at given confidence level γ = 1 − α when the computed confidence interval for that parameter "fails" to contain the value specified by the null hypothesis.
As a matter of good scientific practice, a significance level is chosen before data collection and is often set to 0.05 (5%). 
Other significance levels (e.g., 0.01) may be used, depending on the field of study. 
In any experiment or observation that involves drawing a sample from a population, there is always the possibility that an observed effect would have occurred due to sampling error alone. But if the "p"-value is less than the significance level (e.g., "p" < 0.05), then an investigator may conclude that the observed effect actually reflects the characteristics of the population rather than just sampling error. Investigators may then report that the result attains statistical significance, thereby rejecting the null hypothesis.
The present-day concept of statistical significance originated with Ronald Fisher when he developed statistical hypothesis testing based on "p"-values in the early 20th century. It was Jerzy Neyman and Egon Pearson who later recommended that the significance level be set ahead of time, prior to any data collection.
The term "significance" does not imply "importance" and the term "statistical significance" is not the same as research, theoretical, or practical significance. For example, the term clinical significance refers to the practical importance of a treatment effect.
History.
The concept of statistical significance was originated by Ronald Fisher when he developed statistical hypothesis testing, which he described as "tests of significance", in his 1925 publication "Statistical Methods for Research Workers". Fisher suggested a probability of one in twenty (0.05) as a convenient cutoff level to reject the null hypothesis. In their 1933 paper, Jerzy Neyman and Egon Pearson recommended that the significance level (e.g. 0.05), which they called α, be set ahead of time, prior to any data collection.
Despite his initial suggestion of 0.05 as a significance level, Fisher did not intend this cutoff value to be fixed, and in his 1956 publication "Statistical methods and scientific inference" he recommended that significant levels be set according to specific circumstances.
Role in statistical hypothesis testing.
Statistical significance plays a pivotal role in statistical hypothesis testing, where it is used to determine whether a null hypothesis should be rejected or retained. A null hypothesis is the general or default statement that nothing happened or changed. For a null hypothesis to be rejected as false, the result has to be identified as being statistically significant, i.e. unlikely to have occurred due to sampling error alone.
To determine whether a result is statistically significant, a researcher would have to calculate a "p"-value, which is the probability of observing an effect given that the null hypothesis is true. The null hypothesis is rejected if the "p"-value is less than the significance or α level. The α level is the probability of rejecting the null hypothesis given that it is true (type I error) and is most often set at 0.05 (5%). If the α level is 0.05, then the conditional probability of a type I error, "given that the null hypothesis is true", is 5%. Then a statistically significant result is one in which the observed "p"-value is less than 5%, which is formally written as "p" < 0.05.
If the α level is set at 0.05, it means that the rejection region comprises 5% of the sampling distribution. These 5% can be allocated to one side of the sampling distribution, as in a one-tailed test, or partitioned to both sides of the distribution as in a two-tailed test, with each tail (or rejection region) containing 2.5% of the distribution. One-tailed tests are more powerful than two-tailed tests, as a null hypothesis can be rejected with a less extreme result. Nevertheless, the use of a one-tailed test is dependent on whether the research question specifies a direction such as whether a group of objects is "heavier" or the performance of students on an assessment is "better".
Stringent significance thresholds in specific fields.
In specific fields such as particle physics and manufacturing, statistical significance is often expressed in multiples of the standard deviation or sigma (σ) of a normal distribution, with significance thresholds set at a much stricter level (e.g. 5σ). For instance, the certainty of the Higgs boson particle's existence was based on the 5σ criterion, which corresponds to a "p"-value of about 1 in 3.5 million.
In other fields of scientific research such as genome-wide association studies significance levels as low as are not uncommon.
Effect size.
Researchers focusing solely on whether their results are statistically significant might report findings that are not substantive and not replicable. To gauge the research significance of their result, researchers are therefore encouraged to always report an effect size along with "p"-values. An effect size measure quantifies the strength of an effect, such as the distance between two means in units of standard deviation (cf. Cohen's d), the correlation between two variables or its square, and other measures.

</doc>
<doc id="26685" url="https://en.wikipedia.org/wiki?curid=26685" title="Statistics">
Statistics

Statistics is the study of the collection, analysis, interpretation, presentation, and organization of data. In applying statistics to, e.g., a scientific, industrial, or societal problem, it is conventional to begin with a statistical population or a statistical model process to be studied. Populations can be diverse topics such as "all persons living in a country" or "every atom composing a crystal". Statistics deals with all aspects of data including the planning of data collection in terms of the design of surveys and experiments.
When census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.
Two main statistical methodologies are used in data analysis: descriptive statistics, which summarizes data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draws conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). Descriptive statistics are most often concerned with two sets of properties of a "distribution" (sample or population): "central tendency" (or "location") seeks to characterize the distribution's central or typical value, while "dispersion" (or "variability") characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.
A standard statistical procedure involves the test of the relationship between two statistical data sets, or a data set and a synthetic data drawn from idealized model. An hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a "false positive") and Type II errors (null hypothesis fails to be rejected and an actual difference between populations is missed giving a "false negative"). Multiple problems have come to be associated with this framework: ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis. 
Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other important types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data and/or censoring may result in biased estimates and specific techniques have been developed to address these problems.
Statistics can be said to have begun in ancient civilization, going back at least to the 5th century BC, but it was not until the 18th century that it started to draw more heavily from calculus and probability theory. Statistics continues to be an area of active research, for example on the problem of how to analyze Big data.
Scope.
Statistics is a mathematical body of science that pertains to the collection, analysis, interpretation or explanation, and presentation of data, or as a branch of mathematics. Some consider statistics to be a distinct mathematical science rather than a branch of mathematics.
Mathematical statistics.
Mathematical statistics is the application of mathematics to statistics, which was originally conceived as the science of the state — the collection and analysis of facts about a country: its economy, land, military, population, and so forth. Mathematical techniques used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory.
Overview.
In applying statistics to a problem, it is common practice to start with a population or process to be studied. Populations can be diverse topics such as "all persons living in a country" or "every atom composing a crystal".
Ideally, statisticians compile data about the entire population (an operation called census). This may be organized by governmental statistical institutes. Descriptive statistics can be used to summarize the population data. Numerical descriptors include mean and standard deviation for continuous data types (like income), while frequency and percentage are more useful in terms of describing categorical data (like race).
When a census is not feasible, a chosen subset of the population called a sample is studied. Once a sample that is representative of the population is determined, data is collected for the sample members in an observational or experimental setting. Again, descriptive statistics can be used to summarize the sample data. However, the drawing of the sample has been subject to an element of randomness, hence the established numerical descriptors from the sample are also due to uncertainty. To still draw meaningful conclusions about the entire population, inferential statistics is needed. It uses patterns in the sample data to draw inferences about the population represented, accounting for randomness. These inferences may take the form of: answering yes/no questions about the data (hypothesis testing), estimating numerical characteristics of the data (estimation), describing associations within the data (correlation) and modeling relationships within the data (for example, using regression analysis). Inference can extend to forecasting, prediction and estimation of unobserved values either in or associated with the population being studied; it can include extrapolation and interpolation of time series or spatial data, and can also include data mining.
Data collection.
Sampling.
When full census data cannot be collected, statisticians collect sample data by developing specific experiment designs and survey samples. Statistics itself also provides tools for prediction and forecasting the use of data through statistical models. 
To use a sample as a guide to an entire population, it is important that it truly represents the overall population. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures. There are also methods of experimental design for experiments that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population.
Sampling theory is part of the mathematical discipline of probability theory. Probability is used in mathematical statistics to study the sampling distributions of sample statistics and, more generally, the properties of statistical procedures. The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method.
The difference in point of view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population to deduce probabilities that pertain to samples. Statistical inference, however, moves in the opposite direction—inductively inferring from samples to the parameters of a larger or total population.
Experimental and observational studies.
A common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables. There are two major types of causal statistical studies: experimental studies and observational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective.
An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated. 
While the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data – like natural experiments and observational studies – for which a statistician would use a modified, more structured estimation method (e.g., Difference in differences estimation and instrumental variables, among many others) that produce consistent estimators.
Experiments.
The basic steps of a statistical experiment are:
Experiments on human behavior have special concerns. The famous Hawthorne study examined changes to the working environment at the Hawthorne plant of the Western Electric Company. The researchers were interested in determining whether increased illumination would increase the productivity of the assembly line workers. The researchers first measured the productivity in the plant, then modified the illumination in an area of the plant and checked if the changes in illumination affected productivity. It turned out that productivity indeed improved (under the experimental conditions). However, the study is heavily criticized today for errors in experimental procedures, specifically for the lack of a control group and blindness. The Hawthorne effect refers to finding that an outcome (in this case, worker productivity) changed due to observation itself. Those in the Hawthorne study became more productive not because the lighting was changed but because they were being observed.
Observational study.
An example of an observational study is one that explores the association between smoking and lung cancer. This type of study typically uses a survey to collect observations about the area of interest and then performs statistical analysis. In this case, the researchers would collect observations of both smokers and non-smokers, perhaps through a case-control study, and then look for the number of cases of lung cancer in each group.
Types of data.
Various attempts have been made to produce a taxonomy of levels of measurement. The psychophysicist Stanley Smith Stevens defined nominal, ordinal, interval, and ratio scales. Nominal measurements do not have meaningful rank order among values, and permit any one-to-one transformation. Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation. Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case with longitude and temperature measurements in Celsius or Fahrenheit), and permit any linear transformation. Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation.
Because variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together as categorical variables, whereas ratio and interval measurements are grouped together as quantitative variables, which can be either discrete or continuous, due to their numerical nature. Such distinctions can often be loosely correlated with data type in computer science, in that dichotomous categorical variables may be represented with the Boolean data type, polytomous categorical variables with arbitrarily assigned integers in the integral data type, and continuous variables with the real data type involving floating point computation. But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented.
Other categorizations have been proposed. For example, Mosteller and Tukey (1977) distinguished grades, ranks, counted fractions, counts, amounts, and balances. Nelder (1990) described continuous counts, continuous ratios, count ratios, and categorical modes of data. See also Chrisman (1998), van den Berg (1991).
The issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. "The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer" (Hand, 2004, p. 82).
Terminology and theory of inferential statistics.
Statistics, estimators and pivotal quantities.
Consider an independent identically distributed (IID) random variables with a given probability distribution: standard statistical inference and estimation theory defines a random sample as the random vector given by the column vector of these IID variables. The population being examined is described by a probability distribution that may have unknown parameters.
A statistic is a random variable that is a function of the random sample, but "not a function of unknown parameters". The probability distribution of the statistic, though, may have unknown parameters.
Consider now a function of the unknown parameter: an estimator is a statistic used to estimate such function. Commonly used estimators include sample mean, unbiased sample variance and sample covariance.
A random variable that is a function of the random sample and of the unknown parameter, but whose probability distribution "does not depend on the unknown parameter" is called a pivotal quantity or pivot. Widely used pivots include the z-score, the chi square statistic and Student's t-value.
Between two estimators of a given parameter, the one with lower mean squared error is said to be more efficient. Furthermore, an estimator is said to be unbiased if its expected value is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at the limit to the true value of such parameter.
Other desirable properties for estimators include: UMVUE estimators that have the lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) and consistent estimators which converges in probability to the true value of such parameter.
This still leaves the question of how to obtain estimators in a given situation and carry the computation, several methods have been proposed: the method of moments, the maximum likelihood method, the least squares method and the more recent method of estimating equations.
Null hypothesis and alternative hypothesis.
Interpretation of statistical information can often involve the development of a null hypothesis which is usually (but not necessarily) that no relationship exists among variables or that no change occurred over time.
The best illustration for a novice is the predicament encountered by a criminal trial. The null hypothesis, H0, asserts that the defendant is innocent, whereas the alternative hypothesis, H1, asserts that the defendant is guilty. The indictment comes because of suspicion of the guilt. The H0 (status quo) stands in opposition to H1 and is maintained unless H1 is supported by evidence "beyond a reasonable doubt". However, "failure to reject H0" in this case does not imply innocence, but merely that the evidence was insufficient to convict. So the jury does not necessarily "accept" H0 but "fails to reject" H0. While one can not "prove" a null hypothesis, one can test how close it is to being true with a power test, which tests for type II errors.
What statisticians call an alternative hypothesis is simply an hypothesis that contradicts the null hypothesis.
Error.
Working from a null hypothesis two basic forms of error are recognized:
Standard deviation refers to the extent to which individual observations in a sample differ from a central value, such as the sample or population mean, while Standard error refers to an estimate of difference between sample mean and population mean.
A statistical error is the amount by which an observation differs from its expected value, a residual is the amount an observation differs from the value the estimator of the expected value assumes on a given sample (also called prediction).
Mean squared error is used for obtaining efficient estimators, a widely used class of estimators. Root mean square error is simply the square root of mean squared error.
Many statistical methods seek to minimize the residual sum of squares, and these are called "methods of least squares" in contrast to Least absolute deviations. The latter gives equal weight to small and big errors, while the former gives more weight to large errors. Residual sum of squares is also differentiable, which provides a handy property for doing regression. Least squares applied to linear regression is called ordinary least squares method and least squares applied to nonlinear regression is called non-linear least squares. Also in a linear regression model the non deterministic part of the model is called error term, disturbance or more simply noise. Both linear regression and non-linear regression are addressed in polynomial least squares, which also describes the variance in a prediction of the dependent variable (y axis) as a function of the independent variable (x axis) and the deviations (errors, noise, disturbances) from the estimated (fitted) curve.
Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data and/or censoring may result in biased estimates and specific techniques have been developed to address these problems.
Interval estimation.
Most studies only sample part of a population, so results don't fully represent the whole population. Any estimates obtained from the sample only approximate the population value. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This does "not" imply that the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable. Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by "probability", that is as a Bayesian probability.
In principle confidence intervals can be symmetrical or asymmetrical. An interval can be asymmetrical because it works as lower or upper bound for a parameter (left-sided interval or right sided interval), but it can also be asymmetrical because the two sided interval is built violating symmetry around the estimate. Sometimes the bounds for a confidence interval are reached asymptotically and these are used to approximate the true bounds.
Significance.
Statistics rarely give a simple Yes/No type answer to the question under analysis. Interpretation often comes down to the level of statistical significance applied to the numbers and often refers to the probability of a value accurately rejecting the null hypothesis (sometimes referred to as the p-value).
The standard approach is to test a null hypothesis against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the null hypothesis. The probability of type I error is therefore the probability that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator doesn't belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false.
Referring to statistical significance does not necessarily mean that the overall result is significant in real world terms. For example, in a large study of a drug it may be shown that the drug has a statistically significant but very small beneficial effect, such that the drug is unlikely to help the patient noticeably.
While in principle the acceptable level of statistical significance may be subject to debate, the p-value is the smallest significance level that allows the test to reject the null hypothesis. This is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. Therefore, the smaller the p-value, the lower the probability of committing type I error.
Some problems are usually associated with this framework (See criticism of hypothesis testing):
Examples.
Some well-known statistical tests and procedures are:
Misuse of statistics.
Misuse of statistics can produce subtle, but serious errors in description and interpretation—subtle in the sense that even experienced professionals make such errors, and serious in the sense that they can lead to devastating decision errors. For instance, social policy, medical practice, and the reliability of structures like bridges all rely on the proper use of statistics.
Even when statistical techniques are correctly applied, the results can be difficult to interpret for those lacking expertise. The statistical significance of a trend in the data—which measures the extent to which a trend could be caused by random variation in the sample—may or may not agree with an intuitive sense of its significance. The set of basic statistical skills (and skepticism) that people need to deal with information in their everyday lives properly is referred to as statistical literacy.
There is a general perception that statistical knowledge is all-too-frequently intentionally misused by finding ways to interpret only the data that are favorable to the presenter. A mistrust and misunderstanding of statistics is associated with the quotation, "There are three kinds of lies: lies, damned lies, and statistics". Misuse of statistics can be both inadvertent and intentional, and the book "How to Lie with Statistics" outlines a range of considerations. In an attempt to shed light on the use and misuse of statistics, reviews of statistical techniques used in particular fields are conducted (e.g. Warne, Lazo, Ramos, and Ritter (2012)).
Ways to avoid misuse of statistics include using proper diagrams and avoiding bias. Misuse can occur when conclusions are overgeneralized and claimed to be representative of more than they really are, often by either deliberately or unconsciously overlooking sampling bias. Bar graphs are arguably the easiest diagrams to use and understand, and they can be made either by hand or with simple computer programs. Unfortunately, most people do not look for bias or errors, so they are not noticed. Thus, people may often believe that something is true even if it is not well represented. To make data gathered from statistics believable and accurate, the sample taken must be representative of the whole. According to Huff, "The dependability of a sample can be destroyed by [bias]... allow yourself some degree of skepticism."
To assist in the understanding of statistics Huff proposed a series of questions to be asked in each case:
Misinterpretation: correlation.
The concept of correlation is particularly noteworthy for the potential confusion it can cause. Statistical analysis of a data set often reveals that two variables (properties) of the population under consideration tend to vary together, as if they were connected. For example, a study of annual income that also looks at age of death might find that poor people tend to have shorter lives than affluent people. The two variables are said to be correlated; however, they may or may not be the cause of one another. The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable or confounding variable. For this reason, there is no way to immediately infer the existence of a causal relationship between the two variables. (See Correlation does not imply causation.)
History of statistical science.
Statistical methods date back at least to the 5th century BC.
Some scholars pinpoint the origin of statistics to 1663, with the publication of "Natural and Political Observations upon the Bills of Mortality" by John Graunt. Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence its "stat-" etymology. The scope of the discipline of statistics broadened in the early 19th century to include the collection and analysis of data in general. Today, statistics is widely employed in government, business, and natural and social sciences.
Its mathematical foundations were laid in the 17th century with the development of the probability theory by Gerolamo Cardano, Blaise Pascal and Pierre de Fermat. Mathematical probability theory arose from the study of games of chance, although the concept of probability was already examined in medieval law and by philosophers such as Juan Caramuel. The method of least squares was first described by Adrien-Marie Legendre in 1805.
The modern field of statistics emerged in the late 19th and early 20th century in three stages. The first wave, at the turn of the century, was led by the work of Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton's contributions included introducing the concepts of standard deviation, correlation, regression analysis and the application of these methods to the study of the variety of human characteristics – height, weight, eyelash length among others. Pearson developed the Pearson product-moment correlation coefficient, defined as a product-moment, the method of moments for the fitting of distributions to samples and the Pearson distribution, among many other things. Galton and Pearson founded "Biometrika" as the first journal of mathematical statistics and biostatistics (then called biometry), and the latter founded the world's first university statistics department at University College London.
Ronald Fisher coined the term null hypothesis during the Lady tasting tea experiment, which "is never proved or established, but is possibly disproved, in the course of experimentation".
The second wave of the 1910s and 20s was initiated by William Gosset, and reached its culmination in the insights of Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher's most important publications were his 1918 seminal paper "The Correlation between Relatives on the Supposition of Mendelian Inheritance", which was the first to use the statistical term, variance, his classic 1925 work "Statistical Methods for Research Workers" and his 1935 "The Design of Experiments", where he developed rigorous design of experiments models. He originated the concepts of sufficiency, ancillary statistics, Fisher's linear discriminator and Fisher information. In his 1930 book "The Genetical Theory of Natural Selection" he applied statistics to various biological concepts such as Fisher's principle). Nevertheless, A. W. F. Edwards has remarked that it is "probably the most celebrated argument in evolutionary biology". (about the sex ratio), the Fisherian runaway, a concept in sexual selection about a positive feedback runaway affect found in evolution.
The final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of "Type II" error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.
Today, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of modern computers has expedited large-scale statistical computations, and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research, for example on the problem of how to analyze Big data.
Applications.
Applied statistics, theoretical statistics and mathematical statistics.
"Applied statistics" comprises descriptive statistics and the application of inferential statistics. "Theoretical statistics" concerns both the logical arguments underlying justification of approaches to statistical inference, as well encompassing "mathematical statistics". Mathematical statistics includes not only the manipulation of probability distributions necessary for deriving results related to methods of estimation and inference, but also various aspects of computational statistics and the design of experiments.
Machine learning and data mining.
There are two applications for machine learning and data mining: data management and data analysis. Statistics tools are necessary for the data analysis.
Statistics in society.
Statistics is applicable to a wide variety of academic disciplines, including natural and social sciences, government, and business. Statistical consultants can help organizations and companies that don't have in-house expertise relevant to their particular questions.
Statistical computing.
The rapid and sustained increases in computing power starting from the second half of the 20th century have had a substantial impact on the practice of statistical science. Early statistical models were almost always from the class of linear models, but powerful computers, coupled with suitable numerical algorithms, caused an increased interest in nonlinear models (such as neural networks) as well as the creation of new types, such as generalized linear models and multilevel models.
Increased computing power has also led to the growing popularity of computationally intensive methods based on resampling, such as permutation tests and the bootstrap, while techniques such as Gibbs sampling have made use of Bayesian models more feasible. The computer revolution has implications for the future of statistics with new emphasis on "experimental" and "empirical" statistics. A large number of both general and special purpose statistical software are now available.
Statistics applied to mathematics or the arts.
Traditionally, statistics was concerned with drawing inferences using a semi-standardized methodology that was "required learning" in most sciences. This has changed with use of statistics in non-inferential contexts. What was once considered a dry subject, taken in many fields as a degree-requirement, is now viewed enthusiastically. Initially derided by some mathematical purists, it is now considered essential methodology in certain areas.
Specialized disciplines.
Statistical techniques are used in a wide range of types of scientific and social research, including: biostatistics, computational biology, computational sociology, network biology, social science, sociology and social research. Some fields of inquiry use applied statistics so extensively that they have specialized terminology. These disciplines include:
In addition, there are particular types of statistical analysis that have also developed their own specialised terminology and methodology:
Statistics form a key basis tool in business and manufacturing as well. It is used to understand measurement systems variability, control processes (as in statistical process control or SPC), for summarizing data, and to make data-driven decisions. In these roles, it is a key tool, and perhaps the only reliable tool.

</doc>
<doc id="2473210" url="https://en.wikipedia.org/wiki?curid=2473210" title="Sterile neutrino">
Sterile neutrino

Sterile neutrinos (or inert neutrinos) are hypothetical particles (neutral leptons – neutrinos) that interact only via gravity and do not interact via any of the fundamental interactions of the Standard Model. The term "sterile neutrino" is used to distinguish them from the known "active neutrinos" in the Standard Model, which are charged under the weak interaction.
This term usually refers to neutrinos with right-handed chirality (see right-handed neutrino), which may be added to the Standard Model. Occasionally it is used in a more general sense for any neutral fermion.
The existence of right-handed neutrinos is theoretically well-motivated, as all other known fermions have been observed with left and right chirality, and they can explain the observed active neutrino masses in a natural way. The mass of the right-handed neutrinos themselves is unknown and could have any value between 1015 GeV and less than one eV.
The number of sterile neutrino types is unknown. This is in contrast to the number of active neutrino types, which has to equal that of charged leptons and quark generations to ensure the anomaly freedom of the electroweak interaction.
The search for sterile neutrinos is an active area of particle physics. If they exist and their mass is smaller than the energies of particles in the experiment, they can be produced in the laboratory, either by mixing between active and sterile neutrinos or in high energy particle collisions. If they are heavier, the only directly observable consequence of their existence would be the observed active neutrino masses. They may, however, be responsible for a number of unexplained phenomena in physical cosmology and astrophysics, including dark matter, baryogenesis or dark radiation.
Sterile neutrinos may be Neutral Heavy Leptons (NHLs, or Heavy Neutral Leptons, HNLs).
Motivation.
Experimental results show that all produced and observed neutrinos have left-handed helicities (spins antiparallel to momenta), and all antineutrinos have right-handed helicities, within the margin of error. In the massless limit, it means that only one of two possible chiralities is observed for either particle. These are the only helicities (and chiralities) included in the Standard Model of particle interactions.
Recent experiments such as neutrino oscillation, however, have shown that neutrinos have a non-zero mass, which is not predicted by the Standard Model and suggests new, unknown physics. This unexpected mass explains neutrinos with right-handed helicity and antineutrinos with left-handed helicity: since they do not move at the speed of light, their helicity is not relativistic invariant (it is possible to move faster than them and observe the opposite helicity). Yet all neutrinos have been observed with left-handed "chirality", and all antineutrinos right-handed. Chirality is a fundamental property of particles and "is" relativistic invariant: it is the same regardless of the particle's speed and mass in every reference frame. The question, thus, remains: can neutrinos and antineutrinos be differentiated only by chirality? Or do right-handed neutrinos and left-handed antineutrinos exist as separate particles?
Properties.
Such particles would belong to a singlet representation with respect to the strong interaction and the weak interaction, having zero electric charge, zero weak hypercharge, zero weak isospin, and, as with the other leptons, no color, although they do have a B-L of −1. If the standard model is embedded in a hypothetical SO(10) grand unified theory, they can be assigned an X charge of −5. The left-handed anti-neutrino has a B-L of 1 and an X charge of 5.
Due to the lack of charge, sterile neutrinos would not interact electromagnetically, weakly, or strongly, making them extremely difficult to detect. They have Yukawa interactions with ordinary leptons and Higgs bosons, which via the Higgs mechanism lead to mixing with ordinary neutrinos. 
In experiments involving energies larger than their mass they would participate in all processes in which ordinary neutrinos take part, but with a quantum mechanical probability that is suppressed by the small mixing angle. That makes it possible to produce them in experiments if they are light enough. 
They would also interact gravitationally due to their mass, however, and if they are heavy enough, they could explain cold dark matter or warm dark matter. In some grand unification theories, such as SO(10), they also interact via gauge interactions which are extremely suppressed at ordinary energies because their gauge boson is extremely massive. They do not appear at all in some other GUTs, such as the Georgi–Glashow model (i.e. all its SU(5) charges or quantum numbers are zero).
Mass.
All particles are initially massless under the Standard Model, since there are no Dirac mass terms in the Standard Model's Lagrangian. The only mass terms are generated by the Higgs mechanism, which produces non-zero Yukawa couplings between the left-handed components of fermions, the Higgs field, and their right-handed components. This occurs when the SU(2) doublet Higgs field formula_1 acquires its non-zero vacuum expectation value, formula_2, spontaneously breaking its SU(2)L × U(1) symmetry, and thus yielding non-zero Yukawa couplings:
Such is the case for charged leptons, like the electron; but within the standard model, the right-handed neutrino does not exist, so even with a Yukawa coupling neutrinos remain massless. In other words, there are no mass terms for neutrinos under the Standard Model: the model only contains a left-handed neutrino and its antiparticle, a right-handed antineutrino, for each generation, produced in weak eigenstates during weak interactions. See neutrino masses in the Standard Model for a detailed explanation.
In the seesaw mechanism, one eigenvector of the neutrino mass matrix, which includes sterile neutrinos, is predicted to be significantly heavier than the other.
A sterile neutrino would have the same weak hypercharge, weak isospin, and mass as its antiparticle. For any charged particle, for example the electron, this is not the case: its antiparticle, the positron, has opposite electric charge, among other opposite charges. Similarly, an up quark has a charge of + and (for example) a color charge of red, while its antiparticle has an electric charge of - and a color charge of anti-red.
Dirac and Majorana terms.
Sterile neutrinos allow the introduction of a Dirac mass term as usual. This can yield the observed neutrino mass, but it requires that the strength of the Yukawa coupling be much weaker for the electron neutrino than the electron, without explanation. Similar problems (although less severe) are observed in the quark sector, where the top and bottom masses differ by a factor 40.
Unlike for the left-handed neutrino, a Majorana mass term can be added for a sterile neutrino without violating local symmetries (weak isospin and weak hypercharge) since it has no weak charge. However, this would still violate total lepton number.
It is possible to include both Dirac and Majorana terms: this is done in the seesaw mechanism (below). In addition to satisfying the Majorana equation, if the neutrino were also its own antiparticle, then it would be the first Majorana fermion. In that case, it could annihilate with another neutrino, allowing neutrinoless double beta decay. The other case is that it is a Dirac fermion, which is not its own antiparticle.
To put this in mathematical terms, we have to make use of the transformation properties of particles. For free fields, a Majorana field is defined as an eigenstate of charge conjugation. However, neutrinos interact only via the weak interactions, which are not invariant under charge conjugation (C), so an interacting Majorana neutrino cannot be an eigenstate of C. The generalized definition is: "a Majorana neutrino field is an eigenstate of the CP transformation". Consequently, Majorana and Dirac neutrinos would behave differently under CP transformations (actually Lorentz and CPT transformations). Also, a massive Dirac neutrino would have nonzero magnetic and electric dipole moments, whereas a Majorana neutrino would not. However, the Majorana and Dirac neutrinos are different only if their rest mass is not zero. For Dirac neutrinos, the dipole moments are proportional to mass and would vanish for a massless particle. Both Majorana and Dirac mass terms however can appear in the mass Lagrangian.
Seesaw mechanism.
In addition to the left-handed neutrino, which couples to its family charged lepton in weak charged currents, if there is also a right-handed sterile neutrino partner, a weak isosinglet with no charge, then it is possible to add a Majorana mass term without violating electroweak symmetry. Both neutrinos have mass and handedness is no longer preserved (thus "left or right-handed neutrino" means that the state is mostly left or right-handed). To get the neutrino mass eigenstates, we have to diagonalize the general mass matrix M:
where formula_5 is big and formula_6 is of intermediate size terms.
Apart from empirical evidence, there is also a theoretical justification for the seesaw mechanism in various extensions to the Standard Model. Both Grand Unification Theories (GUTs) and left-right symmetrical models predict the following relation:
According to GUTs and left-right models, the right-handed neutrino is extremely heavy: , while the smaller eigenvalue is approximately equal to
This is the seesaw mechanism: as the sterile right-handed neutrino gets heavier, the normal left-handed neutrino gets lighter. The left-handed neutrino is a mixture of two Majorana neutrinos, and this mixing process is how sterile neutrino mass is generated.
Detection attempts.
The production and decay of sterile neutrinos could happen through the mixing with virtual ("off mass shell") neutrinos. There were several experiments set up to discover or observe NHLs, for example the NuTeV (E815) experiment at Fermilab or LEP-l3 at CERN. They all lead to establishing limits to observation, rather than actual observation of those particles. If they are indeed a constituent of dark matter, sensitive X-ray detectors would be needed to observe the radiation emitted by their decays.
Sterile neutrinos may mix with ordinary neutrinos via a Dirac mass after electroweak symmetry breaking, in analogy to quarks and charged leptons.
Sterile neutrinos and (in more-complicated models) ordinary neutrinos may also have Majorana masses. In type 1 seesaw mechanism both Dirac and Majorana masses are used to drive ordinary neutrino masses down and make the sterile neutrinos much heavier than the Standard Model's interacting neutrinos. In some models the heavy neutrinos can be as heavy as the GUT scale (). In other models they could be lighter than the weak gauge bosons W and Z as in the so-called νMSM model where their masses are between GeV and keV. A light (with the mass ) sterile neutrino was suggested as a possible explanation of the results of the Liquid Scintillator Neutrino Detector experiment.
On April 11, 2007, researchers at the MiniBooNE experiment at Fermilab announced that they had not found any evidence supporting the existence of such a sterile neutrino. More-recent results and analysis have provided some support for the existence of the sterile neutrino.
Two separate detectors near a nuclear reactor in France found 3% of anti-neutrinos missing. They suggested the existence of a 4th neutrino with a mass of 0.7 keV. Sterile neutrinos are also candidates for dark radiation. Daya Bay has also searched for a light sterile neutrino and excluded some mass regions.
The number of neutrinos and the masses of the particles can have large-scale effects that shape the appearance of the cosmic microwave background. The total number of neutrino species, for instance, affects the rate at which the cosmos expanded in its earliest epochs: more neutrinos means a faster expansion. The Planck Satellite 2013 data release found no evidence of additional neutrino-like particles.

</doc>
<doc id="962171" url="https://en.wikipedia.org/wiki?curid=962171" title="Stern–Gerlach experiment">
Stern–Gerlach experiment

The Stern–Gerlach experiment showed that the spatial orientation of angular momentum is quantized. It demonstrated that atomic-scale systems have intrinsically quantum properties, and that measurement in quantum mechanics affects the system being measured. In the original experiment, silver atoms were sent through a non-uniform magnetic field, which deflected them before they struck a detector screen. Other kinds of particles can be used. If the particles have a magnetic moment related to their spin angular momentum, the magnetic field gradient deflects them from a straight path. The screen reveals discrete points of accumulation rather than a continuous distribution, owing to the quantum nature of spin. Historically, this experiment was decisive in convincing physicists of the reality of angular momentum quantization in all atomic-scale systems.
The experiment was first conducted by the German physicists Otto Stern and Walther Gerlach, in 1922.
Basic theory and description.
The Stern–Gerlach experiment involves sending a beam of particles through an inhomogeneous magnetic field and observing their deflection. The results show that particles possess an intrinsic angular momentum that is closely analogous to the angular momentum of a classically spinning object, but that takes only certain quantized values. Another important result is that only one component of a particle's spin can be measured at one time, meaning that the measurement of the spin along the z-axis destroys information about a particle's spin along the x and y axis.
The experiment is normally conducted using electrically neutral particles or atoms. This avoids the large deflection to the orbit of a charged particle moving through a magnetic field and allows spin-dependent effects to dominate. If the particle is treated as a classical spinning dipole, it will precess in a magnetic field because of the torque that the magnetic field exerts on the dipole (see torque-induced precession). If it moves through a homogeneous magnetic field, the forces exerted on opposite ends of the dipole cancel each other out and the trajectory of the particle is unaffected. However, if the magnetic field is inhomogeneous then the force on one end of the dipole will be slightly greater than the opposing force on the other end, so that there is a net force which deflects the particle's trajectory. If the particles were classical spinning objects, one would expect the distribution of their spin angular momentum vectors to be random and continuous. Each particle would be deflected by a different amount, producing some density distribution on the detector screen. Instead, the particles passing through the Stern–Gerlach apparatus are deflected either up or down by a specific amount. This was a measurement of the quantum observable now known as spin angular momentum, which demonstrated possible outcomes of a measurement where the observable has a discrete set of values or point spectrum. Although some discrete quantum phenomena, such as atomic spectra, were observed much earlier, the Stern–Gerlach experiment allowed scientists to conduct measurements of deliberately superposed quantum states for the first time in the history of science.
By now it is known theoretically that quantum angular momentum "of any kind" has a discrete spectrum, which is sometimes imprecisely expressed as "angular momentum is quantized".
If the experiment is conducted using charged particles like electrons, there will be a Lorentz force that tends to bend the trajectory in a circle (see cyclotron motion). This force can be cancelled by an electric field of appropriate magnitude oriented transverse to the charged particle's path.
Electrons are spin- particles. (It must be noted that observation of the Stern-Gerlach effect with free electrons is infeasible.) These have only two possible spin angular momentum values measured along any axis, +ħ/2 or −ħ/2, a sheerly quantum mechanical phenomenon. Because its value is always the same, it is regarded as an intrinsic property of electrons, and is sometimes known as "intrinsic angular momentum" (to distinguish it from orbital angular momentum, which can vary and depends on the presence of other particles).
To describe the experiment with spin + particles mathematically, it is easiest to use Dirac's bra–ket notation. As the particles pass through the Stern–Gerlach device, they are being observed by the detector which resolves to either spin up or spin down. These are described by the angular momentum quantum number "j", which can take on one of the two possible allowed values, either +ħ/2 or −ħ/2. The act of observing (measuring) the momentum along the z axis corresponds to the operator "J"z. In mathematical terms, the initial state of the particles is
where constants "c"1 and "c"2 are complex numbers. This initial state spin can in fact point in any direction. The squares of the absolute values (|"c"1|2 and |"c"2|2) determine the probabilities that for a system in the initial state formula_2 one of the two possible values of "j" is found after the measurement is made. The constants must also be normalized in order that the probability of finding either one of the values be unity. However, this information is not sufficient to determine the values of "c"1 and "c"2, because they are complex numbers. Therefore the measurement yields only the squared magnitudes of the constants, which are interpreted as probabilities.
Sequential experiments.
If we link multiple Stern–Gerlach apparatuses, we can clearly see that they do not act as simple selectors, but alter the states observed (as in light polarization), according to quantum mechanical law:
History.
The Stern–Gerlach experiment was performed in Frankfurt, Germany in 1922 by Otto Stern and Walther Gerlach. At the time, Stern was an assistant to Max Born at the University of Frankfurt's Institute for Theoretical Physics, and Gerlach was an assistant at the same university's Institute for Experimental Physics.
At the time of the experiment, the most prevalent model for describing the atom was the Bohr model, which described electrons as going around the positively charged nucleus only in certain discrete atomic orbitals or energy levels. Since the electron was quantized to be only in certain positions in space, the separation into distinct orbits was referred to as space quantization. The Stern–Gerlach experiment was meant to test the Bohr–Sommerfeld hypothesis that the direction of the angular momentum of a silver atom is quantized.
Note that the experiment was performed several years before Uhlenbeck and Goudsmit formulated their hypothesis of the existence of the electron spin. Even though the result of the Stern−Gerlach experiment has later turned out to be in agreement with the predictions of quantum mechanics for a spin- particle, the experiment should be seen as a corroboration of the Bohr–Sommerfeld theory.
In 1927, T.E. Phipps and J.B. Taylor reproduced the effect using hydrogen atoms in their ground state, thereby eliminating any doubts that may have been caused by the use of silver atoms. (In 1926 the non-relativistic Schrödinger equation had incorrectly predicted the magnetic moment of hydrogen to be zero in its ground state. To correct this problem Wolfgang Pauli introduced "by hand", so to speak, the 3 Pauli matrices which now bear his name, but which were later shown by Paul Dirac in 1928 to be intrinsic in his relativistic equation.)
The experiment was first performed with an electromagnet that allowed the non-uniform magnetic field to be turned on gradually from a null value. When the field was null, the silver atoms were deposited as a single band on the detecting glass slide. When the field was made stronger, the middle of the band began to widen and eventually to split into two, so that the glass-slide image looked like a lip-print, with an opening in the middle, and closure at either end. In the middle, where the magnetic field was strong enough to split the beam into two, statistically half of the silver atoms had been deflected by the non-uniformity of the field. 
Importance.
The Stern–Gerlach experiment strongly influenced later developments in modern physics:

</doc>
<doc id="475008" url="https://en.wikipedia.org/wiki?curid=475008" title="Stiffness">
Stiffness

Stiffness is the rigidity of an object — the extent to which it resists deformation in response to an applied force. 
The complementary concept is flexibility or pliability: the more flexible an object is, the less stiff it is.
Calculations.
The stiffness, "k", of a body is a measure of the resistance offered by an elastic body to deformation. For an elastic body with a single degree of freedom (DOF) (for example, stretching or compression of a rod), the stiffness is defined as
where,
In the International System of Units, stiffness is typically measured in newtons per meter. 
In Imperial units, stiffness is typically measured in pounds(lbs) per inch.
Generally speaking, deflections (or motions) of an infinitesimal element (which is viewed as a point) in an elastic body can occur along multiple DOF (maximum of six DOF at a point). For example, a point on a horizontal beam can undergo both a vertical displacement and a rotation relative to its undeformed axis. When there are M degrees of freedom a M x M matrix must be used to describe the stiffness at the point. The diagonal terms in the matrix are the direct-related stiffnesses (or simply stiffnesses) along the same degree of freedom and the off-diagonal terms are the coupling stiffnesses between two different degrees of freedom (either at the same or different points) or the same degree of freedom at two different points. In industry, the term influence coefficient is sometimes used to refer to the coupling stiffness.
It is noted that for a body with multiple DOF, the equation above generally does not apply since the applied force generates not only the deflection along its own direction (or degree of freedom), but also those along other directions.
For a body with multiple DOF, in order to calculate a particular direct-related stiffness (the diagonal terms), the corresponding DOF is left free while the remaining should be constrained. Under such a condition, the above equation can be used to obtain the direct-related stiffness for the degree of freedom which is unconstrained. The ratios between the reaction forces (or moments) and the produced deflection are the coupling stiffnesses.
A description including all possible stretch and shear parameters is given by the elasticity tensor.
Compliance.
The inverse of stiffness is "compliance" (or sometimes "elastic modulus"), typically measured in units of metres per newton. In rheology it may be defined as the ratio of strain to stress, and so take the units of reciprocal stress, "e.g". 1/Pa.
Rotational stiffness.
A body may also have a rotational stiffness, "k", given by
where
In the SI system, rotational stiffness is typically measured in newton-metres per radian.
In the SAE system, rotational stiffness is typically measured in inch-pounds per degree.
Further measures of stiffness are derived on a similar basis, including:
Relationship to elasticity.
In general, elastic modulus is not the same as stiffness. Elastic modulus is a property of the constituent material; stiffness is a property of a structure. That is, the modulus is an intensive property of the material; stiffness, on the other hand, is an extensive property of the solid body dependent on the material "and" the shape and boundary conditions. For example, for an element in tension or compression, the axial stiffness is
where
Similarly, the rotational stiffness of a straight section is
where
Note that in SI, these units yield formula_5. For the special case of unconstrained uniaxial tension or compression, Young's modulus "can" be thought of as a measure of the stiffness of a material.
Applications.
The stiffness of a structure is of principal importance in many engineering applications, so the modulus of elasticity is often one of the primary properties considered when selecting a material. A high modulus of elasticity is sought when deflection is undesirable, while a low modulus of elasticity is required when flexibility is needed.
In biology, the stiffness of the extracellular matrix is important for guiding the migration of cells in a phenomenon called durotaxis.

</doc>
<doc id="217313" url="https://en.wikipedia.org/wiki?curid=217313" title="Stochastic matrix">
Stochastic matrix

In mathematics, a stochastic matrix (also termed probability matrix, transition matrix, substitution matrix, or Markov matrix) is a matrix used to describe the transitions of a Markov chain. Each of its entries is a nonnegative real number representing a probability. It has found use in probability theory, statistics, mathematical finance and linear algebra, as well as computer science and population genetics. 
There are several different definitions and types of stochastic matrices:
In the same vein, one may define stochastic vector (also called probability vector) as a vector whose elements are nonnegative real numbers which sum to 1. Thus, each row of a right stochastic matrix (or column of a left stochastic matrix) is a stochastic vector.
A common convention in English language mathematics literature is to use row vectors of probabilities and right stochastic matrices rather than column vectors of probabilities and left stochastic matrices; this article follows that convention.
Definition and properties.
A stochastic matrix describes a Markov chain formula_1 over a finite state space "S".
If the probability of moving from formula_2 to formula_3 in one time step is formula_4, the stochastic matrix "P" is given by using formula_5 as the formula_6 row and formula_7 column element, e.g.,
Since the total of transition probability from a state formula_2 to all other states must be 1, this matrix is a right stochastic matrix, so that
The product of two right stochastic matrices is also right stochastic. In particular, the formula_11-th power formula_12 of a right stochastic matrix formula_13 is also right stochastic. The probability of transitioning from formula_2 to formula_3 in two steps is then given by the formula_16 element of the square of formula_13:
In general the probability transition of going from any state to another state in a finite Markov chain given by the matrix formula_13 in "k" steps is given by formula_20.
An initial distribution is given as a row vector.
A stationary probability vector formula_21 is defined as a distribution, written as a row vector, that does not change under application of the transition matrix; that is, it is defined as a probability distribution on the set formula_22 which is also a row eigenvector of the probability matrix, associated with eigenvalue 1:
The right spectral radius of every right stochastic matrix is clearly at most 1. Additionally, every right stochastic matrix has an obvious column eigenvector associated to the eigenvalue 1: The vector formula_24, whose coordinates are all equal to 1. As left and right eigenvalues of a square matrix are the same, every stochastic matrix has, at least, a row eigenvector associated to the eigenvalue 1 and the largest absolute value of all its eigenvalues is also 1. Finally, the Brouwer Fixed Point Theorem (applied to the compact convex set of all probability distributions of the finite set formula_22) implies that there is some left eigenvector which is also a stationary probability vector. 
On the other hand, the Perron–Frobenius theorem also ensures that every irreducible stochastic matrix has such a stationary vector, and that the largest absolute value of an eigenvalue is always 1. However, this theorem cannot be applied directly to such matrices because they need not be irreducible. 
In general, there may be several such vectors. However, for a matrix with strictly positive entries (or, more generally, for an irreducible aperiodic stochastic matrix), this vector is unique and can be computed by observing that for any formula_2 we have the following limit,
where formula_28 is the formula_7 element of the row vector formula_21. Among other things, this says that the long-term probability of being in a state formula_3 is independent of the initial state formula_2. That both of these computations give the same stationary vector is a form of an ergodic theorem, which is generally true in a wide variety of dissipative dynamical systems: the system evolves, over time, to a stationary state. 
Intuitively, a stochastic matrix represents a Markov chain; the application of the stochastic matrix to a probability distribution redistributes the probability mass of the original distribution while preserving its total mass. If this process is applied repeatedly, the distribution converges to a stationary distribution for the Markov chain.
Example: the cat and mouse.
Suppose you have a timer and a row of five adjacent boxes, with a cat in the first box and a mouse in the fifth box at time zero. The cat and the mouse both jump to a random adjacent box when the timer advances. E.g. if the cat is in the second box and the mouse in the fourth one, the probability is one fourth that "the cat will be in the first box and the mouse in the fifth after the timer advances". If the cat is in the first box and the mouse in the fifth one, the probability is one that the cat will be in box two and the mouse will be in box four after the timer advances. The cat eats the mouse if both end up in the same box, at which time the game ends. The random variable "K" gives the number of time steps the mouse stays in the game.
The Markov chain that represents this game contains the following five states specified by the combination of positions (cat,mouse):
We use a stochastic matrix to represent the transition probabilities of this system (rows and columns in this matrix are indexed by the possible states listed above),
Long-term averages.
No matter what the initial state, the cat will eventually catch the mouse (with probability 1) and a stationary state π = (0,0,0,0,1) is approached as a limit. To compute the long-term average or expected value of a stochastic variable Y, for each state Sj and time tk there is a contribution of Yj,k·P(S=Sj,t=tk). Survival can be treated as a binary variable with Y=1 for a surviving state and Y=0 for the terminated state. The states with Y=0 do not contribute to the long-term average.
Phase-type representation.
As State 5 is an absorbing state, the distribution of time to absorption is discrete phase-type distributed. Suppose the system starts in state 2, represented by the vector formula_34. The states where the mouse has perished don't contribute to the survival average so state five can be ignored. The initial state and transition matrix can be reduced to,
and,
where formula_38 is the identity matrix, and formula_39 represents a column matrix of all ones that acts as a sum over states.
Since each state is occupied for one step of time the expected time of the mouse's survival is just the sum of the probability of occupation over all surviving states and steps in time,
Higher order moments are given by

</doc>
<doc id="105340" url="https://en.wikipedia.org/wiki?curid=105340" title="Strange quark">
Strange quark

The strange quark or s quark (from its symbol, "s") is the third-lightest of all quarks, a type of elementary particle. Strange quarks are found in subatomic particles called hadrons. Example of hadrons containing strange quarks include kaons (), strange D mesons (), Sigma baryons (), and other strange particles.
Along with the charm quark, it is part of the second generation of matter, and has an electric charge of − "e" and a bare mass of . Like all quarks, the strange quark is an elementary fermion with spin-, and experiences all four fundamental interactions: gravitation, electromagnetism, weak interactions, and strong interactions. The antiparticle of the strange quark is the strange antiquark (sometimes called "antistrange quark" or simply "antistrange"), which differs from it only in that some of its properties have equal magnitude but opposite sign.
The first strange particle (a particle containing a strange quark) was discovered in 1947 (kaons), but the existence of the strange quark itself (and that of the up and down quarks) was only postulated in 1964 by Murray Gell-Mann and George Zweig to explain the "Eightfold Way" classification scheme of hadrons. The first evidence for the existence of quarks came in 1968, in deep inelastic scattering experiments at the Stanford Linear Accelerator Center. These experiments confirmed the existence of up and down quarks, and by extension, strange quarks, as they were required to explain the Eightfold Way.
History.
In the beginnings of particle physics (first half of the 20th century), hadrons such as protons, neutron and pions were thought to be elementary particles. However, new hadrons were discovered, the 'particle zoo' grew from a few particles in the early 1930s and 1940s to several dozens of them in the 1950s. However some particles were much longer lived than others; most particles decayed through the strong interaction and had lifetimes of around 10−23 seconds. But when they decayed through the weak interactions, they had lifetimes of around 10−10 seconds to decay. While studying these decays Murray Gell-Mann (in 1953) and Kazuhiko Nishijima (in 1955) developed the concept of "strangeness" (which Nishijima called "eta-charge", after the eta meson ()) which explained the 'strangeness' of the longer-lived particles. The Gell-Mann–Nishijima formula is the result of these efforts to understand strange decays.
However, the relationships between each particles and the physical basis behind the strangeness property was still unclear. In 1961, Gell-Mann and Yuval Ne'eman (independently of each other) proposed a hadron classification scheme called the "Eightfold Way", or in more technical terms, SU(3) flavor symmetry. This ordered hadrons into isospin multiplets. The physical basis behind both isospin and strangeness was only explained in 1964, when Gell-Mann and George Zweig (independently of each other) proposed the quark model, then consisting only of up, down, and strange quarks. Up and down quarks were the carriers of isospin, while the strange quark carried strangeness. While the quark model explained the Eightfold Way, no direct evidence of the existence of quarks was found until 1968 at the Stanford Linear Accelerator Center. Deep inelastic scattering experiments indicated that protons had substructure, and that protons made of three more-fundamental particles explained the data (thus confirming the quark model).
At first people were reluctant to identify the three-bodies as quarks, instead preferring Richard Feynman's parton description, but over time the quark theory became accepted (see "November Revolution").

</doc>
<doc id="453626" url="https://en.wikipedia.org/wiki?curid=453626" title="Strangeness">
Strangeness

In particle physics, strangeness ("S") is a property of particles, expressed as a quantum number, for describing decay of particles in strong and electromagnetic reactions, which occur in a short period of time. The strangeness of a particle is defined as:
where "n" represents the number of strange quarks () and "n" represents the number of strange antiquarks ().
The terms "strange" and "strangeness" predate the discovery of the quark, and were adopted after its discovery in order to preserve the continuity of the phrase; strangeness of anti-particles being referred to as +1, and particles as −1 as per the original definition. For all the quark flavor quantum numbers (strangeness, charm, topness and bottomness) the convention is that the flavor charge and the electric charge of a quark have the same sign. With this, any flavor carried by a charged meson has the same sign as its charge.
Conservation.
Strangeness was introduced by Murray Gell-Mann and Kazuhiko Nishijima to explain the fact that certain particles, such as the kaons or certain hyperons, were created easily in particle collisions, yet decayed much more slowly than expected for their large masses and large production cross sections. Noting that collisions seemed to always produce pairs of these particles, it was postulated that a new conserved quantity, dubbed "strangeness", was preserved during their creation, but "not" conserved in their decay.
In our modern understanding, strangeness is conserved during the strong and the electromagnetic interactions, but not during the weak interactions. Consequently, the lightest particles containing a strange quark cannot decay by the strong interaction, and must instead decay via the much slower weak interaction. In most cases these decays change the value of the strangeness by one unit. However, this doesn't necessarily hold in second-order weak reactions, where there are mixes of and mesons. All in all, the amount of strangeness can change in a weak interaction reaction by +1, 0 or -1 (depending on the reaction).

</doc>
<doc id="28305" url="https://en.wikipedia.org/wiki?curid=28305" title="String theory">
String theory

In physics, string theory is a theoretical framework in which the point-like particles of particle physics are replaced by one-dimensional objects called strings. String theory describes how these strings propagate through space and interact with each other. On distance scales larger than the string scale, a string looks just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In string theory, one of the many vibrational states of the string corresponds to the graviton, a quantum mechanical particle that carries gravitational force. Thus string theory is a theory of quantum gravity.
String theory is a broad and varied subject that attempts to address a number of deep questions of fundamental physics. String theory has been applied to a variety of problems in black hole physics, early universe cosmology, nuclear physics, and condensed matter physics, and it has stimulated a number of major developments in pure mathematics. Because string theory potentially provides a unified description of gravity and particle physics, it is a candidate for a theory of everything, a self-contained mathematical model that describes all fundamental forces and forms of matter. Despite much work on these problems, it is not known to what extent string theory describes the real world or how much freedom the theory allows to choose the details.
String theory was first studied in the late 1960s as a theory of the strong nuclear force, before being abandoned in favor of quantum chromodynamics. Subsequently, it was realized that the very properties that made string theory unsuitable as a theory of nuclear physics made it a promising candidate for a quantum theory of gravity. The earliest version of string theory, bosonic string theory, incorporated only the class of particles known as bosons. It later developed into superstring theory, which posits a connection called supersymmetry between bosons and the class of particles called fermions. Five consistent versions of superstring theory were developed before it was conjectured in the mid-1990s that they were all different limiting cases of a single theory in eleven dimensions known as M-theory. In late 1997, theorists discovered an important relationship called the AdS/CFT correspondence, which relates string theory to another type of physical theory called a quantum field theory.
One of the challenges of string theory is that the full theory does not yet have a satisfactory definition in all circumstances. Another issue is that the theory is thought to describe an enormous landscape of possible universes, and this has complicated efforts to develop theories of particle physics based on string theory. These issues have led some in the community to criticize these approaches to physics and question the value of continued research on string theory unification.
Fundamentals.
In the twentieth century, two theoretical frameworks emerged for formulating the laws of physics. One of these frameworks was Albert Einstein's general theory of relativity, a theory that explains the force of gravity and the structure of space and time. The other was quantum mechanics, a radically different formalism for describing physical phenomena using probability. By the late 1970s, these two frameworks had proven to be sufficient to explain most of the observed features of the universe, from elementary particles to atoms to the evolution of stars and the universe as a whole.
In spite of these successes, there are still many problems that remain to be solved. One of the deepest problems in modern physics is the problem of quantum gravity. The general theory of relativity is formulated within the framework of classical physics, whereas the other fundamental forces are described within the framework of quantum mechanics. A quantum theory of gravity is needed in order to reconcile general relativity with the principles of quantum mechanics, but difficulties arise when one attempts to apply the usual prescriptions of quantum theory to the force of gravity. In addition to the problem of developing a consistent theory of quantum gravity, there are many other fundamental problems in the physics of atomic nuclei, black holes, and the early universe.
String theory is a theoretical framework that attempts to address these questions and many others. The starting point for string theory is the idea that the point-like particles of particle physics can also be modeled as one-dimensional objects called strings. String theory describes how strings propagate through space and interact with each other. In a given version of string theory, there is only one kind of string, which may look like a small loop or segment of ordinary string, and it can vibrate in different ways. On distance scales larger than the string scale, a string will look just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In this way, all of the different elementary particles may be viewed as vibrating strings. In string theory, one of the vibrational states of the string gives rise to the graviton, a quantum mechanical particle that carries gravitational force. Thus string theory is a theory of quantum gravity.
One of the main developments of the past several decades in string theory was the discovery of certain "dualities", mathematical transformations that identify one physical theory with another. Physicists studying string theory have discovered a number of these dualities between different versions of string theory, and this has led to the conjecture that all consistent versions of string theory are subsumed in a single framework known as M-theory.
Studies of string theory have also yielded a number of results on the nature of black holes and the gravitational interaction. There are certain paradoxes that arise when one attempts to understand the quantum aspects of black holes, and work on string theory has attempted to clarify these issues. In late 1997 this line of work culminated in the discovery of the anti-de Sitter/conformal field theory correspondence or AdS/CFT. This is a theoretical result which relates string theory to other physical theories which are better understood theoretically. The AdS/CFT correspondence has implications for the study of black holes and quantum gravity, and it has been applied to other subjects, including nuclear and condensed matter physics.
Since string theory incorporates all of the fundamental interactions, including gravity, many physicists hope that it fully describes our universe, making it a theory of everything. One of the goals of current research in string theory is to find a solution of the theory that reproduces the observed spectrum of elementary particles, with a small cosmological constant, containing dark matter and a plausible mechanism for cosmic inflation. While there has been progress toward these goals, it is not known to what extent string theory describes the real world or how much freedom the theory allows to choose the details.
One of the challenges of string theory is that the full theory does not yet have a satisfactory definition in all circumstances. The scattering of strings is most straightforwardly defined using the techniques of perturbation theory, but it is not known in general how to define string theory nonperturbatively. It is also not clear whether there is any principle by which string theory selects its vacuum state, the physical state that determines the properties of our universe. These problems have led some in the community to criticize these approaches to the unification of physics and question the value of continued research on these problems.
Strings.
The application of quantum mechanics to physical objects such as the electromagnetic field, which are extended in space and time, is known as quantum field theory. In particle physics, quantum field theories form the basis for our understanding of elementary particles, which are modeled as excitations in the fundamental fields.
In quantum field theory, one typically computes the probabilities of various physical events using the techniques of perturbation theory. Developed by Richard Feynman and others in the first half of the twentieth century, perturbative quantum field theory uses special diagrams called Feynman diagrams to organize computations. One imagines that these diagrams depict the paths of point-like particles and their interactions.
The starting point for string theory is the idea that the point-like particles of quantum field theory can also be modeled as one-dimensional objects called strings. The interaction of strings is most straightforwardly defined by generalizing the perturbation theory used in ordinary quantum field theory. At the level of Feynman diagrams, this means replacing the one-dimensional diagram representing the path of a point particle by a two-dimensional surface representing the motion of a string. Unlike in quantum field theory, string theory does not yet have a full non-perturbative definition, so many of the theoretical questions that physicists would like to answer remain out of reach.
In theories of particle physics based on string theory, the characteristic length scale of strings is assumed to be on the order of the Planck length, or meters, the scale at which the effects of quantum gravity are believed to become significant. On much larger length scales, such as the scales visible in physics laboratories, such objects would be indistinguishable from zero-dimensional point particles, and the vibrational state of the string would determine the type of particle. One of the vibrational states of a string corresponds to the graviton, a quantum mechanical particle that carries the gravitational force.
The original version of string theory was bosonic string theory, but this version described only bosons, a class of particles which transmit forces between the matter particles, or fermions. Bosonic string theory was eventually superseded by theories called superstring theories. These theories describe both bosons and fermions, and they incorporate a theoretical idea called supersymmetry. This is a mathematical relation that exists in certain physical theories between the bosons and fermions. In theories with supersymmetry, each boson has a counterpart which is a fermion, and vice versa.
There are several versions of superstring theory: type I, type IIA, type IIB, and two flavors of heterotic string theory ( and ). The different theories allow different types of strings, and the particles that arise at low energies exhibit different symmetries. For example, the type I theory includes both open strings (which are segments with endpoints) and closed strings (which form closed loops), while types IIA and IIB include only closed strings.
Extra dimensions.
In everyday life, there are three familiar dimensions of space: height, width and length. Einstein's general theory of relativity treats time as a dimension on par with the three spatial dimensions; in general relativity, space and time are not modeled as separate entities but are instead unified to a four-dimensional spacetime. In this framework, the phenomenon of gravity is viewed as a consequence of the geometry of spacetime.
In spite of the fact that the universe is well described by four-dimensional spacetime, there are several reasons why physicists consider theories in other dimensions. In some cases, by modeling spacetime in a different number of dimensions, a theory becomes more mathematically tractable, and one can perform calculations and gain general insights more easily. There are also situations where theories in two or three spacetime dimensions are useful for describing phenomena in condensed matter physics. Finally, there exist scenarios in which there could actually be more than four dimensions of spacetime which have nonetheless managed to escape detection.
One notable feature of string theories is that these theories require extra dimensions of spacetime for their mathematical consistency. In bosonic string theory, spacetime is 26-dimensional, while in superstring theory it is ten-dimensional. In order to describe real physical phenomena using string theory, one must therefore imagine scenarios in which these extra dimensions would not be observed in experiments.
Compactification is one way of modifying the number of dimensions in a physical theory. In compactification, some of the extra dimensions are assumed to "close up" on themselves to form circles. In the limit where these curled up dimensions become very small, one obtains a theory in which spacetime has effectively a lower number of dimensions. A standard analogy for this is to consider a multidimensional object such as a garden hose. If the hose is viewed from a sufficient distance, it appears to have only one dimension, its length. However, as one approaches the hose, one discovers that it contains a second dimension, its circumference. Thus, an ant crawling on the surface of the hose would move in two dimensions.
Compactification can be used to construct models in which spacetime is effectively four-dimensional. However, not every way of compactifying the extra dimensions produces a model with the right properties to describe nature. In a viable model of particle physics, the compact extra dimensions must be shaped like a Calabi–Yau manifold. A Calabi–Yau manifold is a special space which is typically taken to be six-dimensional in applications to string theory. It is named after mathematicians Eugenio Calabi and Shing-Tung Yau.
Another approach to reducing the number of dimensions is the so-called brane-world scenario. In this approach, physicists assume that the observable universe is a four-dimensional subspace of a higher dimensional space. In such models, the force-carrying bosons of particle physics arise from open strings with endpoints attached to the four-dimensional subspace, while gravity arises from closed strings propagating through the larger ambient space. This idea plays an important role in attempts to develop models of real world physics based on string theory, and it provides a natural explanation for the weakness of gravity compared to the other fundamental forces.
Dualities.
One notable fact about string theory is that the different versions of the theory all turn out to be related in highly nontrivial ways. One of the relationships that can exist between different string theories is called S-duality. This is a relationship which says that a collection of strongly interacting particles in one theory can, in some cases, be viewed as a collection of weakly interacting particles in a completely different theory. Roughly speaking, a collection of particles is said to be strongly interacting if they combine and decay often and weakly interacting if they do so infrequently. Type I string theory turns out to be equivalent by S-duality to the heterotic string theory. Similarly, type IIB string theory is related to itself in a nontrivial way by S-duality.
Another relationship between different string theories is T-duality. Here one considers strings propagating around a circular extra dimension. T-duality states that a string propagating around a circle of radius is equivalent to a string propagating around a circle of radius in the sense that all observable quantities in one description are identified with quantities in the dual description. For example, a string has momentum as it propagates around a circle, and it can also wind around the circle one or more times. The number of times the string winds around a circle is called the winding number. If a string has momentum and winding number in one description, it will have momentum and winding number in the dual description. For example, type IIA string theory is equivalent to type IIB string theory via T-duality, and the two versions of heterotic string theory are also related by T-duality.
In general, the term "duality" refers to a situation where two seemingly different physical systems turn out to be equivalent in a nontrivial way. Two theories related by a duality need not be string theories. For example, Montonen–Olive duality is example of an S-duality relationship between quantum field theories. The AdS/CFT correspondence is example of a duality which relates string theory to a quantum field theory. If two theories are related by a duality, it means that one theory can be transformed in some way so that it ends up looking just like the other theory. The two theories are then said to be "dual" to one another under the transformation. Put differently, the two theories are mathematically different descriptions of the same phenomena.
Branes.
In string theory and related theories, a brane is a physical object that generalizes the notion of a point particle to higher dimensions. For example, a point particle can be viewed as a brane of dimension zero, while a string can be viewed as a brane of dimension one. It is also possible to consider higher-dimensional branes. In dimension "p", these are called "p"-branes. The word brane comes from the word "membrane" which refers to a two-dimensional brane.
Branes are dynamical objects which can propagate through spacetime according to the rules of quantum mechanics. They have mass and can have other attributes such as charge. A "p"-brane sweeps out a ("p"+1)-dimensional volume in spacetime called its "worldvolume". Physicists often study fields analogous to the electromagnetic field which live on the worldvolume of a brane.
In string theory, D-branes are an important class of branes that arise when one considers open strings. As an open string propagates through spacetime, its endpoints are required to lie on a D-brane. The letter "D" in D-brane refers to a certain mathematical condition on the system known as the Dirichlet boundary condition. The study of D-branes in string theory has led to important results such as the AdS/CFT correspondence, which has shed light on many problems in quantum field theory.
Branes are also frequently studied from a purely mathematical point of view. Mathematically, branes can be described as objects of certain categories, such as the derived category of coherent sheaves on a complex algebraic variety, or the Fukaya category of a symplectic manifold. The connection between the physical notion of a brane and the mathematical notion of a category has led to important mathematical insights in the fields of algebraic and symplectic geometry and representation theory.
M-theory.
Prior to 1995, theorists believed that there were five consistent versions of superstring theory (type I, type IIA, type IIB, and two versions of heterotic string theory). This understanding changed in 1995 when Edward Witten suggested that the five theories were just special limiting cases of an eleven-dimensional theory called M-theory. Witten's conjecture was based on the work of a number of other physicists, including Ashoke Sen, Chris Hull, Paul Townsend, and Michael Duff. His announcement led to a flurry of research activity now known as the second superstring revolution.
Unification of superstring theories.
In the 1970s, many physicists became interested in supergravity theories, which combine general relativity with supersymmetry. Whereas general relativity makes sense in any number of dimensions, supergravity places an upper limit on the number of dimensions. In 1978, work by Werner Nahm showed that the maximum spacetime dimension in which one can formulate a consistent supersymmetric theory is eleven. In the same year, Eugene Cremmer, Bernard Julia, and Joel Scherk of the École Normale Supérieure showed that supergravity not only permits up to eleven dimensions but is in fact most elegant in this maximal number of dimensions.
Initially, many physicists hoped that by compactifying eleven-dimensional supergravity, it might be possible to construct realistic models of our four-dimensional world. The hope was that such models would provide a unified description of the four fundamental forces of nature: electromagnetism, the strong and weak nuclear forces, and gravity. Interest in eleven-dimensional supergravity soon waned as various flaws in this scheme were discovered. One of the problems was that the laws of physics appear to distinguish between clockwise and counterclockwise, a phenomenon known as chirality. Edward Witten and others observed this chirality property cannot be readily derived by compactifying from eleven dimensions.
In the first superstring revolution in 1984, many physicists turned to string theory as a unified theory of particle physics and quantum gravity. Unlike supergravity theory, string theory was able to accommodate the chirality of the standard model, and it provided a theory of gravity consistent with quantum effects. Another feature of string theory that many physicists were drawn to in the 1980s and 1990s was its high degree of uniqueness. In ordinary particle theories, one can consider any collection of elementary particles whose classical behavior is described by an arbitrary Lagrangian. In string theory, the possibilities are much more constrained: by the 1990s, physicists had argued that there were only five consistent supersymmetric versions of the theory.
Although there were only a handful of consistent superstring theories, it remained a mystery why there was not just one consistent formulation. However, as physicists began to examine string theory more closely, they realized that these theories are related in intricate and nontrivial ways. They found that a system of strongly interacting strings can, in some cases, be viewed as a system of weakly interacting strings. This phenomenon is known as S-duality. It was studied by Ashoke Sen in the context of heterotic strings in four dimensions and by Chris Hull and Paul Townsend in the context of the type IIB theory. Theorists also found that different string theories may be related by T-duality. This duality implies that strings propagating on completely different spacetime geometries may be physically equivalent.
At around the same time, as many physicists were studying the properties of strings, a small group of physicists was examining the possible applications of higher dimensional objects. In 1987, Eric Bergshoeff, Ergin Sezgin, and Paul Townsend showed that eleven-dimensional supergravity includes two-dimensional branes. Intuitively, these objects look like sheets or membranes propagating through the eleven-dimensional spacetime. Shortly after this discovery, Michael Duff, Paul Howe, Takeo Inami, and Kellogg Stelle considered a particular compactification of eleven-dimensional supergravity with one of the dimensions curled up into a circle. In this setting, one can imagine the membrane wrapping around the circular dimension. If the radius of the circle is sufficiently small, then this membrane looks just like a string in ten-dimensional spacetime. In fact, Duff and his collaborators showed that this construction reproduces exactly the strings appearing in type IIA superstring theory.
Speaking at a string theory conference in 1995, Edward Witten made the surprising suggestion that all five superstring theories were in fact just different limiting cases of a single theory in eleven spacetime dimensions. Witten's announcement drew together all of the previous results on S- and T-duality and the appearance of higher dimensional branes in string theory. In the months following Witten's announcement, hundreds of new papers appeared on the Internet confirming different parts of his proposal. Today this flurry of work is known as the second superstring revolution.
Initially, some physicists suggested that the new theory was a fundamental theory of membranes, but Witten was skeptical of the role of membranes in the theory. In a paper from 1996, Hořava and Witten wrote "As it has been proposed that the eleven-dimensional theory is a supermembrane theory but there are some reasons to doubt that interpretation, we will non-committally call it the M-theory, leaving to the future the relation of M to membranes." In the absence of an understanding of the true meaning and structure of M-theory, Witten has suggested that the "M" should stand for "magic", "mystery", or "membrane" according to taste, and the true meaning of the title should be decided when a more fundamental formulation of the theory is known.
Matrix theory.
In mathematics, a matrix is a rectangular array of numbers or other data. In physics, a matrix model is a particular kind of physical theory whose mathematical formulation involves the notion of a matrix in an important way. A matrix model describes the behavior of a set of matrices within the framework of quantum mechanics.
One important example of a matrix model is the BFSS matrix model proposed by Tom Banks, Willy Fischler, Stephen Shenker, and Leonard Susskind in 1997. This theory describes the behavior of a set of nine large matrices. In their original paper, these authors showed, among other things, that the low energy limit of this matrix model is described by eleven-dimensional supergravity. These calculations led them to propose that the BFSS matrix model is exactly equivalent to M-theory. The BFSS matrix model can therefore be used as a prototype for a correct formulation of M-theory and a tool for investigating the properties of M-theory in a relatively simple setting.
The development of the matrix model formulation of M-theory has led physicists to consider various connections between string theory and a branch of mathematics called noncommutative geometry. This subject is a generalization of ordinary geometry in which mathematicians define new geometric notions using tools from noncommutative algebra. In a paper from 1998, Alain Connes, Michael R. Douglas, and Albert Schwarz showed that some aspects of matrix models and M-theory are described by a noncommutative quantum field theory, a special kind of physical theory in which spacetime is described mathematically using noncommutative geometry. This established a link between matrix models and M-theory on the one hand, and noncommutative geometry on the other hand. It quickly led to the discovery of other important links between noncommutative geometry and various physical theories.
Black holes.
In general relativity, a black hole is defined as a region of spacetime in which the gravitational field is so strong that no particle or radiation can escape. In the currently accepted models of stellar evolution, black holes are thought to arise when massive stars undergo gravitational collapse, and many galaxies are thought to contain supermassive black holes at their centers. Black holes are also important for theoretical reasons, as they present profound challenges for theorists attempting to understand the quantum aspects of gravity. String theory has proved to be an important tool for investigating the theoretical properties of black holes because it provides a framework in which theorists can study their thermodynamics.
Bekenstein–Hawking formula.
In the branch of physics called statistical mechanics, entropy is a measure of the randomness or disorder of a physical system. This concept was studied in the 1870s by the Austrian physicist Ludwig Boltzmann, who showed that the thermodynamic properties of a gas could be derived from the combined properties of its many constituent molecules. Boltzmann argued that by averaging the behaviors of all the different molecules in a gas, one can understand macroscopic properties such as volume, temperature, and pressure. In addition, this perspective led him to give a precise definition of entropy as the natural logarithm of the number of different states of the molecules (also called "microstates") that give rise to the same macroscopic features.
In the twentieth century, physicists began to apply the same concepts to black holes. In most systems such as gases, the entropy scales with the volume. In the 1970s, the physicist Jacob Bekenstein suggested that the entropy of a black hole is instead proportional to the "surface area" of its event horizon, the boundary beyond which matter and radiation is lost to its gravitational attraction. When combined with ideas of the physicist Stephen Hawking, Bekenstein's work yielded a precise formula for the entropy of a black hole. The formula expresses the entropy as
where is the speed of light, is Boltzmann's constant, is the reduced Planck constant, is Newton's constant, and is the surface area of the event horizon.
Like any physical system, a black hole has an entropy defined in terms of the number of different microstates that lead to the same macroscopic features. The Bekenstein–Hawking entropy formula gives the expected value of the entropy of a black hole, but by the 1990s, physicists still lacked a derivation of this formula by counting microstates in a theory of quantum gravity. Finding such a derivation of this formula was considered an important test of the viability of any theory of quantum gravity such as string theory.
Derivation within string theory.
In a paper from 1996, Andrew Strominger and Cumrun Vafa showed how to derive the Beckenstein–Hawking formula for certain black holes in string theory. Their calculation was based on the observation that D-branes—which look like fluctuating membranes when they are weakly interacting—become dense, massive objects with event horizons when the interactions are strong. In other words, a system of strongly interacting D-branes in string theory is indistinguishable from a black hole. Strominger and Vafa analyzed such D-brane systems and calculated the number of different ways of placing D-branes in spacetime so that their combined mass and charge is equal to a given mass and charge for the resulting black hole. Their calculation reproduced the Bekenstein–Hawking formula exactly, including the factor of . Subsequent work by Strominger, Vafa, and others refined the original calculations and gave the precise values of the "quantum corrections" needed to describe very small black holes.
The black holes that Strominger and Vafa considered in their original work were quite different from real astrophysical black holes. One difference was that Strominger and Vafa considered only extremal black holes in order to make the calculation tractable. These are defined as black holes with the lowest possible mass compatible with a given charge. Strominger and Vafa also restricted attention to black holes in five-dimensional spacetime with unphysical supersymmetry.
Although it was originally developed in this very particular and physically unrealistic context in string theory, the entropy calculation of Strominger and Vafa has led to a qualitative understanding of how black hole entropy can be accounted for in any theory of quantum gravity. Indeed, in 1998, Strominger argued that the original result could be generalized to an arbitrary consistent theory of quantum gravity without relying on strings or supersymmetry. In collaboration with several other authors in 2010, he showed that some results on black hole entropy could be extended to non-extremal astrophysical black holes.
AdS/CFT correspondence.
One approach to formulating string theory and studying its properties is provided by the anti-de Sitter/conformal field theory (AdS/CFT) correspondence. This is a theoretical result which implies that string theory is in some cases equivalent to a quantum field theory. In addition to providing insights into the mathematical structure of string theory, the AdS/CFT correspondence has shed light on many aspects of quantum field theory in regimes where traditional calculational techniques are ineffective. The AdS/CFT correspondence was first proposed by Juan Maldacena in late 1997. Important aspects of the correspondence were elaborated in articles by Steven Gubser, Igor Klebanov, and Alexander Markovich Polyakov, and by Edward Witten. By 2010, Maldacena's article had over 7000 citations, becoming the most highly cited article in the field of high energy physics.
Overview of the correspondence.
In the AdS/CFT correspondence, the geometry of spacetime is described in terms of a certain vacuum solution of Einstein's equation called anti-de Sitter space. In very elementary terms, anti-de Sitter space is a mathematical model of spacetime in which the notion of distance between points (the metric) is different from the notion of distance in ordinary Euclidean geometry. It is closely related to hyperbolic space, which can be viewed as a disk as illustrated on the left. This image shows a tessellation of a disk by triangles and squares. One can define the distance between points of this disk in such a way that all the triangles and squares are the same size and the circular outer boundary is infinitely far from any point in the interior.
One can imagine a stack of hyperbolic disks where each disk represents the state of the universe at a given time. The resulting geometric object is three-dimensional anti-de Sitter space. It looks like a solid cylinder in which any cross section is a copy of the hyperbolic disk. Time runs along the vertical direction in this picture. The surface of this cylinder plays an important role in the AdS/CFT correspondence. As with the hyperbolic plane, anti-de Sitter space is curved in such a way that any point in the interior is actually infinitely far from this boundary surface.
This construction describes a hypothetical universe with only two space dimensions and one time dimension, but it can be generalized to any number of dimensions. Indeed, hyperbolic space can have more than two dimensions and one can "stack up" copies of hyperbolic space to get higher-dimensional models of anti-de Sitter space.
An important feature of anti-de Sitter space is its boundary (which looks like a cylinder in the case of three-dimensional anti-de Sitter space). One property of this boundary is that, within a small region on the surface around any given point, it looks just like Minkowski space, the model of spacetime used in nongravitational physics. One can therefore consider an auxiliary theory in which "spacetime" is given by the boundary of anti-de Sitter space. This observation is the starting point for AdS/CFT correspondence, which states that the boundary of anti-de Sitter space can be regarded as the "spacetime" for a quantum field theory. The claim is that this quantum field theory is equivalent to a gravitational theory, such as string theory, in the bulk anti-de Sitter space in the sense that there is a "dictionary" for translating entities and calculations in one theory into their counterparts in the other theory. For example, a single particle in the gravitational theory might correspond to some collection of particles in the boundary theory. In addition, the predictions in the two theories are quantitatively identical so that if two particles have a 40 percent chance of colliding in the gravitational theory, then the corresponding collections in the boundary theory would also have a 40 percent chance of colliding.
Applications to quantum gravity.
The discovery of the AdS/CFT correspondence was a major advance in physicists' understanding of string theory and quantum gravity. One reason for this is that the correspondence provides a formulation of string theory in terms of quantum field theory, which is well understood by comparison. Another reason is that it provides a general framework in which physicists can study and attempt to resolve the paradoxes of black holes.
In 1975, Stephen Hawking published a calculation which suggested that black holes are not completely black but emit a dim radiation due to quantum effects near the event horizon. At first, Hawking's result posed a problem for theorists because it suggested that black holes destroy information. More precisely, Hawking's calculation seemed to conflict with one of the basic postulates of quantum mechanics, which states that physical systems evolve in time according to the Schrödinger equation. This property is usually referred to as unitarity of time evolution. The apparent contradiction between Hawking's calculation and the unitarity postulate of quantum mechanics came to be known as the black hole information paradox.
The AdS/CFT correspondence resolves the black hole information paradox, at least to some extent, because it shows how a black hole can evolve in a manner consistent with quantum mechanics in some contexts. Indeed, one can consider black holes in the context of the AdS/CFT correspondence, and any such black hole corresponds to a configuration of particles on the boundary of anti-de Sitter space. These particles obey the usual rules of quantum mechanics and in particular evolve in a unitary fashion, so the black hole must also evolve in a unitary fashion, respecting the principles of quantum mechanics. In 2005, Hawking announced that the paradox had been settled in favor of information conservation by the AdS/CFT correspondence, and he suggested a concrete mechanism by which black holes might preserve information.
Applications to quantum field theory.
In addition to its applications to theoretical problems in quantum gravity, the AdS/CFT correspondence has been applied to a variety of problems in quantum field theory. One physical system that has been studied using the AdS/CFT correspondence is the quark–gluon plasma, an exotic state of matter produced in particle accelerators. This state of matter arises for brief instants when heavy ions such as gold or lead nuclei are collided at high energies. Such collisions cause the quarks that make up atomic nuclei to deconfine at temperatures of approximately two trillion kelvins, conditions similar to those present at around seconds after the Big Bang.
The physics of the quark–gluon plasma is governed by a theory called quantum chromodynamics, but this theory is mathematically intractable in problems involving the quark–gluon plasma. In an article appearing in 2005, Đàm Thanh Sơn and his collaborators showed that the AdS/CFT correspondence could be used to understand some aspects of the quark–gluon plasma by describing it in the language of string theory. By applying the AdS/CFT correspondence, Sơn and his collaborators were able to describe the quark gluon plasma in terms of black holes in five-dimensional spacetime. The calculation showed that the ratio of two quantities associated with the quark–gluon plasma, the shear viscosity and volume density of entropy, should be approximately equal to a certain universal constant. In 2008, the predicted value of this ratio for the quark–gluon plasma was confirmed at the Relativistic Heavy Ion Collider at Brookhaven National Laboratory.
The AdS/CFT correspondence has also been used to study aspects of condensed matter physics. Over the decades, experimental condensed matter physicists have discovered a number of exotic states of matter, including superconductors and superfluids. These states are described using the formalism of quantum field theory, but some phenomena are difficult to explain using standard field theoretic techniques. Some condensed matter theorists including Subir Sachdev hope that the AdS/CFT correspondence will make it possible to describe these systems in the language of string theory and learn more about their behavior.
So far some success has been achieved in using string theory methods to describe the transition of a superfluid to an insulator. A superfluid is a system of electrically neutral atoms that flows without any friction. Such systems are often produced in the laboratory using liquid helium, but recently experimentalists have developed new ways of producing artificial superfluids by pouring trillions of cold atoms into a lattice of criss-crossing lasers. These atoms initially behave as a superfluid, but as experimentalists increase the intensity of the lasers, they become less mobile and then suddenly transition to an insulating state. During the transition, the atoms behave in an unusual way. For example, the atoms slow to a halt at a rate that depends on the temperature and on Planck's constant, the fundamental parameter of quantum mechanics, which does not enter into the description of the other phases. This behavior has recently been understood by considering a dual description where properties of the fluid are described in terms of a higher dimensional black hole.
Phenomenology.
In addition to being an idea of considerable theoretical interest, string theory provides a framework for constructing models of real world physics that combine general relativity and particle physics. Phenomenology is the branch of theoretical physics in which physicists construct realistic models of nature from more abstract theoretical ideas. String phenomenology is the part of string theory that attempts to construct realistic models based on string theory.
Partly because of theoretical and mathematical difficulties and partly because of the extremely high energies needed to test these theories experimentally, there is so far no experimental evidence that would unambiguously point to any of these models being a correct fundamental description of nature. This has led some in the community to criticize these approaches to unification and question the value of continued research on these problems.
Particle physics.
The currently accepted theory describing elementary particles and their interactions is known as the standard model of particle physics. This theory provides a unified description of three of the fundamental forces of nature: electromagnetism and the strong and weak nuclear forces. Despite its remarkable success in explaining a wide range of physical phenomena, the standard model cannot be a complete description of reality. This is because the standard model fails to incorporate the force of gravity and because of problems such as the hierarchy problem and the inability to explain the structure of fermion masses or dark matter.
String theory has been used to construct a variety of models of particle physics going beyond the standard model. Typically, such models are based on the idea of compactification. Starting with the ten- or eleven-dimensional spacetime of string or M-theory, physicists postulate a shape for the extra dimensions. By choosing this shape appropriately, they can construct models roughly similar to the standard model of particle physics, together with additional undiscovered particles. One popular way of deriving realistic physics from string theory is to start with the heterotic theory in ten dimensions and assume that the six extra dimensions of spacetime are shaped like a six-dimensional Calabi–Yau manifold. Such compactifications offer many ways of extracting realistic physics from string theory. Other similar methods can be used to construct realistic models of our four-dimensional world based on M-theory.
Cosmology.
The Big Bang theory is the prevailing cosmological model for the universe from the earliest known periods through its subsequent large-scale evolution. Despite its success in explaining many observed features of the universe including galactic redshifts, the relative abundance of light elements such as hydrogen and helium, and the existence of a cosmic microwave background, there are several questions that remain unanswered. For example, the standard Big Bang model does not explain why the universe appears to be same in all directions, why it appears flat on very large distance scales, or why certain hypothesized particles such as magnetic monopoles are not observed in experiments.
Currently, the leading candidate for a theory going beyond the Big Bang is the theory of cosmic inflation. Developed by Alan Guth and others in the 1980s, inflation postulates a period of extremely rapid accelerated expansion of the universe prior to the expansion described by the standard Big Bang theory. The theory of cosmic inflation preserves the successes of the Big Bang while providing a natural explanation for some of the mysterious features of the universe. The theory has also received striking support from observations of the cosmic microwave background, the radiation that has filled the sky since around 380,000 years after the Big Bang.
In the theory of inflation, the rapid initial expansion of the universe is caused by a hypothetical particle called the inflaton. The exact properties of this particle are not fixed by the theory but should ultimately be derived from a more fundamental theory such as string theory. Indeed, there have been a number of attempts to identify an inflation within the spectrum of particles described by string theory and to study inflation using string theory. While these approaches might eventually find support in observational data such as measurements of the cosmic microwave background, the application of string theory to cosmology is still in its early stages.
Connections to mathematics.
In addition to influencing research in theoretical physics, string theory has stimulated a number of major developments in pure mathematics. Like many developing ideas in theoretical physics, string theory does not at present have a mathematically rigorous formulation in which all of its concepts can be defined precisely. As a result, physicists who study string theory are often guided by physical intuition to conjecture relationships between the seemingly different mathematical structures that are used to formalize different parts of the theory. These conjectures are later proved by mathematicians, and in this way, string theory serves as a source of new ideas in pure mathematics.
Mirror symmetry.
After Calabi–Yau manifolds had entered physics as a way to compactify extra dimensions in string theory, many physicists began studying these manifolds. In the late 1980s, several physicists noticed that given such a compactification of string theory, it is not possible to reconstruct uniquely a corresponding Calabi–Yau manifold. Instead, two different versions of string theory, type IIA and type IIB, can be compactified on completely different Calabi–Yau manifolds giving rise to the same physics. In this situation, the manifolds are called mirror manifolds, and the relationship between the two physical theories is called mirror symmetry.
Regardless of whether Calabi–Yau compactifications of string theory provide a correct description of nature, the existence of the mirror duality between different string theories has significant mathematical consequences. The Calabi–Yau manifolds used in string theory are of interest in pure mathematics, and mirror symmetry allows mathematicians to solve problems in enumerative geometry, a branch of mathematics concerned with counting the numbers of solutions to geometric questions.
Enumerative geometry studies a class of geometric objects called algebraic varieties which are defined by the vanishing of polynomials. For example, the Clebsch cubic illustrated on the right is an algebraic variety defined using a certain polynomial of degree three in four variables. A celebrated result of nineteenth-century mathematicians Arthur Cayley and George Salmon states that there are exactly 27 straight lines that lie entirely on such a surface.
Generalizing this problem, one can ask how many lines can be drawn on a quintic Calabi–Yau manifold, such as the one illustrated above, which is defined by a polynomial of degree five. This problem was solved by the nineteenth-century German mathematician Hermann Schubert, who found that there are exactly 2,875 such lines. In 1986, geometer Sheldon Katz proved that the number of curves, such as circles, that are defined by polynomials of degree two and lie entirely in the quintic is 609,250.
By the year 1991, most of the classical problems of enumerative geometry had been solved and interest in enumerative geometry had begun to diminish. The field was reinvigorated in May 1991 when physicists Philip Candelas, Xenia de la Ossa, Paul Green, and Linda Parks showed that mirror symmetry could be used to translate difficult mathematical questions about one Calabi–Yau manifold into easier questions about its mirror. In particular, they used mirror symmetry to show that a six-dimensional Calabi–Yau manifold can contain exactly 317,206,375 curves of degree three. In addition to counting degree-three curves, Candelas and his collaborators obtained a number of more general results for counting rational curves which went far beyond the results obtained by mathematicians.
Originally, these results of Candelas were justified on physical grounds. However, mathematicians generally prefer rigorous proofs that do not require an appeal to physical intuition. Inspired by physicists' work on mirror symmetry, mathematicians have therefore constructed their own arguments proving the enumerative predictions of mirror symmetry. Today mirror symmetry is an active area of research in mathematics, and mathematicians are working to develop a more complete mathematical understanding of mirror symmetry based on physicists' intuition. Major approaches to mirror symmetry include the homological mirror symmetry program of Maxim Kontsevich and the SYZ conjecture of Andrew Strominger, Shing-Tung Yau, and Eric Zaslow.
Monstrous moonshine.
Group theory is the branch of mathematics that studies the concept of symmetry. For example, one can consider a geometric shape such as an equilateral triangle. There are various operations that one can perform on this triangle without changing its shape. One can rotate it through 120°, 240°, or 360°, or one can reflect in any of the lines labeled , , or in the picture. Each of these operations is called a "symmetry", and the collection of these symmetries satisfies certain technical properties making it into what mathematicians call a group. In this particular example, the group is known as the dihedral group of order 6 because it has six elements. A general group may describe finitely many or infinitely many symmetries; if there are only finitely many symmetries, it is called a finite group.
Mathematicians often strive for a classification (or list) of all mathematical objects of a given type. It is generally believed that finite groups are too diverse to admit a useful classification. A more modest but still challenging problem is to classify all finite "simple" groups. These are finite groups which may be used as building blocks for constructing arbitrary finite groups in the same way that prime numbers can be used to construct arbitrary whole numbers by taking products. One of the major achievements of contemporary group theory is the classification of finite simple groups, a mathematical theorem which provides a list of all possible finite simple groups.
This classification theorem identifies several infinite families of groups as well as 26 additional groups which do not fit into any family. The latter groups are called the "sporadic" groups, and each one owes its existence to a remarkable combination of circumstances. The largest sporadic group, the so-called monster group, has over elements, more than a thousand times the number of atoms in the Earth.
A seemingly unrelated construction is the -function of number theory. This object belongs to a special class of functions called modular functions, whose graphs form a certain kind of repeating pattern. Although this function appears in a branch of mathematics which seems very different from the theory of finite groups, the two subjects turn out to be intimately related. In the late 1970s, mathematicians John McKay and John Thompson noticed that certain numbers arising in the analysis of the monster group (namely, the dimensions of its irreducible representations) are related to numbers that appear in a formula for the -function (namely, the coefficients of its Fourier series). This relationship was further developed by John Horton Conway and Simon Norton who called it monstrous moonshine because it seemed so far fetched.
In 1992, Richard Borcherds constructed a bridge between the theory of modular functions and finite groups and, in the process, explained the observations of McKay and Thompson. Borcherds' work used ideas from string theory in an essential way, extending earlier results of Igor Frenkel, James Lepowsky, and Arne Meurman, who had realized the monster group as the symmetries of a particular version of string theory. In 1998, Borcherds was awarded the Fields medal for his work.
Since the 1990s, the connection between string theory and moonshine has led to further results in mathematics and physics. In 2010, physicists Tohru Eguchi, Hirosi Ooguri, and Yuji Tachikawa discovered connections between a different sporadic group, the Mathieu group, and a certain version of string theory. Miranda Cheng, John Duncan, and Jeffrey A. Harvey proposed a generalization of this moonshine phenomenon called umbral moonshine, and their conjecture was proved mathematically by Duncan, Michael Griffin, and Ken Ono. Witten has also speculated that the version of string theory appearing in monstrous moonshine might be related to a certain simplified model of gravity in three spacetime dimensions.
History.
Early results.
Some of the structures reintroduced by string theory arose for the first time much earlier as part of the program of classical unification started by Albert Einstein. The first person to add a fifth dimension to a theory of gravity was Gunnar Nordström in 1914, who noted that gravity in five dimensions describes both gravity and electromagnetism in four. Nordström attempted to unify electromagnetism with his theory of gravitation, which was however superseded by Einstein's general relativity in 1919. Thereafter, German mathematician Theodor Kaluza combined the fifth dimension with general relativity, and only Kaluza is usually credited with the idea. In 1926, the Swedish physicist Oskar Klein gave a physical interpretation of the unobservable extra dimension—it is wrapped into a small circle. Einstein introduced a non-symmetric metric tensor, while much later Brans and Dicke added a scalar component to gravity. These ideas would be revived within string theory, where they are demanded by consistency conditions.
String theory was originally developed during the late 1960s and early 1970s as a never completely successful theory of hadrons, the subatomic particles like the proton and neutron that feel the strong interaction. In the 1960s, Geoffrey Chew and Steven Frautschi discovered that the mesons make families called Regge trajectories with masses related to spins in a way that was later understood by Yoichiro Nambu, Holger Bech Nielsen and Leonard Susskind to be the relationship expected from rotating strings. Chew advocated making a theory for the interactions of these trajectories that did not presume that they were composed of any fundamental particles, but would construct their interactions from self-consistency conditions on the S-matrix. The S-matrix approach was started by Werner Heisenberg in the 1940s as a way of constructing a theory that did not rely on the local notions of space and time, which Heisenberg believed break down at the nuclear scale. While the scale was off by many orders of magnitude, the approach he advocated was ideally suited for a theory of quantum gravity.
Working with experimental data, R. Dolen, D. Horn and C. Schmid developed some sum rules for hadron exchange. When a particle and antiparticle scatter, virtual particles can be exchanged in two qualitatively different ways. In the s-channel, the two particles annihilate to make temporary intermediate states that fall apart into the final state particles. In the t-channel, the particles exchange intermediate states by emission and absorption. In field theory, the two contributions add together, one giving a continuous background contribution, the other giving peaks at certain energies. In the data, it was clear that the peaks were stealing from the background—the authors interpreted this as saying that the t-channel contribution was dual to the s-channel one, meaning both described the whole amplitude and included the other.
The result was widely advertised by Murray Gell-Mann, leading Gabriele Veneziano to construct a scattering amplitude that had the property of Dolen-Horn-Schmid duality, later renamed world-sheet duality. The amplitude needed poles where the particles appear, on straight line trajectories, and there is a special mathematical function whose poles are evenly spaced on half the real line— the Gamma function— which was widely used in Regge theory. By manipulating combinations of Gamma functions, Veneziano was able to find a consistent scattering amplitude with poles on straight lines, with mostly positive residues, which obeyed duality and had the appropriate Regge scaling at high energy. The amplitude could fit near-beam scattering data as well as other Regge type fits, and had a suggestive integral representation that could be used for generalization.
Over the next years, hundreds of physicists worked to complete the bootstrap program for this model, with many surprises. Veneziano himself discovered that for the scattering amplitude to describe the scattering of a particle that appears in the theory, an obvious self-consistency condition, the lightest particle must be a tachyon. Miguel Virasoro and Joel Shapiro found a different amplitude now understood to be that of closed strings, while Ziro Koba and Holger Nielsen generalized Veneziano's integral representation to multiparticle scattering. Veneziano and Sergio Fubini introduced an operator formalism for computing the scattering amplitudes that was a forerunner of world-sheet conformal theory, while Virasoro understood how to remove the poles with wrong-sign residues using a constraint on the states. Claud Lovelace calculated a loop amplitude, and noted that there is an inconsistency unless the dimension of the theory is 26. Charles Thorn, Peter Goddard and Richard Brower went on to prove that there are no wrong-sign propagating states in dimensions less than or equal to 26.
In 1969, Yoichiro Nambu, Holger Bech Nielsen, and Leonard Susskind recognized that the theory could be given a description in space and time in terms of strings. The scattering amplitudes were derived systematically from the action principle by Peter Goddard, Jeffrey Goldstone, Claudio Rebbi, and Charles Thorn, giving a space-time picture to the vertex operators introduced by Veneziano and Fubini and a geometrical interpretation to the Virasoro conditions.
In 1970, Pierre Ramond added fermions to the model, which led him to formulate a two-dimensional supersymmetry to cancel the wrong-sign states. John Schwarz and André Neveu added another sector to the fermi theory a short time later. In the fermion theories, the critical dimension was 10. Stanley Mandelstam formulated a world sheet conformal theory for both the bose and fermi case, giving a two-dimensional field theoretic path-integral to generate the operator formalism. Michio Kaku and Keiji Kikkawa gave a different formulation of the bosonic string, as a string field theory, with infinitely many particle types and with fields taking values not on points, but on loops and curves.
In 1974, Tamiaki Yoneya discovered that all the known string theories included a massless spin-two particle that obeyed the correct Ward identities to be a graviton. John Schwarz and Joel Scherk came to the same conclusion and made the bold leap to suggest that string theory was a theory of gravity, not a theory of hadrons. They reintroduced Kaluza–Klein theory as a way of making sense of the extra dimensions. At the same time, quantum chromodynamics was recognized as the correct theory of hadrons, shifting the attention of physicists and apparently leaving the bootstrap program in the dustbin of history.
String theory eventually made it out of the dustbin, but for the following decade all work on the theory was completely ignored. Still, the theory continued to develop at a steady pace thanks to the work of a handful of devotees. Ferdinando Gliozzi, Joel Scherk, and David Olive realized in 1976 that the original Ramond and Neveu Schwarz-strings were separately inconsistent and needed to be combined. The resulting theory did not have a tachyon, and was proven to have space-time supersymmetry by John Schwarz and Michael Green in 1981. The same year, Alexander Polyakov gave the theory a modern path integral formulation, and went on to develop conformal field theory extensively. In 1979, Daniel Friedan showed that the equations of motions of string theory, which are generalizations of the Einstein equations of General Relativity, emerge from the Renormalization group equations for the two-dimensional field theory. Schwarz and Green discovered T-duality, and constructed two superstring theories—IIA and IIB related by T-duality, and type I theories with open strings. The consistency conditions had been so strong, that the entire theory was nearly uniquely determined, with only a few discrete choices.
First superstring revolution.
In the early 1980s, Edward Witten discovered that most theories of quantum gravity could not accommodate chiral fermions like the neutrino. This led him, in collaboration with Luis Alvarez-Gaumé to study violations of the conservation laws in gravity theories with anomalies, concluding that type I string theories were inconsistent. Green and Schwarz discovered a contribution to the anomaly that Witten and Alvarez-Gaumé had missed, which restricted the gauge group of the type I string theory to be SO(32). In coming to understand this calculation, Edward Witten became convinced that string theory was truly a consistent theory of gravity, and he became a high-profile advocate. Following Witten's lead, between 1984 and 1986, hundreds of physicists started to work in this field, and this is sometimes called the first superstring revolution.
During this period, David Gross, Jeffrey Harvey, Emil Martinec, and Ryan Rohm discovered heterotic strings. The gauge group of these closed strings was two copies of E8, and either copy could easily and naturally include the standard model. Philip Candelas, Gary Horowitz, Andrew Strominger and Edward Witten found that the Calabi–Yau manifolds are the compactifications that preserve a realistic amount of supersymmetry, while Lance Dixon and others worked out the physical properties of orbifolds, distinctive geometrical singularities allowed in string theory. Cumrun Vafa generalized T-duality from circles to arbitrary manifolds, creating the mathematical field of mirror symmetry. Daniel Friedan, Emil Martinec and Stephen Shenker further developed the covariant quantization of the superstring using conformal field theory techniques. David Gross and Vipul Periwal discovered that string perturbation theory was divergent. Stephen Shenker showed it diverged much faster than in field theory suggesting that new non-perturbative objects were missing.
In the 1990s, Joseph Polchinski discovered that the theory requires higher-dimensional objects, called D-branes and identified these with the black-hole solutions of supergravity. These were understood to be the new objects suggested by the perturbative divergences, and they opened up a new field with rich mathematical structure. It quickly became clear that D-branes and other p-branes, not just strings, formed the matter content of the string theories, and the physical interpretation of the strings and branes was revealed—they are a type of black hole. Leonard Susskind had incorporated the holographic principle of Gerardus 't Hooft into string theory, identifying the long highly excited string states with ordinary thermal black hole states. As suggested by 't Hooft, the fluctuations of the black hole horizon, the world-sheet or world-volume theory, describes not only the degrees of freedom of the black hole, but all nearby objects too.
Second superstring revolution.
In 1995, at the annual conference of string theorists at the University of Southern California (USC), Edward Witten gave a speech on string theory that in essence united the five string theories that existed at the time, and giving birth to a new 11-dimensional theory called M-theory. M-theory was also foreshadowed in the work of Paul Townsend at approximately the same time. The flurry of activity that began at this time is sometimes called the second superstring revolution.
During this period, Tom Banks, Willy Fischler, Stephen Shenker and Leonard Susskind formulated matrix theory, a full holographic description of M-theory using IIA D0 branes. This was the first definition of string theory that was fully non-perturbative and a concrete mathematical realization of the holographic principle. It is an example of a gauge-gravity duality and is now understood to be a special case of the AdS/CFT correspondence. Andrew Strominger and Cumrun Vafa calculated the entropy of certain configurations of D-branes and found agreement with the semi-classical answer for extreme charged black holes. Petr Hořava and Witten found the eleven-dimensional formulation of the heterotic string theories, showing that orbifolds solve the chirality problem. Witten noted that the effective description of the physics of D-branes at low energies is by a supersymmetric gauge theory, and found geometrical interpretations of mathematical structures in gauge theory that he and Nathan Seiberg had earlier discovered in terms of the location of the branes.
In 1997, Juan Maldacena noted that the low energy excitations of a theory near a black hole consist of objects close to the horizon, which for extreme charged black holes looks like an anti-de Sitter space. He noted that in this limit the gauge theory describes the string excitations near the branes. So he hypothesized that string theory on a near-horizon extreme-charged black-hole geometry, an anti-deSitter space times a sphere with flux, is equally well described by the low-energy limiting gauge theory, the N = 4 supersymmetric Yang–Mills theory. This hypothesis, which is called the AdS/CFT correspondence, was further developed by Steven Gubser, Igor Klebanov and Alexander Polyakov, and by Edward Witten, and it is now well-accepted. It is a concrete realization of the holographic principle, which has far-reaching implications for black holes, locality and information in physics, as well as the nature of the gravitational interaction. Through this relationship, string theory has been shown to be related to gauge theories like quantum chromodynamics and this has led to more quantitative understanding of the behavior of hadrons, bringing string theory back to its roots.
Criticism.
Number of solutions.
To construct models of particle physics based on string theory, physicists typically begin by specifying a shape for the extra dimensions of spacetime. Each of these different shapes corresponds to a different possible universe, or "vacuum state", with a different collection of particles and forces. String theory as it is currently understood has an enormous number of vacuum states, typically estimated to be around , and these might be sufficiently diverse to accommodate almost any phenomena that might be observed at low energies.
Many critics of string theory have expressed concerns about the large number of possible universes described by string theory. In his book "Not Even Wrong", Peter Woit, a lecturer in the mathematics department at Columbia University, has argued that the large number of different physical scenarios renders string theory vacuous as a framework for constructing models of particle physics. According to Woit,
Some physicists believe this large number of solutions is actually a virtue because it may allow a natural anthropic explanation of the observed values of physical constants, in particular the small value of the cosmological constant. The anthropic principle is the idea that some of the numbers appearing in the laws of physics are not fixed by any fundamental principle but must be compatible with the evolution of intelligent life. In 1987, Steven Weinberg published an article in which he argued that the cosmological constant could not have been too large, or else galaxies and intelligent life would not have been able to develop. Weinberg suggested that there might be a huge number of possible consistent universes, each with a different value of the cosmological constant, and observations indicate a small value of the cosmological constant only because humans happen to live in a universe that has allowed intelligent life, and hence observers, to exist.
String theorist Leonard Susskind has argued that string theory provides a natural anthropic explanation of the small value of the cosmological constant. According to Susskind, the different vacuum states of string theory might be realized as different universes within a larger multiverse. The fact that the observed universe has a small cosmological constant is just a tautological consequence of the fact that a small value is required for life to exist. Many prominent theorists and critics have disagreed with Susskind's conclusions. According to Woit, "in this case [anthropic reasoning] is nothing more than an excuse for failure. Speculative scientific ideas fail not just when they make incorrect predictions, but also when they turn out to be vacuous and incapable of predicting anything."
Background independence.
One of the fundamental principles of Einstein's general theory of relativity is the idea that the laws of physics should be background independent. This means that the geometry of spacetime is not specified from the outset but is instead determined dynamically by the theory. In general relativity, the geometry of spacetime can evolve in time, responding to whatever matter is present.
One of the older criticisms of string theory is that it is not manifestly background independent. In string theory, one must typically specify a fixed reference geometry for spacetime, and all other possible geometries are described as perturbations of this fixed one. In his book "The Trouble With Physics", physicist Lee Smolin of the Perimeter Institute for Theoretical Physics claims that this is the principal weakness of string theory as a theory of quantum gravity, saying that string theory has failed to incorporate this important insight from general relativity.
Others have disagreed with Smolin's characterization of string theory. In a review of Smolin's book, string theorist Joseph Polchinski writes
Polchinski notes that an important open problem in quantum gravity is to develop holographic descriptions of gravity which do not require the gravitational field to be asymptotically anti-de Sitter.
Sociological issues.
Since the superstring revolutions of the 1980s and 1990s, string theory has become the dominant paradigm of high energy theoretical physics. Some string theorists have expressed the view that there does not exist an equally successful alternative theory addressing the deep questions of fundamental physics. In an interview from 1987, Nobel laureate David Gross made the following controversial comments about the reasons for the popularity of string theory:
Several other high profile theorists and commentators have expressed similar views, suggesting that there are no viable alternatives to string theory.
Many critics of string theory have commented on this state of affairs. In his book criticizing string theory, Peter Woit views the status of string theory research as unhealthy and detrimental to the future of fundamental physics. He argues that the extreme popularity of string theory among theoretical physicists is partly a consequence of the financial structure of academia and the fierce competition for scarce resources. In his book "The Road to Reality", mathematical physicist Roger Penrose expresses similar views, stating "The often frantic competitiveness that this ease of communication engenders leads to 'bandwagon' effects, where researchers fear to be left behind if they do not join in." Penrose also claims that the technical difficulty of modern physics forces young scientists to rely on the preferences of established researchers, rather than forging new paths of their own. Lee Smolin expresses a slightly different position in his critique, claiming that string theory grew out of a tradition of particle physics which discourages speculation about the foundations of physics, while his preferred approach, loop quantum gravity, encourages more radical thinking. According to Smolin,
Smolin goes on to offer a number of prescriptions for how scientists might encourage a greater diversity of approaches to quantum gravity research.

</doc>
<doc id="27984" url="https://en.wikipedia.org/wiki?curid=27984" title="Strong interaction">
Strong interaction

In particle physics, the strong interaction is the mechanism responsible for the strong nuclear force (also called the strong force, nuclear strong force or color force), one of the four known fundamental interactions of nature, the others being electromagnetism, the weak interaction and gravitation. Despite only operating at a distance of a femtometer, it is the strongest force, being approximately 100 times stronger than electromagnetism, a million times stronger than weak interaction and 1038 times stronger than gravitation at that range. It ensures the stability of ordinary matter, confining quarks into hadron particles, such as the proton and neutron, the largest components of the mass of ordinary matter. Furthermore, most of the mass-energy of a common proton or neutron is in the form of the strong force field energy; the individual quarks provide only about 1% of the mass-energy of a proton.
The strong interaction is observable in two areas: on a larger scale (about 1 to 3 femtometers (fm)), it is the force that binds protons and neutrons (nucleons) together to form the nucleus of an atom. On the smaller scale (less than about 0.8 fm, the radius of a nucleon), it is the force (carried by gluons) that holds quarks together to form protons, neutrons, and other hadron particles. The strong force inherently has such a high strength that hadrons bound by the strong force can produce new massive particles. Thus, if hadrons are struck by high-energy particles, they give rise to new hadrons instead of emitting freely moving radiation (gluons). This property of the strong force is called color confinement, and it prevents the free "emission" of the strong force: instead, in practice, jets of massive particles are observed.
In the context of binding protons and neutrons together to form atomic nuclei, the strong interaction is called the nuclear force (or "residual strong force"). In this case, it is the residuum of the strong interaction between the quarks that make up the protons and neutrons. As such, the residual strong interaction obeys a quite different distance-dependent behavior between nucleons, from when it is acting to bind quarks within nucleons. The binding energy that is partly released on the breakup of a nucleus is related to the residual strong force and is harnessed in nuclear power and fission-type nuclear weapons.
The strong interaction is hypothesized to be mediated by massless particles called gluons, that are exchanged between quarks, antiquarks, and other gluons. Gluons, in turn, are thought to interact with quarks and gluons as all carry a type of charge called color charge. Color charge is analogous to electromagnetic charge, but it comes in three types rather than one (+/- red, +/- green, +/- blue) that results in a different type of force, with different rules of behavior. These rules are detailed in the theory of quantum chromodynamics (QCD), which is the theory of quark-gluon interactions.
After the Big Bang, during the electroweak epoch, the electroweak force separated from the strong force. Although it is expected that a Grand Unified Theory exists to describe this, no such theory has been successfully formulated, and the unification remains an unsolved problem in physics.
History.
Before the 1970s, physicists were uncertain about the binding mechanism of the atomic nucleus. It was known that the nucleus was composed of protons and neutrons and that protons possessed positive electric charge, while neutrons were electrically neutral. However, these facts seemed to contradict one another. By physical understanding at that time, positive charges would repel one another and the nucleus should therefore fly apart. However, this was never observed. New physics was needed to explain this phenomenon.
A stronger attractive force was postulated to explain how the atomic nucleus was bound together despite the protons' mutual electromagnetic repulsion. This hypothesized force was called the "strong force", which was believed to be a fundamental force that acted on the protons and neutrons that make up the nucleus.
It was later discovered that protons and neutrons were not fundamental particles, but were made up of constituent particles called quarks. The strong attraction between nucleons was the side-effect of a more fundamental force that bound the quarks together in the protons and neutrons. The theory of quantum chromodynamics explains that quarks carry what is called a color charge, although it has no relation to visible color. Quarks with unlike color charge attract one another as a result of the strong interaction, which is mediated by particles called gluons.
Details.
The word "strong" is used since the strong interaction is the "strongest" of the four fundamental forces; its strength is around 102 times that of the electromagnetic force, some 106 times as great as that of the weak force, and about 1038 times that of gravitation, at a distance of a femtometer or less.
Behaviour of the strong force.
The contemporary understanding of strong force is described by quantum chromodynamics (QCD), a part of the standard model of particle physics. Mathematically, QCD is a non-Abelian gauge theory based on a local (gauge) symmetry group called SU(3).
Quarks and gluons are the only fundamental particles that carry non-vanishing color charge, and hence participate in strong interactions. The strong force itself acts directly only on elementary quark and gluon particles.
All quarks and gluons in QCD interact with each other through the strong force. The strength of interaction is parametrized by the strong coupling constant. This strength is modified by the gauge color charge of the particle, a group theoretical property.
The strong force acts between quarks. Unlike all other forces (electromagnetic, weak, and gravitational), the strong force does not diminish in strength with increasing distance. After a limiting distance (about the size of a hadron) has been reached, it remains at a strength of about 10,000 newtons, no matter how much farther the distance between the quarks. In QCD, this phenomenon is called color confinement; it implies that only hadrons, not individual free quarks, can be observed. The explanation is that the amount of work done against a force of 10,000 newtons (about the weight of a one-metric ton mass on the surface of the Earth) is enough to create particle-antiparticle pairs within a very short distance of an interaction. In simple terms, the very energy applied to pull two quarks apart will create a pair of new quarks that will pair up with the original ones. The failure of all experiments that have searched for free quarks is considered to be evidence for this phenomenon.
The elementary quark and gluon particles affected are unobservable directly, but they instead emerge as jets of newly created hadrons, whenever energy is deposited into a quark-quark bond, as when a quark in a proton is struck by a very fast quark (in an impacting proton) during a particle accelerator experiment. However, quark–gluon plasmas have been observed.
Every quark in the universe does not attract every other quark in the above distance independent manner, since color-confinement implies that the strong force acts without distance-diminishment only between pairs of single quarks, and that in collections of bound quarks (i.e., hadrons), the net color-charge of the quarks cancels out, as seen from far away. Collections of quarks (hadrons) therefore appear (nearly) without color-charge, and the strong force is therefore nearly absent between these hadrons (i.e., between baryons or mesons). However, the cancellation is not quite perfect. A small residual force remains (described below) known as the residual strong force. This residual force "does" diminish rapidly with distance, and is thus very short-range (effectively a few femtometers). It manifests as a force between the "colorless" hadrons, and is therefore sometimes known as the strong nuclear force or simply nuclear force.
Residual strong force.
The residual effect of the strong force is called the nuclear force. The nuclear force acts between hadrons, such as mesons or the nucleons in atomic nuclei. This "residual strong force", acting indirectly, transmits gluons that form part of the virtual pi and rho mesons, which, in turn, transmit the nuclear force between nucleons.
The residual strong force is thus a minor residuum of the strong force that binds quarks together into protons and neutrons. This same force is much weaker "between" neutrons and protons, because it is mostly neutralized "within" them, in the same way that electromagnetic forces between neutral atoms (van der Waals forces) are much weaker than the electromagnetic forces that hold the atoms internally together.
Unlike the strong force itself, the nuclear force, or residual strong force, "does" diminish in strength, and in fact diminishes rapidly with distance. The decrease is approximately as a negative exponential power of distance, though there is no simple expression known for this; see Yukawa potential. This fact, together with the less-rapid decrease of the disruptive electromagnetic force between protons with distance, causes the instability of larger atomic nuclei, such as all those with atomic numbers larger than 82 (the element lead).

</doc>
<doc id="212490" url="https://en.wikipedia.org/wiki?curid=212490" title="Subatomic particle">
Subatomic particle

In the physical sciences, subatomic particles are particles much smaller than atoms. There are two types of subatomic particles: elementary particles, which according to current theories are not made of other particles; and "composite" particles. Particle physics and nuclear physics study these particles and how they interact.
In particle physics, the concept of a particle is one of several concepts inherited from classical physics. But it also reflects the modern understanding that at the quantum scale matter and energy behave very differently from what much of everyday experience would lead us to expect.
The idea of a particle underwent serious rethinking when experiments showed that light could behave like a stream of particles (called photons) as well as exhibit wave-like properties. This led to the new concept of wave–particle duality to reflect that quantum-scale "particles" behave like both particles and waves (also known as wavicles). Another new concept, the uncertainty principle, states that some of their properties taken together, such as their simultaneous position and momentum, cannot be measured exactly. In more recent times, wave–particle duality has been shown to apply not only to photons but to increasingly massive particles as well.
Interactions of particles in the framework of quantum field theory are understood as creation and annihilation of "quanta" of corresponding fundamental interactions. This blends particle physics with field theory.
Classification.
By statistics.
Any subatomic particle, like any particle in the 3-dimensional space that obeys laws of quantum mechanics, can be either a boson (an integer spin) or a fermion (a half-integer spin).
By composition.
The elementary particles of the Standard Model include:
Various extensions of the Standard Model predict the existence of an elementary graviton particle and many other elementary particles.
Composite subatomic particles (such as protons or atomic nuclei) are bound states of two or more elementary particles. For example, a proton is made of two up quarks and one down quark, while the atomic nucleus of helium-4 is composed of two protons and two neutrons. The neutron is made of two down quarks and one up quark. Composite particles include all hadrons: these include baryons (such as protons and neutrons) and mesons (such as pions and kaons).
By mass.
In special relativity, the energy of a particle at rest equals its mass times the speed of light squared (formula_1). That is, mass can be expressed in terms of energy and vice versa. If a particle has a frame of reference where it lies at rest, then it has a positive rest mass and is referred to as "massive".
All composite particles are massive. Baryons (meaning "heavy") tend to have greater mass than mesons (meaning "intermediate"), which in turn tend to be heavier than leptons (meaning "lightweight"), but the heaviest lepton (the tau particle) is heavier than the two lightest flavours of baryons (nucleons). It is also certain that any particle with an electric charge is massive.
All massless particles (particles whose invariant mass is zero) are elementary. These include the photon and gluon, although the latter cannot be isolated.
Other properties.
Through the work of Albert Einstein, Louis de Broglie, and many others, current scientific theory holds that "all" particles also have a wave nature. This has been verified not only for elementary particles but also for compound particles like atoms and even molecules. In fact, according to traditional formulations of non-relativistic quantum mechanics, wave–particle duality applies to all objects, even macroscopic ones; although the wave properties of macroscopic objects cannot be detected due to their small wavelengths.
Interactions between particles have been scrutinized for many centuries, and a few simple laws underpin how particles behave in collisions and interactions. The most fundamental of these are the laws of conservation of energy and conservation of momentum, which let us make calculations of particle interactions on scales of magnitude that range from stars to quarks. These are the prerequisite basics of Newtonian mechanics, a series of statements and equations in "Philosophiae Naturalis Principia Mathematica", originally published in 1687.
Dividing an atom.
The negatively charged electron has a mass equal to of that of a hydrogen atom. The remainder of the hydrogen atom's mass comes from the positively charged proton. The atomic number of an element is the number of protons in its nucleus. Neutrons are neutral particles having a mass slightly greater than that of the proton. Different isotopes of the same element contain the same number of protons but differing numbers of neutrons. The mass number of an isotope is the total number of nucleons (neutrons and protons collectively).
Chemistry concerns itself with how electron sharing binds atoms into structures such as crystals and molecules. Nuclear physics deals with how protons and neutrons arrange themselves in nuclei. The study of subatomic particles, atoms and molecules, and their structure and interactions, requires quantum mechanics. Analyzing processes that change the numbers and types of particles requires quantum field theory. The study of subatomic particles "per se" is called particle physics. The term "high-energy physics" is nearly synonymous to "particle physics" since creation of particles requires high energies: it occurs only as a result of cosmic rays, or in particle accelerators. Particle phenomenology systematizes the knowledge about subatomic particles obtained from these experiments.
History.
The term ""subatomic" particle" is largely a retronym of 1960s made to distinguish a big number of baryons and mesons (that comprise hadrons) from particles that are now thought to be truly elementary. Before that hadrons were usually classified as "elementary" because their composition was unknown.
A list of important discoveries follows:

</doc>
<doc id="1758807" url="https://en.wikipedia.org/wiki?curid=1758807" title="Superatom">
Superatom

A superatom is any cluster of atoms that seem to exhibit some of the properties of elemental atoms.
Sodium atoms, when cooled from vapor, naturally condense into clusters, preferentially containing a magic number of atoms (2, 8, 20, 40, 58, etc.). The first two of these can be recognized as the numbers of electrons needed to fill the first and second shells, respectively. The superatom suggestion is that free electrons in the cluster occupy a new set of orbitals that are defined by the entire group of atoms, i.e. cluster, rather than each individual atom separately (non-spherical or doped clusters show deviations in the number of electrons that form a closed shell as the potential is defined by the shape of the positive nuclei.) Superatoms tend to behave chemically in a way that will allow them to have a closed shell of electrons, in this new counting scheme. Therefore, a superatom with one more electron than a full shell should give up that electron very easily, similar to an alkali metal, and a cluster with one electron short of full shell should have a large electron affinity, such as a halogen.
Aluminium clusters.
Certain aluminium clusters have superatom properties. These aluminium clusters are generated as anions (Al"n"− with ) in helium gas and reacted with a gas containing iodine. When analyzed by mass spectrometry one main reaction product turns out to be Al13I−.<ref name="bergeron/2004">"Formation of Al13I−: Evidence for the Superhalogen Character of Al13" D. E. Bergeron, A.W. Castleman Jr., T. Morisato, S. N. Khanna Science, Vol 304, Issue 5667, 84–87 , 2 April 2004 Abstract MS spectra</ref> These clusters of 13 aluminium atoms with an extra electron added do not appear to react with oxygen when it is introduced in the same gas stream. Assuming each atom liberates its 3 valence electrons, this means that there are 40 electrons present, which is one of the magic numbers noted above for sodium, and implies that these numbers are a reflection of the noble gases. Calculations show that the additional electron is located in the aluminium cluster at the location directly opposite from the iodine atom. The cluster must therefore have a higher electron affinity for the electron than iodine and therefore the aluminium cluster is called a superhalogen. The cluster component in Al13I− ion is similar to an iodide ion or better still a bromide ion. The related Al13I2− cluster is expected to behave chemically like the triiodide ion.
Similarly it has been noted that Al14 clusters with 42 electrons (2 more than the magic numbers) appear to exhibit the properties of an alkaline earth metal which typically adopt +2 valence states. This is only known to occur when there are at least 3 iodine atoms attached to an Al14− cluster, Al14I3−. The anionic cluster has a total of 43 itinerant electrons, but the three iodine atoms each remove one of the itinerant electrons to leave 40 electrons in the jellium shell.<ref name="bergeron/2005">"Al Cluster Superatoms as Halogens in Polyhalides and as Alkaline Earths in Iodide Salts" D. E. Bergeron, P. J. Roach, A.W. Castleman Jr., N.O. Jones, S. N. Khanna Science, Vol 307, Issue 5707, 231–235 , 14 January 2005 Abstract MS spectrum</ref>
It is particularly easy and reliable to study atomic clusters of inert gas atoms by computer simulation because interaction between two atoms can be approximated very well by the Lennard-Jones potential. Other methods are readily available and it has been established that the magic numbers are 13, 19, 23, 26, 29, 32, 34, 43, 46, 49, 55, etc.
Superatom complexes.
Superatom complexes are a special group of superatoms that incorporate a metal core which is stabilized by organic ligands. In thiolate-protected gold cluster complexes a simple electron counting rule can be used to determine the total number of electrons () which correspond to a magic number via,
where is the number of metal atoms (A) in the core, is the atomic valence, is the number of electron withdrawing ligands, and is the overall charge on the complex. For example the Au102(p-MBA)44 has 58 electrons and corresponds to a closed shell magic number.

</doc>
<doc id="26884" url="https://en.wikipedia.org/wiki?curid=26884" title="Superconductivity">
Superconductivity

Superconductivity is a phenomenon of exactly zero electrical resistance and expulsion of magnetic fields occurring in certain materials when cooled below a characteristic critical temperature. It was discovered by Dutch physicist Heike Kamerlingh Onnes on April 8, 1911 in Leiden. Like ferromagnetism and atomic spectral lines, superconductivity is a quantum mechanical phenomenon. It is characterized by the Meissner effect, the complete ejection of magnetic field lines from the interior of the superconductor as it transitions into the superconducting state. The occurrence of the Meissner effect indicates that superconductivity cannot be understood simply as the idealization of "perfect conductivity" in classical physics.
The electrical resistivity of a metallic conductor decreases gradually as temperature is lowered. In ordinary conductors, such as copper or silver, this decrease is limited by impurities and other defects. Even near absolute zero, a real sample of a normal conductor shows some resistance. In a superconductor, the resistance drops abruptly to zero when the material is cooled below its critical temperature. An electric current flowing through a loop of superconducting wire can persist indefinitely with no power source.
In 1986, it was discovered that some cuprate-perovskite ceramic materials have a critical temperature above . Such a high transition temperature is theoretically impossible for a conventional superconductor, leading the materials to be termed high-temperature superconductors. Liquid nitrogen boils at 77 K, and superconduction at higher temperatures than this facilitates many experiments and applications that are less practical at lower temperatures.
Classification.
There are many criteria by which superconductors are classified. The most common are:
Elementary properties of superconductors.
Most of the physical properties of superconductors vary from material to material, such as the heat capacity and the critical temperature, critical field, and critical current density at which superconductivity is destroyed.
On the other hand, there is a class of properties that are independent of the underlying material. For instance, all superconductors have "exactly" zero resistivity to low applied currents when there is no magnetic field present or if the applied field does not exceed a critical value. The existence of these "universal" properties implies that superconductivity is a thermodynamic phase, and thus possesses certain distinguishing properties which are largely independent of microscopic details.
Zero electrical DC resistance.
The simplest method to measure the electrical resistance of a sample of some material is to place it in an electrical circuit in series with a current source "I" and measure the resulting voltage "V" across the sample. The resistance of the sample is given by Ohm's law as "R = V / I". If the voltage is zero, this means that the resistance is zero.
Superconductors are also able to maintain a current with no applied voltage whatsoever, a property exploited in superconducting electromagnets such as those found in MRI machines. Experiments have demonstrated that currents in superconducting coils can persist for years without any measurable degradation. Experimental evidence points to a current lifetime of at least 100,000 years. Theoretical estimates for the lifetime of a persistent current can exceed the estimated lifetime of the universe, depending on the wire geometry and the temperature.
In a normal conductor, an electric current may be visualized as a fluid of electrons moving across a heavy ionic lattice. The electrons are constantly colliding with the ions in the lattice, and during each collision some of the energy carried by the current is absorbed by the lattice and converted into heat, which is essentially the vibrational kinetic energy of the lattice ions. As a result, the energy carried by the current is constantly being dissipated. This is the phenomenon of electrical resistance.
The situation is different in a superconductor. In a conventional superconductor, the electronic fluid cannot be resolved into individual electrons. Instead, it consists of bound "pairs" of electrons known as Cooper pairs. This pairing is caused by an attractive force between electrons from the exchange of phonons. Due to quantum mechanics, the energy spectrum of this Cooper pair fluid possesses an "energy gap", meaning there is a minimum amount of energy Δ"E" that must be supplied in order to excite the fluid. Therefore, if Δ"E" is larger than the thermal energy of the lattice, given by "kT", where "k" is Boltzmann's constant and "T" is the temperature, the fluid will not be scattered by the lattice. The Cooper pair fluid is thus a superfluid, meaning it can flow without energy dissipation.
In a class of superconductors known as type II superconductors, including all known high-temperature superconductors, an extremely small amount of resistivity appears at temperatures not too far below the nominal superconducting transition when an electric current is applied in conjunction with a strong magnetic field, which may be caused by the electric current. This is due to the motion of magnetic vortices in the electronic superfluid, which dissipates some of the energy carried by the current. If the current is sufficiently small, the vortices are stationary, and the resistivity vanishes. The resistance due to this effect is tiny compared with that of non-superconducting materials, but must be taken into account in sensitive experiments. However, as the temperature decreases far enough below the nominal superconducting transition, these vortices can become frozen into a disordered but stationary phase known as a "vortex glass". Below this vortex glass transition temperature, the resistance of the material becomes truly zero.
Superconducting phase transition.
In superconducting materials, the characteristics of superconductivity appear when the temperature "T" is lowered below a critical temperature "Tc". The value of this critical temperature varies from material to material. Conventional superconductors usually have critical temperatures ranging from around 20 K to less than 1 K. Solid mercury, for example, has a critical temperature of 4.2 K. , the highest critical temperature found for a conventional superconductor is 39 K for magnesium diboride (MgB2), although this material displays enough exotic properties that there is some doubt about classifying it as a "conventional" superconductor. Cuprate superconductors can have much higher critical temperatures: YBa2Cu3O7, one of the first cuprate superconductors to be discovered, has a critical temperature of 92 K, and mercury-based cuprates have been found with critical temperatures in excess of 130 K. The explanation for these high critical temperatures remains unknown. Electron pairing due to phonon exchanges explains superconductivity in conventional superconductors, but it does not explain superconductivity in the newer superconductors that have a very high critical temperature.
Similarly, at a fixed temperature below the critical temperature, superconducting materials cease to superconduct when an external magnetic field is applied which is greater than the "critical magnetic field". This is because the Gibbs free energy of the superconducting phase increases quadratically with the magnetic field while the free energy of the normal phase is roughly independent of the magnetic field. If the material superconducts in the absence of a field, then the superconducting phase free energy is lower than that of the normal phase and so for some finite value of the magnetic field (proportional to the square root of the difference of the free energies at zero magnetic field) the two free energies will be equal and a phase transition to the normal phase will occur. More generally, a higher temperature and a stronger magnetic field lead to a smaller fraction of the electrons in the superconducting band and consequently a longer London penetration depth of external magnetic fields and currents. The penetration depth becomes infinite at the phase transition.
The onset of superconductivity is accompanied by abrupt changes in various physical properties, which is the hallmark of a phase transition. For example, the electronic heat capacity is proportional to the temperature in the normal (non-superconducting) regime. At the superconducting transition, it suffers a discontinuous jump and thereafter ceases to be linear. At low temperatures, it varies instead as "e"−α/"T" for some constant, α. This exponential behavior is one of the pieces of evidence for the existence of the energy gap.
The order of the superconducting phase transition was long a matter of debate. Experiments indicate that the transition is second-order, meaning there is no latent heat. However, in the presence of an external magnetic field there is latent heat, because the superconducting phase has a lower entropy below the critical temperature than the normal phase. It has been experimentally demonstrated that, as a consequence, when the magnetic field is increased beyond the critical field, the resulting phase transition leads to a decrease in the temperature of the superconducting material.
Calculations in the 1970s suggested that it may actually be weakly first-order due to the effect of long-range fluctuations in the electromagnetic field. In the 1980s it was shown theoretically with the help of a disorder field theory, in which the vortex lines of the superconductor play a major role, that the transition is of second order within the type II regime and of first order (i.e., latent heat) within the type I regime, and that the two regions are separated by a tricritical point. The results were strongly supported by Monte Carlo computer simulations.
Meissner effect.
When a superconductor is placed in a weak external magnetic field H, and cooled below its transition temperature, the magnetic field is ejected. The Meissner effect does not cause the field to be completely ejected but instead the field penetrates the superconductor but only to a very small distance, characterized by a parameter "λ", called the London penetration depth, decaying exponentially to zero within the bulk of the material. The Meissner effect is a defining characteristic of superconductivity. For most superconductors, the London penetration depth is on the order of 100 nm.
The Meissner effect is sometimes confused with the kind of diamagnetism one would expect in a perfect electrical conductor: according to Lenz's law, when a "changing" magnetic field is applied to a conductor, it will induce an electric current in the conductor that creates an opposing magnetic field. In a perfect conductor, an arbitrarily large current can be induced, and the resulting magnetic field exactly cancels the applied field.
The Meissner effect is distinct from this—it is the spontaneous expulsion which occurs during transition to superconductivity. Suppose we have a material in its normal state, containing a constant internal magnetic field. When the material is cooled below the critical temperature, we would observe the abrupt expulsion of the internal magnetic field, which we would not expect based on Lenz's law.
The Meissner effect was given a phenomenological explanation by the brothers Fritz and Heinz London, who showed that the electromagnetic free energy in a superconductor is minimized provided
where H is the magnetic field and λ is the London penetration depth.
This equation, which is known as the London equation, predicts that the magnetic field in a superconductor decays exponentially from whatever value it possesses at the surface.
A superconductor with little or no magnetic field within it is said to be in the Meissner state. The Meissner state breaks down when the applied magnetic field is too large. Superconductors can be divided into two classes according to how this breakdown occurs. In Type I superconductors, superconductivity is abruptly destroyed when the strength of the applied field rises above a critical value "Hc". Depending on the geometry of the sample, one may obtain an intermediate state consisting of a baroque pattern of regions of normal material carrying a magnetic field mixed with regions of superconducting material containing no field. In Type II superconductors, raising the applied field past a critical value "H""c"1 leads to a mixed state (also known as the vortex state) in which an increasing amount of magnetic flux penetrates the material, but there remains no resistance to the flow of electric current as long as the current is not too large. At a second critical field strength "H""c"2, superconductivity is destroyed. The mixed state is actually caused by vortices in the electronic superfluid, sometimes called fluxons because the flux carried by these vortices is quantized. Most pure elemental superconductors, except niobium and carbon nanotubes, are Type I, while almost all impure and compound superconductors are Type II.
London moment.
Conversely, a spinning superconductor generates a magnetic field, precisely aligned with the spin axis. The effect, the London moment, was put to good use in Gravity Probe B. This experiment measured the magnetic fields of four superconducting gyroscopes to determine their spin axes. This was critical to the experiment since it is one of the few ways to accurately determine the spin axis of an otherwise featureless sphere.
History of superconductivity.
Superconductivity was discovered on April 8, 1911 by Heike Kamerlingh Onnes, who was studying the resistance of solid mercury at cryogenic temperatures using the recently produced liquid helium as a refrigerant. At the temperature of 4.2 K, he observed that the resistance abruptly disappeared. In the same experiment, he also observed the superfluid transition of helium at 2.2 K, without recognizing its significance. The precise date and circumstances of the discovery were only reconstructed a century later, when Onnes's notebook was found. In subsequent decades, superconductivity was observed in several other materials. In 1913, lead was found to superconduct at 7 K, and in 1941 niobium nitride was found to superconduct at 16 K.
Great efforts have been devoted to finding out how and why superconductivity works; the important step occurred in 1933, when Meissner and Ochsenfeld discovered that superconductors expelled applied magnetic fields, a phenomenon which has come to be known as the Meissner effect. In 1935, Fritz and Heinz London showed that the Meissner effect was a consequence of the minimization of the electromagnetic free energy carried by superconducting current.
London theory.
The first phenomenological theory of superconductivity was London theory. It was put forward by the brothers Fritz and Heinz London in 1935, shortly after the discovery that magnetic fields are expelled from superconductors. A major triumph of the equations of this theory is their ability to explain the Meissner effect, wherein a material exponentially expels all internal magnetic fields as it crosses the superconducting threshold. By using the London equation, one can obtain the dependence of the magnetic field inside the superconductor on the distance to the surface.
There are two London equations: 
The first equation follows from Newton's second law for superconducting electrons.
Conventional theories (1950s).
During the 1950s, theoretical condensed matter physicists arrived at a solid understanding of "conventional" superconductivity, through a pair of remarkable and important theories: the phenomenological Ginzburg-Landau theory (1950) and the microscopic BCS theory (1957).
In 1950, the phenomenological Ginzburg-Landau theory of superconductivity was devised by Landau and Ginzburg. This theory, which combined Landau's theory of second-order phase transitions with a Schrödinger-like wave equation, had great success in explaining the macroscopic properties of superconductors. In particular, Abrikosov showed that Ginzburg-Landau theory predicts the division of superconductors into the two categories now referred to as Type I and Type II. Abrikosov and Ginzburg were awarded the 2003 Nobel Prize for their work (Landau had received the 1962 Nobel Prize for other work, and died in 1968). The four-dimensional extension of the Ginzburg-Landau theory, the Coleman-Weinberg model, is important in quantum field theory and cosmology.
Also in 1950, Maxwell and Reynolds "et al." found that the critical temperature of a superconductor depends on the isotopic mass of the constituent element. This important discovery pointed to the electron-phonon interaction as the microscopic mechanism responsible for superconductivity.
The complete microscopic theory of superconductivity was finally proposed in 1957 by Bardeen, Cooper and Schrieffer. This BCS theory explained the superconducting current as a superfluid of Cooper pairs, pairs of electrons interacting through the exchange of phonons. For this work, the authors were awarded the Nobel Prize in 1972.
The BCS theory was set on a firmer footing in 1958, when N. N. Bogolyubov showed that the BCS wavefunction, which had originally been derived from a variational argument, could be obtained using a canonical transformation of the electronic Hamiltonian. In 1959, Lev Gor'kov showed that the BCS theory reduced to the Ginzburg-Landau theory close to the critical temperature.
Generalizations of BCS theory for conventional superconductors form the basis for understanding of the phenomenon of superfluidity, because they fall into the lambda transition universality class. The extent to which such generalizations can be applied to unconventional superconductors is still controversial.
Further history.
The first practical application of superconductivity was developed in 1954 with Dudley Allen Buck's invention of the cryotron. Two superconductors with greatly different values of critical magnetic field are combined to produce a fast, simple, switch for computer elements.
Soon after discovering superconductivity in 1911, Kamerlingh Onnes attempted to make an electromagnet with superconducting windings but found that relatively low magnetic fields destroyed superconductivity in the materials he investigated. Much later, in 1955, G.B. Yntema succeeded in constructing a small 0.7-tesla iron-core electromagnet with superconducting niobium wire windings. Then, in 1961, J.E. Kunzler, E. Buehler, F.S.L. Hsu, and J.H. Wernick made the startling discovery that, at 4.2 degrees kelvin, a compound consisting of three parts niobium and one part tin, was capable of supporting a current density of more than 100,000 amperes per square centimeter in a magnetic field of 8.8 tesla. Despite being brittle and difficult to fabricate, niobium-tin has since proved extremely useful in supermagnets generating magnetic fields as high as 20 tesla. In 1962 T.G. Berlincourt and R.R. Hake discovered that alloys of niobium and titanium are suitable for applications up to 10 tesla.
Promptly thereafter, commercial production of niobium-titanium supermagnet wire commenced at Westinghouse Electric Corporation and at Wah Chang Corporation. Although niobium-titanium boasts less-impressive superconducting properties than those of niobium-tin, niobium-titanium has, nevertheless, become the most widely-used “workhorse” supermagnet material, in large measure a consequence of its very-high ductility and ease of fabrication. However, both niobium-tin and niobium-titanium find wide application in MRI medical imagers, bending and focusing magnets for enormous high-energy-particle accelerators, and a host of other applications. Conectus, a European superconductivity consortium, estimated that in 2014, global economic activity for which superconductivity was indispensable amounted to about five billion euros, with MRI systems accounting for about 80% of that total.
In 1962, Josephson made the important theoretical prediction that a supercurrent can flow between two pieces of superconductor separated by a thin layer of insulator. This phenomenon, now called the Josephson effect, is exploited by superconducting devices such as SQUIDs. It is used in the most accurate available measurements of the magnetic flux quantum "Φ"0 = "h"/(2"e"), where "h" is the Planck constant. Coupled with the quantum Hall resistivity, this leads to a precise measurement of the Planck constant. Josephson was awarded the Nobel Prize for this work in 1973.
In 2008, it was proposed that the same mechanism that produces superconductivity could produce a superinsulator state in some materials, with almost infinite electrical resistance.
High-temperature superconductivity.
Until 1986, physicists had believed that BCS theory forbade superconductivity at temperatures above about 30 K. In that year, Bednorz and Müller discovered superconductivity in a lanthanum-based cuprate perovskite material, which had a transition temperature of 35 K (Nobel Prize in Physics, 1987). It was soon found that replacing the lanthanum with yttrium (i.e., making YBCO) raised the critical temperature to 92 K.
This temperature jump is particularly significant, since it allows liquid nitrogen as a refrigerant, replacing liquid helium.
This can be important commercially because liquid nitrogen can be produced relatively cheaply, even on-site. Also, the higher temperatures help avoid some of the problems that arise at liquid helium temperatures, such as the formation of plugs of frozen air that can block cryogenic lines and cause unanticipated and potentially hazardous pressure buildup.
Many other cuprate superconductors have since been discovered, and the theory of superconductivity in these materials is one of the major outstanding challenges of theoretical condensed matter physics.
There are currently two main hypotheses – the resonating-valence-bond theory, and spin fluctuation which has the most support in the research community. The second hypothesis proposed that electron pairing in high-temperature superconductors is mediated by short-range spin waves known as paramagnons.
Since about 1993, the highest temperature superconductor was a ceramic material consisting of mercury, barium, calcium, copper and oxygen (HgBa2Ca2Cu3O8+δ) with "T"c = 133–138 K. The latter experiment (138 K) still awaits experimental confirmation, however.
In February 2008, an iron-based family of high-temperature superconductors was discovered. Hideo Hosono, of the Tokyo Institute of Technology, and colleagues found lanthanum oxygen fluorine iron arsenide (LaO1-xFxFeAs), an oxypnictide that superconducts below 26 K. Replacing the lanthanum in LaO1−"x"F"x"FeAs with samarium leads to superconductors that work at 55 K.
In May 2014, hydrogen sulfide () was predicted to be a high-temperature superconductor with a transition temperate of 80 at 160 gigapascals. In 2015, has been observed to exhibit superconductivity at below 203 K but at extremely high pressures — around 150 gigapascals.
Applications.
Superconducting magnets are some of the most powerful electromagnets known. They are used in MRI/NMR machines, mass spectrometers, and the beam-steering magnets used in particle accelerators. They can also be used for magnetic separation, where weakly magnetic particles are extracted from a background of less or non-magnetic particles, as in the pigment industries.
In the 1950s and 1960s, superconductors were used to build experimental digital computers using cryotron switches. More recently, superconductors have been used to make digital circuits based on rapid single flux quantum technology and RF and microwave filters for mobile phone base stations.
Superconductors are used to build Josephson junctions which are the building blocks of SQUIDs (superconducting quantum interference devices), the most sensitive magnetometers known. SQUIDs are used in scanning SQUID microscopes and magnetoencephalography. Series of Josephson devices are used to realize the SI volt. Depending on the particular mode of operation, a superconductor-insulator-superconductor Josephson junction can be used as a photon detector or as a mixer. The large resistance change at the transition from the normal- to the superconducting state is used to build thermometers in cryogenic micro-calorimeter photon detectors. The same effect is used in ultrasensitive bolometers made from superconducting materials.
Other early markets are arising where the relative efficiency, size and weight advantages of devices based on high-temperature superconductivity outweigh the additional costs involved.
Promising future applications include high-performance smart grid, electric power transmission, transformers, power storage devices, electric motors (e.g. for vehicle propulsion, as in vactrains or maglev trains), magnetic levitation devices, fault current limiters, enhancing spintronic devices with superconducting materials, and superconducting magnetic refrigeration. However, superconductivity is sensitive to moving magnetic fields so applications that use alternating current (e.g. transformers) will be more difficult to develop than those that rely upon direct current.

</doc>
<doc id="432632" url="https://en.wikipedia.org/wiki?curid=432632" title="Supergravity">
Supergravity

In theoretical physics, supergravity (supergravity theory; SUGRA for short) is a field theory that combines the principles of supersymmetry and general relativity. Together, these imply that, in supergravity, the supersymmetry is a local symmetry (in contrast to non-gravitational supersymmetric theories, such as the Minimal Supersymmetric Standard Model). Since the generators of supersymmetry (SUSY) are convoluted with the Poincaré group to form a super-Poincaré algebra, it can be seen that supergravity follows naturally from supersymmetry. All traditional literature on supergravity is generally written in terms of Cartan connections.
Gravitons.
Like any field theory of gravity, a supergravity theory contains a spin-2 field whose quantum is the graviton. Supersymmetry requires the graviton field to have a superpartner. This field has spin 3/2 and its quantum is the gravitino. The number of gravitino fields is equal to the number of supersymmetries.
History.
Gauge supersymmetry.
The first theory of local supersymmetry was proposed in 1975 by Dick Arnowitt and Pran Nath and was called gauge supersymmetry.
SUGRA.
SUGRA, or supergravity, was discovered in 1976 by Dan Freedman, Sergio Ferrara and Peter van Nieuwenhuizen, but was quickly generalized to many different theories in various numbers of dimensions and additional (N) supersymmetry charges. Supergravity theories with N>1 are usually referred to as extended supergravity (SUEGRA). Some supergravity theories were shown to be equivalent to certain higher-dimensional supergravity theories via dimensional reduction (e.g. "N" = 1 11-dimensional supergravity is dimensionally reduced on S7 to "N" = 8, "d" = 4 SUGRA). The resulting theories were sometimes referred to as Kaluza–Klein theories as Kaluza and Klein constructed in 1919 a 5-dimensional gravitational theory, that when dimensionally reduced on circle, its 4-dimensional non-massive modes describe electromagnetism coupled to gravity.
mSUGRA.
mSUGRA means minimal SUper GRAvity. The construction of a realistic model of particle interactions within the "N" = 1 supergravity framework where supersymmetry (SUSY) is broken by a super Higgs mechanism was carried out by Ali Chamseddine, Richard Arnowitt and Pran Nath in 1982. In these classes of models collectively now known as minimal supergravity Grand Unification Theories (mSUGRA GUT), gravity mediates the breaking of SUSY through the existence of a hidden sector. mSUGRA naturally generates the Soft SUSY breaking terms which are a consequence of the Super Higgs effect. Radiative breaking of electroweak symmetry through Renormalization Group Equations (RGEs) follows as an immediate consequence. mSUGRA is one of the most widely investigated models of particle physics due to its predictive power—requiring only four input parameters and a sign to determine the low energy phenomenology from the scale of Grand Unification.
11d: the maximal SUGRA.
One of these supergravities, the 11-dimensional theory, generated considerable excitement as the first potential candidate for the theory of everything. This excitement was built on four pillars, two of which have now been largely discredited:
Thus, the first two results appeared to establish 11 dimensions uniquely, the third result appeared to specify the theory, and the last result explained why the observed universe appears to be four-dimensional.
Many of the details of the theory were fleshed out by Peter van Nieuwenhuizen, Sergio Ferrara and Daniel Z. Freedman.
The end of the SUGRA era.
The initial excitement over 11-dimensional supergravity soon waned, as various failings were discovered, and attempts to repair the model failed as well. Problems included:
Some of these difficulties could be avoided by moving to a 10-dimensional theory involving superstrings. However, by moving to 10 dimensions one loses the sense of uniqueness of the 11-dimensional theory.
The core breakthrough for the 10-dimensional theory, known as the first superstring revolution, was a demonstration by Michael B. Green, John H. Schwarz and David Gross that there are only three supergravity models in 10 dimensions which have gauge symmetries and in which all of the gauge and gravitational anomalies cancel. These were theories built on the groups SO(32) and formula_1, the direct product of two copies of E8. Today we know that, using D-branes for example, gauge symmetries can be introduced in other 10-dimensional theories as well.
The second superstring revolution.
Initial excitement about the 10-dimensional theories, and the string theories that provide their quantum completion, died by the end of the 1980s. There were too many Calabi–Yaus to compactify on, many more than Yau had estimated, as he admitted in December 2005 at the 23rd International Solvay Conference in Physics. None quite gave the standard model, but it seemed as though one could get close with enough effort in many distinct ways. Plus no one understood the theory beyond the regime of applicability of string perturbation theory.
There was a comparatively quiet period at the beginning of the 1990s; however, several important tools were developed. For example, it became apparent that the various superstring theories were related by "string dualities", some of which relate weak string-coupling (i.e. perturbative) physics in one model with strong string-coupling (i.e. non-perturbative) in another.
Then it all changed, in what is known as the second superstring revolution. Joseph Polchinski realized that obscure string theory objects, called D-branes, which he had discovered six years earlier, are stringy versions of the p-branes that were known in supergravity theories. The treatment of these p-branes was not restricted by string perturbation theory; in fact, thanks to supersymmetry, p-branes in supergravity were understood well beyond the limits in which string theory was understood.
Armed with this new nonperturbative tool, Edward Witten and many others were able to show that all of the perturbative string theories were descriptions of different states in a single theory which Witten named M-theory. Furthermore he argued that M-theory's long wavelength limit (i.e. when the quantum wavelength associated to objects in the theory are much larger than the size of the 11th dimension) should be described by the 11-dimensional supergravity that had fallen out of favor with the first superstring revolution 10 years earlier, accompanied by the 2- and 5-branes.
Historically, then, supergravity has come "full circle". It is a commonly used framework in understanding features of string theories, M-theory and their compactifications to lower spacetime dimensions.
Relation to superstrings.
Particular 10-dimensional supergravity theories are considered "low energy limits" of the 10-dimensional superstring theories; more precisely, these arise as the massless, tree-level approximation of string theories. True effective field theories of string theories, rather than truncations, are rarely available. Due to string dualities, the conjectured 11-dimensional M-theory is required to have 11-dimensional supergravity as a "low energy limit". However, this doesn't necessarily mean that string theory/M-theory is the only possible UV completion of supergravity; supergravity research is useful independent of those relations.
4D "N" = 1 SUGRA.
Before we move on to SUGRA proper, let's recapitulate some important details about general relativity. We have a 4D differentiable manifold M with a Spin(3,1) principal bundle over it. This principal bundle represents the local Lorentz symmetry. In addition, we have a vector bundle T over the manifold with the fiber having four real dimensions and transforming as a vector under Spin(3,1).
We have an invertible linear map from the tangent bundle TM to T. This map is the vierbein. The local Lorentz symmetry has a gauge connection associated with it, the spin connection.
The following discussion will be in superspace notation, as opposed to the component notation, which isn't manifestly covariant under SUSY. There are actually "many" different versions of SUGRA out there which are inequivalent in the sense that their actions and constraints upon the torsion tensor are different, but ultimately equivalent in that we can always perform a field redefinition of the supervierbeins and spin connection to get from one version to another.
In 4D N=1 SUGRA, we have a 4|4 real differentiable supermanifold M, i.e. we have 4 real bosonic dimensions and 4 real fermionic dimensions. As in the nonsupersymmetric case, we have a Spin(3,1) principal bundle over M. We have an R4|4 vector bundle T over M. The fiber of T transforms under the local Lorentz group as follows; the four real bosonic dimensions transform as a vector and the four real fermionic dimensions transform as a Majorana spinor. This Majorana spinor can be reexpressed as a complex left-handed Weyl spinor and its complex conjugate right-handed Weyl spinor (they're not independent of each other). We also have a spin connection as before.
We will use the following conventions; the spatial (both bosonic and fermionic) indices will be indicated by M, N, ... . The bosonic spatial indices will be indicated by μ, ν, ..., the left-handed Weyl spatial indices by α, β..., and the right-handed Weyl spatial indices by formula_2, formula_3, ... . The indices for the fiber of T will follow a similar notation, except that they will be hatted like this: formula_4. See van der Waerden notation for more details. formula_5. The supervierbein is denoted by formula_6, and the spin connection by formula_7. The "inverse" supervierbein is denoted by formula_8.
The supervierbein and spin connection are real in the sense that they satisfy the reality conditions
The covariant derivative is defined as
The covariant exterior derivative as defined over supermanifolds needs to be super graded. This means that every time we interchange two fermionic indices, we pick up a +1 sign factor, instead of -1.
The presence or absence of R symmetries is optional, but if R-symmetry exists, the integrand over the full superspace has to have an R-charge of 0 and the integrand over chiral superspace has to have an R-charge of 2.
A chiral superfield "X" is a superfield which satisfies formula_15. In order for this constraint to be consistent, we require the integrability conditions that formula_16 for some coefficients "c".
Unlike nonSUSY GR, the torsion has to be nonzero, at least with respect to the fermionic directions. Already, even in flat superspace, formula_17.
In one version of SUGRA (but certainly not the only one), we have the following constraints upon the torsion tensor:
Here, formula_24 is a shorthand notation to mean the index runs over either the left or right Weyl spinors.
The superdeterminant of the supervierbein, formula_25, gives us the volume factor for M. Equivalently, we have the volume 4|4-superformformula_26.
If we complexify the superdiffeomorphisms, there is a gauge where formula_27, formula_28 and formula_29. The resulting chiral superspace has the coordinates x and Θ.
"R" is a scalar valued chiral superfield derivable from the supervielbeins and spin connection. If "f" is any superfield, formula_30 is always a chiral superfield.
The action for a SUGRA theory with chiral superfields "X", is given by
where "K" is the Kähler potential and "W" is the superpotential, and formula_32 is the chiral volume factor. 
Unlike the case for flat superspace, adding a constant to either the Kähler or superpotential is now physical. A constant shift to the Kähler potential changes the effective Planck constant, while a constant shift to the superpotential changes the effective cosmological constant. As the effective Planck constant now depends upon the value of the chiral superfield "X", we need to rescale the supervierbeins (a field redefinition) to get a constant Planck constant. This is called the Einstein frame.
N = 8 supergravity in 4 dimensions.
N=8 Supergravity is the most symmetric quantum field theory which involves gravity and a finite number of fields. It can be found from a dimensional reduction of 11D supergravity by making the size of 7 of the dimensions go to zero. It has 8 supersymmetries which is the most any gravitational theory can have since there are 8 half-steps between spin 2 and spin -2. (A graviton has the highest spin in this theory which is a spin 2 particle). More supersymmetries would mean the particles would have superpartners with spins higher than 2. The only theories with spins higher than 2 which are consistent involve an infinite number of particles (such as String Theory). Stephen Hawking in his "A Brief History of Time" speculated that this theory could be the Theory of Everything. However in later years this was abandoned in favour of String Theory. There has been renewed interest in the 21st century with the possibility that this theory may be finite.
Higher-dimensional SUGRA.
Higher-dimensional SUGRA is the higher-dimensional, supersymmetric generalization of general relativity. Supergravity can be formulated in any number of dimensions up to eleven. Higher-dimensional SUGRA focuses upon supergravity in greater than four dimensions.
The number of supercharges in a spinor depends on the dimension and the signature of spacetime. The supercharges occur in spinors. Thus the limit on the number of supercharges cannot be satisfied in a spacetime of arbitrary dimension. Some theoretical examples in which this is satisfied are:
The supergravity theories that have attracted the most interest contain no spins higher than two. This means, in particular, that they do not contain any fields that transform as symmetric tensors of rank higher than two under Lorentz transformations. The consistency of interacting higher spin field theories is, however, presently a field of very active interest.

</doc>
<doc id="293756" url="https://en.wikipedia.org/wiki?curid=293756" title="Supergroup (physics)">
Supergroup (physics)

The concept of "supergroup" is a generalization of that of group. In other words, every supergroup carries a natural group structure, and vice versa, but there may be more than one way to structure a given group as a supergroup. A supergroup is like a Lie group in that there is a well defined notion of smooth function defined on them. 
However the functions may have even and odd parts. Moreover a supergroup has a super Lie algebra which plays a role similar to that of a Lie algebra for Lie groups in that they determine most of the representation theory and which is the starting point for classification.
More formally, a Lie supergroup is a supermanifold G together with a multiplication morphism formula_1, an inversion morphism formula_2 and a unit morphism formula_3 which makes G a group object in the category of supermanifolds. This means that, formulated as commutative diagrams, the usual associativity and inversion axioms of a group continue to hold. Since every manifold is a super manifold, a Lie supergroup generalises the notion of a Lie group. 
There are many possible supergroups. The ones of most interest in theoretical physics are the ones which extend the Poincaré group or the conformal group. Of particular interest are the orthosymplectic groups "Osp(N/M)" and the superunitary groups "SU(N/M)". 
An equivalent algebraic approach starts from the observation that a super manifold is determined by its ring of supercommutative smooth functions, and that a morphism of super manifolds corresponds one to one with an algebra homomorphism between their functions in the opposite direction, i.e. that the category of supermanifolds is opposite to the category of algebras of smooth graded commutative functions. Reversing all the arrows in the commutative diagrams that define a Lie supergroup then shows that functions over the supergroup have the structure of a Z2-graded Hopf algebra. Likewise the representations of this Hopf algebra turn out to be Z2-graded comodules. This Hopf algebra gives the global properties of the supergroup.
There is another related Hopf algebra which is the dual of the previous Hopf algebra. It can be identified with the Hopf algebra of graded differential operators at the origin. It only gives the local properties of the symmetries i.e., it only gives information about infinitesimal supersymmetry transformations. The representations of this Hopf algebra are modules. Like in the non graded case, this Hopf algebra can be described purely algebraically as the universal enveloping algebra of the Lie superalgebra.
In a similar way one can define an affine algebraic supergroup as a group object in the category of superalgebraic affine varieties. An affine algebraic supergroup has a similar one to one relation to its Hopf algebra of superpolynomials. Using the language of schemes, which combines the geometric and algebraic point of view, algebraic supergroup schemes can be defined including super Abelian varieties.

</doc>
<doc id="618227" url="https://en.wikipedia.org/wiki?curid=618227" title="Superpartner">
Superpartner

In particle physics, a Superpartner (also sparticle) is a hypothetical elementary particle. Supersymmetry is one of the synergistic theories in current high-energy physics that predicts the existence of these "shadow" particles.
The word "sparticle" features the "s-" prefix which is used to form names of superpartners of the individual fermions, e.g. the stop quark.
Theoretical predictions.
According to the supersymmetry theory, each fermion should have a partner boson, the fermion's superpartner, and each boson should have a partner fermion. Exact "unbroken" supersymmetry would predict that a particle and its superpartners would have the same mass. No superpartners of the Standard Model particles have yet been found. This may indicate that supersymmetry is incorrect, or it may also be the result of the fact that supersymmetry is not an exact, "unbroken" symmetry of nature. If superpartners are found, their masses would indicate the scale at which supersymmetry is broken.
For particles that are real scalars (such as an axion), there is a fermion superpartner as well as a second, real scalar field. For axions, these particles are often referred to as axinos and saxions.
In extended supersymmetry there may be more than one superparticle for a given particle. For instance, with two copies of supersymmetry in four dimensions, a photon would have two fermion superpartners and a scalar superpartner. 
In zero dimensions it is possible to have supersymmetry, but no superpartners. However, this is the only situation where supersymmetry does not imply the existence of superpartners.
Recreating superpartners.
If the supersymmetry theory is correct, it should be possible to recreate these particles in high-energy particle accelerators. Doing so will not be an easy task; these particles may have masses up to a thousand times greater than their corresponding "real" particles. 
Some researchers have hoped the Large Hadron Collider at CERN might produce evidence for the existence of superpartner particles. However, as of 2013, no such evidence has been found.

</doc>
<doc id="1201321" url="https://en.wikipedia.org/wiki?curid=1201321" title="Superposition principle">
Superposition principle

In physics and systems theory, the superposition principle, also known as superposition property, states that, for all linear systems, the net response at a given place and time caused by two or more stimuli is the sum of the responses which would have been caused by each stimulus individually. So that if input "A" produces response "X" and input "B" produces response "Y" then input ("A" + "B") produces response ("X" + "Y").
The homogeneity and additivity properties together are called the superposition principle. A linear function is one that satisfies the properties of superposition. Which is defined as 
This principle has many applications in physics and engineering because many physical systems can be modeled as linear systems. For example, a beam can be modeled as a linear system where the input stimulus is the load on the beam and the output response is the deflection of the beam. The importance of linear systems is that they are easier to analyze mathematically; there is a large body of mathematical techniques, frequency domain linear transform methods such as Fourier, Laplace transforms, and linear operator theory, that are applicable. Because physical systems are generally only approximately linear, the superposition principle is only an approximation of the true physical behaviour.
The superposition principle applies to "any" linear system, including algebraic equations, linear differential equations, and systems of equations of those forms. The stimuli and responses could be numbers, functions, vectors, vector fields, time-varying signals, or any other object which satisfies certain axioms. Note that when vectors or vector fields are involved, a superposition is interpreted as a vector sum.
Relation to Fourier analysis and similar methods.
By writing a very general stimulus (in a linear system) as the superposition of stimuli of a specific, simple form, often the response becomes easier to compute.
For example, in Fourier analysis, the stimulus is written as the superposition of infinitely many sinusoids. Due to the superposition principle, each of these sinusoids can be analyzed separately, and its individual response can be computed. (The response is itself a sinusoid, with the same frequency as the stimulus, but generally a different amplitude and phase.) According to the superposition principle, the response to the original stimulus is the sum (or integral) of all the individual sinusoidal responses.
As another common example, in Green's function analysis, the stimulus is written as the superposition of infinitely many impulse functions, and the response is then a superposition of impulse responses.
Fourier analysis is particularly common for waves. For example, in electromagnetic theory, ordinary light is described as a superposition of plane waves (waves of fixed frequency, polarization, and direction). As long as the superposition principle holds (which is often but not always; see nonlinear optics), the behavior of any light wave can be understood as a superposition of the behavior of these simpler plane waves.
Wave superposition.
Waves are usually described by variations in some parameter through space and time—for example, height in a water wave, pressure in a sound wave, or the electromagnetic field in a light wave. The value of this parameter is called the amplitude of the wave, and the wave itself is a function specifying the amplitude at each point.
In any system with waves, the waveform at a given time is a function of the sources (i.e., external forces, if any, that create or affect the wave) and initial conditions of the system. In many cases (for example, in the classic wave equation), the equation describing the wave is linear. When this is true, the superposition principle can be applied.
That means that the net amplitude caused by two or more waves traversing the same space is the sum of the amplitudes which would have been produced by the individual waves separately. For example, two waves traveling towards each other will pass right through each other without any distortion on the other side. (See image at top.)
Wave diffraction vs. wave interference.
With regard to wave superposition, Richard Feynman wrote:
Other authors elaborate:
Yet another source concurs:
Wave interference.
The phenomenon of interference between waves is based on this idea. When two or more waves traverse the same space, the net amplitude at each point is the sum of the amplitudes of the individual waves. In some cases, such as in noise-cancelling headphones, the summed variation has a smaller amplitude than the component variations; this is called "destructive interference". In other cases, such as in Line Array, the summed variation will have a bigger amplitude than any of the components individually; this is called "constructive interference".
Departures from linearity.
In most realistic physical situations, the equation governing the wave is only approximately linear. In these situations, the superposition principle only approximately holds. As a rule, the accuracy of the approximation tends to improve as the amplitude of the wave gets smaller. For examples of phenomena that arise when the superposition principle does not exactly hold, see the articles nonlinear optics and nonlinear acoustics.
Quantum superposition.
In quantum mechanics, a principal task is to compute how a certain type of wave propagates and behaves. The wave is called a wavefunction, and the equation governing the behavior of the wave is called Schrödinger's wave equation. A primary approach to computing the behavior of a wavefunction is to write that wavefunction as a superposition (called "quantum superposition") of (possibly infinitely many) other wavefunctions of a certain type—stationary states whose behavior is particularly simple. Since Schrödinger's wave equation is linear, the behavior of the original wavefunction can be computed through the superposition principle this way.
Boundary value problems.
A common type of boundary value problem is (to put it abstractly) finding a function "y" that satisfies some equation
with some boundary specification
For example, in Laplace's equation with Dirichlet boundary conditions, "F" would be the Laplacian operator in a region "R", "G" would be an operator that restricts "y" to the boundary of "R", and "z" would be the function that "y" is required to equal on the boundary of "R".
In the case that "F" and "G" are both linear operators, then the superposition principle says that a superposition of solutions to the first equation is another solution to the first equation:
while the boundary values superpose:
Using these facts, if a list can be compiled of solutions to the first equation, then these solutions can be carefully put into a superposition such that it will satisfy the second equation. This is one common method of approaching boundary value problems.
Additive State Decomposition.
Consider a simple linear system :<br>
formula_7<br>
By superposition principle, the system can be decomposed into<br>
formula_8 <br>
formula_9 <br>
with<br>
formula_10
Superposition principle is only available for linear systems. However, the Additive State Decomposition can be applied not only to linear systems but also nonlinear systems. Next, consider a nonlinear system<br>
formula_11<br>
where formula_12 is a nonlinear function. By the additive state decomposition, the system can be ‘additively’ decomposed into<br>
formula_13 <br>
formula_14 <br>
with<br>
formula_10<br>
This decomposition can help to simplify controller design.
History.
According to Léon Brillouin, the principle of superposition was first stated by Daniel Bernoulli in 1753: "The general motion of a vibrating system is given by a superposition of its proper vibrations." The principle was rejected by Leonhard Euler and then by Joseph Lagrange. Later it became accepted, largely through the work of Joseph Fourier.

</doc>
<doc id="432630" url="https://en.wikipedia.org/wiki?curid=432630" title="Superspace">
Superspace

"Superspace" is the coordinate space of a theory exhibiting supersymmetry. In such a formulation, along with ordinary space dimensions "x", "y", "z", ..., there are also "anticommuting" dimensions whose coordinates are labeled in Grassmann numbers rather than real numbers. The ordinary space dimensions correspond to bosonic degrees of freedom, the anticommuting dimensions to fermionic degrees of freedom.
See also supermanifold (although the definition of a superspace as a supermanifold here does not agree with the definition used in that article).
R"m"|"n" is the Z2-graded vector space with R"m" as the even subspace and R"n" as the odd subspace. The same definition applies to Cm|n.
The word "superspace" was first used by John Wheeler in an unrelated sense to describe the configuration space of general relativity; for example, this usage may be seen in his 1973 textbook "Gravitation".
Examples.
Trivial examples.
The smallest superspace is a point which contains neither bosonic nor fermionic directions. Other trivial examples include the "n"-dimensional real plane Rn, which is a vector space extending in "n" real, bosonic directions and no fermionic directions. The vector space R0|n, which is the "n"-dimensional real Grassmann algebra. The space R1|1 of one even and one odd direction is known as the space of dual numbers, introduced by William Clifford in 1873.
The superspace of supersymmetric quantum mechanics.
Supersymmetric quantum mechanics with "N" supercharges is often formulated in the superspace R1|2"N", which contains one real direction "t" identified with time and "N" complex Grassmann directions which are spanned by Θ"i" and Θ*"i", where "i" runs for 1 to "N".
Consider the special case "N" = 1. The superspace R1|2 is a 3-dimensional vector space. A given coordinate therefore may be written as a triple ("t", Θ, Θ*). The coordinates form a Lie superalgebra, in which the gradation degree of "t" is even and that of Θ and Θ* is odd. This means that a bracket may be defined between any two elements of this vector space, and that this bracket reduces to the commutator on two even coordinates and on one even and one odd coordinate while it is an anticommutator on two odd coordinates. This superspace is an abelian Lie superalgebra, which means that all of the forementioned brackets vanish
where formula_2 is the commutator of "a" and "b" and formula_3 is the anticommutator of "a" and "b".
One may define functions from this vectorspace to itself, which are called superfields. The above algebraic relations imply that, if we expand our superfield as a power series in Θ and Θ* then we will only find terms at the zeroeth and first orders, because Θ2 = Θ*2 = 0. Therefore superfields may be written as arbitrary functions of "t" multiplied by the zeroeth and first order terms in the two Grassmann coordinates
Superfields, which are representations of the supersymmetry of superspace, generalize the notion of tensors, which are representations of the rotation group of a bosonic space.
One may then define derivatives in the Grassmann directions, which take the first order term in the expansion of a superfield to the zeroeth order term and annihilate the zeroeth order term. One can choose sign conventions such that the derivatives satisfy the anticommutation relations
These derivatives may be assembled into supercharges
whose anticommutators identify them as the fermionic generators of a supersymmetry algebra
where "i" times the time derivative is the Hamiltonian operator in quantum mechanics. Both "Q" and its adjoint anticommute with themselves. The supersymmetry variation with supersymmetry parameter ε of a superfield Φ is defined to be 
We can evaluate this variation using the action of "Q" on the superfields
Similarly one may define covariant derivatives on superspace 
which anticommute with the supercharges and satisfy a wrong sign supersymmetry algebra
The fact that the covariant derivatives anticommute with the supercharges means the supersymmetry transformation of a covariant derivative of a superfield is equal to the covariant derivative of the same supersymmetry transformation of the same superfield. Thus, generalizing the covariant derivative in bosonic geometry which constructs tensors from tensors, the superspace covariant derivative constructs superfields from superfields.
Four-dimensional "N" = 1 superspace.
Perhaps the most popular superspace in physics is "d"=4 "N"=1 super Minkowski space R4|4, which is the direct sum of four real bosonic dimensions and four real Grassmann dimensions. In supersymmetric quantum field theories one is interested in superspaces which furnish representations of a Lie superalgebra called a supersymmetry algebra. The bosonic part of the supersymmetry algebra is the Poincaré algebra, while the fermionic part is constructed using spinors of Grassmann numbers. 
For this reason, in physical applications one considers an action of the supersymmetry algebra on the four fermionic directions of R4|4 such that they transform as a spinor under the Poincaré subalgebra. In four dimensions there are three distinct irreducible 4-component spinors. There is the Majorana spinor, the left-handed Weyl spinor and the right-handed Weyl spinor. The CPT theorem implies that in a unitary, Poincaré invariant theory, which is a theory in which the S-matrix is a unitary matrix and the same Poincaré generators act on the asymptotic in-states as on the asymptotic out-states, the supersymmetry algebra must contain an equal number of left-handed and right-handed Weyl spinors. However, since each Weyl spinor has four components, this means that if one includes any Weyl spinors one must have 8 fermionic directions. Such a theory is said to have extended supersymmetry, and such models have received a lot of attention. For example, supersymmetric gauge theories with eight supercharges and fundamental matter have been solved by Nathan Seiberg and Edward Witten, see Seiberg–Witten gauge theory. However in this subsection we are considering the superspace with four fermionic components and so no Weyl spinors are consistent with the CPT theorem.
"Note": There are many sign conventions in use and this is only one of them.
This leaves us with one possibility, the four fermionic directions transform as a Majorana spinor θα. We can also form a conjugate spinor
where "C" is the charge conjugation matrix, which is defined by the property that when it conjugates a gamma matrix, the gamma matrix is negated and transposed. The first equality is the definition of while the second is a consequence of the Majorana spinor condition θ* = iγ0Cθ. The conjugate spinor plays a role similar to that of θ* in the superspace R1|2, except that the Majorana condition, as manifested in the above equation, imposes that θ and θ* are not independent.
In particular we may construct the supercharges
which satisfy the supersymmetry algebra
where formula_15 is the 4-momentum operator. Again the covariant derivative is defined like the supercharge but with the second term negated and it anticommutes with the supercharges. Thus the covariant derivative of a supermultiplet is another supermultiplet.

</doc>
<doc id="224636" url="https://en.wikipedia.org/wiki?curid=224636" title="Supersymmetry">
Supersymmetry

Supersymmetry (SUSY), a theory of particle physics, is a proposed type of spacetime symmetry that relates two basic classes of elementary particles: bosons, which have an integer-valued spin, and fermions, which have a half-integer spin. Each particle from one group is associated with a particle from the other, known as its superpartner, the spin of which differs by a half-integer. In a theory with perfectly "unbroken" supersymmetry, each pair of superpartners would share the same mass and internal quantum numbers besides spin. For example, there would be a "selectron" (superpartner electron), a bosonic version of the electron with the same mass as the electron, that would be easy to find in a laboratory. Thus, since no superpartners have been observed, if supersymmetry exists it must be a spontaneously broken symmetry so that superpartners may differ in mass. Spontaneously-broken supersymmetry could solve many mysterious problems in particle physics including the hierarchy problem. The simplest realization of spontaneously-broken supersymmetry, the so-called Minimal Supersymmetric Standard Model, is one of the best studied candidates for physics beyond the Standard Model.
There is only indirect evidence and motivation for the existence of supersymmetry. Direct confirmation would entail production of superpartners in collider experiments, such as the Large Hadron Collider (LHC). The first run of the LHC found no evidence for supersymmetry (all results were consistent with the Standard Model), and thus set limits on superpartner masses in supersymmetric theories. Whilst many remain enthusiastic about supersymmetry, this first run at the LHC led some physicists to explore other ideas. The LHC resumed its search for supersymmetry and other new physics in its second run.
Motivations.
There are numerous phenomenological motivations for supersymmetry close to the electroweak scale, as well as technical motivations for supersymmetry at any scale.
The hierarchy problem.
Supersymmetry close to the electroweak scale ameliorates the hierarchy problem that afflicts the Standard Model. In the Standard Model, the electroweak scale receives enormous Planck-scale quantum corrections. The observed hierarchy between the electroweak scale and the Planck scale must be achieved with extraordinary fine tuning. In a supersymmetric theory, on the other hand, Planck-scale quantum corrections cancel between partners and superpartners (owing to a minus sign associated with fermionic loops). The hierarchy between the electroweak scale and the Planck scale is achieved in a natural manner, without miraculous fine-tuning.
Gauge coupling unification.
The idea that the gauge symmetry groups unify at high-energy is called Grand unification theory. In the Standard Model, however, the weak, strong and electromagnetic couplings fail to unify at high energy. In a supersymmetry theory, the running of the gauge couplings are modified, and precise high-energy unification of the gauge couplings is achieved. The modified running also provides a natural mechanism for radiative electroweak symmetry breaking.
Dark matter.
TeV-scale supersymmetry (augmented with a discrete symmetry) typically provides a candidate dark matter particle at a mass scale consistent with thermal relic abundance calculations.
Other technical motivations.
Supersymmetry is also motivated by solutions to several theoretical problems, for generally providing many desirable mathematical properties, and for ensuring sensible behavior at high energies. Supersymmetric quantum field theory is often much easier to analyze, as many more problems become exactly solvable. When supersymmetry is imposed as a "local" symmetry, Einstein's theory of general relativity is included automatically, and the result is said to be a theory of supergravity. It is also a necessary feature of the most popular candidate for a theory of everything, superstring theory.
Another theoretically appealing property of supersymmetry is that it offers the only "loophole" to the Coleman–Mandula theorem, which prohibits spacetime and internal symmetries from being combined in any nontrivial way, for quantum field theories like the Standard Model with very general assumptions. The Haag-Lopuszanski-Sohnius theorem demonstrates that supersymmetry is the only way spacetime and internal symmetries can be combined consistently.
History.
A supersymmetry relating mesons and baryons was first proposed, in the context of hadronic physics, by Hironari Miyazawa during 1966. This supersymmetry did not involve spacetime, that is, it concerned internal symmetry, and was broken badly. Miyazawa's work was largely ignored at the time.
J. L. Gervais and B. Sakita (during 1971), Yu. A. Golfand and E. P. Likhtman (also during 1971), and D.V. Volkov and V.P. Akulov (1972), independently rediscovered supersymmetry in the context of quantum field theory, a radically new type of symmetry of spacetime and fundamental fields, which establishes a relationship between elementary particles of different quantum nature, bosons and fermions, and unifies spacetime and internal symmetries of microscopic phenomena. Supersymmetry with a consistent Lie-algebraic graded structure on which the Gervais−Sakita rediscovery was based directly first arose during 1971 in the context of an early version of string theory by Pierre Ramond, John H. Schwarz and André Neveu.
Finally, Julius Wess and Bruno Zumino (during 1974) identified the characteristic renormalization features of four-dimensional supersymmetric field theories, which identified them as remarkable QFTs, and they and Abdus Salam and their fellow researchers introduced early particle physics applications. The mathematical structure of supersymmetry (Graded Lie superalgebras) has subsequently been applied successfully to other topics of physics, ranging from nuclear physics, critical phenomena, quantum mechanics to statistical physics. It remains a vital part of many proposed theories of physics.
The first realistic supersymmetric version of the Standard Model was proposed during 1977 by Pierre Fayet and is known as the Minimal Supersymmetric Standard Model or MSSM for short. It was proposed to solve, amongst other things, the hierarchy problem.
Applications.
Extension of possible symmetry groups.
One reason that physicists explored supersymmetry is because it offers an extension to the more familiar symmetries of quantum field theory. These symmetries are grouped into the Poincaré group and internal symmetries and the Coleman–Mandula theorem showed that under certain assumptions, the symmetries of the S-matrix must be a direct product of the Poincaré group with a compact internal symmetry group or if there is not any mass gap, the conformal group with a compact internal symmetry group.
During 1971 Golfand and Likhtman were the first to show that the Poincaré algebra can be extended through introduction of four
anticommuting spinor generators (in four dimensions), which later became known as supercharges.
During 1975 the Haag-Lopuszanski-Sohnius theorem
analyzed all possible superalgebras in the general form, including those with an extended number of the supergenerators and central charges.
This extended super-Poincaré algebra paved the way for obtaining a very large and important class of supersymmetric field theories.
The supersymmetry algebra.
Traditional symmetries of physics are generated by objects that transform by the tensor representations of the Poincaré group and internal symmetries. Supersymmetries, however, are generated by objects that transform by the spinor representations. According to the spin-statistics theorem, bosonic fields commute while fermionic fields anticommute. Combining the two kinds of fields into a single algebra requires the introduction of a Z2-grading under which the bosons are the even elements and the fermions are the odd elements. Such an algebra is called a Lie superalgebra.
The simplest supersymmetric extension of the Poincaré algebra is the Super-Poincaré algebra. Expressed in terms of two Weyl spinors, has the following anti-commutation relation:
and all other anti-commutation relations between the "Q"s and commutation relations between the "Q"s and "P"s vanish. In the above expression formula_2 are the generators of translation and formula_3 are the Pauli matrices.
There are representations of a Lie superalgebra that are analogous to representations of a Lie algebra. Each Lie algebra has an associated Lie group and a Lie superalgebra can sometimes be extended into representations of a Lie supergroup.
The Supersymmetric Standard Model.
Incorporating supersymmetry into the Standard Model requires doubling the number of particles since there is no way that any of the particles in the Standard Model can be superpartners of each other. With the addition of new particles, there are many possible new interactions. The simplest possible supersymmetric model consistent with the Standard Model is the Minimal Supersymmetric Standard Model (MSSM) which can include the necessary additional new particles that are able to be superpartners of those in the Standard Model.
One of the main motivations for SUSY comes from the quadratically divergent contributions to the Higgs mass squared. The quantum mechanical interactions of the Higgs boson causes a large renormalization of the Higgs mass and unless there is an accidental cancellation, the natural size of the Higgs mass is the greatest scale possible. This problem is known as the hierarchy problem. Supersymmetry reduces the size of the quantum corrections by having automatic cancellations between fermionic and bosonic Higgs interactions. If supersymmetry is restored at the weak scale, then the Higgs mass is related to supersymmetry breaking which can be induced from small non-perturbative effects explaining the vastly different scales in the weak interactions and gravitational interactions.
In many supersymmetric Standard Models there is a heavy stable particle (such as neutralino) which could serve as a weakly interacting massive particle (WIMP) dark matter candidate. The existence of a supersymmetric dark matter candidate is related closely to R-parity.
The standard paradigm for incorporating supersymmetry into a realistic theory is to have the underlying dynamics of the theory be supersymmetric, but the ground state of the theory does not respect the symmetry and supersymmetry is broken spontaneously. The supersymmetry break can not be done permanently by the particles of the MSSM as they currently appear. This means that there is a new sector of the theory that is responsible for the breaking. The only constraint on this new sector is that it must break supersymmetry permanently and must give superparticles TeV scale masses. There are many models that can do this and most of their details do not matter. In order to parameterize the relevant features of supersymmetry breaking, arbitrary soft SUSY breaking terms are added to the theory which temporarily break SUSY explicitly but could never arise from a complete theory of supersymmetry breaking.
Gauge-coupling unification.
One piece of evidence for supersymmetry existing is gauge coupling unification.
The renormalization group evolution of the three gauge coupling constants of the Standard Model is somewhat sensitive to the present particle content of the theory. These coupling constants do not quite meet together at a common energy scale if we run the renormalization group using the Standard Model. With the addition of minimal SUSY joint convergence of the coupling constants is projected at approximately 1016 GeV.
Supersymmetric quantum mechanics.
"Supersymmetric quantum mechanics" adds the SUSY superalgebra to quantum mechanics as opposed to quantum field theory. Supersymmetric quantum mechanics often becomes relevant when studying the dynamics of supersymmetric solitons, and due to the simplified nature of having fields which are only functions of time (rather than space-time), a great deal of progress has been made in this subject and it is now studied in its own right.
SUSY quantum mechanics involves pairs of Hamiltonians which share a particular mathematical relationship, which are called "partner Hamiltonians". (The potential energy terms which occur in the Hamiltonians are then known as "partner potentials".) An introductory theorem shows that for every eigenstate of one Hamiltonian, its partner Hamiltonian has a corresponding eigenstate with the same energy. This fact can be exploited to deduce many properties of the eigenstate spectrum. It is analogous to the original description of SUSY, which referred to bosons and fermions. We can imagine a "bosonic Hamiltonian", whose eigenstates are the various bosons of our theory. The SUSY partner of this Hamiltonian would be "fermionic", and its eigenstates would be the theory's fermions. Each boson would have a fermionic partner of equal energy.
Supersymmetry: Applications to condensed matter physics.
SUSY concepts have provided useful extensions to the WKB approximation. Additionally, SUSY has been applied to disorder averaged systems both quantum and non-quantum (through statistical mechanics), the Fokker-Planck equation being an example of a non-quantum theory. The 'supersymmetry' in all these systems arises from the fact that one is modelling one particle and as such the 'statistics' don't matter. The use of the supersymmetry method provides a mathematical rigorous alternative to the replica trick, but only in non-interacting systems, which attempts to address the so-called 'problem of the denominator' under disorder averaging. For more on the applications of supersymmetry in condensed matter physics see the book
Supersymmetry in optics.
Integrated optics was recently found to provide a fertile ground on which certain ramifications of SUSY can be explored in readily-accessible laboratory settings. Making use of the analogous mathematical structure of the quantum-mechanical Schrödinger equation and the wave equation governing the evolution of light in one-dimensional settings, one may interpret the refractive index distribution of a structure as a potential landscape in which optical wave packets propagate. In this manner, a new class of functional optical structures with possible applications in phase matching, mode conversion and space-division multiplexing becomes possible. SUSY transformations have been also proposed as a way to address inverse scattering problems in optics and as a one-dimensional transformation optics 
Mathematics.
SUSY is also sometimes studied mathematically for its intrinsic properties. This is because it describes complex fields satisfying a property known as holomorphy, which allows holomorphic quantities to be exactly computed. This makes supersymmetric models useful "toy models" of more realistic theories. A prime example of this has been the demonstration of S-duality in four-dimensional gauge theories that interchanges particles and monopoles.
The proof of the Atiyah-Singer index theorem is much simplified by the use of supersymmetric quantum mechanics.
General supersymmetry.
Supersymmetry appears in many related contexts of theoretical physics. It is possible to have multiple supersymmetries and also have supersymmetric extra dimensions.
Extended supersymmetry.
It is possible to have more than one kind of supersymmetry transformation. Theories with more than one supersymmetry transformation are known as extended supersymmetric theories. The more supersymmetry a theory has, the more constrained are the field content and interactions. Typically the number of copies of a supersymmetry is a power of 2, i.e. 1, 2, 4, 8. In four dimensions, a spinor has four degrees of freedom and thus the minimal number of supersymmetry generators is four in four dimensions and having eight copies of supersymmetry means that there are 32 supersymmetry generators.
The maximal number of supersymmetry generators possible is 32. Theories with more than 32 supersymmetry generators automatically have massless fields with spin greater than 2. It is not known how to make massless fields with spin greater than two interact, so the maximal number of supersymmetry generators considered is 32. This corresponds to an "N" = 8 supersymmetry theory. Theories with 32 supersymmetries automatically have a graviton.
For four dimensions there are the following theories, with the corresponding multiplets(CPT adds a copy, whenever they are not invariant under such symmetry)
Chiral multiplet:
Vector multiplet:
Gravitino multiplet:
Graviton multiplet:
hypermultiplet:
vector multiplet:
supergravity multiplet:
Vector multiplet:
Supergravity multiplet:
Supergravity multiplet:
Supersymmetry in alternate numbers of dimensions.
It is possible to have supersymmetry in dimensions other than four. Because the properties of spinors change drastically between different dimensions, each dimension has its characteristic. In "d" dimensions, the size of spinors is approximately 2"d"/2 or 2("d" − 1)/2. Since the maximum number of supersymmetries is 32, the greatest number of dimensions in which a supersymmetric theory can exist is eleven.
Supersymmetry as a quantum group.
Supersymmetry can be reinterpreted in the language of noncommutative geometry and quantum groups. In particular, it involves a mild form of noncommutativity, namely supercommutativity. See the main article for more details.
Supersymmetry in quantum gravity.
Supersymmetry is part of a larger enterprise of theoretical physics to unify everything we know about the universe into a single consistent set of physical principles, known as the quest for a Theory of Everything (TOE). A significant part of this larger enterprise is the quest for a theory of quantum gravity, which would unify the classical theory of general relativity and the Standard Model, which explains the other three basic forces in physics (electromagnetism, the strong interaction, and the weak interaction), and provides a palette of fundamental particles upon which all four forces act. Two of the most active methods of forming a theory of quantum gravity are string theory and loop quantum gravity (LQG), although in theory, supersymmetry could be a component of other theories as well.
For string theory to be consistent, supersymmetry seems to be required at some level (although it may be a strongly broken symmetry). In particle theory, supersymmetry is recognized as a way to stabilize the hierarchy between the unification scale and the electroweak scale (or the Higgs boson mass), and can also provide a natural dark matter candidate. String theory also requires extra spatial dimensions which have to be compactified as in Kaluza–Klein theory.
Loop quantum gravity (LQG) predicts no additional spatial dimensions, nor anything else about particle physics. These theories can be formulated in three spatial dimensions and one dimension of time, although in some LQG theories dimensionality is an emergent property of the theory, rather than a fundamental assumption of the theory. Also, LQG is a theory of quantum gravity which does not require supersymmetry. Lee Smolin, one of the originators of LQG, has proposed that a loop quantum gravity theory incorporating either supersymmetry or extra dimensions, or both, be called "loop quantum gravity II".
If experimental evidence confirms supersymmetry in the form of supersymmetric particles such as the neutralino that is often believed to be the lightest superpartner, some people believe this would be a major boost to string theory. Since supersymmetry is a required component of string theory, any discovered supersymmetry would be consistent with string theory. If the Large Hadron Collider and other major particle physics experiments fail to detect supersymmetric partners or evidence of extra dimensions, many versions of string theory which had predicted certain low mass superpartners to existing particles may need to be significantly revised. The failure of experiments to discover either supersymmetric partners or extra spatial dimensions, , has encouraged loop quantum gravity researchers.
Current status.
Supersymmetric models are constrained by a variety of experiments, including measurements of low-energy observables – for example, the anomalous magnetic moment of the muon at Brookhaven; the WMAP dark matter density measurement and direct detection experiments – for example, XENON-100 and LUX; and by particle collider experiments, including B-physics, Higgs phenomenology and direct searches for superpartners (sparticles), at the Large Electron–Positron Collider, Tevatron and the LHC.
Historically, the tightest limits were from direct production at colliders. The first mass limits for squarks and gluinos were made at CERN by the UA1 experiment and the UA2 experiment at the Super Proton Synchrotron. LEP later set very strong limits., which in 2006 were extended by the D0 experiment at the Tevatron. From 2003, WMAP's and Planck's dark matter density measurements have strongly constrained supersymmetry models, which, if they explain dark matter, have to be tuned to invoke a particular mechanism to sufficiently reduce the neutralino density.
Prior to the beginning of the LHC, in 2009 fits of available data to CMSSM and NUHM1 indicated that squarks and gluinos were most likely to have masses in the 500 to 800 GeV range, though values as high as 2.5 TeV were allowed with low probabilities. Neutralinos and sleptons were expected to be quite light, with the lightest neutralino and the lightest stau most likely to be found between 100 to 150 GeV.
The first run of the LHC found no evidence for supersymmetry, and, as a result, surpassed existing experimental limits from the Large Electron–Positron Collider and Tevatron and partially excluded the aforementioned expected ranges.
During 2011 and 2012, the LHC discovered a Higgs boson with a mass of about 125 GeV, and with couplings to fermions and bosons which are consistent with the Standard Model. The MSSM predicts that the mass of the lightest Higgs boson should not be much higher than the mass of the Z boson, and, in the absence of fine tuning (with the supersymmetry breaking scale on the order of 1 TeV), should not exceed 130 GeV. Furthermore, for values of the MSSM parameter "tan" β ≤ 3, it predicts a Higgs mass below 114 GeV over most of the parameter space. This region of Higgs mass was excluded by LEP by 2000. The LHC result is somewhat problematic for the minimal supersymmetric model, as the value of 125 GeV is relatively large for the model and can only be achieved with large radiative loop corrections from top squarks, which many theorists consider to be "unnatural" (see naturalness and fine tuning). On the other hand, the lightest Higgs boson in the MSSM is Standard Model-like, which is consistent with measurements of the Higgs boson couplings at the LHC.
In spite of the null searches and the heavy Higgs, a recent analysis of the constrained minimal supersymmetric Standard Model, the CMSSM, suggests that the model is still compatible with all present experimental constraints. The preferred masses for squarks and gluinos is about 2 TeV. The resulting fine-tuning of the electroweak scale, however, is considered "unnatural" (see little hierarchy problem), and some theorists now favor extended supersymmetry models – for example, the NMSSM.

</doc>
<doc id="18865642" url="https://en.wikipedia.org/wiki?curid=18865642" title="Supersymmetry algebra">
Supersymmetry algebra

In theoretical physics, a supersymmetry algebra (or SUSY algebra) is a mathematical formalism for describing the relation between bosons and fermions. The supersymmetry algebra contains not only the Poincaré algebra and a compact subalgebra of internal symmetries, but also contains some fermionic supercharges, transforming as a sum of "N" real spinor representations of the Poincaré group. Such symmetries are allowed by the Haag–Lopuszanski–Sohnius theorem. When "N">1 the algebra is said to have extended supersymmetry. The supersymmetry algebra is a semidirect product of a central extension of the super-Poincaré algebra by a compact Lie algebra "B" of internal symmetries.
Bosonic fields commute while fermionic fields anticommute. In order to have a transformation that relates the two kinds of fields, the introduction of a Z2-grading under which the even elements are bosonic and the odd elements are fermionic is required. Such an algebra is called a Lie superalgebra.
Just as one can have representations of a Lie algebra, one can also have representations of a Lie superalgebra, called supermultiplets. For each Lie algebra, there exists an associated Lie group which is connected and simply connected, unique up to isomorphism, and the representations of the algebra can be extended to create group representations. In the same way, representations of a Lie superalgebra can sometimes be extended into representations of a Lie supergroup.
Structure of a supersymmetry algebra.
The general supersymmetry algebra for spacetime dimension "d", and with the fermionic piece consisting of a sum of "N" irreducible real spinor representations, has a structure of the form
where
The terms "bosonic" and "fermionic" refer to even and odd subspaces of the superalgebra.
The terms "scalar", "spinor", "vector", refer to the behavior of subalgebras under the action of the Lorentz algebra "L".
The number "N" is the number of irreducible real spin representations. When the signature of spacetime is divisible by 4 this is ambiguous as in this case there are two different irreducible real spinor representations, and the number "N" is sometimes replaced by a pair of integers ("N"1, "N"2).
The supersymmetry algebra is sometimes regarded as a real super algebra, and sometimes as a complex algebra with a hermitian conjugation. These two views are essentially equivalent, as the real algebra can be constructed from the complex algebra by taking the skew-Hermitian elements, and the complex algebra can be constructed from the real one by taking tensor product with the complex numbers.
The bosonic part of the superalgebra is isomorphic to the product of the Poincaré algebra "P"."L" with the algebra "Z"×"B" of internal symmetries.
When "N">1 the algebra is said to have extended supersymmetry.
When "Z" is trivial, the subalgebra "P"."Q"."L" is the Super-Poincaré algebra.

</doc>
<doc id="28901" url="https://en.wikipedia.org/wiki?curid=28901" title="Symmetric group">
Symmetric group

In abstract algebra, the symmetric group S"n" on a finite set of "n" symbols is the group whose elements are all the permutation operations that can be performed on "n" distinct symbols, and whose group operation is the composition of such permutation operations, which are defined as bijective functions from the set of symbols to itself. Since there are "n"! ("n" factorial) possible permutation operations that can be performed on a tuple composed of "n" symbols, it follows that the order (the number of elements) of the symmetric group S"n" is "n"!.
Although symmetric groups can be defined on infinite sets as well, this article discusses only the finite symmetric groups: their applications, their elements, their conjugacy classes, a finite presentation, their subgroups, their automorphism groups, and their representation theory. For the remainder of this article, "symmetric group" will mean a symmetric group on a finite set.
The symmetric group is important to diverse areas of mathematics such as Galois theory, invariant theory, the representation theory of Lie groups, and combinatorics. Cayley's theorem states that every group "G" is isomorphic to a subgroup of the symmetric group on "G".
Definition and first properties.
The symmetric group on a finite set "X" is the group whose elements are all bijective functions from "X" to "X" and whose group operation is that of function composition. For finite sets, "permutations" and "bijective functions" refer to the same operation, namely rearrangement. The symmetric group of degree "n" is the symmetric group on the set 
The symmetric group on a set "X" is denoted in various ways including S"X", 𝔖"X", Σ"X", "X"! and Sym("X"). If "X" is the set then the symmetric group on "X" is also denoted S"n", 𝔖"n", Σ"n", and Sym("n").
Symmetric groups on infinite sets behave quite differently from symmetric groups on finite sets, and are discussed in , , and . This article concentrates on the finite symmetric groups.
The symmetric group on a set of "n" elements has order "n"! It is abelian if and only if . For and (the empty set and the singleton set) the symmetric group is trivial (note that this agrees with ), and in these cases the alternating group equals the symmetric group, rather than being an index two subgroup. The group S"n" is solvable if and only if . This is an essential part of the proof of the Abel–Ruffini theorem that shows that for every there are polynomials of degree "n" which are not solvable by radicals, i.e., the solutions cannot be expressed by performing a finite number of operations of addition, subtraction, multiplication, division and root extraction on the polynomial's coefficients.
Applications.
The symmetric group on a set of size "n" is the Galois group of the general polynomial of degree "n" and plays an important role in Galois theory. In invariant theory, the symmetric group acts on the variables of a multi-variate function, and the functions left invariant are the so-called symmetric functions. In the representation theory of Lie groups, the representation theory of the symmetric group plays a fundamental role through the ideas of Schur functors. In the theory of Coxeter groups, the symmetric group is the Coxeter group of type A"n" and occurs as the Weyl group of the general linear group. In combinatorics, the symmetric groups, their elements (permutations), and their representations provide a rich source of problems involving Young tableaux, plactic monoids, and the Bruhat order. Subgroups of symmetric groups are called permutation groups and are widely studied because of their importance in understanding group actions, homogeneous spaces, and automorphism groups of graphs, such as the Higman–Sims group and the Higman–Sims graph.
Elements.
The elements of the symmetric group on a set "X" are the permutations of "X".
Multiplication.
The group operation in a symmetric group is function composition, denoted by the symbol ∘ or simply by juxtaposition of the permutations. The composition of permutations "f" and "g", pronounced ""f" of "g"", maps any element "x" of "X" to "f"("g"("x")). Concretely, let (see permutation for an explanation of notation):
Applying "f" after "g" maps 1 first to 2 and then 2 to itself; 2 to 5 and then to 4; 3 to 4 and then to 5, and so on. So composing "f" and "g" gives
A cycle of length , taken to the "k"-th power, will decompose into "k" cycles of length "m": For example (, ),
Verification of group axioms.
To check that the symmetric group on a set "X" is indeed a group, it is necessary to verify the group axioms of closure, associativity, identity, and inverses. 1) The operation of function composition is closed in the set of permutations of the given set "X", 2) function composition is always associative, 3) The trivial bijection that assigns each element of "X" to itself serves as an identity for the group, and 4) Every bijection has an inverse function that undoes its action, and thus each element of a symmetric group does have an inverse which is a permutation too.
Transpositions.
A transposition is a permutation which exchanges two elements and keeps all others fixed; for example (1 3) is a transposition. Every permutation can be written as a product of transpositions; for instance, the permutation "g" from above can be written as "g" = (1 2)(2 5)(3 4). Since "g" can be written as a product of an odd number of transpositions, it is then called an odd permutation, whereas "f" is an even permutation.
The representation of a permutation as a product of transpositions is not unique; however, the number of transpositions needed to represent a given permutation is either always even or always odd. There are several short proofs of the invariance of this parity of a permutation.
The product of two even permutations is even, the product of two odd permutations is even, and all other products are odd. Thus we can define the sign of a permutation:
With this definition,
is a group homomorphism ({+1, –1} is a group under multiplication, where +1 is e, the neutral element). The kernel of this homomorphism, i.e. the set of all even permutations, is called the alternating group A"n". It is a normal subgroup of S"n", and for it has elements. The group S"n" is the semidirect product of A"n" and any subgroup generated by a single transposition.
Furthermore, every permutation can be written as a product of "adjacent transpositions", that is, transpositions of the form . For instance, the permutation "g" from above can also be written as . The sorting algorithm Bubble sort is an application of this fact. The representation of a permutation as a product of adjacent transpositions is also not unique.
Cycles.
A cycle of "length" "k" is a permutation "f" for which there exists an element "x" in {1...,"n"} such that "x", "f"("x"), "f"2("x"), ..., "f""k"("x") = "x" are the only elements moved by "f"; it is required that since with the element "x" itself would not be moved either. The permutation "h" defined by
is a cycle of length three, since , and , leaving 2 and 5 untouched. We denote such a cycle by , but it could equally well be written or by starting at a different point. The order of a cycle is equal to its length. Cycles of length two are transpositions. Two cycles are "disjoint" if they move disjoint subsets of elements. Disjoint cycles commute, e.g. in "S"6 we have . Every element of S"n" can be written as a product of disjoint cycles; this representation is unique up to the order of the factors, and the freedom present in representing each individual cycle by choosing its starting point.
Cycles admits the following conjugation property with any permutation formula_8, this property is often used to obtain its Generators and relations.
Special elements.
Certain elements of the symmetric group of {1, 2, ..., "n"} are of particular interest (these can be generalized to the symmetric group of any finite totally ordered set, but not to that of an unordered set).
The is the one given by:
This is the unique maximal element with respect to the Bruhat order and the
longest element in the symmetric group with respect to generating set consisting of the adjacent transpositions , .
This is an involution, and consists of formula_11 (non-adjacent) transpositions
so it thus has sign:
which is 4-periodic in "n".
In S2"n", the "perfect shuffle" is the permutation that splits the set into 2 piles and interleaves them. Its sign is also formula_15
Note that the reverse on "n" elements and perfect shuffle on 2"n" elements have the same sign; these are important to the classification of Clifford algebras, which are 8-periodic.
Conjugacy classes.
The conjugacy classes of S"n" correspond to the cycle structures of permutations; that is, two elements of S"n" are conjugate in S"n" if and only if they consist of the same number of disjoint cycles of the same lengths. For instance, in S5, (1 2 3)(4 5) and (1 4 3)(2 5) are conjugate; (1 2 3)(4 5) and (1 2)(4 5) are not. A conjugating element of S"n" can be constructed in "two line notation" by placing the "cycle notations" of the two conjugate permutations on top of one another. Continuing the previous example:
which can be written as the product of cycles, namely: (2 4).
This permutation then relates (1 2 3)(4 5) and (1 4 3)(2 5) via conjugation, i.e.
It is clear that such a permutation is not unique.
Low degree groups.
The low-degree symmetric groups have simpler and exceptional structure, and often must be treated separately.
Maps between symmetric groups.
Other than the trivial map and the sign map , the most notable homomorphisms between symmetric groups, in order of relative dimension, are:
There are also a host of other homomorphisms S"m" → S"n" where .
Properties.
Symmetric groups are Coxeter groups and reflection groups. They can be realized as a group of reflections with respect to hyperplanes . Braid groups B"n" admit symmetric groups S"n" as quotient groups.
Cayley's theorem states that every group "G" is isomorphic to a subgroup of the symmetric group on the elements of "G", as a group acts on itself faithfully by (left or right) multiplication.
Relation with alternating group.
For , the alternating group A"n" is simple, and the induced quotient is the sign map: which is split by taking a transposition of two elements. Thus S"n" is the semidirect product , and has no other proper normal subgroups, as they would intersect An in either the identity (and thus themselves be the identity or a 2-element group, which is not normal), or in A"n" (and thus themselves be A"n" or S"n").
S"n" acts on its subgroup A"n" by conjugation, and for , S"n" is the full automorphism group of A"n": Aut(A"n") ≅ S"n". Conjugation by even elements are inner automorphisms of A"n" while the outer automorphism of A"n" of order 2 corresponds to conjugation by an odd element. For , there is an exceptional outer automorphism of A"n" so S"n" is not the full automorphism group of A"n".
Conversely, for , S"n" has no outer automorphisms, and for it has no center, so for it is a complete group, as discussed in automorphism group, below.
For , S"n" is an almost simple group, as it lies between the simple group A"n" and its group of automorphisms.
formula_18 can be embedded into formula_19 by appending the transposition formula_20 to all odd permutations, while embedding into formula_21 is impossible for formula_22.
Generators and relations.
The symmetric group on "n"-letters, S"n", may be described as follows. It has generators: formula_23 and relations:
One thinks of formula_27 as swapping the "i"th and position.
Other popular generating sets include the set of transpositions that swap 1 and "i" for and a set containing any "n"-cycle and a 2-cycle of adjacent elements in the n-cycle.
Subgroup structure.
A subgroup of a symmetric group is called a permutation group.
Normal subgroups.
The normal subgroups of the finite symmetric groups are well understood. If , S"n" has at most 2 elements, and so has no nontrivial proper subgroups. The alternating group of degree "n" is always a normal subgroup, a proper one for and nontrivial for ; for it is in fact the only non-identity proper normal subgroup of S"n", except when where there is one additional such normal subgroup, which is isomorphic to the Klein four group.
The symmetric group on an infinite set does not have an associated alternating group: not all elements can be written as a (finite) product of transpositions. However it does contain a normal subgroup "S" of permutations that fix all but finitely many elements, and such permutations can be classified as either even or odd. The even elements of "S" form the alternating subgroup "A" of "S", and since "A" is even a characteristic subgroup of "S", it is also a normal subgroup of the full symmetric group of the infinite set. The groups "A" and "S" are the only non-identity proper normal subgroups of the symmetric group on a countably infinite set. For more details see or .
Maximal subgroups.
The maximal subgroups of the finite symmetric groups fall into three classes: the intransitive, the imprimitive, and the primitive. The intransitive maximal subgroups are exactly those of the form for . The imprimitive maximal subgroups are exactly those of the form Sym("k") wr Sym("n"/"k") where is a proper divisor of "n" and "wr" denotes the wreath product acting imprimitively. The primitive maximal subgroups are more difficult to identify, but with the assistance of the O'Nan–Scott theorem and the classification of finite simple groups, gave a fairly satisfactory description of the maximal subgroups of this type according to .
Sylow subgroups.
The Sylow subgroups of the symmetric groups are important examples of "p"-groups. They are more easily described in special cases first:
The Sylow "p"-subgroups of the symmetric group of degree "p" are just the cyclic subgroups generated by "p"-cycles. There are such subgroups simply by counting generators. The normalizer therefore has order "p"·("p"−1) and is known as a Frobenius group (especially for ), and is the affine general linear group, .
The Sylow "p"-subgroups of the symmetric group of degree "p"2 are the wreath product of two cyclic groups of order "p". For instance, when "p" = 3, a Sylow 3-subgroup of Sym(9) is generated by "a" = (1 4 7)(2 5 8)(3 6 9) and the elements "x" = (1 2 3), "y" = (4 5 6), "z" = (7 8 9), and every element of the Sylow 3-subgroup has the form "a""i""x""j""y""k""z""l" for 0 ≤ "i","j","k","l" ≤ 2.
The Sylow "p"-subgroups of the symmetric group of degree "p""n" are sometimes denoted W"p"("n"), and using this notation one has that is the wreath product of W"p"("n") and W"p"(1).
In general, the Sylow "p"-subgroups of the symmetric group of degree "n" are a direct product of "a""i" copies of W"p"("i"), where 0 ≤ "ai" ≤ "p" − 1 and "n" = "a"0 + "p"·"a"1 + ... + "p"k·"a""k".
For instance, W2(1) = C2 and W2(2) = D8, the dihedral group of order 8, and so a Sylow 2-subgroup of the symmetric group of degree 7 is generated by { (1,3)(2,4), (1,2), (3,4), (5,6) } and is isomorphic to D8 × C2.
These calculations are attributed to and described in more detail in . Note however that attributes the result to an 1844 work of Cauchy, and mentions that it is even covered in textbook form in .
Transitive subgroups.
A transitive subgroup of S"n" is a subgroup whose action on {1, 2, ..., "n"} is transitive. For example, the Galois group of a (finite) Galois extension is a transitive subgroup of S"n", for some "n".
Automorphism group.
For , S"n" is a complete group: its center and outer automorphism group are both trivial.
For , the automorphism group is trivial, but S2 is not trivial: it is isomorphic to C2, which is abelian, and hence the center is the whole group.
For , it has an outer automorphism of order 2: , and the automorphism group is a semidirect product
In fact, for any set "X" of cardinality other than 6, every automorphism of the symmetric group on "X" is inner, a result first due to according to .
Homology.
The group homology of S"n" is quite regular and stabilizes: the first homology (concretely, the abelianization) is:
The first homology group is the abelianization, and corresponds to the sign map S"n" → S2 which is the abelianization for "n" ≥ 2; for "n" < 2 the symmetric group is trivial. This homology is easily computed as follows: S"n" is generated by involutions (2-cycles, which have order 2), so the only non-trivial maps are to S2 and all involutions are conjugate, hence map to the same element in the abelianization (since conjugation is trivial in abelian groups). Thus the only possible maps send an involution to 1 (the trivial map) or to −1 (the sign map). One must also show that the sign map is well-defined, but assuming that, this gives the first homology of S"n".
The second homology (concretely, the Schur multiplier) is:
This was computed in , and corresponds to the double cover of the symmetric group, 2 · S"n".
Note that the exceptional low-dimensional homology of the alternating group (formula_31 corresponding to non-trivial abelianization, and formula_32 due to the exceptional 3-fold cover) does not change the homology of the symmetric group; the alternating group phenomena do yield symmetric group phenomena – the map formula_33 extends to formula_34 and the triple covers of A6 and A7 extend to triple covers of S6 and S7 – but these are not "homological" – the map formula_35 does not change the abelianization of S4, and the triple covers do not correspond to homology either.
The homology "stabilizes" in the sense of stable homotopy theory: there is an inclusion map , and for fixed "k", the induced map on homology is an isomorphism for sufficiently high "n". This is analogous to the homology of families Lie groups stabilizing.
The homology of the infinite symmetric group is computed in , with the cohomology algebra forming a Hopf algebra.
Representation theory.
The representation theory of the symmetric group is a particular case of the representation theory of finite groups, for which a concrete and detailed theory can be obtained. This has a large area of potential applications, from symmetric function theory to problems of quantum mechanics for a number of identical particles.
The symmetric group S"n" has order "n"!. Its conjugacy classes are labeled by partitions of "n". Therefore according to the representation theory of a finite group, the number of inequivalent irreducible representations, over the complex numbers, is equal to the number of partitions of "n". Unlike the general situation for finite groups, there is in fact a natural way to parametrize irreducible representation by the same set that parametrizes conjugacy classes, namely by partitions of "n" or equivalently Young diagrams of size "n".
Each such irreducible representation can be realized over the integers (every permutation acting by a matrix with integer coefficients); it can be explicitly constructed by computing the Young symmetrizers acting on a space generated by the Young tableaux of shape given by the Young diagram.
Over other fields the situation can become much more complicated. If the field "K" has characteristic equal to zero or greater than "n" then by Maschke's theorem the group algebra "K"S"n" is semisimple. In these cases the irreducible representations defined over the integers give the complete set of irreducible representations (after reduction modulo the characteristic if necessary).
However, the irreducible representations of the symmetric group are not known in arbitrary characteristic. In this context it is more usual to use the language of modules rather than representations. The representation obtained from an irreducible representation defined over the integers by reducing modulo the characteristic will not in general be irreducible. The modules so constructed are called "Specht modules", and every irreducible does arise inside some such module. There are now fewer irreducibles, and although they can be classified they are very poorly understood. For example, even their dimensions are not known in general. 
The determination of the irreducible modules for the symmetric group over an arbitrary field is widely regarded as one of the most important open problems in representation theory.

</doc>
<doc id="126474" url="https://en.wikipedia.org/wiki?curid=126474" title="Symmetric matrix">
Symmetric matrix

In linear algebra, a symmetric matrix is a square matrix that is equal to its transpose. Formally, matrix "A" is symmetric if
Because equal matrices have equal dimensions, only square matrices can be symmetric. 
The entries of a symmetric matrix are symmetric with respect to the main diagonal. So if the entries are written as "A" = ("a""ij"), then "a""ij" = a"ji", for all indices "i" and "j". 
The following 3×3 matrix is symmetric:
Every square diagonal matrix is symmetric, since all off-diagonal entries are zero. Similarly, each diagonal element of a skew-symmetric matrix must be zero, since each is its own negative.
In linear algebra, a real symmetric matrix represents a self-adjoint operator over a real inner product space. The corresponding object for a complex inner product space is a Hermitian matrix with complex-valued entries, which is equal to its conjugate transpose. Therefore, in linear algebra over the complex numbers, it is often assumed that a symmetric matrix refers to one which has real-valued entries. Symmetric matrices appear naturally in a variety of applications, and typical numerical linear algebra software makes special accommodations for them.
Properties.
The sum and difference of two symmetric matrices is again symmetric, but this is not always true for the product: given symmetric matrices "A" and "B", then "AB" is symmetric if and only if "A" and "B" commute, i.e., if "AB" = "BA". So for integer "n", "An" is symmetric if "A" is symmetric. If "A"−1 exists, it is symmetric if and only if "A" is symmetric.
Let Mat"n" denote the space of matrices. A symmetric "n" × "n" matrix is determined by "n"("n" + 1)/2 scalars (the number of entries on or above the main diagonal). Similarly, a skew-symmetric matrix is determined by "n"("n" − 1)/2 scalars (the number of entries above the main diagonal). If Sym"n" denotes the space of symmetric matrices and Skew"n" the space of skew-symmetric matrices then and }, i.e.
where ⊕ denotes the direct sum. Let then
Notice that and This is true for every square matrix "X" with entries from any field whose characteristic is different from 2.
Any matrix congruent to a symmetric matrix is again symmetric: if "X" is a symmetric matrix then so is "AXA"T for any matrix "A". A symmetric matrix is necessarily a normal matrix.
Real symmetric matrices.
Denote by formula_5 the standard inner product on R"n". The real "n"-by-"n" matrix "A" is symmetric if and only if
Since this definition is independent of the choice of basis, symmetry is a property that depends only on the linear operator A and a choice of inner product. This characterization of symmetry is useful, for example, in differential geometry, for each tangent space to a manifold may be endowed with an inner product, giving rise to what is called a Riemannian manifold. Another area where this formulation is used is in Hilbert spaces.
The finite-dimensional spectral theorem says that any symmetric matrix whose entries are real can be diagonalized by an orthogonal matrix. More explicitly: For every symmetric real matrix "A" there exists a real orthogonal matrix "Q" such that "D" = "Q"T"AQ" is a diagonal matrix. Every symmetric matrix is thus, up to choice of an orthonormal basis, a diagonal matrix.
If "A" and "B" are "n"×"n" real symmetric matrices that commute, then they can be simultaneously diagonalized: there exists a basis of formula_7 such that every element of the basis is an eigenvector for both "A" and "B".
Every real symmetric matrix is Hermitian, and therefore all its eigenvalues are real. (In fact, the eigenvalues are the entries in the diagonal matrix "D" (above), and therefore "D" is uniquely determined by "A" up to the order of its entries.) Essentially, the property of being symmetric for real matrices corresponds to the property of being Hermitian for complex matrices.
Complex symmetric matrices.
A complex symmetric matrix can be `diagonalized' using a unitary matrix: thus if "A" is a complex symmetric matrix, there is a unitary matrix "U" such that
"UAU"T is a real diagonal matrix. This result is referred to as the Autonne–Takagi factorization. It was originally proved by Léon Autonne (1915) and Teiji Takagi (1925) and rediscovered with different proofs by several other mathematicians. In fact the matrix "B" = "A"H"A" is Hermitian and non-negative, so there is a unitary matrix "V" such that "V"H"BV" is diagonal with non-negative real entries. Thus "C" = "V""T""AV" is complex symmetric with "C"H"C" real. Writing "C" = "X" + "iY" with "X" and "Y" real symmetric matrices, "C"H"C" = "X"2 − "Y"2 + i 
Decomposition.
Using the Jordan normal form, one can prove that every square real matrix can be written as a product of two real symmetric matrices, and every square complex matrix can be written as a product of two complex symmetric matrices.
Every real non-singular matrix can be uniquely factored as the product of an orthogonal matrix and a symmetric positive definite matrix, which is called a polar decomposition. Singular matrices can also be factored, but not uniquely.
Cholesky decomposition states that every real positive-definite symmetric matrix "A" is a product of a lower-triangular matrix "L" and its transpose, formula_8.
If the matrix is symmetric indefinite, it may be still decomposed as formula_9 where formula_10 is
a permutation matrix (arising from the need to pivot), formula_11 a lower unit triangular matrix, and formula_12 
is a direct sum of symmetric 1×1 and 2×2 blocks, which is called Bunch-Kaufman decomposition 
A complex symmetric matrix may not be diagonalizable by similarity; every real symmetric matrix is diagonalizable by a real orthogonal similarity.
Every complex symmetric matrix "A" can be diagonalized by unitary congruence
where "Q" is an unitary matrix. If A is real, the matrix "Q" is a real orthogonal matrix, (the columns of which are eigenvectors of "A"), and "Λ" is real and diagonal (having the eigenvalues of "A" on the diagonal). To see orthogonality, suppose formula_14 and formula_15 are eigenvectors corresponding to distinct eigenvalues formula_16, formula_17. Then
Since formula_16 and formula_17 are distinct, thus we have formula_21 the orthogonality.
Hessian.
Symmetric "n"-by-"n" matrices of real functions appear as the Hessians of twice continuously differentiable functions of "n" real variables. 
Every quadratic form "q" on R"n" can be uniquely written in the form "q"(x) = xT"A"x with a symmetric "n"-by-"n" matrix "A". Because of the above spectral theorem, one can then say that every quadratic form, up to the choice of an orthonormal basis of R"n", "looks like"
with real numbers λ"i". This considerably simplifies the study of quadratic forms, as well as the study of the level sets {x : "q"(x) = 1} which are generalizations of conic sections.
This is important partly because the second-order behavior of every smooth multi-variable function is described by the quadratic form belonging to the function's Hessian; this is a consequence of Taylor's theorem.
Symmetrizable matrix.
An "n"-by-"n" matrix "A" is said to be symmetrizable if there exist an invertible diagonal matrix "D" and symmetric matrix "S" such that 
The transpose of a symmetrizable matrix is symmetrizable, since and is symmetric. A matrix is symmetrizable if and only if the following conditions are met:
See also.
Other types of symmetry or pattern in square matrices have special names; see for example:
See also symmetry in mathematics.

</doc>
<doc id="53741" url="https://en.wikipedia.org/wiki?curid=53741" title="Symmetry">
Symmetry

Symmetry (from Greek συμμετρία "symmetria" "agreement in dimensions, due proportion, arrangement") in everyday language refers to a sense of harmonious and beautiful proportion and balance. In mathematics, "symmetry" has a more precise definition, that an object is invariant to a transformation, such as reflection but including other transforms too. Although these two meanings of "symmetry" can sometimes be told apart, they are related, so they are here discussed together.
Mathematical symmetry may be observed with respect to the passage of time; as a spatial relationship; through geometric transformations such as scaling, reflection, and rotation; through other kinds of functional transformations; and as an aspect of abstract objects, theoretic models, language, music and even knowledge itself.
This article describes symmetry from three perspectives: in mathematics, including geometry, the most familiar type of symmetry for many people; in science and nature; and in the arts, covering architecture, art and music.
The opposite of symmetry is asymmetry.
In mathematics.
In geometry.
A geometric shape or object is symmetric if it can be divided into two or more identical pieces that are arranged in an organized fashion. This means that an object is symmetric if there is a transformation that moves individual pieces of the object but doesn't change the overall shape. The type of symmetry is determined by the way the pieces are organized, or by the type of transformation:
In logic.
A dyadic relation "R" is symmetric if and only if, whenever it's true that "Rab", it's true that "Rba". Thus, "is the same age as" is symmetrical, for if Paul is the same age as Mary, then Mary is the same age as Paul.
Symmetric binary logical connectives are "and" (∧, or &), "or" (∨, or |), "biconditional" (if and only if) (↔), "nand" (not-and, or ⊼), "xor" (not-biconditional, or ⊻), and "nor" (not-or, or ⊽).
Other areas of mathematics.
Generalizing from geometrical symmetry in the previous section, we say that a mathematical object is "symmetric" with respect to a given mathematical operation, if, when applied to the object, this operation preserves some property of the object. The set of operations that preserve a given property of the object form a group.
In general, every kind of structure in mathematics will have its own kind of symmetry. Examples include even and odd functions in calculus; the symmetric group in abstract algebra; symmetric matrices in linear algebra; and the Galois group in Galois theory. In statistics, it appears as symmetric probability distributions, and as skewness, asymmetry of distributions.
In science and nature.
In physics.
Symmetry in physics has been generalized to mean invariance—that is, lack of change—under any kind of transformation, for example arbitrary coordinate transformations. This concept has become one of the most powerful tools of theoretical physics, as it has become evident that practically all laws of nature originate in symmetries. In fact, this role inspired the Nobel laureate PW Anderson to write in his widely read 1972 article "More is Different" that "it is only slightly overstating the case to say that physics is the study of symmetry." See Noether's theorem (which, in greatly simplified form, states that for every continuous mathematical symmetry, there is a corresponding conserved quantity; a conserved current, in Noether's original language); and also, Wigner's classification, which says that the symmetries of the laws of physics determine the properties of the particles found in nature.
Important symmetries in physics include continuous symmetries and discrete symmetries of spacetime; internal symmetries of particles; and supersymmetry of physical theories.
In biology.
Bilateral animals, including humans, are more or less symmetric with respect to the sagittal plane which divides the body into left and right halves. Animals that move in one direction necessarily have upper and lower sides, head and tail ends, and therefore a left and a right. The head becomes specialized with a mouth and sense organs, and the body becomes bilaterally symmetric for the purpose of movement, with symmetrical pairs of muscles and skeletal elements, though internal organs often remain asymmetric.
Plants and sessile (attached) animals such as sea anemones often have radial or rotational symmetry, which suits them because food or threats may arrive from any direction. Fivefold symmetry is found in the echinoderms, the group that includes starfish, sea urchins, and sea lilies.
In chemistry.
Symmetry is important to chemistry because it undergirds essentially all "specific" interactions between molecules in nature (i.e., via the interaction of natural and human-made chiral molecules with inherently chiral biological systems). The control of the symmetry of molecules produced in modern chemical synthesis contributes to the ability of scientists to offer therapeutic interventions with minimal side effects. A rigorous understanding of symmetry explains fundamental observations in quantum chemistry, and in the applied areas of spectroscopy and crystallography. The theory and application of symmetry to these areas of physical science draws heavily on the mathematical area of group theory.
In social interactions.
People observe the symmetrical nature, often including asymmetrical balance, of social interactions in a variety of contexts. These include assessments of Reciprocity, empathy, sympathy, apology, dialog, respect, justice, and revenge.
Reflective equilibrium is the balance that may be attained through deliberative mutual adjustment among general principles and specific judgments.
Symmetrical interactions send the moral message "we are all the same" while asymmetrical interactions may send the message "I am special; better than you." Peer relationships, such as can be governed by the golden rule, are based on symmetry, whereas power relationships are based on asymmetry. Symmetrical relationships can to some degree be maintained by simple (game theory) strategies seen in symmetric games such as tit for tat.
In the arts.
In architecture.
Symmetry finds its ways into architecture at every scale, from the overall external views of buildings such as Gothic cathedrals and The White House, through the layout of the individual floor plans, and down to the design of individual building elements such as tile mosaics. Islamic buildings such as the Taj Mahal and the Lotfollah mosque make elaborate use of symmetry both in their structure and in their ornamentation. Moorish buildings like the Alhambra are ornamented with complex patterns made using translational and reflection symmetries as well as rotations.
It has been said that only bad architects rely on a "symmetrical layout of blocks, masses and structures"; Modernist architecture, starting with International style, relies instead on "wings and balance of masses".
In pottery and metal vessels.
Since the earliest uses of pottery wheels to help shape clay vessels, pottery has had a strong relationship to symmetry. Pottery created using a wheel acquires full rotational symmetry in its cross-section, while allowing substantial freedom of shape in the vertical direction. Upon this inherently symmetrical starting point, potters from ancient times onwards have added patterns that modify the rotational symmetry to achieve visual objectives.
Cast metal vessels lacked the inherent rotational symmetry of wheel-made pottery, but otherwise provided a similar opportunity to decorate their surfaces with patterns pleasing to those who used them. The ancient Chinese, for example, used symmetrical patterns in their bronze castings as early as the 17th century BC. Bronze vessels exhibited both a bilateral main motif and a repetitive translated border design.
In quilts.
As quilts are made from square blocks (usually 9, 16, or 25 pieces to a block) with each smaller piece usually consisting of fabric triangles, the craft lends itself readily to the application of symmetry.
In carpets and rugs.
A long tradition of the use of symmetry in carpet and rug patterns spans a variety of cultures. American Navajo Indians used bold diagonals and rectangular motifs. Many Oriental rugs have intricate reflected centers and borders that translate a pattern. Not surprisingly, rectangular rugs typically use quadrilateral symmetry—that is, motifs that are reflected across both the horizontal and vertical axes.
In music.
Symmetry is not restricted to the visual arts. Its role in the history of music touches many aspects of the creation and perception of music.
Musical form.
Symmetry has been used as a formal constraint by many composers, such as the arch (swell) form (ABCBA) used by Steve Reich, Béla Bartók, and James Tenney. In classical music, Bach used the symmetry concepts of permutation and invariance.
Pitch structures.
Symmetry is also an important consideration in the formation of scales and chords, traditional or tonal music being made up of non-symmetrical groups of pitches, such as the diatonic scale or the major chord. Symmetrical scales or chords, such as the whole tone scale, augmented chord, or diminished seventh chord (diminished-diminished seventh), are said to lack direction or a sense of forward motion, are ambiguous as to the key or tonal center, and have a less specific diatonic functionality. However, composers such as Alban Berg, Béla Bartók, and George Perle have used axes of symmetry and/or interval cycles in an analogous way to keys or non-tonal tonal centers.
 explains "C–E, D–F♯, [and] Eb–G, are different instances of the same interval … the other kind of identity. … has to do with axes of symmetry. C–E belongs to a family of symmetrically related dyads as follows:"
Thus in addition to being part of the interval-4 family, C–E is also a part of the sum-4 family (with C equal to 0).
Interval cycles are symmetrical and thus non-diatonic. However, a seven pitch segment of C5 (the cycle of fifths, which are enharmonic with the cycle of fourths) will produce the diatonic major scale. Cyclic tonal progressions in the works of Romantic composers such as Gustav Mahler and Richard Wagner form a link with the cyclic pitch successions in the atonal music of Modernists such as Bartók, Alexander Scriabin, Edgard Varèse, and the Vienna school. At the same time, these progressions signal the end of tonality.
The first extended composition consistently based on symmetrical pitch relations was probably Alban Berg's "Quartet", Op. 3 (1910).
Equivalency.
Tone rows or pitch class sets which are invariant under retrograde are horizontally symmetrical, under inversion vertically. See also Asymmetric rhythm.
In other arts and crafts.
Symmetries appear in the design of objects of all kinds. Examples include beadwork, furniture, sand paintings, knotwork, masks, and musical instruments. Symmetries are central to the art of M.C. Escher and the many applications of tessellation in art and craft forms such as wallpaper, ceramic tilework, batik, ikat, carpet-making, and many kinds of textile and embroidery patterns.
In aesthetics.
The relationship of symmetry to aesthetics is complex. Humans find bilateral symmetry in faces physically attractive; it indicates health and genetic fitness. Opposed to this is the tendency for excessive symmetry to be perceived as boring or uninteresting. People prefer shapes that have some symmetry, but enough complexity to make them interesting.
In literature.
Symmetry can be found in various forms in literature, a simple example being the palindrome where a brief text reads the same forwards or backwards. Stories may have a symmetrical structure, as in the rise:fall pattern of "Beowulf".

</doc>
<doc id="2715469" url="https://en.wikipedia.org/wiki?curid=2715469" title="Symmetry (physics)">
Symmetry (physics)

In physics, a symmetry of a physical system is a physical or mathematical feature of the system (observed or intrinsic) that is preserved or remains unchanged under some transformation.
A family of particular transformations may be "continuous" (such as rotation of a circle) or "discrete" (e.g., reflection of a bilaterally symmetric figure, or rotation of a regular polygon). Continuous and discrete transformations give rise to corresponding types of symmetries. Continuous symmetries can be described by Lie groups while discrete symmetries are described by finite groups (see Symmetry group).
These two concepts, Lie and finite groups, are the foundation for the fundamental theories of modern physics. Symmetries are frequently amenable to mathematical formulations such as group representations and can, in addition, be exploited to simplify many problems.
Arguably the most important example of a symmetry in physics is that the speed of light has the same value in all frames of reference, which is known in mathematical terms as Poincare group, the symmetry group of special relativity. Another important example is the invariance of the form of physical laws under arbitrary differentiable coordinate transformations, which is an important idea in general relativity.
Symmetry as invariance.
Invariance is specified mathematically by transformations that leave some quantity unchanged. This idea can apply to basic real-world observations. For example, temperature may be constant throughout a room. Since the temperature is independent of position within the room, the temperature is "invariant" under a shift in the measurer's position.
Similarly, a uniform sphere rotated about its center will appear exactly as it did before the rotation. The sphere is said to exhibit spherical symmetry. A rotation about any axis of the sphere will preserve how the sphere "looks".
Invariance in force.
The above ideas lead to the useful idea of "invariance" when discussing observed physical symmetry; this can be applied to symmetries in forces as well.
For example, an electric field due to a wire is said to exhibit cylindrical symmetry, because the electric field strength at a given distance "r" from the electrically charged wire of infinite length will have the same magnitude at each point on the surface of a cylinder (whose axis is the wire) with radius "r". Rotating the wire about its own axis does not change its position or charge density, hence it will preserve the field. The field strength at a rotated position is the same. Suppose some configuration of charges (may be non-stationary) produce an electric field in some direction, then rotating the configuration of the charges (without disturbing the internal dynamics that produces the particular field) will lead to a net rotation of the direction of the electric field. These two properties are interconnected through the more general property that rotating "any" system of charges causes a corresponding rotation of the electric field.
In Newton's theory of mechanics, given two bodies, each with mass "m", starting from rest at the origin and moving along the "x"-axis in opposite directions, one with speed "v"1 and the other with speed "v"2 the total kinetic energy of the system (as calculated from an observer at the origin) is and remains the same if the velocities are interchanged. The total kinetic energy is preserved under a reflection in the "y"-axis.
The last example above illustrates another way of expressing symmetries, namely through the equations that describe some aspect of the physical system. The above example shows that the total kinetic energy will be the same if "v"1 and "v"2 are interchanged.
Local and global symmetries.
Symmetries may be broadly classified as "global" or "local". A "global symmetry" is one that holds at all points of spacetime, whereas a "local symmetry" is one that has a different symmetry transformation at different points of spacetime; specifically a local symmetry transformation is parameterised by the spacetime co-ordinates. Local symmetries play an important role in physics as they form the basis for gauge theories.
Continuous symmetries.
The two examples of rotational symmetry described above - spherical and cylindrical - are each instances of continuous symmetry. These are characterised by invariance following a continuous change in the geometry of the system. For example, the wire may be rotated through any angle about its axis and the field strength will be the same on a given cylinder. Mathematically, continuous symmetries are described by continuous or smooth functions. An important subclass of continuous symmetries in physics are spacetime symmetries.
Spacetime symmetries.
Continuous "spacetime symmetries" are symmetries involving transformations of space and time. These may be further classified as "spatial symmetries", involving only the spatial geometry associated with a physical system; "temporal symmetries", involving only changes in time; or "spatio-temporal symmetries", involving changes in both space and time.
Mathematically, spacetime symmetries are usually described by smooth vector fields on a smooth manifold. The underlying local diffeomorphisms associated with the vector fields correspond more directly to the physical symmetries, but the vector fields themselves are more often used when classifying the symmetries of the physical system.
Some of the most important vector fields are Killing vector fields which are those spacetime symmetries that preserve the underlying metric structure of a manifold. In rough terms, Killing vector fields preserve the distance between any two points of the manifold and often go by the name of isometries.
Discrete symmetries.
A discrete symmetry is a symmetry that describes non-continuous changes in a system. For example, a square possesses discrete rotational symmetry, as only rotations by multiples of right angles will preserve the square's original appearance. Discrete symmetries sometimes involve some type of 'swapping', these swaps usually being called "reflections" or "interchanges".
C, P, and T symmetries.
The Standard model of particle physics has three related natural near-symmetries. These state that the actual universe about us is indistinguishable from one where:
T-symmetry is counterintuitive (surely the future and the past are not symmetrical) but explained by the fact that the Standard model describes local properties, not global ones like entropy. To properly reverse the direction of time, one would have to put the big bang and the resulting low-entropy state in the "future." Since we perceive the "past" ("future") as having lower (higher) entropy than the present (see perception of time), the inhabitants of this hypothetical time-reversed universe would perceive the future in the same way as we perceive the past.
These symmetries are near-symmetries because each is broken in the present-day universe. However, the Standard Model predicts that the combination of the three (that is, the simultaneous application of all three transformations) must be a symmetry, called CPT symmetry. In the 4 dimensional matrix description of P,T is through a diagonal matrix, the negative identity, as well as C. Hence CPT is the identity operator. CP violation, the violation of the combination of C- and P-symmetry, is necessary for the presence of significant amounts of baryonic matter in the universe. CP violation is a fruitful area of current research in particle physics.
Supersymmetry.
A type of symmetry known as supersymmetry has been used to try to make theoretical advances in the standard model. Supersymmetry is based on the idea that there is another physical symmetry beyond those already developed in the standard model, specifically a symmetry between bosons and fermions. Supersymmetry asserts that each type of boson has, as a supersymmetric partner, a fermion, called a superpartner, and vice versa. Supersymmetry has not yet been experimentally verified: no known particle has the correct properties to be a superpartner of any other known particle. If superpartners exist they must have masses greater than current particle accelerators can generate.
Mathematics of physical symmetry.
The transformations describing physical symmetries typically form a mathematical group. Group theory is an important area of mathematics for physicists.
Continuous symmetries are specified mathematically by "continuous groups" (called Lie groups). Many physical symmetries are isometries and are specified by symmetry groups. Sometimes this term is used for more general types of symmetries. The set of all proper rotations (about any angle) through any axis of a sphere form a Lie group called the special orthogonal group formula_13. (The "3" refers to the three-dimensional space of an ordinary sphere.) Thus, the symmetry group of the sphere with proper rotations is formula_13. Any rotation preserves distances on the surface of the ball. The set of all Lorentz transformations form a group called the Lorentz group (this may be generalised to the Poincaré group).
Discrete symmetries are described by discrete groups. For example, the symmetries of an equilateral triangle are described by the symmetric group formula_15.
An important type of physical theory based on "local" symmetries is called a "gauge" theory and the symmetries natural to such a theory are called gauge symmetries. Gauge symmetries in the Standard model, used to describe three of the fundamental interactions, are based on the SU(3) × SU(2) × U(1) group. (Roughly speaking, the symmetries of the SU(3) group describe the strong force, the SU(2) group describes the weak interaction and the U(1) group describes the electromagnetic force.)
Also, the reduction by symmetry of the energy functional under the action by a group and spontaneous symmetry breaking of transformations of symmetric groups appear to elucidate topics in particle physics (for example, the unification of electromagnetism and the weak force in physical cosmology).
Conservation laws and symmetry.
The symmetry properties of a physical system are intimately related to the conservation laws characterizing that system. Noether's theorem gives a precise description of this relation. The theorem states that each continuous symmetry of a physical system implies that some physical property of that system is conserved. Conversely, each conserved quantity has a corresponding symmetry. For example, the isometry of space gives rise to conservation of (linear) momentum, and isometry of time gives rise to conservation of energy.
The following table summarizes some fundamental symmetries and the associated conserved quantity.
Mathematics.
Continuous symmetries in physics preserve transformations. One can specify a symmetry by showing how a very small transformation affects various particle fields. The commutator of two of these infinitessimal transformations are equivalent to a third infinitessimal transformation of the same kind hence they form a Lie algebra.
A general coordinate transformation (also known as a diffeomorphism) has the infinitessimal effect on a scalar, spinor and vector field for example:
formula_16
formula_17
formula_18
for a general field, formula_19. Without gravity only the Poincaré symmetries are preserved which restricts formula_19 to be of the form:
formula_21
where M is an antisymmetric matrix (giving the Lorentz and rotational symmetries) and P is a general vector (giving the translational symmetries). Other symmetries affect multiple fields simultaneously. For example local gauge transformations apply to both a vector and spinor field:
formula_22
formula_23
where formula_24 are generators of a particular Lie group. So far the transformations on the right have only included fields of the same type. Supersymmetries are defined according to how the mix fields of "different" types.
Another symmetry which is part of some theories of physics and not in others is scale invariance which involve Weyl transformations of the following kind:
formula_25
If the fields have this symmetry then it can be shown that the field theory is almost certainly conformally invariant also. This means that in the absence of gravity h(x) would restricted to the form:
formula_26
with D generating scale transformations and K generating special conformal transformations. For example N=4 super-Yang-Mills theory has this symmetry while General Relativity doesn't although other theories of gravity such as conformal gravity do. The 'action' of a field theory is an invariant under all the symmetries of the theory. Much of modern theoretical physics is to do with speculating on the various symmetries the Universe may have and finding the invariants to construct field theories as models.
In string theories, since a string can be decomposed into an infinite number of particle fields, the symmetries on the string world sheet is equivalent to special transformations which mix an infinite number of fields.

</doc>
<doc id="1240378" url="https://en.wikipedia.org/wiki?curid=1240378" title="Symmetry breaking">
Symmetry breaking

In physics, symmetry breaking is a phenomenon in which (infinitesimally) small fluctuations acting on a system crossing a critical point decide the system's fate, by determining which branch of a bifurcation is taken. To an outside observer unaware of the fluctuations (or "noise"), the choice will appear arbitrary. This process is called symmetry "breaking", because such transitions usually bring the system from a symmetric but disorderly state into one or more definite states. Symmetry breaking is supposed to play a major role in pattern formation.
In 1972, Nobel laureate P.W. Anderson used the idea of symmetry breaking to show some of the drawbacks of reductionism in his paper titled "More is different" in "Science".
Symmetry breaking can be distinguished into two types, explicit symmetry breaking and spontaneous symmetry breaking, characterized by whether the equations of motion fail to be invariant or the ground state fails to be invariant.
Explicit symmetry breaking.
In explicit symmetry breaking, the equations of motion describing a system are not invariant under the broken symmetry.
Spontaneous symmetry breaking.
In spontaneous symmetry breaking, the equations of motion of the system are invariant, but the system is not because the background (spacetime) of the system, its vacuum, is non-invariant. Such a symmetry breaking is parametrized by an order parameter. A special case of this type of symmetry breaking is dynamical symmetry breaking.
Examples.
Symmetry breaking can cover any of the following scenarios:
One of the first cases of broken symmetry discussed in the physics literature is related to the form taken by a uniformly rotating body of incompressible fluid in gravitational and hydrostatic equilibrium. Jacobi and soon later Liouville, in 1834, discussed the fact that a tri-axial ellipsoid was an equilibrium solution for this problem when the kinetic energy compared to the gravitational energy of the rotating body exceeded a certain critical value. The axial symmetry presented by the McLaurin spheroids is broken at this bifurcation point. Furthermore, above this bifurcation point, and for constant angular momentum, the solutions that minimize the kinetic energy are the "non"-axially symmetric Jacobi ellipsoids instead of the Maclaurin spheroids.

</doc>
<doc id="28186" url="https://en.wikipedia.org/wiki?curid=28186" title="Symmetry group">
Symmetry group

In abstract algebra, the symmetry group of an object (image, signal, etc.) is the group of all transformations under which the object is invariant with composition as the group operation. For a space with a metric, it is a subgroup of the isometry group of the space concerned. If not stated otherwise, this article considers symmetry groups in Euclidean geometry, but the concept may also be studied in more general contexts as expanded below.
Introduction.
The "objects" may be geometric figures, images, and patterns, such as a wallpaper pattern. The definition can be made more precise by specifying what is meant by image or pattern, e.g., a function of position with values in a set of colors. For symmetry of physical objects, one may also want to take their physical composition into account. The group of isometries of space induces a group action on objects in it.
The symmetry group is sometimes also called full symmetry group in order to emphasize that it includes the orientation-reversing isometries (like reflections, glide reflections and improper rotations) under which the figure is invariant. The subgroup of orientation-preserving isometries (i.e. translations, rotations, and compositions of these) that leave the figure invariant is called its proper symmetry group. The proper symmetry group of an object is equal to its full symmetry group if and only if the object is chiral (and thus there are no orientation-reversing isometries under which it is invariant).
Any symmetry group whose elements have a common fixed point, which is true for all finite symmetry groups and also for the symmetry groups of bounded figures, can be represented as a subgroup of the orthogonal group O("n") by choosing the origin to be a fixed point. The proper symmetry group is then a subgroup of the special orthogonal group SO("n"), and is therefore also called rotation group of the figure.
A discrete symmetry group is a symmetry group such that for every point of the space the set of images of the point under the isometries in the symmetry group is a discrete set.
Discrete symmetry groups come in three types: (1) finite point groups, which include only rotations, reflections, inversion and rotoinversion – they are just the finite subgroups of O("n"), (2) infinite lattice groups, which include only translations, and (3) infinite space groups which combines elements of both previous types, and may also include extra transformations like screw axis and glide reflection. There are also "continuous" symmetry groups, which contain rotations of arbitrarily small angles or translations of arbitrarily small distances. The group of all symmetries of a sphere O(3) is an example of this, and in general such continuous symmetry groups are studied as Lie groups. With a categorization of subgroups of the Euclidean group corresponds a categorization of symmetry groups.
Two geometric figures are considered to be of the same symmetry type if their symmetry groups are conjugate subgroups of the Euclidean group E("n") (the isometry group of R"n"), where two subgroups "H"1, "H"2 of a group "G" are "conjugate", if there exists such that . For example:
When considering isometry groups, one may restrict oneself to those where for all points the set of images under the isometries is topologically closed. This includes all discrete isometry groups and also those involved in continuous symmetries, but excludes for example in 1D the group of translations by a rational number. A "figure" with this symmetry group is non-drawable and up to arbitrarily fine detail homogeneous, without being really homogeneous.
One dimension.
The isometry groups in one dimension where for all points the set of images under the isometries is topologically closed are:
See also symmetry groups in one dimension.
Two dimensions.
Up to conjugacy the discrete point groups in two-dimensional space are the following classes:
C1 is the trivial group containing only the identity operation, which occurs when the figure has no symmetry at all, for example the letter F. C2 is the symmetry group of the letter Z, C3 that of a triskelion, C4 of a swastika, and C5, C6, etc. are the symmetry groups of similar swastika-like figures with five, six, etc. arms instead of four.
D1 is the 2-element group containing the identity operation and a single reflection, which occurs when the figure has only a single axis of bilateral symmetry, for example the letter A. D2, which is isomorphic to the Klein four-group, is the symmetry group of a non-equilateral isosceles triangle, and D3, D4 etc. are the symmetry groups of the regular polygons.
The actual symmetry groups in each of these cases have two degrees of freedom for the center of rotation, and in the case of the dihedral groups, one more for the positions of the mirrors.
The remaining isometry groups in two dimensions with a fixed point, where for all points the set of images under the isometries is topologically closed are:
For non-bounded figures, the additional isometry groups can include translations; the closed ones are:
Three dimensions.
Up to conjugacy the set of three-dimensional point groups consists of 7 infinite series, and 7 separate ones. In crystallography they are restricted to be compatible with the discrete translation symmetries of a crystal lattice. This crystallographic restriction of the infinite families of general point groups results in 32 crystallographic point groups (27 from the 7 infinite series, and 5 of the 7 others).
The continuous symmetry groups with a fixed point include those of:
For objects and scalar fields the cylindrical symmetry implies vertical planes of reflection. However, for vector fields it does not: in cylindrical coordinates with respect to some axis, 
formula_1 has cylindrical symmetry with respect to the axis if and only if formula_2 and formula_3 have this symmetry, i.e., they do not depend on φ. Additionally there is reflectional symmetry if and only if formula_4.
For spherical symmetry there is no such distinction, it implies planes of reflection.
The continuous symmetry groups without a fixed point include those with a screw axis, such as an infinite helix. See also subgroups of the Euclidean group.
Symmetry groups in general.
In wider contexts, a symmetry group may be any kind of transformation group, or automorphism group. Once we know what kind of mathematical structure we are concerned with, we should be able to pinpoint what mappings preserve the structure. Conversely, specifying the symmetry can define the structure, or at least clarify what we mean by an invariant, geometric language in which to discuss it; this is one way of looking at the Erlangen programme.
For example, automorphism groups of certain models of finite geometries are not "symmetry groups" in the usual sense, although they preserve symmetry. They do this by preserving "families" of point-sets rather than point-sets (or "objects") themselves.
Like above, the group of automorphisms of space induces a group action on objects in it.
For a given geometric figure in a given geometric space, consider the following equivalence relation: two automorphisms of space are equivalent if and only if the two images of the figure are the same (here "the same" does not mean something like e.g. "the same up to translation and rotation", but it means "exactly the same"). Then the equivalence class of the identity is the symmetry group of the figure, and every equivalence class corresponds to one isomorphic version of the figure.
There is a bijection between every pair of equivalence classes: the inverse of a representative of the first equivalence class, composed with a representative of the second.
In the case of a finite automorphism group of the whole space, its order is the order of the symmetry group of the figure multiplied by the number of isomorphic versions of the figure.
Examples:
Compare Lagrange's theorem (group theory) and its proof.

</doc>
<doc id="113087" url="https://en.wikipedia.org/wiki?curid=113087" title="System of linear equations">
System of linear equations

In mathematics, a system of linear equations (or linear system) is a collection of linear equations involving the same set of variables. For example,
is a system of three equations in the three variables . A solution to a linear system is an assignment of numbers to the variables such that all the equations are simultaneously satisfied. A solution to the system above is given by
since it makes all three equations valid. The word "system" indicates that the equations are to be considered collectively, rather than individually.
In mathematics, the theory of linear systems is the basis and a fundamental part of linear algebra, a subject which is used in most parts of modern mathematics. Computational algorithms for finding the solutions are an important part of numerical linear algebra, and play a prominent role in engineering, physics, chemistry, computer science, and economics. A system of non-linear equations can often be approximated by a linear system (see linearization), a helpful technique when making a mathematical model or computer simulation of a relatively complex system.
Very often, the coefficients of the equations are real or complex numbers and the solutions are searched in the same set of numbers, but the theory and the algorithms apply for coefficients and solutions in any field. For solutions in an integral domain like the ring of the integers, or in other algebraic structures, other theories have been developed, see Linear equation over a ring. Integer linear programming is a collection of methods for finding the "best" integer solution (when there are many). Gröbner basis theory provides algorithms when coefficients and unknowns are polynomials. Also tropical geometry is an example of linear algebra in a more exotic structure.
Elementary example.
The simplest kind of linear system involves two equations and two variables:
One method for solving such a system is as follows. First, solve the top equation for formula_4 in terms of formula_5:
Now substitute this expression for "x" into the bottom equation:
This results in a single equation involving only the variable formula_5. Solving gives formula_9, and substituting this back into the equation for formula_4 yields formula_11. This method generalizes to systems with additional variables (see "elimination of variables" below, or the article on elementary algebra.)
General form.
A general system of "m" linear equations with "n" unknowns can be written as
Here formula_13 are the unknowns, formula_14 are the coefficients of the system, and formula_15 are the constant terms.
Often the coefficients and unknowns are real or complex numbers, but integers and rational numbers are also seen, as are polynomials and elements of an abstract algebraic structure.
Vector equation.
One extremely helpful view is that each unknown is a weight for a column vector in a linear combination.
This allows all the language and theory of "vector spaces" (or more generally, "modules") to be brought to bear. For example, the collection of all possible linear combinations of the vectors on the left-hand side is called their "span", and the equations have a solution just when the right-hand vector is within that span. If every vector within that span has exactly one expression as a linear combination of the given left-hand vectors, then any solution is unique. In any event, the span has a "basis" of linearly independent vectors that do guarantee exactly one expression; and the number of vectors in that basis (its "dimension") cannot be larger than "m" or "n", but it can be smaller. This is important because if we have "m" independent vectors a solution is guaranteed regardless of the right-hand side, and otherwise not guaranteed.
Matrix equation.
The vector equation is equivalent to a matrix equation of the form
where "A" is an "m"×"n" matrix, x is a column vector with "n" entries, and b is a column vector with "m" entries.
The number of vectors in a basis for the span is now expressed as the "rank" of the matrix.
Solution set.
A solution of a linear system is an assignment of values to the variables such that each of the equations is satisfied. The set of all possible solutions is called the solution set.
A linear system may behave in any one of three possible ways:
Geometric interpretation.
For a system involving two variables ("x" and "y"), each linear equation determines a line on the "xy"-plane. Because a solution to a linear system must satisfy all of the equations, the solution set is the intersection of these lines, and is hence either a line, a single point, or the empty set.
For three variables, each linear equation determines a plane in three-dimensional space, and the solution set is the intersection of these planes. Thus the solution set may be a plane, a line, a single point, or the empty set. For example, as three parallel planes do not have a common point, the solution set of their equations is empty; the solution set of the equations of three planes intersecting at a point is single point; if three planes pass through two points, their equations have at least two common solutions; in fact the solution set is infinite and consists in all the line passing through these points.
For "n" variables, each linear equation determines a hyperplane in "n"-dimensional space. The solution set is the intersection of these hyperplanes, which may be a flat of any dimension.
General behavior.
In general, the behavior of a linear system is determined by the relationship between the number of equations and the number of unknowns:
In the first case, the dimension of the solution set is usually equal to , where "n" is the number of variables and "m" is the number of equations.
The following pictures illustrate this trichotomy in the case of two variables:
The first system has infinitely many solutions, namely all of the points on the blue line. The second system has a single unique solution, namely the intersection of the two lines. The third system has no solutions, since the three lines share no common point.
Keep in mind that the pictures above show only the most common case. It is possible for a system of two equations and two unknowns to have no solution (if the two lines are parallel), or for a system of three equations and two unknowns to be solvable (if the three lines intersect at a single point). In general, a system of linear equations may behave differently from expected if the equations are linearly dependent, or if two or more of the equations are inconsistent.
Properties.
Independence.
The equations of a linear system are independent if none of the equations can be derived algebraically from the others. When the equations are independent, each equation contains new information about the variables, and removing any of the equations increases the size of the solution set. For linear equations, logical independence is the same as linear independence.
For example, the equations
are not independent — they are the same equation when scaled by a factor of two, and they would produce identical graphs. This is an example of equivalence in a system of linear equations.
For a more complicated example, the equations
are not independent, because the third equation is the sum of the other two. Indeed, any one of these equations can be derived from the other two, and any one of the equations can be removed without affecting the solution set. The graphs of these equations are three lines that intersect at a single point.
Consistency.
A linear system is inconsistent if it has no solution, and otherwise it is said to be consistent . When the system is inconsistent, it is possible to derive a contradiction from the equations, that may always be rewritten such as the statement .
For example, the equations
are inconsistent. In fact, by subtracting the first equation from the second one and multiplying both sides of the result by 1/6, we get . The graphs of these equations on the "xy"-plane are a pair of parallel lines.
It is possible for three linear equations to be inconsistent, even though any two of them are consistent together. For example, the equations
are inconsistent. Adding the first two equations together gives , which can be subtracted from the third equation to yield . Note that any two of these equations have a common solution. The same phenomenon can occur for any number of equations.
In general, inconsistencies occur if the left-hand sides of the equations in a system are linearly dependent, and the constant terms do not satisfy the dependence relation. A system of equations whose left-hand sides are linearly independent is always consistent.
Putting it another way, according to the Rouché–Capelli theorem, any system of equations (overdetermined or otherwise) is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has "k" free parameters where "k" is the difference between the number of variables and the rank; hence in such a case there are an infinitude of solutions. The rank of a system of equations can never be higher than [the number of variables] + 1, which means that a system with any number of equations can always be reduced to a system that has a number of independent equations that is at most equal to [the number of variables] + 1.
Equivalence.
Two linear systems using the same set of variables are equivalent if each of the equations in the second system can be derived algebraically from the equations in the first system, and vice versa. Two systems are equivalent if either both are inconsistent or each equation of any of them is a linear combination of the equations of the other one. It follows that two linear systems are equivalent if and only if they have the same solution set.
Solving a linear system.
There are several algorithms for solving a system of linear equations.
Describing the solution.
When the solution set is finite, it is reduced to a single element. In this case, the unique solution is described by a sequence of equations whose left-hand sides are the names of the unknowns and right-hand sides are the corresponding values, for example formula_23. When an order on the unknowns has been fixed, for example the alphabetical order the solution may be described as a vector of values, like formula_24 for the previous example.
It can be difficult to describe a set with infinite solutions. Typically, some of the variables are designated as free (or independent, or as parameters), meaning that they are allowed to take any value, while the remaining variables are dependent on the values of the free variables.
For example, consider the following system:
The solution set to this system can be described by the following equations:
Here "z" is the free variable, while "x" and "y" are dependent on "z". Any point in the solution set can be obtained by first choosing a value for "z", and then computing the corresponding values for "x" and "y".
Each free variable gives the solution space one degree of freedom, the number of which is equal to the dimension of the solution set. For example, the solution set for the above equation is a line, since a point in the solution set can be chosen by specifying the value of the parameter "z". An infinite solution of higher order may describe a plane, or higher-dimensional set.
Different choices for the free variables may lead to different descriptions of the same solution set. For example, the solution to the above equations can alternatively be described as follows:
Here "x" is the free variable, and "y" and "z" are dependent.
Elimination of variables.
The simplest method for solving a system of linear equations is to repeatedly eliminate variables. This method can be described as follows:
For example, consider the following system:
Solving the first equation for "x" gives , and plugging this into the second and third equation yields
Solving the first of these equations for "y" yields , and plugging this into the second equation yields . We now have:
Substituting into the second equation gives , and substituting and into the first equation yields . Therefore, the solution set is the single point .
Row reduction.
In row reduction, the linear system is represented as an augmented matrix:
This matrix is then modified using elementary row operations until it reaches reduced row echelon form. There are three types of elementary row operations:
Because these operations are reversible, the augmented matrix produced always represents a linear system that is equivalent to the original.
There are several specific algorithms to row-reduce an augmented matrix, the simplest of which are Gaussian elimination and Gauss-Jordan elimination. The following computation shows Gauss-Jordan elimination applied to the matrix above:
The last matrix is in reduced row echelon form, and represents the system , , . A comparison with the example in the previous section on the algebraic elimination of variables shows that these two methods are in fact the same; the difference lies in how the computations are written down.
Cramer's rule.
Cramer's rule is an explicit formula for the solution of a system of linear equations, with each variable given by a quotient of two determinants. For example, the solution to the system
is given by
For each variable, the denominator is the determinant of the matrix of coefficients, while the numerator is the determinant of a matrix in which one column has been replaced by the vector of constant terms.
Though Cramer's rule is important theoretically, it has little practical value for large matrices, since the computation of large determinants is somewhat cumbersome. (Indeed, large determinants are most easily computed using row reduction.)
Further, Cramer's rule has very poor numerical properties, making it unsuitable for solving even small systems reliably, unless the operations are performed in rational arithmetic with unbounded precision.
Matrix solution.
If the equation system is expressed in the matrix form formula_17, the entire solution set can also be expressed in matrix form. If the matrix "A" is square (has "m" rows and "n"="m" columns) and has full rank (all "m" rows are independent), then the system has a unique solution given by
where formula_37 is the inverse of "A". More generally, regardless of whether "m"="n" or not and regardless of the rank of "A", all solutions (if any exist) are given using the Moore-Penrose pseudoinverse of "A", denoted formula_38, as follows:
where formula_40 is a vector of free parameters that ranges over all possible "n"×1 vectors. A necessary and sufficient condition for any solution(s) to exist is that the potential solution obtained using formula_41 satisfy formula_17 — that is, that formula_43 If this condition does not hold, the equation system is inconsistent and has no solution. If the condition holds, the system is consistent and at least one solution exists. For example, in the above-mentioned case in which "A" is square and of full rank, formula_38 simply equals formula_37 and the general solution equation simplifies to formula_46 as previously stated, where formula_40 has completely dropped out of the solution, leaving only a single solution. In other cases, though, formula_40 remains and hence an infinitude of potential values of the free parameter vector formula_40 give an infinitude of solutions of the equation.
Other methods.
While systems of three or four equations can be readily solved by hand (see Cracovian), computers are often used for larger systems. The standard algorithm for solving a system of linear equations is based on Gaussian elimination with some modifications. Firstly, it is essential to avoid division by small numbers, which may lead to inaccurate results. This can be done by reordering the equations if necessary, a process known as "pivoting". Secondly, the algorithm does not exactly do Gaussian elimination, but it computes the LU decomposition of the matrix "A". This is mostly an organizational tool, but it is much quicker if one has to solve several systems with the same matrix "A" but different vectors b.
If the matrix "A" has some special structure, this can be exploited to obtain faster or more accurate algorithms. For instance, systems with a symmetric positive definite matrix can be solved twice as fast with the Cholesky decomposition. Levinson recursion is a fast method for Toeplitz matrices. Special methods exist also for matrices with many zero elements (so-called sparse matrices), which appear often in applications.
A completely different approach is often taken for very large systems, which would otherwise take too much time or memory. The idea is to start with an initial approximation to the solution (which does not have to be accurate at all), and to change this approximation in several steps to bring it closer to the true solution. Once the approximation is sufficiently accurate, this is taken to be the solution to the system. This leads to the class of iterative methods.
Homogeneous systems.
A system of linear equations is homogeneous if all of the constant terms are zero:
A homogeneous system is equivalent to a matrix equation of the form
where "A" is an matrix, x is a column vector with "n" entries, and 0 is the zero vector with "m" entries.
Solution set.
Every homogeneous system has at least one solution, known as the zero solution (or trivial solution), which is obtained by assigning the value of zero to each of the variables. If the system has a non-singular matrix (det(A) ≠ 0) then it is also the only solution. If the system has a singular matrix then there is a solution set with an infinite number of solutions. This solution set has the following additional properties:
These are exactly the properties required for the solution set to be a linear subspace of R"n". In particular, the solution set to a homogeneous system is the same as the null space of the corresponding matrix "A".
A numerical solutions to a homogeneous system can be found with a SVD decomposition.
Relation to nonhomogeneous systems.
There is a close relationship between the solutions to a linear system and the solutions to the corresponding homogeneous system:
Specifically, if p is any specific solution to the linear system , then the entire solution set can be described as
Geometrically, this says that the solution set for is a translation of the solution set for . Specifically, the flat for the first system can be obtained by translating the linear subspace for the homogeneous system by the vector p.
This reasoning only applies if the system has at least one solution. This occurs if and only if the vector b lies in the image of the linear transformation "A".

</doc>
<doc id="20405650" url="https://en.wikipedia.org/wiki?curid=20405650" title="T meson">
T meson

T mesons are hypothetical mesons composed of a top quark and either an up (), down (), strange () or charm antiquark (). Because of the top quark's short lifetime, T mesons are not expected to be found in nature. The combination of a top quark and top antiquark is not a T meson, but rather toponium. Each T meson has an antiparticle that is composed of a top antiquark and an up (), down (), strange () or charm quark () respectively.

</doc>
<doc id="1387689" url="https://en.wikipedia.org/wiki?curid=1387689" title="Table of Lie groups">
Table of Lie groups

This article gives a table of some common Lie groups and their associated Lie algebras.
The following are noted: the topological properties of the group (dimension; connectedness; compactness; the nature of the fundamental group; and whether or not they are simply connected) as well as on their algebraic properties (abelian; simple; semisimple).
For more examples of Lie groups and other related topics see the list of simple Lie groups; the Bianchi classification of groups of up to three dimensions; and the list of Lie group topics.
Real Lie groups and their algebras.
Column legend
Real Lie algebras.
Table legend:
Complex Lie groups and their algebras.
The dimensions given are dimensions over C. Note that every complex Lie group/algebra can also be viewed as a real Lie group/algebra of twice the dimension.
Complex Lie algebras.
The dimensions given are dimensions over C. Note that every complex Lie algebra can also be viewed as a real Lie algebra of twice the dimension.

</doc>
<doc id="31296" url="https://en.wikipedia.org/wiki?curid=31296" title="Tachyon">
Tachyon

A tachyon or tachyonic particle is a hypothetical particle that always moves faster than light. The word comes from the pronounced "tachy ", meaning rapid. It was coined in 1967 by Gerald Feinberg. The complementary particle types are called luxon (always moving at the speed of light) and bradyon (always moving slower than light), which both exist. The possibility of particles moving faster than light was first proposed by O. M. P. Bilaniuk, V. K. Deshpande, and E. C. G. Sudarshan in 1962, although the term they used for it was "meta-particle".
Most physicists think that faster-than-light particles cannot exist because they are not consistent with the known laws of physics. If such particles did exist, they could be used to build a tachyonic antitelephone and send signals faster than light, which (according to special relativity) would lead to violations of causality. Potentially consistent theories that allow faster-than-light particles include those that break Lorentz invariance, the symmetry underlying special relativity, so that the speed of light is not a barrier.
In the 1967 paper that coined the term, Feinberg proposed that tachyonic particles could be quanta of a quantum field with negative squared mass. However, it was soon realized that excitations of such imaginary mass fields do "not" in fact propagate faster than light, and instead represent an instability known as tachyon condensation. Nevertheless, negative squared mass fields are commonly referred to as "tachyons", and in fact have come to play an important role in modern physics.
Despite theoretical arguments against the existence of faster-than-light particles, experiments have been conducted to search for them. No compelling evidence for their existence has been found. In September 2011, it was reported that a tau neutrino had travelled faster than the speed of light in a major release by CERN; however, later updates from CERN on the OPERA project indicate that the faster-than-light readings were resultant from "a faulty element of the experiment's fibre optic timing system".
Tachyons in relativistic theory.
In special relativity, a faster-than-light particle would have space-like four-momentum, in contrast to ordinary particles that have time-like four-momentum. It would also have imaginary mass and proper time. Being constrained to the spacelike portion of the energy–momentum graph, it could not slow down to subluminal speeds.
Mass.
In a Lorentz invariant theory, the same formulas that apply to ordinary slower-than-light particles (sometimes called "bradyons" in discussions of tachyons) must also apply to tachyons. In particular the energy–momentum relation:
(where p is the relativistic momentum of the bradyon and m is its rest mass) should still apply, along with the formula for the total energy of a particle:
This equation shows that the total energy of a particle (bradyon or tachyon) contains a contribution from its rest mass (the "rest mass–energy") and a contribution from its motion, the kinetic energy.
When "v" is larger than "c", the denominator in the equation for the energy is "imaginary", as the value under the radical is negative. Because the total energy must be real, the numerator must "also" be imaginary: i.e. the rest mass m must be imaginary, as a pure imaginary number divided by another pure imaginary number is a real number.
Speed.
One curious effect is that, unlike ordinary particles, the speed of a tachyon "increases" as its energy decreases. In particular, formula_3 approaches zero when formula_4 approaches infinity. (For ordinary bradyonic matter, "E" increases with increasing speed, becoming arbitrarily large as "v" approaches "c", the speed of light). Therefore, just as bradyons are forbidden to break the light-speed barrier, so too are tachyons forbidden from slowing down to below "c", because infinite energy is required to reach the barrier from either above or below.
As noted by Einstein, Tolman, and others, special relativity implies that faster-than-light particles, if they existed, could be used to communicate backwards in time.
Neutrinos.
In 1985 Chodos et al. proposed that neutrinos can have a tachyonic nature. The possibility of standard model particles moving at superluminal speeds can be modeled using Lorentz invariance violating terms, for example in the Standard-Model Extension. In this framework, neutrinos experience Lorentz-violating oscillations and can travel faster than light at high energies. This proposal was strongly criticized.
Cherenkov radiation.
A tachyon with an electric charge would lose energy as Cherenkov radiation—just as ordinary charged particles do when they exceed the local speed of light in a medium. A charged tachyon traveling in a vacuum therefore undergoes a constant proper time acceleration and, by necessity, its worldline forms a hyperbola in space-time. However reducing a tachyon's energy "increases" its speed, so that the single hyperbola formed is of "two" oppositely charged tachyons with opposite momenta (same magnitude, opposite sign) which annihilate each other when they simultaneously reach infinite speed at the same place in space. (At infinite speed, the two tachyons have no energy each and finite momentum of opposite direction, so no conservation laws are violated in their mutual annihilation. The time of annihilation is frame dependent.)
Even an electrically neutral tachyon would be expected to lose energy via gravitational Cherenkov radiation, because it has a gravitational mass, and therefore increase in speed as it travels, as described above. If the tachyon interacts with any other particles, it can also radiate Cherenkov energy into those particles. Neutrinos interact with the other particles of the Standard Model, and Andrew Cohen and Sheldon Glashow recently used this to argue that the faster-than-light neutrino anomaly cannot be explained by making neutrinos propagate faster than light, and must instead be due to an error in the experiment.
Causality.
Causality is a fundamental principle of physics. If tachyons can transmit information faster than light, then according to relativity they violate causality, leading to logical paradoxes of the "kill your own grandfather" type. This is often illustrated with thought experiments such as the "tachyon telephone paradox" or "logically pernicious self-inhibitor."
The problem can be understood in terms of the relativity of simultaneity in special relativity, which says that different inertial reference frames will disagree on whether two events at different locations happened "at the same time" or not, and they can also disagree on the order of the two events (technically, these disagreements occur when spacetime interval between the events is 'space-like', meaning that neither event lies in the future light cone of the other).
If one of the two events represents the sending of a signal from one location and the second event represents the reception of the same signal at another location, then as long as the signal is moving at the speed of light or slower, the mathematics of simultaneity ensures that all reference frames agree that the transmission-event happened before the reception-event. However, in the case of a hypothetical signal moving faster than light, there would always be some frames in which the signal was received before it was sent, so that the signal could be said to have moved backwards in time. Because one of the two fundamental postulates of special relativity says that the laws of physics should work the same way in every inertial frame, if it is possible for signals to move backwards in time in any one frame, it must be possible in all frames. This means that if observer A sends a signal to observer B which moves faster than light in A's frame but backwards in time in B's frame, and then B sends a reply which moves faster than light in B's frame but backwards in time in A's frame, it could work out that A receives the reply before sending the original signal, challenging causality in "every" frame and opening the door to severe logical paradoxes. Mathematical details can be found in the tachyonic antitelephone article, and an illustration of such a scenario using spacetime diagrams can be found in "Baker, R. (2003)"
Reinterpretation principle.
The reinterpretation principle asserts that a tachyon sent "back" in time can always be "reinterpreted" as a tachyon traveling "forward" in time, because observers cannot distinguish between the emission and absorption of tachyons. The attempt to "detect" a tachyon "from" the future (and violate causality) would actually "create" the same tachyon and send it "forward" in time (which is causal).
However, this principle is not widely accepted as resolving the paradoxes. Instead, what would be required to avoid paradoxes is that unlike any known particle, tachyons do not interact in any way and can never be detected or observed, because otherwise a tachyon beam could be modulated and used to create an anti-telephone or a "logically pernicious self-inhibitor". All forms of energy are believed to interact at least gravitationally, and many authors state that superluminal propagation in Lorentz invariant theories always leads to causal paradoxes.
Fundamental models.
In modern physics, all fundamental particles are regarded as excitations of quantum fields. There are several distinct ways in which tachyonic particles could be embedded into a field theory.
Fields with imaginary mass.
In the paper that coined the term "tachyon", Gerald Feinberg studied Lorentz invariant quantum fields with imaginary mass. Because the group velocity for such a field is superluminal, naively it appears that its excitations propagate faster than light. However, it was quickly understood that the superluminal group velocity does not correspond to the speed of propagation of any localized excitation (like a particle). Instead, the negative mass represents an instability to tachyon condensation, and all excitations of the field propagate subluminally and are consistent with causality. Despite having no faster-than-light propagation, such fields are referred to simply as "tachyons" in many sources.
Tachyonic fields play an important role in modern physics. Perhaps the most famous is the Higgs boson of the Standard Model of particle physics, which—in its uncondensed phase—has an imaginary mass. In general, the phenomenon of spontaneous symmetry breaking, which is closely related to tachyon condensation, plays a very important role in many aspects of theoretical physics, including the Ginzburg–Landau and BCS theories of superconductivity. Another example of a tachyonic field is the tachyon of bosonic string theory.
Tachyons are predicted by bosonic string theory and also the NS (which is the open bosonic sector) and NS-NS (which is the closed bosonic sector) sectors of RNS Superstring theory before GSO projection. However, due to the Sen conjecture—also known as tachyon condensation—this is not possible. This resulted in the necessity for the GSO projection.
Lorentz-violating theories.
In theories that do not respect Lorentz invariance the speed of light is not (necessarily) a barrier, and particles can travel faster than the speed of light without infinite energy or causal paradoxes. A class of field theories of that type are the so-called Standard Model extensions. However, the experimental evidence for Lorentz invariance is extremely good, so such theories are very tightly constrained.
Fields with non-canonical kinetic term.
By modifying the kinetic energy of the field, it is possible to produce Lorentz invariant field theories with excitations that propagate superluminally. However, such theories in general do not have a well-defined Cauchy problem (for reasons related to the issues of causality discussed above), and are probably inconsistent quantum mechanically.
History.
As mentioned above, the term "tachyon" was coined by Gerald Feinberg in a 1967 paper titled "Possibility of Faster-Than-Light Particles". He had been inspired by the science-fiction story "Beep" by James Blish. Feinberg studied the kinematics of such particles according to special relativity. In his paper he also introduced fields with imaginary mass (now also referred to as "tachyons") in an attempt to understand the microphysical origin such particles might have.
The first hypothesis regarding faster-than-light particles is sometimes attributed to German physicist Arnold Sommerfeld in 1904, and more recent discussions happened in 1962 and 1969.
In fiction.
Tachyons have appeared in many works of fiction. They have been used as a standby mechanism upon which many science fiction authors rely to establish faster-than-light communication, with or without reference to causality issues. The word "tachyon" has become widely recognized to such an extent that it can impart a science-fictional connotation even if the subject in question has no particular relation to superluminal travel (a form of technobabble, akin to "positronic brain").

</doc>
<doc id="34566625" url="https://en.wikipedia.org/wiki?curid=34566625" title="Tachyonic field">
Tachyonic field

A tachyonic field, or simply tachyon, is a quantum field with an imaginary mass. Although tachyons (particles that move faster than light) are a purely hypothetical concept, fields with imaginary mass have come to play an important role in modern physics and are discussed in popular books on physics. Under no circumstances do any excitations ever propagate faster than light in such theories—the presence or absence of a tachyonic mass has no effect whatsoever on the maximum velocity of signals (there is no violation of causality).
The term "tachyon" was coined by Gerald Feinberg in a 1967 paper that studied quantum fields with imaginary mass. Feinberg believed such fields permitted faster than light propagation, but it was soon realized that Feinberg's model in fact did not allow for superluminal speeds. Instead, the imaginary mass creates an instability in the configuration: any configuration in which one or more field excitations are tachyonic will spontaneously decay, and the resulting configuration contains no physical tachyons. This process is known as tachyon condensation. A famous example is the condensation of the Higgs boson in the Standard Model of particle physics.
In modern physics, all fundamental particles are regarded as localized excitations of fields. Tachyons are unusual because the instability prevents any such localized excitations from existing. Any localized perturbation, no matter how small, starts an exponentially growing cascade that strongly affects physics everywhere inside the future light cone of the perturbation.
Interpretation.
Overview of tachyonic condensation.
Although the notion of a tachyonic imaginary mass might seem troubling because there is no classical interpretation of an imaginary mass, the mass is not quantized. Rather, the scalar field is; even for tachyonic quantum fields, the field operators at spacelike separated points still commute (or anticommute), thus preserving causality. Therefore, information still does not propagate faster than light, and solutions grow exponentially, but not superluminally (there is no violation of causality).
The "imaginary mass" really means that the system becomes unstable. The zero value field is at a local maximum rather than a local minimum of its potential energy, much like a ball at the top of a hill. A very small impulse (which will always happen due to quantum fluctuations) will lead the field to roll down with exponentially increasing amplitudes toward the local minimum. In this way, tachyon condensation drives a physical system that has reached a local limit and might naively be expected to produce physical tachyons, to an alternate stable state where no physical tachyons exist. Once the tachyonic field reaches the minimum of the potential, its quanta are not tachyons any more but rather are ordinary particles with a positive mass-squared, such as the Higgs boson.
Physical interpretation of a tachyonic field and signal propagation.
There is a simple mechanical analogy that illustrates that tachyonic fields do not propagate faster than light, why they represent instabilities, and helps explain the meaning of imaginary mass (negative squared mass).
Consider a long line of pendulums, all pointing straight down. The mass on the end of each pendulum is connected to the masses of its two neighbors by springs. Wiggling one of the pendulums will create two ripples that propagate in both directions down the line. As the ripple passes, each pendulum in its turn oscillates a few times about the straight down position. The speed of propagation of these ripples is determined in a simple way by the tension of the springs and the inertial mass of the pendulum weights. Formally, these parameters can be chosen so that the propagation speed is the speed of light. In the limit of an infinite density of closely spaced pendulums, this model becomes identical to a relativistic field theory, where the ripples are the analog of particles. Displacing the pendulums from pointing straight down requires positive energy, which indicates that the squared mass of those particles is positive.
Now consider an initial condition where at time t=0, all the pendulums are pointing straight up. Clearly this is unstable, but at least in classical physics one can imagine that they are so carefully balanced they will remain pointing straight up indefinitely so long as they are not perturbed. Wiggling one of the upside-down pendulums will have a very different effect from before. The speed of propagation of the effects of the wiggle is identical to what it was before, since neither the spring tension nor the inertial mass have changed. However, the effects on the pendulums affected by the perturbation are dramatically different. Those pendulums that feel the effects of the perturbation will begin to topple over, and will pick up speed exponentially. Indeed, it is easy to show that any localized perturbation kicks off an exponentially growing instability that affects everything within its future "ripple cone" (a region of size equal to time multiplied by the ripple propagation speed). In the limit of infinite pendulum density, this model is a tachyonic field theory.
Importance in physics.
Tachyonic fields play a very important role in modern physics. Perhaps the most famous example of a tachyon is the Higgs boson of the Standard model of particle physics. In its uncondensed phase, the square of the mass of the Higgs field is negative, and therefore, the associated particle is a tachyon.
The phenomenon of spontaneous symmetry breaking, which is closely related to tachyon condensation, plays a central part in many aspects of theoretical physics, including the Ginzburg–Landau and BCS theories of superconductivity.
Other examples include the inflaton field in certain models of cosmic inflation (such as new inflation), and the tachyon of bosonic string theory.
Condensation.
In quantum field theory, a tachyon is a quantum of a field—usually a scalar field—whose squared mass is negative, and is used to describe spontaneous symmetry breaking: The existence of such a field implies the instability of the field vacuum; the field is at a local maximum rather than a local minimum of its potential energy, much like a ball at the top of a hill. A very small impulse (which will always happen due to quantum fluctuations) will lead the field (ball) to roll down with exponentially increasing amplitudes: it will induce tachyon condensation. It is important to realize that once the tachyonic field reaches the minimum of the potential, its quanta are not tachyons any more but rather have a positive mass-squared. The Higgs boson of the standard model of particle physics is an example.
Technically, the squared mass is the second derivative of the effective potential. For a tachyonic field the second derivative is negative, meaning that the effective potential is at a local maximum rather than a local minimum. Therefore, this situation is unstable and the field will roll down the potential.
Because a tachyon's squared mass is negative, it formally has an imaginary mass. This is a special case of the general rule, where unstable massive particles are formally described as having a complex mass, with the real part being their mass in usual sense, and the imaginary part being the decay rate in natural units.
However, in quantum field theory, a particle (a "one-particle state") is roughly defined as a state which is constant over time; i.e., an eigenvalue of the Hamiltonian. An unstable particle is a state which is only approximately constant over time; If it exists long enough to be measured, it can be formally described as having a complex mass, with the real part of the mass greater than its imaginary part. If both parts are of the same magnitude, this is interpreted as a resonance appearing in a scattering process rather than particle, as it is considered not to exist long enough to be measured independently of the scattering process. In the case of a tachyon the real part of the mass is zero, and hence no concept of a particle can be attributed to it.
Even for tachyonic quantum fields, the field operators at space-like separated points still commute (or anticommute), thus preserving the principle of causality. For closely related reasons, the maximum velocity of signals sent with a tachyonic field is strictly bounded from above by the speed of light. Therefore, information never moves faster than light regardless of the presence or absence of tachyonic fields.
Examples for tachyonic fields are all cases of spontaneous symmetry breaking. In condensed matter physics a notable example is ferromagnetism; in particle physics the best known example is the Higgs mechanism in the standard model.
Tachyons in string theory.
In string theory, tachyons have the same interpretation as in quantum field theory. However, string theory can, at least, in principle, not only describe the physics of tachyonic fields, but also predict whether such fields appear.
Tachyonic fields indeed arise in many versions of string theory. In general, string theory states that what we see as "particles" (electrons, photons, gravitons and so forth) are actually different vibrational states of the same underlying string. The mass of the particle can be deduced from the vibrations which the string exhibits; roughly speaking, the mass depends upon the "note" which the string sounds. Tachyons frequently appear in the spectrum of permissible string states, in the sense that some states have negative mass-squared, and therefore, imaginary mass. If the tachyon appears as a vibrational mode of an open string, this signals an instability of the underlying D-brane system to which the string is attached. The system will then decay to a state of closed strings and/or stable D-branes. If the tachyon is a closed string vibrational mode, this indicates an instability in spacetime itself. Generally, it is not known (or theorized) what this system will decay to. However, if the closed string tachyon is localized around a spacetime singularity, the endpoint of the decay process will often have the singularity resolved.

</doc>
<doc id="31482" url="https://en.wikipedia.org/wiki?curid=31482" title="Tangent">
Tangent

In geometry, the tangent line (or simply tangent) to a plane curve at a given point is the straight line that "just touches" the curve at that point. Leibniz defined it as the line through a pair of infinitely close points on the curve. More precisely, a straight line is said to be a tangent of a curve at a point on the curve if the line passes through the point on the curve and has slope where "f" is the derivative of "f". A similar definition applies to space curves and curves in "n"-dimensional Euclidean space.
As it passes through the point where the tangent line and the curve meet, called the point of tangency, the tangent line is "going in the same direction" as the curve, and is thus the best straight-line approximation to the curve at that point.
Similarly, the tangent plane to a surface at a given point is the plane that "just touches" the surface at that point. The concept of a tangent is one of the most fundamental notions in differential geometry and has been extensively generalized; see Tangent space.
The word "tangent" comes from the Latin "tangere", "to touch".
History.
Euclid makes several references to the tangent (ἐφαπτομένη) to a circle in book III of the Elements (c. 300 BC). In Apollonius work "Conics" (ca. 225 BC) he defines a tangent as being "a line such that no other straight line could
fall between it and the curve".
Archimedes (ca. 287–222 BC) found the tangent to an Archimedean spiral by considering the path of a point moving along the curve.
In the 1630s Fermat developed the technique of adequality to calculate tangents and other problems in analysis and used this to calculate tangents to the parabola, the technique of adeqality is similar to taking the difference between formula_1 and formula_2 and dividing by a power of formula_3. Independently Descartes used his method of normals based on the observation that the radius of a circle is always normal to the circle itself.
These methods led to the development of differential calculus in the 17th Century. Many people contributed, Roberval discovered a general method of drawing tangents, by considering a curve as described by a moving point whose motion is the resultant of several simpler motions.
René-François de Sluse and Johannes Hudde found algebraic algorithms for finding tangents. Further developments we due to John Wallis, Isaac Barrow until the full theory of Isaac Newton and Gottfried Leibniz.
An 1828 definition of a tangent was "a right line which touches a curve, but which when produced, does not cut it". This old definition prevents inflection points from having any tangent. It has been dismissed and the modern definitions are equivalent to those of Leibniz who defined the tangent line as the line through a pair of infinitely close points on the curve.
Tangent line to a curve.
The intuitive notion that a tangent line "touches" a curve can be made more explicit by considering the sequence of straight lines (secant lines) passing through two points, "A" and "B", those that lie on the function curve. The tangent at "A" is the limit when point "B" approximates or tends to "A". The existence and uniqueness of the tangent line depends on a certain type of mathematical smoothness, known as "differentiability." For example, if two circular arcs meet at a sharp point (a vertex) then there is no uniquely defined tangent at the vertex because the limit of the progression of secant lines depends on the direction in which "point "B"" approaches the vertex.
At most points, the tangent touches the curve without crossing it (though it may, when continued, cross the curve at other places away from the point of tangent). A point where the tangent (at this point) crosses the curve is called an "inflection point". Circles, parabolas, hyperbolas and ellipses do not have any inflection point, but more complicated curves do have, like the graph of a cubic function, which has exactly one inflection point.
Conversely, it may happen that the curve lies entirely on one side of a straight line passing through a point on it, and yet this straight line is not a tangent line. This is the case, for example, for a line passing through the vertex of a triangle and not intersecting the triangle—where the tangent line does not exist for the reasons explained above. In convex geometry, such lines are called supporting lines.
Analytical approach.
The geometrical idea of the tangent line as the limit of secant lines serves as the motivation for analytical methods that are used to find tangent lines explicitly. The question of finding the tangent line to a graph, or the tangent line problem, was one of the central questions leading to the development of calculus in the 17th century. In the second book of his "Geometry", René Descartes of the problem of constructing the tangent to a curve, "And I dare say that this is not only the most useful and most general problem in geometry that I know, but even that I have ever desired to know".
Intuitive description.
Suppose that a curve is given as the graph of a function, "y" = "f"("x"). To find the tangent line at the point "p" = ("a", "f"("a")), consider another nearby point "q" = ("a" + "h", "f"("a" + "h")) on the curve. The slope of the secant line passing through "p" and "q" is equal to the difference quotient
As the point "q" approaches "p", which corresponds to making "h" smaller and smaller, the difference quotient should approach a certain limiting value "k", which is the slope of the tangent line at the point "p". If "k" is known, the equation of the tangent line can be found in the point-slope form:
More rigorous description.
To make the preceding reasoning rigorous, one has to explain what is meant by the difference quotient approaching a certain limiting value "k". The precise mathematical formulation was given by Cauchy in the 19th century and is based on the notion of limit. Suppose that the graph does not have a break or a sharp edge at "p" and it is neither plumb nor too wiggly near "p". Then there is a unique value of "k" such that, as "h" approaches 0, the difference quotient gets closer and closer to "k", and the distance between them becomes negligible compared with the size of "h", if "h" is small enough. This leads to the definition of the slope of the tangent line to the graph as the limit of the difference quotients for the function "f". This limit is the derivative of the function "f" at "x" = "a", denoted "f" ′("a"). Using derivatives, the equation of the tangent line can be stated as follows:
Calculus provides rules for computing the derivatives of functions that are given by formulas, such as the power function, trigonometric functions, exponential function, logarithm, and their various combinations. Thus, equations of the tangents to graphs of all these functions, as well as many others, can be found by the methods of calculus.
How the method can fail.
Calculus also demonstrates that there are functions and points on their graphs for which the limit determining the slope of the tangent line does not exist. For these points the function "f" is "non-differentiable". There are two possible reasons for the method of finding the tangents based on the limits and derivatives to fail: either the geometric tangent exists, but it is a vertical line, which cannot be given in the point-slope form since it does not have a slope, or the graph exhibits one of three behaviors that precludes a geometric tangent.
The graph "y" = "x"1/3 illustrates the first possibility: here the difference quotient at "a" = 0 is equal to "h"1/3/"h" = "h"−2/3, which becomes very large as "h" approaches 0. This curve has a tangent line at the origin that is vertical.
The graph "y" = "x"2/3 illustrates another possibility: this graph has a "cusp" at the origin. This means that, when "h" approaches 0, the difference quotient at "a" = 0 approaches plus or minus infinity depending on the sign of "x". Thus both branches of the curve are near to the half vertical line for which "y"=0, but none is near to the negative part of this line. Basically, there is no tangent at the origin in this case, but in some context one may consider this line as a tangent, and even, in algebraic geometry, as a "double tangent".
The graph "y" = |"x"| of the absolute value function consists of two straight lines with different slopes joined at the origin. As a point "q" approaches the origin from the right, the secant line always has slope 1. As a point "q" approaches the origin from the left, the secant line always has slope −1. Therefore, there is no unique tangent to the graph at the origin. Having two different (but finite) slopes is called a "corner".
Finally, since differentiability implies continuity, the contrapositive states "discontinuity" implies non-differentiability. Any such jump or point discontinuity will have no tangent line. This includes cases where one slope approaches positive infinity while the other approaches negative infinity, leading to an infinite jump discontinuity
Equations.
When the curve is given by "y" = "f"("x") then the slope of the tangent is formula_7
so by the point–slope formula the equation of the tangent line at ("X", "Y") is
where ("x", "y") are the coordinates of any point on the tangent line, and where the derivative is evaluated at formula_9.
When the curve is given by "y" = "f"("x"), the tangent line's equation can also be found by using polynomial division to divide formula_10 by formula_11; if the remainder is denoted by formula_12, then the equation of the tangent line is given by
When the equation of the curve is given in the form "f"("x", "y") = 0 then the value of the slope can be found by implicit differentiation, giving
The equation of the tangent line at a point ("X","Y") such that "f"("X","Y") = 0 is then
This equation remains true if formula_16 but formula_17 (in this case the slope of the tangent is infinite). If formula_18 the tangent line is not defined and the point ("X","Y") is said singular.
For algebraic curves, computations may be simplified somewhat by converting to homogeneous coordinates. Specifically, let the homogeneous equation of the curve be "g"("x", "y", "z") = 0 where "g" is a homogeneous function of degree "n". Then, if ("X", "Y", "Z") lies on the curve, Euler's theorem implies
It follows that the homogeneous equation of the tangent line is
The equation of the tangent line in Cartesian coordinates can be found by setting "z"=1 in this equation.
To apply this to algebraic curves, write "f"("x", "y") as
where each "u""r" is the sum of all terms of degree "r". The homogeneous equation of the curve is then
Applying the equation above and setting "z"=1 produces
as the equation of the tangent line. The equation in this form is often simpler to use in practice since no further simplification is needed after it is applied.
If the curve is given parametrically by
then the slope of the tangent is
giving the equation for the tangent line at formula_26 as
If formula_28, the tangent line is not defined. However, it may occur that the tangent line exists and may be computed from an implicit equation of the curve.
Normal line to a curve.
The line perpendicular to the tangent line to a curve at the point of tangency is called the "normal line" to the curve at that point. The slopes of perpendicular lines have product −1, so if the equation of the curve is "y" = "f"("x") then slope of the normal line is
and it follows that the equation of the normal line at (X, Y) is
Similarly, if the equation of the curve has the form "f"("x", "y") = 0 then the equation of the normal line is given by
If the curve is given parametrically by
then the equation of the normal line is
Angle between curves.
The angle between two curves at a point where they intersect is defined as the angle between their tangent lines at that point. More specifically, two curves are said to be tangent at a point if they have the same tangent at a point, and orthogonal if their tangent lines are orthogonal.
Multiple tangents at a point.
The formulas above fail when the point is a singular point. In this case there may be two or more branches of the curve which pass through the point, each branch having its own tangent line. When the point is the origin, the equations of these lines can be found for algebraic curves by factoring the equation formed by eliminating all but the lowest degree terms from the original equation. Since any point can be made the origin by a change of variables, this gives a method for finding the tangent lines at any singular point.
For example, the equation of the limaçon trisectrix shown to the right is
Expanding this and eliminating all but terms of degree 2 gives
which, when factored, becomes
So these are the equations of the two tangent lines through the origin.
When the curve is not self-crossing, the tangent at a reference point may still not be uniquely defined because the curve is not differentiable at that point although it is differentiable elsewhere. In this case the left and right derivatives are defined as the limits of the derivative as the point at which it is evaluated approaches the reference point from respectively the left (lower values) or the right (higher values). For example, the curve "y" = |"x" | is not differentiable at "x" = 0: its left and right derivatives have respective slopes –1 and 1; the tangents at that point with those slopes are called the left and right tangents.
Sometimes the slopes of the left and right tangent lines are equal, so the tangent lines coincide. This is true, for example, for the curve "y" = "x" 2/3, for which both the left and right derivatives at "x" = 0 are infinite; both the left and right tangent lines have equation "x" = 0.
Tangent circles.
Two circles of non-equal radius, both in the same plane, are said to be tangent to each other if they meet at only one point. Equivalently, two circles, with radii of "ri" and centers at ("xi", "yi"), for "i" = 1, 2 are said to be tangent to each other if
Surfaces and higher-dimensional manifolds.
The "tangent plane" to a surface at a given point "p" is defined in an analogous way to the tangent line in the case of curves. It is the best approximation of the surface by a plane at "p", and can be obtained as the limiting position of the planes passing through 3 distinct points on the surface close to "p" as these points converge to "p". More generally, there is a "k"-dimensional tangent space at each point of a "k"-dimensional manifold in the "n"-dimensional Euclidean space.

</doc>
<doc id="330981" url="https://en.wikipedia.org/wiki?curid=330981" title="Tau (particle)">
Tau (particle)

The tau (τ), also called the tau lepton, tau particle, or tauon, is an elementary particle similar to the electron, with negative electric charge and a spin of. Together with the electron, the muon, and the three neutrinos, it is a lepton. Like all elementary particles with half-integral spin, the tau has a corresponding antiparticle of opposite charge but equal mass and spin, which in the tau's case is the antitau (also called the "positive tau"). Tau particles are denoted by and the antitau by .
Tau leptons have a lifetime of and a mass of (compared to for muons and for electrons). Since their interactions are very similar to those of the electron, a tau can be thought of as a much heavier version of the electron. Because of their greater mass, tau particles do not emit as much bremsstrahlung radiation as electrons; consequently they are potentially highly penetrating, much more so than electrons. However, because of their short lifetime, the range of the tau is mainly set by their decay length, which is too small for bremsstrahlung to be noticeable: their penetrating power appears only at ultra high energy (above PeV energies).
As with the case of the other charged leptons, the tau has an associated tau neutrino, denoted by .
History.
The tau was detected in a series of experiments between 1974 and 1977 by Martin Lewis Perl with his colleagues at the SLAC-LBL group. Their equipment consisted of SLAC's then-new – colliding ring, called SPEAR, and the LBL magnetic detector. They could detect and distinguish between leptons, hadrons and photons. They did not detect the tau directly, but rather discovered anomalous events:
""We have discovered 64 events of the form"
"for which we have no conventional explanation.""
The need for at least two undetected particles was shown by the inability to conserve energy and momentum with only one. However, no other muons, electrons, photons, or hadrons were detected. It was proposed that this event was the production and subsequent decay of a new particle pair:
This was difficult to verify, because the energy to produce the pair is similar to the threshold for D meson production. Work done at DESY-Hamburg, and with the Direct Electron Counter (DELCO) at SPEAR, subsequently established the mass and spin of the tau.
The symbol τ was derived from the Greek "τρίτον" ("triton", meaning "third" in English), since it was the third charged lepton discovered.
Martin Perl shared the 1995 Nobel Prize in Physics with Frederick Reines. The latter was awarded his share of the prize for experimental discovery of the neutrino.
Tau decay.
The tau is the only lepton that can decay into hadrons – the other leptons do not have the necessary mass. Like the other decay modes of the tau, the hadronic decay is through the weak interaction.
The branching ratio of the dominant hadronic tau decays are:
In total, the tau lepton will decay hadronically approximately 64.79% of the time.
Since the tauonic lepton number is conserved in weak decays, a tau neutrino is always created when a tau decays.
The branching ratio of the common purely leptonic tau decays are:
The similarity of values of the two branching ratios is a consequence of lepton universality.
Exotic atoms.
The tau lepton is predicted to form exotic atoms like other charged subatomic particles. One of such, called tauonium by the analogy to muonium, consists in antitauon and an electron: .
Another one is an onium atom called "true tauonium" and is difficult to detect due to tau's extremely short lifetime at low (non-relativistic) energies needed to form this atom. Its detection is important for quantum electrodynamics.

</doc>
<doc id="2410184" url="https://en.wikipedia.org/wiki?curid=2410184" title="Tau neutrino">
Tau neutrino

The tau neutrino or tauon neutrino is a subatomic elementary particle which has the symbol and no net electric charge. Together with the tau, it forms the third generation of leptons, hence its name "tau neutrino". Its existence was immediately implied after the tau particle was detected in a series of experiments between 1974 and 1977 by Martin Lewis Perl with his colleagues at the SLAC–LBL group. The discovery of the tau neutrino was announced in July 2000 by the DONUT collaboration.
Discovery.
The tau neutrino is last of the leptons, and is the second most recent particle of the Standard Model to be discovered. The DONUT experiment (which stands for "Direct Observation of the Nu Tau") from Fermilab was built during the 1990s to specifically detect the tau neutrino. These efforts came to fruition in July 2000, when the DONUT collaboration reported its detection.

</doc>
<doc id="30448" url="https://en.wikipedia.org/wiki?curid=30448" title="Taylor series">
Taylor series

In mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point.
The concept of a Taylor series was formulated by the Scottish mathematician James Gregory and formally introduced by the English mathematician Brook Taylor in 1715. If the Taylor series is centered at zero, then that series is also called a Maclaurin series, named after the Scottish mathematician Colin Maclaurin, who made extensive use of this special case of Taylor series in the 18th century.
A function can be approximated by using a finite number of terms of its Taylor series. Taylor's theorem gives quantitative estimates on the error introduced by the use of such an approximation. The polynomial formed by taking some initial terms of the Taylor series is called a Taylor polynomial. The Taylor series of a function is the limit of that function's Taylor polynomials as the degree increases, provided that the limit exists. A function may not be equal to its Taylor series, even if its Taylor series converges at every point. A function that is equal to its Taylor series in an open interval (or a disc in the complex plane) is known as an analytic function in that interval.
Definition.
The Taylor series of a real or complex-valued function "ƒ"("x") that is infinitely differentiable at a real or complex number formula_1 is the power series
which can be written in the more compact sigma notation as
where "n"! denotes the factorial of "n" and "ƒ" ("n")("a") denotes the "n"th derivative of "ƒ" evaluated at the point "a". The derivative of order zero of "ƒ" is defined to be "ƒ" itself and and 0! are both defined to be 1. When , the series is also called a Maclaurin series.
Examples.
The Maclaurin series for any polynomial is the polynomial itself.
The Maclaurin series for is the geometric series
so the Taylor series for "x"−1 at is
By integrating the above Maclaurin series, we find the Maclaurin series for , where log denotes the natural logarithm:
and the corresponding Taylor series for log("x") at is
and more generally, the corresponding Taylor series for log("x") at some "a" = "x"0 is:
The Taylor series for the exponential function e"x" at "a" = 0 is
The above expansion holds because the derivative of e"x" with respect to x is also e"x" and e0 equals 1. This leaves the terms in the numerator and "n"! in the denominator for each term in the infinite sum.
History.
The Greek philosopher Zeno considered the problem of summing an infinite series to achieve a finite result, but rejected it as an impossibility: the result was Zeno's paradox. Later, Aristotle proposed a philosophical resolution of the paradox, but the mathematical content was apparently unresolved until taken up by Archimedes, as it had been prior to Aristotle by the Presocratic Atomist Democritus. It was through Archimedes's method of exhaustion that an infinite number of progressive subdivisions could be performed to achieve a finite result. Liu Hui independently employed a similar method a few centuries later.
In the 14th century, the earliest examples of the use of Taylor series and closely related methods were given by Madhava of Sangamagrama. Though no record of his work survives, writings of later Indian mathematicians suggest that he found a number of special cases of the Taylor series, including those for the trigonometric functions of sine, cosine, tangent, and arctangent. The Kerala school of astronomy and mathematics further expanded his works with various series expansions and rational approximations until the 16th century.
In the 17th century, James Gregory also worked in this area and published several Maclaurin series. It was not until 1715 however that a general method for constructing these series for all functions for which they exist was finally provided by Brook Taylor, after whom the series are now named.
The Maclaurin series was named after Colin Maclaurin, a professor in Edinburgh, who published the special case of the Taylor result in the 18th century.
Analytic functions.
If "f"("x") is given by a convergent power series in an open disc (or interval in the real line) centered at "b" in the complex plane, it is said to be analytic in this disc. Thus for "x" in this disc, "f" is given by a convergent power series
Differentiating by "x" the above formula "n" times, then setting "x"="b" gives:
and so the power series expansion agrees with the Taylor series. Thus a function is analytic in an open disc centered at "b" if and only if its Taylor series converges to the value of the function at each point of the disc.
If "f"("x") is equal to its Taylor series for all "x" in the complex plane, it is called entire. The polynomials, exponential function "e""x", and the trigonometric functions sine and cosine, are examples of entire functions. Examples of functions that are not entire include the square root, the logarithm, the trigonometric function tangent, and its inverse, arctan. For these functions the Taylor series do not converge if "x" is far from "b". That is, the Taylor series diverges at "x" if the distance between "x" and "b" is larger than the radius of convergence. The Taylor series can be used to calculate the value of an entire function at every point, if the value of the function, and of all of its derivatives, are known at a single point.
Uses of the Taylor series for analytic functions include:
Approximation and convergence.
Pictured on the right is an accurate approximation of sin("x") around the point "x" = 0. The pink curve is a polynomial of degree seven:
The error in this approximation is no more than |"x"|9/9!. In particular, for , the error is less than 0.000003.
In contrast, also shown is a picture of the natural logarithm function and some of its Taylor polynomials around "a" = 0. These approximations converge to the function only in the region −1 < "x" ≤ 1; outside of this region the higher-degree Taylor polynomials are worse approximations for the function. This is similar to Runge's phenomenon.
The error incurred in approximating a function by its "n"th-degree Taylor polynomial is called the remainder or "residual" and is denoted by the function "R""n"("x").
Taylor's theorem can be used to obtain a bound on the size of the remainder.
In general, Taylor series need not be convergent at all. And in fact the set of functions with a convergent Taylor series is a meager set in the Fréchet space of smooth functions. And even if the Taylor series of a function "f" does converge, its limit need not in general be equal to the value of the function "f"("x"). For example, the function
is infinitely differentiable at , and has all derivatives zero there. Consequently, the Taylor series of "f"("x") about is identically zero. However, "f"("x") is not the zero function, so does not equal its Taylor series around the origin. Thus, "f"("x") is an example of a non-analytic smooth function.
In real analysis, this example shows that there are infinitely differentiable functions "f"("x") whose Taylor series are "not" equal to "f"("x") even if they converge. By contrast, the holomorphic functions studied in complex analysis always possess a convergent Taylor series, and even the Taylor series of meromorphic functions, which might have singularities, never converge to a value different from the function itself. The complex function e−"z"−2, however, does not approach 0 when "z" approaches 0 along the imaginary axis, so it is not continuous in the complex plane and its Taylor series is undefined at 0.
More generally, every sequence of real or complex numbers can appear as coefficients in the Taylor series of an infinitely differentiable function defined on the real line, a consequence of Borel's lemma. As a result, the radius of convergence of a Taylor series can be zero. There are even infinitely differentiable functions defined on the real line whose Taylor series have a radius of convergence 0 everywhere.
Some functions cannot be written as Taylor series because they have a singularity; in these cases, one can often still achieve a series expansion if one allows also negative powers of the variable "x"; see Laurent series. For example, "f"("x") = "e"−"x"−2 can be written as a Laurent series.
Generalization.
There is, however, a generalization of the Taylor series that does converge to the value of the function itself for any bounded continuous function on (0,∞), using the calculus of finite differences. Specifically, one has the following theorem, due to Einar Hille, that for any "t" > 0,
Here Δ is the "n"-th finite difference operator with step size "h". The series is precisely the Taylor series, except that divided differences appear in place of differentiation: the series is formally similar to the Newton series. When the function "f" is analytic at "a", the terms in the series converge to the terms of the Taylor series, and in this sense generalizes the usual Taylor series.
In general, for any infinite sequence "a""i", the following power series identity holds:
So in particular,
The series on the right is the expectation value of "f"(a + "X"), where "X" is a Poisson distributed random variable that takes the value "jh" with probability "e"−"t"/"h"("t"/"h")"j"/"j"!. Hence,
The law of large numbers implies that the identity holds.
List of Maclaurin series of some common functions.
Several important Maclaurin series expansions follow. All these expansions are valid for complex arguments "x".
Exponential function:
Natural logarithm:
Geometric series and its derivatives (see article for variants):
Binomial series (includes the square root for "α" = 1/2 and the infinite geometric series for "α" = −1):
with generalized binomial coefficients
For instance, with the first several terms written out explicitly for the common square root cases, is:
Trigonometric functions:
Hyperbolic functions:
The numbers "B""k" appearing in the "summation" expansions of tan("x") and tanh("x") are the Bernoulli numbers. The "E""k" in the expansion of sec("x") are Euler numbers.
Calculation of Taylor series.
Several methods exist for the calculation of Taylor series of a large number of functions. One can attempt to use the definition of the Taylor series, though this often requires generalizing the form of the coefficients according to a readily apparent pattern. Alternatively, one can use manipulations such as substitution, multiplication or division, addition or subtraction of standard Taylor series to construct the Taylor series of a function, by virtue of Taylor series being power series. In some cases, one can also derive the Taylor series by repeatedly applying integration by parts. Particularly convenient is the use of computer algebra systems to calculate Taylor series.
First example.
In order to compute the 7th degree Maclaurin polynomial for the function
one may first rewrite the function as
The Taylor series for the natural logarithm is (using the big O notation)
and for the cosine function
The latter series expansion has a zero constant term, which enables us to substitute the second series into the first one and to easily omit terms of higher order than the 7th degree by using the big O notation:
Since the cosine is an even function, the coefficients for all the odd powers "x", "x"3, "x"5, "x"7, .. have to be zero.
Second example.
Suppose we want the Taylor series at 0 of the function
We have for the exponential function
and, as in the first example,
Assume the power series is
Then multiplication with the denominator and substitution of the series of the cosine yields
Collecting the terms up to fourth order yields
Comparing coefficients with the above series of the exponential function yields the desired Taylor series
Third example.
Here we employ a method called "Indirect Expansion" to expand the given function.
This method uses the known Taylor expansion of the exponential function.
In order to expand
as a Taylor series in "x", we use the known Taylor series of function "e""x":
Thus,
Taylor series as definitions.
Classically, algebraic functions are defined by an algebraic equation, and transcendental functions (including those discussed above) are defined by some property that holds for them, such as a differential equation. For example, the exponential function is the function which is equal to its own derivative everywhere, and assumes the value 1 at the origin. However, one may equally well define an analytic function by its Taylor series.
Taylor series are used to define functions and "operators" in diverse areas of mathematics. In particular, this is true in areas where the classical definitions of functions break down. For example, using Taylor series, one may define analytical functions of matrices and operators, such as the matrix exponential or matrix logarithm.
In other areas, such as formal analysis, it is more convenient to work directly with the power series themselves. Thus one may define a solution of a differential equation "as" a power series which, one hopes to prove, is the Taylor series of the desired solution.
Taylor series in several variables.
The Taylor series may also be generalized to functions of more than one variable with
For example, for a function that depends on two variables, "x" and "y", the Taylor series to second order about the point ("a", "b") is
where the subscripts denote the respective partial derivatives.
A second-order Taylor series expansion of a scalar-valued function of more than one variable can be written compactly as
where formula_60 is the gradient of formula_61 evaluated at formula_62 and formula_63 is the Hessian matrix. Applying the multi-index notation the Taylor series for several variables becomes
which is to be understood as a still more abbreviated multi-index version of the first equation of this paragraph, again in full analogy to the single variable case.
Example.
Compute a second-order Taylor series expansion around point ("a", "b") = (0, 0) of a function
Firstly, we compute all partial derivatives we need
The Taylor series is
which in this case becomes
Since is analytic in |"y"| < 1, we have
for |"y"| < 1.
Comparison with Fourier series.
The trigonometric Fourier series enables one to express a periodic function (or a function defined on a closed interval ) as an infinite sum of trigonometric functions (sines and cosines). In this sense, the Fourier series is analogous to Taylor series, since the latter allows to express a function as an infinite sum of powers. Nevertheless the two series differ from each other in several relevant issues:

</doc>
<doc id="296036" url="https://en.wikipedia.org/wiki?curid=296036" title="Technicolor (physics)">
Technicolor (physics)

Technicolor theories are models of physics beyond the standard model that address electroweak gauge symmetry breaking, the mechanism through which W and Z bosons acquire masses. Early technicolor theories were modelled on quantum chromodynamics (QCD), the "color" theory of the strong nuclear force, which inspired their name.
Instead of introducing elementary Higgs bosons to explain observed phenomena, technicolor models hide electroweak symmetry and generate masses for the W and Z bosons through the dynamics of new gauge interactions. Although asymptotically free at very high energies, these interactions must become strong and confining (and hence unobservable) at lower energies that have been experimentally probed. This dynamical approach is natural and avoids issues of Quantum triviality and the hierarchy problem of the Standard Model.
In order to produce quark and lepton masses, technicolor has to be "extended" by additional gauge interactions. Particularly when modelled on QCD, extended technicolor is challenged by experimental constraints on flavor-changing neutral current and precision electroweak measurements. It is not known what is the extended technicolor dynamics.
Much technicolor research focuses on exploring strongly interacting gauge theories other than QCD, in order to evade some of these challenges. A particularly active framework is "walking" technicolor, which exhibits nearly conformal behavior caused by an infrared fixed point with strength just above that necessary for spontaneous chiral symmetry breaking. Whether walking can occur and lead to agreement with precision electroweak measurements is being studied through non-perturbative lattice simulations.
Experiments at the Large Hadron Collider are expected to discover the mechanism responsible for electroweak symmetry breaking, and will be critical for determining whether the technicolor framework provides the correct description of nature. In 2012 these experiments declared the discovery of a Higgs-like boson with mass approximately ; such a particle is not generically predicted by technicolor models, but can be accommodated by them.
Introduction.
The mechanism for the breaking of electroweak gauge symmetry in the Standard Model of elementary particle interactions remains unknown. The breaking must be spontaneous, meaning that the underlying theory manifests the symmetry exactly (the gauge-boson fields are massless in the equations of motion), but the solutions (the ground state and the excited states) do not. In particular, the physical "W" and "Z" gauge bosons become massive. This phenomenon, in which the "W" and "Z" bosons also acquire an extra polarization state, is called the "Higgs mechanism". Despite the precise agreement of the electroweak theory with experiment at energies accessible so far, the necessary ingredients for the symmetry breaking remain hidden, yet to be revealed at higher energies.
The simplest mechanism of electroweak symmetry breaking introduces a single complex field and predicts the existence of the Higgs boson. Typically, the Higgs boson is "unnatural" in the sense that quantum mechanical fluctuations produce corrections to its mass that lift it to such high values that it cannot play the role for which it was introduced. Unless the Standard Model breaks down at energies less than a few TeV, the Higgs mass can be kept small only by a delicate fine-tuning of parameters.
Technicolor avoids this problem by hypothesizing a new gauge interaction coupled to new massless fermions. This interaction is asymptotically free at very high energies and becomes strong and confining as the energy decreases to the electroweak scale of 246 GeV. These strong forces spontaneously break the massless fermions' chiral symmetries, some of which are weakly gauged as part of the Standard Model. This is the dynamical version of the Higgs mechanism. The electroweak gauge symmetry is thus broken, producing masses for the "W" and "Z" bosons.
The new strong interaction leads to a host of new composite, short-lived particles at energies accessible at the Large Hadron Collider (LHC). This framework is natural because there are no elementary Higgs bosons and, hence, no fine-tuning of parameters. Quark and lepton masses also break the electroweak gauge symmetries, so they, too, must arise spontaneously. A mechanism for incorporating this feature is known as extended technicolor. Technicolor and extended technicolor face a number of phenomenological challenges, in particular issues of flavor-changing neutral currents, precision electroweak tests, and the top quark mass. Technicolor models also do not generically predict Higgs-like bosons as light as ; such a particle was discovered by experiments at the Large Hadron Collider in 2012. Some of these issues can be addressed with a class of theories known as walking technicolor.
Early technicolor.
Technicolor is the name given to the theory of electroweak symmetry breaking by new strong gauge-interactions whose characteristic energy scale ΛTC is the weak scale itself, ΛTC ≅ FEW ≡ 246 GeV. The guiding principle of technicolor is "naturalness": basic physical phenomena should not require fine-tuning of the parameters in the Lagrangian that describes them. What constitutes fine-tuning is to some extent a subjective matter, but a theory with elementary scalar particles typically is very finely tuned (unless it is supersymmetric). The quadratic divergence in the scalar's mass requires adjustments of a part in formula_1, where "M"bare is the cutoff of the theory, the energy scale at which the theory changes in some essential way. In the standard electroweak model with "M"bare ∼ 1015 GeV (the grand-unification mass scale), and with the Higgs boson mass "M"physical = 100–500 GeV, the mass is tuned to at least a part in 1025.
By contrast, a natural theory of electroweak symmetry breaking is an asymptotically free gauge theory with fermions as the only matter fields. The technicolor gauge group "G"TC is often assumed to be "SU"("N"TC). Based on analogy with quantum chromodynamics (QCD), it is assumed that there are one or more doublets of massless Dirac "technifermions" transforming vectorially under the same complex representation of "G"TC, "T"iL,R = ("U"i,"D"i)L,R, "i" = 1,2, …, "N"f/2. Thus, there is a chiral symmetry of these fermions, e.g., "SU"("N"f)L ⊗ "SU"("N"f)R, if they all transform according the same complex representation of "G"TC. Continuing the analogy with QCD, the running gauge coupling αTC(μ) triggers spontaneous chiral symmetry breaking, the technifermions acquire a dynamical mass, and a number of massless Goldstone bosons result. If the technifermions transform under ["SU"(2) ⊗ "U"(1)]EW as left-handed doublets and right-handed singlets, three linear combinations of these Goldstone bosons couple to three of the electroweak gauge currents.
In 1973 Jackiw and Johnson and Cornwall and Norton studied the possibility that a (non-vectorial) gauge interaction of fermions can break itself; i.e., is strong enough to form a Goldstone boson coupled to the gauge current. Using Abelian gauge models, they showed that, "if" such a Goldstone boson is formed, it is "eaten" by the Higgs mechanism, becoming the longitudinal component of the now massive gauge boson. Technically, the polarization function Π("p"2) appearing in the gauge boson propagator, Δμν = ("p"μ "p"ν/"p"2 - "g"μν)/["p"2(1 - "g"2 Π("p"2))] develops a pole at "p"2 = 0 with residue "F"2, the square of the Goldstone boson's decay constant, and the gauge boson acquires mass "M" ≅ "g F". In 1973, Weinstein showed that composite Goldstone bosons whose constituent fermions transform in the “standard” way under "SU"(2) ⊗ "U"(1) generate the weak boson masses
formula_2
This standard-model relation is achieved with elementary Higgs bosons in electroweak doublets; it is verified experimentally to better than 1%. Here, "g" and "g"′ are "SU"(2) and "U"(1) gauge couplings and tanθW = "g"′/"g" defines the weak mixing angle.
The important idea of a "new" strong gauge interaction of massless fermions at the electroweak scale "F"EW driving the spontaneous breakdown of its global chiral symmetry, of which an "SU"(2) ⊗ "U"(1) subgroup is weakly gauged, was first proposed in 1979 by S. Weinberg and L. Susskind. This "technicolor" mechanism is natural in that no fine-tuning of parameters is necessary.
Extended technicolor.
Elementary Higgs bosons perform another important task. In the Standard Model, quarks and leptons are necessarily massless because they transform under "SU"(2) ⊗ "U"(1) as left-handed doublets and right-handed singlets. The Higgs doublet couples to these fermions. When it develops its vacuum expectation value, it transmits this electroweak breaking to the quarks and leptons, giving them their observed masses. (In general, electroweak-eigenstate fermions are not mass eigenstates, so this process also induces the mixing matrices observed in charged-current weak interactions.)
In technicolor, something else must generate the quark and lepton masses. The only natural possibility, one avoiding the introduction of elementary scalars, is to enlarge "G"TC to allow technifermions to couple to quarks and leptons. This coupling is induced by gauge bosons of the enlarged group. The picture, then, is that there is a large "extended technicolor" (ETC) gauge group "G"ETC ⊃ "G"TC in which technifermions, quarks, and leptons live in the same representations. At one or more high scales ΛETC, "G"ETC is broken down to "G"TC, and quarks and leptons emerge as the TC-singlet fermions. When αTC(μ) becomes strong at scale ΛTC ≅ "F"EW, the fermionic condensate formula_3 forms. (The condensate is the vacuum expectation value of the technifermion bilinear formula_4. The estimate here is based on naive dimensional analysis of the quark condensate in QCD, expected to be correct as an order of magnitude.) Then, the transitions formula_5 can proceed through the technifermion's dynamical mass by the emission and reabsorption of ETC bosons whose masses "M"ETC ≅ "g"ETC ΛETC are much greater than ΛTC. The quarks and leptons develop masses given approximately by
formula_6
Here, formula_7 is the technifermion condensate renormalized at the ETC boson mass scale,
formula_8
where γm(μ) is the anomalous dimension of the technifermion bilinear formula_9 at the scale μ. The second estimate in Eq. (2) depends on the assumption that, as happens in QCD, αTC(μ) becomes weak not far above ΛTC, so that the anomalous dimension γm of formula_9 is small there. Extended technicolor was introduced in 1979 by Dimopoulos and Susskind, and by Eichten and Lane. For a quark of mass "m"q ≅ 1 GeV, and with ΛTC ≅ 246 GeV, one estimates ΛETC ≅ 15 TeV. Therefore, assuming that formula_11, "M"ETC will be at least this large.
In addition to the ETC proposal for quark and lepton masses, Eichten and Lane observed that the size of the ETC representations required to generate all quark and lepton masses suggests that there will be more than one electroweak doublet of technifermions. If so, there will be more (spontaneously broken) chiral symmetries and therefore more Goldstone bosons than are eaten by the Higgs mechanism. These must acquire mass by virtue of the fact that the extra chiral symmetries are also explicitly broken, by the standard-model interactions and the ETC interactions. These "pseudo-Goldstone bosons" are called technipions, πT. An application of Dashen's theorem gives for the ETC contribution to their mass
formula_12
The second approximation in Eq. (4) assumes that formula_13. For "F"EW ≅ ΛTC ≅ 246 GeV and ΛETC ≅ 15 TeV, this contribution to "M"πT is about 50 GeV. Since ETC interactions generate formula_14 "and" the coupling of technipions to quark and lepton pairs, one expects the couplings to be Higgs-like; i.e., roughly proportional to the masses of the quarks and leptons. This means that technipions are expected to decay to the heaviest formula_15 and formula_16 pairs allowed.
Perhaps the most important restriction on the ETC framework for quark mass generation is that ETC interactions are likely to induce flavor-changing neutral current processes such as μ → "e" γ, "K"L → μ "e", and |Δ "S"| = 2 and |Δ "B"| = 2 interactions that induce formula_17 and formula_18 mixing. The reason is that the algebra of the ETC currents involved in formula_14 generation imply formula_20 and formula_21 ETC currents which, when written in terms of fermion mass eigenstates, have no reason to conserve flavor. The strongest constraint comes from requiring that ETC interactions mediating formula_22 mixing contribute less than the Standard Model. This implies an effective ΛETC greater than 1000 TeV. The actual ΛETC may be reduced somewhat if CKM-like mixing angle factors are present. If these interactions are CP-violating, as they well may be, the constraint from the ε-parameter is that the effective ΛETC > 104 TeV. Such huge ETC mass scales imply tiny quark and lepton masses and ETC contributions to "M"π"T" of at most a few GeV, in conflict with LEP searches for πT at the "Z"0.
Extended technicolor is a very ambitious proposal, requiring that quark and lepton masses and mixing angles arise from experimentally accessible interactions. "If" there exists a successful model, it would not only predict the masses and mixings of quarks and leptons (and technipions), it would explain why there are three families of each: they are the ones that fit into the ETC representations of "q", formula_23 and "T". It should not be surprising that the construction of a successful model has proven to be very difficult.
Walking technicolor.
Since quark and lepton masses are proportional to the bilinear technifermion condensate divided by the ETC mass scale squared, their tiny values can be avoided if the condensate is enhanced above the weak-αTC estimate in Eq. (2), formula_24.
During the 1980s, several dynamical mechanisms were advanced to do this. In 1981 Holdom suggested that, if the αTC(μ) evolves to a nontrivial fixed point in the ultraviolet, with a large positive anomalous dimension γm for formula_4, realistic quark and lepton masses could arise with ΛETC large enough to suppress ETC-induced formula_26 mixing. However, no example of a nontrivial ultraviolet fixed point in a four-dimensional gauge theory has been constructed. In 1985 Holdom analyzed a technicolor theory in which a “slowly varying” αTC(μ) was envisioned. His focus was to separate the chiral breaking and confinement scales, but he also noted that such a theory could enhance formula_27 and thus allow the ETC scale to be raised. In 1986 Akiba and Yanagida also considered enhancing quark and lepton masses, by simply assuming that αTC is constant and strong all the way up to the ETC scale. In the same year Yamawaki, Bando and Matumoto again imagined an ultraviolet fixed point in a non-asymptotically free theory to enhance the technifermion condensate.
In 1986 Appelquist, Karabali and Wijewardhana discussed the enhancement of fermion masses in an asymptotically free technicolor theory with a slowly running, or “walking”, gauge coupling. The slowness arose from the screening effect of a large number of technifermions, with the analysis carried out through two-loop perturbation theory. In 1987 Appelquist and Wijewardhana explored this walking scenario further. They took the analysis to three loops, noted that the walking can lead to a power law enhancement of the technifermion condensate, and estimated the resultant quark, lepton, and technipion masses. The condensate enhancement arises because the associated technifermion mass decreases slowly, roughly linearly, as a function of its renormalization scale. This corresponds to the condensate anomalous dimension γm in Eq. (3) approaching unity (see below).
In the 1990s, the idea emerged more clearly that walking is naturally described by asymptotically free gauge theories dominated in the infrared by an approximate fixed point. Unlike the speculative proposal of ultraviolet fixed points, fixed points in the infrared are known to exist in asymptotically free theories, arising at two loops in the beta function providing that the fermion count "N"f is large enough. This has been known since the first two-loop computation in 1974 by Caswell. If "N"f is close to the value formula_28 at which asymptotic freedom is lost, the resultant infrared fixed point is weak, of parametric order formula_29, and reliably accessible in perturbation theory. This weak-coupling limit was explored by Banks and Zaks in 1982.
The fixed-point coupling αIR becomes stronger as "N"f is reduced from formula_28. Below some critical value "N"fc the coupling becomes strong enough (> αχ "SB") to break spontaneously the massless technifermions' chiral symmetry. Since the analysis must typically go beyond two-loop perturbation theory, the definition of the running coupling αTC(μ), its fixed point value αIR, and the strength αχ SB necessary for chiral symmetry breaking depend on the particular renormalization scheme adopted. For formula_31; i.e., for "N"f just below "N"fc, the evolution of αTC(μ) is governed by the infrared fixed point and it will evolve slowly (walk) for a range of momenta above the breaking scale ΛTC. To overcome the formula_32-suppression of the masses of first and second generation quarks involved in formula_22 mixing, this range must extend almost to their ETC scale, of formula_34. Cohen and Georgi argued that γm = 1 is the signal of spontaneous chiral symmetry breaking, i.e., that γm(αχ "SB") = 1. Therefore, in the walking-αTC region, γm ≅ 1 and, from Eqs. (2) and (3), the light quark masses are enhanced approximately by "M"ETC/ΛTC.
The idea that αTC(μ) walks for a large range of momenta when αIR lies just above αχ "SB" was suggested by Lane and Ramana. They made an explicit model, discussed the walking that ensued, and used it in their discussion of walking technicolor phenomenology at hadron colliders. This idea was developed in some detail by Appelquist, Terning and Wijewardhana. Combining a perturbative computation of the infrared fixed point with an approximation of αχ "SB" based on the Schwinger-Dyson equation, they estimated the critical value "N"fc and explored the resultant electroweak physics. Since the 1990s, most discussions of walking technicolor are in the framework of theories assumed to be dominated in the infrared by an approximate fixed point. Various models have been explored, some with the technifermions in the fundamental representation of the gauge group and some employing higher representations.
The possibility that the technicolor condensate can be enhanced beyond that discussed in the walking literature, has also been considered recently by Luty and Okui under the name "conformal technicolor". They envision an infrared stable fixed point, but with a very large anomalous dimension for the operator formula_4. It remains to be seen whether this can be realized, for example, in the class of theories currently being examined using lattice techniques.
Top quark mass.
The walking enhancement described above may be insufficient to generate the measured top quark mass, even for an ETC scale as low as a few TeV. However, this problem could be addressed if the effective four-technifermion coupling resulting from ETC gauge boson exchange is strong and tuned just above a critical value. The analysis of this strong-ETC possibility is that of a Nambu–Jona–Lasinio model with an additional (technicolor) gauge interaction. The technifermion masses are small compared to the ETC scale (the cutoff on the effective theory), but nearly constant out to this scale, leading to a large top quark mass. No fully realistic ETC theory for all quark masses has yet been developed incorporating these ideas. A related study was carried out by Miransky and Yamawaki. A problem with this approach is that it involves some degree of parameter fine-tuning, in conflict with technicolor’s guiding principle of naturalness.
Finally, it should be noted that there is a large body of closely related work in which ETC does not generate "m"t. These are the top quark condensate, topcolor and top-color-assisted technicolor models, in which new strong interactions are ascribed to the top quark and other third-generation fermions. As with the strong-ETC scenario described above, all these proposals involve a considerable degree of fine-tuning of gauge couplings.
Technicolor on the lattice.
Lattice gauge theory is a non-perturbative method applicable to strongly interacting technicolor theories, allowing first-principles exploration of walking and conformal dynamics. In 2007, Catterall and Sannino used lattice gauge theory to study "SU"(2) gauge theories with two flavors of Dirac fermions in the symmetric representation, finding evidence of conformality that has been confirmed by subsequent studies.
As of 2010, the situation for "SU"(3) gauge theory with fermions in the fundamental representation is not as clear-cut. In 2007, Appelquist, Fleming and Neil reported evidence that a non-trivial infrared fixed point develops in such theories when there are twelve flavors, but not when there are eight. While some subsequent studies confirmed these results, others reported different conclusions, depending on the lattice methods used, and there is not yet consensus.
Further lattice studies exploring these issues, as well as considering the consequences of these theories for precision electroweak measurements, are underway by several research groups.
Technicolor phenomenology.
Any framework for physics beyond the Standard Model must conform with precision measurements of the electroweak parameters. Its consequences for physics at existing and future high-energy hadron colliders, and for the dark matter of the universe must also be explored.
Precision electroweak tests.
In 1990, the phenomenological parameters "S", "T", and "U" were introduced by Peskin and Takeuchi to quantify contributions to electroweak radiative corrections from physics beyond the Standard Model. They have a simple relation to the parameters of the electroweak chiral Lagrangian. The Peskin-Takeuchi analysis was based on the general formalism for weak radiative corrections developed by Kennedy, Lynn, Peskin and Stuart, and alternate formulations also exist.
The "S", "T", and "U"-parameters describe corrections to the electroweak gauge boson propagators from physics Beyond the Standard Model. They can be written in terms of polarization functions of electroweak currents and their spectral representation as follows:
formula_36
where only new, beyond-standard-model physics is included. The quantities are calculated relative to a minimal Standard Model with some chosen reference mass of the Higgs boson, taken to range from the experimental lower bound of 117 GeV to 1000 GeV where its width becomes very large. For these parameters to describe the dominant corrections to the Standard Model, the mass scale of the new physics must be much greater than "M"W and "M"Z, and the coupling of quarks and leptons to the new particles must be suppressed relative to their coupling to the gauge bosons. This is the case with technicolor, so long as the lightest technivector mesons, ρT and "a"T, are heavier than 200–300 GeV. The "S"-parameter is sensitive to all new physics at the TeV scale, while "T" is a measure of weak-isospin breaking effects. The "U"-parameter is generally not useful; most new-physics theories, including technicolor theories, give negligible contributions to it.
The "S" and "T"-parameters are determined by global fit to experimental data including "Z"-pole data from LEP at CERN, top quark and "W"-mass measurements at Fermilab, and measured levels of atomic parity violation. The resultant bounds on these parameters are given in the Review of Particle Properties. Assuming "U" = 0, the "S" and "T" parameters are small and, in fact, consistent with zero:
formula_37
where the central value corresponds to a Higgs mass of 117 GeV and the correction to the central value when the Higgs mass is increased to 300 GeV is given in parentheses. These values place tight restrictions on beyond-standard-model theories—when the relevant corrections can be reliably computed.
The "S" parameter estimated in QCD-like technicolor theories is significantly greater than the experimentally allowed value. The computation was done assuming that the spectral integral for "S" is dominated by the lightest ρT and "a"T resonances, or by scaling effective Lagrangian parameters from QCD. In walking technicolor, however, the physics at the TeV scale and beyond must be quite different from that of QCD-like theories. In particular, the vector and axial-vector spectral functions cannot be dominated by just the lowest-lying resonances. It is unknown whether higher energy contributions to formula_38 are a tower of identifiable ρT and "a"T states or a smooth continuum. It has been conjectured that ρT and "a"T partners could be more nearly degenerate in walking theories (approximate parity doubling), reducing their contribution to "S". Lattice calculations are underway or planned to test these ideas and obtain reliable estimates of "S" in walking theories.
The restriction on the "T"-parameter poses a problem for the generation of the top-quark mass in the ETC framework. The enhancement from walking can allow the associated ETC scale to be as large as a few TeV, but—since the ETC interactions must be strongly weak-isospin breaking to allow for the large top-bottom mass splitting—the contribution to the "T" parameter, as well as the rate for the decay formula_39, could be too large.
Hadron collider phenomenology.
Early studies generally assumed the existence of just one electroweak doublet of technifermions, or of one techni-family including one doublet each of color-triplet techniquarks and color-singlet technileptons (four electroweak doublets in total). The number "ND" of electroweak doublets determines the decay constant "F" needed to produce the correct electroweak scale, as "F" = "F"EW/ = 246 GeV/. In the minimal, one-doublet model, three Goldstone bosons (technipions, πT) have decay constant "F" = "F"EW = 246 GeV and are eaten by the electroweak gauge bosons. The most accessible collider signal is the production through formula_15 annihilation in a hadron collider of spin-one formula_41, and their subsequent decay into a pair of longitudinally polarized weak bosons, formula_42 and formula_43. At an expected mass of 1.5–2.0 TeV and width of 300–400 GeV, such ρT's would be difficult to discover at the LHC. A one-family model has a large number of physical technipions, with "F" = "F"EW/ = 123 GeV. There is a collection of correspondingly lower-mass color-singlet and octet technivectors decaying into technipion pairs. The πT's are expected to decay to the heaviest possible quark and lepton pairs. Despite their lower masses, the ρT's are wider than in the minimal model and the backgrounds to the πT decays are likely to be insurmountable at a hadron collider.
This picture changed with the advent of walking technicolor. A walking gauge coupling occurs if αχ "SB" lies just below the IR fixed point value αIR, which requires either a large number of electroweak doublets in the fundamental representation of the gauge group, e.g., or a few doublets in higher-dimensional TC representations. In the latter case, the constraints on ETC representations generally imply other technifermions in the fundamental representation as well. In either case, there are technipions πT with decay constant formula_44. This implies formula_45 so that the lightest technivectors accessible at the LHC—ρT, ωT, "a"T (with "I"G "J"PC = 1+ 1−−, 0− 1−−, 1− 1++)—have masses well below a TeV. The class of theories with many technifermions and thus formula_44 is called low-scale technicolor.
A second consequence of walking technicolor concerns the decays of the spin-one technihadrons. Since technipion masses formula_47 (see Eq. (4)), walking enhances them much more than it does other technihadron masses. Thus, it is very likely that the lightest "M"ρT < 2"M"πT and that the two and three-πT decay channels of the light technivectors are closed. This further implies that these technivectors are very narrow. Their most probable two-body channels are formula_48, "W"L "W"L, γ πT and γ "W"L. The coupling of the lightest technivectors to "W"L is proportional to "F/F"EW. Thus, all their decay rates are suppressed by powers of formula_49 or the fine-structure constant, giving total widths of a few GeV (for ρT) to a few tenths of a GeV (for ωT and T).
A more speculative consequence of walking technicolor is motivated by consideration of its contribution to the "S"-parameter. As noted above, the usual assumptions made to estimate "S"TC are invalid in a walking theory. In particular, the spectral integrals used to evaluate "S"TC cannot be dominated by just the lowest-lying ρT and "a"T and, if "S"TC is to be small, the masses and weak-current couplings of the ρT and "a"T could be more nearly equal than they are in QCD.
Low-scale technicolor phenomenology, including the possibility of a more parity-doubled spectrum, has been developed into a set of rules and decay amplitudes. An April 2011 announcement of an excess in jet pairs produced in association with a W boson measured at the Tevatron has been interpreted by Eichten, Lane and Martin as a possible signal of the technipion of low-scale technicolor.
The general scheme of low-scale technicolor makes little sense if the limit on formula_50 is pushed past about 700 GeV. The LHC should be able to discover it or rule it out. Searches there involving decays to technipions and thence to heavy quark jets are hampered by backgrounds from formula_51 production; its rate is 100 times larger than that at the Tevatron. Consequently, the discovery of low-scale technicolor at the LHC relies on all-leptonic final-state channels with favorable signal-to-background ratios: formula_52, formula_53 and formula_54.
Dark matter.
Technicolor theories naturally contain dark matter candidates. Almost certainly, models can be built in which the lowest-lying technibaryon, a technicolor-singlet bound state of technifermions, is stable enough to survive the evolution of the universe. If the technicolor theory is low-scale (formula_44), the baryon's mass should be no more than 1–2 TeV. If not, it could be much heavier. The technibaryon must be electrically neutral and satisfy constraints on its abundance. Given the limits on spin-independent dark-matter-nucleon cross sections from dark-matter search experiments (formula_56 for the masses of interest), it may have to be electroweak neutral (weak isospin "I" = 0) as well. These considerations suggest that the "old" technicolor dark matter candidates may be difficult to produce at the LHC.
A different class of technicolor dark matter candidates light enough to be accessible at the LHC was introduced by Francesco Sannino and his collaborators. These states are pseudo Goldstone bosons possessing a global charge that makes them stable against decay.

</doc>
<doc id="20647050" url="https://en.wikipedia.org/wiki?curid=20647050" title="Temperature">
Temperature

A temperature is an objective comparative measure of hot or cold. It is measured by a thermometer, which may work through the bulk behavior of a thermometric material, detection of thermal radiation, or particle kinetic energy. Several scales and units exist for measuring temperature, the most common being Celsius (denoted °C; formerly called "centigrade"), Fahrenheit (denoted °F), and, especially in science, Kelvin (denoted K).
The coldest theoretical temperature is absolute zero, at which the thermal motion in matter would be zero. However, an actual physical system or object can never attain a temperature of absolute zero. Absolute zero is denoted as 0 K on the Kelvin scale, −273.15 °C on the Celsius scale, and −459.67 °F on the Fahrenheit scale.
The kinetic theory offers a valuable but limited account of the behavior of the materials of macroscopic systems, especially of fluids. It indicates the absolute temperature as proportional to the average kinetic energy of the random microscopic motions of their constituent microscopic particles such as electrons, atoms, and molecules.
Temperature is important in all fields of natural science, including physics, geology, chemistry, atmospheric sciences, medicine, and biology—as well as most aspects of daily life.
Effects of temperature.
Many physical processes are affected by temperature, such as
Temperature scales.
Temperature scales differ in two ways: the point chosen as zero degrees, and the magnitudes of incremental units or degrees on the scale.
The Celsius scale (°C) is used for common temperature measurements in most of the world. It is an empirical scale. It developed by a historical progress, which led to its zero point being defined by the freezing point of water, with additional degrees defined so that was the boiling point of water, both at sea-level atmospheric pressure. Because of the 100 degree interval, it is called a centigrade scale. Since the standardization of the kelvin in the International System of Units, it has subsequently been redefined in terms of the equivalent fixing points on the Kelvin scale, and so that a temperature increment of one degree Celsius is the same as an increment of one kelvin, though they differ by an additive offset of 273.15.
The United States commonly uses the Fahrenheit scale, on which water freezes at 32 °F and boils at 212 °F at sea-level atmospheric pressure.
Many scientific measurements use the kelvin temperature scale (unit symbol  K), named in honor of the Scottish physicist who first defined it. It is a thermodynamic or absolute temperature scale. Its zero point, , is defined to coincide with coldest physically-possible temperature (called absolute zero). Its degrees are defined through thermodynamics. The temperature of absolute zero occurs at = (or −459.67 °F), and the freezing point of water at sea-level atmospheric pressure occurs at = .
The International System of Units (SI) defines a scale and unit for the kelvin or thermodynamic temperature by using the reliably reproducible temperature of the triple point of water as a second reference point (the first reference point being 0 K at absolute zero). The triple point is a singular state with its own unique and invariant temperature and pressure, along with, for a fixed mass of water in a vessel of fixed volume, an autonomically and stably self-determining partition into three mutually contacting phases, vapour, liquid, and solid, dynamically depending only on the total internal energy of the mass of water. For historical reasons, the triple point temperature of water is fixed at 273.16 units of the measurement increment.
Thermodynamic approach to temperature.
Temperature is one of the principal quantities in the study of thermodynamics.
Kinds of temperature scale.
There is a variety of kinds of temperature scale. It may be convenient to classify them as empirically and theoretically based. Empirical temperature scales are historically older, while theoretically based scales arose in the middle of the nineteenth century.
Empirically based scales.
Empirically based temperature scales rely directly on measurements of simple physical properties of materials. For example, the length of a column of mercury, confined in a glass-walled capillary tube, is dependent largely on temperature, and is the basis of the very useful mercury-in-glass thermometer. Such scales are valid only within convenient ranges of temperature. For example, above the boiling point of mercury, a mercury-in-glass thermometer is impracticable. Most materials expand with temperature increase, but some materials, such as water, contract with temperature increase over some specific range, and then they are hardly useful as thermometric materials. A material is of no use as a thermometer near one of its phase-change temperatures, for example its boiling-point.
In spite of these restrictions, most generally used practical thermometers are of the empirically based kind. Especially, it was used for calorimetry, which contributed greatly to the discovery of thermodynamics. Nevertheless, empirical thermometry has serious drawbacks when judged as a basis for theoretical physics. Empirically based thermometers, beyond their base as simple direct measurements of ordinary physical properties of thermometric materials, can be re-calibrated, by use of theoretical physical reasoning, and this can extend their range of adequacy.
Theoretically based scales.
Theoretically based temperature scales are based directly on theoretical arguments, especially those of thermodynamics, of kinetic theory, and of quantum mechanics. They rely on theoretical properties of idealized devices and materials. They are more or less comparable with practically feasible physical devices and materials. Theoretically based temperature scales are used to provide calibrating standards for practical empirically based thermometers.
The accepted fundamental thermodynamic temperature scale is the Kelvin scale, based on an ideal cyclic process envisaged for a Carnot heat engine.
An ideal material on which a temperature scale can be based is the ideal gas. The pressure exerted by a fixed volume and mass of an ideal gas is directly proportional to its temperature. Some natural gases show so nearly ideal properties over suitable temperature ranges that they can be used for thermometry; this was important during the development of thermodynamics, and is still of practical importance today. The ideal gas thermometer is, however, not theoretically perfect for thermodynamics. This is because the entropy of an ideal gas at its absolute zero of temperature is not a positive semi-definite quantity, which puts the gas in violation of the third law of thermodynamics. The physical reason is that the ideal gas law, exactly read, refers to the limit of infinitely high temperature and zero pressure.
Measurement of the spectrum of electromagnetic radiation from an ideal three-dimensional black body can provide an accurate temperature measurement because the frequency of maximum spectral radiance of black-body radiation is directly proportional to the temperature of the black body; this is known as Wien's displacement law, and has a theoretical explanation in Planck's law and the Bose–Einstein law.
Measurement of the spectrum of noise-power produced by an electrical resistor can also provide an accurate temperature measurement. The resistor has two terminals and is in effect a one-dimensional body. The Bose-Einstein law for this case indicates that the noise-power is directly proportional to the temperature of the resistor and to the value of its resistance and to the noise band-width. In a given frequency band, the noise-power has equal contributions from every frequency, and is called Johnson noise. If the value of the resistance is known then the temperature can be found.
If molecules, or atoms, or electrons, are emitted from a material and their velocities are measured, the spectrum of their velocities often nearly obeys a theoretical law called the Maxwell–Boltzmann distribution, which gives a well-founded measurement of temperatures for which the law holds. There have not yet been successful experiments of this same kind that directly use the Fermi–Dirac distribution for thermometry, but perhaps that will be achieved in future.
Absolute thermodynamic scale.
The Kelvin scale is called absolute for two reasons. One is Kelvin's, that its formal character is independent of the properties of particular materials. The other reason is that its zero is in a sense absolute, in that it indicates absence of microscopic classical motion of the constituent particles of matter, so that they have a limiting specific heat of zero for zero temperature, according to the third law of thermodynamics. Nevertheless, a Kelvin temperature has a definite numerical value, that has been arbitrarily chosen by tradition. This numerical value also depends on the properties of water, which has a gas–liquid–solid triple point that can be reliably reproduced as a standard experimental phenomenon. The choice of this triple point is also arbitrary and by convention. The Kelvin scale is also called the thermodynamic scale.
Definition of the Kelvin scale.
The thermodynamic definition of temperature is due to Kelvin.
It is framed in terms of an idealized device called a Carnot engine, imagined to define a continuous cycle of states of its working body. The cycle is imagined to run so slowly that at each point of the cycle the working body is in a state of thermodynamic equilibrium. There are four limbs in such a Carnot cycle. The engine consists of four bodies. The main one is called the working body. Two of them are called heat reservoirs, so large that their respective non-deformation variables are not changed by transfer of energy as heat through a wall permeable only to heat to the working body. The fourth body is able to exchange energy with the working body only through adiabatic work; it may be called the work reservoir. The substances and states of the two heat reservoirs should be chosen so that they are not in thermal equilibrium with one another. This means that they must be at different fixed temperatures, one, labeled here with the number 1, hotter than the other, labeled here with the number 2. This can be tested by connecting the heat reservoirs successively to an auxiliary empirical thermometric body that starts each time at a convenient fixed intermediate temperature. The thermometric body should be composed of a material that has a strictly monotonic relation between its chosen empirical thermometric variable and the amount of adiabatic isochoric work done on it. In order to settle the structure and sense of operation of the Carnot cycle, it is convenient to use such a material also for the working body; because most materials are of this kind, this is hardly a restriction of the generality of this definition. The Carnot cycle is considered to start from an initial condition of the working body that was reached by the completion of a reversible adiabatic compression. From there, the working body is initially connected by a wall permeable only to heat to the heat reservoir number 1, so that during the first limb of the cycle it expands and does work on the work reservoir. The second limb of the cycle sees the working body expand adiabatically and reversibly, with no energy exchanged as heat, but more energy being transferred as work to the work reservoir. The third limb of the cycle sees the working body connected, through a wall permeable only to heat, to the heat reservoir 2, contracting and accepting energy as work from the work reservoir. The cycle is closed by reversible adiabatic compression of the working body, with no energy transferred as heat, but energy being transferred to it as work from the work reservoir.
With this set-up, the four limbs of the reversible Carnot cycle are characterized by amounts of energy transferred, as work from the working body to the work reservoir, and as heat from the heat reservoirs to the working body. The amounts of energy transferred as heat from the heat reservoirs are measured through the changes in the non-deformation variable of the working body, with reference to the previously known properties of that body, the amounts of work done on the work reservoir, and the first law of thermodynamics. The amounts of energy transferred as heat respectively from reservoir 1 and from reservoir 2 may then be denoted respectively and . Then the absolute or thermodynamic temperatures, and , of the reservoirs are defined so that to be such that
Kelvin's original work postulating absolute temperature was published in 1848. It was based on the work of Carnot, before the formulation of the first law of thermodynamics. Kelvin wrote in his 1848 paper that his scale was absolute in the sense that it was defined "independently of the properties of any particular kind of matter." His definitive publication, which sets out the definition just stated, was printed in 1853, a paper read in 1851.
This definition rests on the physical assumption that there are readily available walls permeable only to heat. In his detailed definition of a wall permeable only to heat, Carathéodory includes several ideas. The non-deformation state variable of a closed system is represented as a real number. A state of thermal equilibrium between two closed systems connected by a wall permeable only to heat means that a certain mathematical relation holds between the state variables, including the respective non-deformation variables, of those two systems (that particular mathematical relation is regarded by Buchdahl as a preferred statement of the zeroth law of thermodynamics). Also, referring to thermal contact equilibrium, "whenever each of the systems and is made to reach equilibrium with a third system under identical conditions, the systems and are in mutual equilibrium." It may be viewed as a re-statement of the principle stated by Maxwell in the words: "All heat is of the same kind." This physical idea is also expressed by Bailyn as a possible version of the zeroth law of thermodynamics: "All diathermal walls are equivalent." Thus the present definition of thermodynamic temperature rests on the zeroth law of thermodynamics. Explicitly, this present definition of thermodynamic temperature also rests on the first law of thermodynamics, for the determination of amounts of energy transferred as heat.
Implicitly for this definition, the second law of thermodynamics provides information that establishes the virtuous character of the temperature so defined. It provides that any working substance that complies with the requirement stated in this definition will lead to the same ratio of thermodynamic temperatures, which in this sense is universal, or absolute. The second law of thermodynamics also provides that the thermodynamic temperature defined in this way is positive, because this definition requires that the heat reservoirs not be in thermal equilibrium with one another, and the cycle can be imagined to operate only in one sense if net work is to be supplied to the work reservoir.
Numerical details are settled by making one of the heat reservoirs a cell at the triple point of water, which is defined to have an absolute temperature of 273.16 K. The zeroth law of thermodynamics allows this definition to be used to measure the absolute or thermodynamic temperature of an arbitrary body of interest, by making the other heat reservoir have the same temperature as the body of interest.
Temperature as an intensive variable.
In thermodynamic terms, temperature is an intensive variable because it is equal to a differential coefficient of one extensive variable with respect to another, for a given body. It thus has the dimensions of a ratio of two extensive variables. In thermodynamics, two bodies are often considered as connected by contact with a common wall, which has some specific permeability properties. Such specific permeability can be referred to a specific intensive variable. An example is a diathermic wall that is permeable only to heat; the intensive variable for this case is temperature. When the two bodies have been in contact for a very long time, and have settled to a permanent steady state, the relevant intensive variables are equal in the two bodies; for a diathermal wall, this statement is sometimes called the zeroth law of thermodynamics.
In particular, when the body is described by stating its internal energy , an extensive variable, as a function of its entropy , also an extensive variable, and other state variables , with ), then the temperature is equal to the partial derivative of the internal energy with respect to the entropy:
Likewise, when the body is described by stating its entropy as a function of its internal energy , and other state variables , with , then the reciprocal of the temperature is equal to the partial derivative of the entropy with respect to the internal energy:
The above definition, equation (1), of the absolute temperature is due to Kelvin. It refers to systems closed to transfer of matter, and has special emphasis on directly experimental procedures. A presentation of thermodynamics by Gibbs starts at a more abstract level and deals with systems open to the transfer of matter; in this development of thermodynamics, the equations (2) and (3) above are actually alternative definitions of temperature.
Temperature local when local thermodynamic equilibrium prevails.
Real world bodies are often not in thermodynamic equilibrium and not homogeneous. For study by methods of classical irreversible thermodynamics, a body is usually spatially and temporally divided conceptually into 'cells' of small size. If classical thermodynamic equilibrium conditions for matter are fulfilled to good approximation in such a 'cell', then it is homogeneous and a temperature exists for it. If this is so for every 'cell' of the body, then local thermodynamic equilibrium is said to prevail throughout the body.
It makes good sense, for example, to say of the extensive variable , or of the extensive variable , that it has a density per unit volume, or a quantity per unit mass of the system, but it makes no sense to speak of density of temperature per unit volume or quantity of temperature per unit mass of the system. On the other hand, it makes no sense to speak of the internal energy at a point, while when local thermodynamic equilibrium prevails, it makes good sense to speak of the temperature at a point. Consequently, temperature can vary from point to point in a medium that is not in global thermodynamic equilibrium, but in which there is local thermodynamic equilibrium.
Thus, when local thermodynamic equilibrium prevails in a body, temperature can be regarded as a spatially varying local property in that body, and this is because temperature is an intensive variable.
Kinetic theory approach to temperature.
A more thorough account of this is below at Theoretical foundation.
Kinetic theory provides a microscopic explanation of temperature, based on macroscopic systems' being composed of many microscopic particles, such as molecules and ions of various species, the particles of a species being all alike. It explains macroscopic phenomena through the classical mechanics of the microscopic particles. The equipartition theorem of kinetic theory asserts that each classical degree of freedom of a freely moving particle has an average kinetic energy of where denotes Boltzmann's constant. The translational motion of the particle has three degrees of freedom, so that, except at very low temperatures where quantum effects predominate, the average translational kinetic energy of a freely moving particle in a system with temperature will be .
It is possible to measure the average kinetic energy of constituent microscopic particles if they are allowed to escape from the bulk of the system. The spectrum of velocities has to be measured, and the average calculated from that. It is not necessarily the case that the particles that escape and are measured have the same velocity distribution as the particles that remain in the bulk of the system, but sometimes a good sample is possible.
Molecules, such as oxygen (O2), have more degrees of freedom than single spherical atoms: they undergo rotational and vibrational motions as well as translations. Heating results in an increase in temperature due to an increase in the average translational kinetic energy of the molecules. Heating will also cause, through equipartitioning, the energy associated with vibrational and rotational modes to increase. Thus a diatomic gas will require more energy input to increase its temperature by a certain amount, i.e. it will have a greater heat capacity than a monatomic gas.
The process of cooling involves removing internal energy from a system. When no more energy can be removed, the system is at absolute zero, though this cannot be achieved experimentally. Absolute zero is the null point of the thermodynamic temperature scale, also called absolute temperature. If it were possible to cool a system to absolute zero, all classical motion of its particles would cease and they would be at complete rest in this classical sense. Microscopically in the description of quantum mechanics, however, matter still has zero-point energy even at absolute zero, because of the uncertainty principle.
Basic theory.
Temperature is a measure of a quality of a state of a material The quality may be regarded as a more abstract entity than any particular temperature scale that measures it, and is called "hotness" by some writers. The quality of hotness refers to the state of material only in a particular locality, and in general, apart from bodies held in a steady state of thermodynamic equilibrium, hotness varies from place to place. It is not necessarily the case that a material in a particular place is in a state that is steady and nearly homogeneous enough to allow it to have a well-defined hotness or temperature. Hotness may be represented abstractly as a one-dimensional manifold. Every valid temperature scale has its own one-to-one map into the hotness manifold.
When two systems in thermal contact are at the same temperature no heat transfers between them. When a temperature difference does exist heat flows spontaneously from the warmer system to the colder system until they are in thermal equilibrium. Heat transfer occurs by conduction or by thermal radiation.
Experimental physicists, for example Galileo and Newton, found that there are indefinitely many empirical temperature scales. Nevertheless, the zeroth law of thermodynamics says that they all measure the same quality.
Temperature for bodies in thermodynamic equilibrium.
For experimental physics, hotness means that, when comparing any two given bodies in their respective separate thermodynamic equilibria, any two suitably given empirical thermometers with numerical scale readings will agree as to which is the hotter of the two given bodies, or that they have the same temperature. This does not require the two thermometers to have a linear relation between their numerical scale readings, but it does require that the relation between their numerical readings shall be strictly monotonic. A definite sense of greater hotness can be had, independently of calorimetry, of thermodynamics, and of properties of particular materials, from Wien's displacement law of thermal radiation: the temperature of a bath of thermal radiation is proportional, by a universal constant, to the frequency of the maximum of its frequency spectrum; this frequency is always positive, but can have values that tend to zero. Thermal radiation is initially defined for a cavity in thermodynamic equilibrium. These physical facts justify a mathematical statement that hotness exists on an ordered one-dimensional manifold. This is a fundamental character of temperature and thermometers for bodies in their own thermodynamic equilibrium.
Except for a system undergoing a first-order phase change such as the melting of ice, as a closed system receives heat, without change in its volume and without change in external force fields acting on it, its temperature rises. For a system undergoing such a phase change so slowly that departure from thermodynamic equilibrium can be neglected, its temperature remains constant as the system is supplied with latent heat. Conversely, a loss of heat from a closed system, without phase change, without change of volume, and without change in external force fields acting on it, decreases its temperature.
Temperature for bodies in a steady state but not in thermodynamic equilibrium.
While for bodies in their own thermodynamic equilibrium states, the notion of temperature requires that all empirical thermometers must agree as to which of two bodies is the hotter or that they are at the same temperature, this requirement is not safe for bodies that are in steady states though not in thermodynamic equilibrium. It can then well be that different empirical thermometers disagree about which is the hotter, and if this is so, then at least one of the bodies does not have a well defined absolute thermodynamic temperature. Nevertheless, any one given body and any one suitable empirical thermometer can still support notions of empirical, non-absolute, hotness and temperature, for a suitable range of processes. This is a matter for study in non-equilibrium thermodynamics.
Temperature for bodies not in a steady state.
When a body is not in a steady state, then the notion of temperature becomes even less safe than for a body in a steady state not in thermodynamic equilibrium. This is also a matter for study in non-equilibrium thermodynamics.
Thermodynamic equilibrium axiomatics.
For axiomatic treatment of thermodynamic equilibrium, since the 1930s, it has become customary to refer to a zeroth law of thermodynamics. The customarily stated minimalist version of such a law postulates only that all bodies, which when thermally connected would be in thermal equilibrium, should be said to have the same temperature by definition, but by itself does not establish temperature as a quantity expressed as a real number on a scale. A more physically informative version of such a law views empirical temperature as a chart on a hotness manifold. While the zeroth law permits the definitions of many different empirical scales of temperature, the second law of thermodynamics selects the definition of a single preferred, absolute temperature, unique up to an arbitrary scale factor, whence called the thermodynamic temperature. If internal energy is considered as a function of the volume and entropy of a homogeneous system in thermodynamic equilibrium, thermodynamic absolute temperature appears as the partial derivative of internal energy with respect the entropy at constant volume. Its natural, intrinsic origin or null point is absolute zero at which the entropy of any system is at a minimum. Although this is the lowest absolute temperature described by the model, the third law of thermodynamics postulates that absolute zero cannot be attained by any physical system.
Heat capacity.
When an energy transfer to or from a body is only as heat, state of the body changes. Depending on the surroundings and the walls separating them from the body, various changes are possible in the body. They include chemical reactions, increase of pressure, increase of temperature, and phase change. For each kind of change under specified conditions, the heat capacity is the ratio of the quantity of heat transferred to the magnitude of the change. For example, if the change is an increase in temperature at constant volume, with no phase change and no chemical change, then the temperature of the body rises and its pressure increases. The quantity of heat transferred, , divided by the observed temperature change, , is the body's heat capacity at constant volume, .
If heat capacity is measured for a well defined amount of substance, the specific heat is the measure of the heat required to increase the temperature of such a unit quantity by one unit of temperature. For example, to raise the temperature of water by one kelvin (equal to one degree Celsius) requires 4186 joules per kilogram (J/kg)..
Temperature measurement.
Temperature measurement using modern scientific thermometers and temperature scales goes back at least as far as the early 18th century, when Gabriel Fahrenheit adapted a thermometer (switching to mercury) and a scale both developed by Ole Christensen Rømer. Fahrenheit's scale is still in use in the United States for non-scientific applications.
Temperature is measured with thermometers that may be calibrated to a variety of temperature scales. In most of the world (except for Belize, Myanmar, Liberia and the United States), the Celsius scale is used for most temperature measuring purposes. Most scientists measure temperature using the Celsius scale and thermodynamic temperature using the Kelvin scale, which is the Celsius scale offset so that its null point is = , or absolute zero. Many engineering fields in the U.S., notably high-tech and US federal specifications (civil and military), also use the Kelvin and Celsius scales. Other engineering fields in the U.S. also rely upon the Rankine scale (a shifted Fahrenheit scale) when working in thermodynamic-related disciplines such as combustion.
Units.
The basic unit of temperature in the International System of Units (SI) is the kelvin. It has the symbol K.
For everyday applications, it is often convenient to use the Celsius scale, in which corresponds very closely to the freezing point of water and is its boiling point at sea level. Because liquid droplets commonly exist in clouds at sub-zero temperatures, is better defined as the melting point of ice. In this scale a temperature difference of 1 degree Celsius is the same as a increment, but the scale is offset by the temperature at which ice melts (273.15 K).
By international agreement the Kelvin and Celsius scales are defined by two fixing points: absolute zero and the triple point of Vienna Standard Mean Ocean Water, which is water specially prepared with a specified blend of hydrogen and oxygen isotopes. Absolute zero is defined as precisely and . It is the temperature at which all classical translational motion of the particles comprising matter ceases and they are at complete rest in the classical model. Quantum-mechanically, however, zero-point motion remains and has an associated energy, the zero-point energy. Matter is in its ground state, and contains no thermal energy. The triple point of water is defined as and . This definition serves the following purposes: it fixes the magnitude of the kelvin as being precisely 1 part in 273.16 parts of the difference between absolute zero and the triple point of water; it establishes that one kelvin has precisely the same magnitude as one degree on the Celsius scale; and it establishes the difference between the null points of these scales as being ( = and = ).
In the United States, the Fahrenheit scale is widely used. On this scale the freezing point of water corresponds to 32 °F and the boiling point to 212 °F. The Rankine scale, still used in fields of chemical engineering in the U.S., is an absolute scale based on the Fahrenheit increment.
Conversion.
The following table shows the temperature conversion formulas for conversions to and from the Celsius scale.
Plasma physics.
The field of plasma physics deals with phenomena of electromagnetic nature that involve very high temperatures. It is customary to express temperature as energy in units of electronvolts (eV) or kiloelectronvolts (keV). The energy, which has a different dimension from temperature, is then calculated as the product of the Boltzmann constant and temperature, formula_5. Then, 1 eV corresponds to . In the study of QCD matter one routinely encounters temperatures of the order of a few hundred MeV, equivalent to about .
Theoretical foundation.
Historically, there are several scientific approaches to the explanation of temperature: the classical thermodynamic description based on macroscopic empirical variables that can be measured in a laboratory; the kinetic theory of gases which relates the macroscopic description to the probability distribution of the energy of motion of gas particles; and a microscopic explanation based on statistical physics and quantum mechanics. In addition, rigorous and purely mathematical treatments have provided an axiomatic approach to classical thermodynamics and temperature. Statistical physics provides a deeper understanding by describing the atomic behavior of matter, and derives macroscopic properties from statistical averages of microscopic states, including both classical and quantum states. In the fundamental physical description, using natural units, temperature may be measured directly in units of energy. However, in the practical systems of measurement for science, technology, and commerce, such as the modern metric system of units, the macroscopic and the microscopic descriptions are interrelated by the Boltzmann constant, a proportionality factor that scales temperature to the microscopic mean kinetic energy.
The microscopic description in statistical mechanics is based on a model that analyzes a system into its fundamental particles of matter or into a set of classical or quantum-mechanical oscillators and considers the system as a statistical ensemble of microstates. As a collection of classical material particles, temperature is a measure of the mean energy of motion, called kinetic energy, of the particles, whether in solids, liquids, gases, or plasmas. The kinetic energy, a concept of classical mechanics, is half the mass of a particle times its speed squared. In this mechanical interpretation of thermal motion, the kinetic energies of material particles may reside in the velocity of the particles of their translational or vibrational motion or in the inertia of their rotational modes. In monatomic perfect gases and, approximately, in most gases, temperature is a measure of the mean particle kinetic energy. It also determines the probability distribution function of the energy. In condensed matter, and particularly in solids, this purely mechanical description is often less useful and the oscillator model provides a better description to account for quantum mechanical phenomena. Temperature determines the statistical occupation of the microstates of the ensemble. The microscopic definition of temperature is only meaningful in the thermodynamic limit, meaning for large ensembles of states or particles, to fulfill the requirements of the statistical model.
In the context of thermodynamics, the kinetic energy is also referred to as thermal energy. The thermal energy may be partitioned into independent components attributed to the degrees of freedom of the particles or to the modes of oscillators in a thermodynamic system. In general, the number of these degrees of freedom that are available for the equipartitioning of energy depend on the temperature, i.e. the energy region of the interactions under consideration. For solids, the thermal energy is associated primarily with the vibrations of its atoms or molecules about their equilibrium position. In an ideal monatomic gas, the kinetic energy is found exclusively in the purely translational motions of the particles. In other systems, vibrational and rotational motions also contribute degrees of freedom.
Kinetic theory of gases.
Maxwell and Boltzmann developed a kinetic theory that yields a fundamental understanding of temperature in gases. 
This theory also explains the ideal gas law and the observed heat capacity of monatomic (or 'noble') gases.
The ideal gas law is based on observed empirical relationships between pressure ("p"), volume ("V"), and temperature ("T"), and was recognized long before the kinetic theory of gases was developed (see Boyle's and Charles's laws). The ideal gas law states:
where "n" is the number of moles of gas and is the gas constant.
This relationship gives us our first hint that there is an absolute zero on the temperature scale, because it only holds if the temperature is measured on an absolute scale such as Kelvins. The ideal gas law allows one to measure temperature on this absolute scale using the gas thermometer. The temperature in kelvins can be defined as the pressure in pascals of one mole of gas in a container of one cubic meter, divided by the gas constant.
Although it is not a particularly convenient device, the gas thermometer provides an essential theoretical basis by which all thermometers can be calibrated. As a practical matter it is not possible to use a gas thermometer to measure absolute zero temperature since the gases tend to condense into a liquid long before the temperature reaches zero. It is possible, however, to extrapolate to absolute zero by using the ideal gas law, as shown in the figure.
The kinetic theory assumes that pressure is caused by the force associated with individual atoms striking the walls, and that all energy is translational kinetic energy. Using a sophisticated symmetry argument, Boltzmann deduced what is now called the Maxwell–Boltzmann probability distribution function for the velocity of particles in an ideal gas. From that probability distribution function, the average kinetic energy, "Ek" (per particle), of a monatomic ideal gas is:
where the Boltzmann constant, , is the ideal gas constant divided by the Avogadro number, and "vrms" is the root-mean-square speed. Thus the ideal gas law states that internal energy is directly proportional to temperature. This direct proportionality between temperature and internal energy is a special case of the equipartition theorem, and holds only in the classical limit of an ideal gas. It does not hold for most substances, although it is true that temperature is a monotonic (non-decreasing) function of internal energy.
Zeroth law of thermodynamics.
When two otherwise isolated bodies are connected together by a rigid physical path impermeable to matter, there is spontaneous transfer of energy as heat from the hotter to the colder of them. Eventually they reach a state of mutual thermal equilibrium, in which heat transfer has ceased, and the bodies' respective state variables have settled to become unchanging.
One statement of the zeroth law of thermodynamics is that if two systems are each in thermal equilibrium with a third system, then they are also in thermal equilibrium with each other.
This statement helps to define temperature but it does not, by itself, complete the definition. An empirical temperature is a numerical scale for the hotness of a thermodynamic system. Such hotness may be defined as existing on a one-dimensional manifold, stretching between hot and cold. Sometimes the zeroth law is stated to include the existence of a unique universal hotness manifold, and of numerical scales on it, so as to provide a complete definition of empirical temperature. To be suitable for empirical thermometry, a material must have a monontonic relation between hotness and some easily measured state variable, such as pressure or volume, when all other relevant coordinates are fixed. An exceptionally suitable system is the ideal gas, which can provide a temperature scale that matches the absolute Kelvin scale. The Kelvin scale is defined on the basis of the second law of thermodynamics.
Second law of thermodynamics.
In the previous section certain properties of temperature were expressed by the zeroth law of thermodynamics. It is also possible to define temperature in terms of the second law of thermodynamics which deals with entropy. Entropy is often thought of as a measure of the disorder in a system. The second law states that any process will result in either no change or a net increase in the entropy of the universe. This can be understood in terms of probability.
For example, in a series of coin tosses, a perfectly ordered system would be one in which either every toss comes up heads or every toss comes up tails. This means that for a perfectly ordered set of coin tosses, there is only one set of toss outcomes possible: the set in which 100% of tosses come up the same. On the other hand, there are multiple combinations that can result in disordered or mixed systems, where some fraction are heads and the rest tails. A disordered system can be 90% heads and 10% tails, or it could be 98% heads and 2% tails, et cetera. As the number of coin tosses increases, the number of possible combinations corresponding to imperfectly ordered systems increases. For a very large number of coin tosses, the combinations to ~50% heads and ~50% tails dominates and obtaining an outcome significantly different from 50/50 becomes extremely unlikely. Thus the system naturally progresses to a state of maximum disorder or entropy.
It has been previously stated that temperature governs the transfer of heat between two systems and it was just shown that the universe tends to progress so as to maximize entropy, which is expected of any natural system. Thus, it is expected that there is some relationship between temperature and entropy. To find this relationship, the relationship between heat, work and temperature is first considered. A heat engine is a device for converting thermal energy into mechanical energy, resulting in the performance of work, and analysis of the Carnot heat engine provides the necessary relationships. The work from a heat engine corresponds to the difference between the heat put into the system at the high temperature, "qH" and the heat ejected at the low temperature, "qC". The efficiency is the work divided by the heat put into the system or:
where "wcy" is the work done per cycle. The efficiency depends only on "qC"/"qH". Because "qC" and "qH" correspond to heat transfer at the temperatures "TC" and "TH", respectively, "qC"/"qH" should be some function of these temperatures:
Carnot's theorem states that all reversible engines operating between the same heat reservoirs are equally efficient. Thus, a heat engine operating between "T"1 and "T"3 must have the same efficiency as one consisting of two cycles, one between "T"1 and "T"2, and the second between "T"2 and "T"3. This can only be the case if:
which implies:
Since the first function is independent of "T"2, this temperature must cancel on the right side, meaning "f"("T"1,"T"3) is of the form "g"("T"1)/"g"("T"3) (i.e. "f"("T"1,"T"3) = "f"("T"1,"T"2)"f"("T"2,"T"3) = "g"("T"1)/"g"("T"2)· "g"("T"2)/"g"("T"3) = "g"("T"1)/"g"("T"3)), where "g" is a function of a single temperature. A temperature scale can now be chosen with the property that:
Substituting Equation 4 back into Equation 2 gives a relationship for the efficiency in terms of temperature:
For "TC" = 0 K the efficiency is 100% and that efficiency becomes greater than 100% below 0 K. Since an efficiency greater than 100% violates the first law of thermodynamics, this implies that 0 K is the minimum possible temperature. In fact the lowest temperature ever obtained in a macroscopic system was 20 nK, which was achieved in 1995 at NIST. Subtracting the right hand side of Equation 5 from the middle portion and rearranging gives:
where the negative sign indicates heat ejected from the system. This relationship suggests the existence of a state function, "S", defined by:
where the subscript indicates a reversible process. The change of this state function around any cycle is zero, as is necessary for any state function. This function corresponds to the entropy of the system, which was described previously. Rearranging Equation 6 gives a new definition for temperature in terms of entropy and heat:
For a system, where entropy "S"("E") is a function of its energy "E", the temperature "T" is given by:
i.e. the reciprocal of the temperature is the rate of increase of entropy with respect to energy.
Definition from statistical mechanics.
Statistical mechanics defines temperature based on a system's fundamental degrees of freedom. Eq.(10) is the defining relation of temperature. Eq. (9) can be derived from the principles underlying the fundamental thermodynamic relation.
Generalized temperature from single particle statistics.
It is possible to extend the definition of temperature even to systems of few particles, like in a quantum dot. The generalized temperature is obtained by considering time ensembles instead of configuration space ensembles given in statistical mechanics in the case of thermal and particle exchange between a small system of fermions (N even less than 10) with a single/double occupancy system. The finite quantum grand canonical ensemble, obtained under the hypothesis of ergodicity and orthodicity, allows to express the generalized temperature from the ratio of the average time of occupation formula_18"1" and formula_18"2" of the single/double occupancy system:
where "EF" is the Fermi energy which tends to the ordinary temperature when N goes to infinity.
Negative temperature.
On the empirical temperature scales, which are not referenced to absolute zero, a negative temperature is one below the zero-point of the scale used. For example, dry ice has a sublimation temperature of which is equivalent to . On the absolute Kelvin scale, however, this temperature is 194.6 K. On the absolute scale of thermodynamic temperature no material can have a temperature smaller than or equal to 0 K, both of which are forbidden by the third law of thermodynamics.
Temperature is basically defined for a body in its own state of internal thermodynamic equilibrium, and in this definition, on an absolute scale, it is always positive. In an apparent contradiction of this reliable and valid rule, a so-called negative absolute "temperature" may be approximately defined for a component of a body that is not in its own state of internal thermodynamic equilibrium: a component may have a negative approximate "temperature" while the rest of the components of the body have positive approximate temperatures. Such a non-equilibrium situation is either transient in time or is maintained by external factors that drive a flow of energy through the body of interest. An example of such a component is a spin system within a body, as follows.
In the quantum mechanical description of electron and nuclear spin systems that have a limited number of possible states, and therefore a discrete upper limit of energy they can attain, it is possible to obtain a negative temperature, which is numerically indeed less than absolute zero. However, this is not the macroscopic temperature of the material, but instead the temperature of only very specific degrees of freedom, that are isolated from others and do not exchange energy by virtue of the equipartition theorem.
A negative temperature is experimentally achieved with suitable radio frequency techniques that cause a population inversion of spin states from the ground state. As the energy in the system increases upon population of the upper states, the entropy increases as well, as the system becomes less ordered, but attains a maximum value when the spins are evenly distributed among ground and excited states, after which it begins to decrease, once again achieving a state of higher order as the upper states begin to fill exclusively. At the point of maximum entropy, the temperature function shows the behavior of a singularity, because the slope of the entropy function decreases to zero at first and then turns negative. Since temperature is the inverse of the derivative of the entropy, the temperature formally goes to infinity at this point, and switches to negative infinity as the slope turns negative. At energies higher than this point, the spin degree of freedom therefore exhibits formally a negative thermodynamic temperature. As the energy increases further by continued population of the excited state, the negative temperature approaches zero asymptotically. As the energy of the system increases in the population inversion, a system with a negative temperature is not colder than absolute zero, but rather it has a higher energy than at positive temperature, and may be said to be in fact hotter at negative temperatures. When brought into contact with a system at a positive temperature, energy will be transferred from the negative temperature regime to the positive temperature region.

</doc>
<doc id="29965" url="https://en.wikipedia.org/wiki?curid=29965" title="Tensor">
Tensor

Tensors are geometric objects that describe linear relations between geometric vectors, scalars, and other tensors. Elementary examples of such relations include the dot product, the cross product, and linear maps. Euclidean vectors, often used in physics and engineering applications, and scalars themselves are also tensors. A more sophisticated example is the Cauchy stress tensor T, which takes a direction v as input and produces the stress T(v) on the surface normal to this vector for output, thus expressing a relationship between these two vectors, shown in the figure (right).
In terms of a coordinate basis or fixed frame of reference, a tensor can be represented as an organized multidimensional array of numerical values. The (also "degree") of a tensor is the dimensionality of the array needed to represent it, or equivalently, the number of indices needed to label a component of that array. For example, a linear map is represented by a matrix (a 2-dimensional array) in a basis, and therefore is a 2nd-order tensor. A vector is represented as a 1-dimensional array in a basis, and is a 1st-order tensor. Scalars are single numbers and are thus 0th-order tensors. Because they express a relationship between vectors, tensors themselves must be independent of a particular choice of coordinate system. The coordinate independence of a tensor then takes the form of a "covariant" transformation law that relates the array computed in one coordinate system to that computed in another one. The precise form of the transformation law determines the "type" (or "valence") of the tensor. The tensor type is a pair of natural numbers , where is the number of contravariant indices and is the number of covariant indices. The total order of a tensor is the sum of these two numbers.
Tensors are important in physics because they provide a concise mathematical framework for formulating and solving physics problems in areas such as elasticity, fluid mechanics, and general relativity. Tensors were first conceived by Tullio Levi-Civita and Gregorio Ricci-Curbastro, who continued the earlier work of Bernhard Riemann and Elwin Bruno Christoffel and others, as part of the "absolute differential calculus". The concept enabled an alternative formulation of the intrinsic differential geometry of a manifold in the form of the Riemann curvature tensor.
Definition.
There are several approaches to defining tensors. Although seemingly different, the approaches just describe the same geometric concept using different languages and at different levels of abstraction.
As multidimensional arrays.
Just as a vector in an -dimensional space is represented by a one-dimensional array of length with respect to a given basis, any tensor with respect to a basis is represented by a multidimensional array. For example, a linear transformation is represented in a basis as a two-dimensional square array. The numbers in the multidimensional array are known as the "scalar components" of the tensor or simply its "components". They are denoted by indices giving their position in the array, as subscripts and superscripts, following the symbolic name of the tensor. For example, the components of an order tensor could be denoted  , where and are indices running from to , or also by . Whether an index is displayed as a superscript or subscript depends on the transformation properties of the tensor, described below. The total number of indices required to uniquely select each component is equal to the dimension of the array, and is called the "order", "degree" or "rank" of the tensor. However, the term "rank" generally has another meaning in the context of matrices and tensors.
Just as the components of a vector change when we change the basis of the vector space, the components of a tensor also change under such a transformation. Each tensor comes equipped with a "transformation law" that details how the components of the tensor respond to a change of basis. The components of a vector can respond in two distinct ways to a change of basis (see covariance and contravariance of vectors), where the new basis vectors formula_1 are expressed in terms of the old basis vectors formula_2 as,
Here "R""i"" j" are the entries of the change of basis matrix, and in the second expression the summation sign was suppressed: this is the Einstein summation convention, which will be used throughout this article. The components "v""i" of a column vector v transform with the inverse of the matrix "R",
where the hat denotes the components in the new basis. This is called a "contravariant" transformation law, because the vector transforms by the "inverse" of the change of basis. In contrast, the components, "w""i", of a covector (or row vector), w transform with the matrix R itself,
This is called a "covariant" transformation law, because the covector transforms by the "same matrix" as the change of basis matrix. The components of a more general tensor transform by some combination of covariant and contravariant transformations, with one transformation law for each index. If the transformation matrix of an index is the inverse matrix of the basis transformation, then the index is called "contravariant" and is traditionally denoted with an upper index (superscript). If the transformation matrix of an index is the basis transformation itself, then the index is called "covariant" and is denoted with a lower index (subscript).
The transformation law for an order tensor with "p" contravariant indices and "q" covariant indices is thus given as,
Here the primed indices denote components in the new coordinates, and the unprimed indices denote the components in the old coordinates. Such a tensor is said to be of order or "type" . The terms "order", "type", "rank", "valence", and "degree" are all sometimes used for the same concept. Here, the term "order" or "total order" will be used for the total dimension of the array (or its generalisation in other definitions), "p"+"q" in the preceding example, and the term "type" for the pair giving the number contravariant and covariant indices. A tensor of type is also called as a -tensor for short.
As an example, the matrix of a linear transformation in a basis is a rectangular array that transforms under a change of basis "R" by formula_7. In terms of the individual matrix entries, this transformation law has the form
so the tensor corresponding to the matrix of a linear transformation has one covariant and one contravariant index: it has type (1,1). A linear transformation itself does not actually depend on a basis: it is just a linear map that accepts a vector as an argument and produces another vector. The transformation law for the matrix of a linear transformation is consistent with the transformation law for a contravariant vector, so that the action of a linear transformation on a contravariant vector is represented in coordinates as the matrix product of their respective coordinate representations. That is, the components formula_9 are given by formula_10. These components transform contravariantly, since
This discussion motivates the following formal definition:
[\mathbf{f}]</math>
to each basis of a fixed "n"-dimensional vector space such that, if we apply the change of basis
then the multidimensional array obeys the transformation law
The definition of a tensor as a multidimensional array satisfying a transformation law traces back to the work of Ricci. This definition is still used in some physics and engineering text books.
Tensor fields.
In many applications, especially in differential geometry and physics, it is natural to consider a tensor with components that are functions of the point in a space. This was the setting of Ricci's original work. In modern mathematical terminology such an object is called a tensor field, often referred to simply as a tensor.
In this context, a coordinate basis is often chosen for the tangent vector space. The transformation law may then be expressed in terms of partial derivatives of the coordinate functions,
defining a coordinate transformation,
As multilinear maps.
A downside to the definition of a tensor using the multidimensional array approach is that it is not apparent from the definition that the defined object is indeed basis independent, as is expected from an intrinsically geometric object. Although it is possible to show that transformation laws indeed ensure independence from the basis, sometimes a more intrinsic definition is preferred. One approach is to define a tensor as a multilinear map. In that approach a type tensor "T" is defined as a map,
where "V" is a (finite-dimensional) vector space and "V"* is the corresponding dual space of covectors, which is linear in each of its arguments.
By applying a multilinear map "T" of type to a basis {ej} for "V" and a canonical cobasis {εi} for "V"*,
a ("p"+"q")-dimensional array of components can be obtained. A different choice of basis will yield different components. But, because "T" is linear in all of its arguments, the components satisfy the tensor transformation law used in the multilinear array definition. The multidimensional array of components of "T" thus form a tensor according to that definition. Moreover, such an array can be realized as the components of some multilinear map "T". This motivates viewing multilinear maps as the intrinsic objects underlying tensors.
In viewing a tensor as a multilinear map, it is conventional to identify the vector space "V" with the space of linear functionals on the dual of "V", the double dual "V"**. There is always a natural linear map from "V" to its double dual, given by evaluating a linear form in "V"* against a vector in "V". This linear mapping is an isomorphism in finite dimensions, and it is often then expedient to identify "V" with its double dual.
Using tensor products.
For some mathematical applications, a more abstract approach is sometimes useful. This can be achieved by defining tensors in terms of elements of tensor products of vector spaces, which in turn are defined through a universal property. A type tensor is defined in this context as an element of the tensor product of vector spaces,
If is a basis of and is a basis of , then the tensor product has a natural basis . The components of a tensor are the coefficients of the tensor with respect to the basis obtained from a basis for and its dual , i.e.
Using the properties of the tensor product, it can be shown that these components satisfy the transformation law for a type tensor. Moreover, the universal property of the tensor product gives a -to- correspondence between tensors defined in this way and tensors defined as multilinear maps.
Tensor products can be defined in great generality – for example, involving arbitrary modules over a ring. In principle, one could define a "tensor" simply to be an element of any tensor product. However, the mathematics literature usually reserves the term "tensor" for an element of a tensor product of a single vector space and its dual, as above.
Tensors in infinite dimensions.
This discussion of tensors so far assumes finite dimensionality of the spaces involved, where the spaces of tensors obtained by each of these constructions are naturally isomorphic. Constructions of spaces of tensors based on the tensor product and multilinear mappings can be generalized, essentially without modification, to vector bundles or coherent sheaves. For infinite-dimensional vector spaces, inequivalent topologies lead to inequivalent notions of tensor, and these various isomorphisms may or may not hold depending on what exactly is meant by a tensor (see topological tensor product). In some applications, it is the tensor product of Hilbert spaces that is intended, whose properties are the most similar to the finite-dimensional case. A more modern view is that it is the tensors' structure as a symmetric monoidal category that encodes their most important properties, rather than the specific models of those categories
Examples.
This table shows important examples of tensors, including both tensors on vector spaces and tensor fields on manifolds. The tensors are classified according to their type , where "n" is the number of contravariant indices, "m" is the number of covariant indices, and gives the total order of the tensor. For example, a bilinear form is the same thing as a -tensor; an inner product is an example of a -tensor, but not all -tensors are inner products. In the -entry of the table, "M" denotes the dimensionality of the underlying vector space or manifold because for each dimension of the space, a separate index is needed to select that dimension to get a maximally covariant antisymmetric tensor.
Raising an index on an -tensor produces an -tensor; this can be visualized as moving diagonally down and to the left on the table. Symmetrically, lowering an index can be visualized as moving diagonally up and to the right on the table. Contraction of an upper with a lower index of an -tensor produces an -tensor; this can be visualized as moving diagonally up and to the left on the table.
 
Notation.
Ricci calculus.
Ricci calculus is the modern formalism and notation for tensor indices: indicating inner and outer products, covariance and contravariance, summations of tensor components, symmetry and antisymmetry, and partial and covariant derivatives.
Einstein summation convention.
The Einstein summation convention dispenses with writing summation signs, leaving the summation implicit. Any repeated index symbol is summed over: if the index is used twice in a given term of a tensor expression, it means that the term is to be summed for all . Several distinct pairs of indices may be summed this way.
Penrose graphical notation.
Penrose graphical notation is a diagrammatic notation which replaces the symbols for tensors with shapes, and their indices by lines and curves. It is independent of basis elements, and requires no symbols for the indices.
Abstract index notation.
The abstract index notation is a way to write tensors such that the indices are no longer thought of as numerical, but rather are indeterminates. This notation captures the expressiveness of indices and the basis-independence of index-free notation.
Component-free notation.
A component-free treatment of tensors uses notation that emphasises that tensors do not rely on any basis, and is defined in terms of the tensor product of vector spaces.
Operations.
There are a number of basic operations that may be conducted on tensors that again produce a tensor. The linear nature of tensor implies that two tensors of the same type may be added together, and that tensors may be multiplied by a scalar with results analogous to the scaling of a vector. On components, these operations are simply performed component for component. These operations do not change the type of the tensor, however there also exist operations that change the type of the tensors.
Tensor product.
The tensor product takes two tensors, "S" and "T", and produces a new tensor, , whose order is the sum of the orders of the original tensors. When described as multilinear maps, the tensor product simply multiplies the two tensors, i.e.
which again produces a map that is linear in all its arguments. On components the effect similarly is to multiply the components of the two input tensors, i.e.
If is of type and is of type , then the tensor product has type .
Contraction.
Tensor contraction is an operation that reduces the total order of a tensor by two. More precisely, it reduces a type tensor to a type tensor. In terms of components, the operation is achieved by summing over one contravariant and one covariant index of tensor. For example, a -tensor formula_22 can be contracted to a scalar through
Where the summation is again implied. When the -tensor is interpreted as a linear map, this operation is known as the trace.
The contraction is often used in conjunction with the tensor product to contract an index from each tensor.
The contraction can also be understood in terms of the definition of a tensor as an element of a tensor product of copies of the space "V" with the space "V"* by first decomposing the tensor into a linear combination of simple tensors, and then applying a factor from "V"* to a factor from "V". For example, a tensor
can be written as a linear combination
The contraction of "T" on the first and last slots is then the vector
Raising or lowering an index.
When a vector space is equipped with a nondegenerate bilinear form (or "metric tensor" as it is often called in this context), operations can be defined that convert a contravariant (upper) index into a covariant (lower) index and vice versa. A metric tensor is a (symmetric) (-tensor, it is thus possible to contract an upper index of a tensor with one of lower indices of the metric tensor in the product. This produces a new tensor with the same index structure as the previous, but with lower index in the position of the contracted upper index. This operation is quite graphically known as "lowering an index".
Conversely, the inverse operation can be defined, and is called "raising an index". This is equivalent to a similar contraction on the product with a -tensor. This "inverse metric tensor" has components that are the matrix inverse of those if the metric tensor.
Applications.
Continuum mechanics.
Important examples are provided by continuum mechanics. The stresses inside a solid body or fluid are described by a tensor. The stress tensor and strain tensor are both second-order tensors, and are related in a general linear elastic material by a fourth-order elasticity tensor. In detail, the tensor quantifying stress in a 3-dimensional solid object has components that can be conveniently represented as a 3 × 3 array. The three faces of a cube-shaped infinitesimal volume segment of the solid are each subject to some given force. The force's vector components are also three in number. Thus, 3 × 3, or 9 components are required to describe the stress at this cube-shaped infinitesimal segment. Within the bounds of this solid is a whole mass of varying stress quantities, each requiring 9 quantities to describe. Thus, a second-order tensor is needed.
If a particular surface element inside the material is singled out, the material on one side of the surface will apply a force on the other side. In general, this force will not be orthogonal to the surface, but it will depend on the orientation of the surface in a linear manner. This is described by a tensor of type, in linear elasticity, or more precisely by a tensor field of type , since the stresses may vary from point to point.
Other examples from physics.
Common applications include
Applications of tensors of order > 2.
The concept of a tensor of order two is often conflated with that of a matrix. Tensors of higher order do however capture ideas important in science and engineering, as has been shown successively in numerous areas as they develop. This happens, for instance, in the field of computer vision, with the trifocal tensor generalizing the fundamental matrix.
The field of nonlinear optics studies the changes to material polarization density under extreme electric fields. The polarization waves generated are related to the generating electric fields through the nonlinear susceptibility tensor. If the polarization P is not linearly proportional to the electric field E, the medium is termed "nonlinear". To a good approximation (for sufficiently weak fields, assuming no permanent dipole moments are present), P is given by a Taylor series in E whose coefficients are the nonlinear susceptibilities:
Here formula_28 is the linear susceptibility, formula_29 gives the Pockels effect and second harmonic generation, and formula_30 gives the Kerr effect. This expansion shows the way higher-order tensors arise naturally in the subject matter.
Generalizations.
Tensor products of vector spaces.
The vector spaces of a tensor product need not be the same, and sometimes the elements of such a more general tensor product are called "tensors". For example, an element of the tensor product space is a second-order "tensor" in this more general sense, and an order- tensor may likewise be defined as an element of a tensor product of different vector spaces. A type tensor, in the sense defined previously, is also a tensor of order in this more general sense.
Tensors in infinite dimensions.
The notion of a tensor can be generalized in a variety of ways to infinite dimensions. One, for instance, is via the tensor product of Hilbert spaces. Another way of generalizing the idea of tensor, common in nonlinear analysis, is via the multilinear maps definition where instead of using finite-dimensional vector spaces and their algebraic duals, one uses infinite-dimensional Banach spaces and their continuous dual. Tensors thus live naturally on Banach manifolds.
Tensor densities.
The concept of a tensor field can be generalized by considering objects that transform differently. An object that transforms as an ordinary tensor field under coordinate transformations, except that it is also multiplied by the determinant of the Jacobian of the inverse coordinate transformation to the formula_31 power, is called a tensor density with weight formula_32. Invariantly, in the language of multilinear algebra, one can think of tensor densities as multilinear maps taking their values in a density bundle such as the (1-dimensional) space of "n"-forms (where "n" is the dimension of the space), as opposed to taking their values in just R. Higher "weights" then just correspond to taking additional tensor products with this space in the range.
A special case are the scalar densities. Scalar 1-densities are especially important because it makes sense to define their integral over a manifold. They appear, for instance, in the Einstein–Hilbert action in general relativity. The most common example of a scalar 1-density is the volume element, which in the presence of a metric tensor "g" is the square root of its determinant in coordinates, denoted formula_33. The metric tensor is a covariant tensor of order 2, and so its determinant scales by the square of the coordinate transition:
which is the transformation law for a scalar density of weight +2.
More generally, any tensor density is the product of an ordinary tensor with a scalar density of the appropriate weight. In the language of vector bundles, the determinant bundle of the tangent bundle is a line bundle that can be used to 'twist' other bundles "w" times. While locally the more general transformation law can indeed be used to recognise these tensors, there is a global question that arises, reflecting that in the transformation law one may write either the Jacobian determinant, or its absolute value. Non-integral powers of the (positive) transition functions of the bundle of densities make sense, so that the weight of a density, in that sense, is not restricted to integer values. Restricting to changes of coordinates with positive Jacobian determinant is possible on orientable manifolds, because there is a consistent global way to eliminate the minus signs; but otherwise the line bundle of densities and the line bundle of "n"-forms are distinct. For more on the intrinsic meaning, see density on a manifold.
Spinors.
When changing from one orthonormal basis (called a "frame") to another by a rotation, the components of a tensor transform by that same rotation. This transformation does not depend on the path taken through the space of frames. However, the space of frames is not simply connected (see orientation entanglement and plate trick): there are continuous paths in the space of frames with the same beginning and ending configurations that are not deformable one into the other. It is possible to attach an additional discrete invariant to each frame called the "spin" that incorporates this path dependence, and which turns out to have values of ±1. A spinor is an object that transforms like a tensor under rotations in the frame, apart from a possible sign that is determined by the spin.
History.
The concepts of later tensor analysis arose from the work of Carl Friedrich Gauss in differential geometry, and the formulation was much influenced by the theory of algebraic forms and invariants developed during the middle of the nineteenth century. The word "tensor" itself was introduced in 1846 by William Rowan Hamilton to describe something different from what is now meant by a tensor. The contemporary usage was introduced by Woldemar Voigt in 1898.
Tensor calculus was developed around 1890 by Gregorio Ricci-Curbastro under the title "absolute differential calculus", and originally presented by Ricci in 1892. It was made accessible to many mathematicians by the publication of Ricci and Tullio Levi-Civita's 1900 classic text "Méthodes de calcul différentiel absolu et leurs applications" (Methods of absolute differential calculus and their applications).
In the 20th century, the subject came to be known as "tensor analysis", and achieved broader acceptance with the introduction of Einstein's theory of general relativity, around 1915. General relativity is formulated completely in the language of tensors. Einstein had learned about them, with great difficulty, from the geometer Marcel Grossmann. Levi-Civita then initiated a correspondence with Einstein to correct mistakes Einstein had made in his use of tensor analysis. The correspondence lasted 1915–17, and was characterized by mutual respect:
Tensors were also found to be useful in other fields such as continuum mechanics. Some well-known examples of tensors in differential geometry are quadratic forms such as metric tensors, and the Riemann curvature tensor. The exterior algebra of Hermann Grassmann, from the middle of the nineteenth century, is itself a tensor theory, and highly geometric, but it was some time before it was seen, with the theory of differential forms, as naturally unified with tensor calculus. The work of Élie Cartan made differential forms one of the basic kinds of tensors used in mathematics.
From about the 1920s onwards, it was realised that tensors play a basic role in algebraic topology (for example in the Künneth theorem). Correspondingly there are types of tensors at work in many branches of abstract algebra, particularly in homological algebra and representation theory. Multilinear algebra can be developed in greater generality than for scalars coming from a field. For example, scalars can come from a ring. But the theory is then less geometric and computations more technical and less algorithmic. Tensors are generalized within category theory by means of the concept of monoidal category, from the 1960s.

</doc>
<doc id="256738" url="https://en.wikipedia.org/wiki?curid=256738" title="Tensor field">
Tensor field

In mathematics, physics, and engineering, a tensor field assigns a tensor to each point of a mathematical space (typically a Euclidean space or manifold). Tensor fields are used in differential geometry, algebraic geometry, general relativity, in the analysis of stress and strain in materials, and in numerous applications in the physical sciences and engineering. As a tensor is a generalization of a scalar (a pure number representing a value, like length) and a vector (a geometrical arrow in space), a tensor field is a generalization of a scalar field or vector field that assigns, respectively, a scalar or vector to each point of space.
Many mathematical structures informally called 'tensors' are actually 'tensor fields'. An example is the Riemann curvature tensor.
Geometric introduction.
Intuitively, a vector field is best visualized as an 'arrow' attached to each point of a region, with variable length and direction. One example of a vector field on a curved space is a weather map showing horizontal wind velocity at each point of the Earth's surface.
The general idea of tensor field combines the requirement of richer geometry – for example, an ellipsoid varying from point to point, in the case of a metric tensor – with the idea that we don't want our notion to depend on the particular method of mapping the surface. It should exist independently of latitude and longitude, or whatever particular 'cartographic projection' we are using to introduce numerical coordinates.
The vector bundle explanation.
The contemporary mathematical expression of the idea of tensor field breaks it down into a two-step concept.
There is the idea of vector bundle, which is a natural idea of 'vector space depending on parameters' – the parameters being in a manifold "M". For example a "vector space of one dimension depending on an angle" could look like a Möbius strip as well as a cylinder. Given a vector bundle "V" over "M", the corresponding field concept is called a "section" of the bundle: for "m" varying over "M", a choice of vector
the vector space 'at' "m".
Since the tensor product concept is independent of any choice of basis, taking the tensor product of two vector bundles on "M" is routine. Starting with the tangent bundle (the bundle of tangent spaces) the whole apparatus explained at component-free treatment of tensors carries over in a routine way – again independently of coordinates, as mentioned in the introduction.
We therefore can give a definition of tensor field, namely as a section of some tensor bundle. (There are vector bundles which are not tensor bundles: the Möbius band for instance.) This is then guaranteed geometric content, since everything has been done in an intrinsic way. More precisely, a tensor field assigns to any given point of the manifold a tensor in the space
where V is the tangent space at that point and V* is the cotangent space. See also tangent bundle and cotangent bundle.
Given two tensor bundles "E" → "M" and "F" → "M", a map "A": Γ("E") → Γ("F") from the space of sections of "E" to sections of "F" can be considered itself as a tensor section of formula_2 if and only if it satisfies "A"("fs"...) = "fA"("s"...) in each argument, where "f" is a smooth function on "M". Thus a tensor is not only a linear map on the vector space of sections, but a "C"∞("M")-linear map on the module of sections. This property is used to check, for example, that even though the Lie derivative and covariant derivative are not tensors, the torsion and curvature tensors built from them are.
Notation.
The notation for tensor fields can sometimes be confusingly similar to the notation for tensor spaces. Thus, the tangent bundle "TM" = "T"("M") might sometimes be written as
to emphasize that the tangent bundle is the range space of the (1,0) tensor fields (i.e., vector fields) on the manifold "M". Do not confuse this with the very similar looking notation
in the latter case, we just have one tensor space, whereas in the former, we have a tensor space defined for each point in the manifold "M".
Curly (script) letters are sometimes used to denote the set of infinitely-differentiable tensor fields on "M". Thus,
are the sections of the ("m","n") tensor bundle on "M" which are infinitely-differentiable. A tensor field is an element of this set.
The "C"∞("M") module explanation.
There is another more abstract (but often useful) way of characterizing tensor fields on a manifold "M" which turns out to actually make tensor fields into honest tensors (i.e. "single" multilinear mappings), though of a different type (and this is "not" usually why one often says "tensor" when one really means "tensor field"). First, we may consider the set of all smooth (C∞) vector fields on "M", formula_6 (see the section on notation above) as a single space &3; a module over the ring of smooth functions, "C"∞("M"), by pointwise scalar multiplication. The notions of multilinearity and tensor products extend easily to the case of modules over any commutative ring.
As a motivating example, consider the space formula_7 of smooth covector fields (1-forms), also a module over the smooth functions. These act on smooth vector fields to yield smooth functions by pointwise evaluation, namely, given a covector field "ω" and a vector field "X", we define
Because of the pointwise nature of everything involved, the action of "ω" on "X" is a "C"∞("M")-linear map, that is,
for any "p" in "M" and smooth function "f". Thus we can regard covector fields not just as sections of the cotangent bundle, but also linear mappings of vector fields into functions. By the double-dual construction, vector fields can similarly be expressed as mappings of covector fields into functions (namely, we could start "natively" with covector fields and work up from there).
In a complete parallel to the construction of ordinary single tensors (not fields!) on "M" as multilinear maps on vectors and covectors, we can regard general ("k","l") tensor fields on "M" as "C"∞("M")-multilinear maps defined on "l" copies of formula_6 and "k" copies of formula_7 into "C"∞("M").
Now, given any arbitrary mapping "T" from a product of "k" copies of formula_7 and "l" copies of formula_6 into "C"∞("M"), it turns out that it arises from a tensor field on "M" if and only if it is a multilinear over "C"∞("M"). Thus this kind of multilinearity implicitly expresses the fact that we're really dealing with a pointwise-defined object, i.e. a tensor field, as opposed to a function which, even when evaluated at a single point, depends on all the values of vector fields and 1-forms simultaneously.
A frequent example application of this general rule is showing that the Levi-Civita connection, which is a mapping of smooth vector fields formula_12 taking a pair of vector fields to a vector field, does not define a tensor field on "M". This is because it is only "R"-linear in "Y" (in place of full "C"∞("M")-linearity, it satisfies the "Leibniz rule," formula_13)). Nevertheless it must be stressed that even though it is not a tensor field, it still qualifies as a geometric object with a component-free interpretation.
Applications.
The curvature tensor is discussed in differential geometry and the stress–energy tensor is important in physics and engineering. Both of these are related by Einstein's theory of general relativity. In engineering, the underlying manifold will often be Euclidean 3-space.
It is worth noting that differential forms, used in defining integration on manifolds, are a type of tensor field.
Tensor calculus.
In theoretical physics and other fields, differential equations posed in terms of tensor fields provide a very general way to express relationships that are both geometric in nature (guaranteed by the tensor nature) and conventionally linked to differential calculus. Even to formulate such equations requires a fresh notion, the covariant derivative. This handles the formulation of variation of a tensor field "along" a vector field. The original "absolute differential calculus" notion, which was later called "tensor calculus", led to the isolation of the geometric concept of connection.
Twisting by a line bundle.
An extension of the tensor field idea incorporates an extra line bundle "L" on "M". If "W" is the tensor product bundle of "V" with "L", then "W" is a bundle of vector spaces of just the same dimension as "V". This allows one to define the concept of tensor density, a 'twisted' type of tensor field. A "tensor density" is the special case where "L" is the bundle of "densities on a manifold", namely the determinant bundle of the cotangent bundle. (To be strictly accurate, one should also apply the absolute value to the transition functions – this makes little difference for an orientable manifold.) For a more traditional explanation see the tensor density article.
One feature of the bundle of densities (again assuming orientability) "L" is that "L""s" is well-defined for real number values of "s"; this can be read from the transition functions, which take strictly positive real values. This means for example that we can take a "half-density", the case where "s" = ½. In general we can take sections of "W", the tensor product of "V" with "L""s", and consider tensor density fields with weight "s".
Half-densities are applied in areas such as defining integral operators on manifolds, and geometric quantization.
The flat case.
When "M" is a Euclidean space and all the fields are taken to be invariant by translations by the vectors of "M", we get back to a situation where a tensor field is synonymous with a tensor 'sitting at the origin'. This does no great harm, and is often used in applications. As applied to tensor densities, it "does" make a difference. The bundle of densities cannot seriously be defined 'at a point'; and therefore a limitation of the contemporary mathematical treatment of tensors is that tensor densities are defined in a roundabout fashion.
Cocycles and chain rules.
As an advanced explanation of the "tensor" concept, one can interpret the chain rule in the multivariable case, as applied to coordinate changes, also as the requirement for self-consistent concepts of tensor giving rise to tensor fields.
Abstractly, we can identify the chain rule as a 1-cocycle. It gives the consistency required to define the tangent bundle in an intrinsic way. The other vector bundles of tensors have comparable cocycles, which come from applying functorial properties of tensor constructions to the chain rule itself; this is why they also are intrinsic (read, 'natural') concepts.
What is usually spoken of as the 'classical' approach to tensors tries to read this backwards – and is therefore a heuristic, "post hoc" approach rather than truly a foundational one. Implicit in defining tensors by how they transform under a coordinate change is the kind of self-consistency the cocycle expresses. The construction of tensor densities is a 'twisting' at the cocycle level. Geometers have not been in any doubt about the "geometric" nature of tensor "quantities"; this kind of descent argument justifies abstractly the whole theory.

</doc>
<doc id="1784313" url="https://en.wikipedia.org/wiki?curid=1784313" title="Tests of general relativity">
Tests of general relativity

At its introduction in 1915, the general theory of relativity did not have a solid empirical foundation. It was known that it correctly accounted for the "anomalous" precession of the perihelion of Mercury and on philosophical grounds it was considered satisfying that it was able to unify Newton's law of universal gravitation with special relativity. That light appeared to bend in gravitational fields in line with the predictions of general relativity was found in 1919 but it was not until a program of precision tests was started in 1959 that the various predictions of general relativity were tested to any further degree of accuracy in the weak gravitational field limit, severely limiting possible deviations from the theory. Beginning in 1974, Hulse, Taylor and others have studied the behaviour of binary pulsars experiencing much stronger gravitational fields than found in the Solar System. Both in the weak field limit (as in the Solar System) and with the stronger fields present in systems of binary pulsars the predictions of general relativity have been extremely well tested locally.
The very strong gravitational fields that must be present close to black holes, especially those supermassive black holes which are thought to power active galactic nuclei and the more active quasars, belong to a field of intense active research. Observations of these quasars and active galactic nuclei are difficult, and interpretation of the observations is heavily dependent upon astrophysical models other than general relativity or competing fundamental theories of gravitation, but they are qualitatively consistent with the black hole concept as modelled in general relativity.
As a consequence of the equivalence principle, Lorentz invariance holds locally in freely falling reference frames. Experiments related to Lorentz invariance and thus special relativity (i.e., when gravitational effects can be neglected) are described in Tests of special relativity.
Classical tests.
Albert Einstein proposed three tests of general relativity, subsequently called the classical tests of general relativity, in 1916:
In the letter to the London Times on November 28, 1919, he described the theory of relativity and thanked his English colleagues for their understanding and testing of his work. He also mentioned three classical tests with comments: 
Perihelion precession of Mercury.
Under Newtonian physics, a two-body system consisting of a lone object orbiting a spherical mass would trace out an ellipse with the spherical mass at a focus. The point of closest approach, called the periapsis (or, because the central body in the Solar System is the Sun, perihelion), is fixed. A number of effects in the Solar System cause the perihelia of planets to precess (rotate) around the Sun. The principal cause is the presence of other planets which perturb each other's orbit. Another (much less significant) effect is solar oblateness.
Mercury deviates from the precession predicted from these Newtonian effects. This anomalous rate of precession of the perihelion of Mercury's orbit was first recognized in 1859 as a problem in celestial mechanics, by Urbain Le Verrier. His reanalysis of available timed observations of transits of Mercury over the Sun's disk from 1697 to 1848 showed that the actual rate of the precession disagreed from that predicted from Newton's theory by 38" (arc seconds) per tropical century (later re-estimated at 43"). A number of "ad hoc" and ultimately unsuccessful solutions were proposed, but they tended to introduce more problems. In general relativity, this remaining precession, or change of orientation of the orbital ellipse within its orbital plane, is explained by gravitation being mediated by the curvature of spacetime. Einstein showed that general relativity agrees closely with the observed amount of perihelion shift. This was a powerful factor motivating the adoption of general relativity.
Although earlier measurements of planetary orbits were made using conventional telescopes, more accurate measurements are now made with radar. The total observed precession of Mercury is 574.10±0.65 arc-seconds per century relative to the inertial ICFR. This precession can be attributed to the following causes:
The correction by 42.98" is 3/2 multiple of classical prediction with PPN parameters formula_1.
Thus the effect can be fully explained by general relativity. More recent calculations based on more precise measurements have not materially changed the situation.
The other planets experience perihelion shifts as well, but, since they are farther from the Sun and have longer periods, their shifts are lower, and could not be observed accurately until long after Mercury's. For example, the perihelion shift of Earth's orbit due to general relativity is of 3.84 seconds of arc per century, and Venus's is 8.62". Both values are in good agreement with observation. The periapsis shift of binary pulsar systems have been measured, with PSR 1913+16 amounting to 4.2º per year. These observations are consistent with general relativity. It is also possible to measure periapsis shift in binary star systems which do not contain ultra-dense stars, but it is more difficult to model the classical effects precisely - for example, the alignment of the stars' spin to their orbital plane needs to be known and is hard to measure directly - so a few systems such as DI Herculis have been considered as problematic cases for general relativity.
Deflection of light by the Sun.
Henry Cavendish in 1784 (in an unpublished manuscript) and Johann Georg von Soldner in 1801 (published in 1804) had pointed out that Newtonian gravity predicts that starlight will bend around a massive object. The same value as Soldner's was calculated by Einstein in 1911 based on the equivalence principle alone. However, Einstein noted in 1915 in the process of completing general relativity, that his (and thus Soldner's) 1911-result is only half of the correct value. Einstein became the first to calculate the correct value for light bending.
The first observation of light deflection was performed by noting the change in position of stars as they passed near the Sun on the celestial sphere. The observations were performed in May 1919 by Arthur Eddington and his collaborators during a total solar eclipse, so that the stars near the Sun could be observed. Observations were made simultaneously in the cities of Sobral, Ceará, Brazil and in São Tomé and Príncipe on the west coast of Africa. The result was considered spectacular news and made the front page of most major newspapers. It made Einstein and his theory of general relativity world-famous. When asked by his assistant what his reaction would have been if general relativity had not been confirmed by Eddington and Dyson in 1919, Einstein famously made the quip: "Then I would feel sorry for the dear Lord. The theory is correct anyway." 
The early accuracy, however, was poor. The results were argued by some to have been plagued by systematic error and possibly confirmation bias, although modern reanalysis of the dataset suggests that Eddington's analysis was accurate. The measurement was repeated by a team from the Lick Observatory in the 1922 eclipse, with results that agreed with the 1919 results and has been repeated several times since, most notably in 1953 by Yerkes Observatory astronomers and in 1973 by a team from the University of Texas. Considerable uncertainty remained in these measurements for almost fifty years, until observations started being made at radio frequencies. It was not until the 1960s that it was definitively accepted that the amount of deflection was the full value predicted by general relativity, and not half that number.
The Einstein ring is an example of the deflection of light from distant galaxies by more nearby objects.
Gravitational redshift of light.
Einstein predicted the gravitational redshift of light from the equivalence principle in 1907, but it is very difficult to measure astrophysically (see the discussion under "Equivalence Principle" below). Although it was measured by Walter Sydney Adams in 1925, it was only conclusively tested when the Pound–Rebka experiment in 1959 measured the relative redshift of two sources situated at the top and bottom of Harvard University's Jefferson tower using an extremely sensitive phenomenon called the Mössbauer effect. The result was in excellent agreement with general relativity. This was one of the first precision experiments testing general relativity.
Modern tests.
The modern era of testing general relativity was ushered in largely at the impetus of Dicke and Schiff who laid out a framework for testing general relativity. They emphasized the importance not only of the classical tests, but of null experiments, testing for effects which in principle could occur in a theory of gravitation, but do not occur in general relativity. Other important theoretical developments included the inception of alternative theories to general relativity, in particular, scalar-tensor theories such as the Brans–Dicke theory; the parameterized post-Newtonian formalism in which deviations from general relativity can be quantified; and the framework of the equivalence principle.
Experimentally, new developments in space exploration, electronics and condensed matter physics have made additional precise experiments possible, such as the Pound–Rebka experiment, laser interferometry and lunar rangefinding.
Post-Newtonian tests of gravity.
Early tests of general relativity were hampered by the lack of viable competitors to the theory: it was not clear what sorts of tests would distinguish it from its competitors. General relativity was the only known relativistic theory of gravity compatible with special relativity and observations. Moreover, it is an extremely simple and elegant theory. This changed with the introduction of Brans–Dicke theory in 1960. This theory is arguably simpler, as it contains no dimensionful constants, and is compatible with a version of Mach's principle and Dirac's large numbers hypothesis, two philosophical ideas which have been influential in the history of relativity. Ultimately, this led to the development of the parametrized post-Newtonian formalism by Nordtvedt and Will, which parametrizes, in terms of ten adjustable parameters, all the possible departures from Newton's law of universal gravitation to first order in the velocity of moving objects ("i.e." to first order in formula_2, where "v" is the velocity of an object and "c" is the speed of light). This approximation allows the possible deviations from general relativity, for slowly moving objects in weak gravitational fields, to be systematically analyzed. Much effort has been put into constraining the post-Newtonian parameters, and deviations from general relativity are at present severely limited.
The experiments testing gravitational lensing and light time delay limits the same post-Newtonian parameter, the so-called Eddington parameter γ, which is a straightforward parametrization of the amount of deflection of light by a gravitational source. It is equal to one for general relativity, and takes different values in other theories (such as Brans–Dicke theory). It is the best constrained of the ten post-Newtonian parameters, but there are other experiments designed to constrain the others. Precise observations of the perihelion shift of Mercury constrain other parameters, as do tests of the strong equivalence principle.
One of the goals of the mission BepiColombo is testing the general relativity theory by measuring the parameters gamma and beta of the parametrized post-Newtonian formalism with high accuracy.
Gravitational lensing.
One of the most important tests is gravitational lensing. It has been observed in distant astrophysical sources, but these are poorly controlled and it is uncertain how they constrain general relativity. The most precise tests are analogous to Eddington's 1919 experiment: they measure the deflection of radiation from a distant source by the Sun. The sources that can be most precisely analyzed are distant radio sources. In particular, some quasars are very strong radio sources. The directional resolution of any telescope is in principle limited by diffraction; for radio telescopes this is also the practical limit. An important improvement in obtaining positional high accuracies (from milli-arcsecond to micro-arcsecond) was obtained by combining radio telescopes across Earth. The technique is called very long baseline interferometry (VLBI). With this technique radio observations couple the phase information of the radio signal observed in telescopes separated over large distances. Recently, these telescopes have measured the deflection of radio waves by the Sun to extremely high precision, confirming the amount of deflection predicted by general relativity aspect to the 0.03% level. At this level of precision systematic effects have to be carefully taken into account to determine the precise location of the telescopes on Earth. Some important effects are Earth's nutation, rotation, atmospheric refraction, tectonic displacement and tidal waves. Another important effect is refraction of the radio waves by the solar corona. Fortunately, this effect has a characteristic spectrum, whereas gravitational distortion is independent of wavelength. Thus, careful analysis, using measurements at several frequencies, can subtract this source of error.
The entire sky is slightly distorted due to the gravitational deflection of light caused by the Sun (the anti-Sun direction excepted). This effect has been observed by the European Space Agency astrometric satellite Hipparcos. It measured the positions of about 105 stars. During the full mission about relative positions have been determined, each to an accuracy of typically 3 milliarcseconds (the accuracy for an 8–9 magnitude star). Since the gravitation deflection perpendicular to the Earth–Sun direction is already 4.07 milliarcseconds, corrections are needed for practically all stars. Without systematic effects, the error in an individual observation of 3 milliarcseconds, could be reduced by the square root of the number of positions, leading to a precision of 0.0016 milliarcseconds. Systematic effects, however, limit the accuracy of the determination to 0.3% (Froeschlé, 1997).
In the future, the "Gaia" spacecraft will conduct a census of one billion stars in the Milky Way and measure their positions to an accuracy of 24 microarcseconds. Thus it will also provide stringent new tests of gravitational deflection of light caused by the Sun which was predicted by General relativity.
Light travel time delay testing.
Irwin I. Shapiro proposed another test, beyond the classical tests, which could be performed within the Solar System. It is sometimes called the fourth "classical" test of general relativity. He predicted a relativistic time delay (Shapiro delay) in the round-trip travel time for radar signals reflecting off other planets. The mere curvature of the path of a photon passing near the Sun is too small to have an observable delaying effect (when the round-trip time is compared to the time taken if the photon had followed a straight path), but general relativity predicts a time delay that becomes progressively larger when the photon passes nearer to the Sun due to the time dilation in the gravitational potential of the Sun. Observing radar reflections from Mercury and Venus just before and after it is eclipsed by the Sun agrees with general relativity theory at the 5% level. More recently, the Cassini probe has undertaken a similar experiment which gave agreement with general relativity at the 0.002% level. Very Long Baseline Interferometry has measured velocity-dependent (gravitomagnetic) corrections to the Shapiro time delay in the field of moving Jupiter and Saturn.
The equivalence principle.
The equivalence principle, in its simplest form, asserts that the trajectories of falling bodies in a gravitational field should be independent of their mass and internal structure, provided they are small enough not to disturb the environment or be affected by tidal forces. This idea has been tested to incredible precision by Eötvös torsion balance experiments, which look for a differential acceleration between two test masses. Constraints on this, and on the existence of a composition-dependent fifth force or gravitational Yukawa interaction are very strong, and are discussed under fifth force and weak equivalence principle.
A version of the equivalence principle, called the strong equivalence principle, asserts that self-gravitation falling bodies, such as stars, planets or black holes (which are all held together by their gravitational attraction) should follow the same trajectories in a gravitational field, provided the same conditions are satisfied. This is called the Nordtvedt effect and is most precisely tested by the Lunar Laser Ranging Experiment. Since 1969, it has continuously measured the distance from several rangefinding stations on Earth to reflectors on the Moon to approximately centimeter accuracy. These have provided a strong constraint on several of the other post-Newtonian parameters.
Another part of the strong equivalence principle is the requirement that Newton's gravitational constant be constant in time, and have the same value everywhere in the universe. There are many independent observations limiting the possible variation of Newton's gravitational constant, but one of the best comes from lunar rangefinding which suggests that the gravitational constant does not change by more than one part in 1011 per year. The constancy of the other constants is discussed in the Einstein equivalence principle section of the equivalence principle article.
Gravitational redshift.
The first of the classical tests discussed above, the gravitational redshift, is a simple consequence of the Einstein equivalence principle and was predicted by Einstein in 1907. As such, it is not a test of general relativity in the same way as the post-Newtonian tests, because any theory of gravity obeying the equivalence principle should also incorporate the gravitational redshift. Nonetheless, confirming the existence of the effect was an important substantiation of relativistic gravity, since the absence of gravitational redshift would have strongly contradicted relativity. The first observation of the gravitational redshift was the measurement of the shift in the spectral lines from the white dwarf star Sirius B by Adams in 1925. Although this measurement, as well as later measurements of the spectral shift on other white dwarf stars, agreed with the prediction of relativity, it could be argued that the shift could possibly stem from some other cause, and hence experimental verification using a known terrestrial source was preferable.
Experimental verification of gravitational redshift using terrestrial sources took several decades, because it is difficult to find clocks (to measure time dilation) or sources of electromagnetic radiation (to measure redshift) with a frequency that is known well enough that the effect can be accurately measured. It was confirmed experimentally for the first time in 1960 using measurements of the change in wavelength of gamma-ray photons generated with the Mössbauer effect, which generates radiation with a very narrow line width. The experiment, performed by Pound and Rebka and later improved by Pound and Snyder, is called the Pound–Rebka experiment. The accuracy of the gamma-ray measurements was typically 1%. The blueshift of a falling photon can be found by assuming it has an equivalent mass based on its frequency formula_3 (where "h" is Planck's constant) along with formula_4, a result of special relativity. Such simple derivations ignore the fact that in general relativity the experiment compares clock rates, rather than energies. In other words, the "higher energy" of the photon after it falls can be equivalently ascribed to the slower running of clocks deeper in the gravitational potential well. To fully validate general relativity, it is important to also show that the rate of arrival of the photons is greater than the rate at which they are emitted. A very accurate gravitational redshift experiment, which deals with this issue, was performed in 1976, where a hydrogen maser clock on a rocket was launched to a height of 10,000 km, and its rate compared with an identical clock on the ground. It tested the gravitational redshift to 0.007%.
Although the Global Positioning System (GPS) is not designed as a test of fundamental physics, it must account for the gravitational redshift in its timing system, and physicists have analyzed timing data from the GPS to confirm other tests. When the first satellite was launched, some engineers resisted the prediction that a noticeable gravitational time dilation would occur, so the first satellite was launched without the clock adjustment that was later built into subsequent satellites. It showed the predicted shift of 38 microseconds per day. This rate of discrepancy is sufficient to substantially impair function of GPS within hours if not accounted for. An excellent account of the role played by general relativity in the design of GPS can be found in Ashby 2003.
Other precision tests of general relativity, not discussed here, are the Gravity Probe A satellite, launched in 1976, which showed gravity and velocity affect the ability to synchronize the rates of clocks orbiting a central mass; the Hafele–Keating experiment, which used atomic clocks in circumnavigating aircraft to test general relativity and special relativity together; and the forthcoming Satellite Test of the Equivalence Principle.
Frame-dragging tests.
Tests of the Lense–Thirring precession, consisting of small secular precessions of the orbit of a test particle in motion around a central rotating mass like, e.g., a planet or a star, have been performed with the LAGEOS satellites, but many aspects of them remain controversial. The same effect may have been detected in the data of the Mars Global Surveyor (MGS) spacecraft, a former probe in orbit around Mars; also such a test raised a debate. First attempts to detect the Sun's Lense–Thirring effect on the perihelia of the inner planets have been recently reported as well. Frame dragging would cause the orbital plane of stars orbiting near a supermassive black hole to precess about the black hole spin axis. This effect should be detectable within the next few years via astrometric monitoring of stars at the center of the Milky Way galaxy. By comparing the rate of orbital precession of two stars on different orbits, it is possible in principle to test the no-hair theorems of general relativity.
The Gravity Probe B satellite, launched in 2004 and operated until 2005, detected frame-dragging and the geodetic effect. The experiment used four quartz spheres the size of ping pong balls coated with a superconductor. Data analysis continued through 2011 due to high noise levels and difficulties in modelling the noise accurately so that a useful signal could be found. Principal investigators at Stanford University reported on May 4, 2011, that they had accurately measured the framing effect relative to the distant star IM Pegasi, and the calculations proved to be in line with the prediction of Einstein's theory. The results, published in "Physical Review Letters" measured the geodetic effect with an error of about 0.2 percent. The results reported the frame dragging effect (caused by Earth's rotation) added up to 37 milliarcseconds with an error of about 19 percent. Investigator Francis Everitt explained that a milliarcsecond "is the width of a human hair seen at the distance of 10 miles".
In January 2012, LARES satellite was launched on a Vega rocket to measure Lense–Thirring effect with an accuracy of about 1%, according to its proponents.
This evaluation of the actual accuracy obtainable is a subject of debate.
Strong field tests: Binary pulsars.
Pulsars are rapidly rotating neutron stars which emit regular radio pulses as they rotate. As such they act as clocks which allow very precise monitoring of their orbital motions. Observations of pulsars in orbit around other stars have all demonstrated substantial periapsis precessions that cannot be accounted for classically but can be accounted for by using general relativity. For example, the Hulse–Taylor binary pulsar PSR B1913+16 (a pair of neutron stars in which one is detected as a pulsar) has an observed precession of over 4° of arc per year (periastron shift per orbit only about 10−6). This precession has been used to compute the masses of the components.
Similarly to the way in which atoms and molecules emit electromagnetic radiation, a gravitating mass that is in quadrupole type or higher order vibration, or is asymmetric and in rotation, can emit gravitational waves. These gravitational waves are predicted to travel at the speed of light. For example, planets orbiting the Sun constantly lose energy via gravitational radiation, but this effect is so small that it is unlikely it will be observed in the near future (Earth radiates about 200 watts (see gravitational waves) of gravitational radiation).
Gravitational waves have been indirectly detected from the Hulse–Taylor binary (and other binary pulsars). Precise timing of the pulses shows that the stars orbit only approximately according to Kepler's Laws: over time they gradually spiral towards each other, demonstrating an energy loss in close agreement with the predicted energy radiated by gravitational waves. Thus, although the waves have not been directly measured, their effect is necessary to explain these orbits. For their discovery of the first binary pulsar and measuring its orbital decay due to gravitational-wave emission, Hulse and Taylor won the 1993 Nobel Prize in Physics.
A "double pulsar" discovered in 2003, PSR J0737-3039, has a periastron precession of 16.90° per year; unlike the Hulse–Taylor binary, both neutron stars are detected as pulsars, allowing precision timing of both members of the system. Due to this, the tight orbit, the fact that the system is almost edge-on, and the very low transverse velocity of the system as seen from Earth, J0737−3039 provides by far the best system for strong-field tests of general relativity known so far. Several distinct relativistic effects are observed, including orbital decay as in the Hulse–Taylor system. After observing the system for two and a half years, four independent tests of general relativity were possible, the most precise (the Shapiro delay) confirming the general relativity prediction within 0.05% (nevertheless the periastron shift per orbit is only about 0.0013% of a circle and thus it is not a higher-order relativity test).
In 2013, an international team of astronomers reported new data from observing a pulsar-white dwarf system PSR J0348+0432, in which they have been able to measure a change in the orbital period of 8 millionths of a second per year, and confirmed GR predictions in a regime of extreme gravitational fields never probed before; but there are still some competing theories that would agree with these data.
Direct detection of gravitational waves.
As described above, observations have shown conclusively, although indirectly, that gravitational waves exist. A number of gravitational-wave detectors have been built with the intent of directly detecting the gravitational waves emanating from such astronomical events as the merger of two neutron stars or black holes. Currently, the most sensitive of these is the Laser Interferometer Gravitational-wave Observatory (LIGO), which was in operation from 2002 to 2010. To date, there has not been a single detection event by any of the existing detectors. Future detectors are being developed or planned, which will greatly improve the sensitivity of these experiments, such as the Advanced LIGO detector due to start operation in 2015, and the proposed evolved Laser Interferometer Space Antenna (eLISA). It is anticipated, for example, that Advanced LIGO will detect events possibly as often as daily, but there is great uncertainty in this (possibly a factor of 1000).
General relativity predicts gravitational waves, as does any theory of gravitation that obeys special relativity and so has changes in the gravitational field propagate at a finite speed. Continued failure to find waves as the detectors become more sensitive would tend to falsify both special and general relativity. However, because of the uncertainty in astrophysical event rates, a lack of detection is more likely to be initially attributed to a previous over-estimation of how many detectable gravitational-wave signals there should be—for example, it would not be surprising if Advanced LIGO operates for three years and detects nothing if detectable signals only occur on average once every ten years—and so update our understanding of star formation, stellar evolution or galaxy evolution. If, in the future, gravitational waves (of the predicted kind) were discovered, this would be evidence in favour of general relativity.
Once gravitational waves can be directly detected, it is possible to use them to learn about the Universe. This is gravitational-wave astronomy. Gravitational-wave astronomy can test general relativity by verifying that the observed waves are of the form predicted (for example, that they only have two transverse polarizations), and by checking that black holes are the objects described by solutions of the Einstein field equations.
Cosmological tests.
Tests of general relativity on the largest scales are not nearly so stringent as Solar System tests. The earliest such test was prediction and discovery of the expansion of the universe. In 1922 Alexander Friedmann found that Einstein equations have non-stationary solutions (even in the presence of the cosmological constant). In 1927 Georges Lemaître showed that static solutions of the Einstein equations, which are possible in the presence of the cosmological constant, are unstable, and therefore the static universe envisioned by Einstein could not exist (it must either expand or contract). Lemaître made an explicit prediction that the universe should expand. He also derived a redshift-distance relationship, which is now known as the Hubble Law. Later, in 1931, Einstein himself agreed with the results of Friedmann and Lemaître. The expansion of the universe discovered by Edwin Hubble in 1929 was then considered by many (and continues to be considered by some now) as a direct confirmation of general relativity. In the 1930s, largely due to the work of E. A. Milne, it was realised that the linear relationship between redshift and distance derives from the general assumption of uniformity and isotropy rather than specifically from general relativity. However the prediction of a non-static universe was non-trivial, indeed dramatic, and primarily motivated by general relativity.
Some other cosmological tests include searches for primordial gravitational waves generated during cosmic inflation, which may be detected in the cosmic microwave background polarization or by a proposed space-based gravitational-wave interferometer called the Big Bang Observer. Other tests at high redshift are constraints on other theories of gravity, and the variation of the gravitational constant since big bang nucleosynthesis (it varied by no more than 40% since then).

</doc>
<doc id="550622" url="https://en.wikipedia.org/wiki?curid=550622" title="Tetraquark">
Tetraquark

A tetraquark, in particle physics, is an exotic meson composed of four valence quarks. In principle, a tetraquark state may be allowed in quantum chromodynamics, the modern theory of strong interactions. Any established tetraquark state would be an example of an exotic hadron which lies outside the quark model classification. 
History.
In 2003 a particle temporarily called X(3872), by the Belle experiment in Japan, was proposed to be a tetraquark candidate, as originally theorized. The name X is a temporary name, indicating that there are still some questions about its properties to be tested. The number following is the mass of the particle in .
In 2004, the DsJ(2632) state seen in Fermilab's SELEX was suggested as a possible tetraquark candidate.
In 2007, Belle announced the observation of the Z(4430) state, a tetraquark candidate. In 2014, the Large Hadron Collider experiment LHCb confirmed this resonance with a significance of over 13.9σ. There are also indications that the Y(4660), also discovered by Belle in 2007, could be a tetraquark state.
In 2009, Fermilab announced that they have discovered a particle temporarily called Y(4140), which may also be a tetraquark.
In 2010, two physicists from DESY and a physicist from Quaid-i-Azam University re-analyzed former experimental data and announced that, in connection with the (5S) meson (a form of bottomonium), a well-defined tetraquark resonance exists.
In June 2013, two independent groups reported on Zc(3900).

</doc>
<doc id="30436" url="https://en.wikipedia.org/wiki?curid=30436" title="Theory of everything">
Theory of everything

A theory of everything (ToE) or final theory, ultimate theory, or master theory is a hypothetical single, all-encompassing, coherent theoretical framework of physics that fully explains and links together all physical aspects of the universe. Finding a ToE is one of the major unsolved problems in physics. Over the past few centuries, two theoretical frameworks have been developed that, as a whole, most closely resemble a ToE. These two theories upon which all modern physics rests on are general relativity (GR) and quantum field theory (QFT). GR is a theoretical framework that only focuses on the force of gravity for understanding the universe in regions of both large-scale and high-mass: stars, galaxies, clusters of galaxies, etc. On the other hand, QFT is a theoretical framework that only focuses on three non-gravitational forces for understanding the universe in regions of both small scale and low mass: sub-atomic particles, atoms, molecules, etc. QFT successfully implemented the Standard Model and unified the interactions (so-called Grand Unified Theory) between the three non-gravitational forces: weak, strong, and electromagnetic force.
Through years of research, physicists have experimentally confirmed with tremendous accuracy virtually every prediction made by these two theories when in their appropriate domains of applicability. In accordance with their findings, scientists also learned that GR and QFT, as they are currently formulated, are mutually incompatible - they cannot both be right. Since the usual domains of applicability of GR and QFT are so different, most situations require that only one of the two theories be used. As it turns out, this incompatibility between GR and QFT is only an apparent issue in regions of extremely small-scale and high-mass, such as those that exist within a black hole or during the beginning stages of the universe (i.e., the moment immediately following the Big Bang). To resolve this conflict, a theoretical framework revealing a deeper underlying reality, unifying gravity with the other three interactions, must be discovered to harmoniously integrate the realms of GR and QFT into a seamless whole: a single theory that, in principle, is capable of describing all phenomena. In pursuit of this goal, quantum gravity has recently become an area of active research.
Over the past few decades, a single explanatory framework, called "string theory", has emerged that may turn out to be the ultimate theory of the universe. Many physicists believe that, at the beginning of the universe (up to 10−43 seconds after the Big Bang), the four fundamental forces were once a single fundamental force. Unlike most (if not all) other theories, string theory may be on its way to successfully incorporating each of the four fundamental forces into a unified whole. According to string theory, every particle in the universe, at its most microscopic level (Planck length), consists of varying combinations of vibrating strings (or strands) with preferred patterns of vibration. String theory claims that it is through these specific oscillatory patterns of strings that a particle of unique mass and force charge is created (that is to say, the electron is a type of string that vibrates one way, while the up-quark is a type of string vibrating another way, and so forth).
Initially, the term "theory of everything" was used with an ironic connotation to refer to various overgeneralized theories. For example, a grandfather of Ijon Tichy — a character from a cycle of Stanisław Lem's science fiction stories of the 1960s — was known to work on the "General Theory of Everything". Physicist John Ellis claims to have introduced the term into the technical literature in an article in "Nature" in 1986. Over time, the term stuck in popularizations of theoretical physics research.
Historical antecedents.
From ancient Greece to Einstein.
Archimedes was possibly the first scientist known to have described nature with axioms (or principles) and then deduce new results from them. He thus tried to describe "everything" starting from a few axioms. Any "theory of everything" is similarly expected to be based on axioms and to deduce all observable phenomena from them.
The concept of 'atom', introduced by Democritus, unified all phenomena observed in nature as the motion of atoms. In ancient Greek times philosophers speculated that the apparent diversity of observed phenomena was due to a single type of interaction, namely the collisions of atoms. Following atomism, the mechanical philosophy of the 17th century posited that all forces could be ultimately reduced to contact forces between the atoms, then imagined as tiny solid particles.
In the late 17th century, Isaac Newton's description of the long-distance force of gravity implied that not all forces in nature result from things coming into contact. Newton's work in his "Principia" dealt with this in a further example of unification, in this case unifying Galileo's work on terrestrial gravity, Kepler's laws of planetary motion and the phenomenon of tides by explaining these apparent actions at a distance under one single law: the law of universal gravitation.
In 1814, building on these results, Laplace famously suggested that a sufficiently powerful intellect could, if it knew the position and velocity of every particle at a given time, along with the laws of nature, calculate the position of any particle at any other time:
Laplace thus envisaged a combination of gravitation and mechanics as a theory of everything. Modern quantum mechanics implies that uncertainty is inescapable, and thus that Laplace's vision has to be amended: a theory of everything must include gravitation and quantum mechanics.
In 1820, Hans Christian Ørsted discovered a connection between electricity and magnetism, triggering decades of work that culminated in 1865, in James Clerk Maxwell's theory of electromagnetism. During the 19th and early 20th centuries, it gradually became apparent that many common examples of forces – contact forces, elasticity, viscosity, friction, and pressure – result from electrical interactions between the smallest particles of matter.
In his experiments of 1849–50, Michael Faraday was the first to search for a unification of gravity with electricity and magnetism. However, he found no connection.
In 1900, David Hilbert published a famous list of mathematical problems. In Hilbert's sixth problem, he challenged researchers to find an axiomatic basis to all of physics. In this problem he thus asked for what today would be called a theory of everything.
In the late 1920s, the new quantum mechanics showed that the chemical bonds between atoms were examples of (quantum) electrical forces, justifying Dirac's boast that "the underlying physical laws necessary for the mathematical theory of a large part of physics and the whole of chemistry are thus completely known".
After 1915, when Albert Einstein published the theory of gravity (general relativity), the search for a unified field theory combining gravity with electromagnetism began with a renewed interest. In Einstein's day, the strong and the weak forces had not yet been discovered, yet, he found the potential existence of two other distinct forces -gravity and electromagnetism- far more alluring. This launched his thirty-year voyage in search of the so-called "unified field theory" that he hoped would show that these two forces are really manifestations of one grand underlying principle. During these last few decades of his life, this quixotic quest isolated Einstein from the mainstream of physics. Understandably, the mainstream was instead far more excited about the newly emerging framework of quantum mechanics. Einstein wrote to a friend in the early 1940s, "I have become a lonely old chap who is mainly known because he doesn't wear socks and who is exhibited as a curiosity on special occasions." Prominent contributors were Gunnar Nordström, Hermann Weyl, Arthur Eddington, Theodor Kaluza, Oskar Klein, and most notably, Albert Einstein and his collaborators. Einstein intensely searched for, but ultimately failed to find, a unifying theory. (But see:Einstein–Maxwell–Dirac equations.) More than a half a century later, Einstein's dream of discovering a unified theory has become the Holy Grail of modern physics.
Twentieth century and the nuclear interactions.
In the twentieth century, the search for a unifying theory was interrupted by the discovery of the strong and weak nuclear forces (or interactions), which differ both from gravity and from electromagnetism. A further hurdle was the acceptance that in a ToE, quantum mechanics had to be incorporated from the start, rather than emerging as a consequence of a deterministic unified theory, as Einstein had hoped.
Gravity and electromagnetism could always peacefully coexist as entries in a list of classical forces, but for many years it seemed that gravity could not even be incorporated into the quantum framework, let alone unified with the other fundamental forces. For this reason, work on unification, for much of the twentieth century, focused on understanding the three "quantum" forces: electromagnetism and the weak and strong forces. The first two were combined in 1967–68 by Sheldon Glashow, Steven Weinberg, and Abdus Salam into the "electroweak" force.
Electroweak unification is a broken symmetry: the electromagnetic and weak forces appear distinct at low energies because the particles carrying the weak force, the W and Z bosons, have non-zero masses of and , whereas the photon, which carries the electromagnetic force, is massless. At higher energies Ws and Zs can be created easily and the unified nature of the force becomes apparent.
While the strong and electroweak forces peacefully coexist in the Standard Model of particle physics, they remain distinct. So far, the quest for a theory of everything is thus unsuccessful on two points: neither a unification of the strong and electroweak forces – which Laplace would have called 'contact forces' – has been achieved, nor has a unification of these forces with gravitation been achieved.
Modern physics.
Conventional sequence of theories.
A Theory of Everything would unify all the fundamental interactions of nature: gravitation, strong interaction, weak interaction, and electromagnetism. Because the weak interaction can transform elementary particles from one kind into another, the ToE should also yield a deep understanding of the various different kinds of possible particles. The usual assumed path of theories is given in the following graph, where each unification step leads one level up:
In this graph, electroweak unification occurs at around 100 GeV, grand unification is predicted to occur at 1016 GeV, and unification of the GUT force with gravity is expected at the Planck energy, roughly 1019 GeV.
Several Grand Unified Theories (GUTs) have been proposed to unify electromagnetism and the weak and strong forces. Grand unification would imply the existence of an electronuclear force; it is expected to set in at energies of the order of 1016 GeV, far greater than could be reached by any possible Earth-based particle accelerator. Although the simplest GUTs have been experimentally ruled out, the general idea, especially when linked with supersymmetry, remains a favorite candidate in the theoretical physics community. Supersymmetric GUTs seem plausible not only for their theoretical "beauty", but because they naturally produce large quantities of dark matter, and because the inflationary force may be related to GUT physics (although it does not seem to form an inevitable part of the theory). Yet GUTs are clearly not the final answer; both the current standard model and all proposed GUTs are quantum field theories which require the problematic technique of renormalization to yield sensible answers. This is usually regarded as a sign that these are only effective field theories, omitting crucial phenomena relevant only at very high energies.
The final step in the graph requires resolving the separation between quantum mechanics and gravitation, often equated with general relativity. Numerous researchers concentrate their efforts on this specific step; nevertheless, no accepted theory of quantum gravity – and thus no accepted theory of everything – has emerged yet. It is usually assumed that the ToE will also solve the remaining problems of GUTs.
In addition to explaining the forces listed in the graph, a ToE may also explain the status of at least two candidate forces suggested by modern cosmology: an inflationary force and dark energy. Furthermore, cosmological experiments also suggest the existence of dark matter, supposedly composed of fundamental particles outside the scheme of the standard model. However, the existence of these forces and particles has not been proven yet.
String theory and M-theory.
Since the 1990s, many physicists believe that 11-dimensional M-theory, which is described in some limits by one of the five perturbative superstring theories, and in another by the maximally-supersymmetric 11-dimensional supergravity, is the theory of everything. However, there is no widespread consensus on this issue.
A surprising property of string/M-theory is that extra dimensions are required for the theory's consistency. In this regard, string theory can be seen as building on the insights of the Kaluza–Klein theory, in which it was realized that applying general relativity to a five-dimensional universe (with one of them small and curled up) looks from the four-dimensional perspective like the usual general relativity together with Maxwell's electrodynamics. This lent credence to the idea of unifying gauge and gravity interactions, and to extra dimensions, but didn't address the detailed experimental requirements. Another important property of string theory is its supersymmetry, which together with extra dimensions are the two main proposals for resolving the hierarchy problem of the standard model, which is (roughly) the question of why gravity is so much weaker than any other force. The extra-dimensional solution involves allowing gravity to propagate into the other dimensions while keeping other forces confined to a four-dimensional spacetime, an idea that has been realized with explicit stringy mechanisms.
Research into string theory has been encouraged by a variety of theoretical and experimental factors. On the experimental side, the particle content of the standard model supplemented with neutrino masses fits into a spinor representation of SO(10), a subgroup of E8 that routinely emerges in string theory, such as in heterotic string theory or (sometimes equivalently) in F-theory. String theory has mechanisms that may explain why fermions come in three hierarchical generations, and explain the mixing rates between quark generations. On the theoretical side, it has begun to address some of the key questions in quantum gravity, such as resolving the black hole information paradox, counting the correct entropy of black holes and allowing for topology-changing processes. It has also led to many insights in pure mathematics and in ordinary, strongly-coupled gauge theory due to the Gauge/String duality.
In the late 1990s, it was noted that one major hurdle in this endeavor is that the number of possible four-dimensional universes is incredibly large. The small, "curled up" extra dimensions can be compactified in an enormous number of different ways (one estimate is 10500 ) each of which leads to different properties for the low-energy particles and forces. This array of models is known as the string theory landscape.
One proposed solution is that many or all of these possibilities are realised in one or another of a huge number of universes, but that only a small number of them are habitable, and hence the fundamental constants of the universe are ultimately the result of the anthropic principle rather than dictated by theory. This has led to criticism of string theory, arguing that it cannot make useful (i.e., original, falsifiable, and verifiable) predictions and regarding it as a pseudoscience. Others disagree, and string theory remains an extremely active topic of investigation in theoretical physics.
Loop quantum gravity.
Current research on loop quantum gravity may eventually play a fundamental role in a ToE, but that is not its primary aim. Also loop quantum gravity introduces a lower bound on the possible length scales.
There have been recent claims that loop quantum gravity may be able to reproduce features resembling the Standard Model. So far only the first generation of fermions (leptons and quarks) with correct parity properties have been modelled by Sundance Bilson-Thompson using preons constituted of braids of spacetime as the building blocks. However, there is no derivation of the Lagrangian that would describe the interactions of such particles, nor is it possible to show that such particles are fermions, nor that the gauge groups or interactions of the Standard Model are realised. Utilization of quantum computing concepts made it possible to demonstrate that the particles are able to survive quantum fluctuations.
This model leads to an interpretation of electric and colour charge as topological quantities (electric as number and chirality of twists carried on the individual ribbons and colour as variants of such twisting for fixed electric charge).
Bilson-Thompson's original paper suggested that the higher-generation fermions could be represented by more complicated braidings, although explicit constructions of these structures were not given. The electric charge, colour, and parity properties of such fermions would arise in the same way as for the first generation. The model was expressly generalized for an infinite number of generations and for the weak force bosons (but not for photons or gluons) in a 2008 paper by Bilson-Thompson, Hackett, Kauffman and Smolin.
Other attempts.
A recent development is the theory of causal fermion systems, giving all three current physical theories (quantum mechanics, general relativity and quantum field theory) as limiting cases.
A recent and very prolific attempt is called Causal Sets. As some of the approaches mentioned above, its direct goal isn't necessarily to achieve a ToE but primarily a working theory of quantum gravity, which might eventually include the standard model and become a candidate for a ToE. Its founding principle is that spacetime is fundamentally discrete and that the spacetime events are related by a partial order. This partial order has the physical meaning of the causality relations between relative past and future distinguishing spacetime events.
Outside the previously mentioned attempts there is Garrett Lisi's E8 proposal. This theory provides an attempt of identifying general relativity and the standard model within the Lie group E8. The theory doesn't provide a novel quantization procedure and the author suggests its quantization might follow the Loop Quantum Gravity approach above mentioned.
Christoph Schiller's Strand Model attempts to account for the gauge symmetry of the Standard Model of particle physics, U(1)×SU(2)×SU(3), with the three Reidemeister moves of knot theory by equating each elementary particle to a different tangle of one, two, or three strands (selectively a long prime knot or unknotted curve, a rational tangle, or a braided tangle respectively).
Present status.
At present, there is no candidate theory of everything that includes the standard model of particle physics and general relativity. For example, no candidate theory is able to calculate the fine structure constant or the mass of the electron. Most particle physicists expect that the outcome of the ongoing experiments – the search for new particles at the large particle accelerators and for dark matter – are needed in order to provide further input for a ToE.
Theory of everything and philosophy.
The philosophical implications of a physical ToE are frequently debated. For example, if philosophical physicalism is true, a physical ToE will coincide with a philosophical theory of everything.
The "system building" style of metaphysics attempts to answer "all" the important questions in a coherent way, providing a complete picture of the world. Plato and Aristotle could be said to have created early examples of comprehensive systems. In the early modern period (17th and 18th centuries), the system-building "scope" of philosophy is often linked to the rationalist "method" of philosophy, which is the technique of deducing the nature of the world by pure "a priori" reason. Examples from the early modern period include the Leibniz's Monadology, Descarte's Dualism, and Spinoza's Monism. Hegel's Absolute idealism and Whitehead's Process philosophy were later systems.
Arguments against a theory of everything.
In parallel to the intense search for a ToE, various scholars have seriously debated the possibility of its discovery.
Gödel's incompleteness theorem.
A number of scholars claim that Gödel's incompleteness theorem suggests that any attempt to construct a ToE is bound to fail. Gödel's theorem, informally stated, asserts that any formal theory expressive enough for elementary arithmetical facts to be expressed and strong enough for them to be proved is either inconsistent (both a statement and its denial can be derived from its axioms) or incomplete, in the sense that there is a true statement that can't be derived in the formal theory.
Stanley Jaki, in his 1966 book "The Relevance of Physics", pointed out that, because any "theory of everything" will certainly be a consistent non-trivial mathematical theory, it must be incomplete. He claims that this dooms searches for a deterministic theory of everything. In a later reflection, Jaki states that it is wrong to say that a final theory is impossible, but rather that "when it is on hand one cannot know rigorously that it is a final theory."
Freeman Dyson has stated that "Gödel's theorem implies that pure mathematics is inexhaustible. No matter how many problems we solve, there will always be other problems that cannot be solved within the existing rules. [...] Because of Gödel's theorem, physics is inexhaustible too. The laws of physics are a finite set of rules, and include the rules for doing mathematics, so that Gödel's theorem applies to them."
Stephen Hawking was originally a believer in the Theory of Everything but, after considering Gödel's Theorem, concluded that one was not obtainable: "Some people will be very disappointed if there is not an ultimate theory, that can be formulated as a finite number of principles. I used to belong to that camp, but I have changed my mind."
Jürgen Schmidhuber (1997) has argued against this view; he points out that Gödel's theorems are irrelevant for computable physics. In 2000, Schmidhuber explicitly constructed limit-computable, deterministic universes whose pseudo-randomness based on undecidable, Gödel-like halting problems is extremely hard to detect but does not at all prevent formal ToEs describable by very few bits of information.
Related critique was offered by Solomon Feferman, among others. Douglas S. Robertson offers Conway's game of life as an example: The underlying rules are simple and complete, but there are formally undecidable questions about the game's behaviors. Analogously, it may (or may not) be possible to completely state the underlying rules of physics with a finite number of well-defined laws, but there is little doubt that there are questions about the behavior of physical systems which are formally undecidable on the basis of those underlying laws.
Since most physicists would consider the statement of the underlying rules to suffice as the definition of a "theory of everything", most physicists argue that Gödel's Theorem does "not" mean that a ToE cannot exist. On the other hand, the scholars invoking Gödel's Theorem appear, at least in some cases, to be referring not to the underlying rules, but to the understandability of the behavior of all physical systems, as when Hawking mentions arranging blocks into rectangles, turning the computation of prime numbers into a physical question. This definitional discrepancy may explain some of the disagreement among researchers.
Fundamental limits in accuracy.
No physical theory to date is believed to be precisely accurate. Instead, physics has proceeded by a series of "successive approximations" allowing more and more accurate predictions over a wider and wider range of phenomena. Some physicists believe that it
is therefore a mistake to confuse theoretical models with the true nature of reality, and
hold that the series of approximations will never terminate in the "truth". Einstein himself
expressed this view on occasions. Following this view, we may reasonably hope for "a" theory of everything which self-consistently incorporates all currently known forces, but we should not expect it to be the final answer.
On the other hand it is often claimed that, despite the apparently ever-increasing complexity of the mathematics of each new theory, in a deep sense associated with their underlying gauge symmetry and the number of fundamental physical constants, the theories are becoming simpler. If this is the case, the process of simplification cannot continue indefinitely.
Lack of fundamental laws.
There is a philosophical debate within the physics community as to whether a theory of everything deserves to be called "the" fundamental law of the universe. One view is the hard reductionist position that the ToE is the fundamental law and that all other theories that apply within the universe are a consequence of the ToE. Another view is that emergent laws, which govern the behavior of complex systems, should be seen as equally fundamental. Examples of emergent laws are the second law of thermodynamics and the theory of natural selection. The advocates of emergence argue that emergent laws, especially those describing complex or living systems are independent of the low-level, microscopic laws. In this view, emergent laws are as fundamental as a ToE.
The debates do not make the point at issue clear. Possibly the only issue at stake is the right to apply the high-status term "fundamental" to the respective subjects of research. A well-known one took place between Steven Weinberg and Philip Anderson
Impossibility of being "of everything".
Although the name "theory of everything" suggests the determinism of Laplace's quotation, this gives a very misleading impression. Determinism is frustrated by the probabilistic nature of quantum mechanical predictions, by the extreme sensitivity to initial conditions that leads to mathematical chaos, by the limitations due to event horizons, and by the extreme mathematical difficulty of applying the theory. Thus, although the current standard model of particle physics "in principle" predicts almost all known non-gravitational phenomena, in practice only a few quantitative results have been derived from the full theory (e.g., the masses of some of the simplest hadrons), and these results (especially the particle masses which are most relevant for low-energy physics) are less accurate than existing experimental measurements. The ToE would almost certainly be even harder to apply for the prediction of experimental results, and thus might be of limited use.
A motive for seeking a ToE, apart from the pure intellectual satisfaction of completing a centuries-long quest, is that prior examples of unification have predicted new phenomena, some of which (e.g., electrical generators) have proved of great practical importance. And like in these prior examples of unification, the ToE would probably allow us to confidently define the domain of validity and residual error of low-energy approximations to the full theory.
Infinite number of onion layers.
Lee Smolin regularly argues that the layers of nature may be like the layers of an onion, and that the number of layers might be infinite. This would imply an infinite sequence of physical theories.
The argument is not universally accepted, because it is not obvious that infinity is a concept that applies to the foundations of nature.
Impossibility of calculation.
Weinberg points out that calculating the precise motion of an actual projectile in the Earth's atmosphere is impossible. So how can we know we have an adequate theory for describing the motion of projectiles? Weinberg suggests that we know "principles" (Newton's laws of motion and gravitation) that work "well enough" for simple examples, like the motion of planets in empty space. These principles have worked so well on simple examples that we can be reasonably confident they will work for more complex examples. For example, although general relativity includes equations that do not have exact solutions, it is widely accepted as a valid theory because all of its equations with exact solutions have been experimentally verified. Likewise, a ToE must work for a wide range of simple examples in such a way that we can be reasonably confident it will work for every situation in physics.

</doc>
<doc id="30001" url="https://en.wikipedia.org/wiki?curid=30001" title="Theory of relativity">
Theory of relativity

The theory of relativity, or simply relativity in physics, usually encompasses two theories by Albert Einstein: special relativity and general relativity.
Concepts introduced by the theories of relativity include:
The term "theory of relativity" was based on the expression "relative theory" () used in 1906 by Max Planck, who emphasized how the theory uses the principle of relativity. In the discussion section of the same paper, Alfred Bucherer used for the first time the expression "theory of relativity" ().
Scope.
The theory of relativity transformed theoretical physics and astronomy during the 20th century. When first published, relativity superseded a 200-year-old theory of mechanics created primarily by Isaac Newton.
In the field of physics, relativity improved the science of elementary particles and their fundamental interactions, along with ushering in the nuclear age. With relativity, cosmology and astrophysics predicted extraordinary astronomical phenomena such as neutron stars, black holes and gravitational waves.
Two-theory view.
The theory of relativity was representative of more than a single new physical theory. There are some explanations for this. First, special relativity was published in 1905, and the final form of general relativity was published in 1916.
Second, special relativity applies to elementary particles and their interactions, whereas general relativity applies to the cosmological and astrophysical realm, including astronomy.
Third, special relativity was accepted in the physics community by 1920. This theory rapidly became a significant and necessary tool for theorists and experimentalists in the new fields of atomic physics, nuclear physics, and quantum mechanics. Conversely, general relativity did not appear to be as useful. There appeared to be little applicability for experimentalists as most applications were for astronomical scales. It seemed limited to only making minor corrections to predictions of Newtonian gravitation theory.
Finally, the mathematics of general relativity appeared to be very difficult. Consequently, it was thought that a small number of people in the world, at that time, could fully understand the theory in detail, but this has been discredited by Richard Feynman. Then, at around 1960 a critical resurgence in interest occurred which has resulted in making general relativity central to physics and astronomy. New mathematical techniques applicable to the study of general relativity substantially streamlined calculations. From this, physically discernible concepts were isolated from the mathematical complexity. Also, the discovery of exotic astronomical phenomena, in which general relativity was relevant, helped to catalyze this resurgence. The astronomical phenomena included quasars (1963), the 3-kelvin microwave background radiation (1965), pulsars (1967), and the discovery of the first black hole candidates (1981).
On the theory of relativity.
Einstein stated that the theory of relativity belongs to a class of "principle-theories". As such it employs an analytic method. This means that the elements which comprise this theory are not based on hypothesis but on empirical discovery. The empirical discovery leads to understanding the general characteristics of natural processes. Mathematical models are then developed to describe accurately the observed natural processes. Therefore, by analytical means the necessary conditions that have to be satisfied are deduced. Separate events must satisfy these conditions. Experience should then match the conclusions.
The special theory of relativity and the general theory of relativity are connected. As stated below, special theory of relativity applies to all physical phenomena except gravity. The general theory provides the law of gravitation, and its relation to other forces of nature.
Special relativity.
Special relativity is a theory of the structure of spacetime. It was introduced in Einstein's 1905 paper "On the Electrodynamics of Moving Bodies" (for the contributions of many other physicists see History of special relativity). Special relativity is based on two postulates which are contradictory in classical mechanics:
The resultant theory copes with experiment better than classical mechanics. For instance, postulate 2 explains the results of the Michelson–Morley experiment. Moreover, the theory has many surprising and counterintuitive consequences. Some of these are:
The defining feature of special relativity is the replacement of the Galilean transformations of classical mechanics by the Lorentz transformations. (See Maxwell's equations of electromagnetism).
General relativity.
General relativity is a theory of gravitation developed by Einstein in the years 1907–1915. The development of general relativity began with the equivalence principle, under which the states of accelerated motion and being at rest in a gravitational field (for example, when standing on the surface of the Earth) are physically identical. The upshot of this is that free fall is inertial motion: an object in free fall is falling because that is how objects move when there is no force being exerted on them, instead of this being due to the force of gravity as is the case in classical mechanics. This is incompatible with classical mechanics and special relativity because in those theories inertially moving objects cannot accelerate with respect to each other, but objects in free fall do so. To resolve this difficulty Einstein first proposed that spacetime is curved. In 1915, he devised the Einstein field equations which relate the curvature of spacetime with the mass, energy, and momentum within it.
Some of the consequences of general relativity are:
Technically, general relativity is a theory of gravitation whose defining feature is its use of the Einstein field equations. The solutions of the field equations are metric tensors which define the topology of the spacetime and how objects move inertially.
Experimental evidence.
Tests of special relativity.
Like all falsifiable scientific theories, relativity makes predictions that can be tested by experiment. In the case of special relativity, these include the principle of relativity, the constancy of the speed of light, and time dilation. The predictions of special relativity have been confirmed in numerous tests since Einstein published his paper in 1905, but three experiments conducted between 1881 and 1938 were critical to its validation. These are the Michelson–Morley experiment, the Kennedy–Thorndike experiment, and the Ives–Stilwell experiment. Einstein derived the Lorentz transformations from first principles in 1905, but these three experiments allow the transformations to be induced from experimental evidence.
Maxwell's equations – the foundation of classical electromagnetism – describe light as a wave which moves with a characteristic velocity. The modern view is that light needs no medium of transmission, but Maxwell and his contemporaries were convinced that light waves were propagated in a medium, analogous to sound propagating in air, and ripples propagating on the surface of a pond. This hypothetical medium was called the luminiferous aether, at rest relative to the "fixed stars" and through which the Earth moves. Fresnel's partial ether dragging hypothesis ruled out the measurement of first-order (v/c) effects, and although observations of second-order effects (v2/c2) were possible in principle, Maxwell thought they were too small to be detected with then-current technology.
The Michelson–Morley experiment was designed to detect second order effects of the "aether wind" – the motion of the aether relative to the earth. Michelson designed an instrument called the Michelson interferometer to accomplish this. The apparatus was more than accurate enough to detect the expected effects, but he obtained a null result when the first experiment was conducted in 1881, and again in 1887. Although the failure to detect an aether wind was a disappointment, the results were accepted by the scientific community. In an attempt to salvage the aether paradigm, Fitzgerald and Lorentz independently created an "ad hoc" hypothesis in which the length of material bodies changes according to their motion through the aether. This was the origin of FitzGerald–Lorentz contraction, and their hypothesis had no theoretical basis. The interpretation of the null result of the Michelson–Morley experiment is that the round-trip travel time for light is isotropic (independent of direction), but the result alone is not enough to discount the theory of the aether or validate the predictions of special relativity.
While the Michelson–Morley experiment showed that the velocity of light is isotropic, it said nothing about how the magnitude of the velocity changed (if at all) in different inertial frames. The Kennedy–Thorndike experiment was designed to do that, and was first performed in 1932 by Roy Kennedy and Edward Thorndike. They obtained a null result, and concluded that "there is no effect ... unless the velocity of the solar system in space is no more than about half that of the earth in its orbit". That possibility was thought to be too coincidental to provide an acceptable explanation, so from the null result of their experiment it was concluded that the round-trip time for light is the same in all inertial reference frames.
The Ives–Stilwell experiment was carried out by Herbert Ives and G.R. Stilwell first in 1938 and with better accuracy in 1941. It was designed to test the transverse Doppler effect – the redshift of light from a moving source in a direction perpendicular to its velocity – which had been predicted by Einstein in 1905. The strategy was to compare observed Doppler shifts with what was predicted by classical theory, and look for a Lorentz factor correction. Such a correction was observed, from which was concluded that the frequency of a moving atomic clock is altered according to special relativity.
Those classic experiments have been repeated many times with increased precision. Other experiments include, for instance, relativistic energy and momentum increase at high velocities, time dilation of moving particles, and modern searches for Lorentz violations.
Tests of general relativity.
General relativity has also been confirmed many times, the classic experiments being the perihelion precession of Mercury's orbit, the deflection of light by the Sun, and the gravitational redshift of light. Other tests confirmed the equivalence principle and frame dragging.
History.
The history of special relativity consists of many theoretical results and empirical findings obtained by Albert A. Michelson, Hendrik Lorentz, Henri Poincaré and others. It culminated in the theory of special relativity proposed by Albert Einstein, and subsequent work of Max Planck, Hermann Minkowski and others.
General relativity (GR) is a theory of gravitation that was developed by Albert Einstein between 1907 and 1915, with contributions by many others after 1915.
Currently, it can be said that far from being simply of theoretical scientific interest or requiring experimental verification, the analysis of relativistic effects on time measurement is an important practical engineering concern in the operation of the global positioning systems such as GPS, GLONASS, and the forthcoming Galileo, as well as in the high precision dissemination of time. Instruments ranging from electron microscopes to particle accelerators simply will not work if relativistic considerations are omitted.
Everyday applications.
The theory of relativity is used in many of our modern electronics such as the Global Positioning System (GPS). GPS systems are made up of three components, the control component, the space component, and the user component. The space component consists of satellites that are placed in specific orbits. The control component consists of a station to which all of the data from the space component is sent. Many relativistic effects occur in GPS systems. Since each of the components is in different reference frames, all of the relativistic effects need to be accounted for so that the GPS works with precision. The clocks used in the GPS systems need to be synchronized. In GPS systems, the gravitational field of the Earth has to be accounted for. There are relativistic effects within the satellite that is in space that need to be accounted for too. GPS systems work with such precision because of the Theory of Relativity.

</doc>
<doc id="20405837" url="https://en.wikipedia.org/wiki?curid=20405837" title="Theta meson">
Theta meson

The theta meson () is a hypothetical form of quarkonium (i.e. a flavourless meson) formed by a top quark and top antiquark. It is the equivalent of the phi (strange quark, strange antiquark), psi (charm quark, charm antiquark) and upsilon (bottom quark, bottom antiquark) mesons. Due to the top quark's shortlifetime, the theta meson is not expected to be observed in nature.

</doc>
<doc id="30012" url="https://en.wikipedia.org/wiki?curid=30012" title="Time">
Time

Time is a measure in which events can be ordered from the past through the present into the future, and also the measure of durations of events and the intervals between them. Time is often referred to as the fourth dimension, along with the three spatial dimensions.
Time has long been a major subject of study in religion, philosophy, and science, but defining it in a manner applicable to all fields without circularity has consistently eluded scholars.
Nevertheless, diverse fields such as business, industry, sports, the sciences, and the performing arts all incorporate some notion of time into their respective measuring systems.
Some simple definitions of time include "time is what clocks measure", which is a problematically vague and self-referential definition that utilizes the device used to measure the subject as the definition of the subject, and "time is what keeps everything from happening at once", which is without substantive meaning in the absence of the definition of simultaneity in the context of the limitations of human sensation, observation of events, and the perception of such events.
Two contrasting viewpoints on time divide many prominent philosophers.
One view is that time is part of the fundamental structure of the universe—a dimension independent of events, in which events occur in sequence.
Sir Isaac Newton subscribed to this realist view, and hence it is sometimes referred to as Newtonian time.
The opposing view is that "time" does not refer to any kind of "container" that events and objects "move through", nor to any entity that "flows", but that it is instead part of a fundamental intellectual structure (together with space and number) within which humans sequence and compare events. This second view, in the tradition of Gottfried Leibniz
and Immanuel Kant,
holds that "time" is neither an event nor a thing, and thus is not itself measurable nor can it be travelled.
Time is one of the seven fundamental physical quantities in both the International System of Units and International System of Quantities. Time is used to define other quantities—such as velocity—so defining time in terms of such quantities would result in circularity of definition.
An operational definition of time, wherein one says that observing a certain number of repetitions of one or another standard cyclical event (such as the passage of a free-swinging pendulum) constitutes one standard unit such as the second, is highly useful in the conduct of both advanced experiments and everyday affairs of life. The operational definition leaves aside the question whether there is something called time, apart from the counting activity just mentioned, that flows and that can be measured. Investigations of a single continuum called spacetime bring questions about space into questions about time, questions that have their roots in the works of early students of natural philosophy.
Furthermore, it may be that there is a subjective component to time, but whether or not time itself is "felt", as a sensation, or is a judgment, is a matter of debate.
Temporal measurement has occupied scientists and technologists, and was a prime motivation in navigation and astronomy. Periodic events and periodic motion have long served as standards for units of time. Examples include the apparent motion of the sun across the sky, the phases of the moon, the swing of a pendulum, and the beat of a heart. Currently, the international unit of time, the second, is defined by measuring the electronic transition frequency of caesium atoms (see below). Time is also of significant social importance, having economic value ("time is money") as well as personal value, due to an awareness of the limited time in each day and in human life spans.
Temporal measurement and history.
Temporal measurement, chronometry, takes two distinct period forms: the calendar, a mathematical tool for organizing intervals of time,
and the clock, a physical mechanism that counts the passage of time. In day-to-day life, the clock is consulted for periods less than a day, the calendar, for periods longer than a day. Increasingly, personal electronic devices display both calendars and clocks simultaneously. The number (as on a clock dial or calendar) that marks the occurrence of a specified event as to hour or date is obtained by counting from a fiducial epoch—a central reference point.
History of the calendar.
Artifacts from the Paleolithic suggest that the moon was used to reckon time as early as 6,000 years ago.
Lunar calendars were among the first to appear, either 12 or 13 lunar months (either 354 or 384 days). Without intercalation to add days or months to some years, seasons quickly drift in a calendar based solely on twelve lunar months. Lunisolar calendars have a thirteenth month added to some years to make up for the difference between a full year (now known to be about 365.24 days) and a year of just twelve lunar months. The numbers twelve and thirteen came to feature prominently in many cultures, at least partly due to this relationship of months to years.
The reforms of Julius Caesar in 45 BC put the Roman world on a solar calendar. This Julian calendar was faulty in that its intercalation still allowed the astronomical solstices and equinoxes to advance against it by about 11 minutes per year. Pope Gregory XIII introduced a correction in 1582; the Gregorian calendar was only slowly adopted by different nations over a period of centuries, but it is now the most commonly used calendar around the world, by far.
History of time measurement devices.
A large variety of devices has been invented to measure time. The study of these devices is called horology.
An Egyptian device that dates to c.1500 BC, similar in shape to a bent T-square, measured the passage of time from the shadow cast by its crossbar on a nonlinear rule. The T was orientated eastward in the mornings. At noon, the device was turned around so that it could cast its shadow in the evening direction.
A sundial uses a gnomon to cast a shadow on a set of markings calibrated to the hour. The position of the shadow marks the hour in local time.
The most precise timekeeping device of the ancient world was the water clock, or "clepsydra", one of which was found in the tomb of Egyptian pharaoh Amenhotep I (1525–1504 BC). They could be used to measure the hours even at night, but required manual upkeep to replenish the flow of water. The Ancient Greeks and the people from Chaldea (southeastern Mesopotamia) regularly maintained timekeeping records as an essential part of their astronomical observations. Arab inventors and engineers in particular made improvements on the use of water clocks up to the Middle Ages.
In the 11th century, Chinese inventors and engineers invented the first mechanical clocks driven by an escapement mechanism.
The hourglass uses the flow of sand to measure the flow of time. They were used in navigation. Ferdinand Magellan used 18 glasses on each ship for his circumnavigation of the globe (1522).
Incense sticks and candles were, and are, commonly used to measure time in temples and churches across the globe. Waterclocks, and later, mechanical clocks, were used to mark the events of the abbeys and monasteries of the Middle Ages. Richard of Wallingford (1292–1336), abbot of St. Alban's abbey, famously built a mechanical clock as an astronomical orrery about 1330.
Great advances in accurate time-keeping were made by Galileo Galilei and especially Christiaan Huygens with the invention of pendulum driven clocks.
The English word clock probably comes from the Middle Dutch word "klocke" which, in turn, derives from the medieval Latin word "clocca", which ultimately derives from Celtic and is cognate with French, Latin, and German words that mean bell. The passage of the hours at sea were marked by bells, and denoted the time (see ship's bell). The hours were marked by bells in abbeys as well as at sea.
Clocks can range from watches, to more exotic varieties such as the Clock of the Long Now. They can be driven by a variety of means, including gravity, springs, and various forms of electrical power, and regulated by a variety of means such as a pendulum.
A chronometer is a portable timekeeper that meets certain precision standards. Initially, the term was used to refer to the marine chronometer, a timepiece used to determine longitude by means of celestial navigation, a precision firstly achieved by John Harrison. More recently, the term has also been applied to the chronometer watch, a watch that meets precision standards set by the Swiss agency COSC.
The most accurate timekeeping devices are atomic clocks, which are accurate to seconds in many millions of years,
and are used to calibrate other clocks and timekeeping instruments.
Atomic clocks use the frequency of electronic transitions in certain atoms to measure the second. One of the most common atoms used is caesium, most modern atomic clocks probe caesium with microwaves to determine the frequency of these electron vibrations. Since 1967, the International System of Measurements bases its unit of time, the second, on the properties of caesium atoms. SI defines the second as 9,192,631,770 cycles of the radiation that corresponds to the transition between two electron spin energy levels of the ground state of the 133Cs atom.
Today, the Global Positioning System in coordination with the Network Time Protocol can be used to synchronize timekeeping systems across the globe.
In medieval philosophical writings, the atom was a unit of time referred to as the smallest possible division of time. The earliest known occurrence in English is in Byrhtferth's "Enchiridion" (a science text) of 1010–1012,
where it was defined as 1/564 of a "momentum" (1½ minutes),
and thus equal to 15/94 of a second. It was used in the "computus", the process of calculating the date of Easter.
, the smallest time interval uncertainty in direct measurements is on the order of 12 attoseconds (1.2 × 10−17 seconds), about 3.7 × 1026 Planck times.
Definitions and standards.
The SI base unit for time is the SI second. The International System of Quantities, which incorporates the SI, also defines larger units of time equal to fixed integer multiples of one second (1 s), such as the minute, hour and day. These are not part of the SI, but may be used alongside the SI. Other units of time such as the month and the year are not equal to fixed multiples of 1 s, and instead exhibit significant variations in duration.
The official SI definition of the second is as follows:
At its 1997 meeting, the CIPM affirmed that this definition refers to a caesium atom in its ground state at a temperature of 0 K.
Previous to 1967, the second was defined as:
The current definition of the second, coupled with the current definition of the metre, is based on the special theory of relativity, which affirms our spacetime to be a Minkowski space.
World time.
Time-keeping is so critical to the functioning of modern societies that it is coordinated at an international level. The basis for scientific time is a continuous count of seconds based on atomic clocks around the world, known as the International Atomic Time (TAI). Other scientific time standards include Terrestrial Time and Barycentric Dynamical Time.
Coordinated Universal Time (UTC) is the basis for modern civil time. Since 1 January 1972, it has been defined to follow TAI with an exact offset of an integer number of seconds, changing only when a leap second is added to keep clock time synchronized with the rotation of the Earth. In TAI and UTC systems, the duration of a second is constant, as it is defined by the unchanging transition period of the caesium atom.
Greenwich Mean Time (GMT) is an older standard, adopted starting with British railways in 1847. Using telescopes instead of atomic clocks, GMT was calibrated to the mean solar time at the Royal Observatory, Greenwich in the UK. Universal Time (UT) is the modern term for the international telescope-based system, adopted to replace "Greenwich Mean Time" in 1928 by the International Astronomical Union. Observations at the Greenwich Observatory itself ceased in 1954, though the location is still used as the basis for the coordinate system. Because the rotational period of Earth is not perfectly constant, the duration of a second would vary if calibrated to a telescope-based standard like GMT or UT—in which a second was defined as a fraction of a day or year. The terms "GMT" and "Greenwich Mean Time" are sometimes used informally to refer to UT or UTC.
The Global Positioning System also broadcasts a very precise time signal worldwide, along with instructions for converting GPS time to UTC.
Earth is split up into a number of time zones. Most time zones are exactly one hour apart, and by convention compute their local time as an offset from UTC or GMT. In many locations these offsets vary twice yearly due to daylight saving time transitions.
Time conversions.
These conversions are accurate at the millisecond level for time systems involving earth rotation (UT1 & TT). Conversions between atomic time systems (TAI, GPS, and UTC) are accurate at the microsecond level.
Definitions:
Sidereal time.
Sidereal time is the measurement of time relative to a distant star (instead of solar time that is relative to the sun). It is used in astronomy to predict when a star will be overhead. Due to the orbit of the earth around the sun a sidereal day is about 4 minutes (1/366th) less than a solar day.
Chronology.
Another form of time measurement consists of studying the past. Events in the past can be ordered in a sequence (creating a chronology), and can be put into chronological groups (periodization). One of the most important systems of periodization is the geologic time scale, which is a system of periodizing the events that shaped the Earth and its life. Chronology, periodization, and interpretation of the past are together known as the study of history.
Time-like concepts: terminology.
The term "time" is generally used for many close but different concepts, including:
Religion.
Linear and cyclical time.
Ancient cultures such as Incan, Mayan, Hopi, and other Native American Tribes - plus the Babylonians, Ancient Greeks, Hinduism, Buddhism, Jainism, and others - have a concept of a wheel of time: they regard time as cyclical and quantic, consisting of repeating ages that happen to every being of the Universe between birth and extinction.
In general, the Islamic and Judeo-Christian world-view regards time as linear
and directional,
beginning with the act of creation by God. The traditional Christian view sees time ending, teleologically, 
with the eschatological end of the present order of things, the "end time".
In the Old Testament book Ecclesiastes, traditionally ascribed to Solomon (970–928 BC), time (as the Hebrew word עדן, זמן "`iddan(time) zĕman(season)" is often translated) was traditionally regarded as a medium for the passage of predestined events. (Another word, زمان" זמן" "zamān", meant "time fit for an event", and is used as the modern Arabic, Persian, and Hebrew equivalent to the English word "time".)
Time in Greek mythology.
The Greek language denotes two distinct principles, Chronos and Kairos. The former refers to numeric, or chronological, time. The latter, literally "the right or opportune moment", relates specifically to metaphysical or Divine time. In theology, Kairos is qualitative, as opposed to quantitative.
In Greek mythology, Chronos (Ancient Greek: Χρόνος) is identified as the Personification of Time. His name in Greek means "time" and is alternatively spelled Chronus (Latin spelling) or Khronos. Chronos is usually portrayed as an old, wise man with a long, gray beard, such as "Father Time". Some English words whose etymological root is khronos/chronos include "chronology", "chronometer", "chronic", "anachronism", "synchronize", and "chronicle".
Time in Kabbalah.
According to Kabbalists, “time” is a paradox and an illusion. Both the future and the past are recognized to be simultaneously present.
Philosophy.
Two distinct viewpoints on time divide many prominent philosophers.
One view is that time is part of the fundamental structure of the universe, a dimension in which events occur in sequence. Sir Isaac Newton subscribed to this realist view, and hence it is sometimes referred to as Newtonian time.
An opposing view is that "time" does not refer to any kind of actually existing dimension that events and objects "move through", nor to any entity that "flows", but that it is instead an intellectual concept (together with space and number) that enables humans to sequence and compare events.
This second view, in the tradition of Gottfried Leibniz
and Immanuel Kant,
holds that space and time "do not exist in and of themselves, but ... are the product of the way we represent things", because we can know objects only as they appear to us.
The "Vedas", the earliest texts on Indian philosophy and Hindu philosophy dating back to the late 2nd millennium BC, describe ancient Hindu cosmology, in which the universe goes through repeated cycles of creation, destruction and rebirth, with each cycle lasting 4,320 million years.
Ancient Greek philosophers, including Parmenides and Heraclitus, wrote essays on the nature of time.
Plato, in the "Timaeus", identified time with the period of motion of the heavenly bodies. Aristotle, in Book IV of his "Physica" defined time as 'number of movement in respect of the before and after'.
In Book 11 of his "Confessions", St. Augustine of Hippo ruminates on the nature of time, asking, "What then is time? If no one asks me, I know: if I wish to explain it to one that asketh, I know not." He begins to define time by what it is not rather than what it is,
an approach similar to that taken in other negative definitions. However, Augustine ends up calling time a “distention” of the mind (Confessions 11.26) by which we simultaneously grasp the past in memory, the present by attention, and the future by expectation.
In contrast to ancient Greek philosophers who believed that the universe had an infinite past with no beginning, medieval philosophers and theologians developed the concept of the universe having a finite past with a beginning.
This view is shared by Abrahamic faiths as they believe time started by creation, therefore the only thing being infinite is God and everything else, including time, is finite.
Isaac Newton believed in absolute space and absolute time; Leibniz believed that time and space are relational.
The differences between Leibniz's and Newton's interpretations came to a head in the famous Leibniz–Clarke correspondence.
Immanuel Kant, in the "Critique of Pure Reason", described time as an "a priori" intuition that allows us (together with the other "a priori" intuition, space) to comprehend sense experience.
With Kant, neither space nor time are conceived as substances, but rather both are elements of a systematic mental framework that necessarily structures the experiences of any rational agent, or observing subject. Kant thought of time as a fundamental part of an abstract conceptual framework, together with space and number, within which we sequence events, quantify their duration, and compare the motions of objects. In this view, "time" does not refer to any kind of entity that "flows," that objects "move through," or that is a "container" for events. Spatial measurements are used to quantify the extent of and distances between objects, and temporal measurements are used to quantify the durations of and between events. Time was designated by Kant as the purest possible schema of a pure concept or category.
Henri Bergson believed that time was neither a real homogeneous medium nor a mental construct, but possesses what he referred to as "Duration". Duration, in Bergson's view, was creativity and memory as an essential component of reality.
According to Martin Heidegger we do not exist inside time, "we are time". Hence, the relationship to the past is a present awareness of "having been", which allows the past to exist in the present. The relationship to the future is the state of anticipating a potential possibility, task, or engagement. It is related to the human propensity for caring and being concerned, which causes "being ahead of oneself" when thinking of a pending occurrence. Therefore, this concern for a potential occurrence also allows the future to exist in the present. The present becomes an experience, which is qualitative instead of quantitative. Heidegger seems to think this is the way that a linear relationship with time, or temporal existence, is broken or transcended.
We are not stuck in sequential time. We are able to remember the past and project into the future—we have a kind of random access to our representation of temporal existence; we can, in our thoughts, step out of (ecstasis) sequential time.
Time as "unreal".
In 5th century BC Greece, Antiphon the Sophist, in a fragment preserved from his chief work "On Truth", held that: "Time is not a reality (hypostasis), but a concept (noêma) or a measure (metron)."
Parmenides went further, maintaining that time, motion, and change were illusions, leading to the paradoxes of his follower Zeno.
Time as an illusion is also a common theme in Buddhist thought.
J. M. E. McTaggart's 1908 "The Unreality of Time" argues that, since every event has the characteristic of being both present and not present (i.e., future or past), that time is a self-contradictory idea (see also The flow of time).
These arguments often center around what it means for something to be "unreal". Modern physicists generally believe that time is as "real" as space—though others, such as Julian Barbour in his book "The End of Time", argue that quantum equations of the universe take their true form when expressed in the timeless realm containing every possible "now" or momentary configuration of the universe, called 'platonia' by Barbour.
Physical definition.
Until Einstein's profound reinterpretation of the physical concepts associated with time and space, time was considered to be the same everywhere in the universe, with all observers measuring the same time interval for any event.
Non-relativistic classical mechanics is based on this Newtonian idea of time.
Einstein, in his special theory of relativity,
postulated the constancy and finiteness of the speed of light for all observers. He showed that this postulate, together with a reasonable definition for what it means for two events to be simultaneous, requires that distances appear compressed and time intervals appear lengthened for events associated with objects in motion relative to an inertial observer.
The theory of special relativity finds a convenient formulation in Minkowski spacetime, a mathematical structure that combines three dimensions of space with a single dimension of time. In this formalism, distances in space can be measured by how long light takes to travel that distance, e.g., a light-year is a measure of distance, and a meter is now defined in terms of how far light travels in a certain amount of time. Two events in Minkowski spacetime are separated by an "invariant interval", which can be either space-like, light-like, or time-like. Events that have a time-like separation cannot be simultaneous in any frame of reference, there must be a temporal component (and possibly a spatial one) to their separation. Events that have a space-like separation will be simultaneous in some frame of reference, and there is no frame of reference in which they do not have a spatial separation. Different observers may calculate different distances and different time intervals between two events, but the "invariant interval" between the events is independent of the observer (and his velocity).
Classical mechanics.
In non-relativistic classical mechanics, Newton's concept of "relative, apparent, and common time" can be used in the formulation of a prescription for the synchronization of clocks. Events seen by two different observers in motion relative to each other produce a mathematical concept of time that works sufficiently well for describing the everyday phenomena of most people's experience. In the late nineteenth century, physicists encountered problems with the classical understanding of time, in connection with the behavior of electricity and magnetism. Einstein resolved these problems by invoking a method of synchronizing clocks using the constant, finite speed of light as the maximum signal velocity. This led directly to the result that observers in motion relative to one another measure different elapsed times for the same event.
Spacetime.
Time has historically been closely related with space, the two together merging into spacetime in Einstein's special relativity and general relativity. According to these theories, the concept of time depends on the spatial reference frame of the observer, and the human perception as well as the measurement by instruments such as clocks are different for observers in relative motion. For example, if a spaceship carrying a clock flies through space at (very nearly) the speed of light, its crew does not notice a change in the speed of time on board their vessel because everything traveling at the same speed slows down at the same rate (including the clock, the crew's thought processes, and the functions of their bodies). However, to a stationary observer watching the spaceship fly by, the spaceship appears flattened in the direction it is traveling and the clock on board the spaceship appears to move very slowly.
On the other hand, the crew on board the spaceship also perceives the observer as slowed down and flattened along the spaceship's direction of travel, because both are moving at very nearly the speed of light relative to each other. Because the outside universe appears flattened to the spaceship, the crew perceives themselves as quickly traveling between regions of space that (to the stationary observer) are many light years apart. This is reconciled by the fact that the crew's perception of time is different from the stationary observer's; what seems like seconds to the crew might be hundreds of years to the stationary observer. In either case, however, causality remains unchanged: the past is the set of events that can send light signals to an entity and the future is the set of events to which an entity can send light signals.
Time dilation.
Einstein showed in his thought experiments that people travelling at different speeds, while agreeing on cause and effect, measure different time separations between events, and can even observe different chronological orderings between non-causally related events. Though these effects are typically minute in the human experience, the effect becomes much more pronounced for objects moving at speeds approaching the speed of light. Many subatomic particles exist for only a fixed fraction of a second in a lab relatively at rest, but some that travel close to the speed of light can be measured to travel farther and survive much longer than expected (a muon is one example). According to the special theory of relativity, in the high-speed particle's frame of reference, it exists, on the average, for a standard amount of time known as its mean lifetime, and the distance it travels in that time is zero, because its velocity is zero. Relative to a frame of reference at rest, time seems to "slow down" for the particle. Relative to the high-speed particle, distances seem to shorten. Einstein showed how both temporal and spatial dimensions can be altered (or "warped") by high-speed motion.
Einstein ("The Meaning of Relativity"): "Two events taking place at the points A and B of a system K are simultaneous if they appear at the same instant when observed from the middle point, M, of the interval AB. Time is then defined as the ensemble of the indications of similar clocks, at rest relatively to K, which register the same simultaneously."
Einstein wrote in his book, "Relativity", that simultaneity is also relative, i.e., two events that appear simultaneous to an observer in a particular inertial reference frame need not be judged as simultaneous by a second observer in a different inertial frame of reference.
Relativistic time versus Newtonian time.
The animations visualise the different treatments of time in the Newtonian and the relativistic descriptions. At the heart of these differences are the Galilean and Lorentz transformations applicable in the Newtonian and relativistic theories, respectively.
In the figures, the vertical direction indicates time. The horizontal direction indicates distance (only one spatial dimension is taken into account), and the thick dashed curve is the spacetime trajectory ("world line") of the observer. The small dots indicate specific (past and future) events in spacetime.
The slope of the world line (deviation from being vertical) gives the relative velocity to the observer. Note how in both pictures the view of spacetime changes when the observer accelerates.
In the Newtonian description these changes are such that "time" is absolute: the movements of the observer do not influence whether an event occurs in the 'now' (i.e., whether an event passes the horizontal line through the observer).
However, in the relativistic description the "observability of events" is absolute: the movements of the observer do not influence whether an event passes the "light cone" of the observer. Notice that with the change from a Newtonian to a relativistic description, the concept of "absolute time" is no longer applicable: events move up-and-down in the figure depending on the acceleration of the observer.
Arrow of time.
Time appears to have a direction—the past lies behind, fixed and immutable, while the future lies ahead and is not necessarily fixed. Yet for the most part the laws of physics do not specify an arrow of time, and allow any process to proceed both forward and in reverse. This is generally a consequence of time being modeled by a parameter in the system being analyzed, where there is no "proper time": the direction of the arrow of time is sometimes arbitrary. Examples of this include the Second law of thermodynamics, which states that entropy must increase over time (see Entropy); the cosmological arrow of time, which points away from the Big Bang, CPT symmetry, and the radiative arrow of time, caused by light only traveling forwards in time (see light cone). In particle physics, the violation of CP symmetry implies that there should be a small counterbalancing time asymmetry to preserve CPT symmetry as stated above. The standard description of measurement in quantum mechanics is also time asymmetric (see Measurement in quantum mechanics).
Quantized time.
Time quantization is a hypothetical concept. In the modern established physical theories (the Standard Model of Particles and Interactions and General Relativity) time is not quantized.
Planck time (~ 5.4 × 10−44 seconds) is the unit of time in the system of natural units known as Planck units. Current established physical theories are believed to fail at this time scale, and many physicists expect that the Planck time might be the smallest unit of time that could ever be measured, even in principle. Tentative physical theories that describe this time scale exist; see for instance loop quantum gravity.
Time and the Big Bang theory.
Stephen Hawking in particular has addressed a connection between time and the Big Bang. In "A Brief History of Time" and elsewhere, Hawking says that even if time did not begin with the Big Bang and there were another time frame before the Big Bang, no information from events then would be accessible to us, and nothing that happened then would have any effect upon the present time-frame.
Upon occasion, Hawking has stated that time actually began with the Big Bang, and that questions about what happened "before" the Big Bang are "meaningless".
This less-nuanced, but commonly repeated formulation has received criticisms from philosophers such as Aristotelian philosopher Mortimer J. Adler.
Scientists have come to some agreement on descriptions of events that happened 10−35 seconds after the Big Bang, but generally agree that descriptions about what happened before one Planck time (5 × 10−44 seconds) after the Big Bang are likely to remain pure speculation.
Speculative physics beyond the Big Bang.
While the Big Bang model is well established in cosmology, it is likely to be refined in the future. Little is known about the earliest moments of the universe's history. The Penrose–Hawking singularity theorems require the existence of a singularity at the beginning of cosmic time. However, these theorems assume that general relativity is correct, but general relativity must break down before the universe reaches the Planck temperature, and a correct treatment of quantum gravity may avoid the singularity.
There may also be parts of the universe well beyond what can be observed in principle. If inflation occurred this is likely, for exponential expansion would push large regions of space beyond our observable horizon.
Some proposals, each of which entails untested hypotheses, are:
Proposals in the last two categories see the Big Bang as an event in a much larger and older universe, or multiverse, and not the literal beginning.
Time travel.
Time travel is the concept of moving backwards or forwards to different points in time, in a manner analogous to moving through space, and different from the normal "flow" of time to an earthbound observer. In this view, all points in time (including future times) "persist" in some way. Time travel has been a plot device in fiction since the 19th century. Traveling backwards in time has never been verified, presents many theoretic problems, and may be an impossibility.
Any technological device, whether fictional or hypothetical, that is used to achieve time travel is known as a time machine.
A central problem with time travel to the past is the violation of causality; should an effect precede its cause, it would give rise to the possibility of a temporal paradox. Some interpretations of time travel resolve this by accepting the possibility of travel between branch points, parallel realities, or universes.
Another solution to the problem of causality-based temporal paradoxes is that such paradoxes cannot arise simply because they have not arisen. As illustrated in numerous works of fiction, free will either ceases to exist in the past or the outcomes of such decisions are predetermined. As such, it would not be possible to enact the grandfather paradox because it is a historical fact that your grandfather was not killed before his child (your parent) was conceived. This view doesn't simply hold that history is an unchangeable constant, but that any change made by a hypothetical future time traveler would already have happened in his or her past, resulting in the reality that the traveler moves from. More elaboration on this view can be found in the Novikov self-consistency principle.
Time perception.
The specious present refers to the time duration wherein one's perceptions are considered to be in the present. The experienced present is said to be ‘specious’ in that, unlike the objective present, it is an interval and not a durationless instant. The term "specious present" was first introduced by the psychologist E.R. Clay, and later developed by William James.
Biopsychology.
The brain's judgment of time is known to be a highly distributed system, including at least the cerebral cortex, cerebellum and basal ganglia as its components. One particular component, the suprachiasmatic nuclei, is responsible for the circadian (or daily) rhythm, while other cell clusters appear capable of shorter-range (ultradian) timekeeping.
Psychoactive drugs can impair the judgment of time. Stimulants can lead both humans and rats to overestimate time intervals,
while depressants can have the opposite effect.
The level of activity in the brain of neurotransmitters such as dopamine and norepinephrine may be the reason for this.
Such chemicals will either excite or inhibit the firing of neurons in the brain, with a greater firing rate allowing the brain to register the occurrence of more events within a given interval (speed up time) and a decreased firing rate reducing the brain's capacity to distinguish events occurring within a given interval (slow down time).
Mental chronometry is the use of response time in perceptual-motor tasks to infer the content, duration, and temporal sequencing of cognitive operations.
Development of awareness and understanding of time in children.
Children's expanding cognitive abilities allow them to understand time more clearly. Two- and three-year-olds' understanding of time is mainly limited to "now and not now." Five- and six-year-olds can grasp the ideas of past, present, and future. Seven- to ten-year-olds can use clocks and calendars.
Alterations.
In addition to psychoactive drugs, judgments of time can be altered by temporal illusions (like the kappa effect), age,
and hypnosis.
The sense of time is impaired in some people with neurological diseases such as Parkinson's disease and attention deficit disorder.
Psychologists assert that time seems to go faster with age, but the literature on this age-related perception of time remains controversial.
Those who support this notion argue that young people, having more excitatory neurotransmitters, are able to cope with faster external events.
Use of time.
In sociology and anthropology, time discipline is the general name given to social and economic rules, conventions, customs, and expectations governing the measurement of time, the social currency and awareness of time measurements, and people's expectations concerning the observance of these customs by others. Arlie Russell Hochschild and Norbert Elias have written on the use of time from a sociological perspective.
The use of time is an important issue in understanding human behavior, education, and travel behavior. Time-use research is a developing field of study. The question concerns how time is allocated across a number of activities (such as time spent at home, at work, shopping, etc.). Time use changes with technology, as the television or the Internet created new opportunities to use time in different ways. However, some aspects of time use are relatively stable over long periods of time, such as the amount of time spent traveling to work, which despite major changes in transport, has been observed to be about 20–30 minutes one-way for a large number of cities over a long period.
Time management is the organization of tasks or events by first estimating how much time a task requires and when it must be completed, and adjusting events that would interfere with its completion so it is done in the appropriate amount of time. Calendars and day planners are common examples of time management tools.
A sequence of events, or series of events, is a sequence of items, facts, events, actions, changes, or procedural steps, arranged in time order (chronological order), often with causality relationships among the items.
Because of causality, cause precedes effect, or cause and effect may appear together in a single item, but effect never precedes cause. A sequence of events can be presented in text, tables, charts, or timelines. The description of the items or events may include a timestamp. A sequence of events that includes the time along with place or location information to describe a sequential path may be referred to as a world line.
Uses of a sequence of events include stories,
historical events (chronology), directions and steps in procedures,
and timetables for scheduling activities. A sequence of events may also be used to help describe processes in science, technology, and medicine. A sequence of events may be focused on past events (e.g., stories, history, chronology), on future events that must be in a predetermined order (e.g., plans, schedules, procedures, timetables), or focused on the observation of past events with the expectation that the events will occur in the future (e.g., processes). The use of a sequence of events occurs in fields as diverse as machines (cam timer), documentaries ("Seconds From Disaster"), law (choice of law), computer simulation (discrete event simulation), and electric power transmission
(sequence of events recorder). A specific example of a sequence of events is the timeline of the Fukushima Daiichi nuclear disaster.
See also.
Organizations.
"Leading scholarly organizations for researchers on the history and technology of time and timekeeping"

</doc>
<doc id="297839" url="https://en.wikipedia.org/wiki?curid=297839" title="Time dilation">
Time dilation

In the theory of relativity, time dilation is a difference of elapsed time between two events as measured by observers either moving relative to each other or differently situated from a gravitational mass or masses.
An accurate clock at rest with respect to one observer may be measured to tick at a different rate when compared to a second observer's own equally accurate clock. This effect arises neither from technical aspects of the clocks nor from the fact that signals need time to propagate, but from the nature of spacetime itself.
Overview.
Clocks on the Space Shuttle run slightly slower than reference clocks on Earth, while clocks on GPS and Galileo satellites run slightly faster. Such time dilation has been repeatedly demonstrated (see experimental confirmation below), for instance by small disparities in atomic clocks on Earth and in space, even though both clocks work perfectly (it is not a mechanical malfunction). The laws of nature are such that time itself (i.e. spacetime) will bend due to differences in either gravity or velocity – each of which affects time in different ways.
In theory, and to make a clearer example, time dilation could affect planned meetings for astronauts with advanced technologies and greater travel speeds. The astronauts would have to set their clocks to count exactly 80 years, whereas mission control – back on Earth – might need to count 81 years. The astronauts would return to Earth, after their mission, having aged one year less than the people staying on Earth. What is more, the local experience of time passing never actually changes for anyone. In other words, the astronauts on the ship as well as the mission control crew on Earth each feel normal, despite the effects of time dilation (i.e. to the traveling party, those stationary are living "faster"; while to those who stood still, their counterparts in motion live "slower" at any given moment).
With technology limiting the velocities of astronauts, these differences are minuscule: after 6 months on the International Space Station (ISS), the astronaut crew has indeed aged less than those on Earth, but only by about 0.005 seconds (nowhere near the 1 year disparity from the theoretical example). The effects would be greater if the astronauts were traveling nearer to the speed of light (299,792,458 m/s), instead of their actual speed – which is the speed of the orbiting ISS, about 7,700 m/s.
Time dilation is caused by differences in either gravity or relative velocity. In the case of ISS, time is slower due to the velocity in circular orbit; this effect is slightly reduced by the opposing effect of less gravitational potential.
Relative velocity time dilation.
When two observers are in relative uniform motion and uninfluenced by any gravitational mass, the point of view of each will be that the other's (moving) clock is ticking at a "slower" rate than the local clock. The faster the relative velocity, the greater the magnitude of time dilation. This case is sometimes called special relativistic time dilation.
For instance, two rocket ships (A and B) speeding past one another in space would experience time dilation. If they somehow had a clear view into each other's ships, each crew would see the others' clocks and movement as going more slowly. That is, inside the frame of reference of Ship A, everything is moving normally, but everything over on Ship B appears to be moving more slowly (and vice versa).
From a local perspective, time registered by clocks that are at rest with respect to the local frame of reference (and far from any gravitational mass) always appears to pass at the same rate. In other words, if a new ship, Ship C, travels alongside Ship A, it is "at rest" relative to Ship A. From the point of view of Ship A, new Ship C's time would appear normal too.
A question arises: If Ship A and Ship B both think each other's time is moving slower, who will have aged more if they decided to meet up? With a more sophisticated understanding of relative velocity time dilation, this seeming twin paradox turns out not to be a paradox at all (the resolution of the paradox involves a jump in time, as a result of the accelerated observer turning around). Similarly, understanding the twin paradox would help explain why astronauts on the ISS age slower (e.g. 0.007 seconds behind for every six months) even though they are experiencing relative velocity time dilation.
Gravitational time dilation.
Gravitational time dilation is at play for ISS astronauts too, and it has the opposite effect of the relative velocity time dilation. To simplify, relative velocity and gravity each slow down time as they increase. Velocity has increased for the astronauts, slowing down their time, whereas gravity has decreased, speeding up time (the astronauts are experiencing less gravity than on Earth). Nevertheless, the ISS astronaut crew ultimately end up with "slower" time because the two opposing effects are not equally strong. The velocity time dilation (explained above) is making a bigger difference, and slowing down time. The (time-speeding up) effects of low-gravity would not cancel out these (time-slowing down) effects of velocity unless the ISS orbited much farther from Earth.
The key is that both observers are differently situated in their distance from a significant gravitational mass. The general theory of relativity describes how, for both observers, the clock that is closer to the gravitational mass, i.e. deeper in its "gravity well", appears to go more slowly than the clock that is more distant from the mass. This effect is not restricted to astronauts in space; a climber's time is passing slightly faster at the top of a mountain (a high altitude, farther from the Earth's center of gravity) compared to people at sea level. As with all time dilation, the local experience of time is normal (nobody notices a difference within their own frame of reference). In the situations of velocity time dilation, both observers saw the other as moving slower (a reciprocal effect). Now, with gravitational time dilation, both observers – those at sea level, versus the climber – agree that the clock nearer the mass is slower in rate, and they agree on the ratio of the difference (time dilation from gravity is therefore not reciprocal). That is, the climber sees the sea level clocks as moving more slowly, and those living at sea level see the climber's clock as moving faster.
Time dilation: special vs. general theories of relativity.
In Albert Einstein's theory of relativity, time dilation in these two circumstances can be summarized:
Special and general relativistic effects can combine (as seen with ISS astronauts).
In special relativity, the time dilation effect is reciprocal: as observed from the point of view of either of two clocks which are in motion with respect to each other, it will be the other clock that is time dilated. (This presumes that the relative motion of both parties is uniform; that is, they do not accelerate with respect to one another during the course of the observations.) In contrast, gravitational time dilation (as treated in general relativity) is not reciprocal: an observer at the top of a tower will observe that clocks at ground level tick slower, and observers on the ground will agree about the direction and the ratio of the difference. There is still some disagreement in a sense, because all the observers believe their own local clocks are correct, but the direction and ratio of gravitational time dilation is agreed by all observers, independent of their altitude.
Science fiction implications.
Science fiction enthusiasts have noted the implications time dilation has on forward time travel, technically making it possible.The Hafele and Keating experiment involved flying planes around the world with atomic clocks on board. Upon the trips completion the clocks were compared to a static, ground based atomic clock. It was found that 273+/-7 nanoseconds had been gained on the planes' clocks. The current human time travel record is about 20 milliseconds for the cosmonaut Sergei Avdeyev.
Simple inference of time dilation due to relative velocity.
Time dilation can be inferred from the observed fact of the constancy of the speed of light in all reference frames.
This constancy of the speed of light means, counter to intuition, that speeds of material objects and light are not additive. It is not possible to make the speed of light appear greater by approaching at speed towards the material source that is emitting light. It is not possible to make the speed of light appear less by receding from the source at speed. From one point of view, it is the implications of this unexpected constancy that take away from constancies expected elsewhere.
Consider a simple clock consisting of two mirrors and , between which a light pulse is bouncing. The separation of the mirrors is and the clock ticks once each time the light pulse hits a given mirror.
In the frame where the clock is at rest (diagram at right), the light pulse traces out a path of length and the period of the clock is divided by the speed of light
From the frame of reference of a moving observer traveling at the speed relative to the rest frame of the clock (diagram at lower right), the light pulse traces out a "longer", angled path. The second postulate of special relativity states that the speed of light in free space is constant for all inertial observers, which implies a lengthening of the period of this clock from the moving observer's perspective. That is to say, in a frame moving relative to the clock, the clock appears to be running more slowly. Straightforward application of the Pythagorean theorem leads to the well-known prediction of special relativity:
The total time for the light pulse to trace its path is given by
The length of the half path can be calculated as a function of known quantities as
Substituting from this equation into the previous and solving for gives:
and thus, with the definition of :
which expresses the fact that for the moving observer the period of the clock is longer than in the frame of the clock itself.
Due to relative velocity symmetric between observers.
Common sense would dictate that if time passage has slowed for a moving object, the moving object would observe the external world to be correspondingly "sped up". Counterintuitively, special relativity predicts the opposite.
A similar oddity occurs in everyday life. If Sam sees Abigail at a distance she appears small to him and at the same time Sam appears small to Abigail. Being very familiar with the effects of perspective, we see no mystery or a hint of a paradox in this situation.
One is accustomed to the notion of relativity with respect to distance: the distance from Los Angeles to New York is by convention the same as the distance from New York to Los Angeles. On the other hand, when speeds are considered, one thinks of an object as "actually" moving, overlooking that its motion is always relative to something else – to the stars, the ground or to oneself. If one object is moving with respect to another, the latter is moving with respect to the former and with equal relative speed.
In the special theory of relativity, a moving clock is found to be ticking slowly with respect to the observer's clock. If Sam and Abigail are on different trains in near-lightspeed relative motion, Sam measures (by all methods of measurement) clocks on Abigail's train to be running slowly and similarly, Abigail measures clocks on Sam's train to be running slowly.
Note that in all such attempts to establish "synchronization" within the reference system, the question of whether something happening at one location is in fact happening simultaneously with something happening elsewhere, is of key importance. Calculations are ultimately based on determining which events are simultaneous. Furthermore, establishing simultaneity of events separated in space necessarily requires transmission of information between locations, which by itself is an indication that the speed of light will enter the determination of simultaneity.
It is a natural and legitimate question to ask how, in detail, special relativity can be self-consistent if clock C is time-dilated with respect to clock B and clock B is also time-dilated with respect to clock C. It is by challenging the assumptions built into the common notion of simultaneity that logical consistency can be restored. Simultaneity is a relationship between an observer in a particular frame of reference and a set of events. By analogy, left and right are accepted to vary with the position of the observer, because they apply to a relationship. In a similar vein, Plato explained that up and down describe a relationship to the earth and one would not fall off at the antipodes.
In relativity, temporal coordinate systems are set up using a procedure for synchronizing clocks. It is now usually called the "Poincaré-Einstein synchronization procedure". An observer with a clock sends a light signal out at time "t"1 according to his clock. At a distant event, that light signal is reflected back, and arrives back at the observer at time "t"2 according to his clock. Since the light travels the same path at the same rate going both out and back for the observer in this scenario, the coordinate time of the event of the light signal being reflected for the observer "t"E is . In this way, a single observer's clock can be used to define temporal coordinates which are good anywhere in the universe.
However, since those clocks are in motion in all other inertial frames, these clock indications are thus not synchronous in those frames, which is the basis of relativity of simultaneity. Because the pairs of putatively simultaneous moments are identified differently by different observers, each can treat the other clock as being the slow one without relativity being self-contradictory. Symmetric time dilation occurs with respect to coordinate systems set up in this manner. It is an effect where another clock is measured to run more slowly than one's own clock. Observers do not consider their own clock time to be affected, but may find that it is observed to be affected in another coordinate system.
Proper time and Minkowski diagram.
This symmetry can be demonstrated in a Minkowski diagram (second image on the right). Clock C resting in inertial frame S′ meets clock A at "d" and clock B at "f" (both resting in S). All three clocks simultaneously start to tick in S. The worldline of A is the ct-axis, the worldline of B intersecting "f" is parallel to the ct-axis, and the worldline of C is the ct′-axis. All events simultaneous with "d" in S are on the x-axis, in S′ on the x′-axis.
The proper time between two events is indicated by a clock present at both events. It is invariant, i.e., in all inertial frames it is agreed that this time is indicated by that clock. Interval "df" is therefore the proper time of clock C, and is shorter with respect to the coordinate times "ef=dg" of clocks B and A in S. Conversely, also proper time "ef" of B is shorter with respect to time "if" in S′, because event "e" was measured in S′ already at time "i" due to relativity of simultaneity, long before C started to tick.
From that it can be seen, that the proper time between two events indicated by an unaccelerated clock present at both events, compared with the synchronized coordinate time measured in all other inertial frames, is always the "minimal" time interval between those events. However, the interval between two events can also correspond to the proper time of accelerated clocks present at both events. Under all possible proper times between two events, the proper time of the unaccelerated clock is "maximal", which is the solution to the twin paradox.
Overview of formulae.
Time dilation due to relative velocity.
The formula for determining time dilation in special relativity is:
where Δ"t" is the time interval between "two co-local events" (i.e. happening at the same place) for an observer in some inertial frame (e.g. ticks on his clock), known as the "proper time", Δt′ is the time interval between those same events, as measured by another observer, inertially moving with velocity "v" with respect to the former observer, "v" is the relative velocity between the observer and the moving clock, "c" is the speed of light, and the Lorentz factor (conventionally denoted by the Greek letter gamma or γ) is
Thus the duration of the clock cycle of a moving clock is found to be increased: it is measured to be "running slow". The range of such variances in ordinary life, where even considering space travel, are not great enough to produce easily detectable time dilation effects and such vanishingly small effects can be safely ignored for most purposes. It is only when an object approaches speeds on the order of 30,000 km/s (1/10 the speed of light) that time dilation becomes important.
Time dilation by the Lorentz factor was predicted by Joseph Larmor (1897), at least for electrons orbiting a nucleus. Thus "... individual electrons describe corresponding parts of their orbits in times shorter for the [rest] system in the ratio :formula_8" (Larmor 1897). Time dilation of magnitude corresponding to this (Lorentz) factor has been experimentally confirmed, as described below.
Time dilation due to gravitation and motion together.
High accuracy time keeping, low earth orbit satellite tracking, and pulsar timing are applications that require the consideration of the combined effects of mass and motion in producing time dilation. Practical examples include the International Atomic Time standard and its relationship with the Barycentric Coordinate Time standard used for interplanetary objects.
Relativistic time dilation effects for the solar system and the earth can be modeled very precisely by the Schwarzschild solution to the Einstein field equations. In the Schwarzschild metric, the interval "dt"E is given by
where:
The coordinate time "t"c is the time that would be read on a hypothetical "coordinate clock" situated infinitely far from all gravitational masses (), and stationary in the system of coordinates (). The exact relation between the rate of proper time and the rate of coordinate time for a clock with a radial component of velocity is
Muon lifetime.
A comparison of muon lifetimes at different speeds is possible. In the laboratory, slow muons are produced, and in the atmosphere very fast moving muons are introduced by cosmic rays. Taking the muon lifetime at rest as the laboratory value of 2.197 μs, the lifetime of a cosmic ray produced muon traveling at 98% of the speed of light is about five times longer, in agreement with observations. In the muon storage ring at CERN the lifetime of muons circulating with γ = 29.327 was found to be dilated to 64.378 μs, confirming time dilation to an accuracy of 0.9 ± 0.4 parts per thousand. In this experiment the "clock" is the time taken by processes leading to muon decay, and these processes take place in the moving muon at its own "clock rate", which is much slower than the laboratory clock.
Space flight.
Time dilation would make it possible for passengers in a fast-moving vehicle to travel further into the future while aging very little, in that their great speed slows down the passage of on-board time relative to that of an observer. That is, the ship's clock (and according to relativity, any human traveling with it) shows less elapsed time than the clocks of observers on earth. For sufficiently high speeds the effect is dramatic. For example, one year of travel might correspond to ten years at home. Indeed, a constant 1 g acceleration would permit humans to travel through the entire known Universe in one human lifetime. The space travelers could return to Earth billions of years in the future. A scenario based on this idea was presented in the novel "Planet of the Apes" by Pierre Boulle.
A more likely use of this effect would be to enable humans to travel to nearby stars without spending their entire lives aboard a ship. However, any such application of time dilation during interstellar travel would require the use of some new, advanced method of propulsion. The Orion Project has been the only major attempt toward this idea.
Current space flight technology has fundamental theoretical limits based on the practical problem that an increasing amount of energy is required for propulsion as a craft approaches the speed of light. The likelihood of collision with small space debris and other particulate material is another practical limitation. At the velocities presently attained, however, time dilation occurs but is too small to be a factor in space travel. Travel to regions of spacetime where gravitational time dilation is taking place, such as within the gravitational field of a black hole but outside the event horizon (perhaps on a hyperbolic trajectory exiting the field), could also yield results consistent with present theory.
Time dilation at constant force.
In special relativity, time dilation is most simply described in circumstances where relative velocity is unchanging. Nevertheless, the Lorentz equations allow one to calculate proper time and movement in space for the simple case of a spaceship which is applied with a force per unit mass, relative to some reference object in uniform (i.e. constant velocity) motion, equal to "g" throughout the period of measurement.
Let "t" be the time in an inertial frame subsequently called the rest frame. Let "x" be a spatial coordinate, and let the direction of the constant acceleration as well as the spaceship's velocity (relative to the rest frame) be parallel to the "x"-axis. Assuming the spaceship's position at time being and the velocity being "v"0 and defining the following abbreviation
the following formulas hold:
Position:
Velocity:
Proper time:
In the case where "v"(0) = "v"0 = 0 and "τ"(0) = "τ"0 = 0 the integral can be expressed as a logarithmic function or, equivalently, as an inverse hyperbolic function:
Spacetime geometry of velocity time dilation.
The green dots and red dots in the animation represent spaceships. The ships of the green fleet have no velocity relative to each other, so for the clocks on board the individual ships the same amount of time elapses relative to each other, and they can set up a procedure to maintain a synchronized standard fleet time. The ships of the "red fleet" are moving with a velocity of 0.866"c" with respect to the green fleet.
The blue dots represent pulses of light. One cycle of light-pulses between two green ships takes two seconds of "green time", one second for each leg.
As seen from the perspective of the reds, the transit time of the light pulses they exchange among each other is one second of "red time" for each leg. As seen from the perspective of the greens, the red ships' cycle of exchanging light pulses travels a diagonal path that is two light-seconds long. (As seen from the green perspective the reds travel 1.73 (formula_17) light-seconds of distance for every two seconds of green time.)
One of the red ships emits a light pulse towards the greens every second of red time. These pulses are received by ships of the green fleet with two-second intervals as measured in green time. Not shown in the animation is that all aspects of physics are proportionally involved. The light pulses that are emitted by the reds at a particular frequency as measured in red time are received at a lower frequency as measured by the detectors of the green fleet that measure against green time, and vice versa.
The animation cycles between the green perspective and the red perspective, to emphasize the symmetry. As there is no such thing as absolute motion in relativity (as is also the case for Newtonian mechanics), both the green and the red fleet are entitled to consider themselves motionless "in their own frame of reference".
Again, it is vital to understand that the results of these interactions and calculations reflect the real state of the ships as it emerges from their situation of relative motion. It is not a mere quirk of the method of measurement or communication.

</doc>
<doc id="406624" url="https://en.wikipedia.org/wiki?curid=406624" title="Time series">
Time series

A time series is a sequence of data points, typically consisting of successive measurements made over a time interval. Examples of time series are ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average. Time series are very frequently plotted via line charts. Time series are used in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, intelligent transport and trajectory forecasting, earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and largely in any domain of applied science and engineering which involves temporal measurements.
Time series "analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series "forecasting is the use of a model to predict future values based on previously observed values. While regression analysis is often employed in such a way as to test theories that the current values of one or more independent time series affect the current value of another time series, this type of analysis of time series is not called "time series analysis", which focuses on comparing values of a single time series or multiple dependent time series at different points in time.
Time series data have a natural temporal ordering. This makes time series analysis distinct from cross-sectional studies, in which there is no natural ordering of the observations (e.g. explaining people's wages by reference to their respective education levels, where the individuals' data could be entered in any order). Time series analysis is also distinct from spatial data analysis where the observations typically relate to geographical locations (e.g. accounting for house prices by the location as well as the intrinsic characteristics of the houses). A stochastic model for a time series will generally reflect the fact that observations close together in time will be more closely related than observations further apart. In addition, time series models will often make use of the natural one-way ordering of time so that values for a given period will be expressed as deriving in some way from past values, rather than from future values (see time reversibility.)
Time series analysis can be applied to real-valued, continuous data, discrete numeric data, or discrete symbolic data (i.e. sequences of characters, such as letters and words in the English language.).
Methods for time series analyses.
Methods for time series analyses may be divided into two classes: frequency-domain methods and time-domain methods. The former include spectral analysis and recently wavelet analysis; the latter include auto-correlation and cross-correlation analysis. In time domain, correlation analyses can be made in a filter-like manner using scaled correlation, thereby mitigating the need to operate in frequency domain.
Additionally, time series analysis techniques may be divided into parametric and non-parametric methods. The parametric approaches assume that the underlying stationary stochastic process has a certain structure which can be described using a small number of parameters (for example, using an autoregressive or moving average model). In these approaches, the task is to estimate the parameters of the model that describes the stochastic process. By contrast, non-parametric approaches explicitly estimate the covariance or the spectrum of the process without assuming that the process has any particular structure.
Methods of time series analysis may also be divided into linear and non-linear, and univariate and multivariate.
Analysis.
There are several types of motivation and data analysis available for time series which are appropriate for different purposes.
Motivation.
In the context of statistics, econometrics, quantitative finance, seismology, meteorology, and geophysics the primary goal of time series analysis is forecasting. In the context of signal processing, control engineering and communication engineering it is used for signal detection and estimation, while in the context of data mining, pattern recognition and machine learning time series analysis can be used for clustering, classification, query by content, anomaly detection as well as forecasting.
Exploratory analysis.
The clearest way to examine a regular time series manually is with a line chart such as the one shown for tuberculosis in the United States, made with a spreadsheet program. The number of cases was standardized to a rate per 100,000 and the percent change per year in this rate was calculated. The nearly steadily dropping line shows that the TB incidence was decreasing in most years, but the percent change in this rate varied by as much as +/- 10%, with 'surges' in 1975 and around the early 1990s. The use of both vertical axes allows the comparison of two time series in one graphic.
Other techniques include:
Curve fitting.
Curve fitting is the process of constructing a curve, or mathematical function, that has the best fit to a series of data points, possibly subject to constraints. Curve fitting can involve either interpolation, where an exact fit to the data is required, or smoothing, in which a "smooth" function is constructed that approximately fits the data. A related topic is regression analysis, which focuses more on questions of statistical inference such as how much uncertainty is present in a curve that is fit to data observed with random errors. Fitted curves can be used as an aid for data visualization, to infer values of a function where no data are available, and to summarize the relationships among two or more variables. Extrapolation refers to the use of a fitted curve beyond the range of the observed data, and is subject to a degree of uncertainty since it may reflect the method used to construct the curve as much as it reflects the observed data.
The construction of economic time series involves the estimation of some components for some dates by interpolation between values ("benchmarks") for earlier and later dates. Interpolation is estimation of an unknown quantity between two known quantities (historical data), or drawing conclusions about missing information from the available information ("reading between the lines"). Interpolation is useful where the data surrounding the missing data is available and its trend, seasonality, and longer-term cycles are known. This is often done by using a related series known for all relevant dates. Alternatively polynomial interpolation or spline interpolation is used where piecewise polynomial functions are fit into time intervals such that they fit smoothly together. A different problem which is closely related to interpolation is the approximation of a complicated function by a simple function (also called regression).The main difference between regression and interpolation is that polynomial regression gives a single polynomial that models the entire data set. Spline interpolation, however, yield a piecewise continuous function composed of many polynomials to model the data set.
Extrapolation is the process of estimating, beyond the original observation range, the value of a variable on the basis of its relationship with another variable. It is similar to interpolation, which produces estimates between known observations, but extrapolation is subject to greater uncertainty and a higher risk of producing meaningless results.
Function Approximation.
In general, a function approximation problem asks us to select a function among a well-defined class that closely matches ("approximates") a target function in a task-specific way.
One can distinguish two major classes of function approximation problems: First, for known target functions approximation theory is the branch of numerical analysis that investigates how certain known functions (for example, special functions) can be approximated by a specific class of functions (for example, polynomials or rational functions) that often have desirable properties (inexpensive computation, continuity, integral and limit values, etc.).
Second, the target function, call it "g", may be unknown; instead of an explicit formula, only a set of points ( a time series) of the form ("x", "g"("x")) is provided. Depending on the structure of the domain and codomain of "g", several techniques for approximating "g" may be applicable. For example, if "g" is an operation on the real numbers, techniques of interpolation, extrapolation, regression analysis, and curve fitting can be used. If the codomain (range or target set) of "g" is a finite set, one is dealing with a classification problem instead. A related problem of "online" time series approximation is to summarize the data in one-pass and construct an approximate representation that can support a variety of time series queries with bounds on worst-case error.
To some extent the different problems (regression, classification, fitness approximation) have received a unified treatment in statistical learning theory, where they are viewed as supervised learning problems.
Prediction and forecasting.
In statistics, prediction is a part of statistical inference. One particular approach to such inference is known as predictive inference, but the prediction can be undertaken within any of the several approaches to statistical inference. Indeed, one description of statistics is that it provides a means of transferring knowledge about a sample of a population to the whole population, and to other related populations, which is not necessarily the same as prediction over time. When information is transferred across time, often to specific points in time, the process is known as forecasting.
"Also see: Statistical Prediction, Statistical Inference"
Classification.
Assigning time series pattern to a specific category, for example identify a word based on series of hand movements in sign language
Regression analysis (method of prediction).
Estimating future value of a signal based on its previous behavior, e.g. predict the price of AAPL stock based on its previous price movements for that hour, day or month, or predict position of Apollo 11 spacecraft at a certain future moment based on its current trajectory (i.e. time series of its previous locations).
Regression analysis is usually based on statistical interpretation of time series properties in time domain, pioneered by statisticians George Box and Gwilym Jenkins in the 1950s: see Box–Jenkins
Signal estimation.
This approach is based on harmonic analysis and filtering of signals in the frequency domain using the Fourier transform, and spectral density estimation, the development of which was significantly accelerated during World War II by mathematician Norbert Wiener, electrical engineers Rudolf E. Kálmán, Dennis Gabor and others for filtering signals from noise and predicting signal values at a certain point in time. See Kalman filter, Estimation theory, and digital signal processing
Segmentation.
Splitting a time-series into a sequence of segments. It is often the case that a time-series can be represented as a sequence of individual segments, each with its own characteristic properties. For example, the audio signal from a conference call can be partitioned into pieces corresponding to the times during which each person was speaking. In time-series segmentation, the goal is to identify the segment boundary points in the time-series, and to characterize the dynamical properties associated with each segment. One can approach this problem using change-point detection, or by modeling the time-series as a more sophisticated system, such as a Markov jump linear system.
Models.
Models for time series data can have many forms and represent different stochastic processes. When modeling variations in the level of a process, three broad classes of practical importance are the "autoregressive" (AR) models, the "integrated" (I) models, and the "moving average" (MA) models. These three classes depend linearly on previous data points. Combinations of these ideas produce autoregressive moving average (ARMA) and autoregressive integrated moving average (ARIMA) models. The autoregressive fractionally integrated moving average (ARFIMA) model generalizes the former three. Extensions of these classes to deal with vector-valued data are available under the heading of multivariate time-series models and sometimes the preceding acronyms are extended by including an initial "V" for "vector", as in VAR for vector autoregression. An additional set of extensions of these models is available for use where the observed time-series is driven by some "forcing" time-series (which may not have a causal effect on the observed series): the distinction from the multivariate case is that the forcing series may be deterministic or under the experimenter's control. For these models, the acronyms are extended with a final "X" for "exogenous".
Non-linear dependence of the level of a series on previous data points is of interest, partly because of the possibility of producing a chaotic time series. However, more importantly, empirical investigations can indicate the advantage of using predictions derived from non-linear models, over those from linear models, as for example in nonlinear autoregressive exogenous models. Further references on nonlinear time series analysis: (Kantz and Schreiber), and (Abarbanel) 
Among other types of non-linear time series models, there are models to represent the changes of variance over time (heteroskedasticity). These models represent autoregressive conditional heteroskedasticity (ARCH) and the collection comprises a wide variety of representation (GARCH, TARCH, EGARCH, FIGARCH, CGARCH, etc.). Here changes in variability are related to, or predicted by, recent past values of the observed series. This is in contrast to other possible representations of locally varying variability, where the variability might be modelled as being driven by a separate time-varying process, as in a doubly stochastic model.
In recent work on model-free analyses, wavelet transform based methods (for example locally stationary wavelets and wavelet decomposed neural networks) have gained favor. Multiscale (often referred to as multiresolution) techniques decompose a given time series, attempting to illustrate time dependence at multiple scales. See also Markov switching multifractal (MSMF) techniques for modeling volatility evolution.
A Hidden Markov model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (hidden) states. An HMM can be considered as the simplest dynamic Bayesian network. HMM models are widely used in speech recognition, for translating a time series of spoken words into text.
Notation.
A number of different notations are in use for time-series analysis. A common notation specifying a time series "X" that is indexed by the natural numbers is written
Another common notation is
where "T" is the index set.
Conditions.
There are two sets of conditions under which much of the theory is built:
However, ideas of stationarity must be expanded to consider two important ideas: strict stationarity and second-order stationarity. Both models and applications can be developed under each of these conditions, although the models in the latter case might be considered as only partly specified.
In addition, time-series analysis can be applied where the series are seasonally stationary or non-stationary. Situations where the amplitudes of frequency components change with time can be dealt with in time-frequency analysis which makes use of a time–frequency representation of a time-series or signal.
Models.
The general representation of an autoregressive model, well known as AR("p"), is
where the term ε"t" is the source of randomness and is called white noise. It is assumed to have the following characteristics:
With these assumptions, the process is specified up to second-order moments and, subject to conditions on the coefficients, may be second-order stationary.
If the noise also has a normal distribution, it is called normal or Gaussian white noise. In this case, the AR process may be strictly stationary, again subject to conditions on the coefficients.
Tools for investigating time-series data include:
Measures.
Time series metrics or features that can be used for time series classification or regression analysis:
Visualization.
Time series can be visualized with two categories of chart:Overlapping Charts and Separated Charts. Overlapping Charts display all time series on the same layout while Separated Charts presents them on different layouts (but aligned for comparison purpose)
Applications.
Fractal geometry, using a deterministic Cantor structure, is used to model the surface topography, where recent advancements in thermoviscoelastic creep contact of rough surfaces are introduced. Various viscoelastic idealizations are used to model the surface materials, for example, Maxwell, Kelvin-Voigt, Standard Linear Solid and Jeffrey media. Asymptotic power laws, through hypergeometric series, were used to express the surface creep as a function of remote forces, body temperatures and time.

</doc>
<doc id="1291319" url="https://en.wikipedia.org/wiki?curid=1291319" title="Time-invariant system">
Time-invariant system

A time-invariant (TIV) system is a system whose output does not depend explicitly on time. Such systems are regarded as a class of systems in the field of system analysis. Lack of time dependence is captured in the following mathematical property of such a system: 
This property can be satisfied if the transfer function of the system is not a function of time except expressed by the input and output.
In the context of a system schematic, this property can also be stated as follows:
If a time-invariant system is also linear, it is the subject of LTI system theory (linear time-invariant) with direct applications in NMR spectroscopy, seismology, circuits, signal processing, control theory, and other technical areas. Nonlinear time-invariant systems lack a comprehensive, governing theory. Discrete time-invariant systems are known as shift-invariant systems. Systems which lack the time-invariant property are studied as time-variant systems.
Simple example.
To demonstrate how to determine if a system is time-invariant, consider the two systems:
Since system A explicitly depends on "t" outside of formula_1 and formula_2, it is not time-invariant. System B, however, does not depend explicitly on "t" so it is time-invariant.
Formal example.
A more formal proof of why systems A and B above differ is now presented.
To perform this proof, the second definition will be used.
System A:
System B:
Abstract example.
We can denote the shift operator by formula_23 where formula_24 is the amount by which a vector's index set should be shifted. For example, the "advance-by-1" system
can be represented in this abstract notation by
where formula_27 is a function given by
with the system yielding the shifted output
So formula_30 is an operator that advances the input vector by 1.
Suppose we represent a system by an operator formula_31. This system is time-invariant if it commutes with the shift operator, i.e.,
If our system equation is given by
then it is time-invariant if we can apply the system operator formula_31 on formula_27 followed by the shift operator formula_23, or we can apply the shift operator formula_23 followed by the system operator formula_31, with the two computations yielding equivalent results.
Applying the system operator first gives
Applying the shift operator first gives
If the system is time-invariant, then

</doc>
<doc id="2470776" url="https://en.wikipedia.org/wiki?curid=2470776" title="Timeline of particle discoveries">
Timeline of particle discoveries

This is a timeline of subatomic particle discoveries, including all particles thus far discovered which appear to be elementary (that is, indivisible) given the best available evidence. It also includes the discovery of composite particles and antiparticles that were of particular historical importance.
More specifically, the inclusion criteria are:

</doc>
<doc id="241029" url="https://en.wikipedia.org/wiki?curid=241029" title="Top quark">
Top quark

The top quark, also known as the t quark (symbol: t) or truth quark, is an elementary particle and a fundamental constituent of matter. Like all quarks, the top quark is an elementary fermion with spin-, and experiences all four fundamental interactions: gravitation, electromagnetism, weak interactions, and strong interactions. It has an electric charge of + "e", and is the most massive of all observed elementary particles. It has a mass of 173.34 ± 0.27 (stat) ± 0.71 (syst), which is about the same mass as an atom of tungsten. The antiparticle of the top quark is the top antiquark (symbol: , sometimes called "antitop quark" or simply "antitop"), which differs from it only in that some of its properties have equal magnitude but opposite sign.
The top quark interacts primarily by the strong interaction, but can only decay through the weak force. It decays to a W boson and either a bottom quark (most frequently), a strange quark, or, on the rarest of occasions, a down quark. The Standard Model predicts its mean lifetime to be roughly . This is about a twentieth of the timescale for strong interactions, and therefore it does not form hadrons, giving physicists a unique opportunity to study a "bare" quark (all other quarks hadronize, meaning that they combine with other quarks to form hadrons, and can only be observed as such). Because it is so massive, the properties of the top quark allow predictions to be made of the mass of the Higgs boson under certain extensions of the Standard Model (see Mass and coupling to the Higgs boson below). As such, it is extensively studied as a means to discriminate between competing theories.
Its existence (and that of the bottom quark) was postulated in 1973 by Makoto Kobayashi and Toshihide Maskawa to explain the observed CP violations in kaon decay, and was discovered in 1995 by the CDF and DØ experiments at Fermilab. Kobayashi and Maskawa won the 2008 Nobel Prize in Physics for the prediction of the top and bottom quark, which together form the third generation of quarks.
History.
In 1973, Makoto Kobayashi and Toshihide Maskawa predicted the existence of a third generation of quarks to explain observed CP violations in kaon decay. The names top and bottom were introduced by Haim Harari in 1975,
to match the names of the first generation of quarks (up and down) reflecting the fact that the two were the 'up' and 'down' component of a weak isospin doublet. The top quark was sometimes called "truth quark" in the past, but over time "top quark" became the predominant use.
The proposal of Kobayashi and Maskawa heavily relied on the GIM mechanism put forward by Sheldon Lee Glashow, John Iliopoulos and Luciano Maiani, which predicted the existence of the then still unobserved charm quark. When in November 1974 teams at Brookhaven National Laboratory (BNL) and the Stanford Linear Accelerator Center (SLAC) simultaneously announced the discovery of the J/ψ meson, it was soon after identified as a bound state of the missing charm quark with its antiquark. This discovery allowed the GIM mechanism to become part of the Standard Model. With the acceptance of the GIM mechanism, Kobayashi and Maskawa's prediction also gained in credibility. Their case was further strengthened by the discovery of the tau by Martin Lewis Perl's team at SLAC between 1974 and 1978. This announced a third generation of leptons, breaking the new symmetry between leptons and quarks introduced by the GIM mechanism. Restoration of the symmetry implied the existence of a fifth and sixth quark.
It was in fact not long until a fifth quark, the bottom, was discovered by the E288 experiment team, led by Leon Lederman at Fermilab in 1977. This strongly suggested that there must also be a sixth quark, the top, to complete the pair. It was known that this quark would be heavier than the bottom, requiring more energy to create in particle collisions, but the general expectation was that the sixth quark would soon be found. However, it took another 18 years before the existence of the top was confirmed.
Early searches for the top quark at SLAC and DESY (in Hamburg) came up empty-handed. When, in the early eighties, the Super Proton Synchrotron (SPS) at CERN discovered the W boson and the Z boson, it was again felt that the discovery of the top was imminent. As the SPS gained competition from the Tevatron at Fermilab there was still no sign of the missing particle, and it was announced by the group at CERN that the top mass must be at least . After a race between CERN and Fermilab to discover the top, the accelerator at CERN reached its limits without creating a single top, pushing the lower bound on its mass up to .
The Tevatron was (until the start of LHC operation at CERN in 2009) the only hadron collider powerful enough to produce top quarks. In order to be able to confirm a future discovery, a second detector, the DØ detector, was added to the complex (in addition to the Collider Detector at Fermilab (CDF) already present). In October 1992, the two groups found their first hint of the top, with a single creation event that appeared to contain the top. In the following years, more evidence was collected and on April 22, 1994, the CDF group submitted their paper presenting tentative evidence for the existence of a top quark with a mass of about . In the meantime, DØ had found no more evidence than the suggestive event in 1992. A year later, on March 2, 1995, after having gathered more evidence and a reanalysis of the DØ data (who had been searching for a much lighter top), the two groups jointly reported the discovery of the top with a certainty of 99.9998% at a mass of .
In the years leading up to the top quark discovery, it was realized that certain precision measurements of the electroweak vector boson masses and couplings are very sensitive to the value of the top quark mass. These effects become much larger for higher values of the top mass and therefore could indirectly see the top quark even if it could not be directly detected in any experiment at the time. The largest effect from the top quark mass was on the T parameter and by 1994 the precision of these indirect measurements had led to a prediction of the top quark mass to be between and .
 The Discovery of the Top Quark Finding the sixth quark involved the world’s most energetic collisions and a cast of thousands
by Tony M. Liss and Paul L. TiptonIt is the development of techniques such as the (VIOLENT COLLISION between a proton and
an antiproton (center) creates a top quark (red) and an antitop (blue). These decay to other particles, typically producing a number of jets
and possibly an electron or positron) that ultimately allowed such precision calculations that led to Gerardus 't Hooft and Martinus Veltman winning the Nobel Prize in physics in 1999.
Production.
Because top quarks are very massive, large amounts of energy are needed to create one. The only way to achieve such high energies is through high energy collisions. These occur naturally in the Earth's upper atmosphere as cosmic rays collide with particles in the air, or can be created in a particle accelerator. In 2011, after the Tevatron ceased operations, the Large Hadron Collider at CERN became the only accelerator that generates a beam of sufficient energy to produce top quarks, with a center-of-mass energy of 7 TeV.
There are multiple processes that can lead to the production of a top quark. The most common is production of a top–antitop pair via strong interactions. In a collision, a highly energetic gluon is created, which subsequently decays into a top and antitop. This process was responsible for the majority of the top events at Tevatron and was the process observed when the top was first discovered in 1995. It is also possible to produce pairs of top–antitop through the decay of an intermediate photon or Z-boson. However, these processes are predicted to be much rarer and have a virtually identical experimental signature in a hadron collider like Tevatron.
A distinctly different process is the production of single tops via weak interaction. This can happen in two ways (called channels): either an intermediate W-boson decays into a top and antibottom quark ("s-channel") or a bottom quark (probably created in a pair through the decay of a gluon) transforms to top quark by exchanging a W-boson with an up or down quark ("t-channel"). The first evidence for these processes was published by the DØ collaboration in December 2006, and in March 2009 the CDF and DØ collaborations released twin papers with the definitive observation of these processes. The main significance of measuring these production processes is that their frequency is directly proportional to the  component of the CKM matrix.
Decay.
The only known way that a top quark can decay is through the weak interaction producing a W-boson and a down-type quark (down, strange, or bottom). Because of its enormous mass, the top quark is extremely short-lived with a predicted lifetime of only . As a result top quarks do not have time to form hadrons before they decay, as other quarks do. This provides physicists with the unique opportunity to study the behavior of a "bare" quark.
In particular, it is possible to directly determine the branching ratio Γ(W+b) / Γ(W+"q" ("q" = b,s,d)). The best current determination of this ratio is . Since this ratio is equal to according to the Standard Model, this gives another way of determining the CKM element , or in combination with the determination of from single top production provides tests for the assumption that the CKM matrix is unitary.
The Standard Model also allows more exotic decays, but only at one loop level, meaning that they are extremely suppressed. In particular, it is possible for a top quark to decay into another up-type quark (an up or a charm) by emitting a photon or a Z-boson. Searches for these exotic decay modes have provided no evidence for their existence in accordance with expectations from the Standard Model. The branching ratios for these decays have been determined to be less than 5.9 in 1,000 for photonic decay and less than 2.1 in 1,000 for Z-boson decay at 95% confidence.
Mass and coupling to the Higgs boson.
The Standard Model describes fermion masses through the Higgs mechanism. The Higgs boson has a Yukawa coupling to the left- and right-handed top quarks. After electroweak symmetry breaking (when the Higgs acquires a vacuum expectation value), the left- and right-handed components mix, becoming a mass term.
The top quark Yukawa coupling has a value of
where 246 GeV is the value of the Higgs vacuum expectation value.
Yukawa couplings.
In the Standard Model, all of the quark and lepton Yukawa couplings are small compared to the top quark Yukawa coupling. Understanding this hierarchy in the fermion masses is an open problem in theoretical physics. Yukawa couplings are not constants and their values change depending on the energy scale (distance scale) at which they are measured. The dynamics of Yukawa couplings are determined by the renormalization group equation.
One of the prevailing views in particle physics is that the size of the top quark Yukawa coupling is determined by the renormalization group, leading to the "quasi-infrared fixed point."
The Yukawa couplings of the up, down, charm, strange and bottom quarks, are hypothesized to have small values at the extremely high energy scale of grand unification, 1015 GeV. They increase in value at lower energy scales, at which the quark masses are generated by the Higgs. The slight growth is due to corrections from the QCD coupling. The corrections from the Yukawa couplings are negligible for the lower mass quarks.
If, however, a quark Yukawa coupling has a large value at very high energies, its Yukawa corrections will evolve and cancel against the QCD corrections. This is known as a (quasi-) infrared fixed point. No matter what the initial starting value of the coupling is, if it is sufficiently large it will reach this fixed point value. The corresponding quark mass is then predicted.
The top quark Yukawa coupling lies very near the infrared fixed point of the Standard Model. The renormalization group equation is:
where is the color gauge coupling, is the weak isospin gauge coupling, and is the weak hypercharge gauge coupling. This equation describes how the Yukawa coupling changes with energy scale . Solutions to this equation for large initial values cause the right-hand side of the equation to quickly approach zero, locking to the QCD coupling . The value of the fixed point is fairly precisely determined in the Standard Model, leading to a top quark mass of 230 GeV. However, if there is more than one Higgs doublet, the mass value will be reduced by Higgs mixing angle effects in an unpredicted way.
In the minimal supersymmetric extension of the Standard Model (MSSM), there are two Higgs doublets and the renormalization group equation for the top quark Yukawa coupling is slightly modified:
where "y"b is the bottom quark Yukawa coupling. This leads to a fixed point where the top mass is smaller, 170–200 GeV. The uncertainty in this prediction arises because the bottom quark Yukawa coupling can be amplified in the MSSM. Some theorists believe this is supporting evidence for the MSSM.
The quasi-infrared fixed point has subsequently formed the basis of top quark condensation theories of electroweak symmetry breaking in which the Higgs boson is composite at "extremely" short distance scales, composed of a pair of top and antitop quarks.

</doc>
<doc id="1468744" url="https://en.wikipedia.org/wiki?curid=1468744" title="Top quark condensate">
Top quark condensate

In particle physics, the top quark condensate theory is an alternative to the Standard Model fundamental Higgs field, replaced by a composite field of the top quark and its antiquark. These are bound together by a new force, analogous to the binding of Cooper pairs in a BCS superconductor, or mesons in the strong interactions. The top quark can "condense" because it is comparatively heavy, with a measured mass is approximately 173 GeV (comparable to the electroweak scale), and so its Yukawa coupling is of order unity, yielding the possibility of strong coupling dynamics.
The top and antitop quarks form a bound state that is a composite Higgs boson field. This model predicts how the electroweak scale may match the top quark mass. The idea was first described by Yoichiro Nambu and subsequently by Vladimir Miransky, Masaharu Tanabashi, and Koichi Yamawaki (Is the t Quark Responsible for the Mass of W and Z Bosons?) and developed into a predictive framework, based upon the renormalization group, by William A. Bardeen, Christopher T. Hill, and Manfred Lindner in the article Minimal Dynamical Symmetry Breaking of the Standard Model. Top quark condensation is essentially based upon the "quasi-infrared fixed point" for the top quark Higgs-Yukawa coupling, proposed in 1981 by Hill in the paper Quark and Lepton Masses from Renormalization Group Fixed Points. The simplest top condensation models predicted that the Higgs boson mass would be larger than the 175 GeV top quark mass, and have now been ruled out by the LHC discovery of the Higgs boson at a mass scale of 125 GeV. 
More complex schemes may still be viable. Top condensation arises naturally in Topcolor models, that are extensions of the standard model in analogy to quantum chromodynamics. To be natural, without excessive fine-tuning (i.e. to stabilize the Higgs mass from large radiative corrections), the theory requires new physics at a relatively low energy scale. Placing new physics at 10 TeV, for instance, the model predicts the top quark to be significantly heavier than observed (at about 600 GeV vs. 171 GeV). "Top Seesaw" models, also based upon Topcolor, circumvent this difficulty. These theories will ultimately be tested at the LHC in its Run-II commencing in 2015.

</doc>
<doc id="1624397" url="https://en.wikipedia.org/wiki?curid=1624397" title="Topness">
Topness

Topness (also called truth), a flavour quantum number, represents the difference between the number of top quarks (t) and number of top antiquarks () that are present in a particle:
By convention, top quarks have a topness of +1 and top antiquarks have a topness of −1.The term "topness" is rarely used; most physicists simply refer to "the number of top quarks" and "the number of top antiquarks".
Conservation.
Like all flavour quantum numbers, topness is preserved under strong and electromagnetic interactions, but not under weak interaction. However the top quark is extremely unstable, with a half-life under 10−23 s, which is the required time for the strong interaction to take place. For that reason the top quark does not hadronize, that is it never forms any meson or baryon, so the topness of a meson or a baryon is every time equal at zero. By the time it can interact strongly it has already decayed to another flavour of quark (usually to a bottom quark).

</doc>
<doc id="42315" url="https://en.wikipedia.org/wiki?curid=42315" title="Topological group">
Topological group

In mathematics, a topological group is a group "G" together with a topology on "G" such that the group's binary operation and the group's inverse function are continuous functions with respect to the topology. A topological group is a mathematical object with both an algebraic structure and a topological structure. Thus, one may perform algebraic operations, because of the group structure, and one may talk about continuous functions, because of the topology.
Topological groups, along with continuous group actions, are used to study continuous symmetries, which have many applications, for example in physics.
Formal definition.
A topological group "G" is a topological space and group such that the group operations of product:
and taking inverses:
are continuous functions. Here, "G" × "G" is viewed as a topological space by using the product topology.
Although not part of this definition, many authors require that the topology on "G" be Hausdorff; this corresponds to the identity map formula_3 being a closed inclusion (hence also a cofibration). The reasons, and some equivalent conditions, are discussed below. In the end, this is not a serious restriction—any topological group can be made Hausdorff in a canonical fashion.
In the language of category theory, topological groups can be defined concisely as group objects in the category of topological spaces, in the same way that ordinary groups are group objects in the category of sets. Note that the axioms are given in terms of the maps (binary product, unary inverse, and nullary identity), hence are categorical definitions. Adding the further requirement of Hausdorff (and cofibration) corresponds to refining to a model category.
Homomorphisms.
A homomorphism between two topological groups "G" and "H" is just a continuous group homomorphism "G" formula_4 "H". An isomorphism of topological groups is a group isomorphism which is also a homeomorphism of the underlying topological spaces. This is stronger than simply requiring a continuous group isomorphism—the inverse must also be continuous. There are examples of topological groups which are isomorphic as ordinary groups but not as topological groups. Indeed, any nondiscrete topological group is also a topological group when considered with the discrete topology. The underlying groups
are the same, but as topological groups there is not an isomorphism.
Topological groups, together with their homomorphisms, form a category.
Examples.
Every group can be trivially made into a topological group by considering it with the discrete topology; such groups are called discrete groups. In this sense, the theory of topological groups subsumes that of ordinary groups.
The real numbers R, together with addition as operation and its usual topology, form a topological group. More generally, Euclidean "n"-space R"n" with addition and standard topology is a topological group. More generally yet, the additive groups of all topological vector spaces, such as Banach spaces or Hilbert spaces, are topological groups.
The above examples are all abelian. Examples of non-abelian topological groups are given by the classical groups. For instance, the general linear group GL("n",R) of all invertible "n"-by-"n" matrices with real entries can be viewed as a topological group with the topology defined by viewing GL("n",R) as a subset of Euclidean space R"n"×"n".
An example of a topological group which is not a Lie group is given by the rational numbers Q with the topology inherited from R. This is a countable space and it does not have the discrete topology. For a nonabelian example, consider the subgroup of rotations of R3 generated by two rotations by irrational multiples of 2π about different axes.
In every Banach algebra with multiplicative identity, the set of invertible elements forms a topological group under multiplication.
Properties.
The algebraic and topological structures of a topological group interact in non-trivial ways. For example, in any topological group the identity component (i.e. the connected component containing the identity element) is a closed normal subgroup. This is because if "C" is the identity component, "a*C" is the component of "G" (the group) containing a. In fact, the collection of all left cosets (or right cosets) of "C" in "G" is equal to the collection of all components of "G". Therefore, the quotient topology induced by the quotient map from "G" to "G"/"C" is totally disconnected.
The inversion operation on a topological group "G" is a homeomorphism from "G" to itself. Likewise, if "a" is any element of "G", then left or right multiplication by "a" yields a homeomorphism "G" → "G".
Every topological group can be viewed as a uniform space in two ways; the "left uniformity" turns all left multiplications into uniformly continuous maps while the "right uniformity" turns all right multiplications into uniformly continuous maps. If "G" is not abelian, then these two need not coincide. The uniform structures allow one to talk about notions such as completeness, uniform continuity and uniform convergence on topological groups.
As a uniform space, every topological group is completely regular. It follows that if a topological group is T0 (Kolmogorov) then it is already T2 (Hausdorff), even T3½ (Tychonoff).
Every subgroup of a topological group is itself a topological group when given the subspace topology. If "H" is a subgroup of "G", the set of left or right cosets "G"/"H" is a topological space when given the quotient topology (the finest topology on "G"/"H" which makes the natural projection "q" : "G" → "G"/"H" continuous). One can show that the quotient map "q" : "G" → "G"/"H" is always open.
Every open subgroup "H" is also closed, since the complement of "H" is the open set given by the union of open sets "gH" for "g" in G \ H.
If "H" is a normal subgroup of "G", then the factor group, "G"/"H" becomes a topological group when given the quotient topology. However, if "H" is not closed in the topology of "G", then "G"/"H" will not be T0 even if "G" is. It is therefore natural to restrict oneself to the category of T0 topological groups, and restrict the definition of "normal" to "normal and closed".
The isomorphism theorems known from ordinary group theory are not always true in the topological setting. This is because a bijective homomorphism need not be an isomorphism of topological groups. The theorems are valid if one places certain restrictions on the maps involved. For example, the first isomorphism theorem states that if "f" : "G" → "H" is a homomorphism then "G"/ker("f") is isomorphic to im("f") if and only if the map "f" is open onto its image.
If "H" is a subgroup of "G" then the closure of "H" is also a subgroup. Likewise, if "H" is a normal subgroup, the closure of "H" is normal.
A topological group "G" is Hausdorff if and only if the trivial one-element subgroup is closed in "G". If "G" is not Hausdorff then one can obtain a Hausdorff group by passing to the quotient space "G"/"K" where "K" is the closure of the identity. This is equivalent to taking the Kolmogorov quotient of "G".
The fundamental group of a topological group is always abelian. This is a special case of the fact that the fundamental group of an H-space is abelian, since topological groups are H-spaces.
Relationship to other areas of mathematics.
A compact group is a topological group whose topology is compact. Compact groups are a natural generalisation of finite groups with the discrete topology and have properties that carry over in significant fashion. Compact groups have a well-understood theory, in relation to group actions and representation theory.
Of particular importance in harmonic analysis are the locally compact groups, because they admit a natural notion of measure and integral, given by the Haar measure. The theory of group representations is almost identical for finite groups and for compact topological groups. In general, σ-compact Baire topological groups are locally compact.
In topology, the homeomorphism group of a topological space is the group consisting of all homeomorphisms from the space to itself with function composition as the group operation. The homeomorphism group can be given a topology, such as the compact-open topology (in the case of regular, locally compact spaces), making it into a topological group.
Generalizations.
Various generalizations of topological groups can be obtained by weakening the continuity conditions:

</doc>
<doc id="2119193" url="https://en.wikipedia.org/wiki?curid=2119193" title="Topological manifold">
Topological manifold

In topology, a branch of mathematics, a topological manifold is a topological space (which may also be a separated space) which locally resembles real "n"-dimensional space in a sense defined below. Topological manifolds form an important class of topological spaces with applications throughout mathematics.
A "manifold" can mean a topological manifold, or more frequently, a topological manifold together with some additional structure. Differentiable manifolds, for example, are topological manifolds equipped with a differential structure. Every manifold has an underlying topological manifold, obtained simply by forgetting the additional structure. An overview of the manifold concept is given in that article. This article focuses purely on the topological aspects of manifolds.
Formal definition.
A topological space "X" is called locally Euclidean if there is a non-negative integer "n" such that every point in "X" has a neighborhood which is homeomorphic to the Euclidean space E"n" (or, equivalently, to the real "n"-space R"n", or to some connected open subset of either of two).
A topological manifold is a locally Euclidean Hausdorff space. It is common to place additional requirements on topological manifolds. In particular, many authors define them to be paracompact or second-countable. The reasons, and some equivalent conditions, are discussed below.
In the remainder of this article a "manifold" will mean a topological manifold. An "n-manifold" will mean a topological manifold such that every point has a neighborhood homeomorphic to R"n".
Examples.
"See also": List of manifolds
Properties.
The property of being locally Euclidean is preserved by local homeomorphisms. That is, if "X" is locally Euclidean of dimension "n" and "f" : "Y" → "X" is a local homeomorphism, then "Y" is locally Euclidean of dimension "n". In particular, being locally Euclidean is a topological property.
Manifolds inherit many of the local properties of Euclidean space. In particular, they are locally compact, locally connected, first countable, locally contractible, and locally metrizable. Being locally compact Hausdorff spaces, manifolds are necessarily Tychonoff spaces.
Adding the Hausdorff condition can make several properties become equivalent for a manifold. As an example, we can show that for a Hausdorff manifold, the notions of σ-compactness and second-countability are the same. Indeed, a Hausdorff manifold is a locally compact Hausdorff space, hence it is (completely) regular . Assume such a space X is σ-compact. Then it is Lindelöf, and because Lindelöf + regular implies paracompact, X is metrizable. But in a metrizable space, second-countability coincides with being Lindelöf, so X is second-countable. Conversely, if X is a Hausdorff second-countable manifold, it must be σ-compact .
A manifold need not be connected, but every manifold "M" is a disjoint union of connected manifolds. These are just the connected components of "M", which are open sets since manifolds are locally-connected. Being locally path connected, a manifold is path-connected if and only if it is connected. It follows that the path-components are the same as the components.
The Hausdorff axiom.
The Hausdorff property is not a local one; so even though Euclidean space is Hausdorff, a locally Euclidean space need not be. It is true, however, that every locally Euclidean space is T1.
An example of a non-Hausdorff locally Euclidean space is the line with two origins. This space is created by replacing the origin of the real line with "two" points, an open neighborhood of either of which includes all nonzero numbers in some open interval centered at zero. This space is not Hausdorff because the two origins cannot be separated.
Compactness and countability axioms.
A manifold is metrizable if and only if it is paracompact. Since metrizability is such a desirable property for a topological space, it is common to add paracompactness to the definition of a manifold. In any case, non-paracompact manifolds are generally regarded as pathological. An example of a non-paracompact manifold is given by the long line. Paracompact manifolds have all the topological properties of metric spaces. In particular, they are perfectly normal Hausdorff spaces.
Manifolds are also commonly required to be second-countable. This is precisely the condition required to ensure that the manifold embeds in some finite-dimensional Euclidean space. For any manifold the properties of being second-countable, Lindelöf, and σ-compact are all equivalent.
Every second-countable manifold is paracompact, but not vice versa. However, the converse is nearly true: a paracompact manifold is second-countable if and only if it has a countable number of connected components. In particular, a connected manifold is paracompact if and only if it is second-countable. 
Every second-countable manifold is separable and paracompact. Moreover, if a manifold is separable and paracompact then it is also second-countable.
Every compact manifold is second-countable and paracompact.
Dimensionality.
By invariance of domain, a non-empty "n"-manifold cannot be an "m"-manifold for "n" ≠ "m". The dimension of a non-empty "n"-manifold is "n".
Being an "n"-manifold is a topological property, meaning that any topological space homeomorphic to an "n"-manifold is also an "n"-manifold.
A 1-dimensional manifold is often called a curve while a 2-dimensional manifold is called a surface. Higher-dimensional manifolds are usually just called "n"-manifolds. For "n" = 3, 4, or 5 see 3-manifold, 4-manifold, and 5-manifold.
Coordinate charts.
By definition, every point of a locally Euclidean space has a neighborhood homeomorphic to an open subset of R"n". Such neighborhoods are called Euclidean neighborhoods. It follows from invariance of domain that Euclidean neighborhoods are always open sets. One can always find Euclidean neighborhoods that are homeomorphic to "nice" open sets in R"n". Indeed, a space "M" is locally Euclidean if and only if either of the following equivalent conditions holds:
A Euclidean neighborhood homeomorphic to an open ball in R"n" is called a Euclidean ball. Euclidean balls form a basis for the topology of a locally Euclidean space.
For any Euclidean neighborhood "U" a homeomorphism φ : "U" → φ("U") ⊂ R"n" is called a coordinate chart on "U" (although the word "chart" is frequently used to refer to the domain or range of such a map). A space "M" is locally Euclidean if and only if it can be covered by Euclidean neighborhoods. A set of Euclidean neighborhoods that cover "M", together with their coordinate charts, is called an atlas on "M". (The terminology comes from an analogy with cartography whereby a spherical globe can be described by an atlas of flat maps or charts).
Given two charts φ and ψ with overlapping domains "U" and "V" there is a transition function
Such a map is a homeomorphism between open subsets of R"n". That is, coordinate charts agree on overlaps up to homeomorphism. Different types of manifolds can be defined by placing restrictions on types of transition maps allowed. For example, for differentiable manifolds the transition maps are required to be diffeomorphisms.
Classification of manifolds.
A 0-manifold is just a discrete space. Such spaces are classified by their cardinality. Every discrete space is paracompact. A discrete space is second-countable if and only if it is countable.
Every nonempty, paracompact, connected 1-manifold is homeomorphic either to R or the circle. The unconnected ones are just disjoint unions of these.
Every nonempty, compact, connected 2-manifold (or surface) is homeomorphic to the sphere, a connected sum of tori, or a connected sum of projective planes. See the classification theorem for surfaces for more details.
A classification of 3-manifolds results from
Thurston's geometrization conjecture, proven by Grigori Perelman.
The full classification of "n"-manifolds for "n" greater than three is known to be impossible; it is at least as hard as the word problem in group theory, which is known to be algorithmically undecidable. In fact, there is no algorithm for deciding whether a given manifold is simply connected. There is, however, a classification of simply connected manifolds of dimension ≥ 5.
Manifolds with boundary.
A slightly more general concept is sometimes useful. A topological manifold with boundary is a Hausdorff space in which every point has a neighborhood homeomorphic to an open subset of Euclidean half-space (for a fixed "n"):
The terminology is somewhat confusing: every topological manifold is a topological manifold with boundary, but not vice versa.

</doc>
<doc id="29954" url="https://en.wikipedia.org/wiki?curid=29954" title="Topology">
Topology

In mathematics, topology (from the Greek τόπος, "place", and λόγος, "study"), is the study of a collection of open sets, making a given set a topological space. It is an area of mathematics concerned with the properties of space that are preserved under continuous deformations, such as stretching and bending, but not tearing or gluing. Important topological properties include connectedness and compactness.
Topology developed as a field of study out of geometry and set theory, through analysis of such concepts as space, dimension, and transformation. Such ideas go back to Gottfried Leibniz, who in the 17th century envisioned the "geometria situs" (Greek-Latin for "geometry of place") and "analysis situs" (Greek-Latin for "picking apart of place"). Leonhard Euler's Seven Bridges of Königsberg Problem and Polyhedron Formula are arguably the field's first theorems. The term "topology" was introduced by Johann Benedict Listing in the 19th century, although it was not until the first decades of the 20th century that the idea of a topological space was developed. By the middle of the 20th century, topology had become a major branch of mathematics.
Topology has many subfields:
History.
Topology began with the investigation of certain questions in geometry. Leonhard Euler's 1736 paper on the Seven Bridges of Königsberg is regarded as one of the first academic treatises in modern topology.
The term "Topologie" was introduced in German in 1847 by Johann Benedict Listing in "Vorstudien zur Topologie", who had used the word for ten years in correspondence before its first appearance in print. The English form topology was first used in 1883 in Listing's obituary in the journal "Nature" to distinguish "...qualitative geometry from the ordinary geometry in which quantitative relations chiefly are treated." The term topologist in the sense of a specialist in topology was used in 1905 in the magazine "Spectator". However, none of these uses corresponds exactly to the modern definition of topology.
Modern topology depends strongly on the ideas of set theory, developed by Georg Cantor in the later part of the 19th century. In addition to establishing the basic ideas of set theory, Cantor considered point sets in Euclidean space as part of his study of Fourier series.
Henri Poincaré published "Analysis Situs" in 1895, introducing the concepts of homotopy and homology, which are now considered part of algebraic topology.
Unifying the work on function spaces of Georg Cantor, Vito Volterra, Cesare Arzelà, Jacques Hadamard, Giulio Ascoli and others, Maurice Fréchet introduced the metric space in 1906. A metric space is now considered a special case of a general topological space. In 1914, Felix Hausdorff coined the term "topological space" and gave the definition for what is now called a Hausdorff space. Currently, a topological space is a slight generalization of Hausdorff spaces, given in 1922 by Kazimierz Kuratowski.
For further developments, see point-set topology and algebraic topology.
Introduction.
Topology can be formally defined as "the study of qualitative properties of certain objects (called topological spaces) that are invariant under a certain kind of transformation (called a continuous map), especially those properties that are invariant under a certain kind of transformation (called homeomorphism)."
Topology is also used to refer to a structure imposed upon a set "X", a structure that essentially 'characterizes' the set "X" as a topological space by taking proper care of properties such as convergence, connectedness and continuity, upon transformation.
Topological spaces show up naturally in almost every branch of mathematics. This has made topology one of the great unifying ideas of mathematics.
The motivating insight behind topology is that some geometric problems depend not on the exact shape of the objects involved, but rather on the way they are put together. For example, the square and the circle have many properties in common: they are both one dimensional objects (from a topological point of view) and both separate the plane into two parts, the part inside and the part outside.
In one of the first papers in topology, Leonhard Euler demonstrated that it was impossible to find a route through the town of Königsberg (now Kaliningrad) that would cross each of its seven bridges exactly once. This result did not depend on the lengths of the bridges, nor on their distance from one another, but only on connectivity properties: which bridges connect to which islands or riverbanks. This problem in introductory mathematics called "Seven Bridges of Königsberg" led to the branch of mathematics known as graph theory.
Similarly, the hairy ball theorem of algebraic topology says that "one cannot comb the hair flat on a hairy ball without creating a cowlick." This fact is immediately convincing to most people, even though they might not recognize the more formal statement of the theorem, that there is no nonvanishing continuous tangent vector field on the sphere. As with the "Bridges of Königsberg", the result does not depend on the shape of the sphere; it applies to any kind of smooth blob, as long as it has no holes.
To deal with these problems that do not rely on the exact shape of the objects, one must be clear about just what properties these problems "do" rely on. From this need arises the notion of homeomorphism. The impossibility of crossing each bridge just once applies to any arrangement of bridges homeomorphic to those in Königsberg, and the hairy ball theorem applies to any space homeomorphic to a sphere.
Intuitively, two spaces are homeomorphic if one can be deformed into the other without cutting or gluing. A traditional joke is that a topologist cannot distinguish a coffee mug from a doughnut, since a sufficiently pliable doughnut could be reshaped to a coffee cup by creating a dimple and progressively enlarging it, while shrinking the hole into a handle.
Homeomorphism can be considered the most basic "topological equivalence". Another is homotopy equivalence. This is harder to describe without getting technical, but the essential notion is that two objects are homotopy equivalent if they both result from "squishing" some larger object.
An introductory exercise is to classify the uppercase letters of the English alphabet according to homeomorphism and homotopy equivalence. The result depends partially on the font used. The figures use the sans-serif Myriad font. Homotopy equivalence is a rougher relationship than homeomorphism; a homotopy equivalence class can contain several homeomorphism classes. The simple case of homotopy equivalence described above can be used here to show two letters are homotopy equivalent. For example, O fits inside P and the tail of the P can be squished to the "hole" part.
Homeomorphism classes are:
Homotopy classes are larger, because the tails can be squished down to a point. They are:
To classify the letters correctly, we must show that two letters in the same class are equivalent and two letters in different classes are not equivalent. In the case of homeomorphism, this can be done by selecting points and showing their removal disconnects the letters differently. For example, X and Y are not homeomorphic because removing the center point of the X leaves four pieces; whatever point in Y corresponds to this point, its removal can leave at most three pieces. The case of homotopy equivalence is harder and requires a more elaborate argument showing an algebraic invariant, such as the fundamental group, is different on the supposedly differing classes.
Letter topology has practical relevance in stencil typography. For instance, Braggadocio font stencils are made of one connected piece of material.
Concepts.
Topologies on Sets.
The term topology also refers to a specific mathematical idea central to the area of mathematics called topology. Informally, a topology tells how elements of a set relate spatially to each other. The same set can have different topologies. For instance, the real line, the complex plane, and the Cantor set can be thought of as the same set with different topologies.
Formally, let "X" be a set and let "τ" be a family of subsets of "X". Then "τ" is called a "topology on X" if:
If "τ" is a topology on "X", then the pair ("X", "τ") is called a "topological space". The notation "Xτ" may be used to denote a set "X" endowed with the particular topology "τ".
The members of "τ" are called "open sets" in "X". A subset of "X" is said to be closed if its complement is in "τ" (i.e., its complement is open). A subset of "X" may be open, closed, both (clopen set), or neither. The empty set and "X" itself are always both closed and open. An open set containing a point "x" is called a 'neighborhood' of "x".
A set with a topology is called a topological space.
Continuous functions and homeomorphisms.
A function or map from one topological space to another is called "continuous" if the inverse image of any open set is open. If the function maps the real numbers to the real numbers (both spaces with the Standard Topology), then this definition of continuous is equivalent to the definition of continuous in calculus. If a continuous function is one-to-one and onto, and if the inverse of the function is also continuous, then the function is called a homeomorphism and the domain of the function is said to be homeomorphic to the range. Another way of saying this is that the function has a natural extension to the topology. If two spaces are homeomorphic, they have identical topological properties, and are considered topologically the same. The cube and the sphere are homeomorphic, as are the coffee cup and the doughnut. But the circle is not homeomorphic to the doughnut.
Manifolds.
While topological spaces can be extremely varied and exotic, many areas of topology focus on the more familiar class of spaces known as manifolds. A manifold is a topological space that resembles Euclidean space near each point. More precisely, each point of an "n"-dimensional manifold has a neighbourhood that is homeomorphic to the Euclidean space of dimension "n". Lines and circles, but not figure eights, are one-dimensional manifolds. Two-dimensional manifolds are also called surfaces. Examples include the plane, the sphere, and the torus, which can all be realized in three dimensions, but also the Klein bottle and real projective plane, which cannot.
Topics.
General topology.
General topology is the branch of topology dealing with the basic set-theoretic definitions and constructions used in topology. It is the foundation of most other branches of topology, including differential topology, geometric topology, and algebraic topology. Another name for general topology is point-set topology.
The fundamental concepts in point-set topology are "continuity", "compactness", and "connectedness". Intuitively, continuous functions take nearby points to nearby points. Compact sets are those that can be covered by finitely many sets of arbitrarily small size. Connected sets are sets that cannot be divided into two pieces that are far apart. The words "nearby", "arbitrarily small", and "far apart" can all be made precise by using open sets. If we change the definition of "open set", we change what continuous functions, compact sets, and connected sets are. Each choice of definition for "open set" is called a "topology". A set with a topology is called a "topological space".
"Metric spaces" are an important class of topological spaces where distances can be assigned a number called a "metric". Having a metric simplifies many proofs, and many of the most common topological spaces are metric spaces.
Algebraic topology.
Algebraic topology is a branch of mathematics that uses tools from abstract algebra to study topological spaces. The basic goal is to find algebraic invariants that classify topological spaces up to homeomorphism, though usually most classify up to homotopy equivalence.
The most important of these invariants are homotopy groups, homology, and cohomology.
Although algebraic topology primarily uses algebra to study topological problems, using topology to solve algebraic problems is sometimes also possible. Algebraic topology, for example, allows for a convenient proof that any subgroup of a free group is again a free group.
Differential topology.
Differential topology is the field dealing with differentiable functions on differentiable manifolds. It is closely related to differential geometry and together they make up the geometric theory of differentiable manifolds.
More specifically, differential topology considers the properties and structures that require only a smooth structure on a manifold to be defined. Smooth manifolds are 'softer' than manifolds with extra geometric structures, which can act as obstructions to certain types of equivalences and deformations that exist in differential topology. For instance, volume and Riemannian curvature are invariants that can distinguish different geometric structures on the same smooth manifold—that is, one can smoothly "flatten out" certain manifolds, but it might require distorting the space and affecting the curvature or volume.
Geometric topology.
Geometric topology is a branch of topology that primarily focuses on low-dimensional manifolds (i.e. dimensions 2,3 and 4) and their interaction with geometry, but it also includes some higher-dimensional topology.
 Some examples of topics in geometric topology are orientability, handle decompositions, local flatness, and the planar and higher-dimensional Schönflies theorem.
In high-dimensional topology, characteristic classes are a basic invariant, and surgery theory is a key theory.
Low-dimensional topology is strongly geometric, as reflected in the uniformization theorem in 2 dimensions – every surface admits a constant curvature metric; geometrically, it has one of 3 possible geometries: positive curvature/spherical, zero curvature/flat, negative curvature/hyperbolic – and the geometrization conjecture (now theorem) in 3 dimensions – every 3-manifold can be cut into pieces, each of which has one of eight possible geometries.
2-dimensional topology can be studied as complex geometry in one variable (Riemann surfaces are complex curves) – by the uniformization theorem every conformal class of metrics is equivalent to a unique complex one, and 4-dimensional topology can be studied from the point of view of complex geometry in two variables (complex surfaces), though not every 4-manifold admits a complex structure.
Generalizations.
Occasionally, one needs to use the tools of topology but a "set of points" is not available. In pointless topology one considers instead the lattice of open sets as the basic notion of the theory, while Grothendieck topologies are structures defined on arbitrary categories that allow the definition of sheaves on those categories, and with that the definition of general cohomology theories.
Applications.
Biology.
Knot theory, a branch of topology, is used in biology to study the effects of certain enzymes on DNA. These enzymes cut, twist, and reconnect the DNA, causing knotting with observable effects such as slower electrophoresis. Topology is also used in evolutionary biology to represent the relationship between phenotype and genotype. Phenotypic forms that appear quite different can be separated by only a few mutations depending on how genetic changes map to phenotypic changes during development.
Computer science.
Topological data analysis uses techniques from algebraic topology to determine the large scale structure of a set (for instance, determining if a cloud of points is spherical or toroidal). The main method used by topological data analysis is:
Physics.
In physics, topology is used in several areas such as quantum field theory and cosmology.
A topological quantum field theory (or topological field theory or TQFT) is a quantum field theory that computes topological invariants.
Although TQFTs were invented by physicists, they are also of mathematical interest, being related to, among other things, knot theory and the theory of four-manifolds in algebraic topology, and to the theory of moduli spaces in algebraic geometry. Donaldson, Jones, Witten, and Kontsevich have all won Fields Medals for work related to topological field theory.
In cosmology, topology can be used to describe the overall shape of the universe. This area is known as spacetime topology.
Robotics.
The various possible positions of a robot can be described by a manifold called configuration space. In the area of motion planning, one finds paths between two points in configuration space. These paths represent a motion of the robot's joints and other parts into the desired location and pose.

</doc>
