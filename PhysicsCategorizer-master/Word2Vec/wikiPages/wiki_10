<doc id="30400" url="https://en.wikipedia.org/wiki?curid=30400" title="Torque">
Torque

Torque, moment, or moment of force (see the terminology below) is the tendency of a force to rotate an object about an axis, fulcrum, or pivot. Just as a force is a push or a pull, a torque can be thought of as a twist to an object. Mathematically, torque is defined as the cross product of the position vector of the point where the force is applied distance vector and the force vector, which tends to produce rotation.
Loosely speaking, torque is a measure of the turning force on an object such as a bolt or a flywheel. For example, pushing or pulling the handle of a wrench connected to a nut or bolt produces a torque (turning force) that loosens or tightens the nut or bolt.
The symbol for torque is typically formula_1, the lowercase Greek letter "tau". When it is called moment of force, it is commonly denoted "M".
The magnitude of torque depends on three quantities: the force applied, the length of the "lever arm" connecting the axis to the point of force application, and the angle between the force vector and the lever arm. In symbols:
where
The SI unit for torque is the newton metre (N⋅m). For more on the units of torque, see Units.
Terminology.
This article follows US physics terminology in its use of the word "torque". In the UK and in US mechanical engineering, this is called "moment of force", usually shortened to "moment". In US physics and UK physics terminology these terms are interchangeable, unlike in US mechanical engineering, where the term "torque" is used for the closely related "resultant moment of a couple".
"Torque" is defined mathematically as the rate of change of angular momentum of an object. The definition of torque states that one or both of the angular velocity or the moment of inertia of an object are changing. "Moment" is the general term used for the tendency of one or more applied forces to rotate an object about an axis, but not necessarily to change the angular momentum of the object (the concept which is called "torque" in physics). For example, a rotational force applied to a shaft causing acceleration, such as a drill bit accelerating from rest, results in a moment called a "torque". By contrast, a lateral force on a beam produces a moment (called a bending moment), but since the angular momentum of the beam is not changing, this bending moment is not called a "torque". Similarly with any force couple on an object that has no change to its angular momentum, such moment is also not called a "torque".
This article follows the US physics terminology by calling all moments by the term "torque", whether or not they cause the angular momentum of an object to change.
History.
The concept of torque, also called moment or couple, originated with the studies of Archimedes on levers. The term "torque" was apparently introduced into English scientific literature by James Thomson, the brother of Lord Kelvin, in 1884.
Definition and relation to angular momentum.
A force applied at a right angle to a lever multiplied by its distance from the lever's fulcrum (the length of the lever arm) is its torque. A force of three newtons applied two metres from the fulcrum, for example, exerts the same torque as a force of one newton applied six metres from the fulcrum. The direction of the torque can be determined by using the right hand grip rule: if the fingers of the right hand are curled from the direction of the lever arm to the direction of the force, then the thumb points in the direction of the torque.
More generally, the torque on a particle (which has the position r in some reference frame) can be defined as the cross product:
where r is the particle's position vector relative to the fulcrum, and F is the force acting on the particle. The magnitude "τ" of the torque is given by
where "r" is the distance from the axis of rotation to the particle, "F" is the magnitude of the force applied, and "θ" is the angle between the position and force vectors. Alternatively,
where "F"⊥ is the amount of force directed perpendicularly to the position of the particle. Any force directed parallel to the particle's position vector does not produce a torque.
It follows from the properties of the cross product that the "torque vector" is perpendicular to both the "position" and "force" vectors. The "torque vector" points along the axis of the rotation that the "force vector" (starting from rest) would initiate. The resulting "torque vector" direction is determined by the right-hand rule.
The unbalanced torque on a body along axis of rotation determines the rate of change of the body's angular momentum,
where L is the angular momentum vector and "t" is time. If multiple torques are acting on the body, it is instead the net torque which determines the rate of change of the angular momentum:
For rotation about a fixed axis,
where is the moment of inertia and ω is the angular velocity. It follows that
where α is the angular acceleration of the body, measured in rad/s2. This equation has the limitation that the torque equation describes the instantaneous axis of rotation or center of mass for any type of motion – whether pure translation, pure rotation, or mixed motion. = "Moment of inertia" about the point which the torque is written (either instantaneous axis of rotation or center of mass only). If body is in translatory equilibrium then the torque equation is the same about all points in the plane of motion.
A torque is not necessarily limited to rotation around a fixed axis, however. It may change the magnitude and/or direction of the angular momentum vector, depending on the angle between the velocity vector and the non-radial component of the force vector, as viewed in the pivot's frame of reference. A net torque on a spinning body therefore may result in a precession without necessarily causing a change in spin rate.
Proof of the equivalence of definitions.
The definition of angular momentum for a single particle is:
where "×" indicates the vector cross product, p is the particle's linear momentum, and r is the displacement vector from the origin (the origin is assumed to be a fixed location anywhere in space). The time-derivative of this is:
This result can easily be proven by splitting the vectors into components and applying the product rule. Now using the definition of force formula_15 (whether or not mass is constant) and the definition of velocity formula_16
The cross product of momentum formula_18 with its associated velocity formula_19 is zero because velocity and momentum are parallel, so the second term vanishes.
By definition, torque τ = r × F. Therefore, torque on a particle is "equal" to the
first derivative of its angular momentum with respect to time.
If multiple forces are applied, Newton's second law instead reads , and it follows that
This is a general proof.
Units.
Torque has dimension force times distance, symbolically . Official SI literature suggests using the unit "newton metre" (N⋅m) or the unit "joule per radian". The unit "newton metre" is properly denoted N⋅m or N m. This avoids ambiguity with mN, millinewtons.
The SI unit for energy or work is the joule. It is dimensionally equivalent to a force of one newton acting over a distance of one metre, but it is not used for torque. Energy and torque are entirely different concepts, so the practice of using different unit names (i.e., reserving newton metres for torque and using only joules for energy) helps avoid mistakes and misunderstandings. The dimensional equivalence of these units, of course, is not simply a coincidence: A torque of 1 N⋅m applied through a full revolution will require an energy of exactly 2π joules. Mathematically,
where "E" is the energy, "τ" is magnitude of the torque, and "θ" is the angle moved (in radians). This equation motivates the alternate unit name "joules per radian".
In Imperial units, "pound-force-feet" (lb⋅ft), "foot-pounds-force", "inch-pounds-force", "ounce-force-inches" (oz⋅in) are used, and other non-SI units of torque includes "metre-kilograms-force". For all these units, the word "force" is often left out. For example, abbreviating "pound-force-foot" to simply "pound-foot" (in this case, it would be implicit that the "pound" is pound-force and not pound-mass). This is an example of the confusion caused by the use of English units that may be avoided with SI units because of the careful distinction in SI between force (in newtons) and mass (in kilograms).
Torque is sometimes listed with units that do not make dimensional sense, such as the gram-centimeter. In this case, "gram" should be understood as the force given by the weight of 1 gram at the surface of the earth, i.e., . The surface of the earth is understood to have a standard acceleration of gravity ().
Special cases and other facts.
Moment arm formula.
A very useful special case, often given as the definition of torque in fields other than physics, is as follows:
The construction of the "moment arm" is shown in the figure to the right, along with the vectors r and F mentioned above. The problem with this definition is that it does not give the direction of the torque but only the magnitude, and hence it is difficult to use in three-dimensional cases. If the force is perpendicular to the displacement vector r, the moment arm will be equal to the distance to the centre, and torque will be a maximum for the given force. The equation for the magnitude of a torque, arising from a perpendicular force:
For example, if a person places a force of 10 N at the terminal end of a wrench that is 0.5 m long (or a force of 10 N exactly 0.5 m from the twist point of a wrench of any length), the torque will be 5 N.m – assuming that the person moves the wrench by applying force in the plane of movement of and perpendicular to the wrench.
Static equilibrium.
For an object to be in static equilibrium, not only must the sum of the forces be zero, but also the sum of the torques (moments) about any point. For a two-dimensional situation with horizontal and vertical forces, the sum of the forces requirement is two equations: Σ"H" = 0 and Σ"V" = 0, and the torque a third equation: Σ"τ" = 0. That is, to solve statically determinate equilibrium problems in two-dimensions, three equations are used.
Net force versus torque.
When the net force on the system is zero, the torque measured from any point in space is the same. For example, the torque on a current-carrying loop in a uniform magnetic field is the same regardless of your point of reference. If the net force formula_24 is not zero, and formula_25 is the torque measured from formula_26, then the torque measured from formula_27 is …
formula_28
Machine torque.
Torque is part of the basic specification of an engine: the power output of an engine is expressed as its torque multiplied by its rotational speed of the axis. Internal-combustion engines produce useful torque only over a limited range of rotational speeds (typically from around 1,000–6,000 rpm for a small car). The varying torque output over that range can be measured with a dynamometer, and shown as a torque curve.
Steam engines and electric motors tend to produce maximum torque close to zero rpm, with the torque diminishing as rotational speed rises (due to increasing friction and other constraints). Reciprocating steam engines can start heavy loads from zero RPM without a clutch.
Relationship between torque, power, and energy.
If a force is allowed to act through a distance, it is doing mechanical work. Similarly, if torque is allowed to act through a rotational distance, it is doing work. Mathematically, for rotation about a fixed axis through the center of mass,
where "W" is work, "τ" is torque, and "θ"1 and "θ"2 represent (respectively) the initial and final angular positions of the body. It follows from the work-energy theorem that "W" also represents the change in the rotational kinetic energy "E"r of the body, given by
where "I" is the moment of inertia of the body and "ω" is its angular speed.
Power is the work per unit time, given by
where "P" is power, τ is torque, ω is the angular velocity, and ⋅ represents the scalar product.
Algebraically, the equation may be rearranged to compute torque for a given angular speed and power output. Note that the power injected by the torque depends only on the instantaneous angular speed – not on whether the angular speed increases, decreases, or remains constant while the torque is being applied (this is equivalent to the linear case where the power injected by a force depends only on the instantaneous speed – not on the resulting acceleration, if any).
In practice, this relationship can be observed in bicycles: Bicycles are typically composed of two road wheels, front and rear gears (referred to as sprockets) meshing with a circular chain, and a derailleur mechanism if the bicycle's transmission system allows multiple gear ratios to be used (i.e. multi-speed bicycle), all of which attached to the frame. A cyclist, the person who rides the bicycle, provides the input power by turning pedals, thereby cranking the front sprocket (commonly referred to as chainring). The input power provided by the cyclist is equal to the product of cadence (i.e. the number of pedal revolutions per minute) and the torque on spindle of the bicycle's crankset. The bicycle's drivetrain transmits the input power to the road wheel, which in turn conveys the received power to the road as the output power of the bicycle. Depending on the gear ratio of the bicycle, a (torque, rpm)input pair is converted to a (torque, rpm)output pair. By using a larger rear gear, or by switching to a lower gear in multi-speed bicycles, angular speed of the road wheels is decreased while the torque is increased, product of which (i.e. power) does not change.
Consistent units must be used. For metric SI units power is watts, torque is newton metres and angular speed is radians per second (not rpm and not revolutions per second).
Also, the unit newton metre is dimensionally equivalent to the joule, which is the unit of energy. However, in the case of torque, the unit is assigned to a vector, whereas for energy, it is assigned to a scalar.
Conversion to other units.
A conversion factor may be necessary when using different units of power, torque, or For example, if rotational speed (revolutions per time) is used in place of angular speed (radians per time), we multiply by a factor of 2π radians per revolution. In the following formulas, "P" is power, τ is torque and ω is rotational speed.
Showing units:
Dividing by 60 seconds per minute gives us the following.
where rotational speed is in revolutions per minute (rpm).
Some people (e.g. American automotive engineers) use horsepower (imperial mechanical) for power, foot-pounds (lbf⋅ft) for torque and rpm for rotational speed. This results in the formula changing to:
The constant below (in foot pounds per minute) changes with the definition of the horsepower; for example, using metric horsepower, it becomes approximately 32,550.
Use of other units (e.g. BTU per hour for power) would require a different custom conversion factor.
Derivation.
For a rotating object, the "linear distance" covered at the circumference of rotation is the product of the radius with the angle covered. That is: linear distance = radius × angular distance. And by definition, linear distance = linear speed × time = radius × angular speed × time.
By the definition of torque: torque = radius × force. We can rearrange this to determine force = torque ÷ radius. These two values can be substituted into the definition of power:
The radius "r" and time "t" have dropped out of the equation. However, angular speed must be in radians, by the assumed direct relationship between linear speed and angular speed at the beginning of the derivation. If the rotational speed is measured in revolutions per unit of time, the linear speed and distance are increased proportionately by 2π in the above derivation to give:
If torque is in newton metres and rotational speed in revolutions per second, the above equation gives power in newton metres per second or watts. If Imperial units are used, and if torque is in pounds-force feet and rotational speed in revolutions per minute, the above equation gives power in foot pounds-force per minute. The horsepower form of the equation is then derived by applying the conversion factor 33,000 ft⋅lbf/min per horsepower:
because formula_39
Principle of moments.
The Principle of Moments, also known as Varignon's theorem (not to be confused with the geometrical theorem of the same name) states that the sum of torques due to several forces applied to "a single" point is equal to the torque due to the sum (resultant) of the forces. Mathematically, this follows from:
Torque multiplier.
A torque multiplier is a gear box with reduction ratios greater than 1. The given torque at the input gets multiplied as per the reduction ratio and transmitted to the output, thereby achieving greater torque, but with reduced rotational speed.

</doc>
<doc id="74800" url="https://en.wikipedia.org/wiki?curid=74800" title="Torus">
Torus

In geometry, a torus (plural tori) is a surface of revolution generated by revolving a circle in three-dimensional space about an axis coplanar with the circle. If the axis of revolution does not touch the circle, the surface has a ring shape and is called a "torus of revolution".
Real-world examples of (approximately) toroidal objects include inner tubes, swim rings, and the surface of a doughnut or bagel.
A torus should not be confused with a solid torus, which is formed by rotating a disk, rather than a circle, around an axis. A solid torus is a torus plus the volume inside the torus. Real-world approximations include doughnuts, vadai or vada, many lifebuoys, and O-rings.
In topology, a ring torus is homeomorphic to the Cartesian product of two circles: "S"1 × "S"1, and the latter is taken to be the definition in that context. It is a compact 2-manifold of genus 1. The ring torus is one way to embed this space into three-dimensional Euclidean space, but another way to do this is the Cartesian product of the embedding of "S"1 in the plane. This produces a geometric object called the Clifford torus, a surface in 4-space.
In the field of topology, a torus is any topological space that is topologically equivalent to a torus.
Geometry.
A torus can be defined parametrically by:
where
"R" is known as the "major radius" and "r" is known as the "minor radius". The ratio "R" divided by "r" is known as the "aspect ratio".
A doughnut has an aspect ratio of about 2 to 3.
An implicit equation in Cartesian coordinates for a torus radially symmetric about the "z"-axis is formula_2
or the solution of , where
formula_3
Algebraically eliminating the square root gives a quartic equation,
formula_4
The three different classes of standard tori correspond to the three possible aspect ratios between "R" and "r":
When , the interior
formula_5
of this torus is diffeomorphic (and, hence, homeomorphic) to a product of an Euclidean open disc and a circle. The surface area and interior volume of this torus are easily computed using Pappus's centroid theorem giving
These formulas are the same as for a cylinder of length 2π"R" and radius "r", created by cutting the tube and unrolling it by straightening out the line running around the center of the tube. The losses in surface area and volume on the inner side of the tube exactly cancel out the gains on the outer side.
As a torus is the product of two circles, a modified version of the spherical coordinate system is sometimes used.
In traditional spherical coordinates there are three measures, "R", the distance from the center of the coordinate system, and θ and φ, angles measured from the center point. 
As a torus has, effectively, two center points, the centerpoints of the angles are moved; φ measures the same angle as it does in the spherical system, but is known as the "toroidal" direction. The center point of θ is moved to the center of "r", and is known as the "poloidal" direction. These terms were first used in a discussion of the Earth's magnetic field, where "poloidal" was used to denote "the direction toward the poles". 
In modern use these terms are more commonly used to discuss magnetic confinement fusion devices.
Topology.
Topologically, a torus is a closed surface defined as the product of two circles: "S"1 × "S"1. This can be viewed as lying in C2 and is a subset of the 3-sphere "S"3 of radius . This topological torus is also often called the Clifford torus. In fact, "S"3 is filled out by a family of nested tori in this manner (with two degenerate circles), a fact which is important in the study of "S"3 as a fiber bundle over "S"2 (the Hopf bundle).
The surface described above, given the relative topology from R3, is homeomorphic to a topological torus as long as it does not intersect its own axis. A particular homeomorphism is given by stereographically projecting the topological torus into R3 from the north pole of "S"3.
The torus can also be described as a quotient of the Cartesian plane under the identifications
Or, equivalently, as the quotient of the unit square by pasting the opposite edges together, described as a fundamental polygon "ABA"−1"B"−1.
The fundamental group of the torus is just the direct product of the fundamental group of the circle with itself:
Intuitively speaking, this means that a closed path that circles the torus' "hole" (say, a circle that traces out a particular latitude) and then circles the torus' "body" (say, a circle that traces out a particular longitude) can be deformed to a path that circles the body and then the hole. So, strictly 'latitudinal' and strictly 'longitudinal' paths commute. This might be imagined as two shoelaces passing through each other, then unwinding, then rewinding.
If a torus is punctured and turned inside out then another torus results, with lines of latitude and longitude interchanged. This is equivalent to building a torus from a cylinder, by joining the circular ends together, in two different ways: around the outside like joining two ends of a garden hose, or through the inside like rolling a sock (with the toe cut off). Additionally, if the cylinder was made by gluing two opposite sides of a rectangle together, choosing the other two sides instead will cause the same reversal of orientation.
The first homology group of the torus is isomorphic to the fundamental group (this follows from Hurewicz theorem since the fundamental group is abelian).
Two-sheeted cover.
The 2-torus double-covers the 2-sphere, with four ramification points. Every conformal structure on the 2-torus can be represented as a two-sheeted cover of the 2-sphere. The points on the torus corresponding to the ramification points are the Weierstrass points. In fact, the conformal type of the torus is determined by the cross-ratio of the four points.
"n"-dimensional torus.
The torus has a generalization to higher dimensions, the "n-dimensional torus", often called the "n-torus" or "hypertorus" for short. (This is one of two different meanings of the term ""n"-torus".) Recalling that the torus is the product space of two circles, the "n"-dimensional torus is the product of "n" circles. That is:
The 1-torus is just the circle: T1 = "S"1. The torus discussed above is the 2-torus, T2. And similar to the 2-torus, the "n"-torus, T"n" can be described as a quotient of R"n" under integral shifts in any coordinate. That is, the "n"-torus is R"n" modulo the action of the integer lattice Z"n" (with the action being taken as vector addition). Equivalently, the "n"-torus is obtained from the "n"-dimensional hypercube by gluing the opposite faces together.
An "n"-torus in this sense is an example of an "n-"dimensional compact manifold. It is also an example of a compact abelian Lie group. This follows from the fact that the unit circle is a compact abelian Lie group (when identified with the unit complex numbers with multiplication). Group multiplication on the torus is then defined by coordinate-wise multiplication.
Toroidal groups play an important part in the theory of compact Lie groups. This is due in part to the fact that in any compact Lie group "G" one can always find a maximal torus; that is, a closed subgroup which is a torus of the largest possible dimension. Such maximal tori "T" have a controlling role to play in theory of connected "G". Toroidal groups are examples of protori, which (like tori) are compact connected abelian groups, which are not required to be manifolds.
Automorphisms of "T" are easily constructed from automorphisms of the lattice Z"n", which are classified by invertible integral matrices of size "n" with an integral inverse; these are just the integral matrices with determinant ±1. Making them act on R"n" in the usual way, one has the typical "toral automorphism" on the quotient.
The fundamental group of an "n"-torus is a free abelian group of rank "n". The "k"-th homology group of an "n"-torus is a free abelian group of rank "n" choose "k". It follows that the Euler characteristic of the "n"-torus is 0 for all "n". The cohomology ring "H"•(T"n", Z) can be identified with the exterior algebra over the Z-module Z"n" whose generators are the duals of the "n" nontrivial cycles.
Configuration space.
As the "n"-torus is the "n"-fold product of the circle, the "n"-torus is the configuration space of "n" ordered, not necessarily distinct points on the circle. Symbolically, T"n" = ("S"1)"n". The configuration space of "unordered", not necessarily distinct points is accordingly the orbifold T"n"/"Sn", which is the quotient of the torus by the symmetric group on "n" letters (by permuting the coordinates).
For "n" = 2, the quotient is the Möbius strip, the edge corresponding to the orbifold points where the two coordinates coincide. For "n" = 3 this quotient may be described as a solid torus with cross-section an equilateral triangle, with a twist; equivalently, as a triangular prism whose top and bottom faces are connected with a 1/3 twist (120°): the 3-dimensional interior corresponds to the points on the 3-torus where all 3 coordinates are distinct, the 2-dimensional face corresponds to points with 2 coordinates equal and the 3rd different, while the 1-dimensional edge corresponds to points with all 3 coordinates identical.
These orbifolds have found significant applications to music theory in the work of Dmitri Tymoczko and collaborators (Felipe Posada and Michael Kolinas, et al.), being used to model musical triads.
Flat torus.
The flat torus is a torus with the metric inherited from its representation as the quotient, R2/L, where L is a discrete subgroup of R2 isomorphic to Z2. This gives the quotient the structure of a Riemannian manifold. Perhaps the simplest example of this is when L = Z2 itself: R2/Z2, which can also be described as the Cartesian plane under the identifications ("x", "y") ~ ("x"+1, "y") ~ ("x", "y"+1). This particular flat torus (and any uniformly scaled version of it as well) is known as the "square" flat torus.
This metric of the square flat torus can also be realised by specific embeddings of the familiar 2-torus into Euclidean 4-space or higher dimensions. Its surface has zero Gaussian curvature everywhere. Its surface is "flat" in the same sense that the surface of a cylinder is "flat". In 3 dimensions one can bend a flat sheet of paper into a cylinder without stretching the paper, but you cannot then bend this cylinder into a torus without stretching the paper (unless you give up some regularity and differentiability conditions, see below).
A simple 4-dimensional Euclidean embedding of a rectangular flat torus (more general than the square one) is as follows:
where "R" and "P" are constants determining the aspect ratio. It is diffeomorphic to a regular torus but not isometric. It can not be isometrically embedded into Euclidean 3-space. Mapping it into "3"-space requires you to stretch it, in which case it looks like a regular torus, for example, the following map
If "R" and "P" in the above flat torus form a unit vector <R, P> = <cos(η), sin(η)> then "u", "v", and "η" can be used to parameterize the unit 3-sphere in a parameterization associated with the Hopf map. In particular, for certain very specific choices of a square flat torus in the 3-sphere S3, where "η" = π/4 above, the torus will partition the 3-sphere into two congruent solid tori subsets with the aforesaid flat torus surface as their common boundary. One example is the torus T defined by 
 T = { (x,y,z,w) ∈ S3 | x2 + y2 = 1/2,     z2 + w2 = 1/2 }.
Other tori in S3 having this partitioning property include the square tori of the form Q⋅T, where Q is a rotation of 4-dimensional space R4, or in other words Q is a member of the Lie group SO(4).
It is known that there exists no C2 (twice continuously differentiable) embedding of a flat torus into 3-space. (The idea of the proof is to take a large sphere containing such a flat torus in its interior, and shrink the radius of the sphere until it just touches the torus for the first time. Such a point of contact must be a tangency. But that would imply that part of the torus, since it has zero curvature everywhere, must lie strictly outside the sphere, which is a contradiction.) On the other hand, according to the Nash-Kuiper theorem, proven in the 1950s, an isometric C1 embedding exists. This is solely an existence proof, and does not provide explicit equations for such an embedding.
In April 2012, an explicit C1 (continuously differentiable) embedding of a flat torus into 3-dimensional Euclidean space R3 was found. It is similar in structure to a fractal as it is constructed by repeatedly corrugating a normal torus. Like fractals, it has no defined Gaussian curvature. However, unlike fractals, it does have defined surface normals. It "is" a flat torus in the sense that as metric spaces, it is isometric to a flat square torus. (These infinitely recursive corrugations are used only for embedding into three dimensions; they are not an intrinsic feature of the flat torus.) This is the first time that any such embedding was defined by explicit equations, or depicted by computer graphics.
"n"-fold torus.
In the theory of surfaces there is another object, the "n"-fold torus, often called the "n"-holed torus. Instead of the product of "n" circles, an "n"-fold torus is the connected sum of "n" 2-tori. To form a connected sum of two surfaces, remove from each the interior of a disk and "glue" the surfaces together along the disks' boundary circles. To form the connected sum of more than two surfaces, sum two of them at a time until they are all connected. In this sense, an "n"-torus resembles the surface of "n" doughnuts stuck together side by side, or a 2-sphere with "n" handles attached.
An ordinary torus is a 1-fold torus, a 2-fold torus is called a double torus, a 3-fold torus a triple torus, and so on. The "n"-fold torus is said to be an "orientable surface" of "genus" "n", the genus being the number of handles. The 0-fold torus is the 2-sphere.
The classification theorem for surfaces states that every compact connected surface is topologically equivalent to a) the sphere, or b) the "n"-fold torus with "n" > 0, or c) the connected sum of "n" projective planes (that is, projective planes over the real numbers) with "n" > 0.
Toroidal polyhedra.
Polyhedra with the topological type of a torus are called toroidal polyhedra, and have Euler characteristic "V" − "E" + "F" = 0. For any number holes, the formula generalizes to "V" − "E" + "F" = 2 − 2"N", where "N" is the number of holes.
The term "toroidal polyhedron" is also used for higher-genus polyhedra and for immersions of toroidal polyhedra.
Automorphisms.
The homeomorphism group (or the subgroup of diffeomorphisms) of the torus is studied in geometric topology. Its mapping class group (the group of connected components) is isomorphic to the group GL("n", Z) of invertible integer matrices, and can be realized as linear maps on the universal covering space R"n" that preserve the standard lattice Z"n" (this corresponds to integer coefficients) and thus descend to the quotient.
At the level of homotopy and homology, the mapping class group can be identified as the action on the first homology (or equivalently, first cohomology, or on the fundamental group, as these are all naturally isomorphic; also the first cohomology group generates the cohomology algebra):
Since the torus is an Eilenberg–MacLane space "K"("G", 1), its homotopy equivalences, up to homotopy, can be identified with automorphisms of the fundamental group); that this agrees with the mapping class group reflects that all homotopy equivalences can be realized by homeomorphisms – every homotopy equivalence is homotopic to a homeomorphism – and that homotopic homeomorphisms are in fact isotopic (connected through homeomorphisms, not just through homotopy equivalences). More tersely, the map Homeo(T"n") → SHE(T"n") is 1-connected (isomorphic on path-components, onto fundamental group). This is a "homeomorphism reduces to homotopy reduces to algebra" result.
Thus the short exact sequence of the mapping class group splits (an identification of the torus as the quotient of R"n" gives a splitting, via the linear maps, as above):
so the homeomorphism group of the torus is a semidirect product,
The mapping class group of higher genus surfaces is much more complicated, and an area of active research.
Coloring a torus.
If a torus is divided into regions, then it is always possible to color the regions with no more than seven colors so that neighboring regions have different colors. (Contrast with the four color theorem for the plane.)
Cutting a torus.
A solid torus of revolution (informally, a bagel or donut) can be cut with "n" planes into at most
parts.
The initial terms of this sequence, for 0 ≤ n ≤ 10, are as follows:

</doc>
<doc id="473774" url="https://en.wikipedia.org/wiki?curid=473774" title="Transformation geometry">
Transformation geometry

In mathematics, transformation geometry (or transformational geometry) is the name of a mathematical and pedagogic approach to the study of geometry by focusing on groups of geometric transformations, and the properties of figures that are invariant under them. It is opposed to the classical synthetic geometry approach of Euclidean geometry, that focuses on geometric constructions.
For example, within transformation geometry, the properties of an isosceles triangle are deduced from the fact that it is mapped to itself by a reflection about a certain line. This contrasts with the classical proofs by the criteria for congruence of triangles.
The first systematic effort to use transformations as the foundation of geometry was made by Felix Klein in the 19th century, under the name Erlangen programme. For nearly a century this approach remained confined to mathematics research circles. In the 20th century efforts were made to exploit it for mathematical education. Andrei Kolmogorov included this approach (together with set theory) as part of a proposal for geometry teaching reform in Russia. These efforts culminated in the 1960s with the general reform of mathematics teaching known as the New Math movement.
Teaching transformation geometry.
An exploration of transformation geometry often begins with a study of reflection symmetry as found in daily life. The first real transformation is "reflection in a line" or "reflection against an axis". The composition of two reflections results in a rotation when the lines intersect, or a translation when they are parallel. Thus through transformations students learn about Euclidean plane isometry. For instance, consider reflection in a vertical line and a line inclined at 45° to the horizontal. One can observe that one composition yields a counter-clockwise quarter-turn (90°) while the reverse composition yields a clockwise quarter-turn. Such results show that transformation geometry includes non-commutative processes.
An entertaining application of reflection in a line occurs in a proof of the one-seventh area triangle found in any triangle.
Another transformation introduced to young students is the dilation. However, the reflection in a circle transformation seems inappropriate for lower grades. Thus inversive geometry, a larger study than grade school transformation geometry, is usually reserved for college students.
Experiments with concrete symmetry groups make way for abstract group theory. Other concrete activities use computations with complex numbers, hypercomplex numbers, or matrices to express transformation geometry.
Such transformation geometry lessons present an alternate view that contrasts with classical synthetic geometry. When students then encounter analytic geometry, the ideas of coordinate rotations and reflections follow easily. All these concepts prepare for linear algebra where the reflection concept is expanded.
Educators have shown some interest and described projects and experiences with transformation geometry for children from kindergarten to high school. In the case of very young age children, in order to avoid introducing new terminology and to make links with students' everyday experience with concrete objects, it was sometimes recommended to use words they are familiar with, like "flips" for line reflections, "slides" for translations, and "turns" for rotations, although these are not precise mathematical language. In some proposals, students start by performing with concrete objects before they perform the abstract transformations via their definitions of a mapping of each point of the figure.
In an attempt to restructure the courses of geometry in Russia, Kolmogorov suggested presenting it under the point of view of transformations, so the geometry courses were structured based on set theory. This led to the appearance of the term "congruent" in schools, for figures that were before called "equal": since a figure was seen as a set of points, it could only be equal to itself, and two triangles that could be overlapped by isometries were said to be congruent.
One author expressed the importance of group theory to transformation geometry as follows:

</doc>
<doc id="692458" url="https://en.wikipedia.org/wiki?curid=692458" title="Transformation matrix">
Transformation matrix

In linear algebra, linear transformations can be represented by matrices. If "T" is a linear transformation mapping R"n" to R"m" and formula_1 is a column vector with "n" entries, then
for some "m"×"n" matrix A, called the transformation matrix of "T".
There are alternative expressions of transformation matrices involving row vectors that are preferred by some authors.
Uses.
Matrices allow arbitrary linear transformations to be represented in a consistent format, suitable for computation. This also allows transformations to be concatenated easily (by multiplying their matrices).
Linear transformations are not the only ones that can be represented by matrices. Some transformations that are non-linear on a n-dimensional Euclidean space R"n", can be represented as linear transformations on the "n"+1-dimensional space R"n"+1. These include both affine transformations (such as translation) and projective transformations. For this reason, 4×4 transformation matrices are widely used in 3D computer graphics. These "n"+1-dimensional transformation matrices are called, depending on their application, "affine transformation matrices", "projective transformation matrices", or more generally "non-linear transformation matrices". With respect to an "n"-dimensional matrix, an "n"+1-dimensional matrix can be described as an augmented matrix.
In the physical sciences, an active transformation is one which actually changes the physical position of a system, and makes sense even in the absence of a coordinate system whereas a passive transformation is a change in the coordinate description of the physical system (change of basis). The distinction between active and passive transformations is important. By default, by "transformation", mathematicians usually mean active transformations, while physicists could mean either.
Put differently, a "passive" transformation refers to description of the "same" object as viewed from two different coordinate frames.
Finding the matrix of a transformation.
If one has a linear transformation formula_3 in functional form, it is easy to determine the transformation matrix A by transforming each of the vectors of the standard basis by "T", then inserting the result into the columns of a matrix. In other words,
For example, the function formula_5 is a linear transformation. Applying the above process (suppose that "n" = 2 in this case) reveals that
It must be noted that the matrix representation of vectors and operators depends on the chosen basis; a similar matrix will result from an alternate basis. Nevertheless, the method to find the components remains the same.
To elaborate, vector v can be represented in basis vectors, formula_7 with coordinates formula_8 :
Now, express the result of the transformation matrix A upon formula_10, in the given basis:
The formula_13 elements of matrix A are determined for a given basis E by applying A to every formula_14, and observing the response vector formula_15. This equation defines the wanted elements, formula_13, of j-th column of the matrix A.
Eigenbasis and diagonal matrix.
Yet, there is a special basis for an operator in which the components form a diagonal matrix and, thus, multiplication complexity reduces to n. Being diagonal means that all coefficients formula_17 but formula_18 are zeros leaving only one term in the sum formula_19 above. The surviving diagonal elements, formula_18, are known as eigenvalues and designated with formula_21 in the defining equation, which reduces to formula_22. The resulting equation is known as eigenvalue equation. The eigenvectors and eigenvalues are derived from it via the characteristic polynomial.
With diagonalization, it is often possible to translate to and from eigenbases.
Examples in 2D computer graphics.
Most common geometric transformations that keep the origin fixed are linear, including rotation, scaling, shearing, reflection, and orthogonal projection; if an affine transformation is not a pure translation it keeps some point fixed, and that point can be chosen as origin to make the transformation linear. In two dimensions, linear transformations can be represented using a 2×2 transformation matrix.
Rotation.
For rotation by an angle θ clockwise about the origin the functional form is formula_23 and formula_24. Written in matrix form, this becomes:
Similarly, for a rotation counter-clockwise about the origin, the functional form is formula_26 and formula_27 and the matrix form is:
These formulae assume that the "x" axis points right and the "y" axis points up. In formats such as SVG where the "y" axis points down, these matrices must be swapped.
Shearing.
For shear mapping (visually similar to slanting), there are two possibilities.
A shear parallel to the "x" axis has formula_29 and formula_30. Written in matrix form, this becomes:
A shear parallel to the "y" axis has formula_32 and formula_33, which has matrix form:
Reflection.
To reflect a vector about a line that goes through the origin, let formula_35 be a vector in the direction of the line:
Orthogonal projection.
To project a vector orthogonally onto a line that goes through the origin, let formula_37 be a vector in the direction of the line. Then use the transformation matrix:
As with reflections, the orthogonal projection onto a line that does not pass through the origin is an affine, not linear, transformation.
Parallel projections are also linear transformations and can be represented simply by a matrix. However, perspective projections are not, and to represent these with a matrix, homogeneous coordinates must be used.
Examples in 3D computer graphics.
Rotation.
The matrix to rotate an angle "θ" about the axis defined by unit vector ("l","m","n") is
Reflection.
To reflect a point through a plane formula_40 (which goes through the origin), one can use formula_41, where formula_42 is the 3x3 identity matrix and formula_43 is the three-dimensional unit vector for the vector normal of the plane. If the L2 norm of formula_44 and formula_45 is unity, the transformation matrix can be expressed as:
Note that these are particular cases of a Householder reflection in two and three dimensions. A reflection about a line or plane that does not go through the origin is not a linear transformation; it is an affine transformation.
Composing and inverting transformations.
One of the main motivations for using matrices to represent linear transformations is that transformations can then be easily composed (combined) and inverted.
Composition is accomplished by matrix multiplication. If A and B are the matrices of two linear transformations, then the effect of applying first A and then B to a vector "x" is given by:
In other words, the matrix of the combined transformation A followed by B is simply the product of the individual matrices. Note that the multiplication is done in the opposite order from the English sentence: the matrix of "A followed by B" is BA, not AB.
A consequence of the ability to compose transformations by multiplying their matrices is that transformations can also be inverted by simply inverting their matrices. So, A−1 represents the transformation that "undoes" A.
Other kinds of transformations.
Affine transformations.
To represent affine transformations with matrices, we can use homogeneous coordinates. This means representing a 2-vector ("x", "y") as a 3-vector ("x", "y", 1), and similarly for higher dimensions. Using this system, translation can be expressed with matrix multiplication. The functional form formula_48 becomes:
All ordinary linear transformations are included in the set of affine transformations, and can be described as a simplified form of affine transformations. Therefore, any linear transformation can also be represented by a general transformation matrix. The latter is obtained by expanding the corresponding linear transformation matrix by one row and column, filling the extra space with zeros except for the lower-right corner, which must be set to 1. For example, "the clockwise rotation matrix from above" becomes:
Using transformation matrices containing homogeneous coordinates, translations can be seamlessly intermixed with all other types of transformations. The reason is that the real plane is mapped to the "w" = 1 plane in real projective space, and so translation in real Euclidean space can be represented as a shear in real projective space. Although a translation is a non-linear transformation in a 2-D or 3-D Euclidean space described by Cartesian coordinates, it becomes, in a 3-D or 4-D projective space described by homogeneous coordinates, a simple linear transformation (a shear).
More affine transformations can be obtained by composition of two or more affine transformations. For example, given a translation T' with vector formula_51, a rotation R by an angle θ counter-clockwise, a scaling S with factors formula_52 and a translation T of vector formula_53, the result M of T'RST is:
When using affine transformations, the homogeneous component of a coordinate vector (normally called "w") will never be altered. One can therefore safely assume that it is always 1 and ignore it. However, this is not true when using perspective projections.
Perspective projection.
Another type of transformation, of importance in 3D computer graphics, is the perspective projection. Whereas parallel projections are used to project points onto the image plane along parallel lines, the perspective projection projects points onto the image plane along lines that emanate from a single point, called the center of projection. This means that an object has a smaller projection when it is far away from the center of projection and a larger projection when it is closer.
The simplest perspective projection uses the origin as the center of projection, and "z" = 1 as the image plane. The functional form of this transformation is then formula_55; formula_56. We can express this in homogeneous coordinates as:
After carrying out the matrix multiplication, the homogeneous component "wc" will, in general, not be equal to 1. Therefore, to map back into the real plane we must perform the homogeneous divide or perspective divide by dividing each component by "wc":
More complicated perspective projections can be composed by combining this one with rotations, scales, translations, and shears to move the image plane and center of projection wherever they are desired.

</doc>
<doc id="173844" url="https://en.wikipedia.org/wiki?curid=173844" title="Transpose">
Transpose

In linear algebra, the transpose of a matrix A is another matrix AT (also written A′, Atr, tA or At) created by any one of the following equivalent actions:
Formally, the "i" th row, "j" th column element of AT is the "j" th row, "i" th column element of A:
If A is an matrix then AT is an matrix.
The transpose of a matrix was introduced in 1858 by the British mathematician Arthur Cayley.
Properties.
For matrices A, B and scalar "c" we have the following properties of transpose:
 \mathbf{b} ,</math>
which is written as a"i" b"i" in Einstein summation convention.
Special transpose matrices.
A square matrix whose transpose is equal to itself is called a symmetric matrix; that is, A is symmetric if
A square matrix whose transpose is equal to its negative is called a skew-symmetric matrix; that is, A is skew-symmetric if
A square complex matrix whose transpose is equal to the matrix with every entry replaced by its complex conjugate (denoted here with an overline) is called a Hermitian matrix (equivalent to the matrix being equal to its conjugate transpose); that is, A is Hermitian if
A square complex matrix whose transpose is equal to the negation of its complex conjugate is called a skew-Hermitian matrix; that is, A is skew-Hermitian if
A square matrix whose transpose is equal to its inverse is called an orthogonal matrix; that is, A is orthogonal if
Transpose of a linear map.
The transpose may be defined in a more generally:
If is a linear map between right "R"-modules "V" and "W" with respective dual modules "V"∗ and "W"∗, the "transpose" of "f" is the linear map
Equivalently, the transpose t"f" is defined by the relation
where is the duality pairing. This definition also applies unchanged to left modules and to vector spaces.
The definition of the transpose may be seen to be independent of any bilinear form on the vector spaces, unlike the adjoint (below).
If the matrix "A" describes a linear map with respect to bases of "V" and "W", then the matrix "A"T describes the transpose of that linear map with respect to the dual bases.
Transpose of a bilinear form.
Every linear map to the dual space defines a bilinear form , with the relation . By defining the transpose of this bilinear form as the bilinear form t"B" defined by the transpose i.e. , we find that .
Adjoint.
If the vector spaces "V" and "W" have respective nondegenerate bilinear forms "B""V" and "B""W", a concept closely related to the transpose – the "adjoint" – may be defined:
If is a linear map between vector spaces "V" and "W", we define "g" as the "adjoint" of "f" if satisfies
These bilinear forms define an isomorphism between "V" and "V"∗, and between "W" and "W"∗, resulting in an isomorphism between the transpose and adjoint of "f". The matrix of the adjoint of a map is the transposed matrix only if the bases are orthonormal with respect to their bilinear forms. In this context, many authors use the term transpose to refer to the adjoint as defined here.
The adjoint allows us to consider whether is equal to . In particular, this allows the orthogonal group over a vector space "V" with a quadratic form to be defined without reference to matrices (nor the components thereof) as the set of all linear maps for which the adjoint equals the inverse.
Over a complex vector space, one often works with sesquilinear forms (conjugate-linear in one argument) instead of bilinear forms. The Hermitian adjoint of a map between such spaces is defined similarly, and the matrix of the Hermitian adjoint is given by the conjugate transpose matrix if the bases are orthonormal.
Implementation of matrix transposition on computers.
On a computer, one can often avoid explicitly transposing a matrix in memory by simply accessing the same data in a different order. For example, software libraries for linear algebra, such as BLAS, typically provide options to specify that certain matrices are to be interpreted in transposed order to avoid the necessity of data movement.
However, there remain a number of circumstances in which it is necessary or desirable to physically reorder a matrix in memory to its transposed ordering. For example, with a matrix stored in row-major order, the rows of the matrix are contiguous in memory and the columns are discontiguous. If repeated operations need to be performed on the columns, for example in a fast Fourier transform algorithm, transposing the matrix in memory (to make the columns contiguous) may improve performance by increasing memory locality.
Ideally, one might hope to transpose a matrix with minimal additional storage. This leads to the problem of transposing an "n" × "m" matrix in-place, with O(1) additional storage or at most storage much less than "mn". For "n" ≠ "m", this involves a complicated permutation of the data elements that is non-trivial to implement in-place. Therefore efficient in-place matrix transposition has been the subject of numerous research publications in computer science, starting in the late 1950s, and several algorithms have been developed.

</doc>
<doc id="374222" url="https://en.wikipedia.org/wiki?curid=374222" title="Triangular matrix">
Triangular matrix

In the mathematical discipline of linear algebra, a triangular matrix is a special kind of square matrix. A square matrix is called lower triangular if all the entries "above" the main diagonal are zero. Similarly, a square matrix is called upper triangular if all the entries "below" the main diagonal are zero. A triangular matrix is one that is either lower triangular or upper triangular. A matrix that is both upper and lower triangular is called a diagonal matrix.
Because matrix equations with triangular matrices are easier to solve, they are very important in numerical analysis. By the LU decomposition algorithm, an invertible matrix may be written as the product of a lower triangular matrix "L" and an upper triangular matrix "U" if and only if all its leading principal minors are non-zero.
Description.
A matrix of the form 
is called a lower triangular matrix or left triangular matrix, and analogously a matrix of the form
is called an upper triangular matrix or right triangular matrix. The variable "L" (standing for lower or left) is commonly used to represent a lower triangular matrix, while the variable "U" (standing for upper) or "R" (standing for right) is commonly used for upper triangular matrix. A matrix that is both upper and lower triangular is diagonal. 
Matrices that are similar to triangular matrices are called triangularisable.
Many operations on upper triangular matrices preserve the shape: 
Together these facts mean that the upper triangular matrices form a subalgebra of the associative algebra of square matrices for a given size. Additionally, this also shows that the upper triangular matrices can be viewed as a Lie subalgebra of the Lie algebra of square matrices of a fixed size, where the Lie bracket ["a","b"] given by the commutator "ab-ba". The Lie algebra of all upper triangular matrices is often referred to as a Borel subalgebra of the Lie algebra of all square matrices.
All these results hold if "upper triangular" is replaced by "lower triangular" throughout; in particular the lower triangular matrices also form a Lie algebra. However, operations mixing upper and lower triangular matrices do not in general produce triangular matrices. For instance, the sum of an upper and a lower triangular matrix can be any matrix; the product of a lower triangular with an upper triangular matrix is not necessarily triangular either.
Examples.
This matrix
is upper triangular and this matrix
is lower triangular.
Special forms.
Unitriangular matrix.
If the entries on the main diagonal of a (upper or lower) triangular matrix are all 1, the matrix is called (upper or lower) unitriangular. All unitriangular matrices are unipotent. Other names used for these matrices are unit (upper or lower) triangular (of which "unitriangular" might be a contraction), or very rarely normed (upper or lower) triangular. However a "unit" triangular matrix is not the same as the "unit matrix", and a "normed" triangular matrix has nothing to do with the notion of matrix norm. The identity matrix is the only matrix which is both upper and lower unitriangular.
The set of unitriangular matrices forms a Lie group.
Strictly triangular matrix.
If the entries on the main diagonal of a (upper or lower) triangular matrix are all 0, the matrix is called strictly (upper or lower) triangular. All strictly triangular matrices are nilpotent, and the set of strictly upper (or lower) triangular matrices forms a nilpotent Lie algebra, denoted formula_5 This algebra is the derived Lie algebra of formula_6, the Lie algebra of all upper triangular matrices; in symbols, formula_7 In addition, formula_8 is the Lie algebra of the Lie group of unitriangular matrices.
In fact, by Engel's theorem, any finite-dimensional nilpotent Lie algebra is conjugate to a subalgebra of the strictly upper triangular matrices, that is to say, a finite-dimensional nilpotent Lie algebra is simultaneously strictly upper triangularizable.
Atomic triangular matrix.
An atomic (upper or lower) triangular matrix is a special form of unitriangular matrix, where all of the off-diagonal entries are zero, except for the entries in a single column. Such a matrix is also called a Gauss matrix or a Gauss transformation matrix. So an atomic lower triangular matrix is of the form
The inverse of an atomic triangular matrix is again atomic triangular. Indeed, we have
i.e., the off-diagonal entries are replaced in the inverse matrix by their additive inverses.
Examples.
The matrix
is atomic lower triangular. Its inverse is
Special properties.
A matrix which is simultaneously triangular and normal is also diagonal. This can be seen by looking at the diagonal entries of "A"*"A" and "AA"*, where "A" is a normal, triangular matrix. 
The transpose of an upper triangular matrix is a lower triangular matrix and vice versa.
The determinant of a triangular matrix equals the product of the diagonal entries. Since for any triangular matrix "A" the matrix formula_13, whose determinant is the characteristic polynomial of "A", is also triangular, the diagonal entries of "A" in fact give the multiset of eigenvalues of "A" (an eigenvalue with multiplicity "m" occurs exactly "m" times as diagonal entry).
Triangularisability.
A matrix that is similar to a triangular matrix is referred to as triangularisable. Abstractly, this is equivalent to stabilising a flag: upper triangular matrices are precisely those that preserve the standard flag, which is given by the standard ordered basis formula_14 and the resulting flag formula_15 All flags are conjugate (as the general linear group acts transitively on bases), so any matrix that stabilises a flag is similar to one that stabilises the standard flag.
Any complex square matrix is triangularisable. In fact, a matrix "A" over a field containing all of the eigenvalues of "A" (for example, any matrix over an algebraically closed field) is similar to a triangular matrix. This can be proven by using induction on the fact that "A" has an eigenvector, by taking the quotient space by the eigenvector and inducting to show that "A" stabilises a flag, and is thus triangularisable with respect to a basis for that flag.
A more precise statement is given by the Jordan normal form theorem, which states that in this situation, "A" is similar to an upper triangular matrix of a very particular form. The simpler triangularization result is often sufficient however, and in any case used in proving the Jordan normal form theorem.
In the case of complex matrices, it is possible to say more about triangularisation, namely, that any square matrix "A" has a Schur decomposition. This means that "A" is unitarily equivalent (i.e. similar, using a unitary matrix as change of basis) to an upper triangular matrix; this follows by taking an Hermitian basis for the flag.
Simultaneous triangularisability.
A set of matrices formula_16 are said to be if there is a basis under which they are all upper triangular; equivalently, if they are upper triangularizable by a single similarity matrix "P." Such a set of matrices is more easily understood by considering the algebra of matrices it generates, namely all polynomials in the formula_17 denoted formula_18 Simultaneous triangularizability means that this algebra is conjugate into the Lie subalgebra of upper triangular matrices, and is equivalent to this algebra being a Lie subalgebra of a Borel subalgebra.
The basic result is that (over an algebraically closed field), the commuting matrices formula_19 or more generally formula_20 are simultaneously triangularizable. This can be proven by first showing that commuting matrices have a common eigenvector, and then inducting on dimension as before. This was proven by Frobenius, starting in 1878 for a commuting pair, as discussed at commuting matrices. As for a single matrix, over the complex numbers these can be triangularized by unitary matrices.
The fact that commuting matrices have a common eigenvector can be interpreted as a result of Hilbert's Nullstellensatz: commuting matrices form a commutative algebra formula_21 over formula_22 which can be interpreted as a variety in "k"-dimensional affine space, and the existence of a (common) eigenvalue (and hence a common eigenvector) corresponds to this variety having a point (being non-empty), which is the content of the (weak) Nullstellensatz. In algebraic terms, these operators correspond to an algebra representation of the polynomial algebra in "k" variables.
This is generalized by Lie's theorem, which shows that any representation of a solvable Lie algebra is simultaneously upper triangularisable, the case of commuting matrices being the abelian Lie algebra case, abelian being a fortiori solvable.
More generally and precisely, a set of matrices formula_20 is simultaneously triangularisable if and only if the matrix formula_24 is nilpotent for all polynomials "p" in "k" "non"-commuting variables, where formula_25 is the commutator; note that for commuting formula_26 the commutator vanishes so this holds. This was proven in ; a brief proof is given in . One direction is clear: if the matrices are simultaneously triangularisable, then formula_27 is "strictly" upper triangularizable (hence nilpotent), which is preserved by multiplication by any formula_28 or combination thereof – it will still have 0s on the diagonal in the triangularizing basis.
Generalizations.
Because the product of two upper triangular matrices is again upper triangular, the set of upper triangular matrices forms an algebra. Algebras of upper triangular matrices have a natural generalization in functional analysis which yields nest algebras on Hilbert spaces.
A non-square (or sometimes any) matrix with zeros above (below) the diagonal is called a lower (upper) trapezoidal matrix. The non-zero entries form the shape of a trapezoid.
Borel subgroups and Borel subalgebras.
The set of invertible triangular matrices of a given kind (upper or lower) forms a group, indeed a Lie group, which is a subgroup of the general linear group of all invertible matrices. Note that a triangular matrix is invertible precisely
when its diagonal entries are invertible (non-zero).
Over the real numbers, this group is disconnected, having formula_29 components accordingly as each diagonal entry is positive or negative. The identity component is invertible triangular matrices with positive entries on the diagonal, and the group of all invertible triangular matrices is a semidirect product of this group and diagonal entries with formula_30 on the diagonal, corresponding to the components.
The Lie algebra of the Lie group of invertible upper triangular matrices is the set of all upper triangular matrices, not necessarily invertible, and is a solvable Lie algebra. These are, respectively, the standard Borel subgroup "B" of the Lie group GLn and the standard Borel subalgebra formula_6 of the Lie algebra gln.
The upper triangular matrices are precisely those that stabilize the standard flag. The invertible ones among them form a subgroup of the general linear group, whose conjugate subgroups are those defined as the stabilizer of some (other) complete flag. These subgroups are Borel subgroups. The group of invertible lower triangular matrices is such a subgroup, since it is the stabilizer of the standard flag associated to the standard basis in reverse order.
The stabilizer of a partial flag obtained by forgetting some parts of the standard flag can be described as a set of block upper triangular matrices (but its elements are "not" all triangular matrices). The conjugates of such a group are the subgroups defined as the stabilizer of some partial flag. These subgroups are called parabolic subgroups.
Examples.
The group of 2 by 2 upper unitriangular matrices is isomorphic to the additive group of the field of scalars; in the case of complex numbers it corresponds to a group formed of parabolic Möbius transformations; the 3 by 3 upper unitriangular matrices form the Heisenberg group.
Forward and back substitution.
A matrix equation in the form formula_32 or formula_33 is very easy to solve by an iterative process called forward substitution for lower triangular matrices and analogously back substitution for upper triangular matrices.
The process is so called because for lower triangular matrices, one first computes formula_34, then substitutes that "forward" into the "next" equation to solve for formula_35, and repeats through to formula_36. In an upper triangular matrix, one works "backwards," first computing formula_36, then substituting that "back" into the "previous" equation to solve for formula_38, and repeating through formula_34.
Notice that this does not require inverting the matrix.
Forward substitution.
The matrix equation L"x" = "b" can be written as a system of linear equations
Observe that the first equation (formula_41) only involves formula_34, and thus one can solve for formula_34 directly. The second equation only involves formula_34 and formula_35, and thus can be solved once one substitutes in the already solved value for formula_34. Continuing in this way, the formula_47-th equation only involves formula_48, and one can solve for formula_49 using the previously solved values for formula_50.
The resulting formulas are:
A matrix equation with an upper triangular matrix U can be solved in an analogous way, only working backwards.
Algorithm.
The following is an example implementation of this algorithm in the C# programming language. Note that the algorithm performs poorly in C# due to the inefficient handling of non-jagged matrices in this language. Nonetheless, the method of forward and backward substitution "can" be highly efficient.
Applications.
Forward substitution is used in financial bootstrapping to construct a yield curve.

</doc>
<doc id="30367" url="https://en.wikipedia.org/wiki?curid=30367" title="Trigonometric functions">
Trigonometric functions

In mathematics, the trigonometric functions (also called the circular functions) are functions of an angle. They relate the angles of a triangle to the lengths of its sides. Trigonometric functions are important in the study of triangles and modeling periodic phenomena, among many other applications.
The most familiar trigonometric functions are the sine, cosine, and tangent. In the context of the standard unit circle (a circle with radius 1 unit), where a triangle is formed by a ray originating at the origin and making some angle with the "x"-axis, the sine of the angle gives the length of the "y"-component (the opposite to the angle or the rise) of the triangle, the cosine gives the length of the "x"-component (the adjacent of the angle or the run), and the tangent function gives the slope ("y"-component divided by the "x"-component). More precise definitions are detailed below. Trigonometric functions are commonly defined as ratios of two sides of a right triangle containing the angle, and can equivalently be defined as the lengths of various line segments from a unit circle. More modern definitions express them as infinite series or as solutions of certain differential equations, allowing their extension to arbitrary positive and negative values and even to complex numbers.
Trigonometric functions have a wide range of uses including computing unknown lengths and angles in triangles (often right triangles). In this use, trigonometric functions are used, for instance, in navigation, engineering, and physics. A common use in elementary physics is resolving a vector into Cartesian coordinates. The sine and cosine functions are also commonly used to model periodic function phenomena such as sound and light waves, the position and velocity of harmonic oscillators, sunlight intensity and day length, and average temperature variations through the year.
In modern usage, there are six basic trigonometric functions, tabulated here with equations that relate them to one another. Especially with the last four, these relations are often taken as the "definitions" of those functions, but one can define them equally well geometrically, or by other means, and then derive these relations.
Right-angled triangle definitions.
The notion that there should be some standard correspondence between the lengths of the sides of a triangle and the angles of the triangle comes as soon as one recognizes that similar triangles maintain the same ratios between their sides. That is, for any similar triangle the ratio of the hypotenuse (for example) and another of the sides remains the same. If the hypotenuse is twice as long, so are the sides. It is these ratios that the trigonometric functions express.
To define the trigonometric functions for the angle "A", start with any right triangle that contains the angle "A". The three sides of the triangle are named as follows:
In ordinary Euclidean geometry, according to the triangle postulate, the inside angles of every triangle total 180° (π radians). Therefore, in a right-angled triangle, the two non-right angles total 90° (π/2 radians), so each of these angles must be in the range of (0°,90°) as expressed in interval notation. The following definitions apply to angles in this 0° – 90° range. They can be extended to the full set of real arguments by using the unit circle, or by requiring certain symmetries and that they be periodic functions. For example, the figure shows sin "θ" for angles "θ", "π" − "θ", "π" + "θ", and 2"π" − "θ" depicted on the unit circle (top) and as a graph (bottom). The value of the sine repeats itself apart from sign in all four quadrants, and if the range of "θ" is extended to additional rotations, this behavior repeats periodically with a period 2"π".
The trigonometric functions are summarized in the following table and described in more detail below. The angle "θ" is the angle between the hypotenuse and the adjacent line – the angle at A in the accompanying diagram.
Sine, cosine and tangent.
The sine of an angle is the ratio of the length of the opposite side to the length of the hypotenuse. (The word comes from the Latin "sinus" for gulf or bay, since, given a unit circle, it is the side of the triangle on which the angle "opens".) In our case
This ratio does not depend on the size of the particular right triangle chosen, as long as it contains the angle "A", since all such triangles are similar.
The cosine of an angle is the ratio of the length of the adjacent side to the length of the hypotenuse: so called because it is the sine of the complementary or co-angle. In our case
The tangent of an angle is the ratio of the length of the opposite side to the length of the adjacent side: so called because it can be represented as a line segment tangent to the circle, that is the line that touches the circle, from Latin "linea tangens" or touching line (cf. "tangere", to touch). In our case
The acronyms "SOH-CAH-TOA" ("Soak-a-toe", "Sock-a-toa", "So-kah-toa") and "OHSAHCOAT" are commonly used mnemonics for these ratios.
Reciprocal functions.
The remaining three functions are best defined using the above three functions.
The cosecant csc("A"), or cosec("A"), is the reciprocal of sin("A"); i.e. the ratio of the length of the hypotenuse to the length of the opposite side; so called because it is the secant of the complementary or co-angle:
The secant sec("A") is the reciprocal of cos("A"); i.e. the ratio of the length of the hypotenuse to the length of the adjacent side:
It is so called because it represents the line that "cuts" the circle (from Latin: "secare", to cut).
The cotangent cot("A") is the reciprocal of tan("A"); i.e. the ratio of the length of the adjacent side to the length of the opposite side; so called because it is the tangent of the complementary or co-angle:
Slope definitions.
Equivalent to the right-triangle definitions, the trigonometric functions can also be defined in terms of the "rise", "run", and "slope" of a line segment relative to horizontal. The slope is commonly taught as "rise over run" or . The three main trigonometric functions are commonly taught in the order sine, cosine, tangent. With a line segment length of 1 (as in a unit circle), the following mnemonic devices show the correspondence of definitions:
This shows the main use of tangent and arctangent: converting between the two ways of telling the slant of a line, "i.e.," angles and slopes. (The arctangent or "inverse tangent" is not to be confused with the "cotangent," which is cosine divided by sine.)
While the length of the line segment makes no difference for the slope (the slope does not depend on the length of the slanted line), it does affect rise and run. To adjust and find the actual rise and run when the line does not have a length of 1, just multiply the sine and cosine by the line length. For instance, if the line segment has length 5, the run at an angle of 7° is 5 cos(7°)
Unit-circle definitions.
The six trigonometric functions can also be defined in terms of the unit circle, the circle of radius one centered at the origin. The unit circle relies on right triangles for most angles.
The unit circle definition does, however, permit the definition of the trigonometric functions for all positive and negative arguments, not just for angles between 0 and /2 radians.
From the Pythagorean theorem the equation for the unit circle is
Let a line through the origin, making an angle of "θ" with the positive half of the "x"-axis. The line intersects the unit circle at a point "P" whose "x"- and "y"-coordinates are cos "θ" and sin "θ".
Let us consider the right triangle whose vertexes are the point "P", the center of the circle "O", and the point "H" of the "x"-axis, that has the same "x"-coordinate as "P". The radius of the circle is equal to the hypotenuse "OP", and has length 1, so we have sin "θ" = "y"/1 and cos "θ" = "x"/1. The unit circle can be thought of as a way of looking at an infinite number of triangles by varying the lengths of their legs but keeping the lengths of their hypotenuses equal to 1.
The algebraic expressions for sin 0°, sin 30°, sin 45°, sin 60° and sin 90° are
respectively.
The algebraic expressions for 15°, 18°, 36°, 54°, 72°, and 75° are as follows:
From these, the algebraic expressions for all multiples of 3° can be computed. For example:
Though a complex task, the algebraic expression for sin 1° can be obtained by algebraically solving the cubic equation
although this solution involves a cube root of a complex number. From this solution one can algebraically derive the values of the trigonometric functions of all angles of integer degrees.
For angles greater than 2π or less than −2π, one simply continues to rotate around the circle; sine and cosine are periodic functions with period 2π:
for any angle θ and any integer "k".
The "smallest" positive period of a periodic function is called the "primitive period" of the function.
The primitive period of the sine or cosine is a full circle, i.e. 2π radians or 360 degrees.
Above, only sine and cosine were defined directly by the unit circle, but other trigonometric functions can be defined by:
So :
The image at right includes a graph of the tangent function.
Alternatively, "all" of the basic trigonometric functions can be defined in terms of a unit circle centered at "O" (as shown in the picture to the right), and similar such geometric definitions were used historically.
Series definitions.
Trigonometric functions are analytic functions. Using only geometry and properties of limits, it can be shown that the derivative of sine is cosine and the derivative of cosine is the negative of sine. (Here, and generally in calculus, all angles are measured in radians; see also the significance of radians below.) One can then use the theory of Taylor series to show that the following identities hold for all real numbers "x":
These identities are sometimes taken as the "definitions" of the sine and cosine function. They are often used as the starting point in a rigorous treatment of trigonometric functions and their applications (e.g., in Fourier series), since the theory of infinite series can be developed, independent of any geometric considerations, from the foundations of the real number system. The differentiability and continuity of these functions are then established from the series definitions alone. The value of can be defined as the smallest positive number for which sin = 0.
Other series can be found. For the following trigonometric functions:
Tangent.
When this series for the tangent function is expressed in a form in which the denominators are the corresponding factorials, the numerators, called the "tangent numbers", have a combinatorial interpretation: they enumerate alternating permutations of finite sets of odd cardinality. The series itself can be found by a power series solution of the aforementioned differential equation.
Secant.
When this series for the secant function is expressed in a form in which the denominators are the corresponding factorials, the numerators, called the "secant numbers", have a combinatorial interpretation: they enumerate alternating permutations of finite sets of even cardinality.
Cotangent.
From a theorem in complex analysis, there is a unique analytic continuation of this real function to the domain of complex numbers. They have the same Taylor series, and so the trigonometric functions are defined on the complex numbers using the Taylor series above.
There is a series representation as partial fraction expansion where just translated reciprocal functions are summed up, such that the poles of the cotangent function and the reciprocal functions match:
This identity can be proven with the Herglotz trick.
Combining the -th with the -th term lead to absolutely convergent series:
Relationship to exponential function and complex numbers.
It can be shown from the series definitions that the sine and cosine functions are the imaginary and real parts, respectively, of the complex exponential function when its argument is purely imaginary:
This identity is called Euler's formula. In this way, trigonometric functions become essential in the geometric interpretation of complex analysis. For example, with the above identity, if one considers the unit circle in the complex plane, parametrized by "e" "ix", and as above, we can parametrize this circle in terms of cosines and sines, the relationship between the complex exponential and the trigonometric functions becomes more apparent.
Euler's formula can also be used to derive some trigonometric identities, by writing sine and cosine as:
Furthermore, this allows for the definition of the trigonometric functions for complex arguments "z":
where "i" 2 = −1. The sine and cosine defined by this are entire functions. Also, for purely real "x",
It is also sometimes useful to express the complex sine and cosine functions in terms of the real and imaginary parts of their arguments.
This exhibits a deep relationship between the complex sine and cosine functions and their real ("sin", "cos") and hyperbolic real ("sinh", "cosh") counterparts.
Complex graphs.
In the following graphs, the domain is the complex plane pictured, and the range values are indicated at each point by color. Brightness indicates the size (absolute value) of the range value, with black being zero. Hue varies with argument, or angle, measured from the positive real axis. ()
Definitions via differential equations.
Both the sine and cosine functions satisfy the differential equation:
That is to say, each is the additive inverse of its own second derivative. Within the 2-dimensional function space "V" consisting of all solutions of this equation,
Since the sine and cosine functions are linearly independent, together they form a basis of "V". This method of defining the sine and cosine functions is essentially equivalent to using Euler's formula. (See linear differential equation.) It turns out that this differential equation can be used not only to define the sine and cosine functions but also to prove the trigonometric identities for the sine and cosine functions.
Further, the observation that sine and cosine satisfies "y"′′ = −"y" means that they are eigenfunctions of the second-derivative operator.
The tangent function is the unique solution of the nonlinear differential equation
satisfying the initial condition "y"(0) = 0. There is a very interesting visual proof that the tangent function satisfies this differential equation.
The significance of radians.
Radians specify an angle by measuring the length around the path of the unit circle and constitute a special argument to the sine and cosine functions. In particular, only sines and cosines that map radians to ratios satisfy the differential equations that classically describe them. If an argument to sine or cosine in radians is scaled by frequency,
then the derivatives will scale by "amplitude".
Here, "k" is a constant that represents a mapping between units. If "x" is in degrees, then
This means that the second derivative of a sine in degrees does not satisfy the differential equation
but rather
The cosine's second derivative behaves similarly.
This means that these sines and cosines are different functions, and that the fourth derivative of sine will be sine again only if the argument is in radians.
Identities.
Many identities interrelate the trigonometric functions. Among the most frequently used is the Pythagorean identity, which states that for any angle, the square of the sine plus the square of the cosine is 1. This is easy to see by studying a right triangle of hypotenuse 1 and applying the Pythagorean theorem. In symbolic form, the Pythagorean identity is written
where formula_50 is standard notation for formula_51
Other key relationships are the sum and difference formulas, which give the sine and cosine of the sum and difference of two angles in terms of sines and cosines of the angles themselves. These can be derived geometrically, using arguments that date to Ptolemy. One can also produce them algebraically using Euler's formula.
These in turn lead to the following three-angle formulae:
When the two angles are equal, the sum formulas reduce to simpler equations known as the double-angle formulae.
When three angles are equal, the three-angle formulae simplify to
These identities can also be used to derive the product-to-sum identities that were used in antiquity to transform the product of two numbers into a sum of numbers and greatly speed operations, much like the logarithm function.
Calculus.
For integrals and derivatives of trigonometric functions, see the relevant sections of Differentiation of trigonometric functions, Lists of integrals and List of integrals of trigonometric functions. Below is the list of the derivatives and integrals of the six basic trigonometric functions. The number "C" is a constant of integration.
Definitions using functional equations.
In mathematical analysis, one can define the trigonometric functions using functional equations based on properties like the difference formula. Taking as given these formulas, one can prove that only two real functions satisfy those conditions. Symbolically, we say that there exists exactly one pair of real functions — formula_62 and formula_63 — such that for all real numbers formula_64 and formula_65, the following equation hold:
with the added condition that
Other derivations, starting from other functional equations, are also possible, and such derivations can be extended to the complex numbers.
As an example, this derivation can be used to define trigonometry in Galois fields.
Computation.
The computation of trigonometric functions is a complicated subject, which can today be avoided by most people because of the widespread availability of computers and scientific calculators that provide built-in trigonometric functions for any angle. This section, however, describes details of their computation in three important contexts: the historical use of trigonometric tables, the modern techniques used by computers, and a few "important" angles where simple exact values are easily found.
The first step in computing any trigonometric function is range reduction—reducing the given angle to a "reduced angle" inside a small range of angles, say 0 to "π"/2, using the periodicity and symmetries of the trigonometric functions.
Prior to computers, people typically evaluated trigonometric functions by interpolating from a detailed table of their values, calculated to many significant figures. Such tables have been available for as long as trigonometric functions have been described (see History below), and were typically generated by repeated application of the half-angle and angle-addition identities starting from a known value (such as sin("π"/2) = 1).
Modern computers use a variety of techniques. One common method, especially on higher-end processors with floating point units, is to combine a polynomial or rational approximation (such as Chebyshev approximation, best uniform approximation, and Padé approximation, and typically for higher or variable precisions, Taylor and Laurent series) with range reduction and a table lookup—they first look up the closest angle in a small table, and then use the polynomial to compute the correction. Devices that lack hardware multipliers often use an algorithm called CORDIC (as well as related techniques), which uses only addition, subtraction, bitshift, and table lookup. These methods are commonly implemented in hardware floating-point units for performance reasons.
For very high precision calculations, when series expansion convergence becomes too slow, trigonometric functions can be approximated by the arithmetic-geometric mean, which itself approximates the trigonometric function by the (complex) elliptic integral.
Finally, for some simple angles, the values can be easily computed by hand using the Pythagorean theorem, as in the following examples. For example, the sine, cosine and tangent of any integer multiple of formula_68 radians (3°) can be found exactly by hand.
Consider a right triangle where the two other angles are equal, and therefore are both formula_69 radians (45°). Then the length of side "b" and the length of side "a" are equal; we can choose formula_70. The values of sine, cosine and tangent of an angle of formula_69 radians (45°) can then be found using the Pythagorean theorem:
Therefore:
To determine the trigonometric functions for angles of π/3 radians (60 degrees) and π/6 radians (30 degrees), we start with an equilateral triangle of side length 1. All its angles are π/3 radians (60 degrees). By dividing it into two, we obtain a right triangle with π/6 radians (30 degrees) and π/3 radians (60 degrees) angles. For this triangle, the shortest side = 1/2, the next largest side =(√3)/2 and the hypotenuse = 1. This yields:
Special values in trigonometric functions.
There are some commonly used special values in trigonometric functions, as shown in the following table.
The symbol formula_78 here represents the point at infinity on the real projective line, the limit on the extended real line is formula_79 on one side and formula_80 on the other.
Inverse functions.
The trigonometric functions are periodic, and hence not injective, so strictly they do not have an inverse function. Therefore, to define an inverse function we must restrict their domains so that the trigonometric function is bijective. In the following, the functions on the left are "defined" by the equation on the right; these are not proved identities. The principal inverses are usually defined as:
The notations sin−1 and cos−1 are often used for arcsin and arccos, etc. When this notation is used, the inverse functions could be confused with the multiplicative inverses of the functions. The notation using the "arc-" prefix avoids such confusion, though "arcsec" can be confused with "arcsecond".
Just like the sine and cosine, the inverse trigonometric functions can also be defined in terms of infinite series. For example,
These functions may also be defined by proving that they are antiderivatives of other functions. The arcsine, for example, can be written as the following integral:
Analogous formulas for the other functions can be found at Inverse trigonometric functions. Using the complex logarithm, one can generalize all these functions to complex arguments:
Connection to the inner product.
In an inner product space, the angle between two non-zero vectors is defined to be
Properties and applications.
The trigonometric functions, as the name suggests, are of crucial importance in trigonometry, mainly because of the following two results.
Law of sines.
The law of sines states that for an arbitrary triangle with sides "a", "b", and "c" and angles opposite those sides "A", "B" and "C":
where formula_88 is the area of the triangle,
or, equivalently,
where "R" is the triangle's circumradius.
It can be proven by dividing the triangle into two right ones and using the above definition of sine. The law of sines is useful for computing the lengths of the unknown sides in a triangle if two angles and one side are known. This is a common situation occurring in "triangulation", a technique to determine unknown distances by measuring two angles and an accessible enclosed distance.
Law of cosines.
The law of cosines (also known as the cosine formula or cosine rule) is an extension of the Pythagorean theorem:
or equivalently,
In this formula the angle at "C" is opposite to the side "c". This theorem can be proven by dividing the triangle into two right ones and using the Pythagorean theorem.
The law of cosines can be used to determine a side of a triangle if two sides and the angle between them are known. It can also be used to find the cosines of an angle (and consequently the angles themselves) if the lengths of all the sides are known.
Law of tangents.
The following all form the law of tangents
The explanation of the formulae in words would be cumbersome, but the patterns of sums and differences; for the lengths and corresponding opposite angles, are apparent in the theorem.
Law of cotangents.
If
(the radius of the inscribed circle for the triangle) and
(the semi-perimeter for the triangle), then the following all form the law of cotangents
It follows that
In words the theorem is: the cotangent of a half-angle equals the ratio of the semi-perimeter minus the opposite side to the said angle, to the inradius for the triangle.
Periodic functions.
The trigonometric functions are also important in physics. The sine and the cosine functions, for example, are used to describe simple harmonic motion, which models many natural phenomena, such as the movement of a mass attached to a spring and, for small angles, the pendular motion of a mass hanging by a string. The sine and cosine functions are one-dimensional projections of uniform circular motion.
Trigonometric functions also prove to be useful in the study of general periodic functions. The characteristic wave patterns of periodic functions are useful for modeling recurring phenomena such as sound or light waves.
Under rather general conditions, a periodic function "ƒ"("x") can be expressed as a sum of sine waves or cosine waves in a Fourier series. Denoting the sine or cosine basis functions by "φk", the expansion of the periodic function "ƒ"("t") takes the form:
For example, the square wave can be written as the Fourier series
In the animation of a square wave at top right it can be seen that just a few terms already produce a fairly good approximation. The superposition of several terms in the expansion of a sawtooth wave are shown underneath.
History.
While the early study of trigonometry can be traced to antiquity, the trigonometric functions as they are in use today were developed in the medieval period. The chord function was discovered by Hipparchus of Nicaea (180–125 BC) and Ptolemy of Roman Egypt (90–165 AD).
The functions sine and cosine can be traced to the "jyā" and "koti-jyā " functions used in Gupta period Indian astronomy ("Aryabhatiya", "Surya Siddhanta"), via translation from Sanskrit to Arabic and then from Arabic to Latin.
All six trigonometric functions in current use were known in Islamic mathematics by the 9th century, as was the law of sines, used in solving triangles.
al-Khwārizmī produced tables of sines, cosines and tangents.
They were studied by authors including Omar Khayyám, Bhāskara II, Nasir al-Din al-Tusi, Jamshīd al-Kāshī (14th century), Ulugh Beg (14th century), Regiomontanus (1464), Rheticus, and Rheticus' student Valentinus Otho.
Madhava of Sangamagrama (c. 1400) made early strides in the analysis of trigonometric functions in terms of infinite series.
The first published use of the abbreviations 'sin', 'cos', and 'tan' is by the 16th century French mathematician Albert Girard.
In a paper published in 1682, Leibniz proved that sin "x" is not an algebraic function of "x".
Leonhard Euler's "Introductio in analysin infinitorum" (1748) was mostly responsible for establishing the analytic treatment of trigonometric functions in Europe, also defining them as infinite series and presenting "Euler's formula", as well as the near-modern abbreviations "sin., cos., tang., cot., sec.," and "cosec."
A few functions were common historically, but are now seldom used, such as the chord (crd("θ") = 2 sin("θ"/2)), the versine (versin("θ") = 1 − cos("θ") = 2 sin2("θ"/2)) (which appeared in the earliest tables), the haversine (haversin("θ") = versin("θ") / 2 = sin2("θ"/2)), the exsecant (exsec("θ") = sec("θ") − 1) and the excosecant (excsc("θ") = exsec(π/2 − "θ") = csc("θ") − 1). Many more relations between these functions are listed in the article about trigonometric identities.
Etymology.
The word "sine" derives from Latin "sinus" ("bend", "bay", "the hanging fold of the upper part of a toga", "the bosom of a garment"). The use of "sinus" originates in twelfth-century European translations of the Arabic word "jaib" ("pocket" or "fold"). This was in turn based on a misreading of the Arabic written form "j-y-b", which itself originated as a transliteration from Sanskrit, of either "jyā" (the standard Sanskrit term for the sine) or the synonymous "jīvā" (both literally meaning "bowstring").
The word "tangent" comes from Latin "tangens" meaning "touching", since the line "touches" the circle of unit radius, whereas "secant" stems from Latin "secans" — "cutting" — since the line "cuts" the circle.
The prefix "co-" (in "cosine", "cotangent", "cosecant") is found in Edmund Gunter's "Canon triangulorum" (1620), which defines the "cosinus" as an abbreviation for the "sinus complementi" (sine of the complementary angle) and proceeds to define the "cotangens" similarly.

</doc>
<doc id="31429" url="https://en.wikipedia.org/wiki?curid=31429" title="Twin paradox">
Twin paradox

In physics, the twin paradox is a thought experiment in special relativity involving identical twins, one of whom makes a journey into space in a high-speed rocket and returns home to find that the twin who remained on Earth has aged more. This result appears puzzling because each twin sees the other twin as moving, and so, according to an incorrect naive application of time dilation and the principle of relativity, each should paradoxically find the other to have aged more slowly. However, this scenario can be resolved within the standard framework of special relativity: the travelling twin's trajectory involves two different inertial frames, one for the outbound journey and one for the inbound journey, and so there is no symmetry between the spacetime paths of the two twins. Therefore, the twin paradox is not a paradox in the sense of a logical contradiction.
Starting with Paul Langevin in 1911, there have been various explanations of this paradox. These explanations "can be grouped into those that focus on the effect of different standards of simultaneity in different frames, and those that designate the acceleration [experienced by the travelling twin] as the main reason...". Max von Laue argued in 1913 that since the traveling twin must be in two separate inertial frames, one on the way out and another on the way back, this frame switch is the reason for the aging difference, not the acceleration "per se". Explanations put forth by Albert Einstein and Max Born invoked gravitational time dilation to explain the aging as a direct effect of acceleration.
The twin paradox has been verified experimentally by precise measurements of atomic clocks flown in aircraft and satellites. For example, gravitational time dilation and special relativity together have been used to explain the Hafele–Keating experiment. It was also confirmed in particle accelerators by measuring time dilation of circulating particle beams.
History.
In his famous work on special relativity in 1905, Albert Einstein predicted that when two clocks were brought together and synchronized, and then one was moved away and brought back, the clock which had undergone the traveling would be found to be lagging behind the clock which had stayed put. Einstein considered this to be a natural consequence of special relativity, not a paradox as some suggested, and in 1911, he restated and elaborated on this result as follows (with physicist Robert Resnick's comments following Einstein's):
In 1911, Paul Langevin gave a "striking example" by describing the story of a traveler making a trip at a Lorentz factor of (99.995% the speed of light). The traveler remains in a projectile for one year of his time, and then reverses direction. Upon return, the traveler will find that he has aged two years, while 200 years have passed on Earth. During the trip, both the traveler and Earth keep sending signals to each other at a constant rate, which places Langevin's story among the Doppler shift versions of the twin paradox. The relativistic effects upon the signal rates are used to account for the different aging rates. The asymmetry that occurred because only the traveler underwent acceleration, is used to explain why there is any difference at all, because "any change of velocity, or any acceleration has an absolute meaning".
Max von Laue (1911, 1913) elaborated on Langevin's explanation. Using Minkowski's spacetime formalism, Laue went on to demonstrate that the world lines of the inertially moving bodies maximize the proper time elapsed between two events. He also wrote that the asymmetric aging is completely accounted for by the fact that the astronaut twin travels in two separate frames, while the Earth twin remains in one frame, and the time of acceleration can be made arbitrarily small compared with the time of inertial motion. Eventually, Lord Halsbury and others removed any acceleration by introducing the "three-brother" approach. The traveling twin transfers his clock reading to a third one, traveling in the opposite direction. Another way of avoiding acceleration effects is the use of the relativistic Doppler effect (see below).
Neither Einstein nor Langevin considered such results to be problematic: Einstein only called it "peculiar" while Langevin presented it as a consequence of absolute acceleration. Both men argued that, from the time differential illustrated by the story of the twins, no self-contradiction could be constructed. In other words, neither Einstein nor Langevin saw the story of the twins as constituting a challenge to the self-consistency of relativistic physics.
Specific example.
Consider a space ship traveling from Earth to the nearest star system: a distance years away, at a speed (i.e., 80 percent of the speed of light).
The parties will observe the situation as follows:
The Earth-based mission control reasons about the journey this way: the round trip will take in Earth time ("i.e." everybody on Earth will be 10 years older when the ship returns). The amount of time as measured on the ship's clocks and the aging of the travelers during their trip will be reduced by the factor formula_1, the reciprocal of the Lorentz factor. In this case and the travelers will have aged only when they return.
The ship's crew members also calculate the particulars of their trip from their perspective. They know that the distant star system and the Earth are moving relative to the ship at speed "v" during the trip. In their rest frame the distance between the Earth and the star system is years (length contraction), for both the outward and return journeys. Each half of the journey takes , and the round trip takes . Their calculations show that they will arrive home having aged 6 years. The travelers' final calculation is in complete agreement with the calculations of those on Earth, though they experience the trip quite differently from those who stay at home.
If twins are born on the day the ship leaves, and one goes on the journey while the other stays on Earth, they will meet again when the traveler is 6 years old and the stay-at-home twin is 10 years old. The calculation illustrates the usage of the phenomenon of length contraction and the experimentally verified phenomenon of time dilation to describe and calculate consequences and predictions of Einstein's special theory of relativity.
Resolution of the paradox in special relativity.
The paradoxical aspect of the twins' situation arises from the fact that at any given moment the travelling twin's clock is running slow in the earthbound twin's inertial frame, but equally the earthbound twin's clock is running slow in the travelling twin's inertial frame. The resolution is that the earthbound twin is in the same inertial frame throughout the journey, but the travelling twin is not: in the simplest version of the thought-experiment the travelling twin switches at the midpoint of the trip from being at rest in an inertial frame with velocity in one direction (away from the earth) to being at rest in an inertial frame with velocity in the opposite direction (towards the earth).
Role of acceleration.
Although some texts assign a crucial role to the acceleration of the travelling twin at the time of the turnaround, others note that the effect also arises if one imagines separate outward-going and inward-coming travellers, who pass each other and synchronize their clocks at the point corresponding to "turnaround" of a single traveller. In this version, acceleration plays no direct role; "the issue is how long the world-lines are, not how bent". The length referred to here is the Lorentz-invariant length or "proper time interval" of a trajectory which corresponds to the elapsed time measured by a clock following that trajectory (see Section Difference in elapsed time as a result of differences in twins' spacetime paths below). In Minkowski spacetime, the travelling twin must feel a different history of accelerations from the earthbound twin, even if this just means accelerations of the same size separated by different amounts of time, however "even this role for acceleration can be eliminated in formulations of the twin paradox in curved spacetime, where the twins can fall freely along space-time geodesics between meetings".
Relativity of simultaneity.
For a moment-by-moment understanding of how the time difference between the two twins unfolds, one must understand that in special relativity there is no concept of "absolute present". For different inertial frames there are different sets of events that are simultaneous in that frame. This relativity of simultaneity means that switching from one inertial frame to another requires an adjustment in what slice through spacetime counts as the "present". In the spacetime diagram on the right, drawn for the reference frame of the Earth-based twin, that twin's world line coincides with the vertical axis (his position is constant in space, moving only in time). On the first leg of the trip, the second twin moves to the right (black sloped line); and on the second leg, back to the left. Blue lines show the planes of simultaneity for the traveling twin during the first leg of the journey; red lines, during the second leg. Just before turnaround, the traveling twin calculates the age of the Earth-based twin by measuring the interval along the vertical axis from the origin to the upper blue line. Just after turnaround, if he recalculates, he will measure the interval from the origin to the lower red line. In a sense, during the U-turn the plane of simultaneity jumps from blue to red and very quickly sweeps over a large segment of the world line of the Earth-based twin. When one transfers from the outgoing inertial frame to the incoming inertial frame there is a jump discontinuity in the age of the Earth-based twin.
A non space-time approach.
As mentioned above, an "out and back" twin paradox adventure may incorporate the transfer of clock reading from an "outgoing" astronaut to an "incoming" astronaut, thus entirely eliminating the effect of acceleration. Acceleration is not involved in any kinematical effects of special relativity. The time differential between two reunited clocks is deduced through purely uniform linear motion considerations, as seen in Einstein's original paper on the subject, as well as in all subsequent derivations of the Lorentz transformations.
Because spacetime diagrams incorporate Einstein's clock synchronization (with its lattice of clocks methodology), there will be a requisite "jump in time" in the calculation made by a "suddenly returning astronaut" who inherits a "new meaning of simultaneity" in keeping with a new clock synchronization dictated by the reversal of motion as explained in Spacetime Physics by John A. Wheeler.
If, instead of incorporating Einstein's clock synchronization (lattice of clocks), the astronaut (outgoing and incoming) and the Earth-based party regularly update each other on the status of their clocks by way of sending radio signals (which travel at light speed), then all parties will note an incremental buildup of asymmetry in time-keeping, beginning at the "turn around" point. Prior to the "turn around", each party regards the other party's clock to be recording time differently from his own, but the noted difference is symmetrical between the two parties. After the "turn around", the noted differences are not symmetrical, and the asymmetry grows incrementally until the two parties are reunited. Upon finally reuniting, this asymmetry can be seen in the actual difference showing on the two reunited clocks.
The equivalence of biological aging and clock time-keeping.
It would probably be prudent to mention: All processes—chemical, biological, measuring apparatus functioning, human perception involving the eye and brain, the communication of force—everything, is constrained by the speed of light. There is clock functioning at every level, dependent on light speed and the inherent delay at even the atomic level. Thus, we speak of the "twin paradox", involving biological aging. It is in no way different from clock time-keeping. Biological aging is equated to clock time-keeping by John A. Wheeler in "Spacetime Physics".
What it looks like: the relativistic Doppler shift.
In view of the frame-dependence of simultaneity for events at different locations in space, some treatments prefer a more phenomenological approach, describing what the twins would observe if each sent out a series of regular radio pulses, equally spaced in time according to the emitter's clock. This is equivalent to asking, if each twin sent a video feed of themselves to each other, what do they see in their screens? Or, if each twin always carried a clock indicating his age, what time would each see in the image of their distant twin and his clock?
Shortly after departure, the traveling twin sees the stay-at-home twin with no time delay. At arrival, the image in the ship screen shows the staying twin as he was 1 year after launch, because radio emitted from Earth 1 year after launch gets to the other star 4 years afterwards and meets the ship there. During this leg of the trip, the traveling twin sees his own clock advance 3 years and the clock in the screen advance 1 year, so it seems to advance at the normal rate, just 20 image seconds per ship minute. This combines the effects of time dilation due to motion (by factor ε=0.6, five years on earth are 3 years on ship) and the effect of increasing light-time-delay (which grows from 0 to 4 years).
Of course, the observed frequency of the transmission is also the frequency of the transmitter (a reduction in frequency; "red-shifted"). This is called the relativistic Doppler effect. The frequency of clock-ticks (or of wavefronts) which one sees from a source with rest frequency "f"rest is
when the source is moving directly away. This is "f"obs = "f"rest for "v"/"c" = 0.8.
As for the stay-at-home twin, he gets a slowed signal from the ship for 9 years, at a frequency the transmitter frequency. During these 9 years, the clock of the traveling twin in the screen seems to advance 3 years, so both twins see the image of their sibling aging at a rate only their own rate. Expressed in other way, they would both see the other's clock run at their own clock speed. If they factor out of the calculation the fact that the light-time delay of the transmission is increasing at a rate of 0.8 seconds per second, "both" can work out that the other twin is aging slower, at 60% rate.
Then the ship turns back toward home. The clock of the staying twin shows "1 year after launch" in the screen of the ship, and during the 3 years of the trip back it increases up to "10 years after launch", so the clock in the screen seems to be advancing 3 times faster than usual.
When the source is moving towards the observer, the observed frequency is higher ("blue-shifted") and given by
This is "f"obs = 3"f"rest for "v"/"c" = 0.8.
As for the screen on Earth, it shows that trip back beginning 9 years after launch, and the traveling clock in the screen shows that 3 years have passed on the ship. One year later, the ship is back home and the clock shows 6 years. So, during the trip back, "both" twins see their sibling's clock going 3 times faster than their own. Factoring out the fact that the light-time-delay is decreasing by 0.8 seconds every second, each twin calculates that the other twin is aging at 60% his own aging speed.
The "x"–"t" (space–time) diagrams at left show the paths of light signals traveling between Earth and ship (1st diagram) and between ship and Earth (2nd diagram). These signals carry the images of each twin and his age-clock to the other twin. The vertical black line is the Earth's path through spacetime and the other two sides of the triangle show the ship's path through spacetime (as in the Minkowski diagram above). As far as the sender is concerned, he transmits these at equal intervals (say, once an hour) according to his own clock; but according to the clock of the twin receiving these signals, they are not being received at equal intervals.
After the ship has reached its cruising speed of 0.8"c", each twin would see 1 second pass in the received image of the other twin for every 3 seconds of his own time. That is, each would see the image of the other's clock going slow, not just slow by the "ε" factor 0.6, but even slower because light-time-delay is increasing 0.8 seconds per second. This is shown in the figures by red light paths. At some point, the images received by each twin change so that each would see 3 seconds pass in the image for every second of his own time. That is, the received signal has been increased in frequency by the Doppler shift. These high frequency images are shown in the figures by blue light paths.
The asymmetry in the Doppler shifted images.
The asymmetry between the Earth and the space ship is manifested in this diagram by the fact that more blue-shifted (fast aging) images are received by the ship. Put another way, the space ship sees the image change from a red-shift (slower aging of the image) to a blue-shift (faster aging of the image) at the midpoint of its trip (at the turnaround, 5 years after departure); the Earth sees the image of the ship change from red-shift to blue shift after 9 years (almost at the end of the period that the ship is absent). In the next section, one will see another asymmetry in the images: the Earth twin sees the ship twin age by the same amount in the red and blue shifted images; the ship twin sees the Earth twin age by different amounts in the red and blue shifted images.
Calculation of elapsed time from the Doppler diagram.
The twin on the ship sees low frequency (red) images for 3 years. During that time, he would see the Earth twin in the image grow older by . He then sees high frequency (blue) images during the back trip of 3 years. During that time, he would see the Earth twin in the image grow older by When the journey is finished, the image of the Earth twin has aged by 
The Earth twin sees 9 years of slow (red) images of the ship twin, during which the ship twin ages (in the image) by He then sees fast (blue) images for the remaining 1 year until the ship returns. In the fast images, the ship twin ages by The total aging of the ship twin in the images received by Earth is , so the ship twin returns younger (6 years as opposed to 10 years on Earth).
The distinction between what they see and what they calculate.
To avoid confusion, note the distinction between what each twin sees and what each would calculate. Each sees an image of his twin which he knows originated at a previous time and which he knows is Doppler shifted. He does not take the elapsed time in the image as the age of his twin now.
when the image was emitted. A similar calculation reveals that his twin was aging at the same reduced rate of "εf"rest in all low frequency images.
Simultaneity in the Doppler shift calculation.
It may be difficult to see where simultaneity came into the Doppler shift calculation, and indeed the calculation is often preferred because one does not have to worry about simultaneity. As seen above, the ship twin can convert his received Doppler-shifted rate to a slower rate of the clock of the distant clock for both red and blue images. If he ignores simultaneity, he might say his twin was aging at the reduced rate throughout the journey and therefore should be younger than he is. He is now back to square one, and has to take into account the change in his notion of simultaneity at the turnaround. The rate he can calculate for the image (corrected for Doppler effect) is the rate of the Earth twin's clock at the moment it was sent, not at the moment it was received. Since he receives an unequal number of red and blue shifted images, he should realize that the red and blue shifted emissions were not emitted over equal time periods for the Earth twin, and therefore he must account for simultaneity at a distance.
Viewpoint of the traveling twin.
During the turnaround, the traveling twin is in an accelerated reference frame. According to the equivalence principle, the traveling twin may analyze the turnaround phase as if the stay-at-home twin were freely falling in a gravitational field and as if the traveling twin were stationary. A 1918 paper by Einstein presents a conceptual sketch of the idea. From the viewpoint of the traveler, a calculation for each separate leg, ignoring the turnaround, leads to a result in which the Earth clocks age less than the traveler. For example, if the Earth clocks age 1 day less on each leg, the amount that the Earth clocks will lag behind amounts to 2 days. The physical description of what happens at turnaround has to produce a contrary effect of double that amount: 4 days' advancing of the Earth clocks. Then the traveler's clock will end up with a net 2-day delay on the Earth clocks, in agreement with calculations done in the frame of the stay-at-home twin.
The mechanism for the advancing of the stay-at-home twin's clock is gravitational time dilation. When an observer finds that inertially moving objects are being accelerated with respect to themselves, those objects are in a gravitational field insofar as relativity is concerned. For the traveling twin at turnaround, this gravitational field fills the universe. In a weak field approximation, clocks tick at a rate of where "Φ" is the difference in gravitational potential. In this case, where "g" is the acceleration of the traveling observer during turnaround and "h" is the distance to the stay-at-home twin. The rocket is firing towards the stay-at-home twin, thereby placing that twin at a higher gravitational potential. Due to the large distance between the twins, the stay-at-home twin's clocks will appear to be sped up enough to account for the difference in proper times experienced by the twins. It is no accident that this speed-up is enough to account for the simultaneity shift described above. The general relativity solution for a static homogeneous gravitational field and the special relativity solution for finite acceleration produce identical results.
Other calculations have been done for the traveling twin (or for any observer who sometimes accelerates), which do not involve the equivalence principle, and which do not involve any gravitational fields. Such calculations are based only on the special theory, not the general theory, of relativity. One approach calculates surfaces of simultaneity by considering light pulses, in accordance with Hermann Bondi's idea of the k-calculus. A second approach calculates a straightforward but technically complicated integral to determine how the traveling twin measures the elapsed time on the stay-at-home clock. An outline of this second approach is given in a .
Difference in elapsed time as a result of differences in twins' spacetime paths.
The following paragraph shows several things:
Let clock "K" be associated with the "stay at home twin".
Let clock K' be associated with the rocket that makes the trip.
At the departure event both clocks are set to 0.
Knowing that the clock "K" remains inertial (stationary), the total accumulated proper time Δ"τ" of clock K' will be given by the integral function of coordinate time Δ"t"
where "v"("t") is the "coordinate velocity" of clock K' as a function of "t" according to clock "K", and, e.g. during phase 1, given by
This integral can be calculated for the 6 phases:
where "a" is the proper acceleration, felt by clock K' during the acceleration phase(s) and where the following relations hold between "V", "a" and "T"a:
So the traveling clock K' will show an elapsed time of
which can be expressed as
whereas the stationary clock "K" shows an elapsed time of
which is, for every possible value of "a", "T"a, "T"c and "V", larger than the reading of clock K':
Difference in elapsed times: how to calculate it from the ship.
In the standard proper time formula
Δ"τ" represents the time of the non-inertial (travelling) observer K' as a function of the elapsed time Δ"t" of the inertial (stay-at-home) observer "K" for whom observer K' has velocity "v"("t") at time "t".
To calculate the elapsed time Δ"t" of the inertial observer "K" as a function of the elapsed time Δ"τ" of the non-inertial observer K', where only quantities measured by K' are accessible, the following formula can be used:
where "a(τ)" is the proper acceleration of the non-inertial observer K' as measured by himself (for instance with an accelerometer) during the whole round-trip. The Cauchy–Schwarz inequality can be used to show that the inequality follows from the previous expression:
Using the Dirac delta function to model the infinite acceleration phase in the standard case of the traveller having constant speed "v" during the outbound and the inbound trip, the formula produces the known result:
In the case where the accelerated observer K' departs from "K" with zero initial velocity, the general equation reduces to the simpler form:
which, in the "smooth" version of the twin paradox where the traveller has constant proper acceleration phases, successively given by "a", −"a", −"a", "a", results in
where the convention "c" = 1 is used, in accordance with the above expression with acceleration phases and inertial (coasting) phases 
A rotational version.
Twins Bob and Alice inhabit a space station in circular orbit around a massive body in space. Bob suits up and exits the station. While Alice remains inside the station, continuing to orbit with it as before, Bob uses a rocket propulsion system to cease orbiting and hover where he was. When the station completes an orbit and returns to Bob, he rejoins Alice. Alice is now younger than Bob. In addition to rotational acceleration, Bob must decelerate to become stationary and then accelerate again to match the orbital speed of the space station.
Explanation in terms of Mach's principle.
A minority of physicists also favor some version of Mach's principle, which would imply that the difference between accelerated motion and inertial motion can only be defined relative to the rest of the matter in the universe, often referred to as motion relative to the "fixed stars". For example, A.P. French writes, regarding the twin paradox: "Note, though, that we are appealing to the reality of A's acceleration, and to the observability of the inertial forces associated with it. Would such effects as the twin paradox exist if the framework of fixed stars and distant galaxies were not there? Most physicists would say no. Our ultimate definition of an inertial frame may indeed be that it is a frame having zero acceleration with respect to the matter of the universe at large."
Further reading.
The "ideal clock" is a clock whose action depends only on its instantaneous velocity, and is independent of any acceleration of the clock. 

</doc>
<doc id="31883" url="https://en.wikipedia.org/wiki?curid=31883" title="Uncertainty principle">
Uncertainty principle

In quantum mechanics, the uncertainty principle, also known as Heisenberg's uncertainty principle, is any of a variety of mathematical inequalities asserting a fundamental limit to the precision with which certain pairs of physical properties of a particle, known as complementary variables, such as position "x" and momentum "p", can be known simultaneously. 
Introduced first in 1927, by the German physicist Werner Heisenberg, it states that the more precisely the position of some particle is determined, the less precisely its momentum can be known, and vice versa. The formal inequality relating the standard deviation of position "σx" and the standard deviation of momentum "σp" was derived by Earle Hesse Kennard later that year and by Hermann Weyl in 1928:
( is the reduced Planck constant, / ).
Historically, the uncertainty principle has been confused with a somewhat similar effect in physics, called the observer effect, which notes that measurements of certain systems cannot be made without affecting the systems. Heisenberg offered such an observer effect at the quantum level (see below) as a physical "explanation" of quantum uncertainty. It has since become clear, however, that the uncertainty principle is inherent in the properties of all wave-like systems, and that it arises in quantum mechanics simply due to the matter wave nature of all quantum objects. Thus, "the uncertainty principle actually states a fundamental property of quantum systems, and is not a statement about the observational success of current technology". It must be emphasized that "measurement" does not mean only a process in which a physicist-observer takes part, but rather any interaction between classical and quantum objects regardless of any observer.
Since the uncertainty principle is such a basic result in quantum mechanics, typical experiments in quantum mechanics routinely observe aspects of it. Certain experiments, however, may deliberately test a particular form of the uncertainty principle as part of their main research program. These include, for example, tests of number–phase uncertainty relations in superconducting or quantum optics systems. Applications dependent on the uncertainty principle for their operation include extremely low noise technology such as that required in gravitational-wave interferometers.
Introduction.
As a fundamental constraint, higher level descriptions of the universe must supervene on quantum mechanical descriptions which includes Heisenberg's uncertainty relationship. However, humans do not form an intuitive understanding of this uncertainty principle in everyday life. This is because the constraint is not readily apparent on the macroscopic scales of everyday experience. So it may be helpful to demonstrate how it is integral to more easily understood physical situations. Two alternative conceptualizations of quantum physics can be examined with the goal of demonstrating the key role the uncertainty principle plays. A wave mechanics picture of the uncertainty principle provides for a more visually intuitive demonstration, and the somewhat more abstract matrix mechanics picture provides for a demonstration of the uncertainty principle that is more easily generalized to cover a multitude of physical contexts.
Mathematically, in wave mechanics, the uncertainty relation between position and momentum arises because the expressions of the wavefunction in the two corresponding orthonormal bases in Hilbert space are Fourier transforms of one another (i.e., position and momentum are conjugate variables). A nonzero function and its Fourier transform cannot both be sharply localized. A similar tradeoff between the variances of Fourier conjugates arises in all systems underlain by Fourier analysis, for example in sound waves: A pure tone is a sharp spike at a single frequency, while its Fourier transform gives the shape of the sound wave in the time domain, which is a completely delocalized sine wave. In quantum mechanics, the two key points are that the position of the particle takes the form of a matter wave, and momentum is its Fourier conjugate, assured by the de Broglie relation , where is the wavenumber.
In matrix mechanics, the mathematical formulation of quantum mechanics, any pair of non-commuting self-adjoint operators representing observables are subject to similar uncertainty limits. An eigenstate of an observable represents the state of the wavefunction for a certain measurement value (the eigenvalue). For example, if a measurement of an observable is performed, then the system is in a particular eigenstate of that observable. However, the particular eigenstate of the observable need not be an eigenstate of another observable : If so, then it does not have a unique associated measurement for it, as the system is not in an eigenstate of that observable.
Wave mechanics interpretation.
According to the de Broglie hypothesis, every object in the universe is a wave, a situation which gives rise to this phenomenon. The position of the particle is described by a wave function formula_1. The time-independent wave function of a single-moded plane wave of wavenumber "k"0 or momentum "p"0 is
The Born rule states that this should be interpreted as a probability density function in the sense that the probability of finding the particle between "a" and "b" is
In the case of the single-moded plane wave, formula_4 is a uniform distribution. In other words, the particle position is extremely uncertain in the sense that it could be essentially anywhere along the wave packet. Consider a wave function that is a sum of many waves, however, we may write this as
where "A""n" represents the relative contribution of the mode "p""n" to the overall total. The figures to the right show how with the addition of many plane waves, the wave packet can become more localized. We may take this a step further to the continuum limit, where the wave function is an integral over all possible modes
with formula_7 representing the amplitude of these modes and is called the wave function in momentum space. In mathematical terms, we say that formula_7 is the "Fourier transform" of formula_9 and that "x" and "p" are conjugate variables. Adding together all of these plane waves comes at a cost, namely the momentum has become less precise, having become a mixture of waves of many different momenta.
One way to quantify the precision of the position and momentum is the standard deviation σ. Since formula_4 is a probability density function for position, we calculate its standard deviation.
The precision of the position is improved, i.e. reduced σx, by using many plane waves, thereby weakening the precision of the momentum, i.e. increased σp. Another way of stating this is that σx and σp have an inverse relationship or are at least bounded from below. This is the uncertainty principle, the exact limit of which is the Kennard bound. Click the "show" button below to see a semi-formal derivation of the Kennard inequality using wave mechanics.
Matrix mechanics interpretation.
(Ref ) 
In matrix mechanics, observables such as position and momentum are represented by self-adjoint operators. When considering pairs of observables, an important quantity is the "commutator". For a pair of operators and , one defines their commutator as
In the case of position and momentum, the commutator is the canonical commutation relation
The physical meaning of the non-commutativity can be understood by considering the effect of the commutator on position and momentum eigenstates. Let formula_13 be a right eigenstate of position with a constant eigenvalue . By definition, this means that formula_14 Applying the commutator to formula_13 yields
where is the identity operator.
Suppose, for the sake of proof by contradiction, that formula_13 is also a right eigenstate of momentum, with constant eigenvalue . If this were true, then one could write
On the other hand, the above canonical commutation relation requires that
This implies that no quantum state can simultaneously be both a position and a momentum eigenstate.
When a state is measured, it is projected onto an eigenstate in the basis of the relevant observable. For example, if a particle's position is measured, then the state amounts to a position eigenstate. This means that the state is "not" a momentum eigenstate, however, but rather it can be represented as a sum of multiple momentum basis eigenstates. In other words, the momentum must be less precise. This precision may be quantified by the standard deviations, 
As in the wave mechanics interpretation above, one sees a tradeoff between the respective precisions of the two, quantified by the uncertainty principle.
Robertson–Schrödinger uncertainty relations.
The most common general form of the uncertainty principle is the "Robertson uncertainty relation".
For an arbitrary Hermitian operator formula_22 we can associate a standard deviation
where the brackets formula_24 indicate an expectation value. For a pair of operators and , we may define their "commutator" as
In this notation, the Robertson uncertainty relation is given by
The Robertson uncertainty relation immediately follows from a slightly stronger inequality, the "Schrödinger uncertainty relation",
where we have introduced the "anticommutator",
we let formula_28 and formula_29 and substitute these into the equation above to get
The inner product formula_30 is written out explicitly as
and using the fact that formula_32 and formula_33 are Hermitian operators, we find
Similarly it can be shown that
formula_39
Thus we have
and
We now substitute the above two equations above back into Eq. () and get
Substituting the above into Eq. () we get the Schrödinger uncertainty relation
This proof has an issue related to the domains of the operators involved. For the proof to make sense, the vector formula_44 has to be in the domain of the unbounded operator formula_45, which is not always the case. In fact, the Robertson uncertainty relation is false if formula_32 is an angle variable and formula_33 is the derivative with respect to this variable. In this example, the commutator is a nonzero constant—just as in the Heisenberg uncertainty relation—and yet there are states where the product of the uncertainties is zero. This issue can be overcome by using a variational method for the proof., or by working with an exponentiated version of the canonical commutation relations.
Note that in the general form of the Robertson–Schrödinger uncertainty relation, there is no need to assume that the operators formula_32 and formula_33 are self-adjoint operators. It suffices to assume that they are merely symmetric operators. (The distinction between these two notions is generally glossed over in the physics literature, where the term "Hermitian" is used for either or both classes of operators. See Chapter 9 of Hall's book for a detailed discussion of this important but technical distinction.)
Since the Robertson and Schrödinger relations are for general operators, the relations can be applied to any two observables to obtain specific uncertainty relations. A few of the most common relations found in the literature are given below.
Examples.
Quantum harmonic oscillator stationary states.
Consider a one-dimensional quantum harmonic oscillator (QHO). It is possible to express the position and momentum operators in terms of the creation and annihilation operators:
Using the standard rules for creation and annihilation operators on the eigenstates of the QHO,
the variances may be computed directly,
The product of these standard deviations is then
In particular, the above Kennard bound is saturated for the ground state , for which the probability density is just the normal distribution.
Quantum harmonic oscillator with Gaussian initial condition.
In a quantum harmonic oscillator of characteristic angular frequency ω, place a state that is offset from the bottom of the potential by some displacement "x"0 as
where Ω describes the width of the initial state but need not be the same as ω. Through integration over the , we can solve for the -dependent solution. After many cancelations, the probability densities reduce to
where we have used the notation formula_70 to denote a normal distribution of mean μ and variance σ2. Copying the variances above and applying trigonometric identities, we can write the product of the standard deviations as
From the relations
we can conclude
Coherent states.
A coherent state is a right eigenstate of the annihilation operator,
which may be represented in terms of Fock states as
One expects that the factor may be replaced by , 
which is only known if either or is convex.
Hardy's uncertainty principle.
The mathematician G. H. Hardy formulated the following uncertainty principle: it is not possible for and to both be "very rapidly decreasing." Specifically, if in is such that
and
then, if , while if , then there is a polynomial of degree such that
This was later improved as follows: if is such that
then
where is a polynomial of degree and is a real positive definite matrix.
This result was stated in Beurling's complete works without proof and proved in Hörmander (the case formula_82) and Bonami, Demange, and Jaming for the general case. Note that Hörmander–Beurling's version implies the case in Hardy's Theorem while the version by Bonami–Demange–Jaming covers the full strength of Hardy's Theorem. A different proof of Beurling's theorem based on Liouville's theorem appeared in
ref.
A full description of the case as well as the following extension to Schwarz class distributions appears in ref.
Theorem. If a tempered distribution formula_83 is such that
and
then
for some convenient polynomial and real positive definite matrix of type .
History.
Werner Heisenberg formulated the Uncertainty Principle at Niels Bohr's institute in Copenhagen, while working on the mathematical foundations of quantum mechanics.
In 1925, following pioneering work with Hendrik Kramers, Heisenberg developed matrix mechanics, which replaced the ad hoc old quantum theory with modern quantum mechanics. The central premise was that the classical concept of motion does not fit at the quantum level, as electrons in an atom do not travel on sharply defined orbits. Rather, their motion is smeared out in a strange way: the Fourier transform of its time dependence only involves those frequencies that could be observed in the quantum jumps of their radiation.
Heisenberg's paper did not admit any unobservable quantities like the exact position of the electron in an orbit at any time; he only allowed the theorist to talk about the Fourier components of the motion. Since the Fourier components were not defined at the classical frequencies, they could not be used to construct an exact trajectory, so that the formalism could not answer certain overly precise questions about where the electron was or how fast it was going.
In March 1926, working in Bohr's institute, Heisenberg realized that the non-commutativity implies the uncertainty principle. This implication provided a clear physical interpretation for the non-commutativity, and it laid the foundation for what became known as the Copenhagen interpretation of quantum mechanics. Heisenberg showed that the commutation relation implies an uncertainty, or in Bohr's language a complementarity. Any two variables that do not commute cannot be measured simultaneously—the more precisely one is known, the less precisely the other can be known. Heisenberg wrote:It can be expressed in its simplest form as follows: One can never know with perfect accuracy both of those two important factors which determine the movement of one of the smallest particles—its position and its velocity. It is impossible to determine accurately "both" the position and the direction and speed of a particle "at the same instant".
In his celebrated 1927 paper, "Über den anschaulichen Inhalt der quantentheoretischen Kinematik und Mechanik" ("On the Perceptual Content of Quantum Theoretical Kinematics and Mechanics"), Heisenberg established this expression as the minimum amount of unavoidable momentum disturbance caused by any position measurement, but he did not give a precise definition for the uncertainties Δx and Δp. Instead, he gave some plausible estimates in each case separately. In his Chicago lecture he refined his principle:
Kennard in 1927 first proved the modern inequality:
where , and , are the standard deviations of position and momentum. Heisenberg only proved relation () for the special case of Gaussian states.
Terminology and translation.
Throughout the main body of his original 1927 paper, written in German, Heisenberg used the word, "Ungenauigkeit" ("indeterminacy"),
to describe the basic theoretical principle. Only in the endnote did he switch to the word, "Unsicherheit" ("uncertainty"). When the English-language version of Heisenberg's textbook, "The Physical Principles of the Quantum Theory", was published in 1930, however, the translation "uncertainty" was used, and it became the more commonly used term in the English language thereafter.
Heisenberg's microscope.
The principle is quite counter-intuitive, so the early students of quantum theory had to be reassured that naive measurements to violate it were bound always to be unworkable. One way in which Heisenberg originally illustrated the intrinsic impossibility of violating the uncertainty principle is by using an imaginary microscope as a measuring device.
He imagines an experimenter trying to measure the position and momentum of an electron by shooting a photon at it.
The combination of these trade-offs imply that no matter what photon wavelength and aperture size are used, the product of the uncertainty in measured position and measured momentum is greater than or equal to a lower limit, which is (up to a small numerical factor) equal to Planck's constant. Heisenberg did not care to formulate the uncertainty principle as an exact limit (which is elaborated below), and preferred to use it instead, as a heuristic quantitative statement, correct up to small numerical factors, which makes the radically new noncommutativity of quantum mechanics inevitable.
Critical reactions.
The Copenhagen interpretation of quantum mechanics and Heisenberg's Uncertainty Principle were, in fact, seen as twin targets by detractors who believed in an underlying determinism and realism. According to the Copenhagen interpretation of quantum mechanics, there is no fundamental reality that the quantum state describes, just a prescription for calculating experimental results. There is no way to say what the state of a system fundamentally is, only what the result of observations might be.
Albert Einstein believed that randomness is a reflection of our ignorance of some fundamental property of reality, while Niels Bohr believed that the probability distributions are fundamental and irreducible, and depend on which measurements we choose to perform. Einstein and Bohr debated the uncertainty principle for many years. Some experiments within the first decade of the twenty-first century have cast doubt on how extensively the uncertainty principle applies.
Einstein's slit.
The first of Einstein's thought experiments challenging the uncertainty principle went as follows:
Bohr's response was that the wall is quantum mechanical as well, and that to measure the recoil to accuracy , the momentum of the wall must be known to this accuracy before the particle passes through. This introduces an uncertainty in the position of the wall and therefore the position of the slit equal to , and if the wall's momentum is known precisely enough to measure the recoil, the slit's position is uncertain enough to disallow a position measurement.
A similar analysis with particles diffracting through multiple slits is given by Richard Feynman.
In another thought experiment Lawrence Marq Goldberg theorized that one could, for example, determine the position of a particle and then travel back in time to a point before the first reading to measure the velocity, then time travel back to a point before the second (earlier) reading was taken to deliver the resulting measurements before the particle was disturbed so that the measurements did not need to be taken. This, of course, would result in a temporal paradox. But it does support his contention that "the problems inherent to the uncertainly principle lay in the measuring not in the "uncertainty" of physics."
Einstein's box.
Bohr was present when Einstein proposed the thought experiment which has become known as Einstein's box. Einstein argued that "Heisenberg's uncertainty equation implied that the uncertainty in time was related to the uncertainty in energy, the product of the two being related to Planck's constant." Consider, he said, an ideal box, lined with mirrors so that it can contain light indefinitely. The box could be weighed before a clockwork mechanism opened an ideal shutter at a chosen instant to allow one single photon to escape. "We now know, explained Einstein, precisely the time at which the photon left the box." "Now, weigh the box again. The change of mass tells the energy of the emitted light. In this manner, said Einstein, one could measure the energy emitted and the time it was released with any desired precision, in contradiction to the uncertainty principle."
Bohr spent a sleepless night considering this argument, and eventually realized that it was flawed. He pointed out that if the box were to be weighed, say by a spring and a pointer on a scale, "since the box must move vertically with a change in its weight, there will be uncertainty in its vertical velocity and therefore an uncertainty in its height above the table. ... Furthermore, the uncertainty about the elevation above the earth's surface will result in an uncertainty in the rate of the clock," because of Einstein's own theory of gravity's effect on time.
"Through this chain of uncertainties, Bohr showed that Einstein's light box experiment could not simultaneously measure exactly both the energy of the photon and the time of its escape."
EPR paradox for entangled particles.
Bohr was compelled to modify his understanding of the uncertainty principle after another thought experiment by Einstein. In 1935, Einstein, Podolsky and Rosen (see EPR paradox) published an analysis of widely separated entangled particles. Measuring one particle, Einstein realized, would alter the probability distribution of the other, yet here the other particle could not possibly be disturbed. This example led Bohr to revise his understanding of the principle, concluding that the uncertainty was not caused by a direct interaction.
But Einstein came to much more far-reaching conclusions from the same thought experiment. He believed the "natural basic assumption" that a complete description of reality, would have to predict the results of experiments from "locally changing deterministic quantities", and therefore, would have to include more information than the maximum possible allowed by the uncertainty principle.
In 1964, John Bell showed that this assumption can be falsified, since it would imply a certain inequality between the probabilities of different experiments. Experimental results confirm the predictions of quantum mechanics, ruling out Einstein's basic assumption that led him to the suggestion of his "hidden variables". Ironically this fact is one of the best pieces of evidence supporting Karl Popper's philosophy of invalidation of a theory by falsification-experiments. That is to say, here Einstein's "basic assumption" became falsified by experiments based on Bell's inequalities. For the objections of Karl Popper to the Heisenberg inequality itself, see below.
While it is possible to assume that quantum mechanical predictions are due to nonlocal, hidden variables, and in fact David Bohm invented such a formulation, this resolution is not satisfactory to the vast majority of physicists. The question of whether a random outcome is predetermined by a nonlocal theory can be philosophical, and it can be potentially intractable. If the hidden variables are not constrained, they could just be a list of random digits that are used to produce the measurement outcomes. To make it sensible, the assumption of nonlocal hidden variables is sometimes augmented by a second assumption—that the size of the observable universe puts a limit on the computations that these variables can do. A nonlocal theory of this sort predicts that a quantum computer would encounter fundamental obstacles when attempting to factor numbers of approximately 10,000 digits or more; a potentially achievable task in quantum mechanics.
Popper's criticism.
Karl Popper approached the problem of indeterminacy as a logician and metaphysical realist. He disagreed with the application of the uncertainty relations to individual particles rather than to ensembles of identically prepared particles, referring to them as "statistical scatter relations". In this statistical interpretation, a "particular" measurement may be made to arbitrary precision without invalidating the quantum theory. This directly contrasts with the Copenhagen interpretation of quantum mechanics, which is non-deterministic but lacks local hidden variables.
In 1934, Popper published "Zur Kritik der Ungenauigkeitsrelationen" ("Critique of the Uncertainty Relations") in "Naturwissenschaften", and in the same year "Logik der Forschung" (translated and updated by the author as "The Logic of Scientific Discovery" in 1959), outlining his arguments for the statistical interpretation. In 1982, he further developed his theory in "Quantum theory and the schism in Physics", writing:
[Heisenberg's] formulae are, beyond all doubt, derivable "statistical formulae" of the quantum theory. But they have been "habitually misinterpreted" by those quantum theorists who said that these formulae can be interpreted as determining some upper limit to the "precision of our measurements".[original emphasis]
Popper proposed an experiment to falsify the uncertainty relations, although he later withdrew his initial version after discussions with Weizsäcker, Heisenberg, and Einstein; this experiment may have influenced the formulation of the EPR experiment.
Many-worlds uncertainty.
The many-worlds interpretation originally outlined by Hugh Everett III in 1957 is partly meant to reconcile the differences between the Einstein and Bohr's views by replacing Bohr's wave function collapse with an ensemble of deterministic and independent universes whose "distribution" is governed by wave functions and the Schrödinger equation. Thus, uncertainty in the many-worlds interpretation follows from each observer within any universe having no knowledge of what goes on in the other universes.
Free will.
Some scientists including Arthur Compton and Martin Heisenberg have suggested that the uncertainty principle, or at least the general probabilistic nature of quantum mechanics, could be evidence for the two-stage model of free will. The standard view, however, is that apart from the basic role of quantum mechanics as a foundation for chemistry, nontrivial biological mechanisms requiring quantum mechanics are unlikely, due to the rapid decoherence time of quantum systems at room temperature.

</doc>
<doc id="27072071" url="https://en.wikipedia.org/wiki?curid=27072071" title="Unit circle">
Unit circle

In mathematics, a unit circle is a circle with a radius of one. Frequently, especially in trigonometry, the unit circle is the circle of radius one centered at the origin (0, 0) in the Cartesian coordinate system in the Euclidean plane. The unit circle is often denoted "S"1; the generalization to higher dimensions is the unit sphere.
If ("x", "y") is a point on the unit circle's circumference, then |"x"| and |"y"| are the lengths of the legs of a right triangle whose hypotenuse has length 1. Thus, by the Pythagorean theorem, "x" and "y" satisfy the equation
Since "x"² = (−"x")² for all "x", and since the reflection of any point on the unit circle about the "x"- or "y"-axis is also on the unit circle, the above equation holds for all points ("x", "y") on the unit circle, not only those in the first quadrant.
The interior of the unit circle is called the open unit disk, while the interior of the unit circle combined with the unit circle itself is called the closed unit disk.
One may also use other notions of "distance" to define other "unit circles", such as the Riemannian circle; see the article on mathematical norms for additional examples.
In the complex plane.
The unit circle can be considered as the unit complex numbers, i.e., the set of complex numbers "z" of the form 
for all "t". This relation is Euler's formula.
In quantum mechanics, this is referred to as phase factor.
Trigonometric functions on the unit circle.
The trigonometric functions cosine and sine of angle t (t for θ, theta) may be defined on the unit circle as follows: If ("x", "y") is a point on the unit circle, and if the ray from the origin (0, 0) to ("x", "y") makes an angle "t" from the positive "x"-axis, (where counterclockwise turning is positive), then
The equation "x"2 + "y"2 = 1 gives the relation
The unit circle also demonstrates that sine and cosine are periodic functions, with the identities
for any integer "k".
Triangles constructed on the unit circle can also be used to illustrate the periodicity of the trigonometric functions. First, construct a radius OA from the origin to a point P("x"1,"y"1) on the unit circle such that an angle "t" with 0 < "t" < π/2 is formed with the positive arm of the "x"-axis. Now consider a point Q("x"1,0) and line segments PQ formula_8 OQ. The result is a right triangle ΔOPQ with ∠QOP = "t". Because PQ has length "y"1, OQ length "x"1, and OA length 1, sin("t") = "y"1 and cos("t") = "x"1. Having established these equivalences, take another radius OR from the origin to a point R(−"x"1,"y"1) on the circle such that the same angle "t" is formed with the negative arm of the "x"-axis. Now consider a point S("−x1",0) and line segments RS formula_8 OS. The result is a right triangle ΔORS with ∠SOR = "t". It can hence be seen that, because ∠ROQ = π−"t", R is at (cos(π−"t"),sin(π−"t")) in the same way that P is at (cos("t"),sin("t")). The conclusion is that, since (−"x"1,"y"1) is the same as (cos(π−"t"),sin(π−"t")) and ("x"1,"y"1) is the same as (cos("t"),sin("t")), it is true that sin("t") = sin(π−"t") and −cos("t") = cos(π−"t"). It may be inferred in a similar manner that tan(π−"t") = −tan("t"), since tan("t") = "y"1/"x"1 and tan(π−"t") = "y"1/(−"x"1). A simple demonstration of the above can be seen in the equality sin(π/4) = sin(3π/4) = 1/sqrt(2).
When working with right triangles, sine, cosine, and other trigonometric functions only make sense for angle measures more than zero and less than π/2. However, when defined with the unit circle, these functions produce meaningful values for any real-valued angle measure – even those greater than 2π. In fact, all six standard trigonometric functions – sine, cosine, tangent, cotangent, secant, and cosecant, as well as archaic functions like versine and exsecant – can be defined geometrically in terms of a unit circle, as shown at right.
Using the unit circle, the values of any trigonometric function for many angles other than those labeled can be calculated without the use of a calculator by using the Sum and Difference Formulas.
Circle group.
Complex numbers can be identified with points in the Euclidean plane, namely the number "a" + "bi" is identified with the point ("a", "b"). Under this identification, the unit circle is a group under multiplication, called the circle group. On the plane multiplication by formula_10 gives a counterclockwise rotation by θ. This group has important applications in mathematics and science.
Complex dynamics.
Julia set of discrete nonlinear dynamical system with evolution function:
is a unit circle. It is a simplest case so it is widely used in study of dynamical systems.

</doc>
<doc id="285301" url="https://en.wikipedia.org/wiki?curid=285301" title="Unit square">
Unit square

In mathematics, a unit square is a square whose sides have length 1. Often, "the" unit square refers specifically to the square in the Cartesian plane with corners at (0, 0), (1, 0), (0, 1), and (1, 1).
In the real plane.
In a Cartesian coordinate system with coordinates ("x", "y") the unit square is defined as the square consisting of the points where both "x" and "y" lie in a closed unit interval from 0 to 1 on their respective axes.
That is, the unit square is the Cartesian product  × , where denotes the closed unit interval.
It is not known whether any point in the plane is a rational distance from all four vertices of the unit square. However, no such point is on an edge of the square.
In the complex plane.
In the complex plane, the corners of the unit square are at 0, 1, "i", and 1 + "i".

</doc>
<doc id="167053" url="https://en.wikipedia.org/wiki?curid=167053" title="Unit vector">
Unit vector

In mathematics, a unit vector in a normed vector space is a vector (often a spatial vector) of length 1. A unit vector is often denoted by a lowercase letter with a "hat": formula_1 (pronounced "i-hat"). The term direction vector is used to describe a unit vector being used to represent spatial direction, and such quantities are commonly denoted as d. Two 2D direction vectors, d1 and d2 are illustrated. 2D spatial directions represented this way are equivalent numerically to points on the unit circle. 
The same construct is used to specify spatial directions in 3D. As illustrated, each unique direction is equivalent numerically to a point on the unit sphere.
The normalized vector or versor û of a non-zero vector u is the unit vector in the direction of u, i.e.,
where ||u|| is the norm (or length) of u. The term "normalized vector" is sometimes used as a synonym for "unit vector". 
Unit vectors are often chosen to form the basis of a vector space. Every vector in the space may be written as a linear combination of unit vectors.
By definition, in a Euclidean space the dot product of two unit vectors is a scalar value amounting to the cosine of the smaller subtended angle. In three-dimensional Euclidean space, the cross product of two arbitrary unit vectors is a 3rd vector orthogonal to both of them having length equal to the sine of the smaller subtended angle. The normalized cross product corrects for this varying length, and yields the mutually orthogonal unit vector to the two inputs, applying the right-hand rule to resolve one of two possible directions.
Orthogonal coordinates.
Cartesian coordinates.
Unit vectors may be used to represent the axes of a Cartesian coordinate system. For instance, the unit vectors in the direction of the "x", "y", and "z" axes of a three dimensional Cartesian coordinate system are
They are sometimes referred to as the versors of the coordinate system, and they form a set of mutually orthogonal unit vectors, typically referred to as a standard basis in linear algebra.
They are often denoted using normal vector notation (e.g., i or formula_4) rather than standard unit vector notation (e.g., formula_5). In most contexts it can be assumed that i, j, and k, (or formula_6 formula_7 and formula_8) are versors of a 3-D Cartesian coordinate system. The notations formula_9, formula_10, formula_11, or formula_12, with or without hat, are also used, particularly in contexts where i, j, k might lead to confusion with another quantity (for instance with index symbols such as "i", "j", "k", used to identify an element of a set or array or sequence of variables).
When a unit vector in space is expressed, with Cartesian notation, as a linear combination of i, j, k, its three scalar components can be referred to as direction cosines. The value of each component is equal to the cosine of the angle formed by the unit vector with the respective basis vector. This is one of the methods used to describe the orientation (angular position) of a straight line, segment of straight line, oriented axis, or segment of oriented axis (vector).
Cylindrical coordinates.
The three orthogonal unit vectors appropriate to cylindrical symmetry are: 
They are related to the Cartesian basis formula_18, formula_19, formula_20 by:
It is important to note that formula_13 and formula_16 are functions of formula_28, and are "not" constant in direction. When differentiating or integrating in cylindrical coordinates, these unit vectors themselves must also be operated on. For a more complete description, see Jacobian matrix. The derivatives with respect to formula_29 are:
Spherical coordinates.
The unit vectors appropriate to spherical symmetry are: formula_33, the direction in which the radial distance from the origin increases; formula_34, the direction in which the angle in the "x"-"y" plane counterclockwise from the positive "x"-axis is increasing; and formula_35, the direction in which the angle from the positive "z" axis is increasing. To minimize degeneracy, the polar angle is usually taken formula_36. It is especially important to note the context of any ordered triplet written in spherical coordinates, as the roles of formula_16 and formula_35 are often reversed. Here, the American "physics" convention is used. This leaves the azimuthal angle formula_39 defined the same as in cylindrical coordinates. The Cartesian relations are:
The spherical unit vectors depend on both formula_39 and formula_44, and hence there are 5 possible non-zero derivatives. For a more complete description, see Jacobian matrix and determinant. The non-zero derivatives are:
General unit vectors.
Common general themes of unit vectors occur throughout physics and geometry:
Curvilinear coordinates.
In general, a coordinate system may be uniquely specified using a number of linearly independent unit vectors formula_50 equal to the degrees of freedom of the space. For ordinary 3-space, these vectors may be denoted formula_51. It is nearly always convenient to define the system to be orthonormal and right-handed:
formula_52
formula_53
where δ"ij" is the Kronecker delta (which is one for "i" = "j" and zero else) and formula_54 is the Levi-Civita symbol (which is one for permutations ordered as "ijk" and minus one for permutations ordered as "kji").

</doc>
<doc id="173993" url="https://en.wikipedia.org/wiki?curid=173993" title="Unitary group">
Unitary group

In mathematics, the unitary group of degree "n", denoted U("n"), is the group of unitary matrices, with the group operation that of matrix multiplication. The unitary group is a subgroup of the general linear group . Hyperorthogonal group is an archaic name for the unitary group, especially over finite fields.
In the simple case , the group U(1) corresponds to the circle group, consisting of all complex numbers with absolute value 1 under multiplication. All the unitary groups contain copies of this group.
The unitary group U("n") is a real Lie group of dimension "n"2. The Lie algebra of U("n") consists of skew-Hermitian matrices, with the Lie bracket given by the commutator.
The general unitary group (also called the group of unitary similitudes) consists of all matrices "A" such that "A"∗"A" is a nonzero multiple of the identity matrix, and is just the product of the unitary group with the group of all positive multiples of the identity matrix.
Properties.
Since the determinant of a unitary matrix is a complex number with norm formula_1, the determinant gives a group homomorphism
The kernel of this homomorphism is the set of unitary matrices with determinant formula_1. This subgroup is called the special unitary group, denoted formula_4. We then have a short exact sequence of Lie groups:
This short exact sequence splits so that formula_6 may be written as a semidirect product of formula_4 by formula_8. Here the formula_8 subgroup of formula_6 can be taken to consist of matrices, which are diagonal, have formula_11 in the upper left corner and formula_1 on the rest of the diagonal.
The unitary group formula_6 is nonabelian for formula_14. The center of formula_6 is the set of scalar matrices formula_16 with formula_17. This follows from Schur's lemma. The center is then isomorphic to formula_8. Since the center of formula_6 is a formula_1-dimensional abelian normal subgroup of formula_6, the unitary group is not semisimple.
Topology.
The unitary group U("n") is endowed with the relative topology as a subset of , the set of all complex matrices, which is itself homeomorphic to a 2"n"2-dimensional Euclidean space.
As a topological space, U("n") is both compact and connected. The compactness of U("n") follows from the Heine–Borel theorem and the fact that it is a closed and bounded subset of . To show that U("n") is connected, recall that any unitary matrix "A" can be diagonalized by another unitary matrix "S". Any diagonal unitary matrix must have complex numbers of absolute value 1 on the main diagonal. We can therefore write
A path in U("n") from the identity to "A" is then given by
The unitary group is not simply connected; the fundamental group of U("n") is infinite cyclic for all "n":
To see this, note that the above splitting of U("n") as a semidirect product of SU("n") and U(1) induces a topological product structure on U("n"), so that
Now the first unitary group U(1) is topologically a circle, which is well known to have a fundamental group isomorphic to Z, and the inclusion map is an isomorphism on "π"1. (It has quotient the Stiefel manifold.)
The determinant map induces an isomorphism of fundamental groups, with the splitting inducing the inverse.
The Weyl group of U("n") is the symmetric group "Sn", acting on the diagonal torus by permuting the entries:
Related groups.
2-out-of-3 property.
The unitary group is the 3-fold intersection of the orthogonal, symplectic, and complex groups:
Thus a unitary structure can be seen as an orthogonal structure, a complex structure, and a symplectic structure, which are required to be "compatible" (meaning that one uses the same "J" in the complex structure and the symplectic form, and that this "J" is orthogonal; writing all the groups as matrix groups fixes a "J" (which is orthogonal) and ensures compatibility).
In fact, it is the intersection of any "two" of these three; thus a compatible orthogonal and complex structure induce a symplectic structure, and so forth.
At the level of equations, this can be seen as follows:
Any two of these equations implies the third.
At the level of forms, this can be seen by decomposing a Hermitian form into its real and imaginary parts: the real part is symmetric (orthogonal), and the imaginary part is skew-symmetric (symplectic)—and these are related by the complex structure (which is the compatibility). On an almost Kähler manifold, one can write this decomposition as "h" = "g" + "i"ω, where "h" is the Hermitian form, "g" is the Riemannian metric, "i" is the almost complex structure, and ω is the almost symplectic structure.
From the point of view of Lie groups, this can partly be explained as follows: O(2"n") is the maximal compact subgroup of GL(2"n", R), and U("n") is the maximal compact subgroup of both GL("n", C) and Sp(2"n"). Thus the intersection O(2"n") ∩ GL("n", C) or O(2"n") ∩ Sp(2"n") is the maximal compact subgroup of both of these, so U("n"). From this perspective, what is unexpected is the intersection GL("n", C) ∩ Sp(2"n") = U("n").
Special unitary and projective unitary groups.
Just as the orthogonal group has the special orthogonal group SO("n") as subgroup and the projective orthogonal group PO("n") as quotient, and the projective special orthogonal group PSO("n") as subquotient, the unitary group has associated to it the special unitary group SU("n"), the projective unitary group PU("n"), and the projective special unitary group PSU("n"). These are related as by the commutative diagram at right; notably, both projective groups are equal: PSU("n") = PU("n").
The above is for the classical unitary group (over the complex numbers) – for unitary groups over finite fields, one similarly obtains special unitary and projective unitary groups, but in general formula_28.
G-structure: almost Hermitian.
In the language of G-structures, a manifold with a U("n")-structure is an almost Hermitian manifold.
Generalizations.
From the point of view of Lie theory, the classical unitary group is a real form of the Steinberg group formula_29, which is an algebraic group that arises from the combination of the "diagram automorphism" of the general linear group (reversing the Dynkin diagram A"n", which corresponds to transpose inverse) and the "field automorphism" of the extension C/R (namely complex conjugation). Both these automorphisms are automorphisms of the algebraic group, have order 2, and commute, and the unitary group is the fixed points of the product automorphism, as an algebraic group. The classical unitary group is a real form of this group, corresponding to the standard Hermitian form Ψ, which is positive definite.
This can be generalized in a number of ways:
Indefinite forms.
Analogous to the indefinite orthogonal groups, one can define an indefinite unitary group, by considering the transforms that preserve a given Hermitian form, not necessarily positive definite (but generally taken to be non-degenerate). Here one is working with a vector space over the complex numbers.
Given a Hermitian form Ψ on a complex vector space "V", the unitary group U(Ψ) is the group of transforms that preserve the form: the transform "M" such that for all . In terms of matrices, representing the form by a matrix denoted Φ, this says that .
Just as for symmetric forms over the reals, Hermitian forms are determined by signature, and are all unitarily congruent to a diagonal form with "p" entries of 1 on the diagonal and "q" entries of −1. The non-degenerate assumption is equivalent to . In a standard basis, this is represented as a quadratic form as:
and as a symmetric form as:
The resulting group is denoted .
Finite fields.
Over the finite field with elements, F"q", there is a unique quadratic extension field, F"q"2, with order 2 automorphism formula_35 (the "r"th power of the Frobenius automorphism). This allows one to define a Hermitian form on an F"q"2 vector space "V", as an F"q"-bilinear map formula_36 such that formula_37 and formula_38 for . Further, all non-degenerate Hermitian forms on a vector space over a finite field are unitarily congruent to the standard one, represented by the identity matrix, that is, any Hermitian form is unitarily equivalent to
where formula_40 represent the coordinates of in some particular F"q"2-basis of the "n"-dimensional space "V" .
Thus one can define a (unique) unitary group of dimension "n" for the extension F"q"2/Fq, denoted either as or depending on the author. The subgroup of the unitary group consisting of matrices of determinant 1 is called the special unitary group and denoted or . For convenience, this article will use the convention. The center of has order and consists of the scalar matrices that are unitary, that is those matrices "cIV" with formula_41. The center of the special unitary group has order and consists of those unitary scalars which also have order dividing "n". The quotient of the unitary group by its center is called the projective unitary group, , and the quotient of the special unitary group by its center is the projective special unitary group . In most cases ( and ), is a perfect group and is a finite simple group, .
Degree-2 separable algebras.
More generally, given a field "k" and a degree-2 separable "k"-algebra "K" (which may be a field extension but need not be), one can define unitary groups with respect to this extension.
First, there is a unique "k"-automorphism of "K" formula_42 which is an involution and fixes exactly "k" (formula_43 if and only if "a" ∈ "k"). This generalizes complex conjugation and the conjugation of degree 2 finite field extensions, and allows one to define Hermitian forms and unitary groups as above.
Algebraic groups.
The equations defining a unitary group are polynomial equations over "k" (but not over "K"): for the standard form the equations are given in matrices as , where formula_44 is the conjugate transpose. Given a different form, they are . The unitary group is thus an algebraic group, whose points over a "k"-algebra "R" are given by:
For the field extension C/R and the standard (positive definite) Hermitian form, these yield an algebraic group with real and complex points given by:
In fact, the unitary group is a linear algebraic group.
Unitary group of a quadratic module.
The unitary group of a quadratic module is a generalisation of the linear algebraic group U just defined, which incorporates as special cases many different classical algebraic groups. The definition goes back to Anthony Bak's thesis.
To define it, one has to define quadratic modules first:
Let "R" be a ring with anti-automorphism "J", formula_48 such that formula_49 for all "r" in "R" and formula_50. Define
Let be an additive subgroup of "R", then Λ is called "form parameter" if formula_53 and formula_54. A pair such that "R" is a ring and Λ a form parameter is called "form ring".
Let "M" be an "R"-module and "f" a "J"-sesquilinear form on "M" (i.e. formula_55 for any formula_56 and formula_57). Define formula_58 and formula_59, then "f" is said to "define" the "Λ-quadratic form" on "M". A "quadratic module" over is a triple such that "M" is an "R"-module and is a Λ-quadratic form.
To any quadratic module defined by a "J"-sesquilinear form "f" on "M" over a form ring one can associate the "unitary group"
The special case where , with "J" any non-trivial involution (i.e. formula_61 and gives back the "classical" unitary group (as an algebraic group).
Polynomial invariants.
The unitary groups are the automorphisms of two polynomials in real non-commutative variables:
These are easily seen to be the real and imaginary parts of the complex form formula_64. The two invariants separately are invariants of O(2"n") and Sp(2"n", R). Combined they make the invariants of U("n") which is a subgroup of both these groups. The variables must be non-commutative in these invariants otherwise the second polynomial is identically zero.
Classifying space.
The classifying space for U("n") is described in the article classifying space for U(n).

</doc>
<doc id="292788" url="https://en.wikipedia.org/wiki?curid=292788" title="Unitary representation">
Unitary representation

In mathematics, a unitary representation of a group "G" is a linear representation π of "G" on a complex Hilbert space "V" such that π("g") is a unitary operator for every "g" ∈ "G". The general theory is well-developed in case "G" is a locally compact (Hausdorff) topological group and the representations are strongly continuous.
The theory has been widely applied in quantum mechanics since the 1920s, particularly influenced by Hermann Weyl's 1928 book "Gruppentheorie und Quantenmechanik". One of the pioneers in constructing a general theory of unitary representations, for any group "G" rather than just for particular groups useful in applications, was George Mackey.
Context in harmonic analysis.
The theory of unitary representations of groups is closely connected with harmonic analysis. In the case of an abelian group "G", a fairly complete picture of the representation theory of "G" is given by Pontryagin duality. In general, the unitary equivalence classes (see below) of irreducible unitary representations of "G" make up its unitary dual. This set can be identified with the spectrum of the C*-algebra associated to "G" by the group C*-algebra construction. This is a topological space.
The general form of the Plancherel theorem tries to describe the regular representation of "G" on "L"2("G") by means of a measure on the unitary dual. For "G" abelian this is given by the Pontryagin duality theory. For "G" compact, this is done by the Peter–Weyl theorem; in that case the unitary dual is a discrete space, and the measure attaches an atom to each point of mass equal to its degree.
Formal definitions.
Let "G" be a topological group. A strongly continuous unitary representation of "G" on a Hilbert space "H" is a group homomorphism from "G" into the unitary group of "H",
such that "g" → π("g") ξ is a norm continuous function for every ξ ∈ "H".
Note that if G is a Lie group, the Hilbert space also admits underlying smooth and analytic structures. A vector ξ in "H" is said to be smooth or analytic if the map "g" → π("g") ξ is smooth or analytic (in the norm or weak topologies on "H"). Smooth vectors are dense in "H" by a classical argument of Lars Gårding, since convolution by smooth functions of compact support yields smooth vectors. Analytic vectors are dense by a classical argument of Edward Nelson, amplified by Roe Goodman, since vectors in the image of a heat operator "e"–tD, corresponding to an elliptic differential operator "D" in the universal enveloping algebra of "G", are analytic. Not only do smooth or analytic vectors form dense subspaces; they also form common cores for the unbounded skew-adjoint operators corresponding to the elements of the Lie algebra, in the sense of spectral theory.
Two unitary representations π1: "G" → U("H"1), π2: "G" → U("H"2) are said to be unitary equivalent if there is a unitary operator "A":"H"1 → "H"2 such that "A"∘π1("g")=π2("g")∘"A" for all "g" in "G". When this holds, "A" is said to be an intertwining operator for the representations (π1,"H"1), (π2,"H"2).
Complete reducibility.
A unitary representation is completely reducible, in the sense that for any closed invariant subspace, the orthogonal complement is again a closed invariant subspace. This is at the level of an observation, but is a fundamental property. For example, it implies that finite-dimensional unitary representations are always a direct sum of irreducible representations, in the algebraic sense.
Since unitary representations are much easier to handle than the general case, it is natural to consider unitarizable representations, those that become unitary on the introduction of a suitable complex Hilbert space structure. This works very well for finite groups, and more generally for compact groups, by an averaging argument applied to an arbitrary hermitian structure. For example, a natural proof of Maschke's theorem is by this route.
Unitarizability and the unitary dual question.
In general, for non-compact groups, it is a more serious question which representations are unitarizable. One of the important unsolved problems in mathematics is the description of the unitary dual, the effective classification of irreducible unitary representations of all real reductive Lie groups. All irreducible unitary representations are admissible (or rather their Harish-Chandra modules are), and the admissible representations are given by the Langlands classification, and it is easy to tell which of them have a non-trivial invariant sesquilinear form. The problem is that it is in general hard to tell when the quadratic form is positive definite. For many reductive Lie groups this has been solved; see representation theory of SL2(R) and representation theory of the Lorentz group for examples.

</doc>
<doc id="21347678" url="https://en.wikipedia.org/wiki?curid=21347678" title="Units of measurement">
Units of measurement

A unit of measurement is a definite magnitude of a physical quantity, defined and adopted by convention or by law, that is used as a standard for measurement of the same physical quantity. Any other value of the physical quantity can be expressed as a simple multiple of the unit of measurement.
For example, length is a physical quantity. The metre is a unit of length that represents a definite predetermined length. When we say 10 metres (or 10 m), we actually mean 10 times the definite predetermined length called "metre".
The definition, agreement, and practical use of units of measurement have played a crucial role in human endeavour from early ages up to this day. Different systems of units used to be very common. Now there is a global standard, the International System of Units (SI), the modern form of the metric system.
In trade, weights and measures is often a subject of governmental regulation, to ensure fairness and transparency. The International Bureau of Weights and Measures (BIPM) is tasked with ensuring worldwide uniformity of measurements and their traceability to the International System of Units (SI). 
Metrology is the science for developing nationally and internationally accepted units of weights and measures.
In physics and metrology, units are standards for measurement of physical quantities that need clear definitions to be useful. Reproducibility of experimental results is central to the scientific method. A standard system of units facilitates this. Scientific systems of units are a refinement of the concept of weights and measures developed long ago for commercial purposes.
Science, medicine, and engineering often use larger and smaller units of measurement than those used in everyday life and indicate them more precisely. The judicious selection of the units of measurement can aid researchers in problem solving (see, for example, dimensional analysis).
In the social sciences, there are no standard units of measurement and the theory and practice of measurement is studied in psychometrics and the theory of conjoint measurement.
History.
A unit of measurement is a standardised quantity of a physical property, used as a factor to express occurring quantities of that property. Units of measurement were among the earliest tools invented by humans. Primitive societies needed rudimentary measures for many tasks: constructing dwellings of an appropriate size and shape, fashioning clothing, or bartering food or raw materials.
The earliest known uniform systems of weights and measures seem to have all been created sometime in the 4th and 3rd millennia BC among the ancient peoples of Mesopotamia, Egypt and the Indus Valley, and perhaps also Elam in Persia as well.
Weights and measures are mentioned in the Bible (Leviticus 19:35–36). It is a commandment to be honest and have fair measures.
In the "Magna Carta" of 1215 (The Great Charter) with the seal of King John, put before him by the Barons of England, King John agreed in Clause 35 "There shall be one measure of wine throughout our whole realm, and one measure of ale and one measure of corn—namely, the London quart;—and one width of dyed and russet and hauberk cloths—namely, two ells below the selvage..."
Systems of units.
Traditional systems.
Historically many of the systems of measurement which had been in use were to some extent based on the dimensions of the human body. As a result, units of measure could vary not only from location to location, but from person to person.
Metric systems.
A number of metric systems of units have evolved since the adoption of the original metric system in France in 1791. The current international standard metric system is the International System of Units. An important feature of modern systems is standardization. Each unit has a universally recognized size.
Both the Imperial units and US customary units derive from earlier English units. Imperial units were mostly used in the British Commonwealth and the former British Empire. US customary units are still the main system of measurement used in the United States despite Congress having legally authorized metric measure on 28 July 1866. Some steps towards US metrication have been made, particularly the redefinition of basic US and imperial units to derive exactly from SI units. Since the international yard and pound agreement of 1959 the US and imperial inch is now defined as exactly , and the US and imperial avoirdupois pound is now defined as exactly .
Natural systems.
While the above systems of units are based on arbitrary unit values, formalised as standards, some unit values occur naturally in science. Systems of units based on these are called natural units. Similar to natural units, atomic units (au) are a convenient system of units of measurement used in atomic physics.
Also a great number of unusual and non-standard units may be encountered. These may include the solar mass () and the megaton (the energy released by detonating one million tons of trinitrotoluene, TNT).
Legal control of weights and measures.
To reduce the incidence of retail fraud, many national statutes have standard definitions of weights and measures that may be used (hence "statute measure"), and these are verified by legal officers.
Base and derived units.
Different systems of units are based on different choices of a set of base units.
The most widely used system of units is the International System of Units, or SI. There are seven SI base units. All other SI units can be derived from these base units.
For most quantities a unit is absolutely necessary to communicate values of that physical quantity. For example, conveying to someone a particular length without using some sort of unit is impossible, because a length cannot be described without a reference used to make sense of the value given.
But not all quantities require a unit of their own. Using physical laws, units of quantities can be expressed as combinations of units of other quantities. Thus only a small set of units is required. These units are taken as the "base units". Other units are "derived units". Derived units are a matter of convenience, as they can be expressed in terms of basic units. Which units are considered base units is a matter of choice.
The base units of SI are actually not the smallest set possible. Smaller sets have been defined. For example, there are unit sets in which the electric and magnetic field have the same unit. This is based on physical laws that show that electric and magnetic field are actually different manifestations of the same phenomenon.
Calculations with units of measurement.
Units as dimensions.
Any value of a physical quantity is expressed as a comparison to a unit of that quantity. For example, the value of a physical quantity "Z" is expressed as the product of a unit [Z] and a numerical factor:
The multiplication sign is usually left out, just as it is left out between variables in scientific notation of formulas. The conventions used to express quantities is referred to as quantity calculus. In formulas the unit [Z] can be treated as if it were a specific magnitude of a kind of physical dimension: see dimensional analysis for more on this treatment.
Units can only be added or subtracted if they are the same type; however units can always be multiplied or divided, as George Gamow used to explain:
A distinction should be made between units and standards. A unit is fixed by its definition, and is independent of physical conditions such as temperature. By contrast, a standard is a physical realization of a unit, and realizes that unit only under certain physical conditions. For example, the metre is a unit, while a metal bar is a standard. One metre is the same length regardless of temperature, but a metal bar will be exactly one metre long only at a certain temperature.
There are certain rules that have to be used when dealing with units:
Expressing a physical value in terms of another unit.
Conversion of units involves comparison of different standard physical values, either of a single physical quantity or of a physical quantity and a combination of other physical quantities.
Starting with:
just replace the original unit formula_3 with its meaning in terms of the desired unit formula_4, e.g. if formula_5, then:
Now formula_7 and formula_8 are both numerical values, so just calculate their product.
Or, which is just mathematically the same thing, multiply "Z" by unity, the product is still "Z":
For example, you have an expression for a physical value "Z" involving the unit "feet per second" (formula_3) and you want it in terms of the unit "miles per hour" (formula_4):
Or as an example using the metric system, you have a value of fuel economy in the unit "litres per 100 kilometres" and you want it in terms of the unit "microlitres per metre":
Real-world implications.
One example of the importance of agreed units is the failure of the NASA Mars Climate Orbiter, which was accidentally destroyed on a mission to Mars in September 1999 instead of entering orbit due to miscommunications about the value of forces: different computer programs used different units of measurement (newton versus pound force). Considerable amounts of effort, time, and money were wasted.
On 15 April 1999, Korean Air cargo from Shanghai to Seoul was lost due to the crew confusing tower instructions (in metres) and altimeter readings (in feet). Three crew and five people on the ground were killed. Thirty-seven were injured.
In 1983, a Boeing 767 (which came to be known as the Gimli Glider) ran out of fuel in mid-flight because of two mistakes in figuring the fuel supply of Air Canada's first aircraft to use metric measurements. This accident was the result of both confusion due to the simultaneous use of metric and Imperial measures and confusion of mass and volume measures.

</doc>
<doc id="241026" url="https://en.wikipedia.org/wiki?curid=241026" title="Up quark">
Up quark

The up quark or u quark (symbol: "u") is the lightest of all quarks, a type of elementary particle, and a major constituent of matter. It, along with the down quark, forms the neutrons (one up quark, two down quarks) and protons (two up quarks, one down quark) of atomic nuclei. It is part of the first generation of matter, has an electric charge of + "e" and a bare mass of 1.8–. Like all quarks, the up quark is an elementary fermion with spin-, and experiences all four fundamental interactions: gravitation, electromagnetism, weak interactions, and strong interactions. The antiparticle of the up quark is the up antiquark (sometimes called "antiup quark" or simply "antiup"), which differs from it only in that some of its properties have equal magnitude but opposite sign.
Its existence (along with that of the down and strange quarks) was postulated in 1964 by Murray Gell-Mann and George Zweig to explain the "Eightfold Way" classification scheme of hadrons. The up quark was first observed by experiments at the Stanford Linear Accelerator Center in 1968.
History.
In the beginnings of particle physics (first half of the 20th century), hadrons such as protons, neutrons and pions were thought to be elementary particles. However, as new hadrons were discovered, the 'particle zoo' grew from a few particles in the early 1930s and 1940s to several dozens of them in the 1950s. The relationships between each of them were unclear until 1961, when Murray Gell-Mann and Yuval Ne'eman (independently of each other) proposed a hadron classification scheme called the "Eightfold Way", or in more technical terms, SU(3) flavor symmetry.
This classification scheme organized the hadrons into isospin multiplets, but the physical basis behind it was still unclear. In 1964, Gell-Mann and George Zweig (independently of each other) proposed the quark model, then consisting only of up, down, and strange quarks. However, while the quark model explained the Eightfold Way, no direct evidence of the existence of quarks was found until 1968 at the Stanford Linear Accelerator Center. Deep inelastic scattering experiments indicated that protons had substructure, and that protons made of three more-fundamental particles explained the data (thus confirming the quark model).
At first people were reluctant to describe the three bodies as quarks, instead preferring Richard Feynman's parton description, but over time the quark theory became accepted (see "November Revolution").
Mass.
Despite being extremely common, the bare mass of the up quark is not well determined, but probably lies between 1.8 and . Lattice QCD calculations give a more precise value: .
When found in mesons (particles made of one quark and one antiquark) or baryons (particles made of three quarks), the 'effective mass' (or 'dressed' mass) of quarks becomes greater because of the binding energy caused by the gluon field between each quark (see mass–energy equivalence).The bare mass of up quarks is so light, it cannot be straightforwardly calculated because relativistic effects have to be taken into account.

</doc>
<doc id="2470504" url="https://en.wikipedia.org/wiki?curid=2470504" title="Upsilon meson">
Upsilon meson

The Upsilon meson () is a quarkonium state (i.e. flavourless meson) formed from a bottom quark and its antiparticle. It was discovered by the E288 collaboration, headed by Leon Lederman, at Fermilab in 1977, and was the first particle containing a bottom quark to be discovered because it is the lightest that can be produced without additional massive particles. It has a lifetime of and a mass about in the ground state.

</doc>
<doc id="32502" url="https://en.wikipedia.org/wiki?curid=32502" title="Vacuum">
Vacuum

Vacuum is space void of matter. The word stems from the Latin adjective "vacuus" for "vacant" or "void". An approximation to such vacuum is a region with a gaseous pressure much less than atmospheric pressure. Physicists often discuss ideal test results that would occur in a "perfect" vacuum, which they sometimes simply call "vacuum" or free space, and use the term partial vacuum to refer to an actual imperfect vacuum as one might have in a laboratory or in space. In engineering and applied physics on the other hand, vacuum refers to any space in which the pressure is lower than atmospheric pressure. The Latin term in vacuo is used to describe an object as being in what would otherwise be a vacuum.
The "quality" of a partial vacuum refers to how closely it approaches a perfect vacuum. Other things equal, lower gas pressure means higher-quality vacuum. For example, a typical vacuum cleaner produces enough suction to reduce air pressure by around 20%. Much higher-quality vacuums are possible. Ultra-high vacuum chambers, common in chemistry, physics, and engineering, operate below one trillionth (10−12) of atmospheric pressure (100 nPa), and can reach around 100 particles/cm3. Outer space is an even higher-quality vacuum, with the equivalent of just a few hydrogen atoms per cubic meter on average. According to modern understanding, even if all matter could be removed from a volume, it would still not be "empty" due to vacuum fluctuations, dark energy, transiting gamma rays, cosmic rays, neutrinos, and other phenomena in quantum physics. In the electromagnetism in the 19th century, vacuum was thought to be filled with a medium called aether. In modern particle physics, the vacuum state is considered the ground state of matter.
Vacuum has been a frequent topic of philosophical debate since ancient Greek times, but was not studied empirically until the 17th century. Evangelista Torricelli produced the first laboratory vacuum in 1643, and other experimental techniques were developed as a result of his theories of atmospheric pressure. A torricellian vacuum is created by filling with mercury a tall glass container closed at one end and then inverting the container into a bowl to contain the mercury.
Vacuum became a valuable industrial tool in the 20th century with the introduction of incandescent light bulbs and vacuum tubes, and a wide array of vacuum technology has since become available. The recent development of human spaceflight has raised interest in the impact of vacuum on human health, and on life forms in general.
Etymology.
The word "vacuum" comes , noun use of neuter of "vacuus", meaning "empty", related to "vacare", meaning "be empty".
"Vacuum" is one of the few words in the English language that contains two consecutive letter "u"'s.
Historical interpretation.
Historically, there has been much dispute over whether such a thing as a vacuum can exist. Ancient Greek philosophers debated the existence of a vacuum, or void, in the context of atomism, which posited void and atom as the fundamental explanatory elements of physics. Following Plato, even the abstract concept of a featureless void faced considerable skepticism: it could not be apprehended by the senses, it could not, itself, provide additional explanatory power beyond the physical volume with which it was commensurate and, by definition, it was quite literally nothing at all, which cannot rightly be said to exist. Aristotle believed that no void could occur naturally, because the denser surrounding material continuum would immediately fill any incipient rarity that might give rise to a void.
In his "Physics", book IV, Aristotle offered numerous arguments against the void: for example, that motion through a medium which offered no impediment could continue "ad infinitum", there being no reason that something would come to rest anywhere in particular. Although Lucretius argued for the existence of vacuum in the first century BC and Hero of Alexandria tried unsuccessfully to create an artificial vacuum in the first century AD, it was European scholars such as Roger Bacon, Blasius of Parma and Walter Burley in the 13th and 14th century who focused considerable attention on these issues. Eventually following Stoic physics in this instance, scholars from the 14th century onward increasingly departed from the Aristotelian perspective in favor of a supernatural void beyond the confines of the cosmos itself, a conclusion widely acknowledged by the 17th century, which helped to segregate natural and theological concerns.
Almost two thousand years after Plato, René Descartes also proposed a geometrically based alternative theory of atomism, without the problematic nothing–everything dichotomy of void and atom. Although Descartes agreed with the contemporary position, that a vacuum does not occur in nature, the success of his namesake coordinate system and more implicitly, the spacial–corporeal component of his metaphysics would come to define the philosophically modern notion of empty space as a quantified extension of volume. By the ancient definition however, directional information and magnitude were conceptually distinct. With the acquiescence of Cartesian mechanical philosophy to the "brute fact" of action at a distance, and at length, its successful reification by force fields and ever more sophisticated geometric structure, the anachronism of empty space widened until "a seething ferment" of quantum activity in the 20th century filled the vacuum with a virtual pleroma.
The explanation of a "clepsydra" or water clock was a popular topic in the Middle Ages. Although a simple wine skin sufficed to demonstrate a partial vacuum, in principle, more advanced suction pumps had been developed in Roman Pompeii.
In the medieval Middle Eastern world, the physicist and Islamic scholar, Al-Farabi (Alpharabius, 872–950), conducted a small experiment concerning the existence of vacuum, in which he investigated handheld plungers in water. He concluded that air's volume can expand to fill available space, and he suggested that the concept of perfect vacuum was incoherent. However, according to Nader El-Bizri, the physicist Ibn al-Haytham (Alhazen, 965–1039) and the Mu'tazili theologians disagreed with Aristotle and Al-Farabi, and they supported the existence of a void. Using geometry, Ibn al-Haytham mathematically demonstrated that place ("al-makan") is the imagined three-dimensional void between the inner surfaces of a containing body. According to Ahmad Dallal, Abū Rayhān al-Bīrūnī also states that "there is no observable evidence that rules out the possibility of vacuum". The suction pump later appeared in Europe from the 15th century.
Medieval thought experiments into the idea of a vacuum considered whether a vacuum was present, if only for an instant, between two flat plates when they were rapidly separated. There was much discussion of whether the air moved in quickly enough as the plates were separated, or, as Walter Burley postulated, whether a 'celestial agent' prevented the vacuum arising. The commonly held view that nature abhorred a vacuum was called "horror vacui". Speculation that even God could not create a vacuum if he wanted to was shut down by the 1277 Paris condemnations of Bishop Etienne Tempier, which required there to be no restrictions on the powers of God, which led to the conclusion that God could create a vacuum if he so wished.
Jean Buridan reported in the 14th century that teams of ten horses could not pull open bellows when the port was sealed.
The 17th century saw the first attempts to quantify measurements of partial vacuum. Evangelista Torricelli's mercury barometer of 1643 and Blaise Pascal's experiments that both demonstrated a partial vacuum.
In 1654, Otto von Guericke invented the first vacuum pump and conducted his famous Magdeburg hemispheres experiment, showing that teams of horses could not separate two hemispheres from which the air had been partially evacuated. Robert Boyle improved Guericke's design and with the help of Robert Hooke further developed vacuum pump technology. Thereafter, research into the partial vacuum lapsed until 1850 when August Toepler invented the Toepler Pump and Heinrich Geissler invented the mercury displacement pump in 1855, achieving a partial vacuum of about 10 Pa (0.1 Torr). A number of electrical properties become observable at this vacuum level, which renewed interest in further research.
While outer space provides the most rarefied example of a naturally occurring partial vacuum, the heavens were originally thought to be seamlessly filled by a rigid indestructible material called aether. Borrowing somewhat from the pneuma of Stoic physics, aether came to be regarded as the rarefied air from which it took its name, (see Aether (mythology)). Early theories of light posited a ubiquitous terrestrial and celestial medium through which light propagated. Additionally, the concept informed Isaac Newton's explanations of both refraction and of radiant heat. 19th century experiments into this luminiferous aether attempted to detect a minute drag on the Earth's orbit. While the Earth does, in fact, move through a relatively dense medium in comparison to that of interstellar space, the drag is so minuscule that it could not be detected. In 1912, astronomer Henry Pickering commented: "While the interstellar absorbing medium may be simply the ether, [it] is characteristic of a gas, and free gaseous molecules are certainly there".
In 1930, Paul Dirac proposed a model of the vacuum as an infinite sea of particles possessing negative energy, called the Dirac sea. This theory helped refine the predictions of his earlier formulated Dirac equation, and successfully predicted the existence of the positron, confirmed two years later. Werner Heisenberg's uncertainty principle formulated in 1927, predict a fundamental limit within which instantaneous position and momentum, or energy and time can be measured. This has far reaching consequences on the "emptiness" of space between particles. In the late 20th century, so-called virtual particles that arise spontaneously from empty space were confirmed.
Classical field theories.
The strictest criteria to define a vacuum is a region of space and time where all the components of the stress–energy tensor are zero. It means that this region is empty of energy and momentum, and by consequence, it must be empty of particles and other physical fields (such as electromagnetism) that contain energy and momentum.
Gravity.
In general relativity, a vanishing stress-energy tensor implies, through Einstein field equations, the vanishing of all the components of the Ricci tensor. Vacuum does not mean that the curvature of space-time is necessarily flat: the gravitational field can still produce curvature in a vacuum in the form of tidal forces and gravitational waves (technically, these phenomena are the components of the Weyl tensor). The black hole (with zero electric charge) is an elegant example of a region completely "filled" with vacuum, but still showing a strong curvature.
Electromagnetism.
In classical electromagnetism, the vacuum of free space, or sometimes just "free space" or "perfect vacuum", is a standard reference medium for electromagnetic effects. Some authors refer to this reference medium as "classical vacuum", a terminology intended to separate this concept from QED vacuum or QCD vacuum, where vacuum fluctuations can produce transient virtual particle densities and a relative permittivity and relative permeability that are not identically unity.
In the theory of classical electromagnetism, free space has the following properties:
The vacuum of classical electromagnetism can be viewed as an idealized electromagnetic medium with the constitutive relations in SI units:
relating the electric displacement field D to the electric field E and the magnetic field or "H"-field H to the magnetic induction or "B"-field B. Here r is a spatial location and "t" is time.
Quantum mechanics.
In quantum mechanics and quantum field theory, the vacuum is defined as the state (that is, the solution to the equations of the theory) with the lowest possible energy (the ground state of the Hilbert space). In quantum electrodynamics this vacuum is referred to as 'QED vacuum' to distinguish it from the vacuum of quantum chromodynamics, denoted as QCD vacuum. QED vacuum is a state with no matter particles (hence the name), and also no photons. As described above, this state is impossible to achieve experimentally. (Even if every matter particle could somehow be removed from a volume, it would be impossible to eliminate all the blackbody photons.) Nonetheless, it provides a good model for realizable vacuum, and agrees with a number of experimental observations as described next.
QED vacuum has interesting and complex properties. In QED vacuum, the electric and magnetic fields have zero average values, but their variances are not zero. As a result, QED vacuum contains vacuum fluctuations (virtual particles that hop into and out of existence), and a finite energy called vacuum energy. Vacuum fluctuations are an essential and ubiquitous part of quantum field theory. Some experimentally verified effects of vacuum fluctuations include spontaneous emission and the Lamb shift. Coulomb's law and the electric potential in vacuum near an electric charge are modified.
Theoretically, in QCD vacuum multiple vacuum states can coexist. The starting and ending of cosmological inflation is thought to have arisen from transitions between different vacuum states. For theories obtained by quantization of a classical theory, each stationary point of the energy in the configuration space gives rise to a single vacuum. String theory is believed to have a huge number of vacua — the so-called string theory landscape.
Outer space.
Outer space has very low density and pressure, and is the closest physical approximation of a perfect vacuum. But no vacuum is truly perfect, not even in interstellar space, where there are still a few hydrogen atoms per cubic meter.
Stars, planets and moons keep their atmospheres by gravitational attraction, and as such, atmospheres have no clearly delineated boundary: the density of atmospheric gas simply decreases with distance from the object. The Earth's atmospheric pressure drops to about at of altitude, the Kármán line, which is a common definition of the boundary with outer space. Beyond this line, isotropic gas pressure rapidly becomes insignificant when compared to radiation pressure from the Sun and the dynamic pressure of the solar winds, so the definition of pressure becomes difficult to interpret. The thermosphere in this range has large gradients of pressure, temperature and composition, and varies greatly due to space weather. Astrophysicists prefer to use number density to describe these environments, in units of particles per cubic centimetre.
But although it meets the definition of outer space, the atmospheric density within the first few hundred kilometers above the Kármán line is still sufficient to produce significant drag on satellites. Most artificial satellites operate in this region called low Earth orbit and must fire their engines every few days to maintain orbit. The drag here is low enough that it could theoretically be overcome by radiation pressure on solar sails, a proposed propulsion system for interplanetary travel. Planets are too massive for their trajectories to be significantly affected by these forces, although their atmospheres are eroded by the solar winds.
All of the observable universe is filled with large numbers of photons, the so-called cosmic background radiation, and quite likely a correspondingly large number of neutrinos. The current temperature of this radiation is about 3 K, or −270 degrees Celsius or −454 degrees Fahrenheit.
Measurement.
The quality of a vacuum is indicated by the amount of matter remaining in the system, so that a high quality vacuum is one with very little matter left in it. Vacuum is primarily measured by its absolute pressure, but a complete characterization requires further parameters, such as temperature and chemical composition. One of the most important parameters is the mean free path (MFP) of residual gases, which indicates the average distance that molecules will travel between collisions with each other. As the gas density decreases, the MFP increases, and when the MFP is longer than the chamber, pump, spacecraft, or other objects present, the continuum assumptions of fluid mechanics do not apply. This vacuum state is called "high vacuum", and the study of fluid flows in this regime is called particle gas dynamics. The MFP of air at atmospheric pressure is very short, 70 nm, but at 100 mPa (~) the MFP of room temperature air is roughly 100 mm, which is on the order of everyday objects such as vacuum tubes. The Crookes radiometer turns when the MFP is larger than the size of the vanes.
Vacuum quality is subdivided into ranges according to the technology required to achieve it or measure it. These ranges do not have universally agreed definitions, but a typical distribution is shown in the following table. As we travel into orbit, outer space and ultimately intergalactic space, the pressure varies by several orders of magnitude.
Relative versus absolute measurement.
Vacuum is measured in units of pressure, typically as a subtraction relative to ambient atmospheric pressure on Earth. But the amount of relative measurable vacuum varies with local conditions. On the surface of Jupiter, where ground level atmospheric pressure is much higher than on Earth, much higher relative vacuum readings would be possible. On the surface of the moon with almost no atmosphere, it would be extremely difficult to create a measurable vacuum relative to the local environment.
Similarly, much higher than normal relative vacuum readings are possible deep in the Earth's ocean. A submarine maintaining an internal pressure of 1 atmosphere submerged to a depth of 10 atmospheres (98 metres; a 9.8 metre column of seawater has the equivalent weight of 1 atm) is effectively a vacuum chamber keeping out the crushing exterior water pressures, though the 1 atm inside the submarine would not normally be considered a vacuum.
Therefore, to properly understand the following discussions of vacuum measurement, it is important that the reader assumes the relative measurements are being done on Earth at sea level, at exactly 1 atmosphere of ambient atmospheric pressure.
Measurements relative to 1 atm.
The SI unit of pressure is the pascal (symbol Pa), but vacuum is often measured in torrs, named for Torricelli, an early Italian physicist (1608–1647). A torr is equal to the displacement of a millimeter of mercury (mmHg) in a manometer with 1 torr equaling 133.3223684 pascals above absolute zero pressure. Vacuum is often also measured on the barometric scale or as a percentage of atmospheric pressure in bars or atmospheres. Low vacuum is often measured in millimeters of mercury (mmHg) or pascals (Pa) below standard atmospheric pressure. "Below atmospheric" means that the absolute pressure is equal to the current atmospheric pressure.
In other words, most low vacuum gauges that read, for example 50.79 Torr. Many inexpensive low vacuum gauges have a margin of error and may report a vacuum of 0 Torr but in practice this generally requires a two-stage rotary vane or other medium type of vacuum pump to go much beyond (lower than) 1 torr.
Measuring instruments.
Many devices are used to measure the pressure in a vacuum, depending on what range of vacuum is needed.
Hydrostatic gauges (such as the mercury column manometer) consist of a vertical column of liquid in a tube whose ends are exposed to different pressures. The column will rise or fall until its weight is in equilibrium with the pressure differential between the two ends of the tube. The simplest design is a closed-end U-shaped tube, one side of which is connected to the region of interest. Any fluid can be used, but mercury is preferred for its high density and low vapour pressure. Simple hydrostatic gauges can measure pressures ranging from 1 torr (100 Pa) to above atmospheric. An important variation is the McLeod gauge which isolates a known volume of vacuum and compresses it to multiply the height variation of the liquid column. The McLeod gauge can measure vacuums as high as 10−6 torr (0.1 mPa), which is the lowest direct measurement of pressure that is possible with current technology. Other vacuum gauges can measure lower pressures, but only indirectly by measurement of other pressure-controlled properties. These indirect measurements must be calibrated via a direct measurement, most commonly a McLeod gauge.
The kenotometer is a particular type of hydrostatic gauge, typically used in power plants using steam turbines. The kenotometer measures the vacuum in the steam space of the condenser, that is, the exhaust of the last stage of the turbine.
Mechanical or elastic gauges depend on a Bourdon tube, diaphragm, or capsule, usually made of metal, which will change shape in response to the pressure of the region in question. A variation on this idea is the capacitance manometer, in which the diaphragm makes up a part of a capacitor. A change in pressure leads to the flexure of the diaphragm, which results in a change in capacitance. These gauges are effective from 103 torr to 10−4 torr, and beyond.
Thermal conductivity gauges rely on the fact that the ability of a gas to conduct heat decreases with pressure. In this type of gauge, a wire filament is heated by running current through it. A thermocouple or Resistance Temperature Detector (RTD) can then be used to measure the temperature of the filament. This temperature is dependent on the rate at which the filament loses heat to the surrounding gas, and therefore on the thermal conductivity. A common variant is the Pirani gauge which uses a single platinum filament as both the heated element and RTD. These gauges are accurate from 10 torr to 10−3 torr, but they are sensitive to the chemical composition of the gases being measured.
Ion gauges are used in ultrahigh vacuum. They come in two types: hot cathode and cold cathode. In the hot cathode version an electrically heated filament produces an electron beam. The electrons travel through the gauge and ionize gas molecules around them. The resulting ions are collected at a negative electrode. The current depends on the number of ions, which depends on the pressure in the gauge. Hot cathode gauges are accurate from 10−3 torr to 10−10 torr. The principle behind cold cathode version is the same, except that electrons are produced in a discharge created by a high voltage electrical discharge. Cold cathode gauges are accurate from 10−2 torr to 10−9 torr. Ionization gauge calibration is very sensitive to construction geometry, chemical composition of gases being measured, corrosion and surface deposits. Their calibration can be invalidated by activation at atmospheric pressure or low vacuum. The composition of gases at high vacuums will usually be unpredictable, so a mass spectrometer must be used in conjunction with the ionization gauge for accurate measurement.
Uses.
Vacuum is useful in a variety of processes and devices. Its first widespread use was in the incandescent light bulb to protect the filament from chemical degradation. The chemical inertness produced by a vacuum is also useful for electron beam welding, cold welding, vacuum packing and vacuum frying. Ultra-high vacuum is used in the study of atomically clean substrates, as only a very good vacuum preserves atomic-scale clean surfaces for a reasonably long time (on the order of minutes to days). High to ultra-high vacuum removes the obstruction of air, allowing particle beams to deposit or remove materials without contamination. This is the principle behind chemical vapor deposition, physical vapor deposition, and dry etching which are essential to the fabrication of semiconductors and optical coatings, and to surface science. The reduction of convection provides the thermal insulation of thermos bottles. Deep vacuum lowers the boiling point of liquids and promotes low temperature outgassing which is used in freeze drying, adhesive preparation, distillation, metallurgy, and process purging. The electrical properties of vacuum make electron microscopes and vacuum tubes possible, including cathode ray tubes. The elimination of air friction is useful for flywheel energy storage and ultracentrifuges.
Vacuum-driven machines.
Vacuums are commonly used to produce suction, which has an even wider variety of applications. The Newcomen steam engine used vacuum instead of pressure to drive a piston. In the 19th century, vacuum was used for traction on Isambard Kingdom Brunel's experimental atmospheric railway. Vacuum brakes were once widely used on trains in the UK but, except on heritage railways, they have been replaced by air brakes.
Manifold vacuum can be used to drive accessories on automobiles. The best-known application is the vacuum servo, used to provide power assistance for the brakes. Obsolete applications include vacuum-driven windscreen wipers and Autovac fuel pumps. Some aircraft instruments (Attitude Indicator (AI) and the Heading Indicator (HI)) are typically vacuum-powered, as protection against loss of all (electrically powered) instruments, since early aircraft often did not have electrical systems, and since there are two readily available sources of vacuum on a moving aircraft—the engine and an external venturi.
Vacuum induction melting uses electromagnetic induction within a vacuum.
Maintaining a vacuum in the Condenser is an important aspect of the efficient operation of steam turbines. A steam jet ejector or liquid ring vacuum pump is used for this purpose. The typical vacuum maintained in the Condenser steam space at the exhaust of the turbine (also called Condenser Backpressure) is in the range 5 to 15 kPa (absolute), depending on the type of condenser and the ambient conditions.
Outgassing.
Evaporation and sublimation into a vacuum is called outgassing. All materials, solid or liquid, have a small vapour pressure, and their outgassing becomes important when the vacuum pressure falls below this vapour pressure. In man-made systems, outgassing has the same effect as a leak and can limit the achievable vacuum. Outgassing products may condense on nearby colder surfaces, which can be troublesome if they obscure optical instruments or react with other materials. This is of great concern to space missions, where an obscured telescope or solar cell can ruin an expensive mission.
The most prevalent outgassing product in man-made vacuum systems is water absorbed by chamber materials. It can be reduced by desiccating or baking the chamber, and removing absorbent materials. Outgassed water can condense in the oil of rotary vane pumps and reduce their net speed drastically if gas ballasting is not used. High vacuum systems must be clean and free of organic matter to minimize outgassing.
Ultra-high vacuum systems are usually baked, preferably under vacuum, to temporarily raise the vapour pressure of all outgassing materials and boil them off. Once the bulk of the outgassing materials are boiled off and evacuated, the system may be cooled to lower vapour pressures and minimize residual outgassing during actual operation. Some systems are cooled well below room temperature by liquid nitrogen to shut down residual outgassing and simultaneously cryopump the system.
Pumping and ambient air pressure.
Fluids cannot generally be pulled, so a vacuum cannot be created by suction. Suction can spread and dilute a vacuum by letting a higher pressure push fluids into it, but the vacuum has to be created first before suction can occur. The easiest way to create an artificial vacuum is to expand the volume of a container. For example, the diaphragm muscle expands the chest cavity, which causes the volume of the lungs to increase. This expansion reduces the pressure and creates a partial vacuum, which is soon filled by air pushed in by atmospheric pressure.
To continue evacuating a chamber indefinitely without requiring infinite growth, a compartment of the vacuum can be repeatedly closed off, exhausted, and expanded again. This is the principle behind positive displacement pumps, like the manual water pump for example. Inside the pump, a mechanism expands a small sealed cavity to create a vacuum. Because of the pressure differential, some fluid from the chamber (or the well, in our example) is pushed into the pump's small cavity. The pump's cavity is then sealed from the chamber, opened to the atmosphere, and squeezed back to a minute size.
The above explanation is merely a simple introduction to vacuum pumping, and is not representative of the entire range of pumps in use. Many variations of the positive displacement pump have been developed, and many other pump designs rely on fundamentally different principles. Momentum transfer pumps, which bear some similarities to dynamic pumps used at higher pressures, can achieve much higher quality vacuums than positive displacement pumps. Entrapment pumps can capture gases in a solid or absorbed state, often with no moving parts, no seals and no vibration. None of these pumps are universal; each type has important performance limitations. They all share a difficulty in pumping low molecular weight gases, especially hydrogen, helium, and neon.
The lowest pressure that can be attained in a system is also dependent on many things other than the nature of the pumps. Multiple pumps may be connected in series, called stages, to achieve higher vacuums. The choice of seals, chamber geometry, materials, and pump-down procedures will all have an impact. Collectively, these are called "vacuum technique". And sometimes, the final pressure is not the only relevant characteristic. Pumping systems differ in oil contamination, vibration, preferential pumping of certain gases, pump-down speeds, intermittent duty cycle, reliability, or tolerance to high leakage rates.
In ultra high vacuum systems, some very "odd" leakage paths and outgassing sources must be considered. The water absorption of aluminium and palladium becomes an unacceptable source of outgassing, and even the adsorptivity of hard metals such as stainless steel or titanium must be considered. Some oils and greases will boil off in extreme vacuums. The permeability of the metallic chamber walls may have to be considered, and the grain direction of the metallic flanges should be parallel to the flange face.
The lowest pressures currently achievable in laboratory are about 10−13 torr (13 pPa). However, pressures as low as (6.7 fPa) have been indirectly measured in a 4 K cryogenic vacuum system. This corresponds to ≈100 particles/cm3.
Effects on humans and animals.
Humans and animals exposed to vacuum will lose consciousness after a few seconds and die of hypoxia within minutes, but the symptoms are not nearly as graphic as commonly depicted in media and popular culture. The reduction in pressure lowers the temperature at which blood and other body fluids boil, but the elastic pressure of blood vessels ensures that this boiling point remains above the internal body temperature of Although the blood will not boil, the formation of gas bubbles in bodily fluids at reduced pressures, known as ebullism, is still a concern. The gas may bloat the body to twice its normal size and slow circulation, but tissues are elastic and porous enough to prevent rupture. Swelling and ebullism can be restrained by containment in a flight suit. Shuttle astronauts wore a fitted elastic garment called the Crew Altitude Protection Suit (CAPS) which prevents ebullism at pressures as low as 2 kPa (15 Torr). Rapid boiling will cool the skin and create frost, particularly in the mouth, but this is not a significant hazard.
Animal experiments show that rapid and complete recovery is normal for exposures shorter than 90 seconds, while longer full-body exposures are fatal and resuscitation has never been successful. There is only a limited amount of data available from human accidents, but it is consistent with animal data. Limbs may be exposed for much longer if breathing is not impaired. Robert Boyle was the first to show in 1660 that vacuum is lethal to small animals.
An experiment indicates that plants are able to survive in a low pressure environment (1.5 kPa) for about 30 minutes.
During 1942, in one of a series of experiments on human subjects for the Luftwaffe, the Nazi regime experimented on prisoners in Dachau concentration camp by exposing them to low pressure.
Cold or oxygen-rich atmospheres can sustain life at pressures much lower than atmospheric, as long as the density of oxygen is similar to that of standard sea-level atmosphere. The colder air temperatures found at altitudes of up to 3 km generally compensate for the lower pressures there. Above this altitude, oxygen enrichment is necessary to prevent altitude sickness in humans that did not undergo prior acclimatization, and spacesuits are necessary to prevent ebullism above 19 km. Most spacesuits use only 20 kPa (150 Torr) of pure oxygen. This pressure is high enough to prevent ebullism, but decompression sickness and gas embolisms can still occur if decompression rates are not managed.
Rapid decompression can be much more dangerous than vacuum exposure itself. Even if the victim does not hold his or her breath, venting through the windpipe may be too slow to prevent the fatal rupture of the delicate alveoli of the lungs. Eardrums and sinuses may be ruptured by rapid decompression, soft tissues may bruise and seep blood, and the stress of shock will accelerate oxygen consumption leading to hypoxia. Injuries caused by rapid decompression are called barotrauma. A pressure drop of 13 kPa (100 Torr), which produces no symptoms if it is gradual, may be fatal if it occurs suddenly.
Some extremophile microrganisms, such as tardigrades, can survive vacuum for a period of days or weeks.

</doc>
<doc id="308158" url="https://en.wikipedia.org/wiki?curid=308158" title="Vacuum energy">
Vacuum energy

Vacuum energy is an underlying background energy that exists in space throughout the entire Universe. One contribution to the vacuum energy may be from virtual particles which are thought to be particle pairs that blink into existence and then annihilate in a timespan too short to observe. They are expected to do this everywhere, throughout the Universe. Their behavior is codified in Heisenberg's energy–time uncertainty principle. Still, the exact effect of such fleeting bits of energy is difficult to quantify.
The effects of vacuum energy can be experimentally observed in various phenomena such as spontaneous emission, the Casimir effect and the Lamb shift, and are thought to influence the behavior of the Universe on cosmological scales. Using the upper limit of the cosmological constant, the vacuum energy of free space has been estimated to be 10−9 joules (10−2 ergs) per cubic meter. However, in both quantum electrodynamics (QED) and stochastic electrodynamics (SED), consistency with the principle of Lorentz covariance and with the magnitude of the Planck constant requires it to have a much larger value of 10113 joules per cubic meter. This huge discrepancy is known as the vacuum catastrophe.
Origin.
Quantum field theory states that all fundamental fields, such as the electromagnetic field, must be quantized at each and every point in space. A field in physics may be envisioned as if space were filled with interconnected vibrating balls and springs, and the strength of the field were like the displacement of a ball from its rest position. The theory requires "vibrations" in, or more accurately changes in the strength of, such a field to propagate as per the appropriate wave equation for the particular field in question. The second quantization of quantum field theory requires that each such ball-spring combination be quantized, that is, that the strength of the field be quantized at each point in space. Canonically, if the field at each point in space is a simple harmonic oscillator, its quantization places a quantum harmonic oscillator at each point. Excitations of the field correspond to the elementary particles of particle physics. Thus, according to the theory, even the vacuum has a vastly complex structure and all calculations of quantum field theory must be made in relation to this model of the vacuum.
The theory considers vacuum to implicitly have the same properties as a particle, such as spin or polarization in the case of light, energy, and so on. According to the theory, most of these properties cancel out on average leaving the vacuum empty in the literal sense of the word. One important exception, however, is the vacuum energy or the vacuum expectation value of the energy. The quantization of a simple harmonic oscillator requires the lowest possible energy, or zero-point energy of such an oscillator to be:
formula_1
Summing over all possible oscillators at all points in space gives an infinite quantity. To remove this infinity, one may argue that only differences in energy are physically measurable, much as the concept of potential energy has been treated in classical mechanics for centuries. This argument is the underpinning of the theory of renormalization. In all practical calculations, this is how the infinity is handled.
Vacuum energy can also be thought of in terms of virtual particles (also known as vacuum fluctuations) which are created and destroyed out of the vacuum. These particles are always created out of the vacuum in particle-antiparticle pairs, which in most cases shortly annihilate each other and disappear. However, these particles and antiparticles may interact with others before disappearing, a process which can be mapped using Feynman diagrams. Note that this method of computing vacuum energy is mathematically equivalent to having a quantum harmonic oscillator at each point and, therefore, suffers the same renormalization problems.
Additional contributions to the vacuum energy come from spontaneous symmetry breaking in quantum field theory.
Implications.
Vacuum energy has a number of consequences. In 1948, Dutch physicists Hendrik B. G. Casimir and Dirk Polder predicted the existence of a tiny attractive force between closely placed metal plates due to resonances in the vacuum energy in the space between them. This is now known as the Casimir effect and has since been extensively experimentally verified. It is therefore believed that the vacuum energy is "real" in the same sense that more familiar conceptual objects such as electrons, magnetic fields, etc., are real. However, alternate explanations for the Casimir effect have since been proposed.
Other predictions are harder to verify. Vacuum fluctuations are always created as particle–antiparticle pairs. The creation of these virtual particles near the event horizon of a black hole has been hypothesized by physicist Stephen Hawking to be a mechanism for the eventual "evaporation" of black holes. The net energy of the Universe remains zero so long as the particle pairs annihilate each other within Planck time. If one of the pair is pulled into the black hole before this, then the other particle becomes "real" and energy/mass is essentially radiated into space from the black hole. This loss is cumulative and could result in the black hole's disappearance over time. The time required is dependent on the mass of the black hole (the equations indicate that the smaller the black hole, the more rapidly it evaporates) but could be on the order of 10100 years for large solar-mass black holes.
The vacuum energy also has important consequences for physical cosmology. General relativity predicts that energy is equivalent to mass, and therefore, if the vacuum energy is "really there", it should exert a gravitational force. Essentially, a non-zero vacuum energy is expected to contribute to the cosmological constant, which affects the expansion of the universe. In the special case of vacuum energy, general relativity stipulates that the gravitational field is proportional to ρ+3p (where ρ is the mass-energy density, and p is the pressure). Quantum theory of the vacuum further stipulates that the pressure of the zero-state vacuum energy is always negative and equal in magnitude to ρ. Thus, the total is ρ+3p = ρ-3ρ = -2ρ, a negative value. If indeed the vacuum ground state has non-zero energy, the calculation implies a repulsive gravitational field, giving rise to acceleration of the expansion of the universe. However, the vacuum energy is mathematically infinite without renormalization, which is based on the assumption that we can only measure energy in a relative sense, which is not true if we can observe it indirectly via the cosmological constant.
The existence of vacuum energy is also sometimes used as theoretical justification for the possibility of free-energy machines. It has been argued that due to the broken symmetry (in QED), free energy does not violate conservation of energy, since the laws of thermodynamics only apply to equilibrium systems. However, consensus amongst physicists is that this is unknown as the nature of vacuum energy remains an unsolved problem. In particular, the second law of thermodynamics is unaffected by the existence of vacuum energy. However, in Stochastic Electrodynamics, the energy density is taken to be a classical random noise wave field which consists of real electromagnetic noise waves propagating isotropically in all directions. The energy in such a wave field would seem to be accessible, e.g., with nothing more complicated than a directional coupler. The most obvious difficulty appears to be the spectral distribution of the energy, which compatibility with Lorentz invariance requires to take the form Kf3, where K is a constant and f denotes frequency. It follows that the energy and momentum flux in this wave field only becomes significant at extremely short wavelengths where directional coupler technology is currently lacking.
History.
In 1934, Georges Lemaître used an unusual perfect-fluid equation of state to interpret the cosmological constant as due to vacuum energy. In 1948, the Casimir effect provided the first experimental verification of the existence of vacuum energy. In 1957, Lee and Yang proved the concepts of broken symmetry and parity violation, for which they won the Nobel prize. In 1973, Edward Tryon proposed the zero-energy universe hypothesis: that the Universe may be a large-scale quantum-mechanical vacuum fluctuation where positive mass-energy is balanced by negative gravitational potential energy. During the 1980s, there were many attempts to relate the fields that generate the vacuum energy to specific fields that were predicted by attempts at a Grand unification theory and to use observations of the Universe to confirm one or another version. However, the exact nature of the particles (or fields) that generate vacuum energy, with a density such as that required by inflation theory, remains a mystery.

</doc>
<doc id="701188" url="https://en.wikipedia.org/wiki?curid=701188" title="Vacuum state">
Vacuum state

In quantum field theory, the vacuum state (also called the vacuum) is the quantum state with the lowest possible energy. Generally, it contains no physical particles. Zero-point field is sometimes used as a synonym for the vacuum state of an individual quantized field.
According to present-day understanding of what is called the vacuum state or the quantum vacuum, it is "by no means a simple empty space", and again: "it is a mistake to think of any physical vacuum as some absolutely empty void." According to quantum mechanics, the vacuum state is not truly empty but instead contains fleeting electromagnetic waves and particles that pop into and out of existence.
The QED vacuum of quantum electrodynamics (or QED) was the first vacuum of quantum field theory to be developed. QED originated in the 1930s, and in the late 1940s and early 1950s it was reformulated by Feynman, Tomonaga and Schwinger, who jointly received the Nobel prize for this work in 1965. Today the electromagnetic interactions and the weak interactions are unified in the theory of the electroweak interaction.
The Standard Model is a generalization of the QED work to include all the known elementary particles and their interactions (except gravity). Quantum chromodynamics is the portion of the Standard Model that deals with strong interactions, and QCD vacuum is the vacuum of quantum chromodynamics. It is the object of study in the Large Hadron Collider and the Relativistic Heavy Ion Collider, and is related to the so-called "vacuum structure of strong interactions".
Non-zero expectation value.
If the quantum field theory can be accurately described through perturbation theory, then the properties of the vacuum are analogous to the properties of the ground state of a quantum mechanical harmonic oscillator (or more accurately, the ground state of a QM problem). In this case the vacuum expectation value (VEV) of any field operator vanishes. For quantum field theories in which perturbation theory breaks down at low energies (for example, Quantum chromodynamics or the BCS theory of superconductivity) field operators may have non-vanishing vacuum expectation values called condensates. In the Standard Model, the non-zero vacuum expectation value of the Higgs field, arising from spontaneous symmetry breaking, is the mechanism by which the other fields in the theory acquire mass.
Energy.
In many situations, the vacuum state can be defined to have zero energy, although the actual situation is considerably more subtle. The vacuum state is associated with a zero-point energy, and this zero-point energy has measurable effects. In the laboratory, it may be detected as the Casimir effect. In physical cosmology, the energy of the cosmological vacuum appears as the cosmological constant. In fact, the energy of a cubic centimeter of empty space has been calculated figuratively to be one trillionth of an erg (or 0.6 eV). An outstanding requirement imposed on a potential Theory of Everything is that the energy of the quantum vacuum state must explain the physically observed cosmological constant.
Symmetry.
For a relativistic field theory, the vacuum is Poincaré invariant, which follows from
Wightman axioms but can be also proved directly without these axioms. Poincaré invariance implies that only scalar combinations of field operators have non-vanishing VEV's. The VEV may break some of the internal symmetries of the Lagrangian of the field theory. In this case the vacuum has less symmetry than the theory allows, and one says that "spontaneous symmetry breaking" has occurred. See Higgs mechanism, standard model.
Electrical permittivity.
In principle, quantum corrections to Maxwell's equations can cause the experimental electrical permittivity ε of the vacuum state to deviate from the defined scalar value ε0 of the electric constant. These theoretical developments are described, for example, in Dittrich and Gies.
In particular, the theory of quantum electrodynamics predicts that the QED vacuum should exhibit nonlinear effects that will make it behave like a birefringent material with ε slightly greater than ε0 for extremely strong electric fields. Explanations for dichroism from particle physics, outside quantum electrodynamics, also have been proposed. Active attempts to measure such effects have been unsuccessful so far.
Notations.
The vacuum state is written as formula_1 or formula_2. The vacuum expectation value of any field φ, should be written as formula_3, but is usually condensed to formula_4.
Virtual particles.
The presence of virtual particles can be rigorously based upon the non-commutation of the quantized electromagnetic fields. Non-commutation means that although the average values of the fields vanish in a quantum vacuum, their variances do not. The term "vacuum fluctuations" refers to the variance of the field strength in the minimal energy state, and is described picturesquely as evidence of "virtual particles".
It is sometimes attempted to provide an intuitive picture of virtual particles based upon the Heisenberg energy-time uncertainty principle:
(with "ΔE" and "Δt" being the energy and time variations respectively; "ΔE" is the accuracy in the measurement of energy and "Δt" is the time taken in the measurement, and is the Planck constant divided by) arguing along the lines that the short lifetime of virtual particles allows the "borrowing" of large energies from the vacuum and thus permits particle generation for short times.
Although the phenomenon of virtual particles is accepted, this interpretation of the energy-time uncertainty relation is not universal. One issue is the use of an uncertainty relation limiting measurement accuracy as though a time uncertainty "Δt" determines a "budget" for borrowing energy "ΔE". Another issue is the meaning of "time" in this relation, because energy and time (unlike position and momentum , for example) do not satisfy a canonical commutation relation (such as ). Various schemes have been advanced to construct an observable that has some kind of time interpretation, and yet does satisfy a canonical commutation relation with energy. The very many approaches to the energy-time uncertainty principle are a long and continuing subject.
Physical nature of the quantum vacuum.
According to Astrid Lambrecht (2002): "When one empties out a space of all matter and lowers the temperature to absolute zero, one produces in a "Gedankenexperiment" the quantum vacuum state."
According to Fowler & Guggenheim (1939/1965), the third law of thermodynamics may be precisely enunciated as follows:
Photon-photon interaction can occur only through interaction with the vacuum state of some other field, for example through the Dirac electron-positron vacuum field; this is associated with the concept of vacuum polarization.
According to Milonni (1994): "... "all quantum fields have zero-point energies and vacuum fluctuations."" This means that there is a component of the quantum vacuum respectively for each component field (considered in the conceptual absence of the other fields), such as the electromagnetic field, the Dirac electron-positron field, and so on.
According to Milonni (1994), some of the effects attributed to the vacuum electromagnetic field can have several physical interpretations, some more conventional than others. The Casimir attraction between uncharged conductive plates is often proposed as an example of an effect of the vacuum electromagnetic field. Schwinger, DeRaad, and Milton (1978) are cited by Milonni (1994) as validly, though unconventionally, explaining the Casimir effect with a model in which "the vacuum is regarded as truly a state with all physical properties equal to zero." In this model, the observed phenomena are explained as the effects of the electron motions on the electromagnetic field, called the source field effect. Milonni writes: "The basic idea here will be that the Casimir force may be derived from the source fields alone even in completely conventional QED, ..." Milonni provides detailed argument that the measurable physical effects usually attributed to the vacuum electromagnetic field cannot be explained by that field alone, but require in addition a contribution from the self-energy of the electrons, or their radiation reaction. He writes: "The radiation reaction and the vacuum fields are two aspects of the same thing when it comes to physical interpretations of various QED processes including the Lamb shift, van der Waals forces, and Casimir effects." 
This point of view is also stated by Jaffe (2005): "The Casimir force can be calculated without reference to vacuum fluctuations, and like all other observable effects in QED, it vanishes as the fine structure constant, , goes to zero."

</doc>
<doc id="32344" url="https://en.wikipedia.org/wiki?curid=32344" title="Variance">
Variance

In probability theory and statistics, variance measures how far a set of numbers is spread out. A variance of zero indicates that all the values are identical. Variance is always non-negative: a small variance indicates that the data points tend to be very close to the mean (expected value) and hence to each other, while a high variance indicates that the data points are very spread out around the mean and from each other.
An equivalent measure is the square root of the variance, called the standard deviation. The standard deviation has the same dimension as the data, and hence is comparable to deviations from the mean.
There are two distinct concepts that are both called "variance". One variance is a characteristic of a set of observations. The other is part of a theoretical probability distribution and is defined by an equation. When variance is calculated from observations, those observations are either measured from a real world system or generated by a theoretical probability distribution or other generating model. If all possible observations of the system are present then the calculated variance is called the population variance. Normally, however, only a subset is available, and the variance calculated from this is called the sample variance. The variance calculated from a sample is considered an estimate of the full population variance. There are multiple ways to calculate an estimate of the population variance, as discussed in the section below.
The two kinds of variance are closely related. To see how, consider that a theoretical probability distribution can be used as a generator of observations. If an infinite number of observations are generated using a distribution, then the sample variance calculated from that infinite set will match the value calculated using the distribution's equation for variance.
The variance is one of several descriptors of a probability distribution. In particular, the variance is one of the moments of a distribution. In that context, it forms part of a systematic approach to distinguishing between probability distributions. While other such approaches have been developed, those based on moments are advantageous in terms of mathematical and computational simplicity.
Definition.
The variance of a set of samples that is represented by random variable "X" is its second central moment, the expected value of the squared deviation from the mean :
This definition encompasses random variables that are generated by processes that are discrete, continuous, neither, or mixed. The variance can also be thought of as the covariance of a random variable with itself:
The variance is also equivalent to the second cumulant of a probability distribution that generates "X". The variance is typically designated as Var("X"), formula_3, or simply σ2 (pronounced "sigma squared"). The expression for the variance can be expanded:
A mnemonic for the above expression is "mean of square minus square of mean". On computational floating point arithmetic, this equation should not be used, because it suffers from catastrophic cancellation if the two components of the equation are similar in magnitude. There exist numerically stable alternatives.
Continuous random variable.
If the random variable "X" represents samples generated by a continuous with probability density function "f"("x"), then the population variance is given by
where formula_6 is the expected value,
and where the integrals are definite integrals taken for "x" ranging over the range of "X".
If a continuous distribution does not have an expected value, as is the case for the Cauchy distribution, it does not have a variance either. Many other distributions for which the expected value does exist also do not have a finite variance because the integral in the variance definition diverges. An example is a Pareto distribution whose index "k" satisfies .
Discrete random variable.
If the generator of random variable "X" is discrete with probability mass function "x"1 ↦ "p"1, ..., "x""n" ↦ "p""n", then
or equivalently
where formula_6 is the expected value, i.e.
The variance of a set of "n" equally likely values can be written as 
where formula_6 is the expected value, i.e.
The variance of a set of "n" equally likely values can be equivalently expressed, without directly referring to the mean, in terms of squared deviations of all points from each other: 
Examples.
Normal distribution.
The normal distribution with parameters μ and σ is a continuous distribution whose probability density function is given by:
where μ is the mean, and the variance σ² is given as:
The role of the normal distribution in the central limit theorem is in part responsible for the prevalence of the variance in probability and statistics.
Exponential distribution.
The exponential distribution with parameter λ is a continuous distribution whose support is the semi-infinite interval [0,∞). Its probability density function is given by:
and it has expected value μ = λ−1. The variance is equal to:
So for an exponentially distributed random variable σ2 = μ2.
Poisson distribution.
The Poisson distribution with parameter λ is a discrete distribution for "k" = 0, 1, 2, ... Its probability mass function is given by:
and it has expected value μ = λ. The variance is equal to:
So for a Poisson-distributed random variable σ2 = μ.
Binomial distribution.
The binomial distribution with parameters "n" and "p" is a discrete distribution for "k" = 0, 1, 2, ..., "n". Its probability mass function is given by:
and it has expected value μ = "np". The variance is equal to:
Coin toss.
The binomial distribution with formula_24 describes the probability of getting formula_25 heads in formula_26 tosses. Thus the expected value of the number of heads is formula_27, and the variance is formula_28.
Fair die.
A six-sided fair die can be modelled with a discrete random variable with outcomes 1 through 6, each with equal probability formula_29. The expected value is (1 + 2 + 3 + 4 + 5 + 6)/6 = 3.5. Therefore, the variance can be computed to be:
The general formula for the variance of the outcome "X" of a die of n sides is:
Properties.
Basic properties.
Variance is non-negative because the squares are positive or zero.
The variance of a constant random variable is zero, and if the variance of a variable in a data set is 0, then all the entries have the same value.
Variance is invariant with respect to changes in a location parameter. That is, if a constant is added to all values of the variable, the variance is unchanged. 
If all values are scaled by a constant, the variance is scaled by the square of that constant. 
The variance of a sum of two random variables is given by:
where is the covariance.
In general we have for the sum of formula_38 random variables formula_39:
These results lead to the variance of a linear combination as:
If the random variables formula_42 are such that
they are said to be uncorrelated. It follows immediately from the expression given earlier that if the random variables formula_42 are uncorrelated, then the variance of their sum is equal to the sum of their variances, or, expressed symbolically:
Since independent random variables are always uncorrelated, the equation above holds in particular when the random variables formula_46 are independent. Thus independence is sufficient but not necessary for the variance of the sum to equal the sum of the variances.
Sum of uncorrelated variables (Bienaymé formula).
One reason for the use of the variance in preference to other measures of dispersion is that the variance of the sum (or the difference) of uncorrelated random variables is the sum of their variances:
This statement is called the Bienaymé formula and was discovered in 1853. It is often made with the stronger condition that the variables are independent, but being uncorrelated suffices. So if all the variables have the same variance σ2, then, since division by "n" is a linear transformation, this formula immediately implies that the variance of their mean is
That is, the variance of the mean decreases when "n" increases. This formula for the variance of the mean is used in the definition of the standard error of the sample mean, which is used in the central limit theorem.
Product of independent variables.
If two variables X and Y are independent, the variance of their product is given by:
formula_49
Alternatively, using the basic properties of expectation:
formula_50
Product of correlated variables.
In general, if two variables are correlated, the variance of their product is given by:
Sum of correlated variables.
In general, if the variables are correlated, then the variance of their sum is the sum of their covariances:
Here is the covariance, which is zero for independent random variables (if it exists). The formula states that the variance of a sum is equal to the sum of all elements in the covariance matrix of the components. This formula is used in the theory of Cronbach's alpha in classical test theory.
So if the variables have equal variance "σ"2 and the average correlation of distinct variables is "ρ", then the variance of their mean is
This implies that the variance of the mean increases with the average of the correlations. In other words, additional correlated observations are not as effective as additional independent observations at reducing the uncertainty of the mean. Moreover, if the variables have unit variance, for example if they are standardized, then this simplifies to
This formula is used in the Spearman–Brown prediction formula of classical test theory. This converges to "ρ" if "n" goes to infinity, provided that the average correlation remains constant or converges too. So for the variance of the mean of standardized variables with equal correlations or converging average correlation we have
Therefore, the variance of the mean of a large number of standardized variables is approximately equal to their average correlation. This makes clear that the sample mean of correlated variables does not generally converge to the population mean, even though the Law of large numbers states that the sample mean will converge for independent variables.
Weighted sum of variables.
The scaling property and the Bienaymé formula, along with the property of the covariance jointly imply that
This implies that in a weighted sum of variables, the variable with the largest weight will have a disproportionally large weight in the variance of the total. For example, if "X" and "Y" are uncorrelated and the weight of "X" is two times the weight of "Y", then the weight of the variance of "X" will be four times the weight of the variance of "Y".
The expression above can be extended to a weighted sum of multiple variables:
Decomposition.
The general formula for variance decomposition or the law of total variance is: If formula_58 and formula_59 are two random variables, and the variance of formula_58 exists, then
where formula_62 is the conditional expectation of formula_58 given formula_59, and formula_65 is the conditional variance of formula_58 given formula_59. (A more intuitive explanation is that given a particular value of formula_59, then formula_58 follows a distribution with mean formula_62 and variance formula_65. As formula_62 is a function of the variable formula_59, the outer expectation or variance is taken with respect to Y. The above formula tells how to find formula_74 based on the distributions of these two quantities when formula_59 is allowed to vary. A similar formula is applied in analysis of variance, where the corresponding formula is
here formula_77 refers to the Mean of the Squares. In linear regression analysis the corresponding formula is
This can also be derived from the additivity of variances, since the total (observed) score is the sum of the predicted score and the error score, where the latter two are uncorrelated.
Similar decompositions are possible for the sum of squared deviations (sum of squares, formula_79):
Formulae for the variance.
A formula often used for deriving the variance of a theoretical distribution is as follows:
This will be useful when it is possible to derive formulae for the expected value and for the expected value of the square.
This formula is also sometimes used in connection with the sample variance. While useful for hand calculations, it is not advised for computer calculations as it suffers from catastrophic cancellation if the two components of the equation are similar in magnitude and floating point arithmetic is used. This is discussed in the article Algorithms for calculating variance.
Calculation from the CDF.
The population variance for a non-negative random variable can be expressed in terms of the cumulative distribution function "F" using
This expression can be used to calculate the variance in situations where the CDF, but not the density, can be conveniently expressed.
Characteristic property.
The second moment of a random variable attains the minimum value when taken around the first moment (i.e., mean) of the random variable, i.e. formula_84. Conversely, if a continuous function formula_85 satisfies formula_86 for all random variables "X", then it is necessarily of the form formula_87, where . This also holds in the multidimensional case.
Matrix notation for the variance of a linear combination.
Define formula_58 as a column vector of formula_26 random variables formula_90, and formula_91 as a column vector of formula_26 scalars formula_93. Therefore, formula_94 is a linear combination of these random variables, where formula_95 denotes the transpose of formula_91. Also let formula_97 be the covariance matrix of formula_58. The variance of formula_99 is then given by:
Units of measurement.
Unlike expected absolute deviation, the variance of a variable has units that are the square of the units of the variable itself. For example, a variable measured in meters will have a variance measured in square meters. For this reason, describing data sets via their standard deviation or root mean square deviation is often preferred over using the variance. In the dice example the standard deviation is √2.9 ≈ 1.7, slightly larger than the expected absolute deviation of 1.5.
The standard deviation and the expected absolute deviation can both be used as an indicator of the "spread" of a distribution. The standard deviation is more amenable to algebraic manipulation than the expected absolute deviation, and, together with variance and its generalization covariance, is used frequently in theoretical statistics; however the expected absolute deviation tends to be more robust as it is less sensitive to outliers arising from measurement anomalies or an unduly heavy-tailed distribution.
Approximating the variance of a function.
The delta method uses second-order Taylor expansions to approximate the variance of a function of one or more random variables: see Taylor expansions for the moments of functions of random variables. For example, the approximate variance of a function of one variable is given by
provided that "f" is twice differentiable and that the mean and variance of "X" are finite.
Population variance and sample variance.
Real-world observations such as the measurements of yesterday's rain throughout the day typically cannot be complete sets of all possible observations that could be made. As such, the variance calculated from the finite set will in general not match the variance that would have been calculated from the full population of possible observations. This means that one estimates the mean and variance that would have been calculated from an omniscient set of observations by using an estimator equation. The estimator is a function of the sample of "n" observations drawn without observational bias from the whole population of potential observations. In this example that sample would be the set of actual measurements of yesterday's rainfall from available rain gauges within the geography of interest.
The simplest estimators for population mean and population variance are simply the mean and variance of the sample, the sample mean and (uncorrected) sample variance – these are consistent estimators (they converge to the correct value as the number of samples increases), but can be improved. Estimating the population variance by taking the sample's variance is close to optimal in general, but can be improved in two ways. Most simply, the sample variance is computed as an average of squared deviations about the (sample) mean, by dividing by "n." However, using values other than "n" improves the estimator in various ways. Four common values for the denominator are "n," "n" − 1, "n" + 1, and "n" − 1.5: "n" is the simplest (population variance of the sample), "n" − 1 eliminates bias, "n" + 1 minimizes mean squared error for the normal distribution, and "n" − 1.5 mostly eliminates bias in unbiased estimation of standard deviation for the normal distribution.
Firstly, if the omniscient mean is unknown (and is computed as the sample mean), then the sample variance is a biased estimator: it underestimates the variance by a factor of ("n" − 1) / "n"; correcting by this factor (dividing by "n" − 1 instead of "n") is called Bessel's correction. The resulting estimator is unbiased, and is called the (corrected) sample variance or unbiased sample variance. For example, when "n" = 1 the variance of a single observation about the sample mean (itself) is obviously zero regardless of the poplation variance. If the mean is determined in some other way than from the same samples used to estimate the variance then this bias does not arise and the variance can safely be estimated as that of the samples about the (independently known) mean.
Secondly, the sample variance does not generally minimize mean squared error between sample variance and population variance. Correcting for bias often makes this worse: one can always choose a scale factor that performs better than the corrected sample variance, though the optimal scale factor depends on the excess kurtosis of the population (see mean squared error: variance), and introduces bias. This always consists of scaling down the unbiased estimator (dividing by a number larger than "n" − 1), and is a simple example of a shrinkage estimator: one "shrinks" the unbiased estimator towards zero. For the normal distribution, dividing by "n" + 1 (instead of "n" − 1 or "n") minimizes mean squared error. The resulting estimator is biased, however, and is known as the biased sample variation.
Population variance.
In general, the population variance of a "finite" population of size "N" with values "x""i" is given by
where
is the population mean. The population variance therefore matches the variance of the generating probability distribution. In this sense, the concept of population can be extended to continuous random variables with infinite populations.
Sample variance.
In many practical situations, the true variance of a population is not known "a priori" and must be computed somehow. When dealing with extremely large populations, it is not possible to count every object in the population, so the computation must be performed on a sample of the population. Sample variance can also be applied to the estimation of the variance of a continuous distribution from a sample of that distribution.
We take a sample with replacement of "n" values "y"1, ..., "y""n" from the population, where "n" < "N", and estimate the variance on the basis of this sample. Directly taking the variance of the sample data gives the average of the squared deviations:
Here, formula_105 denotes the sample mean: 
Since the "y""i" are selected randomly, both formula_107 and formula_108 are random variables. Their expected values can be evaluated by summing over the ensemble of all possible samples {"y""i"} from the population. For formula_108 this gives:
Hence formula_108 gives an estimate of the population variance that is biased by a factor of formula_112. For this reason, formula_108 is referred to as the "biased sample variance". Correcting for this bias yields the "unbiased sample variance":
Either estimator may be simply referred to as the "sample variance" when the version can be determined by context. The same proof is also applicable for samples taken from a continuous probability distribution.
The use of the term "n" − 1 is called Bessel's correction, and it is also used in sample covariance and the sample standard deviation (the square root of variance). The square root is a concave function and thus introduces negative bias (by Jensen's inequality), which depends on the distribution, and thus the corrected sample standard deviation (using Bessel's correction) is biased. The unbiased estimation of standard deviation is a technically involved problem, though for the normal distribution using the term "n" − 1.5 yields an almost unbiased estimator.
The unbiased sample variance is a U-statistic for the function "ƒ"("y"1, "y"2) = ("y"1 − "y"2)2/2, meaning that it is obtained by averaging a 2-sample statistic over 2-element subsets of the population.
Distribution of the sample variance.
Being a function of random variables, the sample variance is itself a random variable, and it is natural to study its distribution. In the case that "y""i" are independent observations from a normal distribution, Cochran's theorem shows that "s"2 follows a scaled chi-squared distribution:
As a direct consequence, it follows that 
and
If the "y""i" are independent and identically distributed, but not necessarily normally distributed, then
where "κ" is the kurtosis of the distribution and "μ4" is the fourth central moment.
If the conditions of the law of large numbers hold for the squared observations, "s"2 is a consistent estimator of "σ"2. One can see indeed that the variance of the estimator tends asymptotically to zero.
Samuelson's inequality.
Samuelson's inequality is a result that states bounds on the values that individual observations in a sample can take, given that the sample mean and (biased) variance have been calculated. Values must lie within the limits formula_119
Relations with the harmonic and arithmetic means.
It has been shown that for a sample {"y""i"} of real numbers,
where "y"max is the maximum of the sample, "A" is the arithmetic mean, "H" is the harmonic mean of the sample and formula_121 is the (biased) variance of the sample.
This bound has been improved, and it is known that variance is bounded by
where "y"min is the minimum of the sample.
Semivariance.
The "semivariance" is calculated in the same manner as the variance but only those observations that fall below the mean are included in the calculation. It is sometimes described as a measure of downside risk in an investments context. For skewed distributions, the semivariance can provide additional information that a variance does not.
Generalizations.
If formula_124 is a scalar complex-valued random variable, with values in formula_125, then its variance is formula_126, where formula_127 is the complex conjugate of formula_124. This variance is also a complex scalar.
If formula_58 is a vector-valued random variable, with values in formula_130, and thought of as a column vector, then the natural generalization of variance is formula_131, where formula_132 and formula_133 is the transpose of formula_58, and so is a row vector. The result is a positive semi-definite square matrix, commonly referred to as the variance-covariance matrix.
If formula_58 is a vector- and complex-valued random variable, with values in formula_136, then the generalization of its variance is formula_137, where formula_138 is the conjugate transpose of formula_58. This matrix is also positive semi-definite and square.
Tests of equality of variances.
Testing for the equality of two or more variances is difficult. The F test and chi square tests are both adversely affected by non-normality and are not recommended for this purpose.
Several non parametric tests have been proposed: these include the Barton–David–Ansari–Fruend–Siegel–Tukey test, the Capon test, Mood test, the Klotz test and the Sukhatme test. The Sukhatme test applies to two variances and requires that both medians be known and equal to zero. The Mood, Klotz, Capon and Barton–David–Ansari–Fruend–Siegel–Tukey tests also apply to two variances. They allow the median to be unknown but do require that the two medians are equal.
The Lehman test is a parametric test of two variances. Of this test there are several variants known. Other tests of the equality of variances include the Box test, the Box–Anderson test and the Moses test.
Resampling methods, which include the bootstrap and the jackknife, may be used to test the equality of variances.
History.
The term "variance" was first introduced by Ronald Fisher in his 1918 paper "The Correlation Between Relatives on the Supposition of Mendelian Inheritance":
The great body of available statistics show us that the deviations of a human measurement from its mean follow very closely the Normal Law of Errors, and, therefore, that the variability may be uniformly measured by the standard deviation corresponding to the square root of the mean square error. When there are two independent causes of variability capable of producing in an otherwise uniform population distributions with standard deviations formula_140 and formula_141, it is found that the distribution, when both causes act together, has a standard deviation formula_142. It is therefore desirable in analysing the causes of variability to deal with the square of the standard deviation as the measure of variability. We shall term this quantity the Variance...
Moment of inertia.
The variance of a probability distribution is analogous to the moment of inertia in classical mechanics of a corresponding mass distribution along a line, with respect to rotation about its center of mass. It is because of this analogy that such things as the variance are called "moments" of probability distributions. The covariance matrix is related to the moment of inertia tensor for multivariate distributions. The moment of inertia of a cloud of "n" points with a covariance matrix of formula_97 is given by
This difference between moment of inertia in physics and in statistics is clear for points that are gathered along a line. Suppose many points are close to the "x" axis and distributed along it. The covariance matrix might look like
That is, there is the most variance in the "x" direction. However, physicists would consider this to have a low moment "about" the "x" axis so the moment-of-inertia tensor is

</doc>
<doc id="491097" url="https://en.wikipedia.org/wiki?curid=491097" title="Variational principle">
Variational principle

A variational principle is a scientific principle used within the calculus of variations, which develops general methods for finding functions which minimize or maximize the value of quantities that depend upon those functions. For example, to answer this question: "What is the shape of a chain suspended at both ends?" we can use the variational principle that the shape must minimize the gravitational potential energy.
According to Cornelius Lanczos, any physical law which can be expressed as a variational principle describes an expression which is self-adjoint. These expressions are also called Hermitian. Such an expression describes an invariant under a Hermitian transformation.
Felix Klein's Erlangen program attempted to identify such invariants under a group of transformations. In what is referred to in physics as Noether's theorem, the Poincaré group of transformations (what is now called a gauge group) for general relativity defines symmetries under a group of transformations which depend on a variational principle, or action principle.
Statements of variational principles are rewarded by the "Fermat Prize".

</doc>
<doc id="23218956" url="https://en.wikipedia.org/wiki?curid=23218956" title="Vector (mathematics and physics)">
Vector (mathematics and physics)

When used without any further description, vector refers either to:
"Vector" can also have a variety of different meanings depending on context.

</doc>
<doc id="32640" url="https://en.wikipedia.org/wiki?curid=32640" title="Vector calculus">
Vector calculus

Vector calculus (or vector analysis) is a branch of mathematics concerned with differentiation and integration of vector fields, primarily in 3-dimensional Euclidean space formula_1 The term "vector calculus" is sometimes used as a synonym for the broader subject of multivariable calculus, which includes vector calculus as well as partial differentiation and multiple integration. Vector calculus plays an important role in differential geometry and in the study of partial differential equations. It is used extensively in physics and engineering, especially in the description of
electromagnetic fields, gravitational fields and fluid flow.
Vector calculus was developed from quaternion analysis by J. Willard Gibbs and Oliver Heaviside near the end of the 19th century, and most of the notation and terminology was established by Gibbs and Edwin Bidwell Wilson in their 1901 book, "Vector Analysis". In the conventional form using cross products, vector calculus does not generalize to higher dimensions, while the alternative approach of geometric algebra, which uses exterior products does generalize, as discussed below.
Basic objects.
Scalar fields.
A scalar field associates a scalar value to every point in a space. The scalar may either be a mathematical number or a physical quantity. Examples of scalar fields in applications include the temperature distribution throughout space, the pressure distribution in a fluid, and spin-zero quantum fields, such as the Higgs field. These fields are the subject of scalar field theory.
Vector fields.
A vector field is an assignment of a vector to each point in a subset of space. A vector field in the plane, for instance, can be visualized as a collection of arrows with a given magnitude and direction each attached to a point in the plane. Vector fields are often used to model, for example, the speed and direction of a moving fluid throughout space, or the strength and direction of some force, such as the magnetic or gravitational force, as it changes from point to point.
Vectors and pseudovectors.
In more advanced treatments, one further distinguishes pseudovector fields and pseudoscalar fields, which are identical to vector fields and scalar fields except that they change sign under an orientation-reversing map: for example, the curl of a vector field is a pseudovector field, and if one reflects a vector field, the curl points in the opposite direction. This distinction is clarified and elaborated in geometric algebra, as described below.
Vector operations.
Algebraic operations.
The basic algebraic (non-differential) operations in vector calculus are referred to as vector algebra, being defined for a vector space and then globally applied to a vector field, and consist of:
There are also two triple products: 
although these are less often used as basic operations, as they can be expressed in terms of the dot and cross products.
Differential operations.
Vector calculus studies various differential operators defined on scalar or vector fields, which are typically expressed in terms of the del operator (formula_9), also known as "nabla". The five most important differential operations in vector calculus are:
where the curl and divergence differ because the former uses a cross product and the latter a dot product, formula_10 denotes a scalar field and formula_11 denotes a vector field. A quantity called the Jacobian is useful for studying functions when both the domain and range of the function are multivariable, such as a change of variables during integration.
Theorems.
Likewise, there are several important theorems related to these operators which generalize the fundamental theorem of calculus to higher dimensions:
Applications.
Linear approximations.
Linear approximations are used to replace complicated functions with linear functions that are almost the same. Given a differentiable function formula_12 with real values, one can approximate formula_12 for formula_14 close to formula_15 by the formula
The right-hand side is the equation of the plane tangent to the graph of formula_17 at formula_18
Optimization.
For a continuously differentiable function of several real variables, a point "P" (that is a set of values for the input variables, which is viewed as a point in R"n") is critical if all of the partial derivatives of the function are zero at "P", or, equivalently, if its gradient is zero. The critical values are the values of the function at the critical points.
If the function is smooth, or, at least twice continuously differentiable, a critical point may be either a local maximum, a local minimum or a saddle point. The different cases may be distinguished by considering the eigenvalues of the Hessian matrix of second derivatives.
By Fermat's theorem, all local maxima and minima of a differentiable function occur at critical points. Therefore, to find the local maxima and minima, it suffices, theoretically, to compute the zeros of the gradient and the eigenvalues of the Hessian matrix at these zeros.
Physics and engineering.
Vector calculus is particularly useful in studying:
Generalizations.
Different 3-manifolds.
Vector calculus is initially defined for Euclidean 3-space, formula_19 which has additional structure beyond simply being a 3-dimensional real vector space, namely: a norm (giving a notion of length) defined via an inner product (the dot product), which in turn gives a notion of angle, and an orientation, which gives a notion of left-handed and right-handed. These structures give rise to a volume form, and also the cross product, which is used pervasively in vector calculus.
The gradient and divergence require only the inner product, while the curl and the cross product also requires the handedness of the coordinate system to be taken into account (see cross product and handedness for more detail).
Vector calculus can be defined on other 3-dimensional real vector spaces if they have an inner product (or more generally a symmetric nondegenerate form) and an orientation; note that this is less data than an isomorphism to Euclidean space, as it does not require a set of coordinates (a frame of reference), which reflects the fact that vector calculus is invariant under rotations (the special orthogonal group SO(3)).
More generally, vector calculus can be defined on any 3-dimensional oriented Riemannian manifold, or more generally pseudo-Riemannian manifold. This structure simply means that the tangent space at each point has an inner product (more generally, a symmetric nondegenerate form) and an orientation, or more globally that there is a symmetric nondegenerate metric tensor and an orientation, and works because vector calculus is defined in terms of tangent vectors at each point.
Other dimensions.
Most of the analytic results are easily understood, in a more general form, using the machinery of differential geometry, of which vector calculus forms a subset. Grad and div generalize immediately to other dimensions, as do the gradient theorem, divergence theorem, and Laplacian (yielding harmonic analysis), while curl and cross product do not generalize as directly.
From a general point of view, the various fields in (3-dimensional) vector calculus are uniformly seen as being "k"-vector fields: scalar fields are 0-vector fields, vector fields are 1-vector fields, pseudovector fields are 2-vector fields, and pseudoscalar fields are 3-vector fields. In higher dimensions there are additional types of fields (scalar/vector/pseudovector/pseudoscalar corresponding to 0/1/"n"−1/"n" dimensions, which is exhaustive in dimension 3), so one cannot only work with (pseudo)scalars and (pseudo)vectors.
In any dimension, assuming a nondegenerate form, grad of a scalar function is a vector field, and div of a vector field is a scalar function, but only in dimension 3 and 7 (and, trivially, dimension 0) is the curl of a vector field a vector field, and only in 3 or 7 dimensions can a cross product be defined (generalizations in other dimensionalities either require formula_20 vectors to yield 1 vector, or are alternative Lie algebras, which are more general antisymmetric bilinear products). The generalization of grad and div, and how curl may be generalized is elaborated at Curl: Generalizations; in brief, the curl of a vector field is a bivector field, which may be interpreted as the special orthogonal Lie algebra of infinitesimal rotations; however, this cannot be identified with a vector field because the dimensions differ - there are 3 dimensions of rotations in 3 dimensions, but 6 dimensions of rotations in 4 dimensions (and more generally formula_21 dimensions of rotations in "n" dimensions).
There are two important alternative generalizations of vector calculus. The first, geometric algebra, uses "k"-vector fields instead of vector fields (in 3 or fewer dimensions, every "k"-vector field can be identified with a scalar function or vector field, but this is not true in higher dimensions). This replaces the cross product, which is specific to 3 dimensions, taking in two vector fields and giving as output a vector field, with the exterior product, which exists in all dimensions and takes in two vector fields, giving as output a bivector (2-vector) field. This product yields Clifford algebras as the algebraic structure on vector spaces (with an orientation and nondegenerate form). Geometric algebra is mostly used in generalizations of physics and other applied fields to higher dimensions.
The second generalization uses differential forms ("k"-covector fields) instead of vector fields or "k"-vector fields, and is widely used in mathematics, particularly in differential geometry, geometric topology, and harmonic analysis, in particular yielding Hodge theory on oriented pseudo-Riemannian manifolds. From this point of view, grad, curl, and div correspond to the exterior derivative of 0-forms, 1-forms, and 2-forms, respectively, and the key theorems of vector calculus are all special cases of the general form of Stokes' theorem.
From the point of view of both of these generalizations, vector calculus implicitly identifies mathematically distinct objects, which makes the presentation simpler but the underlying mathematical structure and generalizations less clear.
From the point of view of geometric algebra, vector calculus implicitly identifies "k"-vector fields with vector fields or scalar functions: 0-vectors and 3-vectors with scalars, 1-vectors and 2-vectors with vectors. From the point of view of differential forms, vector calculus implicitly identifies "k"-forms with scalar fields or vector fields: 0-forms and 3-forms with scalar fields, 1-forms and 2-forms with vector fields. Thus for example the curl naturally takes as input a vector field, but naturally has as output a 2-vector field or 2-form (hence pseudovector field), which is then interpreted as a vector field, rather than directly taking a vector field to a vector field; this is reflected in the curl of a vector field in higher dimensions not having as output a vector field.

</doc>
<doc id="62641" url="https://en.wikipedia.org/wiki?curid=62641" title="Vector field">
Vector field

In vector calculus, a vector field is an assignment of a vector to each point in a subset of space. A vector field in the plane, for instance, can be visualized as a collection of arrows with a given magnitude and direction each attached to a point in the plane. Vector fields are often used to model, for example, the speed and direction of a moving fluid throughout space, or the strength and direction of some force, such as the magnetic or gravitational force, as it changes from point to point.
The elements of differential and integral calculus extend to vector fields in a natural way. When a vector field represents force, the line integral of a vector field represents the work done by a force moving along a path, and under this interpretation conservation of energy is exhibited as a special case of the fundamental theorem of calculus. Vector fields can usefully be thought of as representing the velocity of a moving flow in space, and this physical intuition leads to notions such as the divergence (which represents the rate of change of volume of a flow) and curl (which represents the rotation of a flow).
In coordinates, a vector field on a domain in "n"-dimensional Euclidean space can be represented as a vector-valued function that associates an "n"-tuple of real numbers to each point of the domain. This representation of a vector field depends on the coordinate system, and there is a well-defined transformation law in passing from one coordinate system to the other. Vector fields are often discussed on open subsets of Euclidean space, but also make sense on other subsets such as surfaces, where they associate an arrow tangent to the surface at each point (a tangent vector). 
More generally, vector fields are defined on differentiable manifolds, which are spaces that look like Euclidean space on small scales, but may have more complicated structure on larger scales. In this setting, a vector field gives a tangent vector at each point of the manifold (that is, a section of the tangent bundle to the manifold). Vector fields are one kind of tensor field.
Definition.
Vector fields on subsets of Euclidean space.
Given a subset "S" in R"n", a vector field is represented by a vector-valued function "V": "S" → R"n" in standard Cartesian coordinates ("x"1, ..., "x""n"). If each component of "V" is continuous, then "V" is a continuous vector field, and more generally "V" is a "Ck" vector field if each component of "V" is "k" times continuously differentiable.
A vector field can be visualized as assigning a vector to individual points within an "n"-dimensional space.
Given two "Ck"-vector fields "V", "W" defined on "S" and a real valued "Ck"-function "f" defined on "S", the two operations scalar multiplication and vector addition
define the module of "Ck"-vector fields over the ring of C"k"-functions.
Coordinate transformation law.
In physics, a vector is additionally distinguished by how its coordinates change when one measures the same vector with respect to a different background coordinate system. The transformation properties of vectors distinguish a vector as a geometrically distinct entity from a simple list of scalars, or from a covector.
Thus, suppose that ("x"1...,"x""n") is a choice of Cartesian coordinates, in terms of which the components of the vector "V" are
and suppose that ("y"1...,"y""n") are "n" functions of the "x""i" defining a different coordinate system. Then the components of the vector "V" in the new coordinates are required to satisfy the transformation law
Such a transformation law is called contravariant. A similar transformation law characterizes vector fields in physics: specifically, a vector field is a specification of "n" functions in each coordinate system subject to the transformation law () relating the different coordinate systems.
Vector fields are thus contrasted with scalar fields, which associate a number or "scalar" to every point in space, and are also contrasted with simple lists of scalar fields, which do not transform under coordinate changes.
Vector fields on manifolds.
Given a differentiable manifold "M", a vector field on "M" is an assignment of a tangent vector to each point in "M". More precisely, a vector field "F" is a mapping from "M" into the tangent bundle "TM" so that formula_4 is the identity mapping
where "p" denotes the projection from "TM" to "M". In other words, a vector field is a section of the tangent bundle. 
If the manifold "M" is smooth or analytic—that is, the change of coordinates is smooth (analytic)—then one can make sense of the notion of smooth (analytic) vector fields. The collection of all smooth vector fields on a smooth manifold "M" is often denoted by Γ(T"M") or "C"∞("M",T"M") (especially when thinking of vector fields as sections); the collection of all smooth vector fields is also denoted by formula_5 (a fraktur "X").
Examples.
Gradient field.
Vector fields can be constructed out of scalar fields using the gradient operator (denoted by the del: ∇).
A vector field "V" defined on a set "S" is called a gradient field or a conservative field if there exists a real-valued function (a scalar field) "f" on "S" such that
The associated flow is called the gradient flow, and is used in the method of gradient descent.
The path integral along any closed curve "γ" ("γ"(0) = "γ"(1)) in a conservative field is zero:
where the angular brackets and comma: , denotes the inner product of two vectors (strictly speaking – the integrand "V"("x") is a 1-form rather than a vector in the elementary sense).
Central field.
A "C"∞-vector field over R"n" \ {0} is called a central field if
where O("n", R) is the orthogonal group. We say central fields are invariant under orthogonal transformations around 0.
The point 0 is called the center of the field.
Since orthogonal transformations are actually rotations and reflections, the invariance conditions mean that vectors of a central field are always directed towards, or away from, 0; this is an alternate (and simpler) definition. A central field is always a gradient field, since defining it on one semiaxis and integrating gives an antigradient.
Operations on vector fields.
Line integral.
A common technique in physics is to integrate a vector field along a curve, i.e. to determine its line integral. Given a particle in a gravitational vector field, where each vector represents the force acting on the particle at a given point in space, the line integral is the work done on the particle when it travels along a certain path.
The line integral is constructed analogously to the Riemann integral and it exists if the curve is rectifiable (has finite length) and the vector field is continuous.
Given a vector field "V" and a curve γ parametrized by ["a", "b"] (where "a" and "b" are real) the line integral is defined as
Divergence.
The divergence of a vector field on Euclidean space is a function (or scalar field). In three-dimensions, the divergence is defined by
with the obvious generalization to arbitrary dimensions. The divergence at a point represents the degree to which a small volume around the point is a source or a sink for the vector flow, a result which is made precise by the divergence theorem.
The divergence can also be defined on a Riemannian manifold, that is, a manifold with a Riemannian metric that measures the length of vectors.
Curl.
The curl is an operation which takes a vector field and produces another vector field. The curl is defined only in three-dimensions, but some properties of the curl can be captured in higher dimensions with the exterior derivative. In three-dimensions, it is defined by
The curl measures the density of the angular momentum of the vector flow at a point, that is, the amount to which the flow circulates around a fixed axis. This intuitive description is made precise by Stokes' theorem.
Index of a vector field.
The index of a vector field is a way of describing the behaviour of a vector field around an isolated zero (i.e. non-singular point) which can distinguish saddles from sources and sinks. Take a small sphere around the zero so that no other zeros are included. A map from this sphere to a unit sphere of dimensions "n" − 1 can be constructed by dividing each vector by its length to form a unit length vector which can then be mapped to the unit sphere. The index of the vector field at the point is the degree of this map. The index of the vector field is the sum of the indices of each zero.
The index will be zero around any non singular point, it is +1 around sources and sinks and −1 around saddles. In two dimensions the index is equivalent to the winding number. For an ordinary sphere in three dimension space it can be shown that the index of any vector field on the sphere must be two, this leads to the hairy ball theorem which shows that every such vector field must have a zero. This theorem generalises to the Poincaré–Hopf theorem which relates the index to the Euler characteristic of the space.
History.
Vector fields arose originally in classical field theory in 19th century physics, specifically in magnetism. They were formalized by Michael Faraday, in his concept of "lines of force," who emphasized that the field "itself" should be an object of study, which it has become throughout physics in the form of field theory.
In addition to the magnetic field, other phenomena that were modeled as vector fields by Faraday include the electrical field and light field.
Flow curves.
Consider the flow of a fluid through a region of space. At any given time, any point of the fluid has a particular velocity associated with it; thus there is a vector field associated to any flow. The converse is also true: it is possible to associate a flow to a vector field having that vector field as its velocity.
Given a vector field "V" defined on "S", one defines curves γ("t") on "S" such that for each "t" in an interval "I"
By the Picard–Lindelöf theorem, if "V" is Lipschitz continuous there is a "unique" "C"1-curve γ"x" for each point "x" in "S" so that
The curves γ"x" are called flow curves (or less commonly, flow lines) of the vector field "V" and partition "S" into equivalence classes. It is not always possible to extend the interval (−ε, +ε) to the whole real number line. The flow may for example reach the edge of "S" in a finite time.
In two or three dimensions one can visualize the vector field as giving rise to a flow on "S". If we drop a particle into this flow at a point "p" it will move along the curve γ"p" in the flow depending on the initial point "p". If "p" is a stationary point of "V" then the particle will remain at "p".
Typical applications are streamline in fluid, geodesic flow, and one-parameter subgroups and the exponential map in Lie groups.
Complete vector fields.
A vector field is complete if its flow curves exist for all time. In particular, compactly supported vector fields on a manifold are complete. If "X" is a complete vector field on "M", then the one-parameter group of diffeomorphisms generated by the flow along "X" exists for all time.
Difference between scalar and vector field.
The difference between a scalar and vector field is not that a scalar is just one number while a vector is several numbers. The difference is in how their coordinates respond to coordinate transformations. A scalar "is" a coordinate whereas a vector "can be described" by coordinates, but it "is not" the collection of its coordinates.
Example 1.
This example is about 2-dimensional Euclidean space (R2) where we examine Euclidean ("x", "y") and polar ("r", θ) coordinates (which are undefined at the origin). Thus "x" = "r" cos θ and "y" = "r" sin θ and also "r"2 = "x"2 + "y"2, cos θ = "x"/("x"2 + "y"2)1/2 and sin θ = "y"/("x"2 + "y"2)1/2. Suppose we have a scalar field which is given by the constant function 1, and a vector field which attaches a vector in the "r"-direction with length 1 to each point. More precisely, they are given by the functions
Let us convert these fields to Euclidean coordinates. The vector of length 1 in the "r"-direction has the "x" coordinate cos θ and the "y" coordinate sin θ. Thus in Euclidean coordinates the same fields are described by the functions
We see that while the scalar field remains the same, the vector field now looks different. The same holds even in the 1-dimensional case, as illustrated by the next example.
Example 2.
Consider the 1-dimensional Euclidean space R with its standard Euclidean coordinate "x". Suppose we have a scalar field and a vector field which are both given in the "x" coordinate by the constant function 1,
Thus, we have a scalar field which has the value 1 everywhere and a vector field which attaches a vector in the "x"-direction with magnitude 1 unit of "x" to each point.
Now consider the coordinate ξ := 2"x". If "x" changes one unit then ξ changes 2 units. But since we wish the integral of v along a path to be independent of coordinate, this means v*dx=v'*dξ. So from x increase by 1 unit, ξ increases by 1/2 unit, so v' must be 2. Thus this vector field has a magnitude of 2 in units of ξ. Therefore, in the ξ coordinate the scalar field and the vector field are described by the functions
which are different.
f-relatedness.
Given a smooth function between manifolds, "f": "M" → "N", the derivative is an induced map on tangent bundles, "f"*: "TM" → "TN". Given vector fields "V": "M" → "TM" and "W": "N" → "TN", we say that "W" is "f"-related to "V" if the equation "W" ∘ "f"* = "f"* ∘ "V" holds.
If "V"i is "f"-related to "W"i, "i" = 1, 2, then the Lie bracket ["V"1, "V"2] is "f"-related to ["W"1, "W"2].
Generalizations.
Replacing vectors by "p"-vectors ("p"th exterior power of vectors) yields "p"-vector fields; taking the dual space and exterior powers yields differential "k"-forms, and combining these yields general tensor fields.
Algebraically, vector fields can be characterized as derivations of the algebra of smooth functions on the manifold, which leads to defining a vector field on a commutative algebra as a derivation on the algebra, which is developed in the theory of differential calculus over commutative algebras.

</doc>
<doc id="7213829" url="https://en.wikipedia.org/wiki?curid=7213829" title="Vector Laplacian">
Vector Laplacian

In mathematics and physics, the vector Laplace operator, denoted by formula_1, named after Pierre-Simon Laplace, is a differential operator defined over a vector field. The vector Laplacian is similar to the scalar Laplacian. Whereas the scalar Laplacian applies to scalar field and returns a scalar quantity, the vector Laplacian applies to the vector fields and returns a vector quantity. When computed in rectangular cartesian coordinates, the returned vector field is equal to the vector field of the scalar Laplacian applied on the individual elements. 
Definition.
The vector Laplacian of a vector field formula_2 is defined as
In Cartesian coordinates, this reduces to the much simpler form:
where formula_5, formula_6, and formula_7 are the components of formula_8. This can be seen to be a special case of Lagrange's formula; see Vector triple product.
For expressions of the vector Laplacian in other coordinate systems see Nabla in cylindrical and spherical coordinates.
Generalization.
The Laplacian of any tensor field formula_9 ("tensor" includes scalar and vector) is defined as the divergence of the gradient of the tensor:
For the special case where formula_9 is a scalar (a tensor of rank zero), the Laplacian takes on the familiar form.
If formula_9 is a vector (a tensor of first rank), the gradient is a covariant derivative which results in a tensor of second rank, and the divergence of this is again a vector. The formula for the vector Laplacian above may be used to avoid tensor math and may be shown to be equivalent to the divergence of the Jacobian matrix shown below for the gradient of a vector:
And, in the same manner, a dot product, which evaluates to a vector, of a vector by the gradient of another vector (a tensor of 2nd rank) can be seen as a product of matrices:
This identity is a coordinate dependent result, and is not general.
Use in physics.
An example of the usage of the vector Laplacian is the Navier-Stokes equations for a Newtonian incompressible flow:
where the term with the vector Laplacian of the velocity field formula_16 represents the viscous stresses in the fluid.
Another example is the wave equation for the electric field that can be derived from
the Maxwell's equations in the absence of charges and currents:
The previous equation can also be written as:
where 
is the D'Alembertian, used in the Klein–Gordon equation.

</doc>
<doc id="34154322" url="https://en.wikipedia.org/wiki?curid=34154322" title="Vector model of the atom">
Vector model of the atom

In physics, in particular quantum mechanics, the vector model of the atom is a model of the atom in terms of angular momentum. It can be considered as the extension of the Rutherford-Bohr-Sommerfeld atom model to multi-electron atoms.
Introduction.
The model is a convenient representation of the angular momenta of the electrons in the atom. Angular momentum is always split into orbital L, spin S and total J:
Given that in quantum mechanics, angular momentum is quantized and there is an uncertainty relation for the components of each vector, the representation turns out to be quite simple (although the background mathematics is quite complex). Geometrically it is a discrete set of right-circular cones, without the circular base, in which the axes of all the cones are lined up onto a common axis, conventionally the z-axis for three-dimensional Cartesian coordinates. Following is the background to this construction.
Mathematical background of angular momenta.
The commutator implies that for each of L, S, and J, only one component of any angular momentum vector can be measured at any instant of time; at the same the other two are indeterminate. The commutator of any two angular momentum operators (corresponding to component directions) is non-zero. Following is a summary of the relevant mathematics in constructing the vector model.
The commutation relations are (using the Einstein summation convention):
where 
The magnitudes of L, S and J however "can" be measured at the same time, since the commutation of the square of an angular momentum operator (full resultant, not components) with any one component is zero, so simultaneous measurement of formula_3 with formula_4, formula_5 with formula_6 and formula_7 with formula_8 satisfy:
The magnitudes satisfy all of the following, in terms of operators and vector components:
and quantum numbers:
where 
which respectively take the values:
These mathematical facts suggest the continuum of all possible angular momenta for a corresponding specified quantum number:
The geometrical result is a cone of vectors, the vector starts at the apex of the cone and its tip reaches the circumference of the cone. It is convention to use the z-component for the measurable component of angular momentum, so the axis of the cone must be the z-axis, directed from the apex to the plane defined by the circular base of the cone, perpendicular to the plane. For different quantum numbers, the cones are different. So there are a "discrete" number of states the angular momenta can be, ruled by the above possible values for formula_14, "s", and "j". Using the previous set-up of the vector as part of a cone, each state must correspond to a cone. This is for increasing formula_14, "s", and "j", and decreasing formula_14, "s", and "j"> Negative quantum numbers correspond to cones reflected in the "x"-"y" plane. One of these states, for a quantum number equal to zero, clearly doesn't correspond to a cone, only a circle in the "x"-"y" plane.
The number of cones (including the degenerate planar circle) equals the multiplicity of states, formula_17.
Bohr model.
It can be considered the extension of the Bohr model because Niels Bohr also proposed angular momentum was quantized according to:
where "m" is an integer, produced correct results for the Hydrogen atom. Although the Bohr model doesn't apply to multi-electron atoms, it was the first successful quantization of angular momentum applied to the atom, preceding the vector model of the atom.
Addition of angular momenta.
For one-electron atoms (i.e. hydrogen), there is only one set of cones for the orbiting electron. For multi-electron atoms, there are many states, due to the increasing number of electrons.
The angular momenta of all electrons in the atom add vectorially. Most atomic processes, both nuclear and chemical (electronic) – except in the absolutely stochastic process of radioactive decay - are determined by spin-pairing and coupling of angular momenta due to neighbouring nucleons and electrons. The term "coupling" in this context means the vector superposition of angular momenta, that is, magnitudes and directions are added. 
In multi-electron atoms, the vector sum of two angular momenta is:
for the z-component, the projected values are:
where
and the magnitudes are:
in which
This process may be repeated for a third electron, then the fourth etc. until the total angular momentum has been found.
LS coupling.
The process of adding all angular momenta together is a laborious task, since the resultant momenta is not definite, the entire cones of precessing momenta about the z-axis must be incorporated into the calculation. This can be simplified by some developed approximations - such as the "Russell-Saunders coupling" scheme in L-S coupling, named after H. N. Russell and F. A. Saunders (1925).

</doc>
<doc id="418352" url="https://en.wikipedia.org/wiki?curid=418352" title="Vector operator">
Vector operator

A vector operator is a differential operator used in vector calculus. Vector operators are defined in terms of del, and include the gradient, divergence, and curl:
The Laplacian is
Vector operators must always come right before the scalar field or vector field on which they operate, in order to produce a result. E.g.
yields the gradient of "f", but
is just another vector operator, which is not operating on anything.
A vector operator can operate on another vector operator, to produce a compound vector operator, as seen above in the case of the Laplacian.

</doc>
<doc id="422936" url="https://en.wikipedia.org/wiki?curid=422936" title="Vector potential">
Vector potential

In vector calculus, a vector potential is a vector field whose curl is a given vector field. This is analogous to a "scalar potential", which is a scalar field whose gradient is a given vector field.
Formally, given a vector field v, a "vector potential" is a vector A such that 
If a vector field v admits a vector potential A, then from the equality 
(divergence of the curl is zero) one obtains
which implies that v must be a solenoidal vector field. 
Theorem.
Let 
be a solenoidal vector field which is twice continuously differentiable. Assume that v(x) decreases sufficiently fast as ||x||→∞. Define 
Then, A is a vector potential for v, that is,
A generalization of this theorem is the Helmholtz decomposition which states that any vector field can be decomposed as a sum of a solenoidal vector field and an irrotational vector field.
Nonuniqueness.
The vector potential admitted by a solenoidal field is not unique. If A is a vector potential for v, then so is
where "m" is any continuously differentiable scalar function. This follows from the fact that the curl of the gradient is zero.
This nonuniqueness leads to a degree of freedom in the formulation of electrodynamics, or gauge freedom, and requires choosing a gauge.

</doc>
<doc id="1508507" url="https://en.wikipedia.org/wiki?curid=1508507" title="Vector projection">
Vector projection

The vector projection of a vector a on (or onto) a nonzero vector b (also known as the vector component or vector resolute of a in the direction of b) is the orthogonal projection of a onto a straight line parallel to b. It is a vector parallel to b, defined as
where formula_2 is a scalar, called the scalar projection of a onto b, and b̂ is the unit vector in the direction of b.
In turn, the scalar projection is defined as
where the operator · denotes a dot product, |a| is the length of a, and "θ" is the angle between a and b. The scalar projection is equal to the length of the vector projection, with a minus sign if the direction of the projection is opposite to the direction of b.
The vector component or vector resolute of a perpendicular to b, sometimes also called the vector rejection of a "from" b,
is the orthogonal projection of a onto the plane (or, in general, hyperplane) orthogonal to b. Both the projection a1 and rejection a2 of a vector a are vectors, and their sum is equal to a, which implies that the rejection is given by
Notation.
Typically, a vector projection is denoted in a bold font (e.g. a1), and the corresponding scalar projection with normal font (e.g. "a"1). In some cases, especially in handwriting, the vector projection is also denoted using a diacritic above or below the letter (e.g., formula_5 or "a"1; see Euclidean vector representations for more details).
The vector projection of a on b and the corresponding rejection are sometimes denoted by a∥b and a⊥b, respectively.
Definitions based on angle "θ".
Scalar projection.
The scalar projection of a on b is a scalar equal to
where "θ" is the angle between a and b.
A scalar projection can be used as a scale factor to compute the corresponding vector projection.
Vector projection.
The vector projection of a on b is a vector whose magnitude is the scalar projection of a on b and whose angle against b is either 0 or 180 degrees.
Namely, it is defined as
where "a"1 is the corresponding scalar projection, as defined above, and b̂ is the unit vector with the same direction as b:
Vector rejection.
By definition, the vector rejection of a on b is
Hence,
Definitions in terms of a and b.
When "θ" is not known, the cosine of "θ" can be computed in terms of a and b, by the following property of the dot product a · b:
Scalar projection.
By the above-mentioned property of the dot product, the definition of the scalar projection becomes
while the latter additionally requires only the division of a scalar by a scalar.
Vector rejection.
By definition, 
Hence,
Properties.
Scalar projection.
The scalar projection a on b is a scalar which has a negative sign if 90 < "θ" ≤ 180 degrees. It coincides with the length |c| of the vector projection if the angle is smaller than 90°. More exactly:
Vector projection.
The vector projection of a on b is a vector a1 which is either null or parallel to b. More exactly:
Vector rejection.
The vector rejection of a on b is a vector a2 which is either null or orthogonal to b. More exactly:
Matrix representation.
The orthogonal projection can be represented by a projection matrix. To project a vector onto the unit vector "a" = ("ax, ay, az"), it would need to be multiplied with this projection matrix:
Uses.
The vector projection is an important operation in the Gram–Schmidt orthonormalization of vector space bases. It is also used in the Separating axis theorem to detect whether two convex shapes intersect.
Generalizations.
Since the notions of vector length and angle between vectors can be generalized to any "n"-dimensional inner product space, this is also true for the notions of orthogonal projection of a vector, projection of a vector onto another, and rejection of a vector from another. In some cases, the inner product coincides with the dot product. Whenever they don't coincide, the inner product is used instead of the dot product in the formal definitions of projection and rejection.
For a three-dimensional inner product space, the notions of projection of a vector onto another and rejection of a vector from another can be generalized to the notions of projection of a vector onto a plane, and rejection of a vector from a plane. 
The projection of a vector on a plane is its orthogonal projection on that plane. The rejection of a vector from a plane is its orthogonal projection on a straight line which is orthogonal to that plane. Both are vectors. The first is parallel to the plane, the second is orthogonal. For a given vector and plane, the sum of projection and rejection is equal to the original vector.
Similarly, for inner product spaces with more than three dimensions, the notions of projection onto a vector and rejection from a vector can be generalized to the notions of projection onto a hyperplane, and rejection from a hyperplane.
In geometric algebra, they can be further generalized to the notions of projection and rejection of a general multivector onto/from any invertible "k"-blade.

</doc>
<doc id="32370" url="https://en.wikipedia.org/wiki?curid=32370" title="Vector space">
Vector space

A vector space (also called a linear space) is a collection of objects called vectors, which may be added together and multiplied ("scaled") by numbers, called "scalars" in this context. Scalars are often taken to be real numbers, but there are also vector spaces with scalar multiplication by complex numbers, rational numbers, or generally any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called "axioms", listed below. 
Euclidean vectors are an example of a vector space. They represent physical quantities such as forces: any two forces (of the same type) can be added to yield a third, and the multiplication of a force vector by a real multiplier is another force vector. In the same vein, but in a more geometric sense, vectors representing displacements in the plane or in three-dimensional space also form vector spaces. Vectors in vector spaces do not necessarily have to be arrow-like objects as they appear in the mentioned examples: vectors are regarded as abstract mathematical objects with particular properties, which in some cases can be visualized as arrows.
Vector spaces are the subject of linear algebra and are well understood from this point of view since vector spaces are characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. A vector space may be endowed with additional structure, such as a norm or inner product. Such spaces arise naturally in mathematical analysis, mainly in the guise of infinite-dimensional function spaces whose vectors are functions. Analytical problems call for the ability to decide whether a sequence of vectors converges to a given vector. This is accomplished by considering vector spaces with additional structure, mostly spaces endowed with a suitable topology, thus allowing the consideration of proximity and continuity issues. These topological vector spaces, in particular Banach spaces and Hilbert spaces, have a richer theory.
Historically, the first ideas leading to vector spaces can be traced back as far as the 17th century's analytic geometry, matrices, systems of linear equations, and Euclidean vectors. The modern, more abstract treatment, first formulated by Giuseppe Peano in 1888, encompasses more general objects than Euclidean space, but much of the theory can be seen as an extension of classical geometric ideas like lines, planes and their higher-dimensional analogs.
Today, vector spaces are applied throughout mathematics, science and engineering. They are the appropriate linear-algebraic notion to deal with systems of linear equations; offer a framework for Fourier expansion, which is employed in image compression routines; or provide an environment that can be used for solution techniques for partial differential equations. Furthermore, vector spaces furnish an abstract, coordinate-free way of dealing with geometrical and physical objects such as tensors. This in turn allows the examination of local properties of manifolds by linearization techniques. Vector spaces may be generalized in several ways, leading to more advanced notions in geometry and abstract algebra.
Introduction and definition.
The concept of vector space will first be explained by describing two particular examples:
First example: arrows in the plane.
The first example of a vector space consists of arrows in a fixed plane, starting at one fixed point. This is used in physics to describe forces or velocities. Given any two such arrows, and , the parallelogram spanned by these two arrows contains one diagonal arrow that starts at the origin, too. This new arrow is called the "sum" of the two arrows and is denoted . In the special case of two arrows on the same line, their sum is the arrow on this line whose length is the sum or the difference of the lengths, depending on whether the arrows have the same direction. Another operation that can be done with arrows is scaling: given any positive real number , the arrow that has the same direction as , but is dilated or shrunk by multiplying its length by , is called "multiplication" of by . It is denoted . When is negative, is defined as the arrow pointing in the opposite direction, instead.
The following shows a few examples: if , the resulting vector has the same direction as , but is stretched to the double length of (right image below). Equivalently is the sum . Moreover, has the opposite direction and the same length as (blue vector pointing down in the right image).
Second example: ordered pairs of numbers.
A second key example of a vector space is provided by pairs of real numbers and . (The order of the components and is significant, so such a pair is also called an ordered pair.) Such a pair is written as . The sum of two such pairs and multiplication of a pair with a number is defined as follows:
and 
The first example above reduces to this one if the arrows are represented by the pair of Cartesian coordinates of their end points.
Definition.
A vector space over a field is a set  together with two operations that satisfy the eight axioms listed below. Elements of are commonly called "vectors". Elements of  are commonly called "scalars". The first operation, called "vector addition" or simply "addition", takes any two vectors  and and assigns to them a third vector which is commonly written as , and called the sum of these two vectors. The second operation, called "scalar multiplication" takes any scalar  and any vector  and gives another vector .
In this article, vectors are distinguished from scalars by boldface. In the two examples above, the field is the field of the real numbers and the set of the vectors consists of the planar arrows with fixed starting point and of pairs of real numbers, respectively.
To qualify as a vector space, the set  and the operations of addition and multiplication must adhere to a number of requirements called axioms. In the list below, let , and be arbitrary vectors in , and and scalars in .
These axioms generalize properties of the vectors introduced in the above examples. Indeed, the result of addition of two ordered pairs (as in the second example above) does not depend on the order of the summands:
Likewise, in the geometric example of vectors as arrows, since the parallelogram defining the sum of the vectors is independent of the order of the vectors. All other axioms can be checked in a similar manner in both examples. Thus, by disregarding the concrete nature of the particular type of vectors, the definition incorporates these two and many more examples in one notion of vector space.
Subtraction of two vectors and division by a (non-zero) scalar can be defined as
When the scalar field is the real numbers , the vector space is called a "real vector space". When the scalar field is the complex numbers, it is called a "complex vector space". These two cases are the ones used most often in engineering. The general definition of a vector space allows scalars to be elements of any fixed field . The notion is then known as an -"vector spaces" or a "vector space over ". A field is, essentially, a set of numbers possessing addition, subtraction, multiplication and division operations. For example, rational numbers also form a field.
In contrast to the intuition stemming from vectors in the plane and higher-dimensional cases, there is, in general vector spaces, no notion of nearness, angles or distances. To deal with such matters, particular types of vector spaces are introduced; see below.
Alternative formulations and elementary consequences.
The requirement that vector addition and scalar multiplication be binary operations includes (by definition of binary operations) a property called closure: that and are in for all in , and , in . Some older sources mention these properties as separate axioms.
In the parlance of abstract algebra, the first four axioms can be subsumed by requiring the set of vectors to be an abelian group under addition. The remaining axioms give this group an -module structure. In other words, there is a ring homomorphism from the field into the endomorphism ring of the group of vectors. Then scalar multiplication is defined as .
There are a number of direct consequences of the vector space axioms. Some of them derive from elementary group theory, applied to the additive group of vectors: for example the zero vector of and the additive inverse of any vector are unique. Other properties follow from the distributive law, for example equals if and only if equals or equals .
History.
Vector spaces stem from affine geometry via the introduction of coordinates in the plane or three-dimensional space. Around 1636, Descartes and Fermat founded analytic geometry by equating solutions to an equation of two variables with points on a plane curve. In 1884, to achieve geometric solutions without using coordinates, Bolzano introduced certain operations on points, lines and planes, which are predecessors of vectors. His work was then used in the conception of barycentric coordinates by Möbius in 1827. The definition of vectors was founded on Bellavitis' notion of the bipoint, an oriented segment of which one end is the origin and the other a target, then further elaborated with the presentation of complex numbers by Argand and Hamilton and the introduction of quaternions and biquaternions by the latter. They are elements in , , and ; their treatment as linear combinations can be traced back to Laguerre in 1867, who also defined systems of linear equations.
In 1857, Cayley introduced matrix notation, which allows for a harmonization and simplification of linear maps. Around the same time, Grassmann studied the barycentric calculus initiated by Möbius. He envisaged sets of abstract objects endowed with operations. In his work, the concepts of linear independence and dimension, as well as scalar products, are present. In fact, Grassmann's 1844 work exceeds the framework of vector spaces, since his consideration of multiplication led him to what are today called algebras. Peano was the first to give the modern definition of vector spaces and linear maps in 1888.
An important development of vector spaces is due to the construction of function spaces by Lebesgue. This was later formalized by Banach and Hilbert, around 1920. At that time, algebra and the new field of functional analysis began to interact, notably with key concepts such as spaces of "p"-integrable functions and Hilbert spaces. Vector spaces, including infinite-dimensional ones, then became a firmly established notion, and many mathematical branches started making use of this concept.
Examples.
Coordinate spaces.
The most simple example of a vector space over a field is the field itself, equipped with its standard addition and multiplication. More generally, a vector space can be composed of
"n"-tuples (sequences of length ) of elements of , such as
A vector space composed of all the -tuples of a field is known as a "coordinate space", usually denoted . The case is the above-mentioned simplest example, in which the field is also regarded as a vector space over itself. The case and was discussed in the introduction above.
Complex numbers and other field extensions.
The set of complex numbers , i.e., numbers that can be written in the form for real numbers and where is the imaginary unit, form a vector space over the reals with the usual addition and multiplication: and for real numbers , , , and . The various axioms of a vector space follow from the fact that the same rules hold for complex number arithmetic.
In fact, the example of complex numbers is essentially the same (i.e., it is "isomorphic") to the vector space of ordered pairs of real numbers mentioned above: if we think of the complex number as representing the ordered pair in the complex plane then we see that the rules for sum and scalar product correspond exactly to those in the earlier example.
More generally, field extensions provide another class of examples of vector spaces, particularly in algebra and algebraic number theory: a field containing a smaller field is an -vector space, by the given multiplication and addition operations of . For example, the complex numbers are a vector space over , and the field extension formula_1 is a vector space over . 
Function spaces.
Functions from any fixed set to a field also form vector spaces, by performing addition and scalar multiplication pointwise. That is, the sum of two functions and is the function given by
and similarly for multiplication. Such function spaces occur in many geometric situations, when is the real line or an interval, or other subsets of . Many notions in topology and analysis, such as continuity, integrability or differentiability are well-behaved with respect to linearity: sums and scalar multiples of functions possessing such a property still have that property. Therefore, the set of such functions are vector spaces. They are studied in greater detail using the methods of functional analysis, see below. Algebraic constraints also yield vector spaces: the vector space is given by polynomial functions:
Linear equations.
Systems of homogeneous linear equations are closely tied to vector spaces. For example, the solutions of 
are given by triples with arbitrary , , and . They form a vector space: sums and scalar multiples of such triples still satisfy the same ratios of the three variables; thus they are solutions, too. Matrices can be used to condense multiple linear equations as above into one vector equation, namely
where formula_2 is the matrix containing the coefficients of the given equations, is the vector , denotes the matrix product, and is the zero vector. In a similar vein, the solutions of homogeneous "linear differential equations" form vector spaces. For example,
yields , where and are arbitrary constants, and is the natural exponential function.
Basis and dimension.
"Bases" allow to represent vectors by a sequence of scalars called "coordinates" or "components". A basis is a (finite or infinite) set of vectors , for convenience often indexed by some index set , that spans the whole space and is linearly independent. "Spanning the whole space" means that any vector can be expressed as a finite sum (called a "linear combination") of the basis elements:
where the are scalars, called the coordinates (or the components) of the vector with respect to the basis , and elements of . Linear independence means that the coordinates are uniquely determined for any vector in the vector space.
For example, the coordinate vectors , , to , form a basis of , called the standard basis, since any vector can be uniquely expressed as a linear combination of these vectors:
The corresponding coordinates , , , are just the Cartesian coordinates of the vector.
Every vector space has a basis. This follows from Zorn's lemma, an equivalent formulation of the Axiom of Choice. Given the other axioms of Zermelo–Fraenkel set theory, the existence of bases is equivalent to the axiom of choice. The ultrafilter lemma, which is weaker than the axiom of choice, implies that all bases of a given vector space have the same number of elements, or cardinality (cf. "Dimension theorem for vector spaces"). It is called the "dimension" of the vector space, denoted dim "V". If the space is spanned by finitely many vectors, the above statements can be proven without such fundamental input from set theory.
The dimension of the coordinate space is , by the basis exhibited above. The dimension of the polynomial ring "F"["x"] introduced above is countably infinite, a basis is given by , , , A fortiori, the dimension of more general function spaces, such as the space of functions on some (bounded or unbounded) interval, is infinite. Under suitable regularity assumptions on the coefficients involved, the dimension of the solution space of a homogeneous ordinary differential equation equals the degree of the equation. For example, the solution space for the above equation is generated by . These two functions are linearly independent over , so the dimension of this space is two, as is the degree of the equation.
A field extension over the rationals can be thought of as a vector space over (by defining vector addition as field addition, defining scalar multiplication as field multiplication by elements of , and otherwise ignoring the field multiplication). The dimension (or degree) of the field extension over depends on . If satisfies some polynomial equation
("α is algebraic"), the dimension is finite. More precisely, it equals the degree of the minimal polynomial having α as a root. For example, the complex numbers C are a two-dimensional real vector space, generated by 1 and the imaginary unit "i". The latter satisfies "i"2 + 1 = 0, an equation of degree two. Thus, C is a two-dimensional R-vector space (and, as any field, one-dimensional as a vector space over itself, C). If α is not algebraic, the dimension of Q(α) over Q is infinite. For instance, for α = π there is no such equation, in other words π is transcendental.
Linear maps and matrices.
The relation of two vector spaces can be expressed by "linear map" or "linear transformation". They are functions that reflect the vector space structure—i.e., they preserve sums and scalar multiplication:
An "isomorphism" is a linear map such that there exists an inverse map , which is a map such that the two possible compositions and are identity maps. Equivalently, "f" is both one-to-one (injective) and onto (surjective). If there exists an isomorphism between "V" and "W", the two spaces are said to be "isomorphic"; they are then essentially identical as vector spaces, since all identities holding in "V" are, via "f", transported to similar ones in "W", and vice versa via "g".
For example, the "arrows in the plane" and "ordered pairs of numbers" vector spaces in the introduction are isomorphic: a planar arrow v departing at the origin of some (fixed) coordinate system can be expressed as an ordered pair by considering the "x"- and "y"-component of the arrow, as shown in the image at the right. Conversely, given a pair ("x", "y"), the arrow going by "x" to the right (or to the left, if "x" is negative), and "y" up (down, if "y" is negative) turns back the arrow v.
Linear maps "V" → "W" between two vector spaces form a vector space Hom"F"("V", "W"), also denoted L("V", "W"). The space of linear maps from "V" to "F" is called the "dual vector space", denoted "V"∗. Via the injective natural map , any vector space can be embedded into its "bidual"; the map is an isomorphism if and only if the space is finite-dimensional.
Once a basis of is chosen, linear maps are completely determined by specifying the images of the basis vectors, because any element of "V" is expressed uniquely as a linear combination of them. If , a 1-to-1 correspondence between fixed bases of and gives rise to a linear map that maps any basis element of to the corresponding basis element of . It is an isomorphism, by its very definition. Therefore, two vector spaces are isomorphic if their dimensions agree and vice versa. Another way to express this is that any vector space is "completely classified" (up to isomorphism) by its dimension, a single number. In particular, any "n"-dimensional -vector space is isomorphic to . There is, however, no "canonical" or preferred isomorphism; actually an isomorphism is equivalent to the choice of a basis of , by mapping the standard basis of to , via . The freedom of choosing a convenient basis is particularly useful in the infinite-dimensional context, see below.
Matrices.
"Matrices" are a useful notion to encode linear maps. They are written as a rectangular array of scalars as in the image at the right. Any "m"-by-"n" matrix "A" gives rise to a linear map from "F""n" to "F""m", by the following
or, using the matrix multiplication of the matrix with the coordinate vector :
Moreover, after choosing bases of and , "any" linear map is uniquely represented by a matrix via this assignment.
The determinant of a square matrix is a scalar that tells whether the associated map is an isomorphism or not: to be so it is sufficient and necessary that the determinant is nonzero. The linear transformation of corresponding to a real "n"-by-"n" matrix is orientation preserving if and only if its determinant is positive.
Eigenvalues and eigenvectors.
Endomorphisms, linear maps , are particularly important since in this case vectors can be compared with their image under , . Any nonzero vector satisfying , where is a scalar, is called an "eigenvector" of with "eigenvalue" . Equivalently, is an element of the kernel of the difference (where Id is the identity map . If is finite-dimensional, this can be rephrased using determinants: having eigenvalue is equivalent to
By spelling out the definition of the determinant, the expression on the left hand side can be seen to be a polynomial function in , called the characteristic polynomial of . If the field is large enough to contain a zero of this polynomial (which automatically happens for algebraically closed, such as ) any linear map has at least one eigenvector. The vector space may or may not possess an eigenbasis, a basis consisting of eigenvectors. This phenomenon is governed by the Jordan canonical form of the map. The set of all eigenvectors corresponding to a particular eigenvalue of forms a vector space known as the "eigenspace" corresponding to the eigenvalue (and ) in question. To achieve the spectral theorem, the corresponding statement in the infinite-dimensional case, the machinery of functional analysis is needed, see below.
Basic constructions.
In addition to the above concrete examples, there are a number of standard linear algebraic constructions that yield vector spaces related to given ones. In addition to the definitions given below, they are also characterized by universal properties, which determine an object by specifying the linear maps from to any other vector space.
Subspaces and quotient spaces.
A nonempty subset "W" of a vector space "V" that is closed under addition and scalar multiplication (and therefore contains the 0-vector of "V") is called a "subspace" of "V". Subspaces of "V" are vector spaces (over the same field) in their own right. The intersection of all subspaces containing a given set "S" of vectors is called its span, and it is the smallest subspace of "V" containing the set "S". Expressed in terms of elements, the span is the subspace consisting of all the linear combinations of elements of "S".
The counterpart to subspaces are "quotient vector spaces". Given any subspace , the quotient space "V"/"W" (""V" modulo "W"") is defined as follows: as a set, it consists of where v is an arbitrary vector in "V". The sum of two such elements and is and scalar multiplication is given by . The key point in this definition is that if and only if the difference of v1 and v2 lies in "W". This way, the quotient space "forgets" information that is contained in the subspace "W".
The kernel ker("f") of a linear map consists of vectors v that are mapped to 0 in "W". Both kernel and image are subspaces of "V" and "W", respectively. The existence of kernels and images is part of the statement that the category of vector spaces (over a fixed field "F") is an abelian category, i.e. a corpus of mathematical objects and structure-preserving maps between them (a category) that behaves much like the category of abelian groups. Because of this, many statements such as the first isomorphism theorem (also called rank–nullity theorem in matrix-related terms)
and the second and third isomorphism theorem can be formulated and proven in a way very similar to the corresponding statements for groups.
An important example is the kernel of a linear map for some fixed matrix "A", as above. The kernel of this map is the subspace of vectors x such that , which is precisely the set of solutions to the system of homogeneous linear equations belonging to "A". This concept also extends to linear differential equations
In the corresponding map
the derivatives of the function "f" appear linearly (as opposed to "f"′′("x")2, for example). Since differentiation is a linear procedure (i.e., and for a constant ) this assignment is linear, called a linear differential operator. In particular, the solutions to the differential equation form a vector space (over or ).
Direct product and direct sum.
The "direct product" of vector spaces and the "direct sum" of vector spaces are two ways of combining an indexed family of vector spaces into a new vector space.
The "direct product" formula_7 of a family of vector spaces "V""i" consists of the set of all tuples (, which specify for each index "i" in some index set "I" an element v"i" of "V""i". Addition and scalar multiplication is performed componentwise. A variant of this construction is the "direct sum" formula_8 (also called coproduct and denoted formula_9), where only tuples with finitely many nonzero vectors are allowed. If the index set "I" is finite, the two constructions agree, but in general they are different.
Tensor product.
The "tensor product" , or simply , of two vector spaces "V" and "W" is one of the central notions of multilinear algebra which deals with extending notions such as linear maps to several variables. A map is called bilinear if "g" is linear in both variables v and w. That is to say, for fixed w the map is linear in the sense above and likewise for fixed v.
The tensor product is a particular vector space that is a "universal" recipient of bilinear maps "g", as follows. It is defined as the vector space consisting of finite (formal) sums of symbols called tensors
subject to the rules 
These rules ensure that the map "f" from the to that maps a tuple to is bilinear. The universality states that given "any" vector space "X" and "any" bilinear map , there exists a unique map "u", shown in the diagram with a dotted arrow, whose composition with "f" equals "g": . This is called the universal property of the tensor product, an instance of the method—much used in advanced abstract algebra—to indirectly define objects by specifying maps from or to this object.
Vector spaces with additional structure.
From the point of view of linear algebra, vector spaces are completely understood insofar as any vector space is characterized, up to isomorphism, by its dimension. However, vector spaces "per se" do not offer a framework to deal with the question—crucial to analysis—whether a sequence of functions converges to another function. Likewise, linear algebra is not adapted to deal with infinite series, since the addition operation allows only finitely many terms to be added. Therefore, the needs of functional analysis require considering additional structures.
A vector space may be given a partial order ≤, under which some vectors can be compared. For example, "n"-dimensional real space R"n" can be ordered by comparing its vectors componentwise. Ordered vector spaces, for example Riesz spaces, are fundamental to Lebesgue integration, which relies on the ability to express a function as a difference of two positive functions
where "f"+ denotes the positive part of "f" and "f"− the negative part.
Normed vector spaces and inner product spaces.
"Measuring" vectors is done by specifying a norm, a datum which measures lengths of vectors, or by an inner product, which measures angles between vectors. Norms and inner products are denoted formula_10 and formula_11, respectively. The datum of an inner product entails that lengths of vectors can be defined too, by defining the associated norm formula_12. Vector spaces endowed with such data are known as "normed vector spaces" and "inner product spaces", respectively.
Coordinate space "F""n" can be equipped with the standard dot product:
In R2, this reflects the common notion of the angle between two vectors x and y, by the law of cosines:
Because of this, two vectors satisfying formula_15 are called orthogonal. An important variant of the standard dot product is used in Minkowski space: R4 endowed with the Lorentz product
In contrast to the standard dot product, it is not positive definite: formula_17 also takes negative values, for example for formula_18. Singling out the fourth coordinate—corresponding to time, as opposed to three space-dimensions—makes it useful for the mathematical treatment of special relativity.
Topological vector spaces.
Convergence questions are treated by considering vector spaces "V" carrying a compatible topology, a structure that allows one to talk about elements being close to each other. Compatible here means that addition and scalar multiplication have to be continuous maps. Roughly, if x and y in "V", and "a" in "F" vary by a bounded amount, then so do and . To make sense of specifying the amount a scalar changes, the field "F" also has to carry a topology in this context; a common choice are the reals or the complex numbers.
In such "topological vector spaces" one can consider series of vectors. The infinite sum
denotes the limit of the corresponding finite partial sums of the sequence ("f""i")"i"∈N of elements of "V". For example, the "f""i" could be (real or complex) functions belonging to some function space "V", in which case the series is a function series. The mode of convergence of the series depends on the topology imposed on the function space. In such cases, pointwise convergence and uniform convergence are two prominent examples.
A way to ensure the existence of limits of certain infinite series is to restrict attention to spaces where any Cauchy sequence has a limit; such a vector space is called complete. Roughly, a vector space is complete provided that it contains all necessary limits. For example, the vector space of polynomials on the unit interval [0,1], equipped with the topology of uniform convergence is not complete because any continuous function on [0,1] can be uniformly approximated by a sequence of polynomials, by the Weierstrass approximation theorem. In contrast, the space of "all" continuous functions on [0,1] with the same topology is complete. A norm gives rise to a topology by defining that a sequence of vectors v"n" converges to v if and only if
Banach and Hilbert spaces are complete topological vector spaces whose topologies are given, respectively, by a norm and an inner product. Their study—a key piece of functional analysis—focusses on infinite-dimensional vector spaces, since all norms on finite-dimensional topological vector spaces give rise to the same notion of convergence. The image at the right shows the equivalence of the 1-norm and ∞-norm on R2: as the unit "balls" enclose each other, a sequence converges to zero in one norm if and only if it so does in the other norm. In the infinite-dimensional case, however, there will generally be inequivalent topologies, which makes the study of topological vector spaces richer than that of vector spaces without additional data.
From a conceptual point of view, all notions related to topological vector spaces should match the topology. For example, instead of considering all linear maps (also called functionals) , maps between topological vector spaces are required to be continuous. In particular, the (topological) dual space consists of continuous functionals (or to ). The fundamental Hahn–Banach theorem is concerned with separating subspaces of appropriate topological vector spaces by continuous functionals.
Banach spaces.
"Banach spaces", introduced by Stefan Banach, are complete normed vector spaces. A first example is the vector space ℓ "p" consisting of infinite vectors with real entries whose "p"-norm given by 
is finite. The topologies on the infinite-dimensional space ℓ "p" are inequivalent for different "p". E.g. the sequence of vectors , i.e. the first 2"n" components are 2−"n", the following ones are 0, converges to the zero vector for , but does not for :
More generally than sequences of real numbers, functions are endowed with a norm that replaces the above sum by the Lebesgue integral
The space of integrable functions on a given domain Ω (for example an interval) satisfying , and equipped with this norm are called Lebesgue spaces, denoted "L""p"(Ω). These spaces are complete. (If one uses the Riemann integral instead, the space is "not" complete, which may be seen as a justification for Lebesgue's integration theory.) Concretely this means that for any sequence of Lebesgue-integrable functions with , satisfying the condition
there exists a function "f"("x") belonging to the vector space "L""p"(Ω) such that
Imposing boundedness conditions not only on the function, but also on its derivatives leads to Sobolev spaces.
Hilbert spaces.
Complete inner product spaces are known as "Hilbert spaces", in honor of David Hilbert.
The Hilbert space "L"2(Ω), with inner product given by
where formula_29 denotes the complex conjugate of "g"("x"), is a key case.
By definition, in a Hilbert space any Cauchy sequence converges to a limit. Conversely, finding a sequence of functions "f""n" with desirable properties that approximates a given limit function, is equally crucial. Early analysis, in the guise of the Taylor approximation, established an approximation of differentiable functions "f" by polynomials. By the Stone–Weierstrass theorem, every continuous function on can be approximated as closely as desired by a polynomial. A similar approximation technique by trigonometric functions is commonly called Fourier expansion, and is much applied in engineering, see below. More generally, and more conceptually, the theorem yields a simple description of what "basic functions", or, in abstract Hilbert spaces, what basic vectors suffice to generate a Hilbert space "H", in the sense that the "closure" of their span (i.e., finite linear combinations and limits of those) is the whole space. Such a set of functions is called a "basis" of "H", its cardinality is known as the Hilbert space dimension. Not only does the theorem exhibit suitable basis functions as sufficient for approximation purposes, but together with the Gram-Schmidt process, it enables one to construct a basis of orthogonal vectors. Such orthogonal bases are the Hilbert space generalization of the coordinate axes in finite-dimensional Euclidean space.
The solutions to various differential equations can be interpreted in terms of Hilbert spaces. For example, a great many fields in physics and engineering lead to such equations and frequently solutions with particular physical properties are used as basis functions, often orthogonal. As an example from physics, the time-dependent Schrödinger equation in quantum mechanics describes the change of physical properties in time by means of a partial differential equation, whose solutions are called wavefunctions. Definite values for physical properties such as energy, or momentum, correspond to eigenvalues of a certain (linear) differential operator and the associated wavefunctions are called eigenstates. The spectral theorem decomposes a linear compact operator acting on functions in terms of these eigenfunctions and their eigenvalues.
Algebras over fields.
General vector spaces do not possess a multiplication between vectors. A vector space equipped with an additional bilinear operator defining the multiplication of two vectors is an "algebra over a field". Many algebras stem from functions on some geometrical object: since functions with values in a given field can be multiplied pointwise, these entities form algebras. The Stone–Weierstrass theorem mentioned above, for example, relies on Banach algebras which are both Banach spaces and algebras.
Commutative algebra makes great use of rings of polynomials in one or several variables, introduced above. Their multiplication is both commutative and associative. These rings and their quotients form the basis of algebraic geometry, because they are rings of functions of algebraic geometric objects.
Another crucial example are "Lie algebras", which are neither commutative nor associative, but the failure to be so is limited by the constraints ( denotes the product of and ):
Examples include the vector space of "n"-by-"n" matrices, with , the commutator of two matrices, and , endowed with the cross product.
The tensor algebra T("V") is a formal way of adding products to any vector space "V" to obtain an algebra. As a vector space, it is spanned by symbols, called simple tensors
The multiplication is given by concatenating such symbols, imposing the distributive law under addition, and requiring that scalar multiplication commute with the tensor product ⊗, much the same way as with the tensor product of two vector spaces introduced above. In general, there are no relations between and . Forcing two such elements to be equal leads to the symmetric algebra, whereas forcing yields the exterior algebra.
When a field, is explicitly stated, a common term used is -algebra.
Applications.
Vector spaces have manifold applications as they occur in many circumstances, namely wherever functions with values in some field are involved. They provide a framework to deal with analytical and geometrical problems, or are used in the Fourier transform. This list is not exhaustive: many more applications exist, for example in optimization. The minimax theorem of game theory stating the existence of a unique payoff when all players play optimally can be formulated and proven using vector spaces methods. Representation theory fruitfully transfers the good understanding of linear algebra and vector spaces to other mathematical domains such as group theory.
Distributions.
A "distribution" (or "generalized function") is a linear map assigning a number to each "test" function, typically a smooth function with compact support, in a continuous way: in the above terminology the space of distributions is the (continuous) dual of the test function space. The latter space is endowed with a topology that takes into account not only "f" itself, but also all its higher derivatives. A standard example is the result of integrating a test function "f" over some domain Ω:
When the set consisting of a single point, this reduces to the Dirac distribution, denoted by δ, which associates to a test function "f" its value at the . Distributions are a powerful instrument to solve differential equations. Since all standard analytic notions such as derivatives are linear, they extend naturally to the space of distributions. Therefore, the equation in question can be transferred to a distribution space, which is bigger than the underlying function space, so that more flexible methods are available for solving the equation. For example, Green's functions and fundamental solutions are usually distributions rather than proper functions, and can then be used to find solutions of the equation with prescribed boundary conditions. The found solution can then in some cases be proven to be actually a true function, and a solution to the original equation (e.g., using the Lax–Milgram theorem, a consequence of the Riesz representation theorem).
Fourier analysis.
Resolving a periodic function into a sum of trigonometric functions forms a "Fourier series", a technique much used in physics and engineering. The underlying vector space is usually the Hilbert space "L"2(0, 2π), for which the functions sin "mx" and cos "mx" ("m" an integer) form an orthogonal basis. The Fourier expansion of an "L"2 function "f" is
The coefficients "a""m" and "b""m" are called Fourier coefficients of "f", and are calculated by the formulas
In physical terms the function is represented as a superposition of sine waves and the coefficients give information about the function's frequency spectrum. A complex-number form of Fourier series is also commonly used. The concrete formulae above are consequences of a more general mathematical duality called Pontryagin duality. Applied to the group R, it yields the classical Fourier transform; an application in physics are reciprocal lattices, where the underlying group is a finite-dimensional real vector space endowed with the additional datum of a lattice encoding positions of atoms in crystals.
Fourier series are used to solve boundary value problems in partial differential equations. In 1822, Fourier first used this technique to solve the heat equation. A discrete version of the Fourier series can be used in sampling applications where the function value is known only at a finite number of equally spaced points. In this case the Fourier series is finite and its value is equal to the sampled values at all points. The set of coefficients is known as the discrete Fourier transform (DFT) of the given sample sequence. The DFT is one of the key tools of digital signal processing, a field whose applications include radar, speech encoding, image compression. The JPEG image format is an application of the closely related discrete cosine transform.
The fast Fourier transform is an algorithm for rapidly computing the discrete Fourier transform. It is used not only for calculating the Fourier coefficients but, using the convolution theorem, also for computing the convolution of two finite sequences. They in turn are applied in digital filters and as a rapid multiplication algorithm for polynomials and large integers (Schönhage-Strassen algorithm).
Differential geometry.
The tangent plane to a surface at a point is naturally a vector space whose origin is identified with the point of contact. The tangent plane is the best linear approximation, or linearization, of a surface at a point. Even in a three-dimensional Euclidean space, there is typically no natural way to prescribe a basis of the tangent plane, and so it is conceived of as an abstract vector space rather than a real coordinate space. The "tangent space" is the generalization to higher-dimensional differentiable manifolds.
Riemannian manifolds are manifolds whose tangent spaces are endowed with a suitable inner product. Derived therefrom, the Riemann curvature tensor encodes all curvatures of a manifold in one object, which finds applications in general relativity, for example, where the Einstein curvature tensor describes the matter and energy content of space-time. The tangent space of a Lie group can be given naturally the structure of a Lie algebra and can be used to classify compact Lie groups.
Generalizations.
Vector bundles.
A "vector bundle" is a family of vector spaces parametrized continuously by a topological space "X". More precisely, a vector bundle over "X" is a topological space "E" equipped with a continuous map 
such that for every "x" in "X", the fiber π−1("x") is a vector space. The case dim is called a line bundle. For any vector space "V", the projection makes the product into a "trivial" vector bundle. Vector bundles over "X" are required to be locally a product of "X" and some (fixed) vector space "V": for every "x" in "X", there is a neighborhood "U" of "x" such that the restriction of π to π−1("U") is isomorphic to the trivial bundle . Despite their locally trivial character, vector bundles may (depending on the shape of the underlying space "X") be "twisted" in the large (i.e., the bundle need not be (globally isomorphic to) the trivial bundle ). For example, the Möbius strip can be seen as a line bundle over the circle "S"1 (by identifying open intervals with the real line). It is, however, different from the cylinder , because the latter is orientable whereas the former is not.
Properties of certain vector bundles provide information about the underlying topological space. For example, the tangent bundle consists of the collection of tangent spaces parametrized by the points of a differentiable manifold. The tangent bundle of the circle "S"1 is globally isomorphic to , since there is a global nonzero vector field on "S"1. In contrast, by the hairy ball theorem, there is no (tangent) vector field on the 2-sphere "S"2 which is everywhere nonzero. K-theory studies the isomorphism classes of all vector bundles over some topological space. In addition to deepening topological and geometrical insight, it has purely algebraic consequences, such as the classification of finite-dimensional real division algebras: R, C, the quaternions H and the octonions.
The cotangent bundle of a differentiable manifold consists, at every point of the manifold, of the dual of the tangent space, the cotangent space. Sections of that bundle are known as differential one-forms.
Modules.
"Modules" are to rings what vector spaces are to fields. The very same axioms, applied to a ring "R" instead of a field "F" yield modules. The theory of modules, compared to that of vector spaces, is complicated by the presence of ring elements that do not have multiplicative inverses. For example, modules need not have bases, as the Z-module (i.e., abelian group) Z/2Z shows; those modules that do (including all vector spaces) are known as free modules. Nevertheless, a vector space can be compactly defined as a module over a ring which is a field with the elements being called vectors. Some authors use the term "vector space" to mean modules over a division ring. The algebro-geometric interpretation of commutative rings via their spectrum allows the development of concepts such as locally free modules, the algebraic counterpart to vector bundles.
Affine and projective spaces.
Roughly, "affine spaces" are vector spaces whose origins are not specified. More precisely, an affine space is a set with a free transitive vector space action. In particular, a vector space is an affine space over itself, by the map
If "W" is a vector space, then an affine subspace is a subset of "W" obtained by translating a linear subspace "V" by a fixed vector ; this space is denoted by (it is a coset of "V" in "W") and consists of all vectors of the form for An important example is the space of solutions of a system of inhomogeneous linear equations
generalizing the homogeneous case above. The space of solutions is the affine subspace where x is a particular solution of the equation, and "V" is the space of solutions of the homogeneous equation (the nullspace of "A").
The set of one-dimensional subspaces of a fixed finite-dimensional vector space "V" is known as "projective space"; it may be used to formalize the idea of parallel lines intersecting at infinity. Grassmannians and flag manifolds generalize this by parametrizing linear subspaces of fixed dimension "k" and flags of subspaces, respectively.

</doc>
<doc id="7136985" url="https://en.wikipedia.org/wiki?curid=7136985" title="Vectorization (mathematics)">
Vectorization (mathematics)

In mathematics, especially in linear algebra and matrix theory, the vectorization of a matrix is a linear transformation which converts the matrix into a column vector. Specifically, the vectorization of an "m×n" matrix "A", denoted by vec("A"), is the "mn" × 1 column vector obtained by stacking the columns of the matrix "A" on top of one another:
Here formula_2 represents the formula_3-th element of matrix formula_4 and the superscript formula_5 denotes the transpose. Vectorization expresses the isomorphism formula_6 between these vector spaces (of matrices and vectors) in coordinates.
For example, for the 2×2 matrix formula_4 = formula_8, the vectorization is formula_9.
Compatibility with Kronecker products.
The vectorization is frequently used together with the Kronecker product to express matrix multiplication as a linear transformation on matrices. In particular,
for matrices "A", "B", and "C" of dimensions "k×l", "l×m", and "m×n". For example, if formula_11 (the adjoint endomorphism of the Lie algebra gl("n",C) of all "n×n" matrices with complex entries), then formula_12, where formula_13 is the "n×n" identity matrix.
There are two other useful formulations:
Compatibility with Hadamard products.
Vectorization is an algebra homomorphism from the space of "n×n" matrices with the Hadamard (entrywise) product to Cn with its Hadamard product:
Compatibility with inner products.
Vectorization is a unitary transformation from the space of "n×n" matrices with the Frobenius (or Hilbert–Schmidt) inner product to Cn :
where the superscript * denotes the conjugate transpose.
Half-vectorization.
For a symmetric matrix "A", the vector vec("A") contains more information than is strictly necessary, since the matrix is completely determined by the symmetry together with the lower triangular portion, that is, the "n"("n" + 1)/2 entries on and below the main diagonal. For such matrices, the half-vectorization is sometimes more useful than the vectorization. The half-vectorization, vech("A"), of a symmetric "n" × "n" matrix "A" is the "n"("n" + 1)/2 × 1 column vector obtained by vectorizing only the lower triangular part of "A":
For example, for the 2×2 matrix "A" = formula_18, the half-vectorization is vech("A") = formula_19.
There exist unique matrices transforming the half-vectorization of a matrix to its vectorization and vice versa called, respectively, the duplication matrix and the elimination matrix.
Programming language.
Programming languages that implement matrices may have easy means for vectorization.
In Matlab/GNU Octave a matrix codice_1 can be vectorized by codice_2.
GNU Octave also allows vectorization and half-vectorization with codice_3 and codice_4 respectively. Julia has the codice_3 function as well.
In Python NumPy arrays implement the 'flatten' method, while in R the desired effect can be achieved via the 'c()' or 'as.vector()' functions.

</doc>
<doc id="20491903" url="https://en.wikipedia.org/wiki?curid=20491903" title="Velocity">
Velocity

The velocity of an object is the rate of change of its position with respect to a frame of reference, and is a function of time. Velocity is equivalent to a specification of its speed and direction of motion (e.g. to the north). Velocity is an important concept in kinematics, the branch of classical mechanics that describes the motion of bodies.
Velocity is a physical vector quantity; both magnitude and direction are needed to define it. The scalar absolute value (magnitude) of velocity is called "speed", a quantity that is measured in metres per second (m/s or m⋅s−1) in the SI (metric) system. For example, "5 metres per second" is a scalar (not a vector), whereas "5 metres per second east" is a vector.
If there is a change in speed, direction, or both, then the object has a changing velocity and is said to be undergoing an "acceleration".
Constant velocity vs acceleration.
To have a constant velocity, an object must have a constant speed in a constant direction. Constant direction constrains the object to motion in a straight path (the object's path does not curve). Thus, a constant velocity means motion in a straight line at a constant speed.
For example, a car moving at a constant 20 kilometres per hour in a circular path has a constant speed, but does not have a constant velocity because its direction changes. Hence, the car is considered to be undergoing an acceleration.
Distinction between speed and velocity.
Speed describes only how fast an object is moving, whereas velocity gives both how fast and in what direction the object is moving. If a car is said to travel at 60 km/h, its speed has been specified. However, if the car is said to move at 60 km/h to the north, its velocity has now been specified.
The big difference can be noticed when we consider movement around a circle. When something moves in a circle and returns to its starting point, its average velocity is zero but its average speed is found by dividing the circumference of the circle by the time taken to move around the circle. This is because the average velocity is calculated by only considering the displacement between the starting and the end points while the average speed considers only the total distance traveled.
Equation of motion.
Average velocity.
Velocity is defined as the rate of change of position with respect to time, which may also be referred to as the "instantaneous velocity" to emphasize the distinction from the average velocity. In some applications the "average velocity" of an object might be needed, that is to say, the constant velocity that would provide the same resultant displacement as a variable velocity in the same time interval, , over some time period . Average velocity can be calculated as:
The average velocity is always less than or equal to the average speed of an object. This can be seen by realizing that while distance is always strictly increasing, displacement can increase or decrease in magnitude as well as change direction.
In terms of a displacement-time ("x" vs. "t") graph, the instantaneous velocity (or, simply, velocity) can be thought of as the slope of the tangent line to the curve at any point, and the average velocity as the slope of the secant line between two points with "t" coordinates equal to the boundaries of the time period for the average velocity.
The average velocity is the same as the velocity averaged over time – that is to say, its time-weighted average, which may be calculated as the time integral of the velocity:
where we may identify
and
Instantaneous velocity.
If we consider as velocity and as the displacement (change in position) vector, then we can express the (instantaneous) velocity of a particle or object, at any particular time , as the derivative of the position with respect to time:
From this derivative equation, in the one-dimensional case it can be seen that the area under a velocity vs. time ( vs. graph) is the displacement, . In calculus terms, the integral of the velocity function is the displacement function . In the figure, this corresponds to the yellow area under the curve labeled ( being an alternative notation for displacement).
Since the derivative of the position with respect to time gives the change in position (in metres) divided by the change in time (in seconds), velocity is measured in metres per second (m/s). Although the concept of an instantaneous velocity might at first seem counter-intuitive, it may be thought of as the velocity that the object would continue to travel at if it stopped accelerating at that moment.
Relationship to acceleration.
Although velocity is defined as the rate of change of position, it is often common to start with an expression for an object's acceleration. As seen by the three green tangent lines in the figure, an object's instantaneous acceleration at a point in time is the slope of the line tangent to the curve of a ( vs. graph at that point. In other words, acceleration is defined as the derivative of velocity with respect to time:
From there, we can obtain an expression for velocity as the area under an acceleration vs. time ( vs. ) graph. As above, this is done using the concept of the integral:
Constant acceleration.
In the special case of constant acceleration, velocity can be studied using the suvat equations. By considering a as being equal to some arbitrary constant vector, it is trivial to show that
with as the velocity at time and as the velocity at time . By combining this equation with the suvat equation , it is possible to relate the displacement and the average velocity by
It is also possible to derive an expression for the velocity independent of time, known as the Torricelli equation, as follows:
where etc.
The above equations are valid for both Newtonian mechanics and special relativity. Where Newtonian mechanics and special relativity differ is in how different observers would describe the same situation. In particular, in Newtonian mechanics, all observers agree on the value of t and the transformation rules for position create a situation in which all non-accelerating observers would describe the acceleration of an object with the same values. Neither is true for special relativity. In other words, only relative velocity can be calculated.
Quantities that are dependent on velocity.
The kinetic energy of a moving object is dependent on its velocity and is given by the equation
ignoring special relativity, where "E"k is the kinetic energy and "m" is the mass. Kinetic energy is a scalar quantity as it depends on the square of the velocity, however a related quantity, momentum, is a vector and defined by 
In special relativity, the dimensionless Lorentz Factor appears frequently, and is given by 
where γ is the Lorentz factor and "c" is the speed of light.
Escape velocity is the minimum speed a ballistic object needs to escape from a massive body such as Earth. It represents the kinetic energy that, when added to the object's gravitational potential energy, (which is always negative) is equal to zero. The general formula for the escape velocity of an object at a distance "r" from the center of a planet with mass "M" is
where "G" is the Gravitational constant and "g" is the Gravitational acceleration. The escape velocity from Earth's surface is about 11 200 m/s, and is irrespective of the direction of the object.
Relative velocity.
Relative velocity is a measurement of velocity between two objects
as determined in a single coordinate system. Relative velocity is fundamental in both classical and modern physics, since many systems in physics deal with the relative motion of two or more particles. In Newtonian mechanics, the relative velocity is independent of the chosen inertial reference frame. This is not the case anymore with special relativity in which velocities depend on the choice of reference frame.
If an object A is moving with velocity vector v and an object B with velocity vector w, then the velocity of object A "relative to" object B is defined as the difference of the two velocity vectors:
Similarly the relative velocity of object B moving with velocity w, relative to object A moving with velocity v is:
Usually the inertial frame is chosen in which the latter of the two mentioned objects is in rest.
Scalar velocities.
In the one-dimensional case, the velocities are scalars and the equation is either:
Polar coordinates.
In polar coordinates, a two-dimensional velocity is described by a radial velocity, defined as the component of velocity away from or toward the origin (also known as "velocity made good"), and an angular velocity, which is the rate of rotation about the origin (with positive quantities representing counter-clockwise rotation and negative quantities representing clockwise rotation, in a right-handed coordinate system).
The radial and angular velocities can be derived from the Cartesian velocity and displacement vectors by decomposing the velocity vector into radial and transverse components. The transverse velocity is the component of velocity along a circle centered at the origin.
where
The "magnitude of the radial velocity" is the dot product of the velocity vector and the unit vector in the direction of the displacement.
where
The "magnitude of the transverse velocity" is that of the cross product of the unit vector in the direction of the displacement and the velocity vector. It is also the product of the angular speed formula_27 and the magnitude of the displacement.
such that
Angular momentum in scalar form is the mass times the distance to the origin times the transverse velocity, or equivalently, the mass times the distance squared times the angular speed. The sign convention for angular momentum is the same as that for angular velocity.
where
The expression formula_33 is known as moment of inertia.
If forces are in the radial direction only with an inverse square dependence, as in the case of a gravitational orbit, angular momentum is constant, and transverse speed is inversely proportional to the distance, angular speed is inversely proportional to the distance squared, and the rate at which area is swept out is constant. These relations are known as Kepler's laws of planetary motion.

</doc>
<doc id="192316" url="https://en.wikipedia.org/wiki?curid=192316" title="Virtual particle">
Virtual particle

In physics, a virtual particle is an explanatory conceptual entity that is found in mathematical calculations about quantum field theory. It refers to mathematical terms that have some appearance of representing particles inside a subatomic process such as a collision. Virtual particles, however, do not appear directly amongst the observable and detectable input and output quantities of those calculations, which refer only to actual, as distinct from virtual, particles. Virtual particle terms represent "particles" that are said to be "off mass shell". For example, they can progress backwards in time, can have apparent mass very different from their regular particle namesake's, and can travel faster than light. That is to say, when looked at individually, they appear to be able to violate basic laws of physics. Regular particles of course never do so. On the other hand, any particle that is actually observed never precisely satisfies the conditions theoretically imposed on regular particles. Virtual particles occur in combinations that mutually more or less nearly cancel from the actual output quantities, so that no actual violation of the laws of physics occurs in completed processes. Often the virtual-particle virtual "events" appear to occur close to one another in time, for example within the time scale of a collision, so that they are virtually and apparently "short-lived". If the mathematical terms that are interpreted as representing virtual particles are omitted from the calculations, the result is an approximation that may or may not be near the correct and accurate answer obtained from the proper full calculation.
Quantum theory is different from classical theory. The difference is in accounting for the inner workings of subatomic processes. Classical physics cannot account for such. It was pointed out by Heisenberg that what "actually" or "really" occurs inside such subatomic processes as collisions is not directly observable and no unique and physically definite visualization is available for it. Quantum mechanics has the specific merit of by-passing speculation about such inner workings. It restricts itself to what is actually observable and detectable. Virtual particles are conceptual devices that in a sense try to by-pass Heisenberg's insight, by offering putative or virtual explanatory visualizations for the inner workings of subatomic processes.
A virtual particle does not necessarily appear to carry the same mass as the corresponding real particle. This is because it appears as "short-lived" and "transient", so that the uncertainty principle allows it to appear not to conserve energy and momentum. The longer a virtual particle appears to "live", the closer its characteristics come to those of an actual particle.
Virtual particles appear in many processes, including particle scattering and Casimir forces. In quantum field theory, even classical forces — such as the electromagnetic repulsion or attraction between two charges — can be thought of as due to the exchange of many virtual photons between the charges.
Virtual particles appear in calculations of subatomic interactions, but never as asymptotic states or indices to the scattering matrix. A subatomic process involving virtual particles is schematically representable by a Feynman diagram in which they are represented by internal lines.
Antiparticles and quasiparticles should not be confused with virtual particles or virtual antiparticles.
Many physicists believe that, because of its intrinsically perturbative character, the concept of virtual particles is often confusing and misleading, and is thus best avoided.
Properties.
The concept of virtual particles arises in the perturbation theory of quantum field theory, an approximation scheme in which interactions (in essence, forces) between actual particles are calculated in terms of exchanges of virtual particles. Such calculations are often performed using schematic representations known as Feynman diagrams, in which virtual particles appear as internal lines. By expressing the interaction in terms of the exchange of a virtual particle with four-momentum q, where q is given by the difference between the four-momenta of the particles entering and leaving the interaction vertex, both momentum and energy are conserved at the interaction vertices of the Feynman diagram.
A virtual particle does not precisely obey the energy–momentum relation . Its kinetic energy may not have the usual relationship to velocity–indeed, it can be negative. This is expressed by the phrase "off mass shell". The probability amplitude for a virtual particle to exist tends to be canceled out by destructive interference over longer distances and times. Quantum tunnelling may be considered a manifestation of virtual particle exchanges. The range of forces carried by virtual particles is limited by the uncertainty principle, which regards energy and time as conjugate variables; thus, virtual particles of larger mass have more limited range.
Written in the usual mathematical notations, in the equations of physics, there is no mark of the distinction between virtual and actual particles. The amplitude that a virtual particle exists interferes with the amplitude for its non-existence, whereas for an actual particle the cases of existence and non-existence cease to be coherent with each other and do not interfere any more. In the quantum field theory view, actual particles are viewed as being detectable excitations of underlying quantum fields. Virtual particles are also viewed as excitations of the underlying fields, but appear only as forces, not as detectable particles. They are "temporary" in the sense that they appear in calculations, but are not detected as single particles. Thus, in mathematical terms, they never appear as indices to the scattering matrix, which is to say, they never appear as the observable inputs and outputs of the physical process being modelled.
There are two principal ways in which the notion of virtual particles appears in modern physics. They appear as intermediate terms in Feynman diagrams; that is, as terms in a perturbative calculation. They also appear as an infinite set of states to be summed or integrated over in the calculation of a semi-non-perturbative effect. In the latter case, it is sometimes said that virtual particles contribute to a mechanism that mediates the effect, or that the effect occurs through the virtual particles.
Manifestations.
There are many observable physical phenomena that arise in interactions involving virtual particles. For bosonic particles that exhibit rest mass when they are free and actual, virtual interactions are characterized by the relatively short range of the force interaction produced by particle exchange. Examples of such short-range interactions are the strong and weak forces, and their associated field bosons. For the gravitational and electromagnetic forces, the zero rest-mass of the associated boson particle permits long-range forces to be mediated by virtual particles. However, in the case of photons, power and information transfer by virtual particles is a relatively short-range phenomenon (existing only within a few wavelengths of the field-disturbance, which carries information or transferred power), as for example seen in the characteristically short range of inductive and capacitative effects in the near field zone of coils and antennas.
Some field interactions which may be seen in terms of virtual particles are:
Most of these have analogous effects in solid-state physics; indeed, one can often gain a better intuitive understanding by examining these cases. In semiconductors, the roles of electrons, positrons and photons in field theory are replaced by electrons in the conduction band, holes in the valence band, and phonons or vibrations of the crystal lattice. A virtual particle is in a virtual state where the probability amplitude is not conserved. Examples of macroscopic virtual phonons, photons, and electrons in the case of the tunneling process were presented by Günter Nimtz and Alfons A. Stahlhofen.
History.
Paul Dirac was the first to propose that empty space (a vacuum) can be visualized as consisting of a sea of electrons with negative energy, known as the Dirac sea. The Dirac sea has a direct analog to the electronic band structure in crystalline solids as described in solid state physics. Here, particles correspond to conduction electrons, and antiparticles to holes. A variety of interesting phenomena can be attributed to this structure. The development of quantum field theory (QFT) in the 1930s made it possible to reformulate the Dirac equation in a way that treats the positron as a "real" particle rather than the absence of a particle, and makes the vacuum the state in which no particles exist instead of an infinite sea of particles.
Feynman diagrams.
The calculation of scattering amplitudes in theoretical particle physics requires the use of some rather large and complicated integrals over a large number of variables. These integrals do, however, have a regular structure, and may be represented as Feynman diagrams. The appeal of the Feynman diagrams is strong, as it allows for a simple visual presentation of what would otherwise be a rather arcane and abstract formula. In particular, part of the appeal is that the outgoing legs of a Feynman diagram can be associated with actual, on-shell particles. Thus, it is natural to associate the other lines in the diagram with particles as well, called the "virtual particles". In mathematical terms, they correspond to the propagators appearing in the diagram.
In the image to the right, the solid lines correspond to actual particles (of momentum p1 and so on), while the dotted line corresponds to a virtual particle carrying momentum "k". For example, if the solid lines were to correspond to electrons interacting by means of the electromagnetic interaction, the dotted line would correspond to the exchange of a virtual photon. In the case of interacting nucleons, the dotted line would be a virtual pion. In the case of quarks interacting by means of the strong force, the dotted line would be a virtual gluon, and so on.
Virtual particles may be mesons or vector bosons, as in the example above; they may also be fermions. However, in order to preserve quantum numbers, most simple diagrams involving fermion exchange are prohibited. The image to the right shows an allowed diagram, a one-loop diagram. The solid lines correspond to a fermion propagator, the wavy lines to bosons.
Vacuums.
In formal terms, a particle is considered to be an eigenstate of the particle number operator a†a, where a is the particle annihilation operator and a† the particle creation operator (sometimes collectively called ladder operators). In many cases, the particle number operator does not commute with the Hamiltonian for the system. This implies the number of particles in an area of space is not a well-defined quantity but, like other quantum observables, is represented by a probability distribution. Since these particles do not have a permanent existence, they are called "virtual particles" or "vacuum fluctuations" of vacuum energy. In a certain sense, they can be understood to be a manifestation of the time-energy uncertainty principle in a vacuum.
An important example of the "presence" of virtual particles in a vacuum is the Casimir effect. Here, the explanation of the effect requires that the total energy of all of the virtual particles in a vacuum can be added together. Thus, although the virtual particles themselves are not directly observable in the laboratory, they do leave an observable effect: Their zero-point energy results in forces acting on suitably arranged metal plates or dielectrics. On the other hand, the Casimir effect can be interpreted as the relativistic van der Waals force.
Pair production.
Virtual particles are often popularly described as coming in pairs, a particle and antiparticle, which can be of any kind. These pairs exist for an extremely short time, and then mutually annihilate. In some cases, however, it is possible to boost the pair apart using external energy so that they avoid annihilation and become actual particles.
This may occur in one of two ways. In an accelerating frame of reference, the virtual particles may appear to be actual to the accelerating observer; this is known as the Unruh effect. In short, the vacuum of a stationary frame appears, to the accelerated observer, to be a warm gas of actual particles in thermodynamic equilibrium.
Another example is pair production in very strong electric fields, sometimes called vacuum decay. If, for example, a pair of atomic nuclei are merged to very briefly form a nucleus with a charge greater than about 140, (that is, larger than about the inverse of the fine structure constant, which is a dimensionless quantity), the strength of the electric field will be such that it will be energetically favorable to create positron-electron pairs out of the vacuum or Dirac sea, with the electron attracted to the nucleus to annihilate the positive charge. This pair-creation amplitude was first calculated by Julian Schwinger in 1951.
Actual and virtual particles compared.
As a consequence of quantum mechanical uncertainty, any object or process that exists for a limited time or in a limited volume cannot have a precisely defined energy or momentum. This is the reason that virtual particles — which exist only temporarily as they are exchanged between ordinary particles — do not necessarily obey the mass-shell relation. However, the longer a virtual particle exists, the more closely it adheres to the mass-shell relation. A "virtual" particle that exists for an arbitrarily long time is simply an ordinary particle.
However, all particles have a finite lifetime, as they are created and eventually destroyed by some processes. As such, there is no absolute distinction between "real" and "virtual" particles. In practice, the lifetime of "ordinary" particles is far longer than the lifetime of the virtual particles that contribute to processes in particle physics, and as such the distinction is useful to make.

</doc>
<doc id="1211986" url="https://en.wikipedia.org/wiki?curid=1211986" title="Virtual work">
Virtual work

Virtual work arises in the application of the "principle of least action" to the study of forces and movement of a mechanical system. The work of a force acting on a particle as it moves along a displacement will be different for different displacements. Among all the possible displacements that a particle may follow, called virtual displacements, one will minimize the action, and, therefore, is the one followed by the particle by the principle of least action. The work of a force on a particle along a virtual displacement is known as the virtual work.
Historically, virtual work and the associated calculus of variations were formulated to analyze systems of rigid bodies, but they have also been developed for the study of the mechanics of deformable bodies.
History.
The introduction of virtual work and the principle of least action was guided by the view that the actual movement of a body is the one in a set of "tentative" realities that minimizes a particular quantity. This idea that nature minimizes is a version of the "simplicity hypothesis" that can be traced to Aristotle. Another form of this hypothesis is Occam's razor which states that "it is futile to employ many principles when it is possible to employ fewer." These ideas illustrate a view of physics that nature optimizes in some way.
Gottfried Leibniz formulated Newton's laws of motion in terms of work and kinetic energy, or "vis viva" (living force), which are minimized as a system moves. Maupertuis adapted Leibniz's ideas as the "principle of least action" that nature minimizes action. But it was Euler and Lagrange who provided the mathematical foundation of the calculus of variations and applied it to the study of the statics and dynamics of mechanical systems.
Hamilton's reformulation of the principle of least action and Lagrange's equations yielded a theory of dynamics that is the foundation for modern physics and quantum mechanics.
Overview.
If a force acts on a particle as it moves from point "A" to point "B", then, for each possible trajectory that the particle may take, it is possible to compute the total work done by the force along the path. The "principle of virtual work", which is the form of the principle of least action applied to these systems, states that the path actually followed by the particle is the one for which the difference between the work along this path and other nearby paths is zero (to first order). The formal procedure for computing the difference of functions evaluated on nearby paths is a generalization of the derivative known from differential calculus, and is termed "the calculus of variations".
Consider a point particle that moves along a path which is described by a function r("t") from point "A", where r("t" = "t"0), to point "B", where r("t" = "t"1). It is possible that the particle moves from "A" to "B" along a nearby path described by r("t") + δr("t"), where δr("t") is called the variation of r("t"). The variation δr(t) satisfies the requirement δr("t"0) = δr("t"1) = 0. The components of the variation, δ"r"1(t), δ"r"2(t) and δ"r"3(t), are called virtual displacements. This can be generalized to an arbitrary mechanical system defined by the generalized coordinates "q""i" , "i" = 1, ..., "n". In which case, the variation of the trajectory "q""i" ("t") is defined by the virtual displacements "δq""i" , "i" = 1, ..., "n".
Virtual work is the total work done by the applied forces and the inertial forces of a mechanical system as it moves through a set of virtual displacements. When considering forces applied to a body in static equilibrium, the principle of least action requires the virtual work of these forces to be zero.
Introduction.
Consider a particle "P" that moves from a point "A" to a point "B" along a trajectory r("t"), while a force F(r("t")) is applied to it. The work done by the force F is given by the integral
where "d"r is the differential element along the curve that is the trajectory of "P", and v is its velocity. It is important to notice that the value of the work "W" depends on the trajectory r("t").
Now consider particle "P" that moves from point "A" to point "B" again, but this time it moves along the nearby trajectory that differs from r("t") by the variation "δr("t")="εh("t"), where "ε" is a scaling constant that can be made as small as desired and h("t") is an arbitrary function that satisfies h("t"0) = h("t"1) = 0. Suppose the force F(r("t")+"ε"h("t")) is the same as F(r("t")). The work done by the force is given by the integral
The variation of the work "δW" associated with this nearby path, known as the "virtual work", can be computed to be
If there is no constraint force, then 6 parameters are needed to completely describe the motion of "P". If there are "k" ("k" ≤ 6) constraint forces, then "n" = (6 - "k") parameters are needed. Hence, we can define "n" generalized coordinates "q""i" ("t") ("i" = 1, 2, ..., "n"), and express r("t") and "δr="εh("t") in terms of the generalized coordinates. That is,
Then, the derivative of the variation "δr="εh("t") is given by
then we have 
The requirement that the virtual work be zero for an arbitrary variation "δ"r("t")=εh("t") is equivalent to the set of requirements
The terms "Qi" are called the "generalized forces" associated with the virtual displacement δr.
Static equilibrium.
Static equilibrium is a state in which the net force and net torque acted upon the system is zero. In other words, both linear momentum and angular momentum of the system are conserved. The principle of virtual work states that "the virtual work of the applied forces is zero for all virtual movements of the system from static equilibrium". This principle can be generalised such that three dimensional rotations are included: the virtual work of the applied forces and applied moments is zero for all virtual movements of the system from static equilibrium. That is
where Fi" , "i" = 1, 2, ..., "m" and Mj" , "j" = 1, 2, ..., "n" are the applied forces and applied moments, respectively, and "δri" , "i" = 1, 2, ..., "m" and "δφj" , "j" = 1, 2, ..., "n" are the virtual displacements and virtual rotations, respectively.
Suppose the system consists of "N" particles, and it has "f" ("f" ≤ 6"N") degrees of freedom. It is sufficient to use only "f" coordinates to give a complete description of the motion of the system, so "f" generalised coordinates "qk" , "k" = 1, 2, ..., "f" are defined such that the virtual movements can be expressed in terms of these generalised coordinates. That is,
The virtual work can then be reparametrised by the generalised coordinates:
where the generalised forces "Qk" are defined as
Kane shows that these generalised forces can also be formulated in terms of the ratio of time derivatives. That is,
The principle of virtual work requires that the virtual work done on a system by the forces F"i" and moments M"j" vanishes if it is in equilibrium. Therefore, the generalized forces "Q""k" are zero, that is
Constraint forces.
An important benefit of the principle of virtual work is that only forces that do work as the system moves through a virtual displacement are needed to determine the mechanics of the system. There are many forces in a mechanical system that do no work during a virtual displacement, which means that they need not be considered in this analysis. The two important examples are (i) the internal forces in a rigid body, and (ii) the constraint forces at an ideal joint.
Lanczos presents this as the postulate: "The virtual work of the forces of reaction is always zero for any virtual displacement which is in harmony with the given kinematic constraints." The argument is as follows. The principle of virtual work states that in equilibrium the virtual work of the forces applied to a system is zero. Newton's laws state that at equilibrium the applied forces are equal and opposite to the reaction, or constraint forces. This means the virtual work of the constraint forces must be zero as well.
Law of the Lever.
A lever is modeled as a rigid bar connected to a ground frame by a hinged joint called a fulcrum. The lever is operated by applying an input force F"A" at a point "A" located by the coordinate vector r"A" on the bar. The lever then exerts an output force F"B" at the point "B" located by r"B". The rotation of the lever about the fulcrum "P" is defined by the rotation angle "θ".
Let the coordinate vector of the point "P" that defines the fulcrum be r"P", and introduce the lengths 
which are the distances from the fulcrum to the input point "A" and to the output point "B", respectively.
Now introduce the unit vectors e"A" and e"B" from the fulcrum to the point "A" and "B", so
This notation allows us to define the velocity of the points "A" and "B" as
where e"A"⊥ and e"B"⊥ are unit vectors perpendicular to e"A" and e"B", respectively.
The angle "θ" is the generalized coordinate that defines the configuration of the lever, therefore using the formula above for forces applied to a one degree-of-freedom mechanism, the generalized force is given by
Now, denote as "F""A" and "F""B" the components of the forces that are perpendicular to the radial segments "PA" and "PB". These forces are given by
This notation and the principle of virtual work yield the formula for the generalized force as
The ratio of the output force "F""B" to the input force "F""A" is the mechanical advantage of the lever, and is obtained from the principle of virtual work as
This equation shows that if the distance "a" from the fulcrum to the point "A" where the input force is applied is greater than the distance "b" from fulcrum to the point "B" where the output force is applied, then the lever amplifies the input force. If the opposite is true that the distance from the fulcrum to the input point "A" is less than from the fulcrum to the output point "B", then the lever reduces the magnitude of the input force.
This is the "law of the lever", which was proven by Archimedes using geometric reasoning.
Gear train.
A gear train is formed by mounting gears on a frame so that the teeth of the gears engage. Gear teeth are designed to ensure the pitch circles of engaging gears roll on each other without slipping, this provides a smooth transmission of rotation from one gear to the next. For this analysis, we consider a gear train that has one degree-of-freedom, which means the angular rotation of all the gears in the gear train are defined by the angle of the input gear. 
The size of the gears and the sequence in which they engage define the ratio of the angular velocity "ωA" of the input gear to the angular velocity "ωB" of the output gear, known as the speed ratio, or gear ratio, of the gear train. Let "R" be the speed ratio, then
The input torque "T""A" acting on the input gear "G""A" is transformed by the gear train into the output torque "T""B" exerted by the output gear "G""B". If we assume, that the gears are rigid and that there are no losses in the engagement of the gear teeth, then the principle of virtual work can be used to analyze the static equilibrium of the gear train.
Let the angle "θ" of the input gear be the generalized coordinate of the gear train, then the speed ratio "R" of the gear train defines the angular velocity of the output gear in terms of the input gear, that is
The formula above for the principle of virtual work with applied torques yields the generalized force
The mechanical advantage of the gear train is the ratio of the output torque "T""B" to the input torque "T""A", and the above equation yields
Thus, the speed ratio of a gear train also defines its mechanical advantage. This shows that if the input gear rotates faster than the output gear, then the gear train amplifies the input torque. And, if the input gear rotates slower than the output gear, then the gear train reduces the input torque.
Dynamic equilibrium for rigid bodies.
If the principle of virtual work for applied forces is used on individual particles of a rigid body, the principle can be generalized for a rigid body: "When a rigid body that is in equilibrium is subject to virtual compatible displacements, the total virtual work of all external forces is zero; and conversely, if the total virtual work of all external forces acting on a rigid body is zero then the body is in equilibrium".
If a system is not in static equilibrium, D'Alembert showed that by introducing the acceleration terms of Newton's laws as inertia forces, this approach is generalized to define dynamic equilibrium. The result is D'Alembert's form of the principle of virtual work, which is used to derive the equations of motion for a mechanical system of rigid bodies.
The expression "compatible displacements" means that the particles remain in contact and displace together so that the work done by pairs of action/reaction inter-particle forces cancel out. Various forms of this principle have been credited to Johann (Jean) Bernoulli (1667–1748) and Daniel Bernoulli (1700–1782).
Generalized inertia forces.
Let a mechanical system be constructed from n rigid bodies, Bi, i=1...,n, and let the resultant of the applied forces on each body be the force-torque pairs, Fi and Ti, i=1...,n. Notice that these applied forces do not include the reaction forces where the bodies are connected. Finally, assume that the velocity Vi and angular velocities ωi, i=,1...,n, for each rigid body, are defined by a single generalized coordinate q. Such a system of rigid bodies is said to have one degree of freedom.
Consider a single rigid body which moves under the action of a resultant for F and torque T, with one degree of freedom defined by the generalized coordinate q. Assume the reference point for the resultant force and torque is the center of mass of the body, then the generalized inertia force Q* associated with the generalized coordinate q is given by
This inertia force can be computed from the kinetic energy of the rigid body,
by using the formula
A system of n rigid bodies with m generalized coordinates has the kinetic energy
which can be used to calculate the m generalized inertia forces
D'Alembert's form of the principle of virtual work.
D'Alembert's form of the principle of virtual work states that a system of rigid bodies is in dynamic equilibrium when the virtual work of the sum of the applied forces and the inertial forces is zero for any virtual displacement of the system. Thus, dynamic equilibrium of a system of n rigid bodies with m generalized coordinates requires that
for any set of virtual displacements δqj. This condition yields m equations,
which can also be written as
The result is a set of m equations of motion that define the dynamics of the rigid body system.
If the generalized forces Qj are derivable from a potential energy V(q1...,qm), then these equations of motion take the form
In this case, introduce the Lagrangian, L=T-V, so these equations of motion become
These are known as Lagrange's equations of motion.
Virtual work principle for a deformable body.
Consider now the free body diagram of a deformable body, which is composed of an infinite number of differential cubes. Let's define two unrelated states for the body:
The superscript * emphasizes that the two states are unrelated. Other than the above stated conditions, there is no need to specify if any of the states are real or virtual.
Imagine now that the forces and stresses in the formula_37-State undergo the displacements and deformations in the formula_39-State: We can compute the total virtual (imaginary) work done by all forces acting on the faces of all cubes in two different ways:
Equating the two results leads to the principle of virtual work for a deformable body:
where the total external virtual work is done by T and f. Thus,
The right-hand-side of (d,e) is often called the internal virtual work. The principle of virtual work then states: "External virtual work is equal to internal virtual work when equilibrated forces and stresses undergo unrelated but consistent displacements and strains". It includes the principle of virtual work for rigid bodies as a special case where the internal virtual work is zero.
Proof of Equivalence between the Principle of Virtual Work and the Equilibrium Equation.
We start by looking at the total work done by surface traction on the body going through the specified deformation:
Applying divergence theorem to the right hand side yields:
Now switch to indicial notation for the ease of derivation.
To continue our derivation, we substitute in the equilibrium equation formula_55. Then
The first term on the right hand side needs to be broken into a symmetric part and a skew part as follows:
where formula_58 is the strain that is consistent with the specified displacement field. The 2nd to last equality comes from the fact that the stress matrix is symmetric and that the product of a skew matrix and a symmetric matrix is zero.
Now recap. We have shown through the above derivation that
Move the 2nd term on the right hand side of the equation to the left:
The physical interpretation of the above equation is, "the External virtual work is equal to internal virtual work when equilibrated forces and stresses undergo unrelated but consistent displacements and strains".
For practical applications:
These two general scenarios give rise to two often stated variational principles. They are valid irrespective of material behaviour.
Principle of virtual displacements.
Depending on the purpose, we may specialize the virtual work equation. For example, to derive the principle of virtual displacements in variational notations for supported bodies, we specify:
The virtual work equation then becomes the principle of virtual displacements:
This relation is equivalent to the set of equilibrium equations written for a differential element in the deformable body as well as of the stress boundary conditions on the part formula_63 of the surface. Conversely, (f) can be reached, albeit in a non-trivial manner, by starting with the differential equilibrium equations and the stress boundary conditions on formula_63, and proceeding in the manner similar to (a) and (b).
Since virtual displacements are automatically compatible when they are expressed in terms of continuous, single-valued functions, we often mention only the need for consistency between strains and displacements. The virtual work principle is also valid for large real displacements; however, Eq.(f) would then be written using more complex measures of stresses and strains.
Principle of virtual forces.
Here, we specify:
The virtual work equation becomes the principle of virtual forces:
This relation is equivalent to the set of strain-compatibility equations as well as of the displacement boundary conditions on the part formula_68. It has another name: the principle of complementary virtual work.
Alternative forms.
A specialization of the principle of virtual forces is the unit dummy force method, which is very useful for computing displacements in structural systems. According to D'Alembert's principle, inclusion of inertial forces as additional body forces will give the virtual work equation applicable to dynamical systems. More generalized principles can be derived by:
These are described in some of the references.
Among the many energy principles in structural mechanics, the virtual work principle deserves a special place due to its generality that leads to powerful applications in structural analysis, solid mechanics, and finite element method in structural mechanics.

</doc>
<doc id="32498" url="https://en.wikipedia.org/wiki?curid=32498" title="Volume">
Volume

Volume is the quantity of three-dimensional space enclosed by some closed boundary, for example, the space that a substance (solid, liquid, gas, or plasma) or shape occupies or contains.
Volume is often quantified numerically using the SI derived unit, the cubic metre. The volume of a container is generally understood to be the capacity of the container, i. e. the amount of fluid (gas or liquid) that the container could hold, rather than the amount of space the container itself displaces.
Three dimensional mathematical shapes are also assigned volumes. Volumes of some simple shapes, such as regular, straight-edged, and circular shapes can be easily calculated using arithmetic formulas. Volumes of a complicated shape can be calculated by integral calculus if a formula exists for the shape's boundary. Where a variance in shape and volume occurs, such as those that exist between different human beings, these can be calculated using three-dimensional techniques such as the Body Volume Index. One-dimensional figures (such as lines) and two-dimensional shapes (such as squares) are assigned zero volume in the three-dimensional space.
The volume of a solid (whether regularly or irregularly shaped) can be determined by fluid displacement. Displacement of liquid can also be used to determine the volume of a gas. The combined volume of two substances is usually greater than the volume of one of the substances. However, sometimes one substance dissolves in the other and the combined volume is not additive.
In "differential geometry", volume is expressed by means of the volume form, and is an important global Riemannian invariant.
In "thermodynamics", volume is a fundamental parameter, and is a conjugate variable to pressure.
Units.
Any unit of length gives a corresponding unit of volume, namely the volume of a cube whose side has the given length. For example, a cubic centimetre (cm3) would be the volume of a cube whose sides are one centimetre (1 cm) in length.
In the International System of Units (SI), the standard unit of volume is the cubic metre (m3). The metric system also includes the litre (L) as a unit of volume, where one litre is the volume of a 10-centimetre cube. Thus
so
Small amounts of liquid are often measured in millilitres, where
Various other traditional units of volume are also in use, including the cubic inch, the cubic foot, the cubic mile, the teaspoon, the tablespoon, the fluid ounce, the fluid dram, the gill, the pint, the quart, the gallon, the minim, the barrel, the cord, the peck, the bushel, and the hogshead.
Related terms.
Volume and "capacity" are sometimes distinguished, with capacity being used for how much a container can hold (with contents measured commonly in litres or its derived units), and volume being how much space an object displaces (commonly measured in cubic metres or its derived units).
Volume and capacity are also distinguished in capacity management, where capacity is defined as volume over a specified time period. However, in this context the term volume may be more loosely interpreted to mean "quantity".
The "density" of an object is defined as mass per unit volume. The inverse of density is "specific volume" which is defined as volume divided by mass. Specific volume is a concept important in thermodynamics where the volume of a working fluid is often an important parameter of a system being studied.
The volumetric flow rate in fluid dynamics is the volume of fluid which passes through a given surface per unit time (for example cubic meters per second [m3 s−1]).
Volume in calculus.
In calculus, a branch of mathematics, the volume of a region "D" in R3 is given by a triple integral of the constant function formula_1 and is usually written as:
The volume integral in cylindrical coordinates is
and the volume integral in spherical coordinates (using the convention for angles with formula_4 as the azimuth and formula_5 measured from the polar axis (see more on conventions)) has the form
Volume formulas.
Volume ratios for a cone, sphere and cylinder of the same radius and height.
The above formulas can be used to show that the volumes of a cone, sphere and cylinder of the same radius and height are in the ratio 1 : 2 : 3, as follows.
Let the radius be "r" and the height be "h" (which is 2"r" for the sphere), then the volume of cone is
the volume of the sphere is
while the volume of the cylinder is
The discovery of the 2 : 3 ratio of the volumes of the sphere and cylinder is credited to Archimedes.
Volume formula derivations.
Sphere.
The volume of a sphere is the integral of an infinite number of infinitesimally small circular disks of thickness "dx".
The calculation for the volume of a sphere with center 0 and radius "r" is as follows.
The surface area of the circular disk is formula_10.
The radius of the circular disks, defined such that the x-axis cuts perpendicularly through them, is
formula_11
or
formula_12
where y or z can be taken to represent the radius of a disk at a particular x value.
Using y as the disk radius, the volume of the sphere can be calculated as formula_13
Now formula_14
Combining yields formula_15
This formula can be derived more quickly using the formula for the sphere's surface area, which is formula_16.
The volume of the sphere consists of layers of infinitesimally thin spherical shells, and the sphere volume is equal to
formula_17 = formula_18
Cone.
The cone is a type of pyramidal shape. The fundamental equation for pyramids, one-third times base times altitude, applies to cones as well.
However, using calculus, the volume of a cone is the integral of an infinite number of infinitesimally thin circular disks of thickness "dx".
The calculation for the volume of a cone of height "h", whose base is centered at (0,0,0) with radius "r", is as follows.
The radius of each circular disk is "r" if "x" = 0 and 0 if "x" = "h", and varying linearly in between—that is, formula_19
The surface area of the circular disk is then formula_20
The volume of the cone can then be calculated as formula_21
and after extraction of the constants: formula_22
Integrating gives us formula_23
Volume in differential geometry.
In differential geometry, a branch of mathematics, a volume form on a differentiable manifold is a differential form of top degree (i.e. whose degree is equal to the dimension of the manifold) that is nowhere equal to zero. A manifold has a volume form if and only if it is orientable. An orientable manifold has infinitely many volume forms, since multiplying a volume form by a non-vanishing function yields another volume form. On non-orientable manifolds, one may instead define the weaker notion of a density. Integrating the volume form gives the volume of the manifold according to that form.
Any oriented Riemannian (or pseudo-Riemannian) manifold has a natural volume (or pseudo volume) form. In local coordinates, it can be expressed as
where the formula_25 are the 1-forms providing an oriented basis for the cotangent bundle of the "n"-dimensional manifold. Here, formula_26 is the absolute value of the determinant of the matrix representation of the metric tensor on the manifold.
Volume in thermodynamics.
In thermodynamics, the volume of a system is an important extensive parameter for describing its thermodynamic state. The specific volume, an intensive property, is the system's volume per unit of mass. Volume is a function of state and is interdependent with other thermodynamic properties such as pressure and temperature. For example, volume is related to the pressure and temperature of an ideal gas by the ideal gas law.

</doc>
<doc id="405532" url="https://en.wikipedia.org/wiki?curid=405532" title="W and Z bosons">
W and Z bosons

The W and Z bosons (together known as the weak bosons or, less specifically, the intermediate vector bosons) are the elementary particles that mediate the weak interaction; their symbols are , , and . The W bosons have a positive and negative electric charge of 1 elementary charge respectively and are each other's antiparticles. The Z boson is electrically neutral and is its own antiparticle. The three particles have a spin of 1, and the W bosons have a magnetic moment, while the Z has none. All three of these particles are very short-lived, with a half-life of about . Their discovery was a major success for what is now called the Standard Model of particle physics.
The W bosons are named after the weak force. The physicist Steven Weinberg named the additional particle the "Z particle", later giving the explanation that it was the last additional particle needed by the model – the W bosons had already been named – and that it has zero electric charge.
The two W bosons are best known as mediators of neutrino absorption and emission, where their charge is associated with electron or positron emission or absorption, always causing nuclear transmutation. The Z boson is not involved in the absorption or emission of electrons and positrons.
The Z boson mediates the transfer of momentum, spin, and energy when neutrinos scatter "elastically" from matter, something that must happen without the production or absorption of new, charged particles. Such behaviour (which is almost as common as inelastic neutrino interactions) is seen in bubble chambers irradiated with neutrino beams. Whenever an electron simply "appears" in such a chamber as a new free particle suddenly moving with kinetic energy, and moves in the direction of the neutrinos as the apparent result of a new impulse, and this behavior happens more often when the neutrino beam is present, it is inferred to be a result of a neutrino interacting directly with the electron. Here, the neutrino simply strikes the electron and scatters away from it, transferring some of the neutrino's momentum to the electron. Since (i) neither neutrinos nor electrons are affected by the strong force, (ii) neutrinos are electrically neutral (therefore don't interact electromagnetically), and (iii) the incredibly small masses of these particles make any gravitational force between them negligible, such an interaction can only happen via the weak force. Since such an electron is not created from a nucleon, and is unchanged except for the new force impulse imparted by the neutrino, this weak force interaction between the neutrino and the electron must be mediated by a weak-force boson particle with no charge. Thus, this interaction requires a Z boson.
Basic properties.
These bosons are among the heavyweights of the elementary particles. With masses of and , respectively, the W and Z bosons are almost 100 times as large as the proton – heavier, even, than entire atoms of iron. The masses of these bosons are significant because they act as the force carriers of a quite short-range fundamental force: their high masses thus limit the range of the weak nuclear force. By way of contrast, the electromagnetic force has an infinite range, because its force carrier, the photon, has zero mass, and the same is supposed of the hypothetical graviton.
All three bosons have particle spin "s" = 1. The emission of a or boson either raises or lowers the electric charge of the emitting particle by one unit, and also alters the spin by one unit. At the same time, the emission or absorption of a W boson can change the type of the particle – for example changing a strange quark into an up quark. The neutral Z boson cannot change the electric charge of any particle, nor can it change any other of the so-called "charges" (such as strangeness, baryon number, charm, etc.). The emission or absorption of a Z boson can only change the spin, momentum, and energy of the other particle. (See also "weak neutral current".)
Weak nuclear force.
The W and Z bosons are carrier particles that mediate the weak nuclear force, much as the photon is the carrier particle for the electromagnetic force.
W bosons.
The W bosons are best known for their role in nuclear decay. Consider, for example, the beta decay of cobalt-60.
This reaction does not involve the whole cobalt-60 nucleus, but affects only one of its 33 neutrons. The neutron is converted into a proton while also emitting an electron (called a beta particle in this context) and an electron antineutrino:
Again, the neutron is not an elementary particle but a composite of an up quark and two down quarks (udd). It is in fact one of the down quarks that interacts in beta decay, turning into an up quark to form a proton (uud). At the most fundamental level, then, the weak force changes the flavour of a single quark:
which is immediately followed by decay of the itself:
Z boson.
The Z boson is its own antiparticle. Thus, all of its flavour quantum numbers and charges are zero. The exchange of a Z boson between particles, called a neutral current interaction, therefore leaves the interacting particles unaffected, except for a transfer of momentum. boson interactions involving neutrinos have distinctive signatures: They provide the only known mechanism for elastic scattering of neutrinos in matter; neutrinos are almost as likely to scatter elastically (via Z boson exchange) as inelastically (via W boson exchange). The first prediction of Z bosons was made by Brazilian physicist José Leite Lopes in 1958, by devising an equation which showed the analogy of the weak nuclear interactions with electromagnetism. Steve Weinberg, Sheldon Glashow and Abdus Salam used later these results to develop the electroweak unification, in 1973. Weak neutral currents via Z boson exchange were confirmed shortly thereafter in 1974, in a neutrino experiment in the Gargamelle bubble chamber at CERN.
Predicting the W and Z.
Following the spectacular success of quantum electrodynamics in the 1950s, attempts were undertaken to formulate a similar theory of the weak nuclear force. This culminated around 1968 in a unified theory of electromagnetism and weak interactions by Sheldon Glashow, Steven Weinberg, and Abdus Salam, for which they shared the 1979 Nobel Prize in Physics. Their electroweak theory postulated not only the W bosons necessary to explain beta decay, but also a new Z boson that had never been observed.
The fact that the W and Z bosons have mass while photons are massless was a major obstacle in developing electroweak theory. These particles are accurately described by an SU(2) gauge theory, but the bosons in a gauge theory must be massless. As a case in point, the photon is massless because electromagnetism is described by a U(1) gauge theory. Some mechanism is required to break the SU(2) symmetry, giving mass to the W and Z in the process. One explanation, the Higgs mechanism, was forwarded by the 1964 PRL symmetry breaking papers. It predicts the existence of yet another new particle; the Higgs boson. Of the four components of a Goldstone boson created by the Higgs field, three are "eaten" by the W+, Z0, and W− bosons to form their longitudinal components and the remainder appears as the spin 0 Higgs boson.
The combination of the SU(2) gauge theory of the weak interaction, the electromagnetic interaction, and the Higgs mechanism is known as the Glashow-Weinberg-Salam model. These days it is widely accepted as one of the pillars of the Standard Model of particle physics. As of 13 December 2011, intensive search for the Higgs boson carried out at CERN has indicated that if the particle is to be found, it seems likely to be found around 125 GeV. On 4 July 2012, the CMS and the ATLAS experimental collaborations at CERN announced the discovery of a new particle with a mass of 125.3 ± 0.6 GeV that appears consistent with a Higgs boson.
Discovery.
Unlike beta decay, the observation of neutral current interactions that involve particles "other than neutrinos" requires huge investments in particle accelerators and detectors, such as are available in only a few high-energy physics laboratories in the world (and then only after 1983). This is because Z-bosons behave in somewhat the same manner as photons, but do not become important until the energy of the interaction is comparable with the relatively huge mass of the Z boson.
The discovery of the W and Z bosons was considered a major success for CERN. First, in 1973, came the observation of neutral current interactions as predicted by electroweak theory. The huge Gargamelle bubble chamber photographed the tracks of a few electrons suddenly starting to move, seemingly of their own accord. This is interpreted as a neutrino interacting with the electron by the exchange of an unseen Z boson. The neutrino is otherwise undetectable, so the only observable effect is the momentum imparted to the electron by the interaction.
The discovery of the W and Z bosons themselves had to wait for the construction of a particle accelerator powerful enough to produce them. The first such machine that became available was the Super Proton Synchrotron, where unambiguous signals of W bosons were seen in January 1983 during a series of experiments made possible by Carlo Rubbia and Simon van der Meer. The actual experiments were called UA1 (led by Rubbia) and UA2 (led by Pierre Darriulat), and were the collaborative effort of many people. Van der Meer was the driving force on the accelerator end (stochastic cooling). UA1 and UA2 found the Z boson a few months later, in May 1983. Rubbia and van der Meer were promptly awarded the 1984 Nobel Prize in Physics, a most unusual step for the conservative Nobel Foundation.
The , , and bosons, together with the photon (), comprise the four gauge bosons of the electroweak interaction.
Decay.
The W and Z bosons decay to fermion–antifermion pairs but neither the W nor the Z bosons can decay into the higher-mass top quark. Neglecting phase space effects and higher order corrections, simple estimates of their branching fractions can be calculated from the coupling constants.
W bosons.
W bosons can decay to a lepton and neutrino or to an up-type quark and a down-type quark. The decay width of the W boson to a quark–antiquark pair is proportional to the corresponding squared CKM matrix element and the number of quark colours, "N"C = 3. The decay widths for the W bosons are then proportional to:
Here, , , denote the three flavours of leptons (more exactly, the positive charged antileptons). , , denote the three flavours of neutrinos. The other particles, starting with and , all denote quarks and antiquarks (factor "N"C is applied). The various "V""ij" denote the corresponding CKM matrix coefficients.
Unitarity of the CKM matrix implies that 
Z bosons.
Z bosons decay into a fermion and its antiparticle. As the Z-boson is a mixture of the pre-symmetry-breaking W0 and B0 bosons (see weak mixing angle), each vertex factor includes a factor "T"3 − "Qsin2"θ"W", where "T"3 is the third component of the weak isospin of the fermion, "Q "is the electric charge of the fermion (in units of the elementary charge), and "θ"W is the weak mixing angle. Because the weak isospin is different for fermions of different chirality, either left-handed or right-handed, the coupling is different as well.
The relative strengths of each coupling can be estimated by considering that the decay rates include the square of these factors, and all possible diagrams (e.g. sum over quark families, and left and right contributions). This is just an estimate, as we are considering only tree-level diagrams in the Fermi theory.
Here, L and R denote the left- and right-handed chiralities of the fermions respectively. (The right-handed neutrinos do not exist in the standard model. However, in some extensions beyond the standard model they do.) The notation "x" = "sin2"θ"W" is used.

</doc>
<doc id="33516" url="https://en.wikipedia.org/wiki?curid=33516" title="Wave">
Wave

In physics, a wave is an oscillation accompanied by a transfer of energy that travels through space or mass. Frequency refers to the addition of time. Wave motion transfers energy from one point to another, which may or may not displace particles of the transmission medium—that is, with little or no associated mass transport. Waves consist, instead, of oscillations or vibrations (of a physical quantity), around almost fixed locations.
There are two main types of waves. Mechanical waves propagate through a medium, and the substance of this medium is deformed. The deformation reverses itself owing to restoring forces resulting from its deformation. For example, sound waves propagate via air molecules colliding with their neighbors. When air molecules collide, they also bounce away from each other (a restoring force). This keeps the molecules from continuing to travel in the direction of the wave.
The second main type of wave, electromagnetic waves, do not require a medium. Instead, they consist of periodic oscillations of electrical and magnetic fields generated by charged particles, and can therefore travel through a vacuum. These types of waves vary in wavelength, and include radio waves, microwaves, infrared radiation, visible light, ultraviolet radiation, X-rays, and gamma rays.
Waves are described by a wave equation which sets out how the disturbance proceeds over time. The mathematical form of this equation varies depending on the type of wave. Further, the behavior of particles in quantum mechanics are described by waves. In addition, gravitational waves also travel through space, which are a result of a vibration or movement in gravitational fields.
A wave can be transverse or longitudinal. Transverse waves occur when a disturbance creates oscillations that are perpendicular to the propagation of energy transfer. Longitudinal waves occur when the oscillations are parallel to the direction of energy propagation. While mechanical waves can be both transverse and longitudinal, all electromagnetic waves are transverse in free space.
General features.
A single, all-encompassing definition for the term "wave" is not straightforward. A vibration can be defined as a "back-and-forth" motion around a reference value. However, a vibration is not necessarily a wave. An attempt to define the necessary and sufficient characteristics that qualify a phenomenon to be called a "wave" results in a fuzzy border line.
The term "wave" is often intuitively understood as referring to a transport of spatial disturbances that are generally not accompanied by a motion of the medium occupying this space as a whole. In a wave, the energy of a vibration is moving away from the source in the form of a disturbance within the surrounding medium . However, this motion is problematic for a standing wave (for example, a wave on a string), where energy is moving in both directions equally, or for electromagnetic (e.g., light) waves in a [vacuum], where the concept of medium does not apply and interaction with a target is the key to wave detection and practical applications. There are [water waves] on the ocean surface; gamma waves and light waves emitted by the Sun; microwaves used in microwave ovens and in [radar] equipment; [radio waves] broadcast by radio stations; and sound waves generated by radio receivers, telephone handsets and living creatures (as voices), to mention only a few wave phenomena.
It may appear that the description of waves is closely related to their physical origin for each specific instance of a wave process. For example, acoustics is distinguished from optics in that sound waves are related to a mechanical rather than an electromagnetic wave transfer caused by vibration. Concepts such as mass, momentum, inertia, or elasticity, become therefore crucial in describing acoustic (as distinct from optic) wave processes. This difference in origin introduces certain wave characteristics particular to the properties of the medium involved. For example, in the case of air: vortices, radiation pressure, shock waves etc.; in the case of solids: Rayleigh waves, dispersion; and so on.
Other properties, however, although usually described in terms of origin, may be generalized to all waves. For such reasons, wave theory represents a particular branch of physics that is concerned with the properties of wave processes independently of their physical origin. For example, based on the mechanical origin of acoustic waves, a moving disturbance in space–time can exist if and only if the medium involved is neither infinitely stiff nor infinitely pliable. If all the parts making up a medium were rigidly "bound", then they would all vibrate as one, with no delay in the transmission of the vibration and therefore no wave motion. On the other hand, if all the parts were independent, then there would not be any transmission of the vibration and again, no wave motion. Although the above statements are meaningless in the case of waves that do not require a medium, they reveal a characteristic that is relevant to all waves regardless of origin: within a wave, the phase of a vibration (that is, its position within the vibration cycle) is different for adjacent points in space because the vibration reaches these points at different times.
Mathematical description of one-dimensional waves.
Wave equation.
Consider a traveling transverse wave (which may be a pulse) on a string (the medium). Consider the string to have a single spatial dimension. Consider this wave as traveling
This wave can then be described by the two-dimensional functions
or, more generally, by d'Alembert's formula:
representing two component waveforms formula_8 and formula_10 traveling through the medium in opposite directions. A generalized representation of this wave can be obtained as the partial differential equation
General solutions are based upon Duhamel's principle.
Wave forms.
The form or shape of "F" in d'Alembert's formula involves the argument "x − vt". Constant values of this argument correspond to constant values of "F", and these constant values occur if "x" increases at the same rate that "vt" increases. That is, the wave shaped like the function "F" will move in the positive "x"-direction at velocity "v" (and "G" will propagate at the same speed in the negative "x"-direction).
In the case of a periodic function "F" with period "λ", that is, "F"("x + λ" − "vt") = "F"("x " − "vt"), the periodicity of "F" in space means that a snapshot of the wave at a given time "t" finds the wave varying periodically in space with period "λ" (the wavelength of the wave). In a similar fashion, this periodicity of "F" implies a periodicity in time as well: "F"("x" − "v(t + T)") = "F"("x " − "vt") provided "vT" = "λ", so an observation of the wave at a fixed location "x" finds the wave undulating periodically in time with period "T = λ"/"v".
Amplitude and modulation.
The amplitude of a wave may be constant (in which case the wave is a "c.w." or "continuous wave"), or may be "modulated" so as to vary with time and/or position. The outline of the variation in amplitude is called the "envelope" of the wave. Mathematically, the modulated wave can be written in the form:
where formula_16 is the amplitude envelope of the wave, formula_17 is the "wavenumber" and formula_18 is the "phase". If the group velocity formula_19 (see below) is wavelength-independent, this equation can be simplified as:
showing that the envelope moves with the group velocity and retains its shape. Otherwise, in cases where the group velocity varies with wavelength, the pulse shape changes in a manner often described using an "envelope equation".
Phase velocity and group velocity.
There are two velocities that are associated with waves, the phase velocity and the group velocity. To understand them, one must consider several types of waveform. For simplification, examination is restricted to one dimension.
The most basic wave (a form of plane wave) may be expressed in the form:
which can be related to the usual sine and cosine forms using Euler's formula. Rewriting the argument, formula_22, makes clear that this expression describes a vibration of wavelength formula_23 traveling in the "x"-direction with a constant "phase velocity" formula_24.
The other type of wave to be considered is one with localized structure described by an envelope, which may be expressed mathematically as, for example:
where now "A(k"1")" (the integral is the inverse Fourier transform of A(k1)) is a function exhibiting a sharp peak in a region of wave vectors Δ"k" surrounding the point "k"1 = "k". In exponential form:
with "A"o the magnitude of "A". For example, a common choice for "A"o is a Gaussian wave packet:
where σ determines the spread of "k"1-values about "k", and "N" is the amplitude of the wave.
The exponential function inside the integral for ψ oscillates rapidly with its argument, say φ("k"1), and where it varies rapidly, the exponentials cancel each other out, interfere destructively, contributing little to ψ. However, an exception occurs at the location where the argument φ of the exponential varies slowly. (This observation is the basis for the method of stationary phase for evaluation of such integrals.) The condition for φ to vary slowly is that its rate of change with "k"1 be small; this rate of variation is:
where the evaluation is made at "k"1 = "k" because "A(k"1")" is centered there. This result shows that the position "x" where the phase changes slowly, the position where ψ is appreciable, moves with time at a speed called the "group velocity":
The group velocity therefore depends upon the dispersion relation connecting ω and "k". For example, in quantum mechanics the energy of a particle represented as a wave packet is "E" = ħω = (ħ"k")2/(2"m"). Consequently, for that wave situation, the group velocity is
showing that the velocity of a localized particle in quantum mechanics is its group velocity. Because the group velocity varies with "k", the shape of the wave packet broadens with time, and the particle becomes less localized. In other words, the velocity of the constituent waves of the wave packet travel at a rate that varies with their wavelength, so some move faster than others, and they cannot maintain the same interference pattern as the wave propagates.
Sinusoidal waves.
Mathematically, the most basic wave is the (spatially) one-dimensional sine wave (or "harmonic wave" or "sinusoid") with an amplitude formula_4 described by the equation:
where
The units of the amplitude depend on the type of wave. Transverse mechanical waves (e.g., a wave on a string) have an amplitude expressed as a distance (e.g., meters), longitudinal mechanical waves (e.g., sound waves) use units of pressure (e.g., pascals), and electromagnetic waves (a form of transverse vacuum wave) express the amplitude in terms of its electric field (e.g., volts/meter).
The wavelength formula_39 is the distance between two sequential crests or troughs (or other equivalent points), generally is measured in meters. A wavenumber formula_17, the spatial frequency of the wave in radians per unit distance (typically per meter), can be associated with the wavelength by the relation
The period formula_42 is the time for one complete cycle of an oscillation of a wave. The frequency formula_43 is the number of periods per unit time (per second) and is typically measured in hertz. These are related by:
In other words, the frequency and period of a wave are reciprocals.
The angular frequency formula_37 represents the frequency in radians per second. It is related to the frequency or period by
The wavelength formula_39 of a sinusoidal waveform traveling at constant speed formula_5 is given by:
where formula_5 is called the phase speed (magnitude of the phase velocity) of the wave and formula_43 is the wave's frequency.
Wavelength can be a useful concept even if the wave is not periodic in space. For example, in an ocean wave approaching shore, the incoming wave undulates with a varying "local" wavelength that depends in part on the depth of the sea floor compared to the wave height. The analysis of the wave can be based upon comparison of the local wavelength with the local water depth.
Although arbitrary wave shapes will propagate unchanged in lossless linear time-invariant systems, in the presence of dispersion the sine wave is the unique shape that will propagate unchanged but for phase and amplitude, making it easy to analyze. Due to the Kramers–Kronig relations, a linear medium with dispersion also exhibits loss, so the sine wave propagating in a dispersive medium is attenuated in certain frequency ranges that depend upon the medium.
The sine function is periodic, so the sine wave or sinusoid has a wavelength in space and a period in time.
The sinusoid is defined for all times and distances, whereas in physical situations we usually deal with waves that exist for a limited span in space and duration in time. Fortunately, an arbitrary wave shape can be decomposed into an infinite set of sinusoidal waves by the use of Fourier analysis. As a result, the simple case of a single sinusoidal wave can be applied to more general cases. In particular, many media are linear, or nearly so, so the calculation of arbitrary wave behavior can be found by adding up responses to individual sinusoidal waves using the superposition principle to find the solution for a general waveform. When a medium is nonlinear, the response to complex waves cannot be determined from a sine-wave decomposition.
Standing waves.
A standing wave, also known as a "stationary wave", is a wave that remains in a constant position. This phenomenon can occur because the medium is moving in the opposite direction to the wave, or it can arise in a stationary medium as a result of interference between two waves traveling in opposite directions.
The "sum" of two counter-propagating waves (of equal amplitude and frequency) creates a "standing wave". Standing waves commonly arise when a boundary blocks further propagation of the wave, thus causing wave reflection, and therefore introducing a counter-propagating wave. For example, when a violin string is displaced, transverse waves propagate out to where the string is held in place at the bridge and the nut, where the waves are reflected back. At the bridge and nut, the two opposed waves are in antiphase and cancel each other, producing a node. Halfway between two nodes there is an antinode, where the two counter-propagating waves "enhance" each other maximally. There is no net propagation of energy over time.
Physical properties.
Waves exhibit common behaviors under a number of standard situations, e. g.
Transmission and media.
Waves normally move in a straight line (i.e. rectilinearly) through a "transmission medium". Such media can be classified into one or more of the following categories:
Absorption.
Absorption of waves mean, if a kind of wave strikes a matter, it will be absorbed by the matter. When a wave with that same natural frequency impinges upon an atom, then the electrons of that atom will be set into vibrational motion. If a wave of a given frequency strikes a material with electrons having the same vibrational frequencies, then those electrons will absorb the energy of the wave and transform it into vibrational motion.
Reflection.
When a wave strikes a reflective surface, it changes direction, such that the angle made by the incident wave and line normal to the surface equals the angle made by the reflected wave and the same normal line.
Interference.
Waves that encounter each other combine through superposition to create a new wave called an interference pattern. Important interference patterns occur for waves that are in phase.
Refraction.
Refraction is the phenomenon of a wave changing its speed. Mathematically, this means that the size of the phase velocity changes. Typically, refraction occurs when a wave passes from one medium into another. The amount by which a wave is refracted by a material is given by the refractive index of the material. The directions of incidence and refraction are related to the refractive indices of the two materials by Snell's law.
Diffraction.
A wave exhibits diffraction when it encounters an obstacle that bends the wave or when it spreads after emerging from an opening. Diffraction effects are more pronounced when the size of the obstacle or opening is comparable to the wavelength of the wave.
Polarization.
A wave is polarized if it oscillates in one direction or plane. A wave can be polarized by the use of a polarizing filter. The polarization of a transverse wave describes the direction of oscillation in the plane perpendicular to the direction of travel.
Longitudinal waves such as sound waves do not exhibit polarization. For these waves the direction of oscillation is along the direction of travel.
Dispersion.
A wave undergoes dispersion when either the phase velocity or the group velocity depends on the wave frequency.
Dispersion is most easily seen by letting white light pass through a prism, the result of which is to produce the spectrum of colours of the rainbow. Isaac Newton performed experiments with light and prisms, presenting his findings in the "Opticks" (1704) that white light consists of several colours and that these colours cannot be decomposed any further.
Mechanical waves.
Waves on strings.
The speed of a transverse wave traveling along a vibrating string (" v ") is directly proportional to the square root of the tension of the string (" T ") over the linear mass density (" μ "):
where the linear density "μ" is the mass per unit length of the string.
Acoustic waves.
Acoustic or sound waves travel at speed given by
or the square root of the adiabatic bulk modulus divided by the ambient fluid density (see speed of sound).
Electromagnetic waves.
An electromagnetic wave consists of two waves that are oscillations of the electric and magnetic fields. An electromagnetic wave travels in a direction that is at right angles to the oscillation direction of both fields. In the 19th century, James Clerk Maxwell showed that, in vacuum, the electric and magnetic fields satisfy the wave equation both with speed equal to that of the speed of light. From this emerged the idea that light is an electromagnetic wave. Electromagnetic waves can have different frequencies (and thus wavelengths), giving rise to various types of radiation such as radio waves, microwaves, infrared, visible light, ultraviolet and X-rays.
Quantum mechanical waves.
The Schrödinger equation describes the wave-like behavior of particles in quantum mechanics. Solutions of this equation are wave functions which can be used to describe the probability density of a particle.
de Broglie waves.
Louis de Broglie postulated that all particles with momentum have a wavelength
where "h" is Planck's constant, and "p" is the magnitude of the momentum of the particle. This hypothesis was at the basis of quantum mechanics. Nowadays, this wavelength is called the de Broglie wavelength. For example, the electrons in a CRT display have a de Broglie wavelength of about 10−13 m.
A wave representing such a particle traveling in the "k"-direction is expressed by the wave function as follows:
where the wavelength is determined by the wave vector k as:
and the momentum by:
However, a wave like this with definite wavelength is not localized in space, and so cannot represent a particle localized in space. To localize a particle, de Broglie proposed a superposition of different wavelengths ranging around a central value in a wave packet, a waveform often used in quantum mechanics to describe the wave function of a particle. In a wave packet, the wavelength of the particle is not precise, and the local wavelength deviates on either side of the main wavelength value.
In representing the wave function of a localized particle, the wave packet is often taken to have a Gaussian shape and is called a "Gaussian wave packet". Gaussian wave packets also are used to analyze water waves.
For example, a Gaussian wavefunction ψ might take the form:
at some initial time "t" = 0, where the central wavelength is related to the central wave vector "k"0 as λ0 = 2π / "k"0. It is well known from the theory of Fourier analysis, or from the Heisenberg uncertainty principle (in the case of quantum mechanics) that a narrow range of wavelengths is necessary to produce a localized wave packet, and the more localized the envelope, the larger the spread in required wavelengths. The Fourier transform of a Gaussian is itself a Gaussian. Given the Gaussian:
the Fourier transform is:
The Gaussian in space therefore is made up of waves:
that is, a number of waves of wavelengths λ such that "k"λ = 2 π.
The parameter σ decides the spatial spread of the Gaussian along the "x"-axis, while the Fourier transform shows a spread in wave vector "k" determined by 1/σ. That is, the smaller the extent in space, the larger the extent in "k", and hence in λ = 2π/"k".
Gravity waves.
Gravity waves are waves generated in a fluid medium or at the interface between two media when the force of gravity or buoyancy tries to restore equilibrium. A ripple on a pond is one example.
Gravitational waves.
Researchers believe that gravitational waves also travel through space, although gravitational waves have never been directly detected.
Gravitational waves are disturbances in the curvature of spacetime, predicted by Einstein's theory of general relativity.
WKB method.
In a nonuniform medium, in which the wavenumber "k" can depend on the location as well as the frequency, the phase term "kx" is typically replaced by the integral of "k"("x")"dx", according to the WKB method. Such nonuniform traveling waves are common in many physical problems, including the mechanics of the cochlea and waves on hanging ropes.

</doc>
<doc id="33691" url="https://en.wikipedia.org/wiki?curid=33691" title="Wave equation">
Wave equation

The wave equation is an important second-order linear partial differential equation for the description of waves—as they occur in physics—such as sound waves, light waves and water waves. It arises in fields like acoustics, electromagnetics, and fluid dynamics.
Historically, the problem of a vibrating string such as that of a musical instrument was studied by Jean le Rond d'Alembert, Leonhard Euler, Daniel Bernoulli, and Joseph-Louis Lagrange. In 1746, d’Alembert discovered the one-dimensional wave equation, and within ten years Euler discovered the three-dimensional wave equation.
Introduction.
The wave equation is a hyperbolic partial differential equation. It typically concerns a time variable , one or more spatial variables , and a scalar function , whose values could model, for example, the mechanical displacement of a wave. The wave equation for is
where ∇2 is the (spatial) Laplacian and "c" is a fixed constant.
Solutions of this equation describe propagation of disturbances out from the region at a fixed speed in one or in all spatial directions, as do physical waves from plane or localized sources; the constant "c" is identified with the propagation speed of the wave. This equation is linear. Therefore, the sum of any two solutions is again a solution: in physics this property is called the superposition principle.
The wave equation alone does not specify a physical solution; a unique solution is usually obtained by setting a problem with further conditions, such as initial conditions, which prescribe the amplitude and phase of the wave. Another important class of problems occurs in enclosed spaces specified by boundary conditions, for which the solutions represent standing waves, or harmonics, analogous to the harmonics of musical instruments.
The wave equation, and modifications of it, are also found in elasticity, quantum mechanics, plasma physics and general relativity.
Scalar wave equation in one space dimension.
The wave equation in one space dimension can be written as follows:
This equation is typically described as having only one space dimension "x", because the only other independent variable is the time "t". Nevertheless, the dependent variable "u" may represent a second space dimension, if, for example, the displacement "u" takes place in y-direction, as in the case of a string that is located in the x-y plane.
Derivation of the wave equation.
The wave equation in one space dimension can be derived in a variety of different physical settings. Most famously, it can be derived for the case of a string that is vibrating in a two-dimensional plane, with each of its elements being pulled in opposite directions by the force of tension.
Another physical setting for derivation of the wave equation in one space dimension utilizes Hooke's Law. In the theory of elasticity, Hooke's Law is an approximation for certain materials, stating that the amount by which a material body is deformed (the strain) is linearly related to the force causing the deformation (the stress).
From Hooke's law.
The wave equation in the one-dimensional case can be derived from Hooke's Law in the following way: Imagine an array of little weights of mass "m" interconnected with massless springs of length "h" . The springs have a spring constant of "k":
Here the dependent variable "u(x)" measures the distance from the equilibrium of the mass situated at "x", so that "u(x)" essentially measures the magnitude of a disturbance (i.e. strain) that is traveling in an elastic material. The forces exerted on the mass "m" at the location "x"+"h" are:
It is apparent that the solution at ("t","x","y") depends not only on the data on the light cone where
but also on data that are interior to that cone.
Scalar wave equation in general dimension and Kirchhoff's formulae.
We want to find solutions to "utt"−Δ"u" = 0 for "u" : R"n" × (0, ∞) → R with "u"("x", 0) = "g"("x") and "ut"("x", 0) = "h"("x"). See Evans for more details.
Odd dimensions.
Assume "n" ≥ 3 is an odd integer and "g" ∈ "C""m"+1(R"n"), "h" ∈ "Cm"(R"n") for "m" = ("n"+1)/2. Let formula_5 and let
then
Even dimensions.
Assume "n" ≥ 2 is an even integer and "g" ∈ "C""m"+1(R"n"), "h" ∈ "Cm"(R"n"), for "m" = ("n"+2)/2. Let formula_8 and let
then
Problems with boundaries.
One space dimension.
The Sturm-Liouville formulation.
A flexible string that is stretched between two points "x" = 0 and "x" = "L" satisfies the wave equation for "t" > 0 and 0 < "x" < "L". On the boundary points, "u" may satisfy a variety of boundary conditions. A general form that is appropriate for applications is
where "a" and "b" are non-negative. The case where u is required to vanish at an endpoint is the limit of this condition when the respective "a" or "b" approaches infinity. The method of separation of variables consists in looking for solutions of this problem in the special form
A consequence is that
The eigenvalue λ must be determined so that there is a non-trivial solution of the boundary-value problem
This is a special case of the general problem of Sturm–Liouville theory. If "a" and "b" are positive, the eigenvalues are all positive, and the solutions are trigonometric functions. A solution that satisfies square-integrable initial conditions for "u" and "ut" can be obtained from expansion of these functions in the appropriate trigonometric series.
Investigation by numerical methods.
Approximating the continuous string with a finite number of equidistant mass points one gets the following physical model:
If each mass point has the mass "m", the tension of the string is "f", the separation between the mass points is Δ"x" and "ui", "i" = 1, ..., "n" are the offset of these "n" points from their equilibrium points (i.e. their position on a straight line between the two attachment points of the string) the vertical component of the force towards point "i"+1 is
and the vertical component of the force towards point "i"−1 is
Taking the sum of these two forces and dividing with the mass "m" one gets for the vertical motion:
As the mass density is 
this can be written
The wave equation is obtained by letting Δ"x" → 0 in which case "ui"("t") takes the form "u"("x", "t") where "u"("x", "t") is continuous function of two variables, formula_18 takes the form formula_19 and
If the string is approximated with 100 discrete mass points one gets the 100 coupled second order differential equations (), () and () or equivalently 200 coupled first order differential equations.
Propagating these up to the times
using an 8th order multistep method the 6 states displayed in figure 2 are found:
The red curve is the initial state at time zero at which the string is "let free" in a predefined shape with all formula_22. The blue curve is the state at time formula_23, i.e. after a time that corresponds to the time a wave that is moving with the nominal wave velocity formula_24 would need for one fourth of the length of the string.
Figure 3 displays the shape of the string at the times formula_25. The wave travels in direction right with the speed formula_24 without being actively constraint by the boundary conditions at the two extremes of the string. The shape of the wave is constant, i.e. the curve is indeed of the form "f"("x"−"ct").
Figure 4 displays the shape of the string at the times formula_27. The constraint on the right extreme starts to interfere with the motion preventing the wave to raise the end of the string.
Figure 5 displays the shape of the string at the times formula_28 when the direction of motion is reversed. The red, green and blue curves are the states at the times formula_29 while the 3 black curves correspond to the states at times formula_30 with the wave starting to move back towards left.
Figure 6 and figure 7 finally display the shape of the string at the times formula_31 and formula_32. The wave now travels towards left and the constraints at the end points are not active any more. When finally the other extreme of the string the direction will again be reversed in a way similar to what is displayed in figure 6
Several space dimensions.
The one-dimensional initial-boundary value theory may be extended to an arbitrary number of space dimensions. Consider a domain "D" in "m"-dimensional "x" space, with boundary "B". Then the wave equation is to be satisfied if "x" is in "D" and "t" > 0. On the boundary of "D", the solution "u" shall satisfy
where "n" is the unit outward normal to "B", and "a" is a non-negative function defined on "B". The case where "u" vanishes on "B" is a limiting case for "a" approaching infinity. The initial conditions are
where "f" and "g" are defined in "D". This problem may be solved by expanding "f" and "g" in the eigenfunctions of the Laplacian in "D", which satisfy the boundary conditions. Thus the eigenfunction "v" satisfies
in "D", and
on "B".
In the case of two space dimensions, the eigenfunctions may be interpreted as the modes of vibration of a drumhead stretched over the boundary "B". If "B" is a circle, then these eigenfunctions have an angular component that is a trigonometric function of the polar angle θ, multiplied by a Bessel function (of integer order) of the radial component. Further details are in Helmholtz equation.
If the boundary is a sphere in three space dimensions, the angular components of the eigenfunctions are spherical harmonics, and the radial components are Bessel functions of half-integer order.
Inhomogeneous wave equation in one dimension.
The inhomogeneous wave equation in one dimension is the following:
with initial conditions given by
The function "s"("x", "t") is often called the source function because in practice it describes the effects of the sources of waves on the medium carrying them. Physical examples of source functions include the force driving a wave on a string, or the charge or current density in the Lorenz gauge of electromagnetism.
One method to solve the initial value problem (with the initial values as posed above) is to take advantage of a special property of the wave equation in an odd number of space dimensions, namely that its solutions respect causality. That is, for any point ("xi", "ti"), the value of "u"("xi", "ti") depends only on the values of "f"("xi"+"cti") and "f"("xi"−"cti") and the values of the function "g"("x") between ("xi"−"cti") and ("xi"+"cti"). This can be seen in d'Alembert's formula, stated above, where these quantities are the only ones that show up in it. Physically, if the maximum propagation speed is "c", then no part of the wave that can't propagate to a given point by a given time can affect the amplitude at the same point and time.
In terms of finding a solution, this causality property means that for any given point on the line being considered, the only area that needs to be considered is the area encompassing all the points that could causally affect the point being considered. Denote the area that casually affects point ("xi", "ti") as "RC". Suppose we integrate the inhomogeneous wave equation over this region.
To simplify this greatly, we can use Green's theorem to simplify the left side to get the following:
The left side is now the sum of three line integrals along the bounds of the causality region. These turn out to be fairly easy to compute
In the above, the term to be integrated with respect to time disappears because the time interval involved is zero, thus "dt" = 0.
For the other two sides of the region, it is worth noting that "x"±"ct" is a constant, namingly "xi"±"cti", where the sign is chosen appropriately. Using this, we can get the relation "dx"±"cdt" = 0, again choosing the right sign:
And similarly for the final boundary segment:
Adding the three results together and putting them back in the original integral:
Solving for "u"("xi", "ti") we arrive at
In the last equation of the sequence, the bounds of the integral over the source function have been made explicit. Looking at this solution, which is valid for all choices ("xi", "ti") compatible with the wave equation, it is clear that the first two terms are simply d'Alembert's formula, as stated above as the solution of the homogeneous wave equation in one dimension. The difference is in the third term, the integral over the source.
Other coordinate systems.
In three dimensions, the wave equation, when written in elliptic cylindrical coordinates, may be solved by separation of variables, leading to the Mathieu differential equation.
Further generalizations.
Elastic waves.
The elastic wave equation in three dimensions describes the propagation of waves in an isotropic homogeneous elastic medium. Most solid materials are elastic, so this equation describes such phenomena as seismic waves in the Earth and ultrasonic waves used to detect flaws in materials. While linear, this equation has a more complex form than the equations given above, as it must account for both longitudinal and transverse motion:
where:
Note that in this equation, both force and displacement are vector quantities. Thus, this equation is sometimes known as the vector wave equation.
As an aid to understanding, the reader will observe that if f and ∇⋅u are set to zero, this becomes (effectively) Maxwell's equation for the propagation of the electric field E, which has only transverse waves.
Nonlinear dispersion relation.
In dispersive wave phenomena, the speed of wave propagation varies with the wavelength of the wave, which is reflected by a dispersion relation
where "ω" is the angular frequency and k is the wavevector describing plane wave solutions. For light waves, the dispersion relation is "ω" = ±"c" |k|, but in general, the constant speed "c" gets replaced by a variable phase velocity:

</doc>
<doc id="145343" url="https://en.wikipedia.org/wiki?curid=145343" title="Wave function">
Wave function

A wave function in quantum mechanics describes the quantum state of an isolated system of one or more particles. There is "one" wave function containing all the information about the entire system, not a separate wave function for each particle in the system. Its interpretation is that of a probability amplitude. Quantities associated with measurements, such as the average momentum of a particle, can be derived from the wave function. It is a central entity in quantum mechanics and is important in all modern theories, like quantum field theory incorporating quantum mechanics, while its interpretation may differ. The most common symbols for a wave function are the Greek letters or (lower-case and capital psi).
For a given system, once a representation corresponding to a maximal set of commuting observables and a suitable coordinate system is chosen, the wave function is a complex-valued function of the system's degrees of freedom corresponding to the chosen representation and coordinate system, continuous as well as discrete. Such a set of observables, by a postulate of quantum mechanics, are Hermitian linear operators on the space of states representing a set of physical observables, like position, momentum and spin that can, in principle, be simultaneously measured with arbitrary precision. Wave functions can be added together and multiplied by complex numbers to form new wave functions, and hence are elements of a vector space. This is the superposition principle of quantum mechanics. This vector space is endowed with an inner product such that it is a complete metric topological space with respect to the metric induced by the inner product. In this way the set of wave functions for a system form a function space that is a Hilbert space. The inner product is a measure of the overlap between physical states and is used in the foundational probabilistic interpretation of quantum mechanics, the Born rule, relating transition probabilities to inner products. The actual space depends on the system's degrees of freedom (hence on the chosen representation and coordinate system) and the exact form of the Hamiltonian entering the equation governing the dynamical behavior. In the non-relativistic case, disregarding spin, this is the Schrödinger equation.
The Schrödinger equation determines the allowed wave functions for the system and how they evolve over time. A wave function behaves qualitatively like other waves, such as water waves or waves on a string, because the Schrödinger equation is mathematically a type of wave equation. This explains the name "wave function", and gives rise to wave–particle duality. The wave of the wave function, however, is not a wave in physical space; it is a wave in an abstract mathematical "space", and in this respect it differs fundamentally from water waves or waves on a string.
For a given system, the choice of which relevant degrees of freedom to use are not unique, and correspondingly the domain of the wave function is not unique. It may be taken to be a function of all the position coordinates of the particles over "position space", or the momenta of all the particles over "momentum space", the two are related by a Fourier transform. These descriptions are the most important, but they are not the only possibilities. Just like in classical mechanics, canonical transformations may be used in the description of a quantum system. Some particles, like electrons and photons, have nonzero spin, and the wave function must include this fundamental property as an intrinsic discrete degree of freedom. In general, for a particle with "half-integer" spin the wave function is a spinor, for a particle with "integer" spin the wave function is a tensor. Particles with spin zero are called scalar particles, those with spin 1 vector particles, and more generally for higher integer spin, tensor particles. The terminology derives from how the wave functions transform under a rotation of the coordinate system. No "elementary" particle with spin or higher is known, except for the hypothesized spin graviton. Other discrete variables can be included, such as isospin. When a system has internal degrees of freedom, the wave function at each point in the continuous degrees of freedom (e.g. a point in space) assigns a complex number for "each" possible value of the discrete degrees of freedom (e.g. z-component of spin). These values are often displayed in a column matrix (e.g. a column vector for a non-relativistic electron with spin ).
In the Copenhagen interpretation, an interpretation of quantum mechanics, the squared modulus of the wave function, , is a real number interpreted as the probability density of measuring a particle as being at a given place at a given time or having a definite momentum, and possibly having definite values for discrete degrees of freedom. The integral of this quantity, over all the system's degrees of freedom, must be 1 in accordance with the probability interpretation, this general requirement a wave function must satisfy is called the "normalization condition". Since the wave function is complex valued, only its relative phase and relative magnitude can be measured. Its value does not in isolation tell anything about the magnitudes or directions of measurable observables; one has to apply quantum operators, whose eigenvalues correspond to sets of possible results of measurements, to the wave function and calculate the statistical distributions for measurable quantities.
The unit of measurement for depends on the system, and can be found by dimensional analysis of the normalization condition for the system. For one particle in three dimensions, its units are [length]−3/2, because an integral of over a region of three-dimensional space is a dimensionless probability.
Historical background.
In 1905 Einstein postulated the proportionality between the frequency of a photon and its energy, ,
and in 1916 the corresponding relation between photon momentum and wavelength, . In 1923, De Broglie was the first to suggest that the relation , now called the De Broglie relation, holds for "massive" particles, the chief clue being Lorentz invariance, and this can be viewed as the starting point for the modern development of quantum mechanics. The equations represent wave–particle duality for both massless and massive particles.
In the 1920s and 1930s, quantum mechanics was developed using calculus and linear algebra. Those who used the techniques of calculus included Louis de Broglie, Erwin Schrödinger, and others, developing "wave mechanics". Those who applied the methods of linear algebra included Werner Heisenberg, Max Born, and others, developing "matrix mechanics". Schrödinger subsequently showed that the two approaches were equivalent.
In 1926, Schrödinger published the famous wave equation now named after him, indeed the Schrödinger equation, based on classical Conservation of energy using quantum operators and the de Broglie relations such that the solutions of the equation are the wave functions for the quantum system. However, no one was clear on how to "interpret it". At first, Schrödinger and others thought that wave functions represent particles that are spread out with most of the particle being where the wave function is large. This was shown to be incompatible with how elastic scattering of a wave packet representing a particle off a target appears; it spreads out in all directions. While a scattered particle may scatter in any direction, it does not break up and take off in all directions. In 1926, Born provided the perspective of probability amplitude. This relates calculations of quantum mechanics directly to probabilistic experimental observations.
It is accepted as part of the Copenhagen interpretation of quantum mechanics. There are many other interpretations of quantum mechanics. In 1927, Hartree and Fock made the first step in an attempt to solve the "N"-body wave function, and developed the "self-consistency cycle": an iterative algorithm to approximate the solution. Now it is also known as the Hartree–Fock method. The Slater determinant and permanent (of a matrix) was part of the method, provided by John C. Slater.
Schrödinger did encounter an equation for the wave function that satisfied relativistic energy conservation "before" he published the non-relativistic one, but discarded it as it predicted negative probabilities and negative energies. In 1927, Klein, Gordon and Fock also found it, but incorporated the electromagnetic interaction and proved that it was Lorentz invariant. De Broglie also arrived at the same equation in 1928. This relativistic wave equation is now most commonly known as the Klein–Gordon equation.
In 1927, Pauli phenomenologically found a non-relativistic equation to describe spin-1/2 particles in electromagnetic fields, now called the Pauli equation. Pauli found the wave function was not described by a single complex function of space and time, but needed two complex numbers, which respectively correspond to the spin +1/2 and −1/2 states of the fermion. Soon after in 1928, Dirac found an equation from the first successful unification of special relativity and quantum mechanics applied to the electron, now called the Dirac equation. In this, the wave function is a "spinor" represented by four complex-valued components: two for the electron and two for the electron's antiparticle, the positron. In the non-relativistic limit, the Dirac wave function resembles the Pauli wave function for the electron. Later, other relativistic wave equations were found.
Wave functions and wave equations in modern theories.
All these wave equations are of enduring importance. The Schrödinger equation and the Pauli equation are under many circumstances excellent approximations of the relativistic variants. They are considerably easier to solve in practical problems than the relativistic equations. The Klein-Gordon equation and the Dirac equation, while being relativistic, do not represent full reconciliation of quantum mechanics and special relativity. The branch of quantum mechanics where these equations are studied the same way as the Schrödinger equation, often called relativistic quantum mechanics, while very successful, has its limitations (see e.g. Lamb shift) and conceptual problems (see e.g. Dirac sea).
Relativity makes it inevitable that the number of particles in a system is not constant. For full reconciliation, quantum field theory is needed. In this theory, the wave equations and the wave functions have their place, but in a somewhat different guise. The main objects of interest are not the wave functions, but rather operators, so called "field operators" (or just fields where "operator" is understood) on the Hilbert space of states (to be described next section). It turns out that the original relativistic wave equations and their solutions are still needed to build the Hilbert space. Moreover, the "free fields operators", i.e. when interactions are assumed not to exist, turn out to (formally) satisfy the same equation as do the fields (wave functions) in many cases.
Thus the Klein-Gordon equation (spin ) and the Dirac equation (spin ) in this guise remain in the theory. Higher spin analogues include the Proca equation (spin ), Rarita–Schwinger equation (spin ), and, more generally, the Bargmann–Wigner equations. For "massless" free fields two examples are the free field Maxwell equation (spin ) and the free field Einstein equation (spin ) for the field operators. All of them are essentially a direct consequence of the requirement of Lorentz invariance. Their solutions must transform under Lorentz transformation in a prescribed way, i.e. under a particular representation of the Lorentz group and that together with few other reasonable demands, e.g. the "cluster decomposition principle", with implications for causality is enough to fix the equations.
It should be emphasized that this applies to free field equations; interactions are not included. It should also be noted that the equations and their solutions, though needed for the theories, are not the central objects of study.
Wave functions and function spaces.
The concept of Function spaces enters naturally in the discussion about wave functions. A function space is a set of functions, usually with some defining requirements on the functions, together with a topology on that set. The latter will sparsely be used here, it is only needed to obtain a precise definition of what it means for a subset of a function space to be closed. A wave function is an element of a function space partly characterized by the following concrete and abstract descriptions.
The first item says that the set of solutions to the Schrödinger equation is a vector space. The second item says that the set of allowable states is a vector space. This similarity is of course not accidental. Not all properties of the respective spaces have been given so far. There are also a distinctions between the spaces to keep in mind.
To make this concrete, in the figure to the right, the 19 sub-images are images of wave functions in position space (their norm squared). The wave functions each represent the abstract state characterized by the triple of quantum numbers , in the lower right of each image. These are the principal quantum number, the orbital angular momentum quantum number and the magnetic quantum number. Together with one spin-projection quantum number of the electron, this is a complete set of observables.
The figure can serve to illustrate some further properties of the function spaces of wave functions. 
These observations encapsulate the essence of the function spaces of which wave functions are elements. Mathematically, this is expressed (in one spatial dimension, disregarding here unimportant issues of normalization) for a particle with no internal degrees of freedom as
where is any "abstract" state, is an eigenfunction of the position operator representing a particle localized at , represents the inner product, is an eigenfunction of the momentum operator representing a particle with precise momentum , is the identity operator and the integrals (first and third) represent the completeness of momentum and position eigenstates, is the coordinate space wave function and is the wave function in momentum space. In Dirac notation, the above equation reads
The description is not yet complete. There is a further technical requirement on the function space, that of completeness, that allows one to take limits of sequences in the function space, and be ensured that, if the limit exists, it is an element of the function space. A complete inner product space is called a Hilbert space. The property of completeness is crucial in advanced treatments and applications of quantum mechanics. It will not be very important in the subsequent discussion of wave functions, and technical details and links may be found in footnotes like the one that follows. The space is a Hilbert space, with inner product presented later. The function space of the example of the figure is a subspace of . A subspace of a Hilbert space is a Hilbert space if it is closed. It is here that the topology of the function space enters into its description.
It is also important to note, in order to avoid confusion, that not all functions to be discussed are elements of some Hilbert space, say . The most glaring example is the set of functions . These are solutions of the Schrödinger equation for a free particle, but are not normalizable, hence not in . But they are nonetheless fundamental for the description. One can, using them, express functions that "are" normalizable using wave packets. They are, in a sense to be made precise later, a basis (but not a Hilbert space basis) in which wave functions of interest can be expressed. There is also the artifact "normalization to a delta function" that is frequently employed for notational convenience, see further down. The delta functions themselves aren't square integrable either.
Physical requirements.
The above description of the function space containing the wave functions is mostly mathematically motivated. The function spaces are, due to completeness, very "large" in a certain sense. Not all functions are realistic descriptions of any physical system. For instance, in the function space one can find the function that takes on the value for all rational numbers and for the irrationals in the interval . This "is" square integrable, but can hardly represent a physical state.
The following constraints on the wave function are sometimes explicitly formulated for the calculations and physical interpretation to make sense:
It is possible to relax these conditions somewhat for special purposes. If these requirements are not met, it is not possible to interpret the wave function as a probability amplitude.
This does not alter the structure of the Hilbert space that these particular wave functions inhabit, but it should be pointed out that the subspace of the square-integrable functions , which is a Hilbert space, satisfying the second requirement "is not closed" in , hence not a Hilbert space in itself. The functions that does not meet the requirements are still needed for both technical and practical reasons.
Definition (one spinless particle in 1d).
For now, consider the simple case of a single particle, without spin, in one spatial dimension. More general cases are discussed below.
Position-space wave function.
The state of such a particle is completely described by its wave function,
where is position and is time. This is a complex-valued function of two real variables and .
If interpreted as a probability amplitude, the square modulus of the wave function, the positive real number
is interpreted as the probability density that the particle is at . The asterisk indicates the complex conjugate. If the particle's position is measured, its location cannot be determined from the wave function, but is described by a probability distribution. The probability that its position will be in the interval is the integral of the density over this interval:
where is the time at which the particle was measured. This leads to the normalization condition:
because if the particle is measured, there is 100% probability that it will be "somewhere".
Since the Schrödinger equation is linear, if any number of wave functions for are solutions of the equation, then so is their sum, and their scalar multiples by complex numbers . Taking scalar multiplication and addition together is known as a linear combination:
This is the superposition principle. Multiplying a wave function by any nonzero constant complex number to obtain does not change any information about the quantum system, because cancels in the Schrödinger equation for .
Momentum-space wave function.
The particle also has a wave function in momentum space:
where is the momentum in one dimension, which can be any value from to , and is time.
All the previous remarks on superposition, normalization, etc. apply similarly. In particular, if the particle's momentum is measured, the result is not deterministic, but is described by a probability distribution:
and the normalization condition is:
Relation between wave functions.
The position-space and momentum-space wave functions are Fourier transforms of each other, therefore both contain the same information, and either one alone is sufficient to calculate any property of the particle. As elements of abstract physical Hilbert space, whose elements are the possible states of the system under consideration, they represent the same object, but they are not equal when viewed as square-integrable functions. (A function and its Fourier transform are not equal.) For one dimension,
In practice, the position-space wave function is used much more often than the momentum-space wave function. The potential entering the Schrödinger equation determines in which basis the description is easiest. For the harmonic oscillator, and enter symmetrically, so there it doesn't matter which description one uses.
Definitions (other cases).
Following are the general forms of the wave function for systems in higher dimensions and more particles, as well as including other degrees of freedom than position coordinates or momentum components.
The position-space wave function of a single particle in three spatial dimensions is similar to the case of one spatial dimension above:
where is the position vector in three-dimensional space, and is time. As always is a complex number, for this case a complex-valued function of four real variables.
If there are many particles, in general there is only one wave function, not a separate wave function for each particle. The fact that "one" wave function describes "many" particles is what makes quantum entanglement and the EPR paradox possible. The position-space wave function for particles is written:
where is the position of the th particle in three-dimensional space, and is time. Altogether, this is a complex-valued function of real variables.
In quantum mechanics there is a fundamental distinction between "identical particles" and "distinguishable" particles. For example, any two electrons are identical and fundamentally indistinguishable from each other; the laws of physics make it impossible to "stamp an identification number" on a certain electron to keep track of it. This translates to a requirement on the wave function for a system of identical particles:
where the sign occurs if the particles are "all bosons" and sign if they are "all fermions". In other words, the wave function is either totally symmetric in the positions of bosons, or totally antisymmetric in the positions of fermions. The physical interchange of particles corresponds to mathematically switching arguments in the wave function. The antisymmetry feature of fermionic wave functions leads to the Pauli principle. Generally, bosonic and fermionic symmetry requirements are the manifestation of particle statistics and are present in other quantum state formalisms.
For "distinguishable" particles (no two being identical, i.e. no two having the same set of quantum numbers), there is no requirement for the wave function to be either symmetric or antisymmetric.
For a collection of particles, some identical with coordinates and others distinguishable (not identical with each other, and not identical to the aforementioned identical particles), the wave function is symmetric or antisymmetric in the identical particle coordinates only:
Again, there is no symmetry requirement for the distinguishable particle coordinates .
For a particle with spin, the wave function can be written in "position–spin space" as:
which is a complex-valued function of position in three-dimensional space, time , and , the spin projection quantum number along the axis. (The axis is an arbitrary choice; other axes can be used instead if the wave function is transformed appropriately, see below.) The parameter, unlike and , is a "discrete variable". For example, for a spin-1/2 particle, can only be or , and not any other value. (In general, for spin , can be .)
Often, the complex values of the wave function for all the spin numbers are arranged into a column vector, in which there are as many entries in the column vector as there are allowed values of . In this case, the spin dependence is placed in indexing the entries and the wave function is a complex vector-valued function of space and time only:
The wave function for "N" particles each with spin is the complex-valued function:
Concerning the general case of particles with spin in 3d, if is interpreted as a probability amplitude, the probability density is:
and the probability that particle 1 is in region with spin "and" particle 2 is in region with spin etc. at time is the integral of the probability density over these regions and spins:
The multidimensional Fourier transforms of the position or position–spin space wave functions yields momentum or momentum–spin space wave functions.
Decompositions into products.
For systems in time-independent potentials, the wave function can always be written as a function of the degrees of freedom multiplied by a time-dependent phase factor, the form of which is given by the Schrödinger equation. For the case of particles position-spin space,
where is the energy eigenvalue of the system corresponding to the eigenstate . Wave functions of this form are called stationary states.
In some situations, the wave function for a particle with spin factors into a product of a space function and a spin function , where each are complex-valued functions, and the time dependence can be placed in either function:
The dynamics of each factor can be studied in isolation. This factorization is always possible when the orbital and spin angular momenta of the particle are separable in the Hamiltonian operator, that is, the Hamiltonian can be split into an orbital term and a spin term. It is not possible for those interactions where an external field or any space-dependent quantity couples to the spin; examples include a particle in a magnetic field, and spin-orbit coupling. For the time-independent case this reduces to
where again is the energy eigenvalue of the system corresponding to the eigenstate . This extends to the case of particles:
and for the case of identical particles, each factor has to have the correct antisymmetry or symmetry, to make the overall wave function antisymmetric for fermions or symmetric for bosons.
Inner product.
Position-space inner products.
The inner product of two wave functions and is useful and important for a number of reasons given below. For the case of one spinless particle in 1d, it can be defined as the complex number (at time )
More generally, the formulae for the inner products are integrals over all coordinates or momenta and sums over all spin quantum numbers. That is, for one spinless particle in 3d the inner product of two wave functions can be defined as the complex number:
while for many spinless particles in 3d:
(altogether, this is three-dimensional volume integrals with differential volume elements , also written " or "). For one particle with spin in 3d:
and for the general case of particles with spin in 3d:
(altogether, three-dimensional volume integrals followed by sums over the spins).
In the Copenhagen interpretation, the modulus squared of the inner product (a complex number) gives a real number
which is interpreted as the probability of the wave function "collapsing" to the new wave function upon measurement of an observable, whose eigenvalues are the possible results of the measurement, with being an eigenvector of the resulting eigenvalue.
Although the inner product of two wave functions is a complex number, the inner product of a wave function with itself,
is "always" a positive real number. The number (not ) is called the norm of the wave function , and is not the same as the modulus .
A wave function is normalized if:
If is not normalized, then dividing by its norm gives the normalized function .
Two wave functions and are orthogonal if their inner product is zero:
A set of wave functions are orthonormal if they are each normalized and are all orthogonal to each other:
where and each take values 1, 2, ..., and is the Kronecker delta ( for and for ). Orthonormality of wave functions is instructive to consider since this guarantees linear independence of the functions. (However, the wave functions do not have to be orthonormal and can still be linearly independent, but the inner product of and is more complicated than the mere ).
Returning to the superposition above:
if the basis wave functions are orthonormal, then the coefficients have a particularly simple form:
If the basis wave functions were not orthonormal, then the coefficients would be different.
Momentum-space inner products.
Analogous to the position case, the inner product of two wave functions and can be defined as:
and similarly for more particles in higher dimensions.
One particular solution to the time-independent Schrödinger equation is
a plane wave, which can be used in the description of a particle with momentum exactly , since it is an eigenfunction of the momentum operator. These functions are not normalizable to unity (they aren't square-integrable), so they are not really elements of physical Hilbert space. The set
forms what is called the momentum basis. This "basis" is not a basis in the usual mathematical sense. For one thing, since the functions aren't normalizable, they are instead normalized to a delta function,
For another thing, though they are linearly independent, there are too many of them (they form an uncountable set) for a basis for physical Hilbert space. They can still be used to express all functions in it using Fourier transforms as described above.
Units of the wave function.
Although wave functions are complex numbers, both the real and imaginary parts each have the same units (the imaginary unit is a pure number without physical units). The units of depend on the number of particles the wave function describes, and the number of spatial or momentum dimensions of the system.
When integrating over all the coordinates, the volume element has units of [length]"Nn". Since the normalization conditions require the integral to be the unitless number 1, must have units of [length]−"Nn", thus the units of and hence are [length]−"Nn"/2. Likewise, in momentum space, length is replaced by momentum, and the units are [momentum]−"Nn"/2. These results are true for particles of any spin, since for particles with spin, the summations are over dimensionless spin quantum numbers.
More on wave functions and abstract state space.
As has been demonstrated, the set of all possible normalizable wave functions for a system with a particular choice of basis constitute a Hilbert space. This vector space is in general infinite-dimensional. Due to the multiple possible choices of basis, these Hilbert spaces are not unique. One therefore talks about an abstract Hilbert space, state space, where the choice of basis is left undetermined. The choice of basis corresponds to a choice of a maximal set of quantum numbers, each quantum number corresponding to an observable. Two observables corresponding to quantum numbers in the maximal set must commute, therefore, the basis isn't entirely arbitrary, but nonetheless, there are always several choices.
Specifically, each state is represented as an abstract vector in state space
where is a "ket" (a vector) written in Dirac's bra–ket notation. Kets that differ by multiplication by a scalar represent the same state. A ray in Hilbert space is a set of normalized vectors differing by a complex number of modulus . If and are two states in the vector space, and and are two complex numbers, then the linear combination
is also in the same vector space. The state space is postulated to have an inner product, denoted by
that is (usually, this differs) linear in the first argument and antilinear in the second argument. The dual vectors are denoted as "bras", . These are linear functionals, elements of the dual space to the state space. The inner product, once chosen, can be used to define a unique map from state space to its dual, see Riesz representation theorem. this map is antilinear. One has
where the asterisk denotes the complex conjugate. For this reason one has under this map
and one may, as a practical consequence, at least notation-wise in this formalism, ignore that bra's are dual vectors.
The state vector for the system evolves in time according to the Schrödinger equation, or other dynamical pictures of quantum mechanics- In bra-ket notation this reads,
Abstract state space is also, by definition, required to be a Hilbert space. The only requirement missing for this in the description so far is completeness. See the quantum state article for more explanation of the Hilbert space formalism and its consequences to quantum physics.
The connection to the Hilbert spaces of wave functions is made as follows. If is a maximal set of quantum numbers, denote the state corresponding to "fixed choices" of these quantum numbers by
The wave function corresponding to an arbitrary state is denoted
for a concrete example,
There are several advantages to understanding wave functions as representing elements of an abstract vector space:
Following is a summary of the bra–ket formalism applied to wave functions, with general discrete or continuous bases.
Discrete and continuous bases.
A Hilbert space with a discrete basis for is orthonormal if the inner product of all pairs of basis kets are given by the Kronecker delta:
Orthonormal bases are convenient to work with because the inner product of two vectors have simple expressions. A wave function expressed in this discrete basis of the Hilbert space, and the corresponding bra in the dual space, are respectively given by:
where the complex numbers
are the components of the vector. The column vector is a useful way to list the numbers, and operations on the entire vector can be done according to matrix addition and multiplication. The entire vector is independent of the basis, but the components depend on the basis. If a change of basis is made, the components of the vector must also change to compensate.
A Hilbert space with a continuous basis } is orthonormal if the inner product of all pairs of basis kets are given by the Dirac delta function:
As with the discrete bases, a symbol is used in the basis states, two common notations are and sometimes . A particular basis ket may be subscripted or primed , or simply given another symbol in place of .
While discrete basis vectors are summed over a discrete index, continuous basis vectors are integrated over a continuous index (a variable of a function). In what follows, all integrals are with respect to the real-valued basis variable (not complex-valued), over the required range. Usually this is just the real line or subsets of it. The state in the continuous basis of the Hilbert space, with the corresponding bra in the dual space, are respectively given by:
where the components are the complex-valued functions
of a real variable .
Completeness conditions.
The completeness conditions (also called closure relations) are
for discrete and continuous orthonormal bases, respectively. An orthonormal set of kets form bases if and only if they satisfy these relations. In each case, the equality to unity means this is an identity operator; its action on any state leaves it unchanged. Multiplying any state on the right of these gives the representation of the state in the basis. The inner product of a first state with a second can also be obtained by multiplying on the left and on the right of the relevant completeness condition.
Inner product.
Physically, the nature of the inner product is dependent on the basis in use, because the basis is chosen to reflect the quantum state of the system.
If is a state in the above basis with components and is another state in the same basis with components , the inner product is the complex number:
If is a state in the above continuous basis with components , and is another state in the same basis with components , the inner product is the complex number:
where the integrals are taken over all and .
The square of the norm (magnitude) of the state vector is given by the inner product of with itself, a real number:
for the discrete and continuous bases, respectively. Each say the projection of a complex probability amplitude onto itself is real. If is normalized, these expressions would be each separately equal to 1. If the state is not normalized, then dividing by its magnitude normalizes the state:
Normalized components and probabilities.
In the literature, the following results are often presented with normalized wavefunctions. Here, we keep the normalization factors to show where they appear if the wavefunction is not already normalized.
For the discrete basis, projecting the normalized state onto a particular state the system may collapse to, , gives the complex number;
so the modulus squared of this gives a real number;
In the Copenhagen interpretation, this is the probability of state occurring.
In the continuous basis, the projection of the normalized state onto some particular basis is a complex-valued function;
so the squared modulus is a real-valued function
In the Copenhagen interpretation, this function is the "probability density function" of measuring the observable , so integrating this with respect to between gives:
the probability of finding the system with between and .
Wave function collapse.
The physical meaning of the components of is given by the "wave function collapse postulate", also known as wave function collapse. If the observable(s) (momentum and/or spin, position and/or spin, etc.) corresponding to states has distinct and definite values, , and a measurement of that variable is performed on a system in the state then the probability of measuring is . If the measurement yields , the system "collapses" to the state irreversibly and instantaneously.
Time dependence.
In the Schrödinger picture, the states evolve in time, so the time dependence is placed in according to
for discrete bases, or
for continuous bases. However, in the Heisenberg picture the states are constant in time and time dependence is placed in the Heisenberg operators, so is not written as . The Heisenberg picture wave function is a snapshot of a Schrödinger picture wave function, representing the whole spacetime history of the system. In the interaction picture (also called Dirac picture), the time dependence is placed in both the states and operators, the subdivision depending on the interaction term in the Hamiltonian, and can be viewed as intermediate between the Heisenberg and Schrödinger pictures. It is useful primarily in computing S-matrix elements.
Tensor product.
It is useful to introduce another operation with the physical interpretation of forming composite states from a collection of other states. This is the tensor product. Given two systems described by states and , the tensor product of the states forms the composite state denoted by or simply without any operation symbol , and the new system includes both of the original systems together. The tensor product state lives in a new space; the tensor product of the original Hilbert spaces. The bases spanning this space are the tensor products of the original bases. The product is not commutative in general, so . If has components and has components , each in a discrete orthonormal basis , then:
and the notation can be simplified by abbreviating , , and , so that
The same procedure follows for continuous bases using integration. This can also be extended to any number of states, however taking tensor products for fermions and bosons is complicated by the symmetry requirements, see identical particles for general results.
Position representations.
This section applies mostly to non-relativistic quantum mechanics. In relativistic quantum mechanics, eigenstates of the position operator are problematic due to a relativistic extension of Heisenberg's uncertainty principle. In relativistic quantum field theory, they are not used at all to label physical states. Associated to a particle perfectly localized to a point in space is an infinite uncertainty in energy. This leads to pair production in the relativistic regime. Thus such a particle automatically has companions, leading to a breakdown of the description.
State space for one spin-0 particle in 1d.
For a spinless particle in one spatial dimension (the "x"-axis or real line), the state can be expanded in terms of a continuum of basis states; , also written , corresponding to the set of all position coordinates . The completeness condition for this basis is
and the orthogonality relation is
The state is expressed by:
in which the "wave function" described as a function is a component of the complex state vector.
The inner product as stated at the beginning of this article is:
If the particle is confined to a region (a subset of the "x"-axis), the integrals in the inner product and completeness condition would be integrals over .
State space (other cases).
The previous example can be extended to more particles in higher dimensions, and include spin.
For one spinless particle in 3d, the basis states are and any state vector in this space is expressed in terms of the basis vectors as :
with components:
For spinless particles in 3d, the basis states are . This is the tensor product of the one-particle position bases , , ..., , each of which spans the separate one-particle Hilbert spaces, so are the basis states for the tensor product of the one-particle Hilbert spaces (the Hilbert space for the composite many particle system). Any state vector in this space is
with components:
For one particle with spin in 3d, the basis states are , the tensor product of the position basis and spin basis , which exists in a new space from the spin space and position space alone. Any state in this space is:
with components:
For particles with spin in 3d, the basis states are , the tensor product of the position basis and spin basis , which exists in a new space from the spin space and position space alone. Any state in this space is:
with components:
If the particles are restricted to regions of position space, then the integrals in the completeness relations are taken over those regions, rather than the entire coordinate space. For the general case of many particles with spin in 3d, if particle 1 is in region , particle 2 is in region , and so on, the state in this position–spin representation is:
The orthogonality relation for this basis is:
and the inner product of and is:
Momentum space wave functions are similar, using the momentum vectors of the particles as continuous bases, namely , , etc.
Ontology.
Whether the wave function really exists, and what it represents, are major questions in the interpretation of quantum mechanics. Many famous physicists of a previous generation puzzled over this problem, such as Schrödinger, Einstein and Bohr. Some advocate formulations or variants of the Copenhagen interpretation (e.g. Bohr, Wigner and von Neumann) while others, such as Wheeler or Jaynes, take the more classical approach and regard the wave function as representing information in the mind of the observer, i.e. a measure of our knowledge of reality. Some, including Schrödinger, Bohm and Everett and others, argued that the wave function must have an objective, physical existence. Einstein thought that a complete description of physical reality should refer directly to physical space and time, as distinct from the wave function, which refers to an abstract mathematical space.
Examples.
Free particle.
A free particle in 3d with wave vector and angular frequency has a wave function
Particle in a box.
A particle is restricted to a 1D region between and ; its wave function is:
To normalize the wave function we need to find the value of the arbitrary constant ; solved from
From , we have , so the integral becomes;
Solving this equation gives , so the normalized wave function in the box is;
One-dimensional quantum tunnelling.
One of most prominent features of the wave mechanics is a possibility for a particle to reach a location with a prohibitive (in classical mechanics) force potential. In the one-dimensional case of particles with energy less than formula_92 in the square potential
Note that these wave functions are not normalized; see scattering theory for discussion.
The standard interpretation of this is as a stream of particles being fired at the step from the left (the direction of negative ): setting corresponds to firing particles singly; the terms containing and signify motion to the right, while and – to the left. Under this beam interpretation, put since no particles are coming from the right. By applying the continuity of wave functions and their derivatives at the boundaries, it is hence possible to determine the constants above.
Quantum Dots.
In a semiconductor crystallite whose radius is smaller than the size of its exciton Bohr radius, the excitons are squeezed, leading to quantum confinement. The energy levels can then be modeled using the particle in a box model in which the energy of different states is dependent on the length of the box.
Other.
Some examples of wave functions for specific applications include:

</doc>
<doc id="71138" url="https://en.wikipedia.org/wiki?curid=71138" title="Wave function collapse">
Wave function collapse

In quantum mechanics, wave function collapse is said to occur when a wave function—initially in a superposition of several eigenstates—appears to reduce to a single eigenstate (by "observation"). It is the essence of measurement in quantum mechanics and connects the wave function with classical observables such as position and momentum. Collapse is one of two processes by which quantum systems evolve in time; the other is continuous evolution via the Schrödinger equation. However, in this role, collapse is merely a black box for thermodynamically irreversible interaction with a classical environment. Calculations of quantum decoherence predict "apparent" wave function collapse when a superposition forms between the quantum system's states and the environment's states. Significantly, the combined wave function of the system and environment continue to obey the Schrödinger equation.
In 1927, Werner Heisenberg used the idea of wave function reduction to explain quantum measurement. Nevertheless, it was debated, for if collapse were a fundamental physical phenomenon, rather than just the epiphenomenon of some other process, it would mean nature was fundamentally stochastic, i.e. nondeterministic, an undesirable property for a theory. This issue remained until quantum decoherence entered mainstream opinion after its reformulation in the 1980s. Decoherence explains the perception of wave function collapse in terms of interacting large- and small-scale quantum systems, and is commonly taught at the graduate level (e.g. the Cohen-Tannoudji textbook).
Mathematical description.
Before collapse, the wave function may be any square-integrable function. This function is expressible as a linear combination of the eigenstates of any observable. Observables represent classical dynamical variables, and when one is measured by a classical observer, the wave function is projected onto a random eigenstate of that observable. The observer simultaneously measures the classical value of that observable to be the eigenvalue of the final state.
Mathematical background.
The quantum state of a physical system is described by a wave function (in turn – an element of a projective Hilbert space). This can be expressed in Dirac or bra–ket notation as a vector:
The kets formula_2, specify the different quantum "alternatives" available - a particular quantum state. They form an orthonormal eigenvector basis, formally
Where formula_4 represents the Kronecker delta.
An observable (i.e. measurable parameter of the system) is associated with each eigenbasis, with each quantum alternative having a specific value or eigenvalue, "e"i, of the observable. A "measurable parameter of the system" could be the usual position r and the momentum p of (say) a particle, but also its energy "E", z-components of spin ("sz"), orbital ("Lz") and total angular ("Jz") momenta etc. In the basis representation these are respectively formula_5.
The coefficients "c"1, "c"2, "c"3... are the probability amplitudes corresponding to each basis formula_2. These are complex numbers. The moduli square of "ci", that is |"ci"|2 = "ci"*"ci" (* denotes complex conjugate), is the probability of measuring the system to be in the state formula_7.
For simplicity in the following, all wave functions are assumed to be normalized; the total probability of measuring all possible states is unity:
The process.
With these definitions it is easy to describe the process of collapse. For any observable, the wave function is initially some linear combination of the eigenbasis formula_9 of that observable. When an external agency (an observer, experimenter) measures the observable associated with the eigenbasis formula_10, the wave function "collapses" from the full formula_11 to just "one" of the basis eigenstates, formula_12, that is:
The probability of collapsing to a given eigenstate formula_14 is the Born probability, formula_15. Post-measurement, other elements of the wave function vector, formula_16, have "collapsed" to zero, and formula_17.
More generally, collapse is defined for an operator formula_18 with eigenbasis formula_19. If the system is in state formula_20, and formula_18 is measured, the probability of collapsing the system to state formula_22 (and measuring formula_23) would be formula_24. Note that this is "not" the probability that the particle is in state formula_12; it is in state formula_20 until cast to an eigenstate of formula_18.
However, we never observe collapse to a single eigenstate of a continuous-spectrum operator (e.g. position, momentum, or a scattering Hamiltonian), because such eigenfunctions are non-normalizable. In these cases, the wave function will partially collapse to a linear combination of "close" eigenstates (necessarily involving a spread in eigenvalues) that embodies the imprecision of the measurement apparatus. The more precise the measurement, the tighter the range. Calculation of probability proceeds identically, except with an integral over the expansion coefficient formula_28. This phenomenon is unrelated to the uncertainty principle, although increasingly precise measurements of one operator (e.g. position) will naturally homogenize the expansion coefficient of wave function with respect to another, incompatible operator (e.g. momentum), lowering the probability of measuring any particular value of the latter.
Interpretations.
The determination of preferred-basis.
The complete set of orthogonal functions which a wave function will collapse to is also called preferred-basis. There lacks theoretical foundation for the preferred-basis to be the eigenstates of observables such as position, momentum, etc. In fact the eigenstates of position are not even physical due to the infinite energy associated with them. A better approach is to derive the preferred-basis from basic principles. It is proved that only special dynamic equation can collapse the wave function. By applying one axiom of the quantum mechanics and the assumption that preferred-basis depends on the total Hamiltonian, a unique set of equations is obtained from the collapse equation which determines the preferred-basis for general situations. Depending on the system Hamiltonian and wave function, the determination equations may yield preferred-basis as energy eigenfunctions, quasi-position eigenfunctions, mixed energy and quasi-position eigenfunctions, i.e., energy eigenfunctions for the interior of a macroscopic object and quasi-position eigenfunctions for the particles on the surface, and so on.
Quantum decoherence.
Wave function collapse is not fundamental from the perspective of quantum decoherence. There are several equivalent approaches to deriving collapse, like the density matrix approach, but each has the same effect: decoherence irreversibly converts the "averaged" or "environmentally traced over" density matrix from a pure state to a reduced mixture, giving the appearance of wave function collapse.
Quantum filtering approach.
The quantum filtering approach and the introduction of quantum causality non-demolition principle allows for a classical-environment derivation of wave function collapse from the stochastic Schrödinger equation.
Physical meaning.
By use of a suitable preparative apparatus, quantum system can be prepared in a pure state with its specimens as elements of an original beam. It has an original wave function.
The beam passes through a diffractive object such as a crystal, from which it emerges split into several sub-beams that are respectively in their several states pure with respect to that diffractor. If the sub-beams, all left intact, are brought together again by a compatible diffractive object, such as a similar crystal, then the originally prepared pure state is restored. The original wave function has not collapsed.
If the sub-beams are conceptually reconsidered and re-labeled as distinct beams, each in its own right, then they can be regarded as having, or can be described by, their respective distinct component wave functions. Then the beam, or the collection of the sub-beams if all are left intact, is said to be in a superposition of states, each pure with respect to that diffractor.
Otherwise, an external intervention may put a particle detector into the path of a chosen one of the sub-beams, so as to prevent the sub-beams from being brought together intact again. Then, according to the probabilities, it will sometimes detect a particle. The external intervention of putting the detector into one sub-beam path changes the physical and conceptual framework, and is said to collapse the original wave function into the component wave function for that sub-beam.
The word 'collapse' does not refer to a spontaneous evolution of an intact collection of sub-beams. No, it refers to an effect of an external intervention. Without the interruption of the path of the chosen component sub-beam by the external intervention that puts in the detector, the original wave function covers the whole process. If, on the contrary, the path is interrupted by the external intervention, then the whole process, including the effect of the intervention, is no longer described by the original wave function, because it is constructed on the assumption that there is no intervention.
If the external intervention is assumed to occur, the whole process needs to be re-considered and re-conceived on that assumption. The preparative device is now re-defined in an alternative way. Its output, that is delivered to the detector, is described by an appropriately re-defined wave function, that belonging to the path of the chosen component sub-beam as if it were an original beam in its own right. It might be labeled as 'the component wave function'.
The two alternative definitions of the preparative device lead to two different wave functions, because they describe two physically different processes. The original wave function describes a process that is said to show "no collapse". The re-defined component wave function describes a re-defined process that is said to show "collapse". The collapse is thus not a physical process in the ordinary sense; instead, it refers to an externally imposed choice between two different physical processes. Those processes have two different preparations. One is described by the original wave function, which does not consider the beam as split. (Dirac uses the phrase 'partly in one beam and partly in the other'. It may in other words be said that the particle is 'jointly in both sub-beams'.) The other is described by the re-defined component wave function, which does consider the beam as split, and refers to a definitely chosen component sub-beam.
The actual physical diffractive splitting is due to quantal momentum transfer by the diffractor, and occurs in both cases. But it can be observed in alternative ways. Without the external intervention that puts the detector into the chosen component sub-beam, the splitting is physically unobserved and may be viewed as only an undetected potential, because the inverse quantum momentum transfer can occur, with re-assembly of the original beam; no collapse is actually observable; the original wave function covers the case. With the external intervention, the detected split wave function is the re-defined component one, not the original one. The splitting is actually observed, according to the probabilities, and this is called collapse.
History and context.
The concept of wavefunction collapse, under the label 'reduction', not 'collapse', was introduced by Werner Heisenberg in his 1927 paper on the uncertainty principle, "Über den anschaulichen Inhalt der quantentheoretischen Kinematic und Mechanik". It was more clearly expressed in his 1930 textbook. It was incorporated into the mathematical formulation of quantum mechanics by John von Neumann, in his 1932 treatise "Mathematische Grundlagen der Quantenmechanik". Consistent with Heisenberg, von Neumann postulated that there were two processes of wave function change:
In general, quantum systems exist in superpositions of those basis states that most closely correspond to classical descriptions, and, in the absence of measurement, evolve according to the Schrödinger equation. However, when a measurement is made, the wave function collapses—from an observer's perspective—to just one of the basis states, and the property being measured uniquely acquires the eigenvalue of that particular state, formula_29. After the collapse, the system again evolves according to the Schrödinger equation.
By explicitly dealing with the interaction of object and measuring instrument, von Neumann has attempted to create consistency of the two processes of wave function change.
He was able to prove the "possibility" of a quantum mechanical measurement scheme consistent with wave function collapse. However, he did not prove the "necessity" of such a collapse. Although von Neumann's projection postulate is often presented as a normative description of quantum measurement, it was conceived by taking into account experimental evidence available during the 1930s (in particular the Compton-Simon experiment was paradigmatic), but many important present-day measurement procedures do not satisfy it (so-called measurements of the second kind).
The existence of the wave function collapse is required in
On the other hand, the collapse is considered a redundant or optional approximation in
The cluster of phenomena described by the expression "wave function collapse" is a fundamental problem in the interpretation of quantum mechanics, and is known as the measurement problem. The problem is deflected by the Copenhagen Interpretation, which postulates that this is a special characteristic of the "measurement" process. Everett's many-worlds interpretation deals with it by discarding the collapse-process, thus reformulating the relation between measurement apparatus and system in such a way that the linear laws of quantum mechanics are universally valid; that is, the only process according to which a quantum system evolves is governed by the Schrödinger equation or some relativistic equivalent.
Originating from de Broglie–Bohm theory, but no longer tied to it, is the physical process of decoherence, which causes an "apparent" collapse. Decoherence is also important for the consistent histories interpretation. A general description of the evolution of quantum mechanical systems is possible by using density operators and quantum operations. In this formalism (which is closely related to the C*-algebraic formalism) the collapse of the wave function corresponds to a non-unitary quantum operation.
The significance ascribed to the wave function varies from interpretation to interpretation, and varies even within an interpretation (such as the Copenhagen Interpretation). If the wave function merely encodes an observer's knowledge of the universe then the wave function collapse corresponds to the receipt of new information. This is somewhat analogous to the situation in classical physics, except that the classical "wave function" does not necessarily obey a wave equation. If the wave function is physically real, in some sense and to some extent, then the collapse of the wave function is also seen as a real process, to the same extent.

</doc>
<doc id="33426" url="https://en.wikipedia.org/wiki?curid=33426" title="Wave–particle duality">
Wave–particle duality

Wave–particle duality is the fact that every elementary particle or quantic entity exhibits the properties of not only particles, but also waves. It addresses the inability of the classical concepts "particle" or "wave" to fully describe the behavior of quantum-scale objects. As Einstein wrote: "It seems as though we must use sometimes the one theory and sometimes the other, while at times we may use either. We are faced with a new kind of difficulty. We have two contradictory pictures of reality; separately neither of them fully explains the phenomena of light, but together they do".
Various opinions have arisen about this.
Initiated by Louis de Broglie, before the discovery of quantum mechanics, and developed later as the de Broglie-Bohm theory, the pilot wave interpretation does not regard the duality as paradoxical, seeing both particle and wave aspects as always coexisting. According to Schrödinger the domain of the de Broglie waves is ordinary physical space-time. This formal feature exhibits the pilot wave theory as non-local, which is considered by many physicists to be a grave defect in a theory.
Still in the days of the old quantum theory, another pre-quantum-mechanical version of wave–particle duality was pioneered by William Duane, and developed by others including Alfred Landé. Duane explained diffraction of x-rays by a crystal in terms solely of their particle aspect. The deflection of the trajectory of each diffracted photon was due to quantal translative momentum transfer from the spatially regular structure of the diffracting crystal. Fourier analysis reveals the wave–particle duality as a simply mathematical equivalence, always present, and universal for all quanta. The same reasoning applies for example to diffraction of electrons by a crystal.
In the light of de Broglie's ideas, Erwin Schrödinger developed his wave mechanics by referring the universal wave aspect not to ordinary physical space-time, but rather to a profoundly different and more abstract 'space'. The domain of Schrödinger's wave function is configuration space. Ordinary physical space-time allows more or less direct visualization of cause and effect relations. In contrast, configuration space does not directly display cause and effect linkages. Sometimes, nevertheless, it seemed as if Schrödinger visualized his own waves as referring to ordinary space-time, and there was much debate about this.
Niels Bohr regarded the "duality paradox" as a fundamental or metaphysical fact of nature. A given kind of quantum object, will exhibit sometimes wave, sometimes particle, character, in respectively different physical settings. He saw such duality as one aspect of the concept of complementarity. Bohr regarded renunciation of the cause-effect relation, or complementarily, of the space-time picture, as essential to the quantum mechanical account.
Werner Heisenberg considered the question further. He saw the duality as present for all quantic entities, but not quite in the usual quantum mechanical account considered by Bohr. He saw it in what is called second quantization, which generates an entirely new concept of fields which exist in ordinary space-time, causality still being visualizable. Classical field values (e.g. the electric and magnetic field strengths of Maxwell) are replaced by an entirely new kind of field value, as considered in quantum field theory. Turning the reasoning around, ordinary quantum mechanics can be deduced as a specialized consequence of quantum field theory.
Because of the difference of views of Bohr and Heisenberg, the main sources of the so-called Copenhagen interpretation, the position of that interpretation on wave–particle duality is ill-defined.
In a modern perspective, wave functions arise naturally in relativistic quantum field theory in the formulation of free quantum fields. They are necessary for the Lorentz invariance of the theory. Their form and the equations of motion they obey are dictated by under which representation of the Lorentz group they transform.
Origin of theory.
The idea of duality originated in a debate over the nature of light and matter that dates back to the 17th century, when Christiaan Huygens and Isaac Newton proposed competing theories of light: light was thought either to consist of waves (Huygens) or of particles (Newton). Through the work of Max Planck, Albert Einstein, Louis de Broglie, Arthur Compton, Niels Bohr, and many others, current scientific theory holds that "all" particles "also" have a wave nature (and vice versa). This phenomenon has been verified not only for elementary particles, but also for compound particles like atoms and even molecules. For macroscopic particles, because of their extremely short wavelengths, wave properties usually cannot be detected.
Brief history of wave and particle viewpoints.
Aristotle was one of the first to publicly hypothesize about the nature of light, proposing that light is a disturbance in the element aether (that is, it is a wave-like phenomenon). On the other hand, Democritus—the original "atomist"—argued that all things in the universe, including light, are composed of indivisible sub-components (light being some form of solar atom). At the beginning of the 11th Century, the Arabic scientist Alhazen wrote the first comprehensive treatise on optics; describing refraction, reflection, and the operation of a pinhole lens via rays of light traveling from the point of emission to the eye. He asserted that these rays were composed of particles of light. In 1630, René Descartes popularized and accredited the opposing wave description in his treatise on light, showing that the behavior of light could be re-created by modeling wave-like disturbances in a universal medium ("plenum"). Beginning in 1670 and progressing over three decades, Isaac Newton developed and championed his corpuscular hypothesis, arguing that the perfectly straight lines of reflection demonstrated light's particle nature; only particles could travel in such straight lines. He explained refraction by positing that particles of light accelerated laterally upon entering a denser medium. Around the same time, Newton's contemporaries Robert Hooke and Christiaan Huygens—and later Augustin-Jean Fresnel—mathematically refined the wave viewpoint, showing that if light traveled at different speeds in different media (such as water and air), refraction could be easily explained as the medium-dependent propagation of light waves. The resulting Huygens–Fresnel principle was extremely successful at reproducing light's behavior and, subsequently supported by Thomas Young's 1803 discovery of double-slit interference, was the beginning of the end for the particle light camp.
The final blow against corpuscular theory came when James Clerk Maxwell discovered that he could combine four simple equations, which had been previously discovered, along with a slight modification to describe self-propagating waves of oscillating electric and magnetic fields. When the propagation speed of these electromagnetic waves was calculated, the speed of light fell out. It quickly became apparent that visible light, ultraviolet light, and infrared light (phenomena thought previously to be unrelated) were all electromagnetic waves of differing frequency. The wave theory had prevailed—or at least it seemed to.
While the 19th century had seen the success of the wave theory at describing light, it had also witnessed the rise of the atomic theory at describing matter. In 1789, Antoine Lavoisier securely differentiated chemistry from alchemy by introducing rigor and precision into his laboratory techniques; allowing him to deduce the conservation of mass and categorize many new chemical elements and compounds. However, the nature of these essential chemical elements remained unknown. In 1799, Joseph Louis Proust advanced chemistry towards the atom by showing that elements combined in definite proportions. This led John Dalton to resurrect Democritus' atom in 1803, when he proposed that elements were invisible sub components; which explained why the varying oxides of metals (e.g. stannous oxide and cassiterite, SnO and SnO2 respectively) possess a 1:2 ratio of oxygen to one another. But Dalton and other chemists of the time had not considered that some elements occur in monatomic form (like Helium) and others in diatomic form (like Hydrogen), or that water was H2O, not the simpler and more intuitive HO—thus the atomic weights presented at the time were varied and often incorrect. Additionally, the formation of H2O by two parts of hydrogen gas and one part of oxygen gas would require an atom of oxygen to split in half (or two half-atoms of hydrogen to come together). This problem was solved by Amedeo Avogadro, who studied the reacting volumes of gases as they formed liquids and solids. By postulating that equal volumes of elemental gas contain an equal number of atoms, he was able to show that H2O was formed from two parts H2 and one part O2. By discovering diatomic gases, Avogadro completed the basic atomic theory, allowing the correct molecular formulae of most known compounds—as well as the correct weights of atoms—to be deduced and categorized in a consistent manner. The final stroke in classical atomic theory came when Dimitri Mendeleev saw an order in recurring chemical properties, and created a table presenting the elements in unprecedented order and symmetry. But there were holes in Mendeleev's table, with no element to fill them in. His critics initially cited this as a fatal flaw, but were silenced when new elements were discovered that perfectly fit into these holes. The success of the periodic table effectively converted any remaining opposition to atomic theory; even though no single atom had ever been observed in the laboratory, chemistry was now an atomic science.
Turn of the 20th century and the paradigm shift.
Particles of electricity.
At the close of the 19th century, the reductionism of atomic theory began to advance into the atom itself; determining, through physics, the nature of the atom and the operation of chemical reactions. Electricity, first thought to be a fluid, was now understood to consist of particles called electrons. This was first demonstrated by J. J. Thomson in 1897 when, using a cathode ray tube, he found that an electrical charge would travel across a vacuum (which would possess infinite resistance in classical theory). Since the vacuum offered no medium for an electric fluid to travel, this discovery could only be explained via a particle carrying a negative charge and moving through the vacuum. This "electron" flew in the face of classical electrodynamics, which had successfully treated electricity as a fluid for many years (leading to the invention of batteries, electric motors, dynamos, and arc lamps). More importantly, the intimate relation between electric charge and electromagnetism had been well documented following the discoveries of Michael Faraday and James Clerk Maxwell. Since electromagnetism was "known" to be a wave generated by a changing electric or magnetic "field" (a continuous, wave-like entity itself) an atomic/particle description of electricity and charge was a non sequitur. Furthermore, classical electrodynamics was not the only classical theory rendered incomplete.
Radiation quantization.
Black-body radiation, the emission of electromagnetic energy due to an object's heat, could not be explained from classical arguments alone. The equipartition theorem of classical mechanics, the basis of all classical thermodynamic theories, stated that an object's energy is partitioned equally among the object's vibrational modes. This worked well when describing thermal objects, whose vibrational modes were defined as the speeds of their constituent atoms, and the speed distribution derived from egalitarian partitioning of these vibrational modes closely matched experimental results. Speeds much higher than the average speed were suppressed by the fact that kinetic energy is quadratic—doubling the speed requires four times the energy—thus the number of atoms occupying high energy modes (high speeds) quickly drops off because the constant, equal partition can excite successively fewer atoms. Low speed modes would "ostensibly" dominate the distribution, since low speed modes would require ever less energy, and "prima facie" a zero-speed mode would require zero energy and its energy partition would contain an infinite number of atoms. "But" this would only occur in the absence of atomic interaction; when collisions are allowed, the low speed modes are immediately suppressed by jostling from the higher energy atoms, exciting them to higher energy modes. An equilibrium is swiftly reached where most atoms occupy a speed proportional to the temperature of the object (thus defining temperature as the average kinetic energy of the object).
But applying the same reasoning to the electromagnetic emission of such a thermal object was not so successful. It had been long known that thermal objects emit light. Hot metal glows red, and upon further heating, white (this is the underlying principle of the incandescent bulb). Since light was known to be waves of electromagnetism, physicists hoped to describe this emission via classical laws. This became known as the black body problem. Since the equipartition theorem worked so well in describing the vibrational modes of the thermal object itself, it was trivial to assume that it would perform equally well in describing the radiative emission of such objects. But a problem quickly arose when determining the vibrational modes of light. To simplify the problem (by limiting the vibrational modes) a longest allowable wavelength was defined by placing the thermal object in a cavity. Any electromagnetic mode at equilibrium (i.e. any standing wave) could only exist if it used the walls of the cavities as nodes. Thus there were no waves/modes with a wavelength larger than twice the length ("L") of the cavity.
The first few allowable modes would therefore have wavelengths of : 2"L", "L", 2"L"/3, "L"/2, etc. (each successive wavelength adding one node to the wave). However, while the wavelength could never exceed 2"L", there was no such limit on decreasing the wavelength, and adding nodes to reduce the wavelength could proceed "ad infinitum". Suddenly it became apparent that the short wavelength modes completely dominated the distribution, since ever shorter wavelength modes could be crammed into the cavity. If each mode received an equal partition of energy, the short wavelength modes would consume all the energy. This became clear when plotting the Rayleigh–Jeans law which, while correctly predicting the intensity of long wavelength emissions, predicted infinite total energy as the intensity diverges to infinity for short wavelengths. This became known as the ultraviolet catastrophe.
The solution arrived in 1900 when Max Planck hypothesized that the frequency of light emitted by the black body depended on the frequency of the "oscillator" that emitted it, and the energy of these oscillators increased linearly with frequency (according to his constant "h", where E = hν). This was not an unsound proposal considering that macroscopic oscillators operate similarly: when studying five simple harmonic oscillators of equal amplitude but different frequency, the oscillator with the highest frequency possesses the highest energy (though this relationship is not linear like Planck's). By demanding that high-frequency light must be emitted by an oscillator of equal frequency, and further requiring that this oscillator occupy higher energy than one of a lesser frequency, Planck avoided any catastrophe; giving an equal partition to high-frequency oscillators produced successively fewer oscillators and less emitted light. And as in the Maxwell–Boltzmann distribution, the low-frequency, low-energy oscillators were suppressed by the onslaught of thermal jiggling from higher energy oscillators, which necessarily increased their energy and frequency.
The most revolutionary aspect of Planck's treatment of the black body is that it inherently relies on an integer number of oscillators in thermal equilibrium with the electromagnetic field. These oscillators "give" their entire energy to the electromagnetic field, creating a quantum of light, as often as they are "excited" by the electromagnetic field, absorbing a quantum of light and beginning to oscillate at the corresponding frequency. Planck had intentionally created an atomic theory of the black body, but had unintentionally generated an atomic theory of light, where the black body never generates quanta of light at a given frequency with an energy less than hν. However, once realizing that he had quantized the electromagnetic field, he denounced particles of light as a limitation of his approximation, not a property of reality.
Photoelectric effect illuminated.
While Planck had solved the ultraviolet catastrophe by using atoms and a quantized electromagnetic field, most contemporary physicists agreed that Planck's "light quanta" represented only flaws in his model. A more-complete derivation of black body radiation would yield a fully continuous and 'wave-like' electromagnetic field with no quantization. However, in 1905 Albert Einstein took Planck's black body model to produce his solution to another outstanding problem of the day: the photoelectric effect, wherein electrons are emitted from atoms when they absorb energy from light. Since their discovery eight years previously, electrons had been "the" thing to study in physics laboratories worldwide.
In 1902 Philipp Lenard discovered that the energy of these ejected electrons did "not" depend on the intensity of the incoming light, but instead on its "frequency". So if one shines a little low-frequency light upon a metal, a few low energy electrons are ejected. If one now shines a very intense beam of low-frequency light upon the same metal, a whole slew of electrons are ejected; however they possess the same low energy, there are merely "more of them". The more light there is, the more electrons are ejected. Whereas in order to get high energy electrons, one must illuminate the metal with high-frequency light. Like blackbody radiation, this was at odds with a theory invoking continuous transfer of energy between radiation and matter. However, it can still be explained using a fully classical description of light, as long as matter is quantum mechanical in nature.
If one used Planck's energy quanta, and demanded that electromagnetic radiation at a given frequency could only transfer energy to matter in integer multiples of an energy quantum hν, then the photoelectric effect could be explained very simply. Low-frequency light only ejects low-energy electrons because each electron is excited by the absorption of a single photon. Increasing the intensity of the low-frequency light (increasing the number of photons) only increases the number of excited electrons, not their energy, because the energy of each photon remains low. Only by increasing the frequency of the light, and thus increasing the energy of the photons, can one eject electrons with higher energy. Thus, using Planck's constant "h" to determine the energy of the photons based upon their frequency, the energy of ejected electrons should also increase linearly with frequency; the gradient of the line being Planck's constant. These results were not confirmed until 1915, when Robert Andrews Millikan, who had previously determined the charge of the electron, produced experimental results in perfect accord with Einstein's predictions. While the energy of ejected electrons reflected Planck's constant, the existence of photons was not explicitly proven until the discovery of the photon antibunching effect, of which a modern experiment can be performed in undergraduate-level labs. This phenomenon could only be explained via photons, and not through any semi-classical theory (which could alternatively explain the photoelectric effect). When Einstein received his Nobel Prize in 1921, it was not for his more difficult and mathematically laborious special and general relativity, but for the simple, yet totally revolutionary, suggestion of quantized light. Einstein's "light quanta" would not be called photons until 1925, but even in 1905 they represented the quintessential example of wave-particle duality. Electromagnetic radiation propagates following linear wave equations, but can only be emitted or absorbed as discrete elements, thus acting as a wave and a particle simultaneously.
Developmental milestones.
Huygens and Newton.
The earliest comprehensive theory of light was advanced by Christiaan Huygens, who proposed a wave theory of light, and in particular demonstrated how waves might interfere to form a wavefront, propagating in a straight line. However, the theory had difficulties in other matters, and was soon overshadowed by Isaac Newton's corpuscular theory of light. That is, Newton proposed that light consisted of small particles, with which he could easily explain the phenomenon of reflection. With considerably more difficulty, he could also explain refraction through a lens, and the splitting of sunlight into a rainbow by a prism. Newton's particle viewpoint went essentially unchallenged for over a century.
Young, Fresnel, and Maxwell.
In the early 19th century, the double-slit experiments by Young and Fresnel provided evidence for Huygens' wave theories. The double-slit experiments showed that when light is sent through a grid, a characteristic interference pattern is observed, very similar to the pattern resulting from the interference of water waves; the wavelength of light can be computed from such patterns. The wave view did not immediately displace the ray and particle view, but began to dominate scientific thinking about light in the mid 19th century, since it could explain polarization phenomena that the alternatives could not.
In the late 19th century, James Clerk Maxwell explained light as the propagation of electromagnetic waves according to the Maxwell equations. These equations were verified by experiment by Heinrich Hertz in 1887, and the wave theory became widely accepted.
Planck's formula for black-body radiation.
In 1901, Max Planck published an analysis that succeeded in reproducing the observed spectrum of light emitted by a glowing object. To accomplish this, Planck had to make an ad hoc mathematical assumption of quantized energy of the oscillators (atoms of the black body) that emit radiation. It was Einstein who later proposed that it is the electromagnetic radiation itself that is quantized, and not the energy of radiating atoms.
Einstein's explanation of the photoelectric effect.
In 1905, Albert Einstein provided an explanation of the photoelectric effect, a hitherto troubling experiment that the wave theory of light seemed incapable of explaining. He did so by postulating the existence of photons, quanta of light energy with particulate qualities.
In the photoelectric effect, it was observed that shining a light on certain metals would lead to an electric current in a circuit. Presumably, the light was knocking electrons out of the metal, causing current to flow. However, using the case of potassium as an example, it was also observed that while a dim blue light was enough to cause a current, even the strongest, brightest red light available with the technology of the time caused no current at all. According to the classical theory of light and matter, the strength or amplitude of a light wave was in proportion to its brightness: a bright light should have been easily strong enough to create a large current. Yet, oddly, this was not so.
Einstein explained this conundrum by postulating that the electrons can receive energy from electromagnetic field only in discrete portions (quanta that were called photons): an amount of energy "E" that was related to the frequency "f" of the light by
where "h" is Planck's constant (6.626 × 10−34 J seconds). Only photons of a high enough frequency (above a certain "threshold" value) could knock an electron free. For example, photons of blue light had sufficient energy to free an electron from the metal, but photons of red light did not. More intense light above the threshold frequency could release more electrons, but no amount of light (using technology available at the time) below the threshold frequency could release an electron. To "violate" this law would require extremely high intensity lasers which had not yet been invented. Intensity-dependent phenomena have now been studied in detail with such lasers.
Einstein was awarded the Nobel Prize in Physics in 1921 for his discovery of the law of the photoelectric effect.
De Broglie's wavelength.
In 1924, Louis-Victor de Broglie formulated the de Broglie hypothesis, claiming that "all" matter, not just light, has a wave-like nature; he related wavelength (denoted as "λ"), and momentum (denoted as "p"):
This is a generalization of Einstein's equation above, since the momentum of a photon is given by "p" = formula_3 and the wavelength (in a vacuum) by "λ" = formula_4, where "c" is the speed of light in vacuum.
De Broglie's formula was confirmed three years later for electrons (which differ from photons in having a rest mass) with the observation of electron diffraction in two independent experiments. At the University of Aberdeen, George Paget Thomson passed a beam of electrons through a thin metal film and observed the predicted interference patterns. At Bell Labs Clinton Joseph Davisson and Lester Halbert Germer guided their beam through a crystalline grid.
De Broglie was awarded the Nobel Prize for Physics in 1929 for his hypothesis. Thomson and Davisson shared the Nobel Prize for Physics in 1937 for their experimental work.
Heisenberg's uncertainty principle.
In his work on formulating quantum mechanics, Werner Heisenberg postulated his uncertainty principle, which states:
where
Heisenberg originally explained this as a consequence of the process of measuring: Measuring position accurately would disturb momentum and vice versa, offering an example (the "gamma-ray microscope") that depended crucially on the de Broglie hypothesis. It is now thought, however, that this only partly explains the phenomenon, but that the uncertainty also exists in the particle itself, even before the measurement is made.
In fact, the modern explanation of the uncertainty principle, extending the Copenhagen interpretation first put forward by Bohr and Heisenberg, depends even more centrally on the wave nature of a particle: Just as it is nonsensical to discuss the precise location of a wave on a string, particles do not have perfectly precise positions; likewise, just as it is nonsensical to discuss the wavelength of a "pulse" wave traveling down a string, particles do not have perfectly precise momenta (which corresponds to the inverse of wavelength). Moreover, when position is relatively well defined, the wave is pulse-like and has a very ill-defined wavelength (and thus momentum). And conversely, when momentum (and thus wavelength) is relatively well defined, the wave looks long and sinusoidal, and therefore it has a very ill-defined position.
de Broglie–Bohm theory.
De Broglie himself had proposed a pilot wave construct to explain the observed wave-particle duality. In this view, each particle has a well-defined position and momentum, but is guided by a wave function derived from Schrödinger's equation. The pilot wave theory was initially rejected because it generated non-local effects when applied to systems involving more than one particle. Non-locality, however, soon became established as an integral feature of quantum theory (see EPR paradox), and David Bohm extended de Broglie's model to explicitly include it.
In the resulting representation, also called the de Broglie–Bohm theory or Bohmian mechanics, the wave-particle duality vanishes, and explains the wave behaviour as a scattering with wave appearance, because the particle's motion is subject to a guiding equation or quantum potential. "This idea seems to me so natural and simple, to resolve the wave-particle dilemma in such a clear and ordinary way, that it is a great mystery to me that it was so generally ignored", J.S.Bell.
The best illustration of the "pilot-wave model" was given by Couder's 2010 "walking droplets" experiments, demonstrating the pilot-wave behaviour in a macroscopic mechanical analog.
Wave behavior of large objects.
Since the demonstrations of wave-like properties in photons and electrons, similar experiments have been conducted with neutrons and protons. Among the most famous experiments are those of Estermann and Otto Stern in 1929.
Authors of similar recent experiments with atoms and molecules, described below, claim that these larger particles also act like waves. A wave is basically a group of particles which moves in a particular form of motion i.e. to and fro, if we break that flow by an object it will convert into radiants.
A dramatic series of experiments emphasizing the action of gravity in relation to wave–particle duality was conducted in the 1970s using the neutron interferometer. Neutrons, one of the components of the atomic nucleus, provide much of the mass of a nucleus and thus of ordinary matter. In the neutron interferometer, they act as quantum-mechanical waves directly subject to the force of gravity. While the results were not surprising since gravity was known to act on everything, including light (see tests of general relativity and the Pound–Rebka falling photon experiment), the self-interference of the quantum mechanical wave of a massive fermion in a gravitational field had never been experimentally confirmed before.
In 1999, the diffraction of C60 fullerenes by researchers from the University of Vienna was reported. Fullerenes are comparatively large and massive objects, having an atomic mass of about 720 u. The de Broglie wavelength is 2.5 pm, whereas the diameter of the molecule is about 1 nm, about 400 times larger. In 2012, these far-field diffraction experiments could be extended to phthalocyanine molecules and their heavier derivatives, which are composed of 58 and 114 atoms respectively. In these experiments the build-up of such interference patterns could be recorded in real time and with single molecule sensitivity.
In 2003, the Vienna group also demonstrated the wave nature of tetraphenylporphyrin—a flat biodye with an extension of about 2 nm and a mass of 614 u. For this demonstration they employed a near-field Talbot Lau interferometer. In the same interferometer they also found interference fringes for C60F48., a fluorinated buckyball with a mass of about 1600 u, composed of 108 atoms. Large molecules are already so complex that they give experimental access to some aspects of the quantum-classical interface, i.e., to certain decoherence mechanisms. In 2011, the interference of molecules as heavy as 6910 u could be demonstrated in a Kapitza–Dirac–Talbot–Lau interferometer. In 2013, the interference of molecules beyond 10,000 u has been demonstrated.
Whether objects heavier than the Planck mass (about the weight of a large bacterium) have a de Broglie wavelength is theoretically unclear and experimentally unreachable; above the Planck mass a particle's Compton wavelength would be smaller than the Planck length and its own Schwarzschild radius, a scale at which current theories of physics may break down or need to be replaced by more general ones.
Recently Couder, Fort, "et al." showed that we can use macroscopic oil droplets on a vibrating surface as a model of wave–particle duality—localized droplet creates periodical waves around and interaction with them leads to quantum-like phenomena: interference in double-slit experiment, unpredictable tunneling (depending in complicated way on practically hidden state of field), orbit quantization (that particle has to 'find a resonance' with field perturbations it creates—after one orbit, its internal phase has to return to the initial state) and Zeeman effect.
Treatment in modern quantum mechanics.
Wave–particle duality is deeply embedded into the foundations of quantum mechanics. In the formalism of the theory, all the information about a particle is encoded in its "wave function", a complex-valued function roughly analogous to the amplitude of a wave at each point in space. This function evolves according to a differential equation (generically called the Schrödinger equation). For particles with mass this equation has solutions that follow the form of the wave equation. Propagation of such waves leads to wave-like phenomena such as interference and diffraction. Particles without mass, like photons, have no solutions of the Schrödinger equation so have another wave.
The particle-like behavior is most evident due to phenomena associated with measurement in quantum mechanics. Upon measuring the location of the particle, the particle will be forced into a more localized state as given by the uncertainty principle. When viewed through this formalism, the measurement of the wave function will randomly "collapse", or rather "decohere", to a sharply peaked function at some location. For particles with mass the likelihood of detecting the particle at any particular location is equal to the squared amplitude of the wave function there. The measurement will return a well-defined position, (subject to uncertainty), a property traditionally associated with particles. It is important to note that a measurement is only a particular type of interaction where some data is recorded and the measured quantity is forced into a particular eigenstate. The act of measurement is therefore not fundamentally different from any other interaction.
Following the development of quantum field theory the ambiguity disappeared. The field permits solutions that follow the wave equation, which are referred to as the wave functions. The term particle is used to label the irreducible representations of the Lorentz group that are permitted by the field. An interaction as in a Feynman diagram is accepted as a calculationally convenient approximation where the outgoing legs are known to be simplifications of the propagation and the internal lines are for some order in an expansion of the field interaction. Since the field is non-local and quantized, the phenomena which previously were thought of as paradoxes are explained. Within the limits of the wave-particle duality the quantum field theory gives the same results.
Visualization.
There are two ways to visualize the wave-particle behaviour: by the "standard model", described below; and by the Broglie–Bohm model, where no duality is perceived.
Below is an illustration of wave–particle duality as it relates to De Broglie's hypothesis and Heisenberg's uncertainty principle (above), in terms of the position and momentum space wavefunctions for one spinless particle with mass in one dimension. These wavefunctions are Fourier transforms of each other.
The more localized the position-space wavefunction, the more likely the particle is to be found with the position coordinates in that region, and correspondingly the momentum-space wavefunction is less localized so the possible momentum components the particle could have are more widespread.
Conversely the more localized the momentum-space wavefunction, the more likely the particle is to be found with those values of momentum components in that region, and correspondingly the less localized the position-space wavefunction, so the position coordinates the particle could occupy are more widespread.
Alternative views.
Wave–particle duality is an ongoing conundrum in modern physics. Most physicists accept wave-particle duality as the best explanation for a broad range of observed phenomena; however, it is not without controversy. Alternative views are also presented here. These views are not generally accepted by mainstream physics, but serve as a basis for valuable discussion within the community.
Both-particle-and-wave view.
The pilot wave model, originally developed by Louis de Broglie and further developed by David Bohm into the hidden variable theory proposes that there is no duality, but rather a system exhibits both particle properties and wave properties simultaneously, and particles are guided, in a deterministic fashion, by the pilot wave (or its "quantum potential") which will direct them to areas of constructive interference in preference to areas of destructive interference. This idea is held by a significant minority within the physics community.
At least one physicist considers the "wave-duality" as not being an incomprehensible mystery. L.E. Ballentine, "Quantum Mechanics, A Modern Development", p. 4, explains:
When first discovered, particle diffraction was a source of great puzzlement. Are "particles" really "waves?" In the early experiments, the diffraction patterns were detected holistically by means of a photographic plate, which could not detect individual particles. As a result, the notion grew that particle and wave properties were mutually incompatible, or complementary, in the sense that different measurement apparatuses would be required to observe them. That idea, however, was only an unfortunate generalization from a technological limitation. Today it is possible to detect the arrival of individual electrons, and to see the diffraction pattern emerge as a statistical pattern made up of many small spots (Tonomura et al., 1989). Evidently, quantum particles are indeed particles, but whose behaviour is very different from classical physics would have us to expect.
It has been claimed that the Afshar experiment (2007) shows that it is possible to simultaneously observe both wave and particle properties of photons. This claim is, however, rejected by other scientists.
Wave-only view.
At least one scientist proposes that the duality can be replaced by a "wave-only" view. In his book "Collective Electrodynamics: Quantum Foundations of Electromagnetism" (2000), Carver Mead purports to analyze the behavior of electrons and photons purely in terms of electron wave functions, and attributes the apparent particle-like behavior to quantization effects and eigenstates. According to reviewer David Haddon:
Mead has cut the Gordian knot of quantum complementarity. He claims that atoms, with their neutrons, protons, and electrons, are not particles at all but pure waves of matter. Mead cites as the gross evidence of the exclusively wave nature of both light and matter the discovery between 1933 and 1996 of ten examples of pure wave phenomena, including the ubiquitous laser of CD players, the self-propagating electrical currents of superconductors, and the Bose–Einstein condensate of atoms.
Albert Einstein, who, in his search for a Unified Field Theory, did not accept wave-particle duality, wrote:
This double nature of radiation (and of material corpuscles)...has been interpreted by quantum-mechanics in an ingenious and amazingly successful fashion. This interpretation...appears to me as only a temporary way out...
The many-worlds interpretation (MWI) is sometimes presented as a waves-only theory, including by its originator, Hugh Everett who referred to MWI as "the wave interpretation".
The " of R. Horodecki relates the particle to wave. The hypothesis implies that a massive particle is an intrinsically spatially as well as temporally extended wave phenomenon by a nonlinear law.
Neither-wave-nor-particle view.
It has been argued that there are never exact particles or waves, but only some compromise or intermediate between them. For this reason, in 1928 Arthur Eddington coined the name "wavicle"" to describe the objects although it is not regularly used today. One consideration
is that zero-dimensional mathematical points cannot be observed. Another is that the formal representation of such points, the Dirac delta function is unphysical, because it cannot be normalized. Parallel arguments apply to pure wave states. Roger Penrose states:
"Such 'position states' are idealized wavefunctions in the opposite sense from the momentum states. Whereas the momentum states are infinitely spread out, the position states are infinitely concentrated. Neither is normalizable [...]."
Relational approach to wave–particle duality.
Relational quantum mechanics is developed which regards the detection event as establishing a relationship between the quantized field and the detector. The inherent ambiguity associated with applying Heisenberg's uncertainty principle and thus wave–particle duality is subsequently avoided.
Image of Wave-Particle nature of light has been captured.
Researchers at the Ecole polytechnique federale de Lausanne have experimentally been able to image the wave-particle nature of light waves. The full length technical paper published on 2 March 2015, is available for access.
Applications.
Although it is difficult to draw a line separating wave–particle duality from the rest of quantum mechanics, it is nevertheless possible to list some applications of this basic idea.

</doc>
<doc id="1624795" url="https://en.wikipedia.org/wiki?curid=1624795" title="Weak hypercharge">
Weak hypercharge

The weak hypercharge in particle physics is a quantum number relating the electric charge and the third component of weak isospin. It is conserved (only terms that are overall weak-hypercharge neutral are allowed in the Lagrangian) and is similar to the Gell-Mann–Nishijima formula for the hypercharge of strong interactions (which is not conserved in weak interactions). It is frequently denoted "Y"W and corresponds to the gauge symmetry U(1).
Definition.
Weak hypercharge is the generator of the U(1) component of the electroweak gauge group, and its associated quantum field "B" mixes with the "W"3 electroweak quantum field to produce the observed gauge boson and the photon of quantum electrodynamics.
Weak hypercharge, usually written as "Y"W, satisfies the equality:
where "Q" is the electrical charge (in elementary charge units) and "T"3 is the third component of weak isospin. Rearranging, the weak hypercharge can be explicitly defined as:
Note: sometimes weak hypercharge is scaled so that 
although this is a minority usage.
Hypercharge assignments in the Standard Model are determined up to a twofold ambiguity by demanding cancellation of all anomalies.
Baryon and lepton number.
Weak hypercharge is related to baryon number minus lepton number via:
where "X" is a GUT-associated conserved quantum number. Since weak hypercharge is always conserved this implies that baryon number minus lepton number is also always conserved, within the Standard Model and most extensions.
Neutron decay.
Hence neutron decay conserves baryon number "B" and lepton number "L" separately, so also the difference "B" − "L" is conserved.
Proton decay.
Proton decay is a prediction of many grand unification theories.
Hence proton decay conserves "B" − "L", even though it violates both lepton number and baryon number conservation.

</doc>
<doc id="33629" url="https://en.wikipedia.org/wiki?curid=33629" title="Weak interaction">
Weak interaction

In particle physics, the weak interaction is the mechanism responsible for the weak force or weak nuclear force, one of the four known fundamental interactions of nature, alongside the strong interaction, electromagnetism, and gravitation. The weak interaction is responsible for the radioactive decay of subatomic particles, and it plays an essential role in nuclear fission. The theory of the weak interaction is sometimes called quantum flavordynamics (QFD), in analogy with the terms QCD and QED, but the term is rarely used because the weak force is best understood in terms of electro-weak theory (EWT).
In the Standard Model of particle physics, the weak interaction is caused by the emission or absorption of W and Z bosons. All known fermions interact through the weak interaction. Fermions are particles that have half-integer spin (one of the fundamental properties of particles). A fermion can be an elementary particle, such as the electron, or it can be a composite particle, such as the proton. The masses of W+, W−, and Z bosons are each far greater than that of protons or neutrons, consistent with the short range of the weak force. The force is termed "weak" because its field strength over a given distance is typically several orders of magnitude less than that of the strong nuclear force and electromagnetic force.
During the quark epoch, the electroweak force split into the electromagnetic and weak forces. Important examples of weak interaction include beta decay, and the production, from hydrogen, of deuterium needed to power the sun's thermonuclear process. Most fermions will decay by a weak interaction over time. Such decay also makes radiocarbon dating possible, as carbon-14 decays through the weak interaction to nitrogen-14. It can also create radioluminescence, commonly used in tritium illumination, and in the related field of betavoltaics.
Quarks, which make up composite particles like neutrons and protons, come in six "flavours" – up, down, strange, charm, top and bottom – which give those composite particles their properties. The weak interaction is unique in that it allows for quarks to swap their flavour for another. For example, during beta minus decay, a down quark decays into an up quark, converting a neutron to a proton. Also the weak interaction is the only fundamental interaction that breaks parity-symmetry, and similarly, the only one to break CP-symmetry.
History.
In 1933, Enrico Fermi proposed the first theory of the weak interaction, known as Fermi's interaction. He suggested that beta decay could be explained by a four-fermion interaction, involving a contact force with no range.
However, it is better described as a non-contact force field having a finite range, albeit very short. In 1968, Sheldon Glashow, Abdus Salam and Steven Weinberg unified the electromagnetic force and the weak interaction by showing them to be two aspects of a single force, now termed the electro-weak force.
The existence of the W and Z bosons was not directly confirmed until 1983.
Properties.
The weak interaction is unique in a number of respects:
Due to their large mass (approximately 90 GeV/c2) these carrier particles, termed the W and Z bosons, are short-lived: they have a lifetime of under 1×10−24 seconds. The weak interaction has a coupling constant (an indicator of interaction strength) of between 10−7 and 10−6, compared to the strong interaction's coupling constant of about 1 and the electromagnetic coupling constant of about 10−2; consequently the weak interaction is weak in terms of strength. The weak interaction has a very short range (around 10−17–10−16 m). At distances around 10−18 meters, the weak interaction has a strength of a similar magnitude to the electromagnetic force, but this starts to decrease exponentially with increasing distance. At distances of around 3×10−17 m, the weak interaction is 10,000 times weaker than the electromagnetic.
The weak interaction affects all the fermions of the Standard Model, as well as the Higgs boson; neutrinos interact through gravity and the weak interaction only, and neutrinos were the original reason for the name "weak force". The weak interaction does not produce bound states (nor does it involve binding energy) – something that gravity does on an astronomical scale, that the electromagnetic force does at the atomic level, and that the strong nuclear force does inside nuclei.
Its most noticeable effect is due to its first unique feature: flavor changing. A neutron, for example, is heavier than a proton (its sister nucleon), but it cannot decay into a proton without changing the flavor (type) of one of its two "down" quarks to "up". Neither the strong interaction nor electromagnetism permit flavour changing, so this must proceed by weak decay; without weak decay, quark properties such as strangeness and charm (associated with the quarks of the same name) would also be conserved across all interactions. All mesons are unstable because of weak decay. In the process known as beta decay, a "down" quark in the neutron can change into an "up" quark by emitting a virtual boson which is then converted into an electron and an electron antineutrino. Another example is the electron capture, a common variant of radioactive decay, where a proton (up quark) and an electron within an atom interact, and are changed to a neutron (down quark) and an electron neutrino.
Due to the large mass of a boson, weak decay is much more unlikely than strong or electromagnetic decay, and hence occurs less rapidly. For example, a neutral pion (which decays electromagnetically) has a life of about 10−16 seconds, while a charged pion (which decays through the weak interaction) lives about 10−8 seconds, a hundred million times longer. In contrast, a free neutron (which also decays through the weak interaction) lives about 15 minutes.
Weak isospin and weak hypercharge.
All particles have a property called weak isospin (T3), which serves as a quantum number and governs how that particle interacts in the weak interaction. Weak isospin therefore plays the same role in the weak interaction as electric charge does in electromagnetism, and color charge in the strong interaction. All fermions have a weak isospin value of either + or −. For example, the up quark has a T3 of + and the down quark −. A quark never decays through the weak interaction into a quark of the same T3: quarks with a T3 of + decay into quarks with a T3 of − and vice versa.
In any given interaction, weak isospin is conserved: the sum of the weak isospin numbers of the particles entering the interaction equals the sum of the weak isospin numbers of the particles exiting that interaction. For example, a (left-handed) , with a weak isospin of 1 normally decays into a (+1/2) and a (as a right-handed antiparticle, +1/2).
Following the development of the electroweak theory, another property, weak hypercharge, was developed. It is dependent on a particle's electrical charge and weak isospin, and is defined as:
where "YW" is the weak hypercharge of a given type of particle, "Q" is its electrical charge (in elementary charge units) and "T3" is its weak isospin. Whereas some particles have a weak isospin of zero, all particles, except gluons, have non-zero weak hypercharge. Weak hypercharge is the generator of the U(1) component of the electroweak gauge group.
Interaction types.
There are two types of weak interaction (called "vertices"). The first type is called the "charged-current interaction" because it is mediated by particles that carry an electric charge (the or bosons), and is responsible for the beta decay phenomenon. The second type is called the "neutral-current interaction" because it is mediated by a neutral particle, the Z boson.
Charged-current interaction.
In one type of charged current interaction, a charged lepton (such as an electron or a muon, having a charge of −1) can absorb a boson (a particle with a charge of +1) and be thereby converted into a corresponding neutrino (with a charge of 0), where the type ("flavour") of neutrino (electron, muon or tau) is the same as the type of lepton in the interaction, for example:
Similarly, a down-type quark ("d" with a charge of −) can be converted into an up-type quark ("u", with a charge of +), by emitting a boson or by absorbing a boson. More precisely, the down-type quark becomes a quantum superposition of up-type quarks: that is to say, it has a possibility of becoming any one of the three up-type quarks, with the probabilities given in the CKM matrix tables. Conversely, an up-type quark can emit a boson – or absorb a boson – and thereby be converted into a down-type quark, for example:
The W boson is unstable so will rapidly decay, with a very short lifetime. For example:
Decay of the W boson to other products can happen, with varying probabilities.
In the so-called beta decay of a neutron (see picture, above), a down quark within the neutron emits a virtual boson and is thereby converted into an up quark, converting the neutron into a proton. Because of the energy involved in the process (i.e., the mass difference between the down quark and the up quark), the boson can only be converted into an electron and an electron-antineutrino. At the quark level, the process can be represented as:
Neutral-current interaction.
In neutral current interactions, a quark or a lepton (e.g., an electron or a muon) emits or absorbs a neutral Z boson. For example:
Like the W boson, the Z boson also decays rapidly, for example:
Electroweak theory.
The Standard Model of particle physics describes the electromagnetic interaction and the weak interaction as two different aspects of a single electroweak interaction, the theory of which was developed around 1968 by Sheldon Glashow, Abdus Salam and Steven Weinberg. They were awarded the 1979 Nobel Prize in Physics for their work. The Higgs mechanism provides an explanation for the presence of three massive gauge bosons (the three carriers of the weak interaction) and the massless photon of the electromagnetic interaction.
According to the electroweak theory, at very high energies, the universe has four massless gauge boson fields similar to the photon and a complex scalar Higgs field doublet. However, at low energies, gauge symmetry is spontaneously broken down to the U(1) symmetry of electromagnetism (one of the Higgs fields acquires a vacuum expectation value). This symmetry breaking would produce three massless bosons, but they become integrated by three photon-like fields (through the Higgs mechanism) giving them mass. These three fields become the , and Z bosons of the weak interaction, while the fourth gauge field, which remains massless, is the photon of electromagnetism.
This theory has made a number of predictions, including a prediction of the masses of the Z and W bosons before their discovery. On 4 July 2012, the CMS and the ATLAS experimental teams at the Large Hadron Collider independently announced that they had confirmed the formal discovery of a previously unknown boson of mass between 125–127 GeV/"c"2, whose behaviour so far was "consistent with" a Higgs boson, while adding a cautious note that further data and analysis were needed before positively identifying the new boson as being a Higgs boson of some type. By 14 March 2013, the Higgs boson was tentatively confirmed to exist .
Violation of symmetry.
The laws of nature were long thought to remain the same under mirror reflection, the reversal of one spatial axis. The results of an experiment viewed via a mirror were expected to be identical to the results of a mirror-reflected copy of the experimental apparatus. This so-called law of parity conservation was known to be respected by classical gravitation, electromagnetism and the strong interaction; it was assumed to be a universal law. However, in the mid-1950s Chen Ning Yang and Tsung-Dao Lee suggested that the weak interaction might violate this law. Chien Shiung Wu and collaborators in 1957 discovered that the weak interaction violates parity, earning Yang and Lee the 1957 Nobel Prize in Physics.
Although the weak interaction used to be described by Fermi's theory, the discovery of parity violation and renormalization theory suggested that a new approach was needed. In 1957, Robert Marshak and George Sudarshan and, somewhat later, Richard Feynman and Murray Gell-Mann proposed a V−A (vector minus axial vector or left-handed) Lagrangian for weak interactions. In this theory, the weak interaction acts only on left-handed particles (and right-handed antiparticles). Since the mirror reflection of a left-handed particle is right-handed, this explains the maximal violation of parity. Interestingly, the V−A theory was developed before the discovery of the Z boson, so it did not include the right-handed fields that enter in the neutral current interaction.
However, this theory allowed a compound symmetry CP to be conserved. CP combines parity P (switching left to right) with charge conjugation C (switching particles with antiparticles). Physicists were again surprised when in 1964, James Cronin and Val Fitch provided clear evidence in kaon decays that CP symmetry could be broken too, winning them the 1980 Nobel Prize in Physics. In 1973, Makoto Kobayashi and Toshihide Maskawa showed that CP violation in the weak interaction required more than two generations of particles, effectively predicting the existence of a then unknown third generation. This discovery earned them half of the 2008 Nobel Prize in Physics. Unlike parity violation, CP violation occurs in only a small number of instances, but remains widely held as an answer to the difference between the amount of matter and antimatter in the universe; it thus forms one of Andrei Sakharov's three conditions for baryogenesis.

</doc>
<doc id="149861" url="https://en.wikipedia.org/wiki?curid=149861" title="Work (physics)">
Work (physics)

In physics, a force is said to do work if, when acting on a body, there is a displacement of the point of application in the direction of the force. For example, when a ball is held above the ground and then dropped, the work done on the ball as it falls is equal to the weight of the ball (a force) multiplied by the distance to the ground (a displacement).
The term "work" was introduced in 1826 by the French mathematician Gaspard-Gustave Coriolis as "weight "lifted" through a height", which is based on the use of early steam engines to lift buckets of water out of flooded ore mines. The SI unit of work is the newton-metre or joule (J).
Units.
The SI unit of work is the joule (J), which is defined as the work expended by a force of one newton through a distance of one metre.
The dimensionally equivalent newton-metre (N⋅m) is sometimes used as the measuring unit for work, but this can be confused with the unit newton-metre, which is the measurement unit of torque. Usage of N⋅m is discouraged by the SI authority, since it can lead to confusion as to whether the quantity expressed in newton metres is a torque measurement, or a measurement of energy.
Non-SI units of work include the erg, the foot-pound, the foot-poundal, the kilowatt hour, the litre-atmosphere, and the horsepower-hour. Due to work having the same physical dimension as heat, occasionally measurement units typically reserved for heat or energy content, such as therm, BTU and Calorie, are utilized as a measuring unit.
Work and energy.
The work done by a constant force of magnitude "F" on a point that moves a displacement (not distance) "s" in the direction of the force is the product,
For example, if a force of 10 newtons () acts along a point that travels 2 metres (), then it does the work . This is approximately the work done lifting a 1 kg weight from ground to over a person's head against the force of gravity. Notice that the work is doubled either by lifting twice the weight the same distance or by lifting the same weight twice the distance.
Work is closely related to energy. The law of conservation of energy states that the change in total internal energy of a system equals the added heat, minus the work performed by the system (see the first law of thermodynamics),
where the symbol formula_3 indicates that heat ("Q") and work ("W") are inexact differentials.
From Newton's second law, it can be shown that work on a free (no fields), rigid (no internal degrees of freedom) body, is equal to the change in kinetic energy of the velocity and rotation of that body,
The work of forces generated by a potential function is known as potential energy and the forces are said to be conservative. Therefore, work on an object that is merely displaced in a conservative force field, without change in velocity or rotation, is equal to "minus" the change of potential energy of the object,
These formulas demonstrate that work is the energy associated with the action of a force, so work subsequently possesses the physical dimensions, and units, of energy.
The work/energy principles discussed here are identical to Electric work/energy principles.
Constraint forces.
Constraint forces determine the movement of components in a system, constraining the object within a boundary (in the case of a slope plus gravity, the object is "stuck to" the slope, when attached to a taut string it cannot move in an outwards direction to make the string any 'tauter'). Constraint forces ensure the velocity in the direction of the constraint is zero, which means the constraint forces do not perform work on the system.
If the system doesn't change in time, they eliminate all movement in the direction of the constraint, thus constraint forces do not perform work on the system, as the velocity of that object is constrained to be 0 parallel to this force, due to this force. This only applies for a single particle system. For example, in an Atwood machine, the rope does work on each body, but keeping always the net virtual work null. There are, however, cases where this is not true.
For example, the centripetal force exerted "inwards" by a string on a ball in uniform circular motion "sideways" constrains the ball to circular motion restricting its movement away from the center of the circle. This force does zero work because it is perpendicular to the velocity of the ball.
Another example is a book on a table. If external forces are applied to the book so that it slides on the table, then the force exerted by the table constrains the book from moving downwards. The force exerted by the table supports the book and is perpendicular to its movement which means that this constraint force does not perform work.
The magnetic force on a charged particle is , where "q" is the charge, v is the velocity of the particle, and B is the magnetic field. The result of a cross product is always perpendicular to both of the original vectors, so . The dot product of two perpendicular vectors is always zero, so the work , and the magnetic force does not do work. It can change the direction of motion but never change the speed.
Mathematical calculation.
For moving objects, the quantity of work/time (power) is calculated. Thus, at any instant, the rate of the work done by a force (measured in joules/second, or watts) is the scalar product of the force (a vector), and the velocity vector of the point of application. This scalar product of force and velocity is classified as instantaneous power. Just as velocities may be integrated over time to obtain a total distance, by the fundamental theorem of calculus, the total work along a path is similarly the time-integral of instantaneous power applied along the trajectory of the point of application.
Work is the result of a force on a point that moves through a distance. As the point moves, it follows a curve X, with a velocity v, at each instant. The small amount of work "δW" that occurs over an instant of time "dt" is calculated as
where the is the power over the instant "dt". The sum of these small amounts of work over the trajectory of the point yields the work,
where "C" is the trajectory from x("t"1) to x("t"2). This integral is computed along the trajectory of the particle, and is therefore said to be "path dependent".
If the force is always directed along this line, and the magnitude of the force is "F", then this integral simplifies to
where "s" is distance along the line. If F is constant, in addition to being directed along the line, then the integral simplifies further to
where "s" is the distance travelled by the point along the line.
This calculation can be generalized for a constant force that is not directed along the line, followed by the particle. In this case the dot product , where "θ" is the angle between the force vector and the direction of movement, that is
In the notable case of a force applied to a body always at an angle of 90° from the velocity vector (as when a body moves in a circle under a central force), no work is done at all, since the cosine of 90 degrees is zero. Thus, no work can be performed by gravity on a planet with a circular orbit (this is ideal, as all orbits are slightly elliptical). Also, no work is done on a body moving circularly at a constant speed while constrained by mechanical force, such as moving at constant speed in a frictionless ideal centrifuge.
Calculating the work as "force times straight path segment" would only apply in the most simple of circumstances, as noted above. If force is changing, or if the body is moving along a curved path, possibly rotating and not necessarily rigid, then only the path of the application point of the force is relevant for the work done, and only the component of the force parallel to the application point velocity is doing work (positive work when in the same direction, and negative when in the opposite direction of the velocity). This component of force can be described by the scalar quantity called "scalar tangential component" (formula_11, where formula_12 is the angle between the force and the velocity). And then the most general definition of work can be formulated as follows:
Torque and rotation.
A force couple results from equal and opposite forces, acting on two different points of a rigid body. The sum (resultant) of these forces may cancel, but their effect on the body is the couple or torque T. The work of the torque is calculated as
where the is the power over the instant "δt". The sum of these small amounts of work over the trajectory of the rigid body yields the work,
This integral is computed along the trajectory of the rigid body with an angular velocity ω that varies with time, and is therefore said to be "path dependent".
If the angular velocity vector maintains a constant direction, then it takes the form,
where φ is the angle of rotation about the constant unit vector S. In this case, the work of the torque becomes,
where "C" is the trajectory from "φ"("t"1) to "φ"("t"2). This integral depends on the rotational trajectory "φ"("t"), and is therefore path-dependent.
If the torque T is aligned with the angular velocity vector so that,
and both the torque and angular velocity are constant, then the work takes the form,
This result can be understood more simply by considering the torque as arising from a force of constant magnitude "F", being applied perpendicularly to a lever arm at a distance "r", as shown in the figure. This force will act through the distance along the circular arc , so the work done is
Introduce the torque , to obtain
as presented above.
Notice that only the component of torque in the direction of the angular velocity vector contributes to the work.
Work and potential energy.
The scalar product of a force F and the velocity v of its point of application defines the power input to a system at an instant of time. Integration of this power over the trajectory of the point of application, , defines the work input to the system by the force.
Path dependence.
Therefore, the work done by a force F on an object that travels along a curve "C" is given by the line integral:
where "dx"("t") defines the trajectory "C" and v is the velocity along this trajectory. In general this integral requires the path along which the velocity is defined, so the evaluation of work is said to be path dependent.
The time derivative of the integral for work yields the instantaneous power,
Path independence.
If the work for an applied force is independent of the path, then the work done by the force is, by the gradient theorem, the potential function evaluated at the start and end of the trajectory of the point of application. Such a force is said to be conservative. This means that there is a potential function "U"(x), that can be evaluated at the two points x("t"1) and x("t"2) to obtain the work over any trajectory between these two points. It is tradition to define this function with a negative sign so that positive work is a reduction in the potential, that is
The function "U"(x) is called the potential energy associated with the applied force. Examples of forces that have potential energies are gravity and spring forces.
In this case, the gradient of work yields
and the force F is said to be "derivable from a potential."
Because the potential "U" defines a force F at every point x in space, the set of forces is called a force field. The power applied to a body by a force field is obtained from the gradient of the work, or potential, in the direction of the velocity V of the body, that is
Work by gravity.
In the absence of other forces, gravity results in a constant downward acceleration of every freely moving object. Near Earth's surface the acceleration due to gravity is and the gravitational force on an object of mass "m" is . It is convenient to imagine this gravitational force concentrated at the center of mass of the object.
If an object is displaced upwards or downwards a vertical distance , the work "W" done on the object by its weight mg is:
where "Fg" is weight (pounds in imperial units, and newtons in SI units), and Δ"y" is the change in height "y". Notice that the work done by gravity depends only on the vertical movement of the object. The presence of friction does not affect the work done on the object by its weight.
Work by gravity in space.
The force of gravity exerted by a mass "M" on another mass "m" is given by
where r is the position vector from "M" to "m".
Let the mass "m" move at the velocity v then the work of gravity on this mass as it moves from position r("t"1) to r("t"2) is given by
Notice that the position and velocity of the mass "m" are given by
where e"r" and e"t" are the radial and tangential unit vectors directed relative to the vector from "M" to "m". Use this to simplify the formula for work of gravity to,
This calculation uses the fact that
The function
is the gravitational potential function, also known as gravitational potential energy. The negative sign follows the convention that work is gained from a loss of potential energy.
Work by a spring.
Consider a spring that exerts a horizontal force that is proportional to its deflection in the "x" direction independent of how a body moves. The work of this spring on a body moving along the space curve , is calculated using its velocity, , to obtain
For convenience, consider contact with the spring occurs at , then the integral of the product of the distance "x" and the x-velocity, "xv"x, is (1/2)"x"2.
Work by a gas.
Where "P" is pressure, "V" is volume, and "a" and "b" are initial and final volumes.
Work–energy principle.
The principle of work and kinetic energy (also known as the work–energy principle) states that "the work done by all forces acting on a particle (the work of the resultant force) equals the change in the kinetic energy of the particle." That is, the work "W" done by the resultant force on a particle equals the change in the particle's kinetic energy formula_35,
where formula_37 and formula_38 are the speeds of the particle before and after the work is done and "m" is its mass.
The derivation of the "work–energy principle" begins with Newton's second law and the resultant force on a particle which includes forces applied to the particle and constraint forces imposed on its movement. Computation of the scalar product of the forces with the velocity of the particle evaluates the instantaneous power added to the system.
Constraints define the direction of movement of the particle by ensuring there is no component of velocity in the direction of the constraint force. This also means the constraint forces do not add to the instantaneous power. The time integral of this scalar equation yields work from the instantaneous power, and kinetic energy from the scalar product of velocity and acceleration. The fact the work–energy principle eliminates the constraint forces underlies Lagrangian mechanics.
This section focuses on the work–energy principle as it applies to particle dynamics. In more general systems work can change the potential energy of a mechanical device, the heat energy in a thermal system, or the electrical energy in an electrical device. Work transfers energy from one place to another or one form to another.
Derivation for a particle moving along a straight line.
In the case the resultant force F is constant in both magnitude and direction, and parallel to the velocity of the particle, the particle is moving with constant acceleration "a" along a straight line. The relation between the net force and the acceleration is given by the equation "F" = "ma" (Newton's second law), and the particle displacement "s" can be expressed by the equation
which follows from formula_40 (see Equations of motion).
The work of the net force is calculated as the product of its magnitude and the particle displacement. Substituting the above equations, one obtains:
Other derivation:
Vertical displacement derivation
"W = F × S = mg × h"
In the general case of rectilinear motion, when the net force F is not constant in magnitude, but is constant in direction, and parallel to the velocity of the particle, the work must be integrated along the path of the particle:
General derivation of the work–energy theorem for a particle.
For any net force acting on a particle moving along any curvilinear path, it can be demonstrated that its work equals the change in the kinetic energy of the particle by a simple derivation analogous to the equation above. Some authors call this result "work–energy principle", but it is more widely known as the work–energy theorem:
The identity formula_47 requires some algebra.
From the identity formula_48 and definition formula_49
it follows
The remaining part of the above derivation is just simple calculus, same as in the preceding rectilinear case.
Derivation for a particle in constrained movement.
In particle dynamics, a formula equating work applied to a system to its change in kinetic energy is obtained as a first integral of Newton's second law of motion. It is useful to notice that the resultant force used in Newton's laws can be separated into forces that are applied to the particle and forces imposed by constraints on the movement of the particle. Remarkably, the work of a constraint force is zero, therefore only the work of the applied forces need be considered in the work–energy principle.
To see this, consider a particle P that follows the trajectory X("t") with a force F acting on it. Isolate the particle from its environment to expose constraint forces R, then Newton's Law takes the form
where "m" is the mass of the particle.
Vector formulation.
Note that n dots above a vector indicates its nth time derivative.
The scalar product of each side of Newton's law with the velocity vector yields
because the constraint forces are perpendicular to the particle velocity. Integrate this equation along its trajectory from the point X("t"1) to the point X("t"2) to obtain
The left side of this equation is the work of the applied force as it acts on the particle along the trajectory from time "t"1 to time "t"2. This can also be written as
This integral is computed along the trajectory X("t") of the particle and is therefore path dependent.
The right side of the first integral of Newton's equations can be simplified using the following identity
(see product rule for derivation). Now it is integrated explicitly to obtain the change in kinetic energy,
where the kinetic energy of the particle is defined by the scalar quantity,
Tangential and normal components.
It is useful to resolve the velocity and acceleration vectors into tangential and normal components along the trajectory X("t"), such that
where
Then, the scalar product of velocity with acceleration in Newton's second law takes the form
where the kinetic energy of the particle is defined by the scalar quantity,
The result is the work–energy principle for particle dynamics,
This derivation can be generalized to arbitrary rigid body systems.
Moving in a straight line (skid to a stop).
Consider the case of a vehicle moving along a straight horizontal trajectory under the action of a driving force and gravity that sum to F. The constraint forces between the vehicle and the road define R, and we have
For convenience let the trajectory be along the X-axis, so and the velocity is , then , and , where "F"x is the component of F along the X-axis, so
Integration of both sides yields
If Fx is constant along the trajectory, then the integral of velocity is distance, so
As an example consider a car skidding to a stop, where "k" is the coefficient of friction and "W" is the weight of the car. Then the force along the trajectory is . The velocity "v" of the car can be determined from the length "s" of the skid using the work–energy principle,
Notice that this formula uses the fact that the mass of the vehicle is .
Coasting down a mountain road (gravity racing).
Consider the case of a vehicle that starts at rest and coasts down a mountain road, the work-energy principle helps compute the minimum distance that the vehicle travels to reach a velocity "V", of say 60 mph (88 fps). Rolling resistance and air drag will slow the vehicle down so the actual distance will be greater than if these forces are neglected.
Let the trajectory of the vehicle following the road be X("t") which is a curve in three-dimensional space. The force acting on the vehicle that pushes it down the road is the constant force of gravity , while the force of the road on the vehicle is the constraint force R. Newton's second law yields,
The scalar product of this equation with the velocity, , yields
where "V" is the magnitude of V. The constraint forces between the vehicle and the road cancel from this equation because , which means they do no work.
Integrate both sides to obtain
The weight force "W" is constant along the trajectory and the integral of the vertical velocity is the vertical distance, therefore,
Recall that V(t1)=0. Notice that this result does not depend on the shape of the road followed by the vehicle.
In order to determine the distance along the road assume the downgrade is 6%, which is a steep road. This means the altitude decreases 6 feet for every 100 feet traveled—for angles this small the sin and tan functions are approximately equal. Therefore, the distance "s" in feet down a 6% grade to reach the velocity "V" is at least
This formula uses the fact that the weight of the vehicle is .
Work of forces acting on a rigid body.
The work of forces acting at various points on a single rigid body can be calculated from the work of a resultant force and torque. To see this, let the forces F1, F2 ... Fn act on the points X1, X2 ... X"n" in a rigid body.
The trajectories of X"i", "i" = 1, ..., "n" are defined by the movement of the rigid body. This movement is given by the set of rotations ["A"("t")] and the trajectory d("t") of a reference point in the body. Let the coordinates x"i" "i" = 1, ..., "n" define these points in the moving rigid body's reference frame "M", so that the trajectories traced in the fixed frame "F" are given by
The velocity of the points X"i" along their trajectories are
where ω is the angular velocity vector obtained from the skew symmetric matrix
known as the angular velocity matrix.
The small amount of work by the forces over the small displacements "δ"r"i" can be determined by approximating the displacement by so
or
This formula can be rewritten to obtain
where F and T are the resultant force and torque applied at the reference point d of the moving frame "M" in the rigid body.

</doc>
<doc id="165487" url="https://en.wikipedia.org/wiki?curid=165487" title="World line">
World line

In physics, the world line of an object is the path of that object in 4-dimensional spacetime, tracing the history of its location in space at each instant in time. The concept of "world line" is distinguished from the concept of "orbit" or "trajectory" (such as an "orbit in space" or a "trajectory" of a truck on a road map) by the "time" dimension, and typically encompasses a large area of spacetime wherein perceptually straight paths are recalculated to show their (relatively) more absolute position states — to reveal the nature of special relativity or gravitational interactions. The idea of world lines originates in physics and was pioneered by Hermann Minkowski. The term is now most often used in relativity theories (i.e., special relativity and general relativity).
However, world lines are a general way of representing the course of events. The use of it is not bound to any specific theory. Thus in general usage, a world line is the sequential path of personal human events (with "time" and "place" as dimensions) that marks the history of a person — perhaps starting at the time and place of one's birth until one's death. The log book of a ship is a description of the ship's world line, as long as it contains a time tag attached to every position. The world line allows one to calculate the speed of the ship, given a measure of distance (a so-called metric) appropriate for the curved surface of the Earth.
Usage in physics.
In physics, a world line of an object (approximated as a point in space, e.g., a particle or observer) is the sequence of spacetime events corresponding to the history of the object. A world line is a special type of curve in spacetime. Below an equivalent definition will be explained: A world line is a time-like curve in spacetime. Each point of a world line is an event that can be labeled with the time and the spatial position of the object at that time.
For example, the "orbit" of the Earth in space is approximately a circle, a three-dimensional (closed) curve in space: the Earth returns every year to the same point in space. However, it arrives there at a different (later) time. The "world line" of the Earth is helical in spacetime (a curve in a four-dimensional space) and does not return to the same point.
Spacetime is the collection of points called events, together with a continuous and smooth coordinate system identifying the events. Each event can be labeled by four numbers: a time coordinate and three space coordinates; thus spacetime is a four-dimensional space. The mathematical term for spacetime is a four-dimensional manifold. The concept may be applied as well to a higher-dimensional space. For easy visualizations of four dimensions, two space coordinates are often suppressed. The event is then represented by a point in a Minkowski diagram, which is a plane usually plotted with the time coordinate, say formula_1, upwards and the space coordinate, say formula_2 horizontally.
As expressed by F.R. Harvey
A world line traces out the path of a single point in spacetime. A world sheet is the analogous two-dimensional surface traced out by a one-dimensional line (like a string) traveling through spacetime. The world sheet of an open string (with loose ends) is a strip; that of a closed string (a loop) is a volume.
Once the object is not approximated as a mere point but has extended volume, it traces out not a "world line" but rather a world tube.
World lines as a tool to describe events.
A one-dimensional "line" or "curve" can be represented by the coordinates as a function of one parameter. Each value of the parameter corresponds to a point in spacetime and varying the parameter traces out a line. So in mathematical terms a curve is defined by four coordinate functions formula_3 (where formula_4 usually denotes the time coordinate) depending on one parameter formula_5. A coordinate grid in spacetime is the set of curves one obtains if three out of four coordinate functions are set to a constant.
Sometimes, the term world line is loosely used for "any" curve in spacetime. This terminology causes confusions. More properly, a world line is a curve in spacetime which traces out the "(time) history" of a particle, observer or small object. One usually takes the proper time of an object or an observer as the curve parameter formula_5 along the world line.
Trivial examples of spacetime curves.
A curve that consists of a horizontal line segment (a line at constant coordinate time), may represent a rod in spacetime and would not be a world line in the proper sense. The parameter traces the length of the rod.
A line at constant space coordinate (a vertical line in the convention adopted above) may represent a particle at rest (or a stationary observer). A tilted line represents a particle with a constant coordinate speed (constant change in space coordinate with increasing time coordinate). The more the line is tilted from the vertical, the larger the speed.
Two world lines that start out separately and then intersect, signify a "collision" or "encounter." Two world lines starting at the same event in spacetime, each following its own path afterwards, may represent the decay of a particle into two others or the emission of one particle by another.
World lines of a particle and an observer may be interconnected with the world line of a photon (the path of light) and form a diagram which depicts the emission of a photon by a particle which is subsequently observed by the observer (or absorbed by another particle).
Tangent vector to a world line, four-velocity.
The four coordinate functions formula_3
defining a world line, are real functions of a real variable formula_5 and can simply be differentiated in the usual calculus. Without the existence of a metric (this is important to realize) one can speak of the difference between a point formula_9 on the curve at the parameter value formula_10 and a point on the curve a little (parameter formula_11) farther away. In the limit formula_12, this difference divided by formula_13 defines a vector, the tangent vector of the world line at the point formula_9. It is a four-dimensional vector, defined in the point formula_9. It is associated with the normal 3-dimensional velocity of the object (but it is not the same) and therefore called four-velocity formula_16, or in components:
where the derivatives are taken at the point formula_9, so at formula_19.
All curves through point p have a tangent vector, not only world lines. The sum of two vectors is again a tangent vector to some other curve and the same holds for multiplying by a scalar. Therefore, all tangent vectors in a point p span a linear space, called the tangent space at point p. For example, taking a 2-dimensional space, like the (curved) surface of the Earth, its tangent space at a specific point would be the flat approximation of the curved space.
World lines in special relativity.
So far a world line (and the concept of tangent vectors) has been described without a means of quantifying the interval between events. The basic mathematics is as follows: The theory of special relativity puts some constraints on possible world lines. In special relativity the description of spacetime is limited to "special" coordinate systems that do not accelerate (and so do not rotate either), called inertial coordinate systems. In such coordinate systems, the speed of light is a constant. The structure of spacetime is determined by a bilinear form η which gives a real number for each pair of events. The bilinear form is sometimes called a "spacetime metric", but since distinct events sometimes result in a zero value, unlike metrics in metric spaces of mathematics, the bilinear form is "not" a mathematical metric on spacetime.
World lines of particles/objects at constant speed are called geodesics. In special relativity these are straight lines in Minkowski space.
Often the time units are chosen such that the speed of light is represented by lines at a fixed angle, usually at 45 degrees, forming a cone with the vertical (time) axis. In general, useful curves in spacetime can be of three types (the other types would be partly one, partly another type):
At a given event on a world line, spacetime (Minkowski space) is divided into three parts.
Simultaneous hyperplane.
Since a world line formula_20 determines a velocity 4-vector formula_21 that is time-like, the Minkowski form formula_22 determines a linear function formula_23 by formula_24 Let "N" be the null space of this linear functional. Then "N" is called the simultaneous hyperplane with respect to "v". The relativity of simultaneity is a statement that "N" depends on "v". Indeed, "N" is the orthogonal complement of "v" with respect to η. 
When two world lines "u" and "w" are related by formula_25 then they share the same simultaneous hyperplane. This hyperplane exists mathematically, but physical relations in relativity involve the movement of information by light. For instance, the traditional electro-static force described by Coulomb's law may be pictured in a simultaneous hyperplane, but relativistic relations of charge and force involve retarded potentials.
World lines in general relativity.
The use of world lines in general relativity is basically the same as in special relativity, with the difference that spacetime can be curved. A metric exists and its dynamics are determined by the Einstein field equations and are dependent on the mass-energy distribution in spacetime. Again the metric defines lightlike (null), spacelike and timelike curves. Also, in general relativity, world lines are timelike curves in spacetime, where timelike curves fall within the lightcone. However, a lightcone is not necessarily inclined at 45 degrees to the time axis. However, this is an artifact of the chosen coordinate system, and reflects the coordinate freedom (diffeomorphism invariance) of general relativity. Any timelike curve admits a comoving observer whose "time axis" corresponds to that curve, and, since no observer is privileged, we can always find a local coordinate system in which lightcones are inclined at 45 degrees to the time axis. See also for example Eddington-Finkelstein coordinates.
World lines of free-falling particles or objects (such as planets around the Sun or an astronaut in space) are called geodesics.
World lines in literature.
In 1884 C. H. Hinton wrote an essay "What is the fourth dimension ?" which he published as a scientific romance. He wrote
A popular description of human world lines was given by J. C. Fields at the University of Toronto in the early days of relativity. As described by Toronto lawyer Norman Robertson:
Because they oversimplify world lines, which traverse four-dimensional spacetime, into one-dimensional timelines, almost all purported science-fiction stories about time travel are actually wishful fantasy stories. Some device or superpowered person is generally portrayed as departing from one point in time, and with little or no subjective lag, arriving at some other point in time — but at the same literally geographic point in space, typically inside a workshop or near some historic site. However, in reality the planet, its solar system, and its galaxy would all be at vastly different spatial positions on arrival. Thus, the time travel mechanism would also have to provide instantaneous teleportation, with infinitely accurate and simultaneous adjustment of final 3D location, linear momentum, and angular momentum.
World lines appeared in Jeffrey Rowland's webcomic "Wigu Adventures" as part of the "Magical Adventures in Space" side story line, in which Topato Potato and Sheriff Pony accidentally delete a world line relating to the initial creation of Earth from asteroids, causing the Earth to never have existed. According to this webcomic, calculating the exact coordinates of a world line is "embarrassingly simple", and the deletion of the world line specified is executed by making a call and entering the coordinates of the world line, and pressing 3.
Author Oliver Franklin published a science fiction work in 2008 entitled "World Lines" in which he related a simplified explanation of the hypothesis for laymen.
In the short story "Life-Line", author Robert A. Heinlein describes the world line of a person:
Heinlein's "Methuselah's Children" uses the term, as does James Blish's "The Quincunx of Time" (expanded from "Beep").
A visual novel named Steins;Gate, produced by 5pb., tells a story based on the shifting of world lines. Its series of works under the name "hypothetical science ADV" also utilized the concept.

</doc>
<doc id="12253446" url="https://en.wikipedia.org/wiki?curid=12253446" title="X and Y bosons">
X and Y bosons

In particle physics, the X and Y bosons (sometimes collectively called "X bosons") are hypothetical elementary particles analogous to the W and Z bosons, but corresponding to a new type of force predicted by the Georgi–Glashow model, a grand unified theory.
Details.
The X and Y bosons couple quarks to leptons, allowing violation of the conservation of baryon number, and thus permitting proton decay.
An X boson would have the following decay modes:
where the two decay products in each process have opposite chirality, is an up quark, is a down quark and is a positron.
A Y boson would have the following decay modes:
where the first decay product in each process has left-handed chirality and the second has right-handed chirality and is an electron antineutrino.
Similar decay products exist for the other quark-lepton generations
In these reactions, neither the lepton number ("L") nor the baryon number ("B") is conserved, but is. Different branching ratios between the X boson and its antiparticle (as is the case with the K-meson) would explain baryogenesis.

</doc>
<doc id="5481852" url="https://en.wikipedia.org/wiki?curid=5481852" title="Xi baryon">
Xi baryon

The Xi baryons or "cascade particles" are a family of subatomic hadron particles which have the symbol Ξ and may have an elementary charge ("Q") of +2, +1, 0, or −1. Like all baryons, they contain three quarks: one up or down quark, and two more massive quarks. They are historically called the "cascade particles" because of their unstable state; they decay rapidly into lighter particles through a chain of decays. The first discovery of a charged Xi baryon was in cosmic ray experiments by the Manchester group in 1952. The first discovery of the neutral Xi particle was at Lawrence Berkeley Laboratory in 1959. It was also observed as a daughter product from the decay of the omega baryon () observed at Brookhaven National Laboratory in 1964. The Xi spectrum is important to nonperturbative quantum chromodynamics (QCD).
The particle is also known as the cascade B particle and contains quarks from all three families. It was discovered by D0 and CDF experiments at Fermilab. The discovery was announced on 12 June 2007. It was the first known particle made of quarks from all three quark generations – namely, a down quark, a strange quark, and a bottom quark. The D0 and CDF collaborations reported the consistent masses of the new state. The Particle Data Group world average mass is .
Unless specified, the non-up/down quark content of Xi baryons is strange (i.e. there is one up or down quark and two strange quarks). However a contains one up, one strange, and one bottom quark, while a contains one up and two bottom quarks.
In 2012, the CMS experiment at the Large Hadron Collider detected a baryon (reported mass ). LHCb discovered two new Xi baryons in 2014: and 

</doc>
<doc id="2393975" url="https://en.wikipedia.org/wiki?curid=2393975" title="Yang–Mills existence and mass gap">
Yang–Mills existence and mass gap

In mathematical physics, the Yang–Mills existence and mass gap problem is an unsolved problem and one of the seven Millennium Prize Problems defined by the Clay Mathematics Institute, which has offered a prize of US$1,000,000 to the one who solves it.
Official problem description.
The problem is phrased as follows:
In this statement, Yang–Mills theory is a non-Abelian quantum field theory similar to that underlying the Standard Model of particle physics; formula_1 is Euclidean 4-space; the mass gap Δ is the mass of the least massive particle predicted by the theory.
Therefore, the winner must prove that:
For example, in the case of G=SU(3)—the strong nuclear interaction—the winner must prove that glueballs have a lower mass bound, and thus cannot be arbitrarily light.
Background.
The problem requires the construction of a QFT satisfying the Wightman axioms and showing the existence of a mass gap. Both of these topics are described in sections below.
The Wightman axioms.
The Millenium problem requires the proposed Yang-Mill theory to satisfy the Wightman axioms or similarly stringent axioms. There are four axioms:
Quantum mechanics is described according to von Neumann; in particular, the pure states are given by the rays, i.e. the one-dimensional subspaces, of some separable complex Hilbert space. 
The Wightman axioms require that the Poincaré group acts unitarily on the Hilbert space. In other words, they have position dependent operators called quantum fields which form covariant representations of the Poincaré group.
The group of space-time translations is commutative, and so the operators can be simultaneously diagonalised. The generators of these groups give us four self-adjoint operators, formula_3, "j" = 1, 2, 3, which transform under the homogeneous group as a four-vector, called the energy-momentum four-vector.
The second part of the zeroth axiom of Wightman is that the representation "U"("a", "A") fulfills the spectral condition - that the simultaneous spectrum of energy-momentum is contained in the forward cone:
The third part of the axiom is that there is a unique state, represented by a ray in the Hilbert space, which is invariant under the action of the Poincaré group. It is called a vacuum.
For each test function "f", there exists a set of operators formula_6 which, together with their adjoints, are defined on a dense subset of the Hilbert state space, containing the vacuum. The fields "A" are operator-valued tempered distributions. The Hilbert state space is spanned by the field polynomials acting on the vacuum (cyclicity condition).
The fields are covariant under the action of Poincaré group, and they transform according to some representation S of the Lorentz group, or SL(2,C) if the spin is not integer:
If the supports of two fields are space-like separated, then the fields either commute or anticommute.
Cyclicity of a vacuum, and uniqueness of a vacuum are sometimes considered separately. Also, there is property of asymptotic completeness - that Hilbert state space is spanned by the asymptotic spaces formula_8 and formula_9, appearing in the collision S matrix. The other important property of field theory is mass gap which is not required by the axioms - that energy-momentum spectrum has a gap between zero and some positive number.
Mass gap.
In quantum field theory, the mass gap is the difference in energy between the vacuum and the next lowest energy state. The energy of the vacuum is zero by definition, and assuming that all energy states can be thought of as particles in plane-waves, the mass gap is the mass of the lightest particle.
For a given real field formula_10, we can say that the theory has a mass gap if the two-point function has the property
with formula_12 being the lowest energy value in the spectrum of the Hamiltonian and thus the mass gap. This quantity, easy to generalize to other fields, is what is generally measured in lattice computations. It was proved in this way that Yang–Mills theory develops a mass gap on a lattice. 
Importance of Yang–Mills theory.
Most known and nontrivial (i.e. interacting) quantum field theories in 4 dimensions are effective field theories with a cutoff scale. Since the beta-function is positive for most models, it appears that most such models have a Landau pole as it is not at all clear whether or not they have nontrivial UV fixed points. This means that if such a QFT is well-defined at all scales, as it has to be to satisfy the axioms of axiomatic quantum field theory, it would have to be trivial (i.e. a free field theory).
Quantum Yang-Mills theory with a non-abelian gauge group and no quarks is an exception, because asymptotic freedom characterizes this theory, meaning that it has a trivial UV fixed point. Hence it is the simplest nontrivial constructive QFT in 4 dimensions. (QCD is a more complicated theory because it involves quarks.)
Quark confinement.
It has already been well proven—at least at the level of rigor of theoretical physics but not that of mathematical physics—that the quantum Yang–Mills theory for a non-abelian Lie group exhibits a property known as confinement. This property is covered in more detail in the relevant QCD articles (QCD, color confinement, lattice gauge theory, etc.), although not at the level of rigor of mathematical physics. A consequence of this property is that beyond a certain scale, known as the QCD scale (more properly, the confinement scale, as this theory is devoid of quarks), the color charges are connected by chromodynamic flux tubes leading to a linear potential between the charges. Hence free color charge and free gluons cannot exist. In the absence of confinement, we would expect to see massless gluons, but since they are confined, all we would see are color-neutral bound states of gluons, called glueballs. If glueballs exist, they are massive, which is why we expect a mass gap.

</doc>
<doc id="672202" url="https://en.wikipedia.org/wiki?curid=672202" title="Yang–Mills theory">
Yang–Mills theory

Yang–Mills theory is a gauge theory based on the SU("N") group, or more generally any compact, semi-simple Lie group. Yang–Mills theory seeks to describe the behavior of elementary particles using these non-Abelian Lie groups and is at the core of the unification of the electromagnetic and weak forces (i.e. U(1) × SU(2)) as well as quantum chromodynamics, the theory of the strong force (based on SU(3)). Thus it forms the basis of our understanding of particle physics, the Standard Model.
History and theoretical description.
In a private correspondence, Wolfgang Pauli formulated in 1953 a six-dimensional theory of Einstein's field equations of general relativity, extending the five-dimensional theory of Kaluza, Klein, Fock and others to a higher-dimensional internal space. However, there is no evidence that Pauli developed the Lagrangian of a gauge field or the quantization of it. Because Pauli found that his theory "leads to some rather unphysical shadow particles”, he refrained from publishing his results formally. Although Pauli did not publish his six-dimensional theory, he gave two talks about it in Zürich. Recent research shows that an extended Kaluza–Klein theory is in general not equivalent to Yang–Mills theory, as the former contains additional terms.
In early 1954, Chen Ning Yang and Robert Mills extended the concept of gauge theory for abelian groups, e.g. quantum electrodynamics, to nonabelian groups to provide an explanation for strong interactions. The idea by Yang–Mills was criticized by Pauli, as the quanta of the Yang–Mills field must be massless in order to maintain gauge invariance. The idea was set aside until 1960, when the concept of particles acquiring mass through symmetry breaking in massless theories was put forward, initially by Jeffrey Goldstone, Yoichiro Nambu, and Giovanni Jona-Lasinio.
This prompted a significant restart of Yang–Mills theory studies that proved successful in the formulation of both electroweak unification and quantum chromodynamics (QCD). The electroweak interaction is described by SU(2) × U(1) group while QCD is an SU(3) Yang–Mills theory. The electroweak theory is obtained by combining SU(2) with U(1), where quantum electrodynamics (QED) is described by a U(1) group, and is replaced in the unified electroweak theory by a U(1) group representing a weak hypercharge rather than electric charge. The massless bosons from the SU(2) × U(1) theory mix after spontaneous symmetry breaking to produce the 3 massive weak bosons, and the photon field. The Standard Model combines the strong interaction, with the unified electroweak interaction (unifying the weak and electromagnetic interaction) through the symmetry group SU(2) × U(1) × SU(3). In the current epoch the strong interaction is not unified with the electroweak interaction, but from the observed running of the coupling constants it is believed they all converge to a single value at very high energies.
Phenomenology at lower energies in quantum chromodynamics is not completely understood due to the difficulties of managing such a theory with a strong coupling. This may be the reason why confinement has not been theoretically proven, though it is a consistent experimental observation. Proof that QCD confines at low energy is a mathematical problem of great relevance, and an award has been proposed by the Clay Mathematics Institute for whoever is also able to show that the Yang–Mills theory has a mass gap and its existence.
Mathematical overview.
Yang–Mills theories are a special example of gauge theory with a non-abelian symmetry group given by the Lagrangian
with the generators of the Lie algebra corresponding to the "F"-quantities (the curvature or field-strength form) satisfying
and the covariant derivative defined as
where "I" is the identity for the group generators, formula_4 is the vector potential, and "g" is the coupling constant. In four dimensions, the coupling constant "g" is a pure number and for a SU("N") group one has formula_5
The relation
can be derived by the commutator
The field has the property of being self-interacting and equations of motion that one obtains are said to be semilinear, as nonlinearities are both with and without derivatives. This means that one can manage this theory only by perturbation theory, with small nonlinearities.
Note that the transition between "upper" ("contravariant") and "lower" ("covariant") vector or tensor components is trivial for "a" indices (e.g. formula_8), whereas for μ and ν it is nontrivial, corresponding e.g. to the usual Lorentz signature, formula_9.
From the given Lagrangian one can derive the equations of motion given by
Putting formula_11, these can be rewritten as
A Bianchi identity holds
which is equivalent to the Jacobi identity
since formula_15. Define the dual strength tensor
formula_16, then the Bianchi identity can be rewritten as
A source formula_18 enters into the equations of motion as
Note that the currents must properly change under gauge group transformations.
We give here some comments about the physical dimensions of the coupling. We note that, in "D" dimensions, the field scales as formula_20 and so the coupling must scale as formula_21. This implies that Yang–Mills theory is not renormalizable for dimensions greater than four. Further, we note that, for "D" = 4, the coupling is dimensionless and both the field and the square of the coupling have the same dimensions of the field and the coupling of a massless quartic scalar field theory. So, these theories share the scale invariance at the classical level.
Quantization of Yang–Mills theory.
A method of quantizing the Yang–Mills theory is by functional methods, i.e. path integrals. One introduces a generating functional for "n"-point functions as
but this integral has no meaning as it is because the potential vector can be arbitrarily chosen due to the gauge freedom. This problem was already known for quantum electrodynamics but here becomes more severe due to non-abelian properties of the gauge group. A way out has been given by Ludvig Faddeev and Victor Popov with the introduction of a ghost field (see Faddeev–Popov ghost) that has the property of being unphysical since, although it agrees with Fermi–Dirac statistics, it is a complex scalar field, which violates the spin-statistics theorem. So, we can write the generating functional as
being
for the field,
for the gauge fixing and
for the ghost. This is the expression commonly used to derive Feynman's rules (see Feynman diagram). Here we have "ca" for the ghost field while α fixes the gauge's choice for the quantization. Feynman's rules obtained from this functional are the following
These rules for Feynman diagrams can be obtained when the generating functional given above is rewritten as
with
being the generating functional of the free theory. Expanding in "g" and computing the functional derivatives, we are able to obtain all the "n"-point functions with perturbation theory. Using LSZ reduction formula we get from the "n"-point functions the corresponding process amplitudes, cross sections and decay rates. The theory is renormalizable and corrections are finite at any order of perturbation theory.
For quantum electrodynamics the ghost field decouples because the gauge group is abelian. This can be seen from the coupling between the gauge field and the ghost field that is formula_29. For the abelian case, all the structure constants formula_30 are zero and so there is no coupling. In the non-abelian case, the ghost field appears as a useful way to rewrite the quantum field theory without physical consequences on the observables of the theory such as cross sections or decay rates.
One of the most important results obtained for Yang–Mills theory is asymptotic freedom. This result can be obtained by assuming that the coupling constant "g" is small (so small nonlinearities), as for high energies, and applying perturbation theory. The relevance of this result is due to the fact that a Yang–Mills theory that describes strong interaction and asymptotic freedom permits proper treatment of experimental results coming from deep inelastic scattering.
To obtain the behavior of the Yang–Mills theory at high energies, and so to prove asymptotic freedom, one applies perturbation theory assuming a small coupling. This is verified a posteriori in the ultraviolet limit. In the opposite limit, the infrared limit, the situation is the opposite, as the coupling is too large for perturbation theory to be reliable. Most of the difficulties that research meets is just managing the theory at low energies. That is the interesting case, being inherent to the description of hadronic matter and, more generally, to all the observed bound states of gluons and quarks and their confinement (see hadrons). The most used method to study the theory in this limit is to try to solve it on computers (see lattice gauge theory). In this case, large computational resources are needed to be sure the correct limit of infinite volume (smaller lattice spacing) is obtained. This is the limit the results must be compared with. Smaller spacing and larger coupling are not independent of each other, and larger computational resources are needed for each. As of today, the situation appears somewhat satisfactory for the hadronic spectrum and the computation of the gluon and ghost propagators, but the glueball and hybrids spectra are yet a questioned matter in view of the experimental observation of such exotic states. Indeed, the σ resonance is not seen in any of such lattice computations and contrasting interpretations have been put forward. This is a hotly debated issue.
Propagators.
In order to understand the behavior of the theory at large and small momenta, a key quantity is the propagator. For a Yang–Mills theory we have to consider both the gluon and the ghost propagators. At large momenta (ultraviolet limit), the question was completely settled with the discovery of the asymptotic freedom. In this case it is seen that the theory becomes free (trivial ultraviolet fixed point for renormalization group) and both the gluon and ghost propagators are those of a free massless particle. The asymptotic states of the theory are represented by massless gluons that carry the interaction. The coupling runs to zero as we will see in the next section.
At low momenta (infrared limit) the question has been more involved to settle. The reason is that the theory becomes strongly coupled in this case and perturbation theory cannot be applied. The only reliable approach to get an answer is performing lattice computation on a computer powerful enough to afford large volumes. An answer to this question is a fundamental one as it would provide an understanding to the problem of confinement. On the other side, it should not be forgotten that propagators are gauge-dependent quantities and so, they must be managed carefully when one wants to get meaningful physical results.
On the other side, theoretical approaches were conceived to get an understanding of the theory in this case. Pioneering works were due to Vladimir Gribov and Daniel Zwanziger. Gribov uncovered the question of the gauge-fixing in a Yang–Mills theory: He showed that, even once a gauge is fixed, a freedom is left yet (Gribov ambiguity). Besides, he was able to provide a functional form for the gluon propagator in the Landau gauge
This propagator cannot be correct in this way as it would violate causality. On the other side, it provides a linear rising potential, formula_32, that would give reason to quark confinement. An important aspect of this functional form is that "the gluon propagator appears to go to zero with momenta". This will become a crucial point in the following. From these studies by Gribov, Zwanziger extended his approach. The inescapable conclusion was that the gluon propagator should go to zero with momenta while the ghost propagator should be enhanced with respect to the free case running to infinity. This became known in literature as the Gribov-Zwanziger scenario. When this scenario was proposed, computational resources were insufficient to decide whether it was correct or not. Rather, people pursued a different approach using the Dyson-Schwinger equations. This is a set of coupled equations for the n-point functions of the theory forming a hierarchy. This means that the equation for the n-point function will depend on the (n+1)-point function. So, to solve them one needs a proper truncation. On the other side, these equations are non-perturbative and could permit to obtain the behavior of the n-point functions in any regime. A solution to this hierarchy through truncation was proposed by Reinhard Alkofer, Andreas Hauck and Lorenz von Smekal. This paper and the following publications from this group, the German group, set the agenda for the determination of the behavior of the propagators in the Landau gauge in the subsequent years. The main conclusions these authors arrived to were to confirm the Gribov-Zwanziger scenario and that the running coupling should reach a finite non-null fixed point when momenta runs to zero. This paper represents the birth of the so-called "scaling solution" as the propagators are seen to obey scaling laws with given exponents. A proposal in the eighties by John Cornwall was in contrast with this scenario rather showing that the gluons get massive when momenta goes to zero and the propagator should be finite and non-null there but went ignored at that time because the theoretical evidence appeared overwhelming for the Gribov-Zwanziger scenario. Attempts to solve the Dyson-Schwinger equations numerically seemed to provide a different scenario but this could have been due to the way truncation and approximations were applied.
The significant improvement in the computational resources made possible to unveil the proper behavior of the propagators in the Landau gauge. These results where firstly announced in Regensburg at the Lattice 2007 Conference. The results were somewhat unexpected and an example is given in the following figure for the gluon propagator 
that was obtained for the SU(2) case with a lattice of formula_33 points reaching momenta in the very deep infrared. This result from a huge lattice shows that the gluon propagator never goes to zero with momenta but rather reaches a plateau with a finite value at zero momenta. This went called the "decoupling solution" in literature. Similarly, the ghost propagator is seen to behave as that of a free particle. The ghost field just decouples from the gauge field and becomes free in the deep infrared. Other groups at the same conference confirmed similar results.
The decoupling scenario is consistent with a Yukawa-like propagator in the very deep infrared
with formula_35 a constant. The gluon field develops a mass gap parametrized by formula_36 in the above formula, while the BRST symmetry appears to be dynamically broken. These results hold in dimensions greater than 2 while for two dimensions the scaling solution holds. Today, this scenario is generally accepted as the correct one for Yang-Mills theories in the infrared limit having such a strong support from lattice computations. Researches are ongoing for a deeper theoretical understanding of these results and eventual phenomenological applications.
Beta function and running coupling.
One of the key properties of a quantum field theory is the behavior over all the energy range of the running coupling. Such a behavior can be obtained from a theory once its beta function is known. Our ability to extract results from a quantum field theory relies on perturbation theory. Once the beta function is known, the behavior at all energy scales of the running coupling is obtained through the equation
being formula_38. Yang–Mills theory has the property of being asymptotically free in the large energy limit (ultraviolet limit). This means that, in this limit, the beta function has a minus sign driving the behavior of the running coupling toward even smaller values as the energy increases. Perturbation theory permits to evaluate beta function in this limit producing the following result for SU("N")
In the opposite limit of low energies (infrared limit), the beta function is not known. It is note the exact one for a supersymmetric Yang–Mills theory. This has been obtained by Novikov, Shifman, Vainshtein and Zakharov and can be written as
With this starting point, Thomas Ryttov and Francesco Sannino were able to postulate a non-supersymmetric version of it writing down
As can be seen from the beta function of the supersymmetric theory, the limit of a large coupling (infrared limit) implies
and so the running coupling in the deep infrared limit goes to zero making this theory trivial. This implies that the coupling reaches a maximum at some value of the energy turning again to zero as the energy is lowered. Then, if Ryttov and Sannino hypothesis is correct, the same should be true for ordinary Yang–Mills theory. This would be in agreement with recent lattice computations.
Open problems.
Yang–Mills theories met with general acceptance in the physics community after Gerard 't Hooft, in 1972, worked out their renormalization, relying on a formulation of the problem worked out by his advisor Martinus Veltman. (Their work was recognized by the 1999 Nobel prize in physics.) Renormalizability is obtained even if the gauge bosons described by this theory are massive, as in the electroweak theory, provided the mass is only an "acquired" one, generated by the Higgs mechanism.
Concerning the mathematics, it should be noted that presently, i.e. in 2014, the Yang–Mills theory is a very active field of research, yielding e.g. invariants of differentiable structures on four-dimensional manifolds via work of Simon Donaldson. Furthermore, the field of Yang–Mills theories was included in the Clay Mathematics Institute's list of "Millennium Prize Problems". Here the prize-problem consists, especially, in a proof of the conjecture that the lowest excitations of a pure Yang–Mills theory (i.e. without matter fields) have a finite mass-gap with regard to the vacuum state. Another open problem, connected with this conjecture, is a proof of the confinement property in the presence of additional Fermion particles.
In physics the survey of Yang–Mills theories does not usually start from perturbation analysis or analytical methods, but more recently from systematic application of numerical methods to lattice gauge theories.

</doc>
<doc id="7455889" url="https://en.wikipedia.org/wiki?curid=7455889" title="Zero object (algebra)">
Zero object (algebra)

In algebra, the zero object of a given algebraic structure is, in the sense explained below, the simplest object of such structure. As a set it is a singleton, and as a magma has a trivial structure, which is also an abelian group. The aforementioned abelian group structure is usually identified as addition, and the only element is called zero, so the object itself is typically denoted as . One often refers to "the" trivial object (of a specified category) since every trivial object is isomorphic to any other (under a unique isomorphism).
Instances of the zero object include, but are not limited to the following:
These objects are described jointly not only based on the common singleton and trivial group structure, but also because of shared category-theoretical properties.
In the last three cases the scalar multiplication by an element of the base ring (or field) is defined as:
The most general of them, the zero module, is a finitely-generated module with an empty generating set.
For structures requiring the multiplication structure inside the zero object, such as the trivial ring, there is only one possible, , because there are no non-zero elements. This structure is associative and commutative. A ring which has both an additive and multiplicative identity is trivial if and only if , since this equality implies that for all within , 
In this case it is possible to define division by zero, since the single element is its own multiplicative inverse. Some properties of depend on exact definition of the multiplicative identity; see the section Unital structures below.
Any trivial algebra is also a trivial ring. A trivial algebra over a field is simultaneously a zero vector space considered below. Over a commutative ring, a trivial algebra is simultaneously a zero module.
The trivial ring is an example of a rng of square zero. A trivial algebra is an example of a zero algebra.
The zero-dimensional is an especially ubiquitous example of a zero object, a vector space over a field with an empty basis. It therefore has dimension zero. It is also a trivial group over addition, and a "trivial module" mentioned above.
Properties.
The trivial ring, zero module and zero vector space are zero objects of the corresponding categories, namely Rng, -Mod and Vect.
The zero object, by definition, must be a terminal object, which means that a morphism  must exist and be unique for an arbitrary object . This morphism maps any element of  to .
The zero object, also by definition, must be an initial object, which means that a morphism  must exist and be unique for an arbitrary object . This morphism maps , the only element of , to the zero element , called the zero vector in vector spaces. This map is a monomorphism, and hence its image is isomorphic to . For modules and vector spaces, this subset  is the only empty-generated submodule (or 0-dimensional linear subspace) in each module (or vector space) .
Unital structures.
The object is a terminal object of any algebraic structure where it exists, like it was described for examples above. But its existence and, if it exists, the property to be an initial object (and hence, a "zero object" in the category-theoretical sense) depend on exact definition of the multiplicative identity 1 in a specified structure.
If the definition of  requires that , then the object cannot exist because it may contain only one element. In particular, the zero ring is not a field. If mathematicians sometimes talk about a field with one element, this abstract and somewhat mysterious mathematical object is not a field.
In categories where the multiplicative identity must be preserved by morphisms, but can equal to zero, the object can exist. But not as initial object because identity-preserving morphisms from to any object where do not exist. For example, in the category of rings Ring the ring of integers Z is the initial object, not .
If an algebraic structure requires the multiplicative identity, but does not require neither its preserving by morphisms nor , then zero morphisms exist and the situation is not different from non-unital structures considered in the previous section.
Notation.
Zero vector spaces and zero modules are usually denoted by (instead of ). This is always the case when they occur in an exact sequence.

</doc>
<doc id="264210" url="https://en.wikipedia.org/wiki?curid=264210" title="Zero of a function">
Zero of a function

In mathematics, a zero, also sometimes called a root, of a real-, complex- or generally vector-valued function "f" is a member "x" of the domain of "f" such that "f"("x") vanishes at "x"; that is, "x" is a solution of the equation
In other words, a "zero" of a function is an input value that produces an output of zero (0).
A root of a polynomial is a zero of the associated polynomial function.
The fundamental theorem of algebra shows that any non-zero polynomial has a number of roots at most equal to its degree and that the number of roots and the degree are equal when one considers the complex roots (or more generally the roots in an algebraically closed extension) counted with their multiplicities. For example, the polynomial "f" of degree two, defined by 
has the two roots 2 and 3, since
If the function maps real numbers to real numbers, its zeroes are the "x"-coordinates of the points where its graph meets the "x"-axis. An alternative name for such a point ("x",0) in this context is an "x"-intercept.
Solution of an equation.
Every equation in the unknown may easily be rewritten as
by regrouping all terms in the left-hand side. It follows that the solutions of such an equation are exactly the zeros of the function . In other words, "zero of a function" is a phrase denoting a "solution of the equation obtained by equating the function to 0", and the study of zeros of functions is exactly the same as the study of solutions of equations.
Polynomial roots.
Every real polynomial of odd degree has an odd number of real roots (counting multiplicities); likewise, a real polynomial of even degree must have an even number of real roots. Consequently, real odd polynomials must have at least one real root (because one is the smallest odd whole number), whereas even polynomials may have none. This principle can be proven by reference to the intermediate value theorem: since polynomial functions are continuous, the function value must cross zero in the process of changing from negative to positive or vice versa.
Fundamental theorem of algebra.
The fundamental theorem of algebra states that every polynomial of degree "n" has "n" complex roots, counted with their multiplicities. The non-real roots of polynomials with real coefficients come in conjugate pairs. Vieta's formulas relate the coefficients of a polynomial to sums and products of its roots.
Computing roots.
Computing roots of functions, for example polynomial functions, frequently requires the use of specialised or approximation techniques (for example, Newton's method). However, some polynomial functions, including all those of degree no greater than 4, can have all their roots expressed algebraically in terms of their coefficients. (See algebraic solution.)
Zero set.
In topology and other areas of mathematics, the zero set of a real-valued function "f" : "X" → R (or more generally, a function taking values in some additive group) is the subset formula_3 of "X" (the inverse image of {0}).
Zero sets are important in many areas of mathematics. One area of particular importance is algebraic geometry, where the first definition of an algebraic variety is through zero-sets. For instance, for each set "S" of polynomials in "k"["x"1, ..., "x""n"], one defines the zero-locus "Z"("S") to be the set of points in A"n" on which the functions in "S" simultaneously vanish, that is to say

</doc>
<doc id="84400" url="https://en.wikipedia.org/wiki?curid=84400" title="Zero-point energy">
Zero-point energy

Zero-point energy, also called "quantum vacuum zero-point energy", is the lowest possible energy that a quantum mechanical physical system may have; it is the energy of its ground state.
All quantum mechanical systems undergo fluctuations even in their ground state and have an associated zero-point energy, a consequence of their wave-like nature. The uncertainty principle requires every physical system to have a zero-point energy greater than the minimum of its classical potential well. This results in motion even at absolute zero. For example, liquid helium does not freeze under atmospheric pressure at any temperature because of its zero-point energy.
The concept of zero-point energy was developed by Max Planck in Germany in 1911 as a corrective term added to a zero-grounded formula developed in his original quantum theory in 1900. The term "zero-point energy" is a translation from the German "Nullpunktsenergie".
Vacuum energy is the zero-point energy of all the fields in space, which in the Standard Model includes the electromagnetic field, other gauge fields, fermionic fields, and the Higgs field. It is the energy of the vacuum, which in quantum field theory is defined not as empty space but as the ground state of the fields. In cosmology, the vacuum energy is one possible explanation for the cosmological constant. A related term is "zero-point field", which is the lowest energy state of a particular field.
Scientists are not in agreement about how much energy is contained in the vacuum and for what purpose if any it could be used. Quantum mechanics requires the energy to be large as Paul Dirac claimed it is, like a sea of energy. Other scientists specializing in General Relativity require the energy to be small enough for curvature of space to agree with observed astronomy. The Heisenberg uncertainty principle allows the energy to be as large as needed to promote quantum actions for a brief moment of time, even if the average energy is small enough to satisfy relativity and flat space. To cope with disagreements, the vacuum energy is described as a virtual energy potential of positive and negative energy.
While much is known about physical laws, little is known about how the laws are contained in nature, or how the gauge group finds expression in physical actions. Much theoretical work has been done on symmetry groups and topics related to the Standard Model with expectations that a Theory of Everything might be found using fundamental principles to describe the Zero Point Energy, as well as interactions of physical laws and the observed particles of physics. An example is given that Kaluza Klein theory found the Maxwell Equations by adding a fifth dimension to Albert Einstein's field equations. Additional work is continuing in 10 to 12 dimensions of Super Symmetry to describe the vacuum and actions that occur in it. Popular choices for the unifying group are the special unitary group in five dimensions SU(5) and the special orthogonal group in ten dimensions SO(10).
History.
In 1900, Max Planck derived the average energy of a single "energy radiator", e.g., a vibrating atomic unit, as a function of absolute temperature:
where is Planck's constant, is the frequency, is Boltzmann's constant, and is the absolute temperature.
In a series of works from 1911 to 1913, Planck proposed his "second quantum theory", in which he introduced the zero-point energy. Only the emitted radiation was attributed to discrete energy quanta, while the absorbed radiation could be continuous in energy. From these ideas, he found that the average energy of an oscillator is
Soon, the idea of zero-point energy attracted the attention of Albert Einstein and his assistant Otto Stern. They attempted to prove the existence of zero-point energy by calculating the specific heat of hydrogen gas and compared it with the experimental data. However, after assuming they had succeeded and after publishing the findings, they retracted the support of the idea because they found Planck's second theory may not apply to their example.
In 1916 Walther Nernst proposed that empty space was filled with zero-point electromagnetic radiation. Then in 1925, the existence of zero-point energy was shown to be “required by quantum mechanics, as a direct consequence of Heisenberg's uncertainty principle” in Werner Heisenberg's famous article "Quantum theoretical re-interpretation of kinematic and mechanical relations".
Relation to the uncertainty principle.
Zero-point energy is fundamentally related to the Heisenberg uncertainty principle. Roughly speaking, the uncertainty principle states that complementary variables (such as a particle's position and momentum, or a field's value and derivative at a point in space) cannot simultaneously be specified precisely by any given quantum state. In particular, there cannot exist a state in which the system simply sits motionless at the bottom of its potential well: for, then, its position and momentum would both be completely determined to arbitrarily great precision. Therefore, instead, the lowest-energy state (the ground state) of the system must have a distribution in position and momentum that satisfies the uncertainty principle−−which implies its energy must be greater than the minimum of the potential well.
Near the bottom of a potential well, the Hamiltonian of a general system (the quantum-mechanical operator giving its energy) can be approximated as a quantum harmonic oscillator,
where is the minimum of the classical potential well.
The uncertainty principle tells us that
making the expectation values of the kinetic and potential terms above satisfy
The expectation value of the energy must therefore be at least
where formula_7 is the angular frequency at which the system oscillates.
A more thorough treatment, showing that the energy of the ground state actually saturates this bound and is exactly , requires solving for the ground state of the system. 
Varieties.
The concept of zero-point energy occurs in a number of situations.
In ordinary quantum mechanics, the zero-point energy is the energy associated with the ground state of the system. The professional physics literature tends to measure frequency, as denoted by above, using angular frequency, denoted with and defined by . This leads to a convention of writing Planck's constant with a bar through its top () to denote the quantity . In these terms, the most famous such example of zero-point energy is the above associated with the ground state of the quantum harmonic oscillator. In quantum mechanical terms, the zero-point energy is the expectation value of the Hamiltonian of the system in the ground state.
In quantum field theory, the fabric of space is visualized as consisting of fields, with the field at every point in space and time being a quantum harmonic oscillator, with neighboring oscillators interacting. In this case, one has a contribution of from every point in space, resulting in a calculation of infinite zero-point energy in any finite volume; this is one reason renormalization is needed to make sense of quantum field theories. The zero-point energy is again the expectation value of the Hamiltonian; here, however, the phrase vacuum expectation value is more commonly used, and the energy is called the vacuum energy.
In quantum perturbation theory, it is sometimes said that the contribution of one-loop and multi-loop Feynman diagrams to elementary particle propagators are the contribution of vacuum fluctuations, or the zero-point energy to the particle masses.
Experimental observations.
A phenomenon that is commonly presented as evidence for the existence of zero-point energy in vacuum is the Casimir effect, proposed in 1948 by Dutch physicist Hendrik B. G. Casimir (Philips Research), who considered the quantized electromagnetic field between a pair of grounded, neutral metal plates. The vacuum energy contains contributions from all wavelengths, except those excluded by the spacing between plates. As the plates draw together, more wavelengths are excluded and the vacuum energy decreases. The decrease in energy means there must be a force doing work on the plates as they move. This force has been measured and found to be in good agreement with the theory. However, there is still some debate on whether vacuum energy is necessary to explain the Casimir effect. Robert Jaffe of MIT argues that the Casimir force should not be considered evidence for vacuum energy, since it can be derived in QED without reference to vacuum energy by considering charge-current interactions (the radiation-reaction picture).
The experimentally measured Lamb shift has been argued to be, in part, a zero-point energy effect.
Gravitation and cosmology.
In cosmology, the zero-point energy offers an intriguing possibility for explaining the speculative positive values of the proposed cosmological constant.
In brief, if the energy is "really there", then it should exert a gravitational force. In general relativity, mass and energy are equivalent; both produce a gravitational field. One obvious difficulty with this association is that the zero-point energy of the vacuum is absurdly large. Naively, it is infinite, because it includes the energy of waves with arbitrarily short wavelengths. But since only "differences" in energy are physically measurable, the infinity can be removed by renormalization. In all practical calculations, this is how the infinity is handled. It is also arguable that undiscovered physics relevant at the Planck scale reduces or eliminates the energy of waves shorter than the Planck length, making the total zero-point energy finite.
Utilization controversy.
As a scientific concept, the existence of zero-point energy is not controversial. However, the ability to harness zero point energy for useful work is considered pseudoscience by the scientific community at large.
Over the years, there have been numerous claims of devices capable of extracting usable zero-point energy. None of the claims have ever been confirmed by the scientific community, and most of these claims are dismissed by physicists and engineers after third-party inspection of such a device or based on disbelief in the viability of a technical design and theoretical corroboration.
Science skeptic and writer, Martin Gardner has called claims of such zero-point-energy-based systems, "as hopeless as past efforts to build perpetual motion machines" "Perpetual motion machine" refers to technical designs of machines that can operate indefinitely, optionally with additional output of excessive energy, without any cited input source of energy, which is in violation of the laws of thermodynamics. Formally, technical designs that claim to harness zero-point energy would not fall into this category because zero-point energy is claimed as the input source of energy. Despite the scientific stance to typically discount the claims, numerous articles and books have been published addressing and discussing the potential of tapping zero-point-energy from the quantum vacuum or elsewhere. Examples of such are the work of the following authors: Claus Wilhelm Turtur, Jeane Manning, Joel Garbon, John Bedini, Tom Bearden, Thomas Valone, Moray B King, Christopher Toussaint, Bill Jenkins, Nick Cook and William James.
In quantum theory, zero-point energy is a minimum energy below which a thermodynamic system can never go. Thus, none of this energy can be withdrawn without altering the system to a different form in which the system has a lower zero-point energy. One of the hypotheses that claims that zero-point energy is infinite is stochastic electrodynamics. In it, the zero-point field is viewed as simply a classical background isotropic noise wave field which excites all systems present in the vacuum and thus is "responsible for" their minimum-energy or "ground" states. The requirement of Lorentz invariance at a statistical level then implies that the energy density spectrum must increase with the third power of frequency, implying infinite energy density when integrated over all frequencies.
According to a NASA contractor report, "the concept of accessing a significant amount of useful energy from the ZPE gained much credibility when a major article on this topic was published in "Aviation Week & Space Technology" (1 March 2004), a leading aerospace industry magazine".
The calculation that underlies the Casimir experiment, a calculation based on the formula predicting infinite vacuum energy, shows the zero-point energy of a system consisting of a vacuum between two plates will decrease at a finite rate as the two plates are drawn together. The vacuum energies are predicted to be infinite, but the changes are predicted to be finite. Casimir combined the projected rate of change in zero-point energy with the principle of conservation of energy to predict a force on the plates. The predicted force, which is very small and was experimentally measured to be within 5% of its predicted value, is finite. Even though the zero-point energy is theoretically infinite, there is as yet no evidence to suggest that infinite amounts of zero-point energy are available for use, that zero-point energy can be withdrawn for free, or that zero-point energy can be used in violation of conservation of energy.
In the contrary of energy generation, a field of 